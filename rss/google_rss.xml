<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-03-19T22:21:10.826-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="datasets"></category><category term="Education"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="Health"></category><category term="University Relations"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="Computer Science"></category><category term="Quantum AI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Self-Supervised Learning"></category><category term="accessibility"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="NeurIPS"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Responsible AI"></category><category term="ACL"></category><category term="Audio"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ML Fairness"></category><category term="video"></category><category term="Awards"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Supervised Learning"></category><category term="Collaboration"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Speech Recognition"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Large Language Models"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="UI"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Interspeech"></category><category term="Systems"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Differential Privacy"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="crowd-sourcing"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Graphs"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1348&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-520087429457973735&lt;/id>;&lt;发布>;2024-03-19T13:15:00.000-07:00&lt;/发布>;&lt;更新>;2024-03- 19T13:15:33.664-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;ScreenAI：一种视觉语言UI 和视觉情境语言理解模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Srinivas Sunkara 和 Gilles Baechler&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_ L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s320/ScreenAI%20-%20hero.jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 屏幕用户界面 (UI) 和信息图表（例如图表、图表和表格）在人类交流和人机交互中发挥着重要作用，因为它们促进了丰富的交互式用户体验。 UI 和信息图表共享相似的设计原则和视觉语言（例如图标和布局），这提供了构建可以理解、推理并与这些界面交互的单一模型的机会。然而，由于其复杂性和不同的呈现格式，信息图表和 UI 提出了独特的建模挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入“&lt;a href=&quot;https://arxiv.org/abs/2402.04615&quot;>;ScreenAI：一种视觉语言UI 和信息图表理解模型&lt;/a>;”。 ScreenAI 通过来自 &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI 架构&lt;/a>;的灵活修补策略进行了改进”pix2结构&lt;/a>;。我们在独特的数据集和任务组合上训练 ScreenAI，其中包括一项新颖的屏幕注释任务，该任务要求模型识别屏幕上的 UI 元素信息（即类型、位置和描述）。这些文本注释为大型语言模型 (LLM) 提供了屏幕描述，使它们能够自动大规模生成问答 (QA)、UI 导航和摘要训练数据集。仅用 5B 参数，ScreenAI 就可以在基于 UI 和信息图表的任务上实现最先进的结果 (&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;WebSRC&lt;/a>;和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;），以及 &lt;a href=&quot;https://github.com/vis-nlp 上一流的性能/ChartQA&quot;>;图表 QA&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>; 和 &lt; a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。我们还发布了三个新数据集：&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#screen-annotation-dataset-details&quot;>;屏幕注释&lt;/ a>; 评估模型的布局理解能力，以及&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa/tree/main?tab=readme-ov-file#short_answers- directory&quot;>;ScreenQA Short&lt;/a>; 和&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#complexqa&quot; target=&quot;_blank&quot;>;复杂 ScreenQA&lt; /a>; 对其 QA 能力进行更全面的评估。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ScreenAI&lt;/h2>; &lt;p>; ScreenAI 的架构基于 &lt;a href=&quot;https:// /arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>;，由多模态编码器块和自回归解码器组成。 PaLI 编码器使用创建图像嵌入的视觉转换器 (ViT) 和将图像和文本嵌入连接起来的多模态编码器作为输入。这种灵活的架构使 ScreenAI 能够解决可以重新转换为文本+图像到文本问题的视觉任务。 &lt;/p>; &lt;p>; 在 PaLI 架构之上，我们采用了 pix2struct 中引入的灵活修补策略。不使用固定网格图案，而是选择网格尺寸以保留输入图像的原始纵横比。这使得 ScreenAI 能够在各种长宽比的图像上正常工作。 &lt;/p>; &lt;p>; ScreenAI 模型分两个阶段进行训练：预训练阶段和微调阶段。首先，应用自监督学习自动生成数据标签，然后用于训练 ViT 和语言模型。 ViT 在微调阶段被冻结，其中使用的大多数数据都是由人类评估者手动标记的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls 1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s1600/image6.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;583&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8F JBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;ScreenAI模型架构。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;数据生成&lt;/h2>; &lt;p>; 为了创建 ScreenAI 的预训练数据集，我们首先编译来自各种设备（包括台式机、移动设备和平板电脑）的大量屏幕截图。这是通过使用&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot; target=&quot;_blank&quot;>;可公开访问的网页&lt;/a>;并遵循用于&lt;a href=&quot;的编程探索方法来实现的https://dl.acm.org/doi/10.1145/3126594.3126651&quot; target=&quot;_blank&quot;>;RICO 数据集&lt;/a>;，适用于移动应用。然后，我们应用基于 &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; target=&quot;_blank&quot;>;DETR&lt;/a>; 模型的布局注释器，该模型可识别并标记各种 UI 元素（例如，图像、象形图、按钮、文本）及其空间关系。使用能够区分 77 种不同图标类型的&lt;a href=&quot;https://arxiv.org/abs/2210.02663&quot; target=&quot;_blank&quot;>;图标分类器&lt;/a>;对象形图进行进一步分析。这种详细的分类对于解释通过图标传达的微妙信息至关重要。对于分类器未覆盖的图标以及信息图表和图像，我们使用 PaLI 图像字幕模型来生成提供上下文信息的描述性字幕。我们还应用&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot; target=&quot;_blank&quot;>;光学字符识别&lt;/a>; (OCR) 引擎来提取和注释屏幕上的文本内容。我们将 OCR 文本与之前的注释相结合，创建每个屏幕的详细描述。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1V pdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s1747/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1055&quot; data-original-width=&quot;1747&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn 3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;带有生成注释的移动应用屏幕截图，其中包括 UI 元素及其描述，例如，&lt;code>;TEXT&lt;/code>; 元素还包含来自 OCR 的文本内容，&lt;code>;IMAGE&lt;/code>; 元素包含图像标题，&lt;code>;LIST_ITEMs&lt;/code>; 包含其所有子元素。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;基于LLM的数据生成&lt;/h3>; &lt;p>;我们使用&lt;a href=&quot;https://blog.google/technology/ai/google增强预训练数据的多样性-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>; 通过两步过程生成输入-输出对。首先，使用上述技术生成屏幕注释，然后我们围绕此模式制作提示，以便法学硕士创建合成数据。这个过程需要及时的工程和迭代细化才能找到有效的提示。我们通过针对质量阈值的人工验证来评估生成的数据的质量。 &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px;white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;你只讲 JSON。不要编写非 JSON 的文本。您将看到以下移动屏幕截图，并用文字进行了描述。您能否针对屏幕截图的内容生成 5 个问题以及相应的简短答案？答案应尽可能简短，仅包含必要的信息。您的答案结构应如下： questions: [ {{question: 问题，answer: 答案 }}, ... ] {THE SCREEN SCHEMA} &lt;/font>;&lt;/pre>; &lt;br />; &lt;tablealign= &quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;QA 数据生成提示示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 通过结合法学硕士的自然语言能力通过结构化模式，我们模拟各种用户交互和场景，以生成合成的、真实的任务。特别是，我们生成三类任务：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;问答&lt;/strong>;：模型被要求回答有关屏幕截图内容的问题，例如，“什么时候餐厅开门了吗？” &lt;/li>;&lt;li>;&lt;strong>;屏幕导航&lt;/strong>;：要求模型将自然语言话语转换为屏幕上的可执行操作，例如“单击搜索按钮”。 &lt;/li>;&lt;li>;&lt;strong>;屏幕摘要&lt;/strong>;：要求模型用一两句话总结屏幕内容。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceN wDpkvaSLa4R3v4C-hESnHDEC-JUUx31zZmDHDDwhWAMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7 /s1398/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1398&quot; data-original-width=&quot;1272&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHdEc -JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用现有 ScreenAI 模型和 LLM 生成用于 QA、摘要和导航任务的数据的工作流程框图。每个任务都使用自定义提示来强调所需的方面，例如与计数相关的问题、涉及推理的问题等。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;img height=&quot;540&quot; src=&quot;https://lh7-us.googleusercontent.com/LmUtXBMXK-zy_rMShHQ_Hk4vQeXu2Kpx8zfzjhE3uAREczbkbGTEjZ7OMTbqtB37lD4rF31xJsoWdVXNAXLbbM1Uc_01WZWmOfBg9RwyAUETo Ppa1W38Pt117Zj5LrNfnxXqjXoAJDZd-zcAIgU4QSoBaAKsIrSi8_POI14F5hguN1NJL9a2RsrKg6WHz7w&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top : 0 像素;&quot; width=&quot;705&quot; />;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM 生成的数据。屏幕 QA、导航和摘要示例。对于导航，操作边界框在屏幕截图上显示为红色。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;实验与结果&lt;/h2>; &lt;p>; 如前所述，ScreenAI 的训练分两个阶段：预训练和微调。预训练数据标签是使用自我监督学习获得的，微调数据标签来自人类评估者。 &lt;/p>; &lt;p>; 我们使用公共 QA、摘要和导航数据集以及与 UI 相关的各种任务来微调 ScreenAI。对于 QA，我们使用多模式和文档理解领域的完善基准，例如 &lt;a href=&quot;https://github.com/vis-nlp/ChartQA&quot;>;ChartQA&lt;/a>;、&lt;a href=&quot;https ://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch =17&amp;com=tasks&quot;>;多页 DocVQA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>;、&lt;a href=&quot;https://ocr -vqa.github.io/&quot;>;OCR VQA&lt;/a>;、&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;Web SRC&lt;/a>; 和 &lt;a href=&quot;https ://github.com/google-research-datasets/screen_qa&quot;>;ScreenQA&lt;/a>;。对于导航，使用的数据集包括&lt;a href=&quot;https://github.com/google-research-datasets/uibert/tree/main&quot;>;引用表达式&lt;/a>;、&lt;a href=&quot;https://github.com/uibert/tree/main&quot;>;引用表达式&lt;/a>; com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2209.15099&quot;>;马克杯&lt;/a>;和&lt;a href=&quot;https://github.com /google-research/google-research/tree/master/android_in_the_wild&quot;>;野外 Android&lt;/a>;。最后，我们使用 &lt;a href=&quot;https://github.com/google-research-datasets/screen2words&quot;>;Screen2Words&lt;/a>; 进行屏幕摘要，并使用 &lt;a href=&quot;https://paperswithcode.com/paper/ widget-captioning-generate-natural-language/review/&quot;>;小部件标题&lt;/a>;，用于描述特定的 UI 元素。除了微调数据集之外，我们还使用三个新颖的基准来评估微调的 ScreenAI 模型： &lt;/p>; &lt;ol>; &lt;li>;屏幕注释：启用评估模型布局注释和空间理解功能。 &lt;/li>;&lt;li>;ScreenQA Short：ScreenQA 的变体，其真实答案已被缩短，仅包含与其他 QA 任务更好地结合的相关信息。 &lt;/li>;&lt;li>;复杂 ScreenQA：用更困难的问题（计数、算术、比较和不可回答的问题）补充 ScreenQA Short，并包含各种宽高比的屏幕。 &lt;/li>; &lt;/ol>; &lt;p>; 经过微调的 ScreenAI 模型在各种 UI 和基于信息图表的任务上实现了最先进的结果 (&lt;a href=&quot;https://x-lance.github. io/WebSRC/&quot;>;WebSRC&lt;/a>; 和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;）以及 &lt;a href=&quot;https 上一流的性能://github.com/vis-nlp/ChartQA&quot;>;图表质量检查&lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1 &quot;>;DocVQA&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。 ScreenAI 在 Screen2Words 和 OCR-VQA 上实现了具有竞争力的性能。此外，我们还报告了新引入的基准数据集的结果，作为进一步研究的基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOswns-GWJRdSCj3UmyxytOZxfoM64 psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s1183/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1137&quot; data-original-width=&quot;1183&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFbN pydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;将 ScreenAI 的模型性能与相似尺寸的最先进 (SOTA) 模型进行比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们检查 ScreenAI 的扩展能力，并观察到在所有任务中，增加模型大小可以提高性能，并且在最大尺寸时改进尚未饱和。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGw KGyO6YkaW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys869f 7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模型性能随着尺寸的增大而提高，即使在最大 5B 参数尺寸下性能也没有饱和。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们引入了 ScreenAI 模型以及统一的表示使我们能够利用所有这些领域的数据开发自我监督的学习任务。我们还说明了使用法学硕士生成数据的影响，并研究通过修改训练混合物来提高模型在特定方面的性能。我们应用所有这些技术来构建多任务训练模型，这些模型在许多公共基准上与最先进的方法相媲美。然而，我们也注意到，我们的方法仍然落后于大型模型，需要进一步的研究来弥补这一差距。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目是与 Maria 共同工作的结果Wang、Fedir Zubach、Hassan Mansoor、Vincent Etter、Victor Carbune、Jason Lin、Jindong Chen 和 Abhanshu Sharma。我们感谢 Fangyu Liu、Xi Chen、Efi Kokiopoulou、Jesse Berent、Gabriel Barcik、Lukas Zilka、Oriana Riva、Gang Li、Yang Li、Radu Soricut 和 Tania Bedrax-Weiss 的富有洞察力的反馈和讨论，以及 Rahul Aralikatte、Hao Cheng 和 Daniel Kim 在数据准备方面的支持。我们还感谢 Jay Yagnik、Blaise Aguera y Arcas、Ewa Dominowska、David Petrou 和 Matt Sharifi 的领导、远见和支持。我们非常感谢 Tom Small 帮助我们创建本文中的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/520087429457973735/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/screenai-visual-language-model- for-ui.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/520087429457973735&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/520087429457973735&quot; rel=&quot;self “ type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html&quot; rel=&quot;alternate&quot; title=&quot;ScreenAI：用于 UI 和视觉情境语言理解的视觉语言模型&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFj tvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s72-c/ScreenAI%20-%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search .yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post -4328167517765145678&lt;/id>;&lt;发布>;2024-03-19T08:00:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-19T08:00:00.150-07:00&lt;/更新>;&lt;类别方案= “http://www.blogger.com/atom/ns#” term=&quot;crowd-source&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= “数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“多样性”>;&lt;/类别>;&lt;类别方案=“http://www.blogger” .com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SCIN：代表性皮肤病图像的新资源&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class =&quot;byline-author&quot;>;发布者：Google 研究部科学家 Pooja Rao&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OY nZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUBJ/s1600/ SCINHero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 健康数据集在研究和医学教育中发挥着至关重要的作用，但创建代表现实世界的数据集可能具有挑战性。例如，皮肤病的外观和严重程度各不相同，并且不同肤色的表现也不同。然而，现有的皮肤科图像数据集通常缺乏对日常状况（如皮疹、过敏和感染）的表征，并且偏向较浅的肤色。此外，种族和民族信息经常缺失，阻碍了我们评估差异或制定解决方案的能力。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了解决这些限制，我们发布了&lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>;皮肤状况图像网络 (SIN) 数据集&lt;/a>;与 &lt;a href=&quot;https://med.stanford.edu/&quot;>;Stanford Medicine&lt;/a>; 的医生合作。我们设计 SCIN 是为了反映人们在线搜索的广泛问题，补充临床数据集中常见的疾病类型。它包含各种肤色和身体部位的图像，有助于确保未来的人工智能工具对所有人有效工作。我们已将 &lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>;SCIN 数据集&lt;/a>;作为开放访问资源免费提供给研究人员、教育工作者和开发人员，并且已采取谨慎的措施来保护贡献者的隐私。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO 6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s1327/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1118&quot; data-original-width=&quot;1327&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDARYkvnLUz5glvPlNpI 1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;来自 SCIN 数据集的图像和元数据示例集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;数据集组成&lt;/h2>; &lt;p>; SCIN 数据集目前包含超过 10,000 张皮肤、指甲或头发状况的图像，这些图像由经历这些状况的个人直接贡献。根据机构审查委员会批准的一项研究，所有贡献都是在美国个人知情同意的情况下自愿做出的。为了为皮肤科医生的回顾性标签提供背景信息，贡献者被要求拍摄特写和稍远距离的图像。他们可以选择自我报告人口统计信息和&lt;a href=&quot;https://en.wikipedia.org/wiki/Fitzpatrick_scale&quot;>;晒黑倾向&lt;/a>;（自我报告的菲茨帕特里克皮肤类型，即 sFST） ，并描述与其关注相关的质地、持续时间和症状。 &lt;/p>; &lt;p>; 一到三名皮肤科医生用最多五个皮肤病学条件标记每个贡献，以及每个标签的置信度得分。 SCIN 数据集包含这些单独的标签，以及从它们导出的聚合和加权的鉴别诊断，可用于模型测试或训练。这些标签是回顾性分配的，并不等同于临床诊断，但它们使我们能够将 SCIN 数据集中皮肤病状况的分布与现有数据集进行比较。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3 A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s1851/image2.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;775&quot; data-original-width=&quot;1851&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZ y54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;SCIN 数据集主要包含过敏、炎症和感染性疾病，而来自临床来源的数据集则侧重于良性和恶性 &lt;a href=&quot;https://en.wikipedia.org/ wiki/Neoplasm&quot;>;肿瘤&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;虽然许多现有的皮肤病学数据集侧重于恶性和良性肿瘤，旨在协助皮肤癌诊断，SCIN 数据集主要由常见的过敏、炎症和感染性疾病组成。 SCIN 数据集中的大多数图像都显示出早期阶段的问题——超过一半是在照片拍摄前不到一周出现的，30% 是在照片拍摄前不到一天出现的。此时间范围内的情况在卫生系统中很少见，因此在现有皮肤病学数据集中代表性不足。 &lt;/p>; &lt;p>; 我们还获得了皮肤科医生对 Fitzpatrick 皮肤类型的估计（估计 FST 或 eFST）以及外行贴标者对 &lt;a href=&quot;https://en.wikipedia.org/wiki/Monk_Skin_Tone_Scale&quot;>;和尚肤色的估计&lt;/a>; (eMST) 用于图像。这使得可以将皮肤状况和皮肤类型分布与现有皮肤病学数据集中的皮肤状况和皮肤类型分布进行比较。尽管我们没有选择性地针对任何皮肤类型或肤色，但与临床来源的类似数据集相比，SCIN 数据集具有平衡的 Fitzpatrick 皮肤类型分布（更多类型 3、4、5 和 6）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeIN Xc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s1851 /image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;620&quot; data-original-width=&quot;第1851章JL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;与现有未丰富的皮肤病学数据集相比，SCIN 数据集中自我报告和皮肤科医生估计的 Fitzpatrick 皮肤类型分布 &lt; a href=&quot;https://github.com/mattgroh/fitzpatrick17k&quot;>;(Fitzpatrick17k&lt;/a>;, &lt;a href=&quot;https://www.fc.up.pt/addi/ph2%20database.html&quot;>; PH²&lt;/a>;、&lt;a href=&quot;https://www.it.pt/AutomaticPage?id=3459&quot;>;SKINL2&lt;/a>; 和&lt;a href=&quot;https://www.ncbi.nlm。 nih.gov/pmc/articles/PMC7479321/&quot;>;PAD-UFES-20&lt;/a>;)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; &lt;a href=&quot;https: //en.wikipedia.org/wiki/Fitzpatrick_scale&quot;>;菲茨帕特里克皮肤类型&lt;/a>;量表最初是作为照片打字量表开发的，用于测量皮肤类型对紫外线辐射的反应，广泛应用于皮肤病学研究。 Monk 肤色等级是一种较新的 10 色度等级，用于测量肤色而不是皮肤光型，捕捉较深肤色之间更细微的差异。虽然这两种量表都不是为了使用图像进行回顾性估计，但包含这些标签的目的是为了使未来能够对皮肤病学中的皮肤类型和色调表示进行研究。例如，SCIN 数据集为美国人口中这些皮肤类型和色调的分布提供了初始基准。 &lt;/p>; &lt;p>; SCIN 数据集女性和年轻人的代表性很高，这可能反映了多种因素的综合影响。这些可能包括皮肤状况发生率的差异、在线寻求健康信息的倾向以及不同人群对研究做出贡献的意愿的差异。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;众包方法&lt;/h2>; &lt;p>; 为了创建 SCIN 数据集，我们使用了一种新颖的众包方法，我们在与 &lt;a href=&quot;https://med.stanford.edu 的研究人员共同撰写的&lt;a href=&quot;https://arxiv.org/abs/2402.18545&quot;>;研究论文&lt;/a>;中对此进行了描述/&quot;>;斯坦福大学医学&lt;/a>;。这种方法使个人能够在医疗保健研究中发挥积极作用。它使我们能够在人们健康问题的早期阶段（可能在他们寻求正式护理之前）接触到他们。至关重要的是，这种方法使用网络搜索结果页面上的广告（许多人健康之旅的起点）来与参与者建立联系。 &lt;/p>; &lt;p>; 我们的结果表明，众包可以产生具有较低垃圾邮件率的高质量数据集。超过 97.5% 的贡献是皮肤状况的真实图像。在执行进一步的过滤步骤以排除超出 SCIN 数据集范围的图像并删除重复项后，我们能够发布在 8 个月的研究期间收到的近 90% 的贡献。大多数图像都很清晰且曝光良好。大约一半的贡献包括自我报告的人口统计数据，80% 包含与皮肤状况相关的自我报告信息，例如质地、持续时间或其他症状。我们发现皮肤科医生回顾性鉴别诊断的能力更多地取决于自我报告信息的可用性，而不是图像质量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ 54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s1999/image4.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1610&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fD ufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;皮肤科医生对其标签的信心（范围从 1 到 5）取决于自我报告的人口统计和症状信息的可用性。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 虽然永远无法保证完美的图像去识别，但在创建 SCIN 数据集时，保护贡献图像的个人的隐私是首要任务。通过知情同意，贡献者意识到潜在的重新识别风险，并建议避免上传具有识别特征的图像。提交后的隐私保护措施包括手动编辑或裁剪以排除潜在的识别区域、反向图像搜索以排除公开可用的副本以及元数据删除或聚合。 SCIN &lt;a href=&quot;https://github.com/google-research-datasets/scin?tab=License-1-ov-file#readme&quot;>;数据使用许可&lt;/a>;禁止尝试重新识别贡献者。 &lt;/p>; &lt;p>; 我们希望 SCIN 数据集能够为那些致力于推进包容性皮肤病学研究、教育和人工智能工具开发的人们提供有用的资源。通过展示传统数据集创建方法的替代方法，SCIN 为自我报告数据或回顾性标记可行的领域中更具代表性的数据集铺平了道路。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢所有合著者 Abbi Ward , Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirisokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel (斯坦福大学医学院) ）、Steven Lin（斯坦福医学）、Justin Ko（斯坦福医学）、Alan Karthikesalingam 和 Christopher Semturs。我们还感谢 Yetunde Ibitoye、Sami Lachgar、Lisa Lehmann、Javier Perez、Margaret Ann Smith（斯坦福大学医学院）、Rachelle Sico、Amit Talreja、Annisah Um&#39;rani 和 Wayne Westerlind 对这项工作的重要贡献。最后，我们感谢 Heather Cole-Lewis、Naama Hammel、Ivor Horn、Michael Howell、Yun Liu 和 Eric Teasley 对研究设计和手稿的富有洞察力的评论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4328167517765145678/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/scin-new-resource-for-representative.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4328167517765145678&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4328167517765145678&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/scin-new-resource-for-representative.html&quot; rel=&quot;alternate&quot; title=&quot;SCIN：代表性皮肤病图像的新资源&quot; type=&quot;text/html&quot; />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image高度=“16” rel=“http://schemas.google.com/g/2005#thumbnail” src=“https://img1.blogblog.com/img/b16-rounded.gif” 宽度=“16” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpAr Gn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s72-c/SCINHero .png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;条目>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-757976001746578714&lt;/id>;&lt;发布>;2024-03-18T11:41:00.000-07:00&lt;/发布>;&lt;更新>;2024-03 -18T12：01：42.865-07：00 &lt;/更新>; &lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“计算机视觉”>;&lt;/类别>;&lt;类别方案=“ http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MELON：从姿势未知的图像重建 3D 对象&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Research 高级软件工程师 Mark Matthews 和研究科学家 Dmitry Lagun&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gVa Y-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s320/MELON%20HERO.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 一个人先前的经验和对世界的理解通常使他们能够轻松地推断出一个物体的整体外观，即使只看它的一些 2D 图片。然而，计算机在仅给定少量图像的情况下以 3D 形式重建物体形状的能力多年来仍然是一个困难的算法问题。这项基本的计算机视觉任务的应用范围从电子商务 3D 模型的创建到自动车辆导航。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 问题的关键部分是如何确定拍摄图像的确切位置，即&lt;em>;姿势推断&lt;/em>;。如果相机姿势已知，则可以使用一系列成功的技术 - 例如&lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;神经辐射场&lt;/a>; (NeRF) 或&lt;a href=&quot;https: //repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&quot;>;3D 高斯分布&lt;/a>; — 可以重建 3D 对象。但如果这些姿势不可用，那么我们将面临一个困难的“先有鸡还是先有蛋”的问题，如果我们知道 3D 对象，我们就可以确定姿势，但在我们知道相机姿势之前，我们无法重建 3D 对象。伪对称性使问题变得更加困难——即，从不同角度观察时，许多物体看起来很相似。例如，像椅子这样的方形物体每旋转 90° 看起来就会很相似。通过从不同角度将物体渲染在转盘上并绘制其光度&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-similarity&quot;>;自相似性&lt;/a，可以揭示物体的伪对称性>; 地图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRm kpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s1764/image5.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;923&quot; data-original-width=&quot;1764&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL 2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;玩具卡车模型的自相似图。 &lt;strong>;左：&lt;/strong>;模型在转盘上以各种&lt;a href=&quot;https://en.wikipedia.org/wiki/Azimuth&quot;>;方位角&lt;/a>;、θ 进行渲染。 &lt;strong>;右：&lt;/strong>; θ 渲染的平均 &lt;a href=&quot;https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm&quot;>;L2&lt;/a>; RGB 相似度θ*。伪相似性由红色虚线表示。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 上图仅可视化一维旋转。当引入更多自由度时，它会变得更加复杂（并且难以可视化）。伪对称性使问题变得不适定，简单的方法常常收敛于局部最小值。在实践中，这种方法可能会将后视图误认为是对象的前视图，因为它们具有相似的轮廓。以前的技术（例如 &lt;a href=&quot;https://chenhsuanlin.bitbucket.io/bundle- adjustment-NeRF/&quot;>;BARF&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/abs/2205.15768 &quot;>;SAMURAI&lt;/a>;）通过依赖接近全局最小值的初始姿态估计来回避这个问题。但如果这些都不可用，我们该如何解决这个问题呢？ &lt;/p>; &lt;p>; 方法，例如 &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_GNeRF_GAN-Based_Neural_Radiance_Field_Without_Posed_Camera_ICCV_2021_paper.pdf&quot;>;GNeRF&lt;/a>; 和 &lt;a href=&quot; https://dl.acm.org/doi/10.1145/3503161.3548078&quot;>;VMRF&lt;/a>;利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;>;生成对抗网络&lt;/a>; （GAN）来克服这个问题。这些技术能够人为地“放大”有限数量的训练视图，从而帮助重建。然而，GAN 技术通常具有复杂且有时不稳定的训练过程，使得在实践中难以实现稳健且可靠的收敛。一系列其他成功的方法，例如 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.html&quot;>;SparsePose&lt;/a>; 或 &lt;a href=&quot;https: //rust-paper.github.io/&quot;>;RUST&lt;/a>;，可以从有限数量的视图中推断姿势，但需要对大量姿势图像数据集进行预训练，而这些图像并不总是可用的，并且可能会受到影响推断不同类型图像的姿势时的“域间隙”问题。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;MELON: NeRF with Unposeed Images in SO(3)&lt;/a>;”中，重点关注&lt;a href= “https://3dvconf.github.io/2024/&quot;>;3DV 2024&lt;/a>;，我们提出了一种技术，可以在以 3D 方式重建对象的同时完全从头开始确定以对象为中心的相机姿势。 &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON&lt;/a>;（NeRF 的模等效潜在优化）是第一个无需初始相机姿态估计和复杂训练即可实现此目的的技术对标记数据进行方案或预训练。 MELON 是一种相对简单的技术，可以轻松集成到现有的 NeRF 方法中。我们证明，MELON 可以从未摆出的图像中以最先进的精度重建 NeRF，同时只需要 4-6 个物体的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MELON&lt;/h2>; &lt;p>; 我们利用两项关键技术来帮助这种不适定的收敛问题。第一个是一个非常轻量级的、动态训练的&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 编码器，可以从训练图像中回归相机姿势。我们将缩小的训练图像传递给四层 CNN，以推断相机姿态。该 CNN 从噪声中初始化，无需预训练。它的容量非常小，以至于它迫使相似的图像呈现相似的姿势，提供隐式正则化，极大地帮助收敛。 &lt;/p>; &lt;p>; 第二种技术是模损失，同时考虑对象的伪对称性。我们从每个训练图像的一组固定视点渲染对象，仅通过最适合训练图像的视图反向传播损失。这有效地考虑了每个图像的多个视图的合理性。在实践中，我们发现大多数情况下只需要 &lt;em>;N&lt;/em>;=2 个视图（从另一侧查看对象），但有时使用 &lt;em>;N&lt;/em>;=4 可以获得更好的结果方形物体。 &lt;/p>; &lt;p>; 这两种技术被集成到标准 NeRF 训练中，只不过不是固定相机姿势，而是由 CNN 推断姿势并通过模损失复制。光度梯度通过最合适的相机反向传播到 CNN。我们观察到相机通常会快速收敛到全局最佳姿势（参见下面的动画）。经过神经场训练后，MELON 可以使用标准 NeRF 渲染方法合成新颖的视图。 &lt;/p>; &lt;p>; 我们通过使用 &lt;a href=&quot;https://github.com/bmild/nerf&quot;>;NeRF-Synthetic&lt;/a>; 数据集来简化问题，该数据集是 NeRF 研究的流行基准，在姿势推断文献。这个合成数据集的摄像头距离精确固定，并且方向一致，要求我们仅推断出相机。这与位于地球中心的物体相同，相机始终指向它，并沿着表面移动。然后我们只需要纬度和经度（2 个自由度）来指定相机姿态。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7l ApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s1315/image4.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;395&quot; data-original-width=&quot;1315&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gayA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekg bFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;MELON 使用动态训练的轻量级 CNN 编码器来预测每个图像的姿势。预测的姿势通过模损失来复制，它仅惩罚距地面真实颜色的最小 L2 距离。在评估时，神经场可用于生成新颖的视图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们计算两个关键指标来评估 MELON 在 NeRF 综合数据集上的性能。地面实况和推断姿势之间的方向误差可以量化为我们在所有训练图像上平均的单个角度误差，即姿势误差。然后，我们通过测量&lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;峰值信噪比&lt;/a>;来测试 MELON 从新视图渲染对象的准确性（PSNR）反对提出测试意见。我们看到，MELON 在训练的前 1,000 步内快速收敛到大多数相机的近似姿势，并在 50k 步后实现了 27.5 dB 的有竞争力的 PSNR。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02y b3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s960/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz 8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;优化过程中玩具卡车模型上的 MELON 收敛。 &lt;strong>;左&lt;/strong>;：NeRF 的渲染。 &lt;strong>;右&lt;/strong>;：预测（蓝色&lt;em>;x&lt;/em>;）和地面实况（红点）摄像机的极坐标图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MELON 在 NeRF 合成数据集中的其他场景中取得了类似的结果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRK ToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s1999/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7ck-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqy WIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;100k 训练步骤后 NeRF-Synthetic 场景上的 ground-truth (GT) 和 MELON 的重建质量比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;嘈杂的图像&lt;/h3>; &lt;p>; MELON 在执行&lt;a href=&quot;https://en.wikipedia 时也能很好地工作。 org/wiki/View_synthesis&quot;>;新颖的视图合成&lt;/a>;来自极其嘈杂的、未摆姿势的图像。我们向训练图像中添加不同数量的&lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise&quot;>;高斯白噪声&lt;/a>;。例如，下面 &lt;em>;σ&lt;/em>;=1.0 中的物体是无法辨认的，但 MELON 可以确定该物体的姿势并生成该物体的新颖视图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTR VkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s1182/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1182&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh _hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从嘈杂的未摆姿势的 128×128 图像中合成新颖的视图。顶部：训练视图中存在的噪声级别示例。底部：根据嘈杂的训练视图和平均角度姿势误差重建模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;考虑到诸如 &lt;a href= 之类的技术，这也许不应该太令人惊讶“https://bmild.github.io/rawnerf/&quot;>;RawNeRF&lt;/a>; 展示了 NeRF 在已知相机姿势的情况下出色的去噪能力。事实上，MELON 对于未知相机姿势的噪声图像如此有效，这是出乎意料的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 MELON，一种可以确定以对象为中心的相机的技术姿势重建 3D 对象，无需近似姿势初始化、复杂的 GAN 训练方案或对标记数据进行预训练。 MELON 是一种相对简单的技术，可以轻松集成到现有的 NeRF 方法中。虽然我们只在合成图像上演示了 MELON，但我们正在调整我们的技术以适应现实世界的条件。请参阅&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;论文&lt;/a>;和&lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON 网站&lt;/a>; >; 了解更多。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的论文共同作者Axel Levy、Matan Sela 和 Gordon Wetzstein，以及 Florian Schroff 和 Hartwig Adam 在构建这项技术方面不断提供帮助。我们还感谢 Matthew Brown、Ricardo Martin-Brualla 和 Frederic Poitevin 对论文草稿提供的有益反馈。我们还感谢使用 SLAC 共享科学数据设施 (SDF) 的计算资源。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/757976001746578714 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing- 3d-objects-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/757976001746578714&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel= “self” type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html&quot; rel=&quot;alternate&quot; title=&quot;MELON：从姿势未知的图像重建 3D 对象&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_ gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s72-c/MELON%20HERO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search. yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post- 977556648557231190&lt;/id>;&lt;发布>;2024-03-15T11:22:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-15T11:22:13.760-07:00&lt;/更新>;&lt;类别方案= http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI &quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;HEAL：机器学习性能健康公平评估框架&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者迈克·谢克曼 (Mike Schaekermann)，谷歌研究院研究科学家；艾弗·霍恩 (Ivor Horn)，首席健康股权官Google 核心总监&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn 2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s1600/HEAL-Hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 健康公平是世界范围内的一个主要社会问题，其差异有多种原因。这些来源包括获得医疗保健的限制、临床治疗的差异，甚至诊断技术的根本差异。例如，在皮肤科中，少数族裔、社会经济地位较低的人或医疗保健机会有限的个人等人群的皮肤癌结果更差。虽然机器学习 (ML) 和人工智能 (AI) 的最新进展有望帮助改善医疗保健，但从研究到临床的转变必须伴随着仔细了解它们是否以及如何影响健康公平。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;em>;健康公平&lt;/em>;由公共卫生组织定义为每个人尽可能保持健康的机会公平。重要的是，公平可能不同于&lt;em>;平等&lt;/em>;。例如，在改善健康方面存在更大障碍的人可能需要更多或不同的努力才能体验这个公平的机会。同样，公平并不是医疗保健人工智能文献中定义的&lt;em>;公平&lt;/em>;。尽管人工智能公平性通常力求人工智能技术在不同患者群体中具有平等的性能，但这并不以针对先前存在的健康差异优先考虑性能为目标。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmF n4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s1999 /image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1609&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6X QVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s16000/image2.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;健康公平考虑因素。如果干预措施（例如基于机器学习的工具，以深蓝色表示）有助于减少现有的健康结果差异（以浅蓝色表示），则可以促进健康公平。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; 在“&lt;a href=&quot;https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00058-0/fulltext&quot;>;机器学习性能的健康公平评估 (HEAL) ：框架和皮肤病学人工智能模型案例研究&lt;/a>;”，发表于&lt;a href=&quot;https://www.thelancet.com/journals/eclinm/home&quot;>;&lt;i>;The Lancet eClinicalMedicine&lt;/i>;&lt; /a>;，我们提出了一种定量评估基于机器学习的医疗技术是否公平运行的方法。换句话说，机器学习模型对于那些在模型要解决的情况下健康状况最差的人来说是否表现良好？这一目标基于以下原则：健康公平应优先考虑并衡量模型在不同健康结果方面的表现，这可能是由于包括结构性不平等在内的许多因素（例如人口、社会、文化、政治、经济、环境和地理）。 &lt;/p>; &lt;br />; &lt;h2>;健康公平框架 (HEAL)&lt;/h2>; &lt;p>; HEAL 框架提出了一个 4 步流程来估计基于机器学习的健康技术公平执行的可能性：&lt;/p>; p>; &lt;ol>; &lt;li>; 确定与健康不平等相关的因素并定义工具性能指标，&lt;/li>; &lt;li>; 确定并量化先前存在的健康差异，&lt;/li>; &lt;li>; 衡量工具的性能每个亚群体，&lt;/li>; &lt;li>; 衡量该工具根据健康差异优先考虑绩效的可能性。 &lt;/li>; &lt;/ol>; &lt;p>; 最后一步的输出称为 HEAL 指标，它量化 ML 模型的性能与健康差异的反相关程度。换句话说，该模型对于健康状况较差的人群是否表现更好？ &lt;/p>; &lt;p>; 这个 4 步流程旨在提供改进信息，使 ML 模型性能更加公平，并定期进行迭代和重新评估。例如，步骤（2）中健康结果数据的可用性可以告知步骤（1）中人口因素和括号的选择，并且该框架可以再次应用新的数据集、模型和人群。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4 sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s1999/image1.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1352&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14 QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;机器学习性能健康公平评估框架 (HEAL)。我们的指导原则是避免加剧健康不平等，这些步骤帮助我们识别差异并评估不公平模型&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;通过这项工作，我们朝着鼓励对人工智能技术的健康公平考虑因素进行明确评估迈出了一步，并且鼓励在模型开发过程中优先考虑减少面临结构性不平等的亚人群的健康不平等，因为结构性不平等可能会导致不同的结果。我们应该注意到，目前的框架并未对因果关系进行建模，因此无法量化新技术对减少健康结果差异的实际影响。然而，HEAL 指标可能有助于识别改进的机会，其中当前的表现并未优先考虑先前存在的健康差异。 &lt;/p>; &lt;br />; &lt;h2>;皮肤病学模型案例研究&lt;/h2>; &lt;p>; 作为一个说明性案例研究，我们将该框架应用于皮肤病学模型，该模型利用类似于中描述的卷积神经网络&lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;之前的工作&lt;/a>;。此示例皮肤病学模型经过训练，可使用包含 29,000 个病例的开发数据集对 288 种皮肤状况进行分类。模型的输入包括三张皮肤问题的照片以及人口统计信息和简短的结构化病史。输出包含可能匹配的皮肤状况的排名列表。 &lt;/p>; &lt;p>; 使用 HEAL 框架，我们通过评估该模型是否根据预先存在的健康结果优先考虑性能来评估该模型。该模型旨在根据皮肤问题的照片和患者元数据预测可能的皮肤病（从数百个列表中）。模型的评估是使用前 3 个一致性指标来完成的，该指标量化了前 3 个输出条件与皮肤科医生小组建议的最可能条件相匹配的频率。 HEAL 指标是通过此前 3 名协议与健康结果排名的反相关来计算的。 &lt;/p>; &lt;p>; 我们使用了包含 5,420 个远程皮肤病病例的数据集（丰富了年龄、性别和种族/民族的多样性）来回顾性评估模型的 HEAL 指标。该数据集包含来自美国初级保健提供者和澳大利亚皮肤癌诊所的 20 岁或以上患者的“存储并转发”病例。基于对文献的回顾，我们决定探索种族/民族、性别和年龄作为不平等的潜在因素，并使用抽样技术来确保我们的评估数据集充分代表所有种族/民族、性别和年龄组。为了量化每个亚组预先存在的健康结果，我们依赖于 &lt;a href=&quot;https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates/global- health-estimates-leading-causes-of-dalys&quot;>;公共&lt;/a>; &lt;a href=&quot;https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30925-9/fulltext &quot;>;世界卫生组织认可的数据库&lt;/a>;，例如&lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4427&quot;>;损失寿命&lt;/a>; (YLL) 和&lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158&quot;>;伤残调整生命年&lt;/a >;（伤残调整生命年；生命损失年数加上残疾年数）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6r OqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s1511 /Table1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;第1511章QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s16000/Table1.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;跨种族/族裔亚群的所有皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 YLL）、模型性能（ top-3 协议），以及健康结果和工具性能的排名。&lt;br />;（*越高越好；衡量模型相对于表中的轴公平执行的可能性。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHt FmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s1518/Table2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;316&quot; data-original-width=&quot;1518&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza- QS-MJR3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZG jxSkNt9JQK/s16000/Table2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;针对不同性别的所有皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 DALY）、模型性能（前 3 名一致性）以及健康结果和工具性能的排名。 （* 如上所述。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table &lt;p>; 我们的分析估计，该模型有 80.5% 的可能性在种族/民族亚组中公平地表现，92.1% 的可能性在不同种族/族裔亚组中公平地表现。性别。 &lt;/p>; &lt;p>; 然而，虽然该模型在癌症疾病的各个年龄组中可能表现相当，但我们发现它在非癌症疾病的各个年龄组中都有改进的空间。例如，70 岁以上的人与非癌症皮肤状况相关的健康状况最差，但该模型并未优先考虑该亚组的表现。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_N fzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s1508/Table3 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1508&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnL kVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s16000/Table3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;跨年龄组的所有癌症和非癌症皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 DALY）、模型性能（顶部-3 协议），以及健康结果和工具性能的排名。 （* 同上。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;将事物放在上下文中&lt;/h2>; &lt;p>;对于整体评估，不能采用 HEAL 指标隔离中。相反，该指标应与许多其他因素结合起来，从计算效率和数据隐私到道德价值观，以及可能影响结果的方面（例如，选择偏差或不同人口群体的评估数据的代表性差异）。 &lt;/p>; &lt;p>; 作为一个对抗性的例子，可以通过故意降低最有利的子群体的模型性能，直到该子群体的性能比所有其他子群体更差来人为地改进 HEAL 指标。出于说明目的，给定亚群 A 和 B，其中 A 的健康结果比 B 差，请考虑在两个模型之间进行选择：模型 1 (M1) 对亚群 A 的表现比对亚群 B 的表现好 5%。模型 2 (M2) 的表现好 5%子群体 A 比 B 更差。M1 的 HEAL 指标会更高，因为它优先考虑结果更差的子群体的表现。然而，M1 对于亚群 A 和 B 的绝对性能可能分别仅为 75% 和 70%，而 M2 对于亚群 A 和 B 的绝对性能分别为 75% 和 80%。选择 M1 而不是 M2 会导致所有子群体的整体表现更差，因为有些子群体的状况更糟，而没有子群体的状况更好。 &lt;/p>; &lt;p>; 因此，HEAL 指标应与&lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_efficiency&quot;>;帕累托条件&lt;/a>;一起使用（本文将进一步讨论） ，这限制了模型的变化，使得每个亚群的结果与现状相比要么保持不变，要么有所改善，并且任何亚群的表现都不会恶化。 &lt;/p>; &lt;p>; HEAL 框架目前的形式评估了基于机器学习的模型相对于特定亚群预先存在的健康差异优先考虑亚群表现的可能性。这与了解机器学习是否会减少现实中不同亚人群结果差异的目标不同。具体来说，对结果的改善进行建模需要对使用任何给定模型之前和之后发生的护理旅程中的步骤进行因果理解。需要未来的研究来解决这一差距。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; HEAL 框架可以定量评估健康人工智能技术在健康差异方面优先考虑性能的可能性。该案例研究展示了如何在皮肤病学领域应用该框架，表明模型性能很可能优先考虑性别和种族/民族之间的健康差异，但也揭示了改善跨年龄非癌症疾病的潜力。该案例研究还说明了应用该框架所有建议方面的能力的局限性（例如，绘制社会背景、数据的可用性），从而凸显了基于机器学习的工具在健康公平考虑方面的复杂性。 &lt;/p>; &lt;p>; 这项工作是为应对人工智能和健康公平的巨大挑战而提出的方法，不仅可以在模型开发过程中提供有用的评估框架，而且可以在预实施和现实世界监测阶段提供有用的评估框架，例如，以健康公平仪表板的形式。我们认为 HEAL 框架的优势在于其未来在各种人工智能工具和用例中的应用以及在此过程中的完善。最后，我们承认，了解人工智能技术对健康公平影响的成功方法需要的不仅仅是一组指标。这需要代表受模型影响最大的群体的社区商定的一系列目标。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的研究是 Google 多个团队的共同工作。我们感谢所有合著者：Terry Spitz、Malcolm Pyles、Heather Cole-Lewis、Ellery Wulczyn、Stephen R. Pfohl、Donald Martin, Jr.、Ronnachai Jaroensri、Geoff Keeling、Yuan Liu、Stephanie Farquhar、Qinghan Xu、 Jenna Lester、Cían Hughes、Patricia Strachan、Fraser Tan、Peggy Bui、Craig H. Mermel、Lily H. Peng、Yossi Matias、Greg S. Corrado、Dale R. Webster、Sunny Virmani、Christopher Semturs、Yun Liu 和 Po-陈轩.我们还感谢 Lauren Winer、Sami Lachgar、Ting-An Lin、Aaron Loh、Morgan Du、Jenny Rizk、Renee Wong、Ashley Carrick、Preeti Singh、Annisah Um&#39;rani、Jessica Schrouff、Alexander Brown 和 Anna Iurchenko 对我们的支持此项目。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/977556648557231190/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;edit&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html&quot; rel=&quot;alternate&quot; title=&quot;HEAL：机器学习性能健康公平评估框架&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIx Rz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s72 -c/HEAL-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7868032799856333119&lt;/id>;&lt;已发布>;2024-03-14T12:38:00.000-07:00&lt;/已发布>; &lt;更新>;2024-03-14T12:38:11.597-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/” ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;Cappy：用小记分器超越并提升大型多任务语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：软件工程师 Yun Zhu 和 Lijuan Liu , Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjU xzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s320/Cappy%20hero.jpg&quot;样式= “显示：无；” />; &lt;p>; 大型语言模型 (LLM) 的进步催生了一种新范式，将各种自然语言处理 (NLP) 任务统一在指令跟踪框架内。这种范式以最近的多任务法学硕士为例，例如 &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;、&lt;a href=&quot;https://arxiv.org/ abs/2210.11416&quot;>;FLAN&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML&lt;/a>;。首先，收集多任务数据，每个任务都遵循特定于任务的模板，其中每个标记的示例都转换为指令（例如，&lt;em>;“&lt;/em>;将概念放在一起形成一个句子：滑雪，山，滑雪者&lt;em>;”&lt;/em>;）与相应的响应配对（例如，&lt;em>;“&lt;/em>;滑雪者滑雪下山&lt;em>;”&lt;/em>;）。这些指令-响应对用于训练LLM，产生一个以指令作为输入并生成响应的条件生成模型。此外，多任务LLM表现出了卓越的任务泛化能力，因为它们可以通过理解和解决全新指令来解决看不见的任务。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt- ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s640/Cappy% 20instruction-following.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;177&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawaA5VYDEj6VtSyOTNGZ tjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s16000/Cappy%20instruction-following.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;多任务LLM（例如FLAN）的指令跟踪预训练演示。该范式下的预训练任务提高了未见任务的性能。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 由于仅使用指令理解和解决各种任务的复杂性，多任务 LLM 的大小通常从数十亿个参数到数千亿个参数（例如，&lt;a href=&quot;https ://arxiv.org/abs/2210.11416&quot;>;FLAN-11B&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0-11B&lt;/a>; 和 &lt;a href= “https://arxiv.org/abs/2212.12017&quot;>;OPT-IML-175B&lt;/a>;）。因此，操作如此庞大的模型带来了巨大的挑战，因为它们需要相当大的计算能力，并对 GPU 和 TPU 的内存容量提出了很高的要求，使得它们的训练和推理成本高昂且效率低下。需要大量存储来为每个下游任务维护唯一的 LLM 副本。此外，最强大的多任务LLM（例如FLAN-PaLM-540B）是闭源的，这使得它们不可能被改编。然而，在实际应用中，利用单个多任务LLM以零样本的方式管理所有可以想象的任务仍然很困难，特别是在处理复杂任务、个性化任务以及无法使用指令简洁定义的任务时。另一方面，如果不结合丰富的先验知识，下游训练数据的大小通常不足以很好地训练模型。因此，长期以来人们一直希望使法学硕士能够接受下游监督，同时绕过存储、内存和访问问题。 &lt;/p>; &lt;p>; 某些&lt;em>;参数高效调整&lt;/em>;策略，包括&lt;a href=&quot;https://aclanthology.org/2021.acl-long.353.pdf&quot;>;提示调整&lt;/em>; a>; 和 &lt;a href=&quot;https://openreview.net/pdf?id=nZeVKeeFYf9&quot;>;适配器&lt;/a>; 大大减少了存储需求，但它们在调整过程中仍然通过 LLM 参数执行反向传播，从而保持高记忆力要求。此外，一些&lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/2301.00234.pdf&quot;>;上下文学习&lt;/a>;&lt;/em>;技术通过集成有限数量的监督示例来规避参数调整进入指令。然而，这些技术受到模型最大输入长度的限制，仅允许少量样本来指导任务解决。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2311.06720&quot;>;Cappy：使用小型记分器超越并提升大型多任务 LM&lt;/a>;”中，发表于&lt; a href=&quot;https://nips.cc/virtual/2023/index.html&quot;>;NeurIPS 2023&lt;/a>;，我们提出了一种新方法，可以提高多任务 LLM 的性能和效率。我们引入了一种轻量级预训练记分器 Cappy，它基于 &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; 的持续预训练，仅包含 3.6 亿个参数。 Cappy 将指令和候选响应作为输入，并产生 0 到 1 之间的分数，指示响应相对于指令的估计正确性。 Cappy 可以独立执行分类任务，也可以作为法学硕士的辅助组件，提高其性能。此外，Cappy 可以有效地实现下游监督，无需任何微调，从而避免了通过 LLM 参数进行反向传播的需要，并减少了内存需求。最后，Cappy 的适配不需要访问 LLM 参数，因为它与闭源多任务 LLM 兼容，例如那些只能通过 WebAPI 访问的参数。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIR NHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s1999/Cappy %20overview.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;975&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1k nDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s16000/Cappy%20overview.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy 将指令和响应对作为输入，并输出 0 到 1 之间的分数，表示对响应正确性的估计遵守指示。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;预训练&lt;/h2>; &lt;p>; 我们从相同的数据集集合开始，其中包括使用的来自 &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; 的 39 个不同数据集训练&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;。该集合包含广泛的任务类型，例如问答、情感分析和摘要。每个数据集都与一个或多个模板相关联，这些模板将每个实例从原始数据集转换为与其真实响应配对的指令。 &lt;/p>; &lt;p>; Cappy 的回归建模要求每个预训练数据实例包含一个指令-响应对以及响应的正确性注释，因此我们生成一个正确性注释范围从 0 到 1 的数据集。在生成任务中的实例中，我们利用现有的多任务 LLM 通过采样生成多个响应，以给定的指令为条件。随后，我们利用响应与实例的真实响应之间的相似性，为指令和每个响应形成的对分配一个注释。具体来说，我们采用 &lt;a href=&quot;https://aclanthology.org/W04-1013/&quot;>;Rouge-L&lt;/a>;，这是一种用于衡量整体多任务性能的常用指标，已证明与人类评估，将这种相似性计算为弱监督的一种形式。 &lt;/p>; &lt;p>; 结果，我们获得了包含 1.6 亿个实例的有效回归数据集，并配有正确性分数注释。最终的 Cappy 模型是在 &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; 模型之上使用回归数据集进行连续预训练的结果。 Cappy 的预训练是在 Google 的 &lt;a href=&quot;https://arxiv.org/abs/2304.01433&quot;>;TPU-v4&lt;/a>; 上进行的，使用 &lt;a href=&quot;https://arxiv.org/ pdf/2310.16355.pdf&quot;>;RedCoast&lt;/a>;，一个用于自动化分布式训练的轻量级工具包。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZy mVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s1999/Cappy%20data%20增强.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;438&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdI K_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s16000/Cappy%20data%20augmentation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用多任务LLM进行数据增强，为Cappy的预训练和微调构建弱监督回归数据集。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;应用Cappy&lt;/h2​​>; &lt;p>; Cappy解决实际任务在候选人选择机制内。更具体地说，给定指令和一组候选响应，Cappy 会为每个候选响应生成一个分数。这是通过在每个单独的响应旁边输入指令，然后指定得分最高的响应作为其预测来实现的。在分类任务中，所有候选响应本质上都是预定义的。例如，对于情感分类任务的指令（例如，“根据此评论，用户会推荐该产品吗？：‘即使对于非游戏玩家来说也令人惊叹。’”），候选回答是“是”或“不”。在这种情况下，Cappy 独立运作。另一方面，在生成任务中，候选答案不是预先定义的，需要现有的多任务 LLM 来生成候选答案。在这种情况下，Cappy 充当多任务 LLM 的辅助组件，增强其解码。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;使用 Cappy 调整多任务 LLM &lt;/h3>; &lt;p>; 当有可用的下游训练数据时，Cappy 能够有效且高效地适应下游任务上的多任务 LLM。具体来说，我们对 Cappy 进行微调，将下游任务信息集成到 LLM 预测中。此过程涉及创建特定于下游训练数据的单独回归数据集，并使用与构建预训练数据相同的数据注释过程。因此，经过微调的 Cappy 与多任务 LLM 协作，提高了 LLM 在下游任务上的性能。 &lt;/p>; &lt;p>; 与其他 LLM 调整策略相比，使用 Cappy 调整 LLM 显着降低了对设备内存的高需求，因为它避免了下游任务通过 LLM 参数进行反向传播的需要。此外，Cappy 适配不依赖于对 LLM 参数的访问，使其与闭源多任务 LLM 兼容，例如只能通过 WebAPI 访问的 LLM。与通过将训练示例附加到指令前缀来规避模型调整的上下文学习方法相比，Cappy 不受 LLM 最大输入长度的限制。因此，Cappy 可以合并无限数量的下游训练示例。 Cappy 还可以与其他适应方法一起应用，例如微调和上下文学习，进一步提高其整体性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncR dCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s1999/Cappy%20downstream%20适应.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP 9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s16000/Cappy%20downstream%20adaptation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy 和依赖于 LLM 参数的方法（例如微调和提示调整）之间的下游适应性比较。 Cappy 的应用程序增强了多任务 LLM。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估了 Cappy 在来自 &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; 的十一项语言理解分类任务中的表现。我们证明了 Cappy 具有 360M 参数，其性能优于 OPT-175B 和 OPT-IML-30B，并且与现有最好的多任务 LLM（T0-11B 和 OPT-IML-175B）的准确性相匹配。这些发现凸显了 Cappy 的能力和参数效率，这可以归功于其基于评分的预训练策略，该策略通过区分高质量和低质量的响应来整合对比信息。相反，以前的多任务法学硕士完全依赖于仅利用真实答案的教师强制培训。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGI WWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s1999/Cappy%20精度.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5d LAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s16000/Cappy%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;总体准确度是 PromptSource 十一项测试任务的平均值。 “RM”是指&lt;a href=&quot;https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2&quot;>;预训练的 RLHF 奖励模型&lt;/a>;。 Cappy 与现有多任务 LLM 中最好的匹配。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们还研究了多任务 LLM 与 Cappy 对复杂任务的适应性 &lt;a href =&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;，一组手动策划的任务，被认为超出了许多法学硕士的能力范围。我们专注于所有 45 代 BIG-Bench 任务，特别是那些不提供预先确定的答案选择的任务。我们在每个测试集上使用 Rouge-L 分数（代表模型生成与相应的基本事实之间的总体相似性）来评估性能，报告 45 次测试的平均分数。在这个实验中，FLAN-T5 的所有变体都作为 LLM 的骨干，并且基础 FLAN-T5 模型被冻结。如下所示的这些结果表明，Cappy 大幅提高了 FLAN-T5 模型的性能，始终优于通过使用 LLM 本身的自我评分进行样本选择而实现的最有效基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhX Efus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s1999/Cappy%20平均%20Rouge-L%20score.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1625&quot; data-original-width=&quot;1999 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOX cSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s16000/Cappy%20averaging%20Rouge-L%20score.png&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;BIG-Bench 中 45 项复杂任务的 Rouge-L 平均得分。 X轴代表不同尺寸的FLAN-T5型号。每条虚线代表一种适用于 FLAN-T5 的方法。自我评分是指利用LLM的交叉熵来选择答案。 Cappy大幅提升了FLAN-T5模型的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们介绍 Cappy，这是一种提高多任务 LLM 性能和效率的新颖方法。在我们的实验中，我们使用 Cappy 将单个法学硕士应用于多个领域。未来，Cappy 作为预训练模型，有可能以除单一法学硕士以外的其他创造性方式使用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢Bowen Tan、Jindong Chen、Lei Meng 、Abhanshu Sharma 和 Ewa Dominowska 的宝贵反馈。我们还要感谢 Eric Xing 和zhiting Hu 的建议。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7868032799856333119/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html&quot; rel=&quot;alternate&quot; title=&quot;Cappy：用小记分器超越并提升大型多任务语言模型&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIz TDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up -Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s72-c/Cappy%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:总计>;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6090481872694489715&lt;/id>;&lt;发布>;2024-03-12T14:15:00.000 -07:00&lt;/已发布>;&lt;更新>;2024-03-19T09:12:05.568-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”term= “生成人工智能”&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category schema=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;像图一样说话：大型语言模型的编码图&lt;/stitle>;&lt;content type=&quot;html &quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究科学家 Bahare Fatemi 和 Bryan Perozzi&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4D WP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s1600/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 想象一下您周围的所有事物 - 您的朋友、厨房里的工具，甚至自行车的零件。它们都以不同的方式连接起来。在计算机科学中，术语&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;图&lt;/a>;&lt;/em>;用于描述对象之间的连接。图由节点（对象本身）和边（两个节点之间的连接，指示它们之间的关系）组成。现在图表无处不在。互联网本身就是一个由链接在一起的网站组成的巨大图表。甚至搜索引擎使用的知识也是以类似图表的方式组织的。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 此外，请考虑人工智能的显着进步 - 例如可以在几秒钟内写出故事的聊天机器人，甚至可以解释医疗报告的软件。这一令人兴奋的进步很大程度上要归功于大型语言模型（LLM）。新的法学硕士技术正在不断开发用于不同的用途。 &lt;/p>; &lt;p>; 由于图无处不在，而且 LLM 技术正在兴起，因此在“&lt;a href=&quot;https://openreview.net/forum?id=IuXR1CCrSi&quot;>;像图一样交谈：对大型图进行编码”中语言模型&lt;/a>;”，在 &lt;a href=&quot;https://iclr.cc/&quot;>;ICLR 2024&lt;/a>; 上提出，我们提出了一种方法来教授强大的法学硕士如何更好地利用图形信息进行推理。图表是组织信息的一种有用方式，但法学硕士大多接受常规文本的培训。目的是测试不同的技术，看看哪种技术最有效并获得实用的见解。将图表翻译成法学硕士可以理解的文本是一项非常复杂的任务。困难源于具有多个节点的图结构固有的复杂性以及连接它们的复杂边缘网络。我们的工作研究如何获取图表并将其转换为法学硕士可以理解的格式。我们还设计了一个名为&lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>;的基准来研究不同的方法不同的图形推理问题，并展示如何以使得法学硕士能够解决图形问题的方式来表达图形相关问题。我们表明，LLM 在图推理任务上的表现在三个基本层面上有所不同：1）图编码方法，2）图任务本身的性质，3）有趣的是，所考虑的图的结构。这些发现为我们提供了如何最好地表示法学硕士图表的线索。选择正确的方法可以使法学硕士在图形任务方面的成绩提高高达 60%！ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WG QD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s1600/image7.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4 MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;如图所示，使用两种不同的方法将图形编码为文本，并将文本和有关图形的问题提供给法学硕士。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图形文本&lt;/h2>; &lt;p>; 能够系统地找出翻译图形的最佳方式为了文本，我们首先设计一个名为 &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>; 的基准测试。将 GraphQA 视为一项考试，旨在评估针对特定图问题的强大法学硕士。我们希望了解法学硕士能够如何很好地理解和解决涉及不同设置中的图形的问题。为了为法学硕士创建全面且真实的考试，我们不只使用一种类型的图表，而是使用多种图表的混合，以确保连接数量的广度。这主要是因为不同的图类型使解决此类问题变得更容易或更困难。通过这种方式，GraphQA 可以帮助揭露法学硕士对图表的看法的偏见，并且整个考试更接近法学硕士在现实世界中可能遇到的现实设置。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9S r3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s1368/image6.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;291&quot; data-original-width=&quot;1368&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJ xUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;我们使用 LLM 进行图形推理的框架概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; GraphQA 专注于与图形相关的简单任务，例如检查边缘是否存在，计算节点或边的数量，查找连接到特定节点的节点，并检查图中的循环。这些任务可能看起来很基本，但它们需要理解节点和边之间的关系。通过涵盖从识别模式到创建新连接等不同类型的挑战，GraphQA 可以帮助模型学习如何有效地分析图形。这些基本任务对于更复杂的图推理至关重要，例如寻找节点之间的最短路径、检测社区或识别有影响力的节点。此外，GraphQA 还包括使用各种算法生成随机图，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/ a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki /Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabasi-Albert 模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>; >;，以及更简单的图结构，如路径、完整图和星图，为训练提供了多样化的数据集。 &lt;/p>; &lt;p>; 在处理图表时，我们还需要找到方法来提出法学硕士可以理解的与图表相关的问题。 &lt;em>;提示启发式&lt;/em>;是实现此目的的不同策略。让我们分解一下常见的：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;零射击&lt;/em>;：简单地描述任务（“这张图中有循环吗？”）并告诉法学硕士去为了它。没有提供示例。 &lt;/li>;&lt;li>;&lt;em>;Few-shot&lt;/em>;：这就像在真正的交易之前给法学硕士进行一次小型练习测试。我们提供了一些示例图形问题及其正确答案。 &lt;/li>;&lt;li>;&lt;em>;思想链&lt;/em>;：在这里，我们通过示例向法学硕士展示如何逐步分解问题。目标是教它在面对新图表时生成自己的“思维过程”。 &lt;/li>;&lt;li>;&lt;em>;Zero-CoT&lt;/em>;：与 CoT 类似，但我们没有提供训练示例，而是给 LLM 一个简单的提示，例如“让我们逐步思考”，以触发其自己解决问题的崩溃。 &lt;/li>;&lt;li>;&lt;em>;BAG（构建图形）&lt;/em>;：这是专门用于图形任务的。我们在描述中添加了短语“让我们构建一个图...”，帮助法学硕士专注于图结构。 &lt;/li>; &lt;/ul>; &lt;p>; 我们探索了将图表转换为法学硕士可以使用的文本的不同方法。我们的关键问题是： &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;节点编码&lt;/em>;：我们如何表示各个节点？测试的选项包括简单的&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer&quot;>;整数&lt;/a>;、常用名称（人物、字符）和字母。 &lt;/li>;&lt;li>;&lt;em>;边缘编码&lt;/em>;：我们如何描述节点之间的关系？方法涉及括号符号、“是朋友”等短语以及箭头等符号表示。 &lt;/li>; &lt;/ul>; &lt;p>; 各种节点和边编码被系统地组合起来。这导致了如下图所示的函数： &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh -Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZchDY-oS1ts/s855/ image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-宽度=“855”高度=“302”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7 151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/w640-h302/image1.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于通过文本对图形进行编码的图形编码函数示例。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分析与结果&lt;/h2>; &lt;p>;我们进行了三项关键实验：一项是测试 LLM 如何处理图形任务，两项是了解 LLM 的大小和不同图形形状如何影响性能。我们在 GraphQA 上运行所有实验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;LLM 如何处理图形任务&lt;/h3>; &lt;p>; 在这个实验中，我们测试了 pre 的效果如何。经过训练的法学硕士可以解决图形问题，例如识别连接、循环和节点度。以下是我们了解到的内容： &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;LLM 的挣扎：&lt;/em>;在大多数基本任务上，LLM 的表现并不比随机猜测好多少。 &lt;/li>;&lt;li>;&lt;em>;编码非常重要&lt;/em>;：我们如何将图形表示为文本对 LLM 性能有很大影响。一般来说，“事件”编码对于大多数任务都表现出色。 &lt;/li>; &lt;/ul>; &lt;p>; 我们的结果总结在下面的图表中。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8 BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s1864/image8 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOAL M_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据各种图形编码器函数在不同图形任务上的准确性进行比较。该图的主要结论是图形编码函数非常重要。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;（通常）越大越好&lt;/h3>; &lt;p>; 在这个实验中，我们想看看 LLM 的大小（就参数数量而言）是否会影响他们处理图形问题的能力。为此，我们在 &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>; 的 XXS、XS、S 和 L 尺寸上测试了相同的图形任务。以下是我们的发现摘要：&lt;/p>; &lt;ul>; &lt;li>;一般来说，较大的模型在图形推理任务上表现更好。额外的参数似乎给了他们学习更复杂模式的空间。奇怪的是，对于“边存在”任务（找出图中的两个节点是否相连）来说，大小并不那么重要。 &lt;/li>;&lt;li>;即使是最大的法学硕士也无法始终击败循环检查问题（找出图表是否包含循环）的简单基线解决方案。这表明法学硕士在某些图形任务方面仍有改进的空间。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc4 2M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/s1227 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;959&quot; data-original-width=&quot;1227&quot; height=&quot; 500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORP liE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/w640-h500/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模型容量对 PaLM 2-XXS、XS、S 和 L 的图形推理任务的影响。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;不同的图形形状是否会让法学硕士感到困惑&lt;/h3 >; &lt;p>; 我们想知道图的“形状”（节点如何连接）是否会影响法学硕士解决问题的能力。将下图视为图形形状的不同示例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOafKwr7_w9dHTUD1xtnI6IMAswp0py ManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s1400/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N -7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;使用 GraphQA 的不同图形生成器生成的图形示例。 ER、BA、SBM 和 SFN 指的是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;巴拉巴西-阿尔伯特&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;分别.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们发现图结构对LLM性能有很大影响。例如，在询问循环是否存在的任务中，法学硕士在紧密互连的图（循环在那里很常见）上表现出色，但在路径图（循环永远不会发生）上表现不佳。有趣的是，提供一些混合的例子有助于它适应。例如，对于循环检查，我们在提示中添加了一些包含循环的示例和一些不包含循环的示例作为少样本示例。其他任务也出现类似的模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMA VFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s1864/image5.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKETHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7LAym 4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;比较不同图形任务上的不同图形生成器。这里的主要观察是图结构对法学硕士的表现有重大影响。 ER、BA、SBM 和 SFN 指的是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;巴拉巴西-阿尔伯特&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;分别.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;简而言之，我们深入研究了如何最好地将图表表示为文本，以便法学硕士能够理解它们。我们发现三个主要因素会产生影响：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;如何将图表转换为文本&lt;/em>;：我们如何将图表表示为文本会显着影响法学硕士的表现。一般来说，事件编码对于大多数任务都表现出色。&lt;/li>;&lt;li>;&lt;em>;任务类型&lt;/em>;：某些类型的图形问题对于法学硕士来说往往更难，即使从图形到图形的良好翻译也是如此。文本。 &lt;/li>;&lt;li>;&lt;em>;图结构&lt;/em>;：令人惊讶的是，我们进行推理的图的“形状”（连接密集、稀疏等）会影响法学硕士的表现。 &lt;/li>; &lt;/ul>; &lt;p>; 这项研究揭示了如何为法学硕士准备图表的关键见解。正确的编码技术可以显着提高法学硕士在图问题上的准确性（提高约 5% 到 60% 以上）。我们的新基准 GraphQA 将有助于推动该领域的进一步研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们谨向我们的合作伙伴表示感谢——作者 Jonathan Halcrow，感谢他对本书做出的宝贵贡献。我们衷心感谢 Anton Tsitsulin、Dustin Zelle、Silvio Lattanzi、Vahab Mirrokni 以及 Google Research 的整个图挖掘团队，他们富有洞察力的评论、彻底的校对和建设性的反馈极大地提高了我们的工作质量。我们还要特别感谢 Tom Small 创建了本文中使用的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6090481872694489715 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like- graph-encoding-graphs-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds /8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html&quot; rel =&quot;alternate&quot; title=&quot;像图一样说话：为大型语言模型编码图&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger .com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6 tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s72-c/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot;宽度=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id >;标签：blogger.com，1999：blog-8474926331452026626.post-470840348983280912&lt;/id>;&lt;已发布>;2024-03-11T12:08:00.000-07:00&lt;/已发布>;&lt;更新>;2024-03-11T12:13 ：03.824-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Chain-of-table：在推理链中不断演化表格以实现表格理解&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：云 AI 团队学生研究员王子龙和研究科学家李振宇&lt;/span>; &lt;img src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUTIhHxVvsBjbPxvZLcNc D_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s832/Chain-of-Table.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人们每天都使用表格以结构化、易于访问的格式组织和解释复杂的信息。由于此类表格无处不在，表格数据的推理长期以来一直是自然语言处理 (NLP) 的中心主题。该领域的研究人员致力于利用语言模型来帮助用户回答问题、验证陈述以及基于表格分析数据。然而，语言模型是在大量纯文本上进行训练的，因此表格数据固有的结构化性质可能很难让语言模型完全理解和利用。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 最近，&lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;>;大型语言模型&lt;/a>;（法学硕士）通过生成可靠的推理链，在各种自然语言理解（NLU）任务中取得了出色的表现，如作品所示就像&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;思想链&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2205.10625&quot;>;最少到-大多数&lt;/a>;。然而，法学硕士对表格数据进行推理的最合适方式仍然是一个悬而未决的问题。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2401.04398&quot;>;Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding&lt;/a>;”中，我们提出一个解决表格理解任务的框架，我们训练法学硕士逐步概述他们的推理，迭代更新给定的表格以反映思维过程的每个部分，类似于人们如何解决基于表格的问题。这使得LLM能够将表格转化为更简单、更易于管理的段，以便能够深入理解和分析表格的每个部分。这种方法取得了重大改进，并在 &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/1909.02164&quot;>;TabFact&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>; 基准。下图显示了所提出的表链和其他方法的高级概述。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo 6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s1478/image2.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1478&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7 qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;给定一个复杂的表格，其中骑车人的国籍和姓名位于同一单元格中，（a）通用的多步骤推理无法提供正确的答案（b）程序辅助推理生成并执行程序（例如、SQL 查询）来提供答案，但无法准确解决问题。相比之下，(c) Chain-of-Table 对一系列操作进行迭代采样，有效地将复杂的表转换为专门针对问题定制的版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chain-of-Table&lt;/h2>; &lt;p>; 在 Chain-of-Table 中，我们指导法学硕士使用&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;情境学习&lt;/a>; 迭代生成操作并更新表以表示其基于表格数据的推理链。这使得法学硕士能够根据之前操作的结果动态规划下一个操作。该表的这种不断演变形成了一条链，它为给定问题的推理过程提供了更加结构化和清晰的表示，并使法学硕士能够做出更准确和可靠的预测。 &lt;/p>; &lt;p>; 例如，当被问到“哪位演员获得最多的全国有色人种协进会形象奖？”表链框架促使法学硕士生成反映表格推理过程的表格运算。它首先标识相关列。然后，它根据共享内容聚合行。最后，它对汇总结果进行重新排序，以生成明确回答所提出问题的最终表格。 &lt;/p>; &lt;p>; 这些操作会转换表格以与提出的问题保持一致。为了平衡大表上的性能和计算费用，我们根据表格行的子集构建操作链。同时，逐步操作通过显示表格操作的中间结果来揭示底层推理过程，促进增强可解释性和理解性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08 eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s1999/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;983&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-w YwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Chain-of-Table 中表格推理过程的图示。这个迭代过程涉及动态规划操作链并将中间结果准确地存储在转换后的表中。这些中间表格作为表格思维过程，可以指导法学硕士更可靠地找到正确答案。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Chain-of-表由三个主要阶段组成。在第一阶段，它指示法学硕士通过上下文学习动态规划下一步操作。具体来说，提示涉及三个组成部分，如下图所示：&lt;/p>; &lt;ol>; &lt;li>;问题&lt;em>;Q&lt;/em>;：“哪个国家的自行车运动员进入前三名的人数最多？” &lt;/li>;&lt;li>;操作历史&lt;em>;链&lt;/em>;：&lt;code>;f_add_col(Country)&lt;/code>;和&lt;code>;f_select_row(1, 2, 3)&lt;/code>;。 &lt;/li>;&lt;li>;最新中间表&lt;em>;T&lt;/em>;：转换后的中间表。 &lt;/li>; &lt;/ol>; &lt;p>; 通过在提示中提供三元组&lt;em>;(T, Q, chain)&lt;/em>;，LLM可以观察之前的表格推理过程，并从操作中选择下一个操作pool逐步完成推理链。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdi yuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s1958/image1.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1233&quot; data-original-width=&quot;1958&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjiIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZ kT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Chain-of-Table 如何从操作池中选择下一个操作并生成该操作的参数。(a) Chain-of-Table 对操作池中的下一个操作进行采样。 (b) 它将选定的操作作为输入并生成其参数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 在下一个操作之后&lt;em>;f&lt;/em>;确定后，在第二阶段，我们需要生成参数。如上所述，Chain-of-Table 考虑提示中的三个组成部分，如图所示：(1) 问题，(2) 所选操作及其所需参数，以及 (3) 最新的中间表。 &lt;/p>; &lt;p>; 例如，当选择操作&lt;code>;f_group_by&lt;/code>;时，它需要一个标头名称作为其参数。 &lt;/p>; &lt;p>; 法学硕士在表格中选择合适的标题。配备了选定的操作和生成的参数，Chain-of-Table 执行操作并构造一个新的中间表以进行以下推理。 &lt;/p>; &lt;p>; Chain-of-Table 迭代前两个阶段来计划下一个操作并生成所需的参数。在此过程中，我们创建一个操作链作为表格推理步骤的代理。这些操作生成中间表，向法学硕士呈现每个步骤的结果。因此，输出表包含有关表格推理的中间阶段的全面信息。在最后阶段，我们使用此输出表来制定最终查询，并提示法学硕士以及问题以获得最终答案。 &lt;/p>; &lt;br />; &lt;h2>;实验设置&lt;/h2>; &lt;p>;我们使用&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2-S&lt;/a>;&amp;nbsp ；以及&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>; 作为 LLM 的骨干和在三个公共表理解基准上进行实验：&lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/1909.02164 &quot;>;TabFact&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>;。 WikiTQ 和 FeTaQA 是基于表格的问答的数据集。 TabFact 是一个基于表的事实验证基准。在这篇博文中，我们将重点关注 WikiTQ 和 TabFact 上的结果。我们将 Chain-of-Table 与通用推理方法（例如，End-to-End QA、Few-Shot QA 和 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-思想&lt;/a>;）和程序辅助方法（例如，&lt;a href=&quot;https://arxiv.org/abs/2204.00498&quot;>;文本到SQL&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/2210.02875&quot;>;活页夹&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;日期&lt;/a>;）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;答案更准确&lt;/h3>; &lt;p>;与通用推理方法和程序辅助推理相比方法中，Chain-of-Table 在 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 和&lt;a href=&quot;https://openai 上实现了更好的性能.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;。这归因于动态采样操作和信息丰富的中间表。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3mu AAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s1018/ChainOfTableUnderstanding.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;755&quot; data-original-width=&quot;1018&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn 9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s16000/ChainOfTableUnderstanding.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;了解 WikiTQ 和 TabFact 上使用 PaLM 2 和 GPT 3.5 与各种模型的比较结果。&lt;/span>;&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;在较难的问题上具有更好的稳健性&lt;/h3>; &lt;p>; 在 Chain-of 中-表格，操作链越长，说明题及其对应表格的难度和复杂度越高。我们根据 Chain-of-Table 中的操作长度对测试样本进行分类。我们将 Chain-of-Table 与 Chain-of-Thought 和 Dater 进行比较，作为代表性的通用和程序辅助推理方法。我们使用 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 在 &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>; 上的结果来说明这一点维基百科&lt;/a>;。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrS ZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIAEsHJohvlrSna7/s1548/CoTOpChainLength.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;624&quot; data-original-width=&quot;1548&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744 FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIAEsHJohvlrSna7/s16000/CoTOpChainLength.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于需要不同长度操作链的问题，思想链、日期器和 WikiTQ 上建议的表链的性能。我们提出的原子操作显着提高了通用和程序辅助推理对应物的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 值得注意的是，Chain-of-Table 始终超越了两个基线跨所有操作链长度的方法，与 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; 相比，显着提升高达 11.6%，高达 7.9%与&lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;日期&lt;/a>;相比。此外，与其他基线方法相比，表链的性能随着操作数量的增加而缓慢下降，当操作数量从 4 增加到 5 时，仅表现出最小的下降。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;更大的表格具有更好的鲁棒性&lt;/h3>; &lt;p>;我们对表格进行分类&lt;a href= &quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>; 根据代币数量分为三组：小型（&lt;2000 个代币）、中型（2000 到 4000 个代币）和大型（>;4000 个代币） 。然后我们将 Chain-of-Table 与 &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Datar&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot; 进行比较>;Binder&lt;/a>;，两个最新且最强的基线。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsai xakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1008&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4 VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Binder、Dater 和建议的 Chain-of-Table 在小型（&lt;2000 个代币）、中型上的性能来自 WikiTQ 的（2000 到 4000 个令牌）和大型（>; 4000 个令牌）表。我们观察到，性能随着输入表的增大而降低，而表链则优雅地减小，与竞争方法相比取得了显着的改进。 （如上，下划线文字表示性能第二好；粗体表示性能最好。）&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Binder性能、 Dater 以及来自 WikiTQ 的小型（&lt;2000 个令牌）、中型（2000 到 4000 个令牌）和大型（>;4000 个令牌）表上提议的 Chain-of-Table。我们观察到，性能随着输入表的增大而降低，而表链则优雅地减小，与竞争方法相比取得了显着的改进。 （如上所述，带下划线的文本表示第二好的性能；粗体表示最佳的性能。） &lt;/p>; &lt;p>; 正如预期的那样，性能随着输入表的增大而下降，因为模型需要通过较长的上下文进行推理。尽管如此，所提出的 Chain-of-Table 的性能会优雅地下降，在处理大型表时，比第二最佳竞争方法实现了 10+% 的显着改进。这证明了推理链在处理长表格输入方面的有效性。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出的表链方法通过利用表格结构来表达基于表格的推理的中间步骤，从而增强了 LLM 的推理能力。它指示法学硕士根据输入表及其相关问题动态规划操作链。这种不断发展的表格设计为促进法学硕士对表格的理解提供了新的思路。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由王子龙、张浩、李春亮、朱利安·马丁·艾森施洛斯、文森特·佩罗、王子峰、莱斯利·米库里奇进行, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister.感谢 Chih-Kuan Yeh 和 Sergey Ioffe 提供的宝贵反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/470840348983280912/comments/default&quot; rel =&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables- in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /470840348983280912&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/470840348983280912&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html&quot; rel=&quot;alternate&quot; title= “Chain-of-table：在推理链中不断演化表格以实现表格理解” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUTIhHxVvsBjbPxvZLcNc D_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s72-c/Chain-of-Table.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376&lt;/id>;&lt;已发布>;2024-03-08T11:33:00.000-08:00&lt;/已发布>;&lt;更新>;2024-03-13T09:18:01.747-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;健康&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“图像分类”>;&lt;/类别>;&lt;title type =“text”>;皮肤病学和病理学的健康特定嵌入工具&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Health 临床研究科学家 Dave Steiner 和 Google Research 产品经理 Rory Pilgrim&lt;/span>; &lt;img src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiA eG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s16000/Path%20+%20Derm%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 全球范围内缺乏跨专业的医学影像专家解读，包括&lt;a href=&quot;https://www.rsna.org/news/2022/may/Global-Radiographer-Shortage&quot;>;放射学&lt;/ a>;、&lt;a href=&quot;https://www.aad.org/dw/monthly/2021/december/feature-running-dry&quot;>;皮肤科&lt;/a>;和&lt;a href=&quot;https://proscia. com/infographic-the-state-of-the-pathology-workforce-2022/&quot;>;病理学&lt;/a>;。机器学习 (ML) 技术可以通过支持工​​具帮助减轻这一负担，使医生能够更准确、更高效地解读这些图像。然而，此类机器学习工具的开发和实施通常受到高质量数据、机器学习专业知识和计算资源的可用性的限制。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 促进机器学习在医学成像中的应用的一种方法是通过特定领域的模型，利用深度学习 (DL) 来捕获医学图像中的信息作为压缩的数值向量（称为嵌入）。这些嵌入代表了对图像中重要特征的一种预先学习的理解。与&lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_Dimensionity&quot;>;处理高维数据&lt;相比，识别嵌入中的模式可以减少训练高性能模型所需的数据量、专业知识和计算量/a>;，如图像，直接。事实上，这些嵌入可用于执行专业领域内的各种下游任务（请参见下面的动画图）。这种利用预先学习的理解来解决相关任务的框架类似于经验丰富的吉他手通过耳朵快速学习新歌曲的框架。因为吉他手已经建立了技能和理解的基础，所以他们可以快速掌握新歌曲的模式和节奏。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xB x3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPSH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s1600/路径%20+% 20Derm%20train%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THph EkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s16000/Path%20+%20Derm%20train%20LP.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation 用于将（图像，标签）对的小数据集转换为（嵌入，标签）对。然后，这些对可用于使用线性探针（即轻量级线性分类器）来训练特定于任务的分类器（如图所示），或使用嵌入作为输入的其他类型的模型。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-右：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag- ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUpplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s1600/Path%20+%20Derm%20-%20评估%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “500”数据原始宽度=“1600”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0e Cl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqbHdlxQyyz/s16000/Path%20+%20Derm%20-%20评估%20LP.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;一旦线性探针被训练，它可用于对新图像的嵌入进行预测。这些预测可以与地面真实信息进行比较，以评估线性探针的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;为了使这种类型的嵌入模型可用并进一步推动随着医学成像领域 ML 工具的开发，我们很高兴发布两个用于研究用途的特定领域工具：&lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation &quot;>;Derm Foundation&lt;/a>; 和 &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/path-foundation&quot;>;Path Foundation&lt;/a>;。在此之前，我们已经使用 &lt;a href=&quot;https://blog.research.google/2022/07/simplified-transfer-learning-for-chest.html&quot;>;CXR 基金会&lt; /a>; 用于胸部 X 光片的嵌入工具，代表了我们跨多种医疗专业模式不断扩展的研究产品的一部分。这些嵌入工具将图像作为输入并生成分别专门用于皮肤病学和数字病理学图像领域的数值向量（嵌入）。通过使用相应的嵌入工具运行胸部 X 射线、皮肤病学或病理学图像的数据集，研究人员可以获得自己图像的嵌入，并使用这些嵌入为其应用快速开发新模型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;路径基础&lt;/h2>; &lt;p>; 在“&lt;a href=&quot;https://arxiv”中.org/abs/2310.13259&quot;>;组织病理学自监督模型的特定领域优化和多样化评估&lt;/a>;”，我们表明病理图像的自监督学习（SSL）模型优于传统的预训练方法，并且能够下游任务分类器的有效训练。这项工作的重点是&lt;a href=&quot;https://en.wikipedia.org/wiki/H%26E_stain&quot;>;苏木精和伊红&lt;/a>; (H&amp;E) 染色载玻片，这是诊断病理学中的主要组织染色剂，能够病理学家在显微镜下观察细胞特征。使用 SSL 模型的输出训练的线性分类器的性能与之前在更多数量级的标记数据上训练的深度学习模型的性能相匹配。 &lt;/p>; &lt;p>; 由于数字病理图像和“自然图像”照片之间存在显着差异，这项工作涉及模型训练过程中的多项病理特定优化。一个关键要素是病理学中的&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/&quot;>;全幻灯片图像&lt;/a>; (WSI) 的跨度可以是 100,000 像素（比典型智能手机照片大数千倍）并由专家在多个放大倍数（缩放级别）下进行分析。因此，WSI 通常会被分解为更小的块或补丁，以供计算机视觉和深度学习应用程序使用。生成的图像信息密集，细胞或组织结构分布在整个帧中，而不是具有明显的语义对象或前景与背景变化，从而为稳健的 SSL 和特征提取带来了独特的挑战。此外，物理（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Microtome&quot;>;切割&lt;/a>;）和化学（例如，&lt;a href=&quot;https://en.wikipedia. org/wiki/Fixation_(histology)&quot;>;固定&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Staining&quot;>;染色&lt;/a>;）用于制备样品的过程会影响图像外观显着。 &lt;/p>; &lt;p>; 考虑到这些重要方面，特定于病理学的 SSL 优化包括帮助模型学习&lt;a href=&quot;https://arxiv.org/abs/2206.12694&quot;>;污点不可知特征&lt;/a>; ，将模型推广到多个放大倍数的补丁，&lt;a href=&quot;https://blog.research.google/2020/02/generating-diverse-synthetic-medical.html&quot;>;增强&lt;/a>;数据以模拟扫描和图像后处理以及自定义数据平衡，以改善 SSL 训练的输入异构性。使用涉及 17 种不同组织类型和 12 种不同任务的广泛基准任务对这些方法进行了广泛评估。 &lt;/p>; &lt;p>; 利用视觉转换器 (&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;>;ViT-S/16&lt;/a>;) 架构，Path Foundation 被选为上述优化和评估过程中表现最佳的模型（如下图所示）。因此，该模型在性能和模型大小之间提供了重要的平衡，以便在大型病理 WSI 的许多单独图像块上生成嵌入时能够进行有价值且可扩展的使用。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJ b79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s1999/Path%20+%20Derm% 20SSL.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1097&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxW gYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s16000/Path%20+%20Derm%20SSL.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;针对 Path Foundation 进行特定于病理学优化的 SSL 训练。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;特定领域图像表示的值也可以在下图中看到，它显示了 Path Foundation 的线性探测性能改进（通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;>; 测量） AUROC&lt;/a>;）与传统的自然图像预训练（&lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;ImageNet-21k&lt;/a>;）相比。这包括对诸如&lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2665774&quot;>;淋巴结转移性乳腺癌检测&lt;/a>;、&lt;a href=&quot;https:// jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;前列腺癌分级&lt;/a>;和&lt;a href=&quot;https://www.nature.com/articles/s41523-022-00478-y&quot;>;乳腺癌评分&lt;/a>;等。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9 D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s1999/路径%20+%20Derm%20embeddings.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;890&quot; data-original-width=&quot; 1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9 ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s16000/Path%20+%20Derm%20embeddings.png&quot; />;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据组织病理学中多个评估任务的线性探测评估，Path Foundation 嵌入显着优于传统 ImageNet 嵌入.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;真皮粉底&lt;/ h2>; &lt;p>; &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; 是一种嵌入工具，源自我们的研究应用深度学习&lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;解读皮肤病图像&lt;/a>;，其中包括我们最近的工作：添加&lt;a href=&quot;https://arxiv.org/abs/2402.15566&quot;>;改进以更好地泛化到新数据集&lt;/a>;。由于其针对皮肤病学的预训练，它对皮肤状况图像中存在的特征有潜在的了解，可用于快速开发模型来对皮肤状况进行分类。 API 底层的模型是经过两个阶段训练的 &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>;BiT ResNet-101x3&lt;/a>;。第一个预训练阶段使用对比学习，类似于 &lt;a href=&quot;https://arxiv.org/abs/2010.00747&quot;>;ConVIRT&lt;/a>;，在大量图像-文本对上进行训练&lt;a href =&quot;https://blog.research.google/2017/07/revisiting-unreasonable-effectness.html&quot;>;来自互联网&lt;/a>;。在第二阶段，然后使用临床数据集（例如来自远程皮肤病学服务的数据集）对该预训练模型的图像组件进行微调，以进行病情分类。 &lt;/p>; &lt;p>; 与组织病理学图像不同，皮肤病学图像更类似于用于训练当今许多计算机视觉模型的现实世界图像。然而，对于专门的皮肤病学任务，创建高质量模型可能仍然需要大型数据集。借助 Derm Foundation，研究人员可以使用自己的较小数据集来检索特定领域的嵌入，并使用这些数据来构建较小的模型（例如线性分类器或其他小型非线性模型），使他们能够验证他们的研究或产品想法。为了评估这种方法，我们使用远程皮肤病学数据在下游任务上训练模型。模型训练涉及不同的数据集大小（12.5%、25%、50%、100%），以将基于嵌入的线性分类器与微调进行比较。 &lt;/p>; &lt;p>; 考虑的建模变体是： &lt;/p>; &lt;ul>; &lt;li>;来自 &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>; 的冻结嵌入的线性分类器BiT-M&lt;/a>;（标准预训练图像模型）&lt;/li>;&lt;li>;BiT-M 的微调版本，具有用于下游任务的额外密集层&lt;/li>;&lt;li>;线性分类器来自 Derm Foundation API 的冻结嵌入 &lt;/li>;&lt;li>;Derm Foundation API 底层模型的微调版本，带有用于下游任务的额外层 &lt;/li>; &lt;/ul>; &lt;p>; 我们发现模型建立在 Derm Foundation 嵌入之上的用于皮肤病学相关任务的质量比仅基于嵌入构建或通过 BiT-M 微调的质量要高得多。人们发现，对于较小的训练数据集大小，这一优势最为明显。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7e IqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s1240/Path%20+%20Derm%20task% 20accuracy.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;842&quot; data-original-width=&quot;1240&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn 7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s16000/Path%20+%20Derm%20task%20accuracy.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;这些结果表明 Derm Foundation 也可以作为加速皮肤相关建模任务的有用起点。我们的目标是让其他研究人员能够以模型学到的皮肤病学的基本特征和表征为基础。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;但是，这种分析存在局限性。我们仍在探索这些嵌入在任务类型、患者群体和图像设置中的推广效果如何。使用 Derm Foundation 构建的下游模型仍然需要仔细评估，以了解其在预期环境中的预期性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Access Path 和 Derm Foundation&lt;/h2>; &lt;p>; 我们设想 Derm Foundation 和 Path Foundation嵌入工具将支持一系列用例，包括诊断任务模型的高效开发、质量保证和分析前工作流程改进、图像索引和管理以及生物标志物发现和验证。我们正在向研究界发布这两种工具，以便他们能够探索嵌入对于自己的皮肤病学和病理学数据的实用性。 &lt;/p>; &lt;p>; 要获得访问权限，请使用以下 Google 表单签署每个工具的服务条款。 &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSe5icNBzU_lO2CwjLLIOwbqIcWnJC-m4Sl7MgvI9Lng3QT6Zg/viewform?resourcekey=0-dahJtiVe2CqYkNEdWPcXgw&quot;>;Derm 基金会访问表&lt;/ a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/1auyo2VkzlzuiAXavZy1AWUyQHAqO7T3BLK-7ofKUvug/edit?resourcekey=0-Z9pRxjDI-kaDEUIiNfMAWQ#question=1168037695&amp;field=173852432&quot;>;路径基础访问表单&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>; 获得每个工具的访问权限后，您可以使用 API 从存储在 Google Cloud 中的皮肤病学图像或数字病理学图像中检索嵌入。只是想看看模型和嵌入的实际效果的批准用户可以使用提供的 Colab 笔记本示例，使用公共数据来训练模型以进行分类 &lt;a href=&quot;https://github.com/Google-Health/imaging-research/ blob/master/derm-foundation/derm_foundation_demo.ipynb&quot;>;六种常见皮肤病&lt;/a>;或在 &lt;a href=&quot;https://github.com/Google-Health/imaging-research/blob/master/ 中识别肿瘤path-foundation/linear-classifier-demo.ipynb&quot;>;组织病理学补丁&lt;/a>;。我们期待看到这些工具可以解锁的用例范围。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢许多提供帮助的合作者使这项工作成为可能的成员包括 Yun Liu、Can Kirmizi、Fereshteh Mahvar、Bram Sterling、Arman Tajback、Kenneth Philbrik、Arnav Agharwal、Aurora Cheung、Andrew Sellergren、Boris Babenko、Basil Mustafa、Jan Freyberg、Terry Spitz、Yuan Liu、Pinal Bavishi，阿尤什·杰恩、阿米特·塔尔雷贾、拉吉夫·里克耶、艾比·沃德、杰里米·赖、法鲁克·艾哈迈德、苏普里亚·维贾伊、蒂亚姆·贾罗恩斯里、杰西卡·卢、Saurabh Vyawahare、萨洛尼·阿加瓦尔、埃勒里·乌尔钦、乔纳森·克劳斯、法亚兹·贾米尔、汤姆·斯莫尔、安妮莎·乌姆拉尼、 Lauren Winer、Sami Lachgar、Yossi Matias、Greg Corrado 和 Dale Webster。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1106624361649572376/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools- for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /1106624361649572376&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1106624361649572376&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools-for.html&quot; rel=&quot;alternate&quot; title=&quot;健康-皮肤病学和病理学专用嵌入工具&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9J x1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s72-c/Path%20+%20Derm%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/ mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1765359719068432739&lt;/id >;&lt;发布>;2024-03-07T10:15:00.000-08:00&lt;/发布>;&lt;更新>;2024-03-07T10:19:31.177-08:00&lt;/更新>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器智能&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;社交学习：协作学习大语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究实习生 Amirkeivan Mohtashami 和软件工程师 Florian Hartmann&lt;/span>; &lt;img src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8 ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 大型语言模型 (LLM) 显着提高了解决使用自然语言指定的任务的技术水平，通常达到接近人类的性能。随着这些模型越来越多地支持辅助代理，他们有效地相互学习可能是有益的，就像人们在社交环境中所做的那样，这将使基于 LLM 的代理能够提高彼此的表现。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 讨论人类、班杜拉和沃尔特斯的学习过程&lt;a href=&quot;https://books.google.ch/books/about/Social_Learning_Theory .html?id=IXvuAAAAMAAJ&amp;amp;redir_esc=y&quot;>;在 1977 年描述了&lt;/a>;&lt;em>;社会学习&lt;/em>;的概念，概述了人们使用的观察学习的不同模型。向他人学习的一种常见方法是通过描述如何参与特定行为的&lt;em>;口头指导&lt;/em>;（例如，来自老师）。或者，学习可以通过模仿行为的活生生例子的&lt;em>;实时模型&lt;/em>;来进行。 &lt;/p>; &lt;p>; 鉴于法学硕士模仿人类交流的成功，在我们的论文“&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;社交学习：利用大型语言模型进行协作学习&lt;/ a>;”，我们调查法学硕士是否能够利用社交学习相互学习。为此，我们概述了一个社会学习框架，其中法学硕士使用自然语言以隐私意识方式相互分享知识。我们评估了我们的框架在各种数据集上的有效性，并提出了在这种情况下衡量隐私的定量方法。与之前的协作学习方法相比，例如常见的&lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;联合学习&lt;/a>;方法通常依赖于关于梯度，在我们的框架中，智能体纯粹使用自然语言互相教学。 &lt;/p>; &lt;br />; &lt;h2>;法学硕士的社交学习&lt;/h2>; &lt;p>;为了将社交学习扩展到语言模型，我们考虑这样的场景：法学硕士学生应该学习解决来自多个教师实体的任务，这些教师实体已经知道那个任务。在我们的论文中，我们评估了学生在各种任务上的表现，例如短短信中的&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2034691.2034742&quot;>;垃圾邮件检测&lt;/a>;（ SMS），解决&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;小学数学问题&lt;/a>;，以及&lt;a href=&quot;https://arxiv.org/abs/1905.10044&quot;>;根据给定的文本回答问题&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3 Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s900/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;381&quot; data-original-width=&quot;900&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5tDB BpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;社会学习过程的可视化：教师模型向学生模型提供说明或少量示例，而无需共享其私有数据。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 语言模型已经显示出在仅给出少数示例的情况下执行任务的非凡能力 - 这个过程称为 &lt;a href=&quot;https://arxiv.org/abs/2005.14165 ”少样本学习&lt;/a>;。考虑到这一点，我们提供了人工标记的任务示例，使教师模型能够将其教授给学生。当由于隐私问题等原因而无法直接与学生共享这些示例时，就会出现社交学习的主要用例之一。 &lt;/p>; &lt;p>; 为了说明这一点，让我们看一个垃圾邮件检测任务的假设示例。教师模型位于设备上，一些用户自愿将他们收到的传入消息标记为“垃圾邮件”或“非垃圾邮件”。这是有用的数据，可以帮助训练学生模型区分垃圾邮件和非垃圾邮件，但与其他用户共享个人消息是侵犯隐私的行为，应该避免。为了防止这种情况，社交学习过程可以将知识从教师模型转移到学生，这样学生就可以了解垃圾邮件的样子，而无需共享用户的个人短信。 &lt;/p>; &lt;p>; 我们通过与我们上面讨论的已建立的人类社会学习理论进行类比来研究这种社会学习方法的有效性。在这些实验中，我们使用 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-S&lt;/a>; 模型老师和学生。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3ry gqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1117&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa 5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;社会学习的系统观点：在培训时，多名教师教授学生。在推理时，学生正在使用从老师那里学到的知识。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;综合示例&lt;/h3>; &lt;p>;作为对应物针对传统社会学习描述的实时教学模型，我们提出了一种学习方法，教师为任务生成新的综合示例并与学生分享。这样做的动机是，人们可以创建一个与原始示例完全不同但同样具有教育意义的新示例。事实上，我们观察到我们生成的示例与真实示例有很大不同，可以保护隐私，同时仍然可以实现与使用原始示例实现的性能相当的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeK pICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s1456/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFh sCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;对于多项任务，生成的 8 个示例的性能与原始数据一样好（请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们通过任务套件上的综合示例来评估学习的效果。特别是当示例数量足够多时，例如，n = 16，我们观察到对于大多数任务来说，共享原始数据和通过社交学习使用合成数据进行教学之间没有统计上的显着差异，这表明隐私改进不一定会出现以模型质量为代价。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHR8sU3WkHsPKVlsQmVnzW -YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s1456/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHR8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE 2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;生成 16 个示例（而不是仅 8 个示例）进一步缩小了相对于原始示例的性能差距。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;p>; 一个例外是垃圾邮件检测，使用合成数据进行教学的准确性较低。这可能是因为当前模型的训练过程使它们偏向于仅生成非垃圾邮件示例。在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;中，我们还研究了用于选择要使用的良好示例子集的聚合方法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;综合指令&lt;/h3>; &lt;p>;鉴于语言模型在以下指令中的成功，言语通过让教师为任务生成指令，教学模型也可以自然地适应语言模型。我们的实验表明，提供这样的生成指令可以有效地提高零样本提示的性能，达到与原始示例的少样本提示相当的准确性。然而，我们确实发现教师模型可能无法在某些任务上提供良好的指导，例如由于输出的复杂格式要求。 &lt;/p>; &lt;p>;对于&lt;a href=&quot;https://arxiv.org/abs/1606.06031&quot;>;兰巴达&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>; GSM8k&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;随机插入&lt;/a>;，提供合成示例比提供生成的指令表现更好，而在其他任务中生成的指令获得更高的准确度。这一观察结果表明，教学模式的选择取决于手头的任务，就像最有效的教学方法因任务而异一样。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuL HHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s1451/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1451&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8 nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据任务的不同，生成指令比生成新示例效果更好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;记忆私人例子&lt;/h2>; &lt;p>;我们希望社会学习中的教师在不透露原始数据细节的情况下教授学生。为了量化这个过程泄露信息的可能性，我们使用了 &lt;a href=&quot;https://research.google/pubs/the-secret-sharer-evaluating-and-testing-unintending-memorization-in-neural-networks/ &quot;>;Secret Sharer&lt;/a>;，一种流行的方法，用于量化模型记忆其训练数据的程度，并将其适应社交学习环境。我们选择此方法是因为它之前&lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for.html&quot;>;用于&lt;/a>;评估联邦学习中的记忆情况。 &lt;/p>; &lt;p>; 为了将秘密分享者方法应用于社交学习，我们设计了“金丝雀”数据点，以便我们可以具体测量训练过程记住了多少它们。这些数据点包含在教师用来生成新示例的数据集中。社会学习过程完成后，我们可以衡量学生对老师使用的秘密数据点的信心，与那些甚至没有与老师分享的类似数据点相比。 &lt;/p>; &lt;p>; 在我们的分析中（在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;中详细讨论），我们使用包含名称和代码的金丝雀示例。我们的结果表明，学生对老师使用的金丝雀的信心仅稍高一些。相反，当直接与学生共享原始数据点时，对包含的金丝雀的置信度远高于对保留集的置信度。这支持了这样的结论：教师确实使用其数据进行教学，而不是简单地复制它。 &lt;/p>; &lt;br />; &lt;h2>;结论和后续步骤&lt;/h2>; &lt;p>;我们引入了一个社会学习框架，该框架允许能够访问私有数据的语言模型通过文本通信传输知识，同时维护该数据的隐私。数据。在此框架中，我们将共享示例和共享指令确定为基本模型，并在多个任务上对其进行评估。此外，我们将秘密共享者指标调整到我们的框架中，提出了衡量数据泄漏的指标。 &lt;/p>; &lt;p>; 接下来，我们正在寻找改进教学过程的方法，例如通过添加反馈循环和迭代。此外，我们希望研究将社交学习用于文本以外的方式。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们谨向共同作者 Matt Sharifi、Sian Gooding、Lukas Zilka 和 Blaise Aguera y Arcas 表示感谢。在纸上。此外，我们还要感谢 Victor Cărbune、Zachary Garrett、Tautvydas Misiunas、Sofia Neata 和 John Platt 的反馈，这些反馈极大地改进了本文。我们还要感谢 Tom Small 创作了这个动画人物。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1765359719068432739/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html #comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;self&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html&quot; rel=&quot;alternate&quot; title=&quot;社交学习：协作学习大语言模型” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@ blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/ b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEicN2GYOP9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyyp TVVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8393293208018757284&lt;/id>;&lt;发布>;2024-03-06T10：26：00.000-08： 00&lt;/published>;&lt;updated>;2024-03-06T14:44:03.387-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot; >;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/” atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Croissant：用于 ML 就绪数据集的元数据格式&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;发布者：Google 研究部软件工程师 Omar Benjelloun 和 Google Core ML 软件工程师兼 MLCommons 协会主席 Peter Mattson&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ IMG/B/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN _LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s1600/CroissantHero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 机器学习 (ML) 从业者希望重用现有数据集来训练 ML 模型，他们通常会花费大量时间来理解数据、理解其组织或弄清楚要使用哪个子集作为特征。事实上，机器学习领域的进展长期以来一直受到一个根本障碍的阻碍：数据表示的多样性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML 数据集涵盖广泛的内容类型，从文本和结构化数据到图像、音频和视频。即使在涵盖相同类型内容的数据集中，每个数据集也具有独特的文件和数据格式&lt;em>;临时&lt;/em>;排列。这一挑战降低了整个机器学习开发过程（从查找数据到训练模型）的生产力。它还阻碍了急需的数据集工具的开发。 &lt;/p>; &lt;p>; 数据集有通用元数据格式，例如 &lt;a href=&quot;http://schema.org/Dataset&quot;>;schema.org&lt;/a>; 和 &lt;a href=&quot;https:// www.w3.org/TR/vocab-dcat-3/&quot;>;DCAT&lt;/a>;。然而，这些格式是为数据发现而设计的，而不是为了满足机器学习数据的特定需求，例如从结构化和非结构化源中提取和组合数据的能力，以包含能够实现的元数据.google/responsibility/responsible-ai-practices/&quot;>;负责任地使用数据&lt;/a>;，或描述 ML 使用特征，例如定义训练、测试和验证集。 &lt;/p>; &lt;p>; 今天，我们推出 &lt;a href=&quot;https://mlcommons.org/croissant&quot;>;Croissant&lt;/a>;，这是一种适用于 ML 就绪数据集的新元数据格式。 Croissant 是由工业界和学术界社区合作开发的，是 &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; 工作的一部分。 Croissant 格式不会改变实际数据的表示方式（例如图像或文本文件格式）——它提供了描述和组织数据的标准方法。 Croissant 建立在 &lt;a href=&quot;https://schema.org/&quot;>;schema.org&lt;/a>; 之上，这是在网络上发布结构化数据的事实标准，已被超过 4000 万个数据集使用。 Croissant 通过 ML 相关元数据、数据资源、数据组织和默认 ML 语义的综合层对其进行了增强。 &lt;/p>; &lt;p>; 此外，我们还宣布主要工具和存储库的支持：今天，三个广泛使用的 ML 数据集集合 - &lt;a href=&quot;http://www.kaggle.com/datasets&quot;>;Kaggle&lt; /a>;、&lt;a href=&quot;https://huggingface.co/datasets?other=croissant&amp;amp;sort=trending&quot;>;拥抱脸&lt;/a>; 和 &lt;a href=&quot;https://openml.org/search ?type=data&quot;>;OpenML&lt;/a>; — 将开始支持其托管数据集的 Croissant 格式； &lt;a href=&quot;http://g.co/datasetsearch&quot;>;数据集搜索&lt;/a>;工具可让用户在网络上搜索 Croissant 数据集；以及流行的机器学习框架，包括 &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;、&lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;、和 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;，可以使用 &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; 轻松加载 Croissant 数据集TensorFlow 数据集&lt;/a>; (TFDS) 包。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Croissant&lt;/h2>; &lt;p>; 此 Croissant 1.0 版本包含完整的 &lt;a href=&quot;https ://mlcommons.org/croissant/1.0&quot;>;格式规范&lt;/a>;，一组&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/datasets&quot;>;示例数据集&lt;/a>;，一个开源 &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/python/mlcroissant&quot;>;Python 库&lt;/a>;，用于验证、使用和生成 Croissant 元数据，以及一个开源&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>;可视化编辑器&lt;/a>;，用于以直观的方式加载、检查和创建 Croissant 数据集描述。 &lt;/p>; &lt;p>; 支持负责任的人工智能 (RAI) 从一开始就是 Croissant 工作的一个关键目标。我们还发布了 &lt;a href=&quot;https://mlcommons.org/croissant/RAI/1.0&quot;>;Croissant RAI 词汇表&lt;/a>;扩展的第一个版本，该扩展通过描述重要 RAI 使用所需的关键属性来增强 Croissant数据生命周期管理、数据标签、参与式数据、机器学习安全性和公平性评估、可解释性和合规性等案例。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;为什么要采用 ML 数据的共享格式？&lt;/h2>; &lt;p>; 大部分 ML 工作实际上是数据工作。训练数据是决定模型行为的“代码”。数据集的范围很广，从用于训练大型语言模型 (LLM) 的文本集合到用于训练汽车防撞系统的驾驶场景（带注释的视频）集合。但是，开发 ML 模型的步骤通常遵循相同的以数据为中心的迭代过程：(1) 查找或收集数据，(2) 清理和细化数据，(3) 根据数据训练模型，(4) 测试在更多数据上建立模型，(5) 发现模型不起作用，(6) 分析数据找出原因，(7) 重复直到获得可行的模型。由于缺乏通用格式，许多步骤变得更加困难。对于资源有限的研究和早期创业工作来说，这种“数据开发负担”尤其沉重。 &lt;/p>; &lt;p>; 像牛角面包这样的格式的目标是使整个过程变得更容易。例如，搜索引擎和数据集存储库可以利用元数据来更轻松地找到正确的数据集。数据资源和组织信息使得开发用于清理、提炼和分析数据的工具变得更加容易。这些信息和默认的 ML 语义使 ML 框架能够使用数据以最少的代码来训练和测试模型。总之，这些改进大大减轻了数据开发负担。 &lt;/p>; &lt;p>; 此外，数据集作者关心其数据集的可发现性和易用性。得益于可用的创建工具和 ML 数据平台的支持，采用 Croissant 提高了数据集的价值，同时只需要很少的努力。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;牛角面包今天可以做什么？&lt;/h2>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh- 1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left : auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V 2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;Croissant 生态系统：用户可以搜索 Croissant 数据集，从主要存储库下载它们，然后轻松地将它们加载到自己喜欢的 ML 框架中。他们可以使用 Croissant 编辑器创建、检查和修改 Croissant 元数据。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 今天，用户可以在以下位置找到 Croissant 数据集：&lt;/p>; &lt;ul>; &lt; li>;Google &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;>;数据集搜索&lt;/a>;，它提供了牛角面包过滤器。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://huggingface.co/datasets?other=croissant&amp;amp;sort=trending&quot;>;HuggingFace&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;http: //kaggle.com/datasets&quot;>;Kaggle&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://openml.org/search?type=data&quot;>;OpenML&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>; 使用 Croissant 数据集，可以： &lt;/p>; &lt;ul>; &lt;li>;通过 &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>;TensorFlow 轻松摄取数据用于流行机器学习框架的数据集&lt;/a>;，例如 &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;、&lt;a href=&quot;https://pytorch.org/&quot;>; PyTorch&lt;/a>; 和 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;。 &lt;/li>;&lt;li>;使用&lt;a href=&quot;https://huggingface.co/spaces/MLCommons/croissant-editor&quot;>;Croissant 编辑器 UI&lt;/a>; 检查和修改元数据 (&lt;a href=&quot;https ://github.com/mlcommons/croissant/tree/main/editor&quot;>;github&lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;p>; 要发布 Croissant 数据集，用户可以： &lt;/p>; &lt;ul>; &lt;li>;使用 &lt;a href=&quot;https://huggingface.co/spaces/MLCommons/croissant -editor&quot;>;Croissant 编辑器 UI&lt;/a>; (&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>;github&lt;/a>;) 用于生成大部分 Croissant 元数据通过分析用户提供的数据自动填充重要的元数据字段，例如 RAI 属性。 &lt;/li>;&lt;li>;将羊角面包信息作为数据集网页的一部分发布，以使其可发现和可重用。 &lt;/li>;&lt;li>;将数据发布到支持 Croissant 的存储库之一（例如 Kaggle、HuggingFace 和 OpenML），并自动生成 Croissant 元数据。 &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;未来方向&lt;/h2>; &lt;p>; 我们对 Croissant 帮助机器学习的潜力感到兴奋但要使这种格式真正有用需要社区的支持。我们鼓励数据集创建者考虑提供羊角面包元数据。我们鼓励托管数据集的平台提供 Croissant 文件供下载，并将 Croissant 元数据嵌入数据集网页中，以便数据集搜索引擎可以发现它们。帮助用户处理 ML 数据集的工具（例如标签或数据分析工具）也应考虑支持 Croissant 数据集。我们可以共同减轻数据开发负担，打造更丰富的机器学习研究和开发生态系统。 &lt;/p>; &lt;p>; 我们鼓励社区&lt;a href=&quot;http://mlcommons.org/croissant&quot;>;与我们一起&lt;/a>;为这项工作做出贡献。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Croissant 是由 &lt;a href=&quot;https 开发的://datasetsearch.research.google.com/&quot;>;数据集搜索&lt;/a>;、&lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; 和 &lt;a href=&quot;https: //www.tensorflow.org/datasets&quot;>;来自 Google 的 TensorFlow Datasets&lt;/a>; 团队，作为 &lt;a href=&quot;http://mlcommons.org&quot;>;MLCommons&lt;/a>; 社区工作组的一部分，该工作组还包括来自以下组织的贡献者：Bayer、cTuning Foundation、DANS-KNAW、Dotphoton、Harvard、Hugging Face、伦敦国王学院、LIST、Meta、NASA、北卡罗来纳州立大学、开放数据研究所、加泰罗尼亚开放大学、Sage Bionetworks 和埃因霍温工业大学。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8393293208018757284/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html#comment-form&quot; rel=&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8393293208018757284&quot; rel=&quot;edit&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8393293208018757284&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt; link href=&quot;http://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html&quot; rel=&quot;alternate&quot; title=&quot;Croissant：ML就绪数据集的元数据格式&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm -T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r 5r_uAU2P4srnP/s72-c/CroissantHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2754526782497247497&lt;/id>;&lt;已发布>;2024-03-04T07:06 ：00.000-08:00&lt;/已发布>;&lt;已更新>;2024-03-05T08:40:45.490-08:00&lt;/已更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=“会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://www” .blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;量子计算&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 参加 APS 2024&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Kate Weber 和 Shannon Leon，Google 研究部量子 AI 团队&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npW dS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s1600/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 今天，&lt;a href=&quot;https://www.aps.org/meetings/meeting.cfm?name=MAR24&quot;>;2024 年 3 月会议&lt;/a>; &lt;a href=&quot;https:// /www.aps.org/&quot;>;美国物理学会&lt;/a>; (APS) 在明尼苏达州明尼阿波利斯成立。 APS 2024 是一次涵盖物理学及相关领域主题的顶级会议，汇集了研究人员、学生和行业专业人士，分享他们的发现并建立合作伙伴关系，以实现物理相关科学和技术的根本性进步。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今年，Google 在 APS 上表现强劲，其展位由 Google &lt;a href=&quot;https://quantumai.google/&quot; 主办>;量子AI&lt;/a>;团队，整个会议期间进行了50+次演讲，并参与了会议组织活动、专题会议和活动。亲自参加 APS 2024？欢迎参观 Google 的量子人工智能展位，详细了解我们为解决该领域一些最有趣的挑战而所做的令人兴奋的工作。 &lt;!--访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 帐户了解 Google 展位活动（例如演示和问答环节） .-->; &lt;/p>; &lt;p>; 您可以详细了解我们在会议上展示的最新前沿工作以及下面的展位活动时间表（Google 员工以&lt;strong>;粗体&lt;/strong>;列出）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p >; 会议主席包括：&lt;strong>;Aaron Szasz&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;展位活动&lt; /h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;em>;此时间表可能会发生变化。请访问 Google Quantum AI 展位了解更多信息。&lt;/em>; &lt;/p>; &lt;p>; Crumble：用于可视化 QEC 电路的原型交互工具&lt;br />; 演讲者：&lt;strong>;Matt McEwen&lt;/strong>; &lt;br / >; 3 月 5 日，星期二 |中部标准时间上午 11:00 &lt;/p>; &lt;p>; Qualtran：用于容错算法的有效资源估计的开源库 &lt;br />; 演讲者：&lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; 3 月 5 日，星期二|下午 2:30 CST &lt;/p>; &lt;p>; Qualtran：用于容错算法的有效资源估计的开源库&lt;br />; 演讲者：&lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; 3 月 7 日星期四|上午 11:00 CST &lt;/p>; &lt;p>; 价值 500 万美元的 XPRIZE / Google Quantum AI 竞赛，旨在加速量子应用问答 &lt;br />; 演讲者：&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; 3 月 7 日，星期四|上午 11:00（美国中部标准时间） &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;演讲&lt;/h2>; &lt;h3>;周一&lt;/h3 >; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/A45.1&quot;>;证明少数国家的高度纠缠状态单量子位测量&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Hsin-Yuan Huang&lt;/strong>; &lt;br />; 作者：&lt;strong>;Hsin-Yuan Huang&lt;/strong>; &lt;br />; &lt;em>;会议A45：机器学习量子物理新领域&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/A51.2&quot;>;迈向高保真超导量子位的模拟量子模拟&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Trond Andersen&lt;/strong>; &lt;br />; 作者：&lt;strong>;Trond I Andersen&lt;/strong>;、&lt;strong>;小米&lt;/strong >;、&lt;strong>;阿米尔·H·卡拉姆卢&lt;/strong>;、&lt;strong>;尼基塔·阿斯特拉罕采夫&lt;/strong>;、&lt;strong>;安德烈·克洛茨&lt;/strong>;、&lt;strong>;朱莉娅·伯恩特森&lt;/strong>;、&lt;strong>;安德烈·佩图霍夫&lt;/strong>; &lt;strong>;、&lt;strong>;德米特里·阿巴宁&lt;/strong>;、&lt;strong>;Lev B Ioffe&lt;/strong>;、&lt;strong>;陈宇&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>;、&lt;strong>;Pedram Roushan&lt; /strong>; &lt;br />; &lt;em>;会议 A51：噪声量子硬件上的应用 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session /B50.6&quot;>;测量表面代码电路上下文中的电路错误&lt;/a>; &lt;br />;演讲者：&lt;strong>;Dripto M Debroy&lt;/strong>; &lt;br />;作者：&lt;strong>;Dripto M Debroy&lt;/strong >;、&lt;strong>;Jonathan A Gross&lt;/strong>;、&lt;strong>;Élie Genois&lt;/strong>;、&lt;strong>;张江&lt;/strong>; &lt;br />; &lt;em>;会议 B50：使用 QCVV 技术表征噪声&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.6&quot;>;惯性聚变目标设计的阻止本领的量子计算 I：物理概述和经典算法的局限性&lt;/a>; &lt;br />;演讲者：Andrew D. Baczewski &lt;br />;作者：&lt;strong>;Nicholas C. Rubin&lt;/strong>;、Dominic W. Berry、Alina Kononov、&lt;strong>;Fionn D.马龙、塔努吉·哈塔尔、亚历克·怀特、李俊浩、哈特穆特·内文、瑞安·巴布什、安德鲁D. Baczewski &lt;br />; &lt;em>;会议 B51：量子应用的异构设计&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.12352.pdf&quot;>;论文链接&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.7&quot;>;惯性聚变目标设计停止本领的量子计算二：物理概述以及经典算法的局限性&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Nicholas C. Rubin&lt;/strong>; &lt;br />; 作者：&lt;strong>;Nicholas C. Rubin&lt;/strong>;、Dominic W. Berry、阿丽娜·科诺诺夫、&lt;strong>;菲恩·D·马龙&lt;/strong>;、&lt;strong>;塔努吉·卡塔尔&lt;/strong>;、亚历克·怀特、&lt;strong>;李俊浩&lt;/strong>;、&lt;strong>;哈特穆特·内文&lt;/strong>;、&lt;strong >;Ryan Babbush&lt;/strong>;、Andrew D. Baczewski &lt;br />; &lt;em>;会议 B51：量子应用的异构设计&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/ 2308.12352.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B56.4&quot;>;校准超导量子位：来自NISQ 到容错&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Sabrina S Hong&lt;/strong>; &lt;br />; 作者：&lt;strong>;Sabrina S Hong&lt;/strong>; &lt;br />; &lt;em>;Session B56:从 NISQ 到容错&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B31.9&quot;>;测量和前馈引起的纠缠负性转变&lt; /a>; &lt;br />; 演讲者：&lt;strong>;Ramis Movassagh&lt;/strong>; &lt;br />; 作者：Alireza Seif、Yu-Xin Wang、&lt;strong>;Ramis Movassagh&lt;/strong>;、Aashish A. Clerk &lt;br />; &lt;em>;会议 B31：多体系统中测量引起的临界&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2310.18305.pdf&quot;>;论文链接&lt;/a>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B52.9&quot;>;噪声量子处理实验的有效量子体积、保真度和计算成本&lt;/a>; &lt; br />; 演讲者：&lt;strong>;Salvatore Mandra&lt;/strong>; &lt;br />; 作者：&lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;、&lt;strong>;Sergei V Isakov&lt;/strong>;、&lt;strong>;Salvatore Mandra&lt;/strong>; ，&lt;strong>;本杰明·维拉隆加&lt;/strong>;，&lt;strong>;X。 Mi&lt;/strong>;、&lt;strong>;Sergio Boixo&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;会议 B52：量子算法和复杂性&lt;/em>; &lt;br />; &lt;a href =&quot;https://arxiv.org/pdf/2306.15970.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/ Session/D60.4&quot;>;使用机器学习相互作用势和原子位置协方差绘制准确的固体热力学表&lt;/a>; &lt;br />;演讲者：Mgcini K Phuthi &lt;br />;作者：Mgcini K Phuthi、Yang Huang、Michael Widom , &lt;strong>;Ekin D Cubuk&lt;/strong>;, Venkat Viswanathan &lt;br />; &lt;em>;会议 D60：分子和材料的机器学习：化学空间和动力学&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期二&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https: //meetings.aps.org/Meeting/MAR24/Session/F50.4&quot;>;原位脉冲包络表征技术(INSPECT)&lt;/a>; &lt;br />;主讲人：&lt;strong>;张江&lt;/strong>; &lt;br />; 作者：&lt;strong>;张江&lt;/strong>;、&lt;strong>;Jonathan A Gross&lt;/strong>;、&lt;strong>;Élie Genois&lt;/strong>; &lt;br />; &lt;em>;会议 F50：高级随机基准测试和门校准&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/F50.11&quot;>;利用动态解耦来表征两个量子位门&lt;/a>; &lt; br />; 演讲者：&lt;strong>;Jonathan A Gross&lt;/strong>; &lt;br />; 作者：&lt;strong>;Jonathan A Gross&lt;/strong>;、&lt;strong>;张江&lt;/strong>;、&lt;strong>;Élie Genois、Dripto M Debroy&lt;/strong>;、Ze-Pei Cian*、&lt;strong>;Wojciech Mruczkiewicz&lt;/strong>; &lt;br />; &lt;em>;会议 F50：高级随机基准测试和门校准&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/EE01.2&quot;>;二次模型回归的统计物理&lt;/a>; &lt;br />;演讲者：Blake Bordelon &lt;br />;作者：Blake Bordelon、Cengiz Pehlevan、&lt;strong>;Yasaman Bahri&lt;/strong>; &lt;br />; &lt;em>;会议 EE01：V：统计和非线性物理 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /meetings.aps.org/Meeting/MAR24/Session/G51.2&quot;>;改进电子结构首次量子化模拟的状态准备&lt;/a>; &lt;br />; 演讲者：&lt;strong>;William J Huggins&lt;/strong>; &lt; br />; 作者：&lt;strong>;William J Huggins&lt;/strong>;、&lt;strong>;Oskar Leimkuhler&lt;/strong>;、&lt;strong>;Torin F Stetina&lt;/strong>;、&lt;strong>;Birgitta Whaley&lt;/strong>; &lt;br />; &lt;em>;会议 G51：哈密顿模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G30.2&quot;>;控制大型超导量子处理器&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Paul V. Klimov&lt;/strong>; &lt;br />; 作者：&lt;strong>;Paul V. Klimov&lt;/strong>;、&lt;strong>;Andreas Bengtsson&lt;/strong>;、&lt;克里斯·昆塔纳 (Chris Quintana)、亚历山大·布拉萨 (Alexandre Bourassa)、萨布丽娜·洪 (Sabrina Hong)、安德鲁·邓斯沃思 (Andrew Dunsworth)、凯文·J·萨辛格 (Kevin J. Satzinger) ，&lt;strong>;威廉·P·利文斯顿&lt;/strong>;，&lt;strong>;弗拉基米尔·西瓦克&lt;/strong>;，&lt;strong>;墨菲·Y·纽&lt;/strong>;，&lt;strong>;特隆德·I·安德森&lt;/strong>;，&lt;strong>;张亚星&lt;/strong>;、&lt;strong>;德斯蒙德·奇克&lt;/strong>;、&lt;strong>;陈子君&lt;/strong>;、&lt;strong>;查尔斯·尼尔&lt;/strong>;、&lt;strong>;凯瑟琳·埃里克森&lt;/strong>;、&lt;strong>;亚历杭德罗·格拉哈莱斯·道&lt;/strong>;、&lt;strong>;安东尼·梅格兰特&lt;/strong>;、&lt;strong>;佩德拉姆·鲁山&lt;/strong>;、&lt;strong>;亚历山大·科罗特科夫&lt;/strong>;、&lt;strong>;朱利安·凯利&lt;/strong>;、 &lt;strong>;Vadim Smelyanskiy&lt;/strong>;、&lt;strong>;Yu Chen&lt;/strong>;、&lt;strong>;Hartmut Neven&lt;/strong>; &lt;br />; &lt;em>;G30 会议：量子计算的商业应用&lt;/em>;&lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.02321.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org /Meeting/MAR24/Session/G50.5&quot;>;高斯玻色子采样：确定量子优势&lt;/a>; &lt;br />; 演讲者：Peter D Drummond &lt;br />; 作者：Peter D Drummond、Alex Dellios、Ned Goodman、Margaret D Reid，&lt;strong>;Ben Villalonga&lt;/strong>; &lt;br />; &lt;em>;G50 会议：量子表征、验证和验证 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings .aps.org/Meeting/MAR24/Session/G50.8&quot;>;关注复杂性 III：学习随机量子电路状态的复杂性&lt;/a>; &lt;br />; 演讲者：Hyejin Kim &lt;br />; 作者：Hyejin Kim，周一清、徐一辰、万超、周锦、&lt;strong>;Yuri D Lensky&lt;/strong>;、Jesse Hoke、&lt;strong>;Pedram Roushan&lt;/strong>;、Kilian Q Weinberger、Eun-Ah Kim &lt;br />; &lt;em >;会议 G50：量子表征、验证和验证 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/K48.10&quot;>;平衡超导电路中的耦合&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Daniel T Sank&lt;/strong>; &lt;br />; 作者：&lt;strong>;Daniel T Sank&lt;/strong>;、&lt;strong>;Sergei V Isakov&lt;/strong >;, &lt;strong>;Mostafa Khezri&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>; &lt;br />; &lt;em>;K48 会议：强驱动超导系统&lt;/em>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://meetings.aps.org/Meeting/MAR24/Session/K49.12&quot;>;使用 Qᴜᴀʟᴛʀᴀɴ 进行容错算法的资源估计&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Tanuj Khattar&lt;/strong>; &lt; br />; 作者：&lt;strong>;Tanuj Khattar&lt;/strong>;、&lt;b>;Matthew Harrigan&lt;/b>;、&lt;b>;Fionn D. Malone&lt;/b>;、&lt;b>;Nour Yosri&lt;/b>;、&lt;b>; Nicholas C. Rubin&lt;/b>;&lt;br />; &lt;em>;K49 会议：近期量子计算机的算法和实现&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期三&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/ Meeting/MAR24/Session/M24.1&quot;>;用超导量子比特发现新颖的量子动力学&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; 作者：&lt;strong>;Pedram Roushan&lt;/a>; &lt;br />; strong>; &lt;br />; &lt;em>;会议 M24：跨平台的模拟量子模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/M27 .7&quot;>;破译三阴性乳腺癌中的肿瘤异质性：动态细胞-细胞和细胞-基质相互作用的关键作用&lt;/a>; &lt;br />;演讲者：Susan Leggett &lt;br />;作者：Susan Leggett、Ian Wong , Celeste Nelson, Molly Brennan, &lt;strong>;Mohak Patel&lt;/strong>;, Christian Franck, Sophia Martinez, Joe Tien, Lena Gamboa, Thomas Valentin, Amanda Khoo, Evelyn K Williams &lt;br />; &lt;em>;第 M27 节：力学细胞和组织 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48.2&quot;>;实现受保护的电荷奇偶校验量子位&lt; /a>; &lt;br />; 演讲者：Abigail Shearrow &lt;br />; 作者：Abigail Shearrow、Matthew Snyder、Bradley G Cole、Kenneth R Dodge、Yebin Liu、Andrey Klots、&lt;strong>;Lev B Ioffe&lt;/strong>;、Britton L Plourde，Robert McDermott &lt;br />; &lt;em>;第 N48 场会议：非常规超导量子位&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48 .3&quot;>;受保护的电荷奇偶校验量子位的隧道结中的电子电容&lt;/a>; &lt;br />; 演讲者：Bradley G Cole &lt;br />; 作者：Bradley G Cole、Kenneth R Dodge、Yebin Liu、Abigail Shearrow、Matthew Snyder 、&lt;strong>;Andrey Klots&lt;/strong>;、&lt;strong>;Lev B Ioffe&lt;/strong>;、Robert McDermott、BLT Plourde &lt;br />; &lt;em>;第 N48 场会议：非常规超导量子位&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.7&quot;>;克服量子纠错中的泄漏&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Kevin C. Miao &lt;/strong>; &lt;br />; 作者：&lt;strong>;Kevin C. Miao&lt;/strong>;、&lt;strong>;Matt McEwen&lt;/strong>;、&lt;strong>;Juan Atalaya&lt;/strong>;、&lt;strong>;Dvir Kafri&lt;/strong>; >;、&lt;strong>;Leonid P. Pryadko&lt;/strong>;、&lt;strong>;Andreas Bengtsson&lt;/strong>;、&lt;strong>;Alex Opremcak&lt;/strong>;、&lt;strong>;Kevin J. Satzinger&lt;/strong>;、&lt;strong>;子君陈&lt;/strong>;、&lt;strong>;保罗·V·克里莫夫&lt;/strong>;、&lt;strong>;克里斯·昆塔纳&lt;/strong>;、&lt;strong>;拉吉夫·阿查里亚&lt;/strong>;、&lt;strong>;凯尔·安德森&lt;/strong>;、&lt;strong>; >;马库斯·安斯曼&lt;/strong>;、&lt;strong>;弗兰克·阿鲁特&lt;/strong>;、&lt;strong>;库纳尔·艾莉亚&lt;/strong>;、&lt;strong>;亚伯拉罕·阿斯福&lt;/strong>;、&lt;strong>;约瑟夫·C·巴丁&lt;/strong>;、 &lt;strong>;亚历山大·布拉萨&lt;/strong>;、&lt;strong>;珍娜·博瓦伊德&lt;/strong>;、&lt;strong>;莱昂·布里尔&lt;/strong>;、&lt;strong>;鲍勃·B·巴克利&lt;/strong>;、&lt;strong>;大卫·A·布尔&lt; /strong>;、&lt;strong>;蒂姆·伯格&lt;/strong>;、&lt;strong>;布莱恩·伯克特&lt;/strong>;、&lt;strong>;尼古拉斯·布什内尔&lt;/strong>;、&lt;strong>;胡安·坎佩罗&lt;/strong>;、&lt;strong>;本·基亚罗&lt; /strong>;、&lt;strong>;罗伯托·柯林斯&lt;/strong>;、&lt;strong>;保罗·康纳&lt;/strong>;、&lt;strong>;亚历山大·克鲁克&lt;/strong>;、&lt;strong>;本·科廷&lt;/strong>;、&lt;strong>;Dripto M.德布罗伊&lt;/strong>;、&lt;strong>;肖恩·德穆拉&lt;/strong>;、&lt;strong>;安德鲁·邓斯沃斯&lt;/strong>;、&lt;strong>;凯瑟琳·埃里克森&lt;/strong>;、&lt;strong>;雷扎·法特米&lt;/strong>;、&lt;strong>; >;维尼修斯·费雷拉&lt;/strong>;、&lt;strong>;莱斯利·弗洛雷斯·布尔戈斯&lt;/strong>;、&lt;strong>;易卜拉欣·福拉蒂&lt;/strong>;、&lt;strong>;奥斯汀·G·福勒&lt;/strong>;、&lt;strong>;布鲁克斯·福克斯&lt;/strong>; &lt;strong>;、&lt;strong>;贡萨洛·加西亚&lt;/strong>;、&lt;strong>;威廉·江&lt;/strong>;、&lt;strong>;克雷格·吉德尼&lt;/strong>;、&lt;strong>;玛丽莎·朱斯蒂娜&lt;/strong>;、&lt;strong>;Raja Gosula&lt;/strong>; &lt;strong>;、&lt;strong>;Alejandro Grajales Dau&lt;/strong>;、&lt;strong>;乔纳森 A. 格罗斯&lt;/strong>;、&lt;strong>;迈克尔 C. 汉密尔顿&lt;/strong>;、&lt;strong>;肖恩 D. 哈林顿&lt;/strong>;、 &lt;strong>;Paula Heu&lt;/strong>;、&lt;strong>;杰里米·希尔顿&lt;/strong>;、&lt;strong>;Markus R. Hoffmann&lt;/strong>;、&lt;strong>;Sabrina Hong&lt;/strong>;、&lt;strong>;Trent Huang&lt;/strong>; >;、&lt;strong>;阿什利·赫夫&lt;/strong>;、&lt;strong>;贾斯汀·艾夫兰&lt;/strong>;、&lt;strong>;埃文·杰弗里&lt;/strong>;、&lt;strong>;张江&lt;/strong>;、&lt;strong>;科迪·琼斯&lt;/strong>; >;、&lt;strong>;朱利安·凯利&lt;/strong>;、&lt;strong>;Seon Kim&lt;/strong>;、&lt;strong>;费多尔·科斯特里萨&lt;/strong>;、&lt;strong>;约翰·马克·克莱克鲍姆&lt;/strong>;、&lt;strong>;大卫·兰德休斯&lt;/strong>; 、&lt;strong>;帕维尔·拉普捷夫&lt;/strong>;、&lt;strong>;莉莉·劳斯&lt;/strong>;、&lt;strong>;肯尼·李&lt;/strong>;、&lt;strong>;布莱恩·J·莱斯特&lt;/strong>;、&lt;strong>;亚历山大·T . Lill&lt;/strong>;、&lt;strong>;Wayne Liu&lt;/strong>;、&lt;strong>;Aditya Locharla&lt;/strong>;、&lt;strong>;Erik Lucero&lt;/strong>;、&lt;strong>;Steven Martin&lt;/strong>;、&lt;strong>;安东尼·梅格兰特、&lt;strong>;小米&lt;/strong>;、&lt;strong>;希林·蒙塔泽里&lt;/strong>;、&lt;strong>;亚历克西斯·莫万&lt;/strong>;、&lt;strong>;奥弗·纳曼&lt;/strong>;、&lt;strong>;马修·尼利 (Matthew Neeley)、&lt;strong>;查尔斯·尼尔 (Charles Neill)&lt;/strong>;、&lt;strong>;Ani Nersisyan&lt;/strong>;、&lt;strong>;迈克尔·纽曼 (Michael Newman)&lt;/strong>;、&lt;strong>;Jiun How Ng&lt;/strong>;、&lt;strong>; >;安东尼·阮&lt;/strong>;、&lt;strong>;默里·阮&lt;/strong>;、&lt;strong>;丽贝卡·波特&lt;/strong>;、&lt;strong>;查尔斯·罗克&lt;/strong>;、&lt;strong>;佩德拉姆·鲁山&lt;/strong>;、&lt;strong>; >;坎南·桑卡拉戈马蒂&lt;/strong>;、&lt;strong>;克里斯托弗·舒斯特&lt;/strong>;、&lt;strong>;迈克尔·J·谢恩&lt;/strong>;、&lt;strong>;亚伦·肖特&lt;/strong>;、&lt;strong>;诺亚·舒蒂&lt;/strong>;、 &lt;strong>;弗拉基米尔·施瓦茨&lt;/strong>;、&lt;strong>;金德拉·斯克鲁兹尼&lt;/strong>;、&lt;strong>;W.克拉克·史密斯&lt;/strong>;、&lt;strong>;乔治·斯特林&lt;/strong>;、&lt;strong>;马可·萨莱&lt;/strong>;、&lt;strong>;道格拉斯·托尔&lt;/strong>;、&lt;strong>;阿尔弗雷多·托雷斯&lt;/strong>;、&lt;strong>;西奥多·怀特 (Theodore White)、&lt;strong>;布莱恩 WK Woo&lt;/strong>;、&lt;strong>;Z.杰米·姚&lt;/strong>;、&lt;strong>;叶平&lt;/strong>;、&lt;strong>;Juhwan Yoo&lt;/strong>;、&lt;strong>;格雷森·杨&lt;/strong>;、&lt;strong>;亚当·扎尔克曼&lt;/strong>;、&lt;strong>;朱宁峰&lt;/strong>;、&lt;strong>;尼古拉斯·佐布里斯特&lt;/strong>;、&lt;strong>;哈特穆特·内文&lt;/strong>;、&lt;strong>;瓦迪姆·斯梅尔扬斯基&lt;/strong>;、&lt;strong>;安德烈·佩图霍夫&lt;/strong>;、&lt;strong>; Alexander N. Korotkov&lt;/strong>;、&lt;strong>;Daniel Sank&lt;/strong>;、&lt;strong>;Yu Chen&lt;/strong>; &lt;br />; &lt;em>;会议 N51：量子纠错码性能和实现 I&lt;/em>; &lt;br />; &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://meetings.aps.org/Meeting/MAR24/Session/N51.11&quot;>;使用非均匀误差分布对表面代码的性能进行建模：第 1 部分&lt;/a>; &lt;br />; 演示者：&lt;strong>;Yuri D Lensky&lt;/strong>; &lt;br />; 作者：&lt;strong>;Yuri D Lensky&lt;/strong>;、&lt;strong>;Volodymyr Sivak&lt;/strong>;、&lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;、&lt;strong>;Igor Aleiner&lt;/strong>; strong>; &lt;br />; &lt;em>;会议 N51：量子纠错码性能和实现 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/ Session/N51.12&quot;>;使用非均匀误差分布对表面代码的性能进行建模：第 2 部分&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Volodymyr Sivak&lt;/strong>; &lt;br />; 作者：&lt;strong >;弗拉基米尔·西瓦克&lt;/strong>;、&lt;strong>;迈克尔·纽曼&lt;/strong>;、&lt;strong>;科迪·琼斯&lt;/strong>;、&lt;strong>;亨利·舒尔克斯&lt;/strong>;、&lt;strong>;德维尔·卡弗里&lt;/strong>;、&lt;strong>; >;Yuri D Lensky&lt;/strong>;、&lt;strong>;Paul Klimov&lt;/strong>;、&lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;会议 N51：量子误差修正代码性能和实现 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q51.7&quot;>;高度优化的张量网络收缩经典挑战性量子计算的模拟&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; 作者：&lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; &lt;em>;会议 Q51：量子经典算法的共同进化&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q61.7&quot;>;使用现代量子计算概念进行教学各级实践开源软件&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; 作者：&lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; &lt;em >;课程 Q61：各级量子信息教学 II&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期四&lt; /h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.1&quot;>;新电路和开放颜色代码的源解码器&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Craig Gidney&lt;/strong>; &lt;br />; 作者：&lt;strong>;Craig Gidney&lt;/strong>;、&lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;会议 S51：量子纠错码性能和实现 II&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2312.08813.pdf&quot;>;论文链接&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S18.2&quot;>;使用大型语言模型执行 Hartree-Fock 多体物理计算&lt; /a>; &lt;br />; 演讲者：&lt;strong>;Eun-Ah Kim&lt;/strong>; &lt;br />; 作者：&lt;strong>;Eun-Ah Kim&lt;/strong>;、Haining Pan、&lt;strong>;Nayantara Mudur&lt;/strong>; , William Taranto,&lt;strong>; Subhashini Venugopalan&lt;/strong>;, &lt;strong>;Yasaman Bahri&lt;/strong>;, &lt;strong>;Michael P Brenner&lt;/strong>; &lt;br />; &lt;em>;会议 S18：数据科学、人工智能和机器物理学习 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.5&quot;>;减少地表资源开销的新方法代码&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Michael Newman&lt;/strong>; &lt;br />; 作者：&lt;strong>;Craig M Gidney&lt;/strong>;、&lt;strong>;Michael Newman&lt;/strong>;、&lt;strong>; Peter Brooks&lt;/strong>;、&lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;会议 S51：量子纠错码性能和实现 II&lt;/em>; &lt;br />; &lt;a href=&quot;https:/ /arxiv.org/pdf/2312.04522.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S49.10 &quot;>;将量子计算机应用于药物设计的挑战和机遇&lt;/a>; &lt;br />; 演讲者：Raffaele Santagati &lt;br />; 作者：Raffaele Santagati、Alan Aspuru-Guzik、&lt;strong>;Ryan Babbush&lt;/strong>;、Matthias Degroote 、莱蒂西亚·冈萨雷斯、艾丽卡·基瑟娃、尼古拉·莫尔、马库斯·奥佩尔、罗伯特·M·帕里什、&lt;strong>;尼古拉斯·C·鲁宾&lt;/strong>;、迈克尔·斯特雷夫、克里斯托弗·S·陶特曼、霍斯特·韦斯、内森·维贝、克莱门斯·乌奇-乌奇&lt;br />; &lt;em>;会议 S49：近期应用的量子算法进展&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2301.04114.pdf&quot;>;论文链接&lt;/em>; a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T45.1&quot;>;谷歌在新应用中寻找超二次量子优势的报道&lt;/ a>; &lt;br />; 演讲者：&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; 作者：&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;会议 T45：量子算法的最新进展&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T48.11&quot;>;Qubit 作为反射计&lt;/a>; &lt;br />; 演讲者：&lt;strong >;张亚星&lt;/strong>; &lt;br />; 作者：&lt;strong>;张亚星&lt;/strong>;、&lt;strong>;Benjamin Chiaro&lt;/strong>; &lt;br />; &lt;em>;会议T48：超导制造、封装和技术验证&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W14.3&quot;>;非局域测量引起的相变的随机矩阵理论Floquet 量子电路&lt;/a>; &lt;br />; 演讲者：Aleksei Khindanov &lt;br />; 作者：Aleksei Khindanov、&lt;strong>;Lara Faoro&lt;/strong>;、&lt;strong>;Lev Ioffe&lt;/strong>;、&lt;strong>;Igor Aleiner&lt; /strong>; &lt;br />; &lt;em>;第 W14 场会议：测量引起的相变&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/ W58.5&quot;>;MERA 有限密度多体基态的连续极限&lt;/a>; &lt;br />; 演讲者：Subhayan Sahu &lt;br />; 作者：Subhayan Sahu，&lt;strong>;Guifré Vidal&lt;/strong>; &lt;br / >; &lt;em>;W58 会议：流体动力学及相关学科的超大规模计算科学发现 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/ Session/W50.8&quot;>;海森堡自旋链中无限温度下的磁化动力学&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Eliott Rosenberg&lt;/strong>; &lt;br />; 作者：&lt;strong>;Eliott Rosenberg&lt;/strong>;&lt;/ &lt;strong>;、&lt;strong>;特隆德·安德森&lt;/strong>;、Rhine Samajdar、&lt;strong>;安德烈·佩图霍夫&lt;/strong>;、杰西·霍克*、&lt;strong>;德米特里·阿巴宁&lt;/strong>;、&lt;strong>;Andreas Bengtsson&lt;/strong>;、 &lt;strong>;伊利亚·德罗兹多夫&lt;/strong>;、&lt;strong>;凯瑟琳·埃里克森&lt;/strong>;、&lt;strong>;保罗·克里莫夫&lt;/strong>;、&lt;strong>;小米&lt;/strong>;、&lt;strong>;亚历克西斯·莫万&lt;/strong>;、 &lt;strong>;马修·尼利&lt;/strong>;、&lt;strong>;查尔斯·尼尔&lt;/strong>;、&lt;strong>;拉吉夫·阿查亚&lt;/strong>;、&lt;strong>;理查德·艾伦&lt;/strong>;、&lt;strong>;凯尔·安德森&lt;/strong>;、 &lt;strong>;马库斯·安斯曼&lt;/strong>;、&lt;strong>;弗兰克·阿鲁特&lt;/strong>;、&lt;strong>;库纳尔·艾莉亚&lt;/strong>;、&lt;strong>;亚伯拉罕·阿斯福&lt;/strong>;、&lt;strong>;胡安·阿塔拉亚&lt;/strong>;、 &lt;strong>;约瑟夫·巴尔丁&lt;/strong>;，&lt;strong>;A.比尔梅斯&lt;/strong>;、&lt;strong>;吉娜·博尔托利&lt;/strong>;、&lt;strong>;亚历山大·布拉萨&lt;/strong>;、&lt;strong>;珍娜·博瓦尔德&lt;/strong>;、&lt;strong>;莱昂·布里尔&lt;/strong>;、&lt;strong>;迈克尔布劳顿&lt;/strong>;、&lt;strong>;鲍勃·B·巴克利&lt;/strong>;、&lt;strong>;大卫·布尔&lt;/strong>;、&lt;strong>;蒂姆·伯格&lt;/strong>;、&lt;strong>;布莱恩·伯克特&lt;/strong>;、&lt;strong>; >;尼古拉斯·布什内尔&lt;/strong>;、&lt;strong>;胡安·坎佩罗&lt;/strong>;、&lt;strong>;张洪深&lt;/strong>;、&lt;strong>;陈子君&lt;/strong>;、&lt;strong>;本杰明·基亚罗&lt;/strong>;、 &lt;strong>;德斯蒙德·奇克&lt;/strong>;、&lt;strong>;乔什·科根&lt;/strong>;、&lt;strong>;罗伯托·柯林斯&lt;/strong>;、&lt;strong>;保罗·康纳&lt;/strong>;、&lt;strong>;威廉·考特尼&lt;/strong>;、 &lt;strong>;亚历山大·克鲁克&lt;/strong>;、&lt;strong>;本·科廷&lt;/strong>;、&lt;strong>;德里普托·德布罗伊&lt;/strong>;、&lt;strong>;亚历山大·德尔·托罗·巴尔巴&lt;/strong>;、&lt;strong>;肖恩·德穆拉&lt;/strong>; >;、&lt;strong>;奥古斯丁·迪保罗&lt;/strong>;、&lt;strong>;安德鲁·邓斯沃斯&lt;/strong>;、&lt;strong>;克林特·厄尔&lt;/strong>;、&lt;strong>;E. Farhi&lt;/strong>;、&lt;strong>;雷扎·法特米&lt;/strong>;、&lt;strong>;维尼修斯·费雷拉&lt;/strong>;、&lt;strong>;莱斯利·弗洛雷斯&lt;/strong>;、&lt;strong>;易卜拉欣·福拉蒂&lt;/strong>;、&lt;strong>;奥斯汀福勒、&lt;strong>;布鲁克斯·福克斯&lt;/strong>;、&lt;strong>;贡萨洛·加西亚&lt;/strong>;、&lt;strong>;艾莉·杰诺瓦&lt;/strong>;、&lt;strong>;威廉·江&lt;/strong>;、&lt;strong>;克雷格吉德尼&lt;/strong>;、&lt;strong>;达尔·吉尔博亚&lt;/strong>;、&lt;strong>;玛丽莎·朱斯蒂娜&lt;/strong>;、&lt;strong>;拉贾·戈苏拉&lt;/strong>;、&lt;strong>;亚历杭德罗·格拉哈莱斯·道&lt;/strong>;、&lt;strong>;乔纳森·格罗斯、&lt;strong>;史蒂夫·哈贝格&lt;/strong>;、&lt;strong>;迈克尔·汉密尔顿&lt;/strong>;、&lt;strong>;莫妮卡·汉森&lt;/strong>;、&lt;strong>;马修·哈里根&lt;/strong>;、&lt;strong>;肖恩·哈灵顿、&lt;strong>;宝拉·休&lt;/strong>;、&lt;strong>;戈登·希尔&lt;/strong>;、&lt;strong>;马库斯·霍夫曼&lt;/strong>;、&lt;strong>;萨布丽娜·洪&lt;/strong>;、&lt;strong>;特伦特·黄&lt;/strong>;、&lt;strong>;阿什利·赫夫&lt;/strong>;、&lt;strong>;威廉·哈金斯&lt;/strong>;、&lt;strong>;列夫·伊奥夫&lt;/strong>;、&lt;strong>;谢尔盖·伊萨科夫&lt;/strong>;、&lt;strong>;贾斯汀·艾维兰&lt;/strong>;、&lt;strong>;埃文·杰弗里&lt;/strong>;、&lt;strong>;张江&lt;/strong>;、&lt;strong>;科迪·琼斯&lt;/strong>;、&lt;strong>;帕沃尔·尤哈斯&lt;/strong>;、&lt;strong>; D .卡夫里&lt;/strong>;、&lt;strong>;塔努吉·哈塔尔&lt;/strong>;、&lt;strong>;莫斯塔法·赫兹里&lt;/strong>;、&lt;strong>;玛丽亚·基费洛瓦&lt;/strong>;、&lt;strong>;Seon Kim&lt;/strong>;、&lt;strong>;阿列克谢基塔耶夫、&lt;strong>;安德烈·克洛茨&lt;/strong>;、&lt;strong>;亚历山大·科罗特科夫&lt;/strong>;、&lt;strong>;费多尔·科斯特里察&lt;/strong>;、&lt;strong>;约翰·马克·克莱克鲍姆&lt;/strong>;、&lt;strong>;大卫·兰德休斯 (David Landhuis)、&lt;strong>;帕维尔·拉普捷夫 (Pavel Laptev)&lt;/strong>;、&lt;strong>;刘金铭&lt;/strong>;、&lt;strong>;莉莉·劳斯 (Lily Laws)&lt;/strong>;、&lt;strong>;李俊浩&lt;/strong>;、&lt;strong>; >;肯尼思·李&lt;/strong>;、&lt;strong>;尤里·连斯基&lt;/strong>;、&lt;strong>;布莱恩·莱斯特&lt;/strong>;、&lt;strong>;亚历山大·利尔&lt;/strong>;、&lt;strong>;韦恩·刘&lt;/strong>;、&lt;strong>; >;威廉·P·利文斯顿&lt;/strong>;，&lt;strong>;A.洛查拉&lt;/strong>;、&lt;strong>;萨尔瓦托雷·曼德拉&lt;/strong>;、&lt;strong>;猎户座马丁&lt;/strong>;、&lt;strong>;史蒂文·马丁&lt;/strong>;、&lt;strong>;贾罗德·麦克林&lt;/strong>;、&lt;strong>;马修麦克尤恩、&lt;strong>;塞内卡·米克斯&lt;/strong>;、&lt;strong>;苗凯文&lt;/strong>;、&lt;strong>;阿曼达·米斯萨拉&lt;/strong>;、&lt;strong>;希林·蒙塔泽里&lt;/strong>;、&lt;strong>;拉米斯Movassagh&lt;/strong>;、&lt;strong>;Wojciech Mruczkiewicz&lt;/strong>;、&lt;strong>;Ani Nersisyan&lt;/strong>;、&lt;strong>;迈克尔·纽曼&lt;/strong>;、&lt;strong>;Jiun How Ng&lt;/strong>;、&lt;strong>;安东尼·阮&lt;/strong>;、&lt;strong>;默里·阮&lt;/strong>;、&lt;strong>;M. Niu&lt;/strong>;、&lt;strong>;托马斯·奥布莱恩&lt;/strong>;、&lt;strong>;Seun Omonije&lt;/strong>;、&lt;strong>;亚历克斯·奥普莱姆卡&lt;/strong>;、&lt;strong>;丽贝卡·波特&lt;/strong>;、&lt;strong>; >;列昂尼德·普里亚德科&lt;/strong>;、&lt;strong>;克里斯·昆塔纳&lt;/strong>;、&lt;strong>;大卫·罗兹&lt;/strong>;、&lt;strong>;查尔斯·罗克&lt;/strong>;、&lt;strong>;N.鲁宾&lt;/strong>;、&lt;strong>;Negar Saei&lt;/strong>;、&lt;strong>;丹尼尔·桑克&lt;/strong>;、&lt;strong>;卡南·桑卡拉戈马蒂&lt;/strong>;、&lt;strong>;凯文·萨辛格&lt;/strong>;、&lt;strong>;亨利舒尔克斯&lt;/strong>;、&lt;strong>;克里斯托弗·舒斯特&lt;/strong>;、&lt;strong>;迈克尔·谢恩&lt;/strong>;、&lt;strong>;亚伦·肖特&lt;/strong>;、&lt;strong>;诺亚·舒蒂&lt;/strong>;、&lt;strong>;弗拉基米尔什瓦茨&lt;/strong>;、&lt;strong>;弗拉基米尔·西瓦克&lt;/strong>;、&lt;strong>;金德拉·斯克鲁兹尼&lt;/strong>;、&lt;strong>;克拉克·史密斯&lt;/strong>;、&lt;strong>;罗兰多·索玛&lt;/strong>;、&lt;strong>;乔治斯特林、&lt;strong>;道格·斯特兰&lt;/strong>;、&lt;strong>;马可·萨莱&lt;/strong>;、&lt;strong>;道格拉斯·托尔&lt;/strong>;、&lt;strong>;阿尔弗雷多·托雷斯&lt;/strong>;、&lt;strong>;吉弗雷维达尔，&lt;strong>;本杰明·维拉隆加&lt;/strong>;，&lt;strong>;凯瑟琳·沃尔格拉夫·海德韦勒&lt;/strong>;，&lt;strong>;西奥多·怀特&lt;/strong>;，&lt;strong>;布莱恩·吴&lt;/strong>;，&lt;strong>;程星&lt;/strong>;、&lt;strong>;杰米·姚&lt;/strong>;、&lt;strong>;叶平&lt;/strong>;、&lt;strong>;刘周焕&lt;/strong>;、&lt;strong>;格雷森·杨&lt;/strong>;、&lt;strong>; Adam Zalcman&lt;/strong>;、&lt;strong>;张亚星&lt;/strong>;、&lt;strong>;朱宁峰&lt;/strong>;、&lt;strong>;尼古拉斯·佐布里斯特&lt;/strong>;、&lt;strong>;Hartmut Neven&lt;/strong>;、&lt;strong>;瑞安·巴布什&lt;/strong>;、&lt;strong>;戴夫·培根&lt;/strong>;、&lt;strong>;塞尔吉奥·博伊索&lt;/strong>;、&lt;strong>;杰里米·希尔顿&lt;/strong>;、&lt;strong>;埃里克·卢塞罗&lt;/strong>;、&lt;strong>;安东尼·梅格兰特、&lt;strong>;朱利安·凯利&lt;/strong>;、&lt;strong>;陈宇&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>;、Vedika Khemani、Sarang Gopalakrishnan、&lt;strong>;托马兹·普罗森&lt;/strong>; strong>;, &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;W50 会议：多体物理的量子模拟&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/ pdf/2306.09333.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W50.13&quot;>;快速多极量子计算机上的方法&lt;/a>; &lt;br />; 演讲者：Kianna Wan &lt;br />; 作者：Kianna Wan、Dominic W Berry、&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;W50 会议：量子多体物理模拟&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期五&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y43.1&quot;>;量子计算产业和保护国家安全：什么工具会起作用吗？&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Kate Weber&lt;/strong>; &lt;br />; 作者：&lt;strong>;Kate Weber&lt;/strong>; &lt;br />; &lt;em>;第 Y43 场会议：行业、创新与国家安全：找到适当的平衡&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y46.3&quot;>;新颖的收费效果Fluxium 量子比特&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Agustin Di Paolo&lt;/strong>; &lt;br />; 作者：&lt;strong>;Agustin Di Paolo&lt;/strong>;、Kyle Serniak、Andrew J Kerman、&lt;strong>; >;William D Oliver&lt;/strong>; &lt;br />; &lt;em>;Y46 会议：基于 Fluxium 的超导 Quibits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting /MAR24/Session/Z46.3&quot;>;超导电路参数相互作用的微波工程&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Ofer Naaman&lt;/strong>; &lt;br />; 作者：&lt;strong>;Ofer Naaman&lt;/a>; &lt;br />; strong>; &lt;br />; &lt;em>;会议 Z46：宽带参量放大器和循环器&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Z62 .3&quot;>;使用核多项式方法的大磁性晶胞的线性自旋波理论&lt;/a>; &lt;br />;演讲者：Harry Lane&lt;br />;作者：Harry Lane，Hao Zhang，David A Dahlbom，Sam Quinn，&lt; strong>;Rolando D Somma&lt;/strong>;、Martin P Mourigal、Cristian D Batista、Kipton Barros &lt;br />; &lt;em>;会议 Z62：合作现象、理论&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;b>;*&lt;/ b>;&lt;/sup>;在 Google 期间完成的工作&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2754526782497247497/comments/default&quot; rel=&quot;replies &quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html#comment-form &quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;编辑“ type =“application/atom+xml”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497”rel =“self”type =“application/atom+xml” &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html&quot; rel=&quot;alternate&quot; title=&quot;Google at APS 2024&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixld OZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1695264277638670894&lt;/id>;&lt;发布>;2024-02-22T12:05:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-23T10 :07:08.500-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器感知&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPrism：用于视频理解的基础视觉编码器&lt;/stitle>;&lt;content type=&quot; html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院高级研究科学家 Long Zhao 和高级软件工程师 Ting Liu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mag3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-g JSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 网络上有数量惊人的视频，涵盖了从人们分享的日常生活瞬间到历史时刻再到科学观察的各种内容，每个视频都包含了对世界的独特记录。正确的工具可以帮助研究人员分析这些视频，改变我们理解周围世界的方式。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 视频提供比静态图像丰富得多的动态视觉内容，捕捉实体之间的运动、变化和动态关系。分析这种复杂性以及公开视频数据的巨大多样性，需要超越传统图像理解的模型。因此，许多在视频理解方面表现最好的方法仍然依赖于为特定任务量身定制的专门模型。最近，使用视频基础模型 (ViFM) 在该领域取得了令人兴奋的进展，例如 &lt;a href=&quot;https://arxiv.org/abs/2109.14084&quot;>;VideoCLIP&lt;/a>;、&lt;a href=&quot;https ://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>; 和 &lt;a href=&quot;https: //arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;。然而，构建一个能够处理视频数据多样性的 ViFM 仍然是一个挑战。 &lt;/p>; &lt;p>; 为了构建通用视频理解的单一模型，我们引入了“&lt;a href=&quot;https://arxiv.org/abs/2402.13217&quot;>;VideoPrism：基础视觉编码器视频理解&lt;/a>;”。 VideoPrism 是一种 ViFM，旨在处理广泛的视频理解任务，包括分类、本地化、检索、字幕和问答 (QA)。我们在预训练数据和建模策略方面提出了创新。我们在海量且多样化的数据集上对 VideoPrism 进行预训练：3600 万个高质量视频文本对和 5.82 亿个带有噪声或机器生成的并行文本的视频剪辑。我们的预训练方法是针对这种混合数据而设计的，可以从视频文本对和视频本身中学习。 VideoPrism 非常容易适应新的视频理解挑战，并使用单个冻结模型实现最先进的性能。 &lt;/p>;&lt;p>;&lt;/p>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao /videoprism-blog/raw/main/teaser.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr -caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 是一种通用视频编码器，通过从单个冻结模型生成视频表示，可以在广泛的视频理解任务（包括分类、本地化、检索、字幕和问答）中获得最先进的结果。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;预训练数据&lt;/h2>; &lt;p>;强大的 ViFM 需要大量视频来进行训练 - 与其他视频类似基础模型 (FM)，例如大型语言模型 (LLM)。理想情况下，我们希望预训练数据能够成为世界上所有视频的代表性样本。虽然大多数视频自然没有完美的字幕或描述，但即使不完美的文本也可以提供有关视频语义内容的有用信息。 &lt;/p>; &lt;p>; 为了给我们的模型提供最好的起点，我们整理了一个由多个公共和私人数据集组成的庞大预训练语料库，包括 &lt;a href=&quot;https://rowanzellers.com/merlot/ &quot;>;YT-Temporal-180M&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2307.06942&quot;>;InternVid&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs /2204.00679&quot;>;VideoCC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2007.14937&quot;>;WTS-70M&lt;/a>;等。其中包括3600万个精心挑选的带有高质量字幕的视频，以及额外的 5.82 亿个剪辑，其中包含不同程度的嘈杂文本（例如自动生成的文字记录）。据我们所知，这是同类中最大、最多样化的视频训练语料库。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7 PE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s1999/image18 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;779&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZec sO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;视频文本预训练数据统计。 &lt;a href=&quot;https://arxiv.org/abs/2104.14806&quot;>;CLIP 相似度得分&lt;/a>;的巨大变化（越高越好）证明了我们预训练的多样化字幕质量数据，它是用于获取文本的各种方式的副产品。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;两阶段训练&lt;/h2>; &lt;p>; VideoPrism 模型架构源自标准&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;视觉转换器&lt;/a>; (ViT)，采用分解设计，可按&lt;a href>;顺序编码空间和时间信息=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>;。我们的训练方法利用了高质量的视频文本数据和上述带有噪声文本的视频数据。首先，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning&quot;>;对比学习&lt;/a>;（一种最小化正视频文本对之间距离的方法）同时最大化负视频-文本对之间的距离）来教我们的模型将视频与其自己的文本描述（包括不完美的文本描述）进行匹配。这为将语义语言内容与视觉内容匹配奠定了基础。 &lt;/p>; &lt;p>; 经过视频-文本对比训练后，我们利用没有文本描述的视频集合。在这里，我们基于&lt;a href=&quot;https://arxiv.org/abs/2212.04500&quot;>;屏蔽视频建模框架&lt;/a>;来预测视频中的屏蔽补丁，并进行了一些改进。我们训练模型来预测第一阶段模型的视频级全局嵌入和令牌明智嵌入，以有效利用该阶段获得的知识。然后，我们随机打乱预测的标记，以防止模型学习捷径。 &lt;/p>; &lt;p>; VideoPrism 设置的独特之处在于我们使用两个互补的预训练信号：文本描述和视频中的视觉内容。文本描述通常关注事物的外观，而视频内容则提供有关运动和视觉动态的信息。这使得 VideoPrism 能够在需要了解外观和运动的任务中表现出色。 &lt;/p>; &lt;br />; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们对 VideoPrism 进行了广泛的评估，涵盖四大类视频理解任务，包括视频分类和本地化、视频文本检索、视频字幕、问答，以及科学的视频理解。 VideoPrism 在 33 个视频理解基准测试中的 30 个上实现了最先进的性能 - 所有这些都对单个冻结模型进行了最小程度的调整。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1 -TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999 /image20.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot; 1959&quot; 高度 = &quot;640&quot; src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNY elYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png&quot; 宽度 = &quot;628 &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 与之前性能最佳的 FM 相比。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />; &lt;/div>; &lt;h3>;分类和本地化&lt;/h3>; &lt;p>; 我们在涵盖分类和本地化任务的现有大规模视频理解基准 (&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;VideoGLUE&lt;/a>;) 上评估 VideoPrism。我们发现 (1) VideoPrism 优于所有其他最先进的 FM，并且 (2) 没有其他单一模型始终位居第二。这告诉我们，VideoPrism 已经学会了如何有效地将各种视频信号打包到一个编码器中（从不同粒度的语义到外观和运动提示），并且它可以在各种视频源上正常工作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4Lr LvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s1816/image12.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1816&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzm GuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;VideoPrism 优于最先进的方法（包括 &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;、&lt;a href=&quot; https://arxiv.org/abs/2104.11178&quot;>;VATT&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>; 和 &lt;a href=&quot;https ://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;）在&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;视频理解基准&lt;/a>;上。在此图中，我们显示了与之前最佳模型相比的绝对分数差异，以突出 VideoPrism 的相对改进。关于&lt;a href=&quot;http://vuchallenge.org/charades.html&quot;>;猜谜游戏&lt;/a>;、&lt;a href=&quot;http://activity-net.org/&quot;>;ActivityNet&lt;/a>;、&lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA&lt;/a>; 和 &lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA-K&lt;/a>; ，我们使用平均精度（mAP）作为评估指标。在其他数据集上，我们报告 top-1 准确率。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;与法学硕士结合&lt;/h3>; &lt;p>;我们进一步探索将 VideoPrism 与法学硕士结合起来，以释放其处理各种视频语言任务的能力。特别是，当与文本编码器（遵循 &lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;>;LiT&lt;/a>;）或语言解码器（例如 &lt;a href=&quot;https:/ /arxiv.org/abs/2305.10403&quot;>;PaLM-2&lt;/a>;），VideoPrism 可用于视频文本检索、视频字幕和视频 QA 任务。我们在一组广泛且具有挑战性的视觉语言基准上对组合模型进行了比较。 VideoPrism 在大多数基准测试中树立了新的技术水平。从视觉结果中，我们发现VideoPrism能够理解视频中的复杂运动和外观（例如，在下面的视觉示例中，模型可以识别窗口上旋转物体的不同颜色）。这些结果表明VideoPrism与语言模型具有很强的兼容性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx8 0PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1028&quot; height=&quot;580&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42Rz xZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/w640-h580/VideoPrismResults.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 与最先进的方法（包括 &lt;a href=&quot;https:// arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>; 和 &lt;a href=&quot;https://arxiv. org/abs/2204.14198&quot;>;Flamingo&lt;/a>;）在多个视频文本检索（顶部）以及视频字幕和视频质量检查（底部）基准上。我们还显示了与之前最佳模型相比的绝对分数差异，以突出 VideoPrism 的相对改进。我们在 &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video- 上报告了 Recall@1 and-language/&quot;>;MASRVTT&lt;/a>;、&lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX&lt;/a>; 和 &lt;a href=&quot; https://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;>;ActivityNet&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr 得分&lt;/a>; &lt; a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/&quot;>;MSRVTT- Cap&lt;/a>;、&lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX-Cap&lt;/a>; 和 &lt;a href=&quot;http:// youcook2.eecs.umich.edu/&quot;>;YouCook2&lt;/a>;，&lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSRVTT-QA&lt;/a>; 准确率排名第一和&lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSVD-QA&lt;/a>;，以及&lt;a href=&quot;https://arxiv.org/abs/cmp-lg /9406033&quot;>;&lt;a href=&quot;https://doc-doc.github.io/docs/nextqa.html&quot;>;NExT-QA&lt;/a>; 上的 WUPS 索引&lt;/a>;。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://github.com /garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;宽度=&quot;100%&quot;>; &lt;源src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/spin_roller_skating.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>; &lt;/视频>; &lt;视频自动播放=“”循环=“”静音=“”playsinline=“”宽度=“100％”>; &lt;source src =“https://github.com/garyzhao/videoprism-blog/raw/main/making_ice_cream_ski_lifting.mp4 &quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用 VideoPrism 和用于视频文本检索的文本编码器显示定性结果 (第一行）并适用于视频 QA 的语言解码器（第二行和第三行）。对于视频文本检索示例，蓝色条表示视频和文本查询之间的嵌入相似性。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;科学应用&lt;/h3>; &lt;p>; 最后，我们在使用的数据集上测试 VideoPrism跨领域的科学家，包括行为学、行为神经科学和生态学等领域。这些数据集通常需要领域专业知识来注释，为此我们利用社区开源的现有科学数据集，包括 &lt;a href=&quot;https://data. caltech.edu/records/zrznw-w7386&quot;>;飞行与飞行&lt;/a>;，&lt;a href=&quot;https://data.caltech.edu/records/s0vdx-0k302&quot;>;CalMS21&lt;/a>;，&lt;a href=&quot;https://shirleymaxx.github.io/ChimpACT/&quot;>;ChimpACT&lt;/a>; 和 &lt;a href=&quot;https://dirtmaxim.github.io/kabr/&quot;>;KABR&lt;/a>;。 VideoPrism 不仅表现出色，而且实际上超越了专门为这些任务设计的模型。这表明像 VideoPrism 这样的工具有潜力改变科学家分析不同领域视频数据的方式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkya Zv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1 -s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200 “高度=“397”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE 5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 在各种科学基准上均优于领域专家。我们显示绝对分数差异以突出 VideoPrism 的相对改进。我们报告了所有数据集的平均精度 (mAP)，但 KABR 除外，它使用类平均 top-1 精度。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论&lt; /h2>; &lt;p>; 通过 VideoPrism，我们推出了一款功能强大且多功能的视频编码器，为通用视频理解树立了新标准。我们对构建庞大且多样化的预训练数据集和创新建模技术的重视已经通过我们的广泛评估得到了验证。 VideoPrism 不仅始终优于强大的基线，而且其独特的泛化能力使其能够很好地处理一系列现实世界的应用程序。由于其潜在的广泛用途，我们致力于在我们的&lt;a href=&quot;http://ai.google/principles&quot;>;人工智能原则&lt;/a>;的指导下，继续在这一领域进行进一步负责任的研究。我们希望 VideoPrism 为人工智能和视频分析交叉领域的未来突破铺平道路，帮助实现 ViFM 在科学发现、教育和医疗保健等领域的潜力。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本博文代表所有 VideoPrism 作者撰写：Long Zhao、Nitesh B. Gundavarapu、Liangzhe Yuan、Hao Zhou、Shen严、詹妮弗·J·孙、卢克·弗里德曼、钱睿、托拜厄斯·韦安德、赵越、雷切尔·霍农、弗洛里安·施罗夫、杨明轩、大卫·A·罗斯、王惠生、哈特维格·亚当、米哈伊尔·西罗滕科、刘婷和龚博清。我们衷心感谢 David Hendon 的产品管理工作，以及 Alex Siegman、Ramya Ganeshan 和 Victor Gomes 的计划和资源管理工作。我们还要感谢 Hassan Akbari、Sherry Ben、Yoni Ben-Meshulam、Chun-Te Chu、Sam Clearwater、Yin Cui、Ilya Figotin、Anja Hauth、Sergey Ioffe、Xuhui Jia、Yeqing Li、Lu Jiang、Zu Kim、Dan Kondratyuk、Bill Mark、Arsha Nagrani、Caroline Pantofaru、Sushant Prakash、Cordelia Schmid、Bryan Seybold、Mojtaba Seyedhosseini、Amanda Sadler、Rif A. Saurous、Rachel Stigler、Paul Voigtlaender、Pingmei Xu、Chachao Yan、Xuan Yang 和 Yukun Zhu 参与讨论，支持和反馈对这项工作做出了巨大贡献。我们感谢 Jay Yagnik、Rahul Sukthankar 和 Tomas Izo 对这个项目的热情支持。最后，我们感谢 Tom Small、Jennifer J. Sun、Hao Zhou、Nitesh B. Gundavarapu、Luke Friedman 和 Mikhail Sirotenko 对撰写本文提供的巨大帮助。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p >;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1695264277638670894/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/02/videoprism-foundational-visual-encoder.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPrism：用于视频理解的基础视觉编码器&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt; /name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http:// /schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>; &lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1 sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif&quot; width=&quot;72&quot; xmlns:media =&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com， 1999：blog-8474926331452026626.post-4343235509909091741&lt;/id>;&lt;发布>;2024-02-21T12:15:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-21T12:15:36.694-08:00 &lt; /updated>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“差异隐私”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;Gboard&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;category schema =&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;生产设备上语言模型的私人培训取得进展&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：谷歌研究科学家郑旭和软件工程师张彦翔&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl 1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 经过训练来预测给定输入文本的下一个单词的语言模型 (LM) 是许多应用程序的关键技术 [&lt;a href=&quot;https://blog.google/technology/ai/google-palm-2- ai-large-language-model/&quot;>;1&lt;/a>;、&lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;2&lt;/a>;]。在 &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;Gboard&lt;/a>; 中，LM用于通过支持诸如&lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;下一个单词预测&lt;/a>; (NWP)、&lt;a href=&quot;https:// support.google.com/gboard/answer/7068415&quot;>;智能撰写&lt;/a>;、&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;智能完成&lt;/a>;和&lt; a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;建议&lt;/a>;、&lt;a href=&quot;https://support.google.com/gboard/answer/2811346&quot;>;幻灯片输入&lt;/a>;&lt;span style=&quot;text-decoration: underline;&quot;>;、&lt;/span>;并&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;校对&lt;/一个>;。在用户设备而不是企业服务器上部署模型具有较低延迟和更好的模型使用隐私等优点。直接根据用户数据训练设备上模型可以有效提高 NWP 和 &lt;a href=&quot;https://blog.research.google/2021/11/predicting-text-selections-with.html&quot; 等应用程序的实用性能>;智能文本选择&lt;/a>;，保护用户数据的隐私对于模型训练很重要。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49 -Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/图像45.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “1600”数据原始宽度=“1996”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP- ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;由设备上语言模型提供支持的 Gboard 功能。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在本博客中，我们讨论了自 &lt;a href=&quot;https:// 的概念验证开发以来，多年来的研究进展如何为 Gboard LM 的私人培训提供动力。 blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;2017 年联邦学习&lt;/a>;（佛罗里达州）和正式&lt;a href=&quot;https://blog.research.google/2022/02 /federated-learning-with-formal.html&quot;>;2022 年的差异化隐私&lt;/a>; (DP) 保证。&lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative。 html&quot;>;FL&lt;/a>; 使手机能够协作学习模型，同时将所有训练数据保留在设备上，而 &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;DP&lt;/a >; 提供数据匿名化的可量化衡量标准。形式上，DP 通常用 (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) 来表征，值越小表示保证越强。机器学习 (ML) 模型被认为对 ε=10 具有&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;合理的 DP 保证，并且当 &lt;em>;δ&lt;/em>; 很小时，强 DP 保证 ε=1&lt;/a>;。 &lt;/p>; &lt;p>; 截至今天，Gboard 中的所有 NWP 神经网络 LM 均通过 FL 进行训练，并提供正式的 DP 保证，并且未来推出的所有基于用户数据训练的 Gboard LM 都需要 DP。这 30 多个 Gboard 设备上 LM 以 7 种以上语言和 15 多个国家/地区推出，并满足小 &lt;em>;δ&lt; 的 (&lt;em>;ɛ&lt;/em>;, &lt;em>;δ&lt;/em>;)-DP 保证/em>; 为 10&lt;sup>;-10&lt;/sup>;，ɛ 介于 0.994 和 13.69 之间。据我们所知，这是 Google 或其他任何地方在生产环境中已知的最大的用户级 DP 部署，并且首次实现了 &lt;em>;ɛ&lt;/em>; &lt;em>;ɛ&lt;/em>; &lt;em>; 的强大 DP 保证。 1 已公布，用于直接根据用户数据训练的模型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Gboard 中的隐私原则和实践&lt;/h2>; &lt;p>; 在“&lt;a href=&quot;https”中://arxiv.org/abs/2306.14793&quot;>;Gboard 中的私有联合学习&lt;/a>;”，我们讨论了不同之处&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; >;隐私原则&lt;/a>;目前反映在生产模型中，包括：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;透明度和用户控制&lt;/em>;：我们披露使用哪些数据及其目的用途、在各个渠道中的处理方式以及 Gboard 用户如何轻松&lt;a href=&quot;https://support.google.com/gboard/answer/12373137&quot;>;配置&lt;/a>;学习中的数据使用情况楷模。 &lt;/li>;&lt;li>;&lt;em>;数据最小化&lt;/em>;：FL 立即聚合仅改进特定模型的重点更新。 &lt;a href=&quot;https://eprint.iacr.org/2017/281.pdf&quot;>;安全聚合&lt;/a>; (SecAgg) 是一种加密方法，可进一步保证只能访问临时更新的聚合结果。 &lt;/li>;&lt;li>;&lt;em>;数据匿名化&lt;/em>;：服务器应用DP来防止模型记住单个用户训练数据中的唯一信息。 &lt;/li>;&lt;li>;&lt;em>;可审计性和可验证性&lt;/em>;：我们在开源代码中公开了关键算法方法和隐私核算（&lt;a href=&quot;https://github.com/tensorflow/ federated/blob/main/tensorflow_federated/python/aggregators/ Differential_privacy.py&quot;>;TFF聚合器&lt;/a>;，&lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/dp_query /tree_aggregation_query.py&quot;>;TFP DPQuery&lt;/a>;、&lt;a href=&quot;https://github.com/google-research/federated/blob/master/dp_ftrl/blogpost_supplemental_privacy_accounting.ipynb&quot;>;DP 会计&lt;/a>;、和&lt;a href=&quot;https://github.com/google/federated-compute&quot;>;FL 系统&lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;简史&lt;/h3>; &lt;p>; 近年来，FL 已成为根据用户数据训练 &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;Gboard 设备上 LM&lt;/a>; 的默认方法。 2020 年，&lt;a href=&quot;https://arxiv.org/abs/1710.06963&quot;>;对模型更新进行剪辑并添加噪音&lt;/a>;的 DP 机制被用于&lt;a href=&quot;https://arxiv. org/abs/2009.10031&quot;>;防止在西班牙训练西班牙 LM 时的记忆&lt;/a>;，满足有限 DP 保证 (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml- models- Differentially-private.html&quot;>;“&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;如何 DP-fy ML“&lt;/a>; 指南中描述的第 3 层&lt;/a>;）。 2022 年，在 &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-Follow-The-Regularized-Leader (DP-FTRL) 算法&lt;/a>;的帮助下，西班牙 LM 成为第一个直接在用户数据上训练的生产神经网络宣布&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;正式的 DP 保证 (ε= 8.9, δ=10&lt;sup>;-10&lt;/sup>;)-DP&lt;/a>;（相当于报告的&lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated- Learning-with-formal.html&quot;>;ρ=0.81&lt;/a>;&lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1605.02065&quot;>;零集中差异隐私&lt;/a>;） ，因此满足&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;合理的隐私保证&lt;/a>; (&lt;a href=&quot;https ://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;第 2 层&lt;/a>;）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;联邦学习中默认的差异隐私&lt;/h2>; &lt;p>; In “&lt;a href=&quot; https://arxiv.org/abs/2305.18465&quot;>;具有差异隐私的 Gboard 语言模型的联邦学习&lt;/a>;”，我们宣布 Gboard 中的所有 NWP 神经网络 LM 都具有 DP 保证，并且未来推出的所有 Gboard LM根据用户数据进行训练需要 DP 保证。通过应用以下实践在 FL 中启用 DP：&lt;/p>; &lt;ul>; &lt;li>;使用&lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;>;多语言&lt;/a预训练模型>; &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;C4&lt;/a>; 数据集。 &lt;/li>;&lt;li>;通过对公共数据集的模拟实验，找到具有高实用性的大DP信噪比。增加参与一轮模型更新的客户端数量可以提高隐私性，同时保持噪声比固定以获得良好的效用，直到满足 DP 目标或系统允许的最大值和人口规模。 &lt;/li>;&lt;li>;根据&lt;a href=&quot;https://arxiv.org/abs/1902.01046中的计算预算和估计人口配置参数以限制每个客户端可以贡献的频率（例如，每隔几天一次） &quot;>;FL 系统&lt;/a>;。 &lt;/li>;&lt;li>;运行&lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;训练，并限制通过&lt;a href选择的每台设备更新的幅度=&quot;https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e2b3a90e103368b6b6&quot;>;自适应裁剪&lt;/a>;，或根据经验进行修复。 &lt;/li>; &lt;/ul>; &lt;p>; SecAgg 还可以通过采用&lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for.html&quot;>;先进技术来应用改善尺度和灵敏度的计算和通信&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gx fKfT4aEv-_2MULO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N /s1600/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mULO5z laWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;具有差异隐私和 (SecAgg) 的联邦学习。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;报告 DP 保证&lt;/h3>; &lt;p>; 已启动的 Gboard NWP LM 的 DP 保证在下面的条形图中可视化。 &lt;em>;x&lt;/em>; 轴显示按语言区域标记并在相应群体上进行训练的 LM； &lt;em>;y&lt;/em>; 轴显示当 &lt;em>;δ&lt;/em>; 固定为较小值 10&lt;sup>;-10&lt;/sup>; 时 &lt;em>;ε&lt;/em>; 值 &lt; a href=&quot;https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf&quot;>;(ε, δ)-DP&lt;/a>;（越低越好）。根据 A/B 测试期间的用户交互指标进行测量，这些模型的实用性要么显着优于之前生产中的非神经模型，要么与之前没有 DP 的 LM 相当。例如，通过应用最佳实践，西班牙模式的DP保证从&lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal .html&quot;>;ε=8.9&lt;/a>;&lt;/em>; 至 &lt;em>;ε&lt;/em>;=5.37。 SecAgg 还用于在西班牙训练西班牙语模型和在美国训练英语模型。有关 DP 保证的更多详细信息，请参阅&lt;a href=&quot;https://blog.research.google/ 后的&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;附录&lt;/a>; 2023/05/making-ml-models- Differentially-private.html&quot;>;“&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;如何 DP-fy ML&lt;”中概述的指南&lt;/a>; /a>;”。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实现更强大的 DP 保证&lt;/h2>; &lt;p>; &lt;em>;ε&lt;/em>;~许多已推出的 LM 的 10 DP 保证对于 ML 模型来说已经被认为是&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;合理&lt;/a>;在实践中，Gboard 中的 DP FL 的旅程仍在继续，以改善用户的打字体验，同时保护数据隐私。我们很高兴地宣布，巴西的葡萄牙语和拉丁美洲的西班牙语生产语言模型首次在 DP 保证 &lt;em>;ε&lt;/em>; ≤ 1 的情况下进行培训和启动，满足 &lt;a href=&quot; https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;第 1 层强大的隐私保证&lt;/a>;。具体来说，(&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP 保证是通过运行高级 &lt;a href=&quot;https ://arxiv.org/abs/2306.08153&quot;>;矩阵分解 DP-FTRL&lt;/a>; (MF-DP-FTRL) 算法，每轮服务器模型更新训练都有超过 12,000 台设备参与，规模大于&lt;a href= “https://arxiv.org/abs/2305.18465&quot;>;6500+台设备的通用设置&lt;/a>;，以及精心配置的策略，限制每个客户在 14 天内最多参加 2000 轮训练的两次巴西庞大的葡萄牙语用户群体。使用类似的设置，es-US 西班牙语 LM 在拉丁美洲多个国家/地区的大量人群中进行了训练，以实现 (&lt;em>;ε&lt;/em>;=0.994，&lt;em>;δ&lt;/em>;=10&lt;sup>; -10&lt;/sup>;)-DP。 &lt;em>;ε&lt;/em>; ≤ 1 es-US 模型显着提高了许多国家的实用性，并在哥伦比亚、厄瓜多尔、危地马拉、墨西哥和委内瑞拉推出。对于人口较少的西班牙，es-ES LM 的 DP 保证从 &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;ε=5.37&lt;/a>;&lt;/em>; 提高到 &lt;em>;ε&lt;/em>;=3.42，只需将 &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; 替换为 &lt;a href=&quot;https://arxiv .org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>;，无需增加每轮参与的设备数量。为了保护隐私，&lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/federated/blob/master/mf_dpftrl_matrices/privacy_accounting.ipynb&quot;>;colab&lt;/a>; 中披露了更多技术细节会计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_A HnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;709&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA- M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Gboard NWP LM 的 DP 保证（紫色条代表 ε=8.9 的首次 es-ES 启动；青色条代表使用 &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>; 训练的模型的隐私改进； &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;层级&lt;/a>;来自“&lt;a href=&quot;https://arxiv .org/abs/2303.00654&quot;>;如何实现 DP-fy ML&lt;/a>;“指南； en-US* 和 es-ES* 还使用 SecAgg 进行训练。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;讨论和后续步骤&lt;/h2>; &lt;p>;我们的经验表明，DP可以在实践中通过客户参与的系统算法协同设计来实现，并且当人群参与时，隐私性和实用性都可以很强。大量&lt;em>;和&lt;/em>;设备的贡献被汇总。隐私-实用性-计算权衡可以通过&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;使用公共数据&lt;/a>;来改善，&lt;a href=&quot;https://arxiv. org/abs/2306.08153&quot;>;新的 MF-DP-FTRL 算法&lt;/a>;，&lt;a href=&quot;https://github.com/google/differential-privacy&quot;>;并加强核算&lt;/a>;。通过这些技术，&lt;em>;ε&lt;/em>; ≤ 1 的强大 DP 保证是可能的，但仍然具有挑战性。实证隐私审计的积极研究 [&lt;a href=&quot;https://arxiv.org/abs/2302.03098&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2305.08846&quot;>;2 &lt;/a>;]表明 DP 模型可能比最坏情况 DP 保证所暗示的更加私密。在我们不断推动算法前沿的同时，隐私-效用-计算的哪个维度应该优先考虑？ &lt;/p>; &lt;p>; 我们正在积极致力于 ML 的所有隐私方面的工作，包括将 DP-FTRL 扩展到 &lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for .html&quot;>;分布式DP&lt;/a>;并提高&lt;a href=&quot;https://arxiv.org/abs/2306.14793&quot;>;可审计性和可验证性&lt;/a>;。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;可信执行环境&lt;/a>;为大幅增加模型大小和可验证隐私提供了机会。最近&lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;大型 LM 的突破&lt;/a>; (LLM) 激励我们&lt;a href=&quot;https:// arxiv.org/abs/2305.12132&quot;>;重新思考&lt;/a>;在私人培训中使用&lt;a href=&quot;https://arxiv.org/abs/2212.06470&quot;>;公共&lt;/a>;信息以及法学硕士之间未来更多的互动、设备上 LM 和 Gboard 制作。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;作者衷心感谢 Peter Kairouz、Brendan McMahan 和 Daniel Ramage 对博客文章本身的早期反馈，Shafeng Li 和 Tom Small 对动画人物的帮助，以及 Google 团队对算法设计、基础设施实施和生产维护的帮助。以下合作者直接对所呈现的结果做出了贡献：&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;研究和算法开发：Galen Andrew、Stanislav Chiknavaryan、Christopher A. Choquette-Choo、Arun Ganesh、Peter Kairouz、Ryan McKenna 、H. Brendan McMahan、Jesse Rosenstock、Timon Van Overveldt、Keith Rush、Shuang Song、Thomas Steinke、Abhradeep Guha Thakurta、Om Thakkar 和 Yuanbo Zhang。&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;基础设施、生产和领导支持：Mingqing Chen、Stefan Dierauf、Billy Dou、Hubert Eichner、Zachary Garrett、Jeremy Gillula、Jianpeng Hou、Hui Li、Xu Liu、Wenzhi Mao、Brett McLarnon、Mengchen Pei、Daniel Ramage、Swaroop Ramaswamy、Hai Cheng Sun、Andreas Terzis、王云、吴珊珊、肖宇和斋淑敏。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4343235509909091741/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 4343235509909091741&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;self&quot; type= &quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;进展设备上生产语言模型的私人培训” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri >;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1. blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6 MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025&lt;/id>;&lt;发布>;2024-02- 14T10:32:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-14T10:32:25.557-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;监督学习&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;了解概念漂移下训练数据的重要性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：博士前研究员 Nishant Jain 和研究科学家 Pradeep Shenoy , Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmq QFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s1600/temporalreweightinghero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 我们周围世界不断变化的性质对人工智能模型的开发提出了重大挑战。通常，模型是根据纵向数据进行训练的，希望所使用的训练数据能够准确地表示模型将来可能收到的输入。更一般地说，所有训练数据都同等相关的默认假设在实践中经常被打破。例如，下图显示了来自 &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; 非平稳学习基准的图像，它说明了对象的视觉特征如何在 10 年内显着演变。年跨度（我们称之为&lt;em>;缓慢概念漂移&lt;/em>;的现象），对对象分类模型提出了挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw -_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “662”数据原始宽度=“1999”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF- Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png&quot; />;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;来自 CLEAR 基准测试的示例图像。 （改编自 Lin 等人&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;.&lt;/a>;）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt; p>;替代方法，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/online_machine_learning&quot;>;在线&lt;/a>;和&lt;a href =“ https：///wiki.continualai.org/the-wiki.org/the--the------------------持续的wiki/介绍到 - 访问学习”>;连续学习&lt;/a>;，反复更新一个具有少量最近数据的模型，以使其保持最新。这隐含优先考虑最近的数据，因为从过去的数据中学习的学习是通过后续更新逐渐消除的。但是，在现实世界中，不同类型的信息以不同的速度失去相关性，因此有两个关键问题：1）通过设计，它们专注于最新数据，并丢失了来自较旧数据的任何信号被删除。 2）随着时间的推移，数据实例衰减的贡献均匀&lt;em>;均匀&lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>;不论数据内容。 &lt;/p>; &lt;p>;在我们最近的工作中，“ &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>; instance-conditional decay of非遗产学习&lt;/a>;”，我们建议为每个实例分配培训期间的重要性得分，以最大程度地提高未来数据模型性能。为此，我们采用了一种辅助模型，该模型使用培训实例及其年龄来产生这些分数。该模型与主要模型共同学习。我们解决了上述挑战，并在用于非组织学习的一系列基准数据集上取得了重大收益。例如，在&lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;最近的大规模基准&lt;/a>;用于非组织的学习（在10年内〜39m照片），我们出现在15％的相对准确性通过学习的培训数据重新加权。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;概念漂移的挑战&lt;/h2>; &lt;p>; &lt;/h2>; &lt;p>;获得对慢速概念的定量见解漂流，我们在&lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;最近的照片分类任务&lt;/a>;上构建了分类器&lt;/a>;，该任务大约是从社交媒体网站上获得的大约3900万张照片。我们比较了离线培训，该培训在所有训练数据中以随机顺序进行了多次迭代，并持续培训，这些培训在每个月的数据中以顺序（时间）顺序进行了多次迭代。我们在训练期间和随后的两个模型都冻结的时期均测量了模型的准确性，即，未对新数据进行进一步更新（如下所示）。在训练期结束时（左图，X轴= 0），两种方法都看到了相同数量的数据，但显示了较大的性能差距。这是由于&lt;a href=&quot;https://www.sciencedirect.com/science/article/article/arbs/pii/S0079742108605368&quot;>;在训练序列中以不受控制的方式减少。另一方面，忘记具有其优势 - 在测试期间（右图显示），持续训练的模型的降级速度远低于离线模型，因为它依赖于较旧的数据。两种模型在测试期间的衰减都在确认数据随着时间的推移确实在不断发展，并且两个模型的相关性越来越小。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png “ ImaNeanChor =” 1“样式=” Margin-Left：auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 616” data-Original-width =“ 1554” SRC = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>;在照片分类任务上比较离线和经过训练的模型。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/try>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;br>; &lt;br>; &lt; div style =“线路高：40％;”>; &lt;br>; &lt;br>; &lt;/div>; &lt;h2>;时间敏感培训数据&lt;/h2>; &lt;p>;我们设计一种方法，结合了离线学习的好处（灵活性有效重复所有可用数据）和持续学习（淡化旧数据的能力）以解决缓慢的概念漂移。我们以离线学习为基础，然后仔细控制过去数据的影响和优化目标，均旨在减少未来的模型衰减。 &lt;/p>; &lt;p>;假设我们希望训练模型，&lt;em>; m &lt;/em>;，&lt;em>; &lt;/em>;给定一些随时间收集的训练数据。我们建议还训练一个辅助模型，该模型根据其内容和年龄为每个点分配重量。该重量在&lt;em>; m &lt;/em>;的训练目标中从该数据点缩放了贡献。权重的目的是在未来数据上提高&lt;em>; m &lt;/em>;的性能。 &lt;/p>; &lt;p>; in &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;我们的工作&lt;/a>;，我们描述了如何可以&lt;em>; meta-learned，&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/ em>; ie，以帮助学习模型&lt;em>; m &lt;/em>;本身的方式与&lt;em>; m &lt;/em>;一起学习。助手模型的关键设计选择是我们以阶级的方式分开了与年龄相关的贡献。具体而言，我们通过结合来自多个不同固定时间表的衰减时尺度的贡献来设置重量，并将给定实例的大约“分配”到其最合适的时间表上。我们在实验中发现，这种形式的助手模型的表现优于我们考虑的许多其他替代方案，从不受约束的关节函数到单个衰减的时间尺度（指数或线性），由于其简单性和表现性的结合。可以在&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>; paper &lt;/a>;中找到完整的详细信息。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;实例重量评分&lt;/h2>; &lt;p>;下面的顶部图表明，我们学到的助手模型确实上升了 - 在&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;&quot;>;清晰的对象识别挑战挑战&lt;/a>;中&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; &lt;a href=&quot;https://arxiv.org/abs/a>;中，重量更现代的对象&lt;/a>;;外观较旧的物体相应地加权。在仔细检查（下图下图，基于梯度的&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;特征重要性&lt;/a>;评估），我们看到助手模型集中在内部的主要对象上图像，而不是可能与实例年龄相关的背景特征。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 499” data-Original-width =“ 1999” src =“ src =” https ：//blogger.googleusercontent.com/img/b/r29vz2xl/avvxseggqimnpfiw7s3jet9qoxqom1KT8VIAHIHAPUSLX8VIAHPUSLX8LZCAXYBLJCAXYBLJCCHENLJCHENNLJHEWNN7LPONN7J6QTIZ9QTIZ9-QKWBJBJBJBJBJBLEMXLSQLMESQQLBLEMNMENMENMENM ZOO6HYGR5OXIMP5VO9ZUNCF1Q3BPVAU94HM9D71XWOGRQM9C8LJ6IXRB69W_JJJNEQW5JGCG_U6ZW2J/S16000/S16000/IMAGE1.PNG -caption“ style =” text-align：center;“>; &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; clear &lt;/a>;基准（cameral＆amp;计算机类别）分别由我们的助手模型分别分配了最高和最低权重。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;table align =“ center” cellpadding =“ 0” CellSpacing =“ 0 “ class =“ tr-caption-container” style =“边距 - 左：auto; margin-right：auto;”>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center; center;”>; &lt;a href = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin -right：auto;“>; &lt;img border =” 0“ data-eriginal-height =“ 339” data-eriginal-width =“ 1999” src =“ https：//blogger.googger.googleusercontent.com/img/img/img/img/b/r29vz2xxl /avvxsehikcafyxnrhjukwv3kjofmjk_v9wsplzfmyya-tzcodzdbncncncnuologogf9njygqp_twzcz-twzcz-twzcz-twzcz-p5smlhsmlhlhhvfd_jasbmsbmsbmsbmsbmsbmsbmssbmssbmss9son5aotsqunsquunsquunmsqunmsqunmzwkdckdcwkdcwk.wkcofkwkcow k. cofk k.fkcofkwk.wmsqow k. ic ofk k.fk k.fk k. k. k. k. k. k.fk k. k. k. k. k.fk k. ca0ornf_kgw8ph0uzci8ubcqdbhs0j4nq5hb5j7e/s16000/image5.png“/>; &lt;/a>; &lt;/a>; &lt;/>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;tr>; &lt;td class =“ tr-caption” tr-caption对齐：中心;“>;“>;特征对助手模型”在示例图像上的特征重要性分析，来自&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; clear &lt;/a>;基准&lt;/td>; &lt;/td>; &lt;/td>; &lt;/tr >; &lt;/tbody>; &lt;/table>; &lt;br>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;/h2>; &lt;div style =“ line-height： 40％;“>; &lt;br>; &lt;/div>; &lt;h3>;在大规模数据上获取&lt;/h3>; &lt;p>;我们首先研究大型&lt;a href =“ https://arxiv.org/abs/ 2108.09020“>;照片分类任务&lt;/a>;（PCAT）在&lt;a href=&quot;https://arxiv.org/abs/1503.01817&quot;>; yfcc100m数据集培训和接下来的五年作为测试数据。我们的方法（如下所示）在无夹式基线（黑色）以及许多其他健壮的学习技术上大大改善了。有趣的是，我们的方法故意在遥远的过去（将来不太可能重复发生培训数据）中进行准确的折衷，以换取测试期间的明显改进。同样，根据需要，我们的方法在测试期间的降解少于其他基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png&quot; imageanchor=&quot;1&quot; style =“边距 - 左：自动;边缘权利：自动;”>; &lt;img border =“ 0” data-foriginal-height =“ 600” data-eriginal-width =“ 800” src =“ https：// 。 20xp5og5yowr2SmyVQndHhasunt5IY_RM_SJANFASM4JT1PF_TCHYPHENHYPHENK8Y0MNI2JJI1ODWCSIH_7VAC7B/S16000/S16000/IMPAIM3G.PNG3G.PNG 3.png3g对齐：中心;“>;对PCAT数据集上的方法和相关基线的比较。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;div style =“ line-height：40％;” >; &lt;br>; &lt;/div>; &lt;h3>;广泛的适用性&lt;/h3>; &lt;p>;我们在广泛的非组织学习挑战挑战数据集中验证了我们的发现（请参阅&lt;a href = &lt;a href =“ https：// arxiv .org/abs/2108.09020“>; 1 &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; 2 &lt;/a>;，&lt;a href =“ /abs/2211.14238&quot;>; 3 &lt;/a>;，&lt;a href=&quot;https://proceedings.mlr.press/v206/awasthi23b/awasthi23b/awasthi23b.pdf&quot;>; 4 &lt;/a>; 4 &lt;/a>;有关详细信息） （照片，卫星图像，社交媒体文本，医疗记录，传感器读数，表格数据）和尺寸（从10k到39m的实例不等）。与每个数据集的最近发布的基准方法相比，我们报告了测试期间的显着收益（如下所示）。请注意，每个数据集的先前最著名方法可能有所不同。这些结果展示了我们方法的广泛适用性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleuserercontent.com/img/b/r29vz2xl/avvxssehw95hiflfz4ebmltddwiwi ninddwiwi ninddwi yxonjybmlt2ybmltpplyqrltplltmqrltmqurltm eybm ey1 ey1 eybm ey1 eybm ey1 ey1 IIHJUBWTMQDBTJ7TVTVTWIU0EZB4_QSEEE5U4K_Z70Y_9SSSS3IF8Y5XKMXKMXKQYII5VZATAFWC7NVVC7NVVNWMGVNWMGVNW_ERMGVNW_YL8HA6N7-GUPGGGGGGGGGGGGCJI2QTGKKKTGN2/SIMPERL ImageAnChor =“ 1”样式=“边距左：自动;缘右右：auto;”>; &lt;img border =“ 0” data-Original-height =“ 552” data-Original-width =“ 765” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= “ TR-CAPTION”样式=“ Text-Align：Center;”>;我们方法在研究自然概念漂移的各种任务上的性能增益。我们报告的收益是每个数据集的先前最著名方法。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br>; &lt;br>; &lt;div style =“ line-height：40％;”>; &lt;br >; &lt;/div>; &lt;h3>;持续学习的扩展&lt;/h3>; &lt;p>;最后，我们考虑了我们的工作的有趣扩展。上面的工作描述了如何使用受持续学习启发的想法来扩展离线学习以处理概念漂移。但是，有时离线学习是不可行的 - 例如，如果可用的培训数据数量太大而无法维护或处理。我们通过在&lt;/em>;的上下文中应用时间重新加权&lt;em>;来调整我们的方法，以直接的方式进行持续学习。该提案仍然保留了持续学习的某些局限性，例如，模型更新仅在大多数数据上执行，并且所有优化决策（包括我们的重量重量）仅在该数据上做出。然而，我们的方法始终击败定期的持续学习以及照片分类基准上的其他各种持续学习算法（见下文）。由于我们的方法与这里许多基线相比的思想是互补的，因此与它们结合使用时，我们预计会有更大的收益。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png&quot; imageanchor =“ 1” style =“边距左：auto; margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 600” data-Original-width =“ 800” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>;与最新基线相比，我们的方法的结果适用于持续学习。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;br>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们通过结合先前方法的优势 - 离线学习，以解决数据漂移的挑战它有效地重用数据，并继续学习，重点是最新数据。我们希望我们的工作有助于改善实践中概念漂移的模型鲁棒性，并在解决缓慢的概念漂移的无处不在问题方面产生越来越多的兴趣和新想法。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;我们感谢迈克·莫泽尔（Mike Mozer）在早期的早期进行了许多有趣的讨论这项工作的阶段，以及在开发过程中非常有用的建议和反馈。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/56059333333329926261025/comments/默认值“ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/learning-importance-of-raining-ogning-importance-of-training -data.html＃comment-form“ rel =” reply =“ title =“ 0 comment” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/847492626331452026262626/posts/ default/5605933033299261025&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;self&quot; type =“ application/application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/02/learning-importance-of-training-data.hta.html” rel =“替代”在概念漂移下学习培训数据的重要性“ type =” text/html“/>; &lt;unam>; &lt;名称>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri &lt;/uri >; &lt;Email>; noreply@blogger.com &lt;/email>; &lt;gd：image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https：// img1。 blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr：thr>; 0 &lt;/thr：total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999年：Blog-8474926331452026626.POST-5933333365460125094774 &lt;/id>; 11：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-02-13T14：11：49.258-08：00 &lt;/updated>; &lt;category scheme =&#39; “ term =“差异隐私”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ pancys ai”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http：http：http：http：http：http：http：http：http：http： //www.blogger.com/atom/ns#“ term =“安全与隐私”>; &lt;/category>; &lt;title type =“ text”>; dp-auditorium：一个灵活的库库，用于审核差异隐私&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由MónicaRiberoDíaz发表，研究科学家，Google Research &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img/b/b/ R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/differential_privacy&quot;>;差异隐私&lt;/a>;（DP）是限制任何个人用户信息的随机机制的属性处理和分析数据。 DP提供了一种强大的解决方案，以解决对数据保护的日益关注，启用技术&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-learning-with-formal.html&quot;>;跨越&lt;/a>; &lt;a href=&quot;https://www.apple.com/privacy/docs/differential_privacy_overview.pdf&quot;>; Industries &lt;/a>;和政府应用程序（例如程序 - 策略/年十年级/十年/2020/计划管理/process/procutsion-discluse-avidence/dindialial-privacy.html“>;美国人口普查&lt;/a>;），而不会损害个人用户身份。随着其采用率的增加，重要的是要确定具有错误实施的机制的潜在风险。研究人员最近发现了私人机制的数学证据及其实施的错误。例如，&lt;a href=&quot;https://arxiv.org/pdf/1603.01699.pdf&quot;>;研究人员比较&lt;/a>;六个稀疏矢量技术（SVT）变化，发现六个实际上只遇到了六个的私密性保证。即使数学证明是正确的，实现该机制的代码也容易受到人为错误的影响。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;但是，实际和有效的DP审核主要是由于机制的固有随机性以及测试保证的概率性质。此外，存在一系列保证类型，（例如，&lt;a href=&quot;https://dl.acm.org/doi/10.1007/11681878_14&quot;>; pure dp &lt;/a>;，&lt;a href =“ /link.springer.com/chapter/10.1007/11761679_29&quot;>; approximate dp &lt;/a>;，&lt;a href=&quot;https://arxiv.org/arxiv.org/abs/1702.07476&quot;>; =“ https://arxiv.org/pdf/1603.01887.pdf”>;集中的DP &lt;/a>;），这种多样性有助于制定审计问题的复杂性。此外，考虑到提出的机制的数量，调试数学证明和代码库是一项棘手的任务。虽然在特定的机制假设下存在&lt;em>; Ad hoc &lt;/em>;测试技术，但很少有努力为测试DP机制开发可扩展的工具。 &lt;/p>; &lt;p>;到此为止，在“ &lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>; dp-auditorium：用于审核差异隐私的大型库&lt;/a>;”，我们介绍&lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/main/main/python/dp_auditorium&quot;>;开源库库&lt;/a>;用于审计DP保证，只有Black-box访问机制（即，不知道该机制的内部属性）。 DP-Auditorium在Python中实现，并提供了一个灵活的接口，允许贡献不断提高其测试功能。我们还引入了新的测试算法，这些算法对RényiDP，Pure DP和近似DP的功能空间进行了差异优化。我们证明，DP原告可以有效地识别DP保证违规行为，并建议哪些测试最适合在各种隐私保证下检测特定的错误。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; dp保证&lt;/h2>; &lt;p>; dp机制的输出是从一个绘制的样本概率分布（&lt;em>; m &lt;/em>;（&lt;em>; d &lt;/em>;））可满足数学属性，以确保用户数据的隐私。因此，DP保证与概率分布对之间的属性密切相关。如果由数据集&lt;em>; d &lt;/em>;确定的概率分布和一个相邻的数据集&lt;em>; d&#39;&lt;/em>;，则机制是私有的是&lt;em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/computational_indistrinishability&quot;>;在给定的分歧度量下&lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/em>;。 &lt;/p>; &lt;p>;例如，经典&lt;a href=&quot;https://software.imdea.org/~federico/pubs/2013.icalp.pdf&quot;>;近似DP &lt;/a>;定义指出机制is approximately DP with parameters (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) if the &lt;a href=&quot;https://arxiv.org/pdf/1508.00335.pdf&quot;>;hockey-stick divergence &lt;/a>;在&lt;em>; m &lt;/em>;之间>;（&lt;em>; d&#39;&lt;/em>;），最多是&lt;em>;Δ&lt;/em>;。纯dp是近似dp的特殊实例，其中&lt;em>;δ= 0 &lt;/em>;。最后，一种机制被认为&lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;rényidp &lt;/a>;带有参数（&lt;em>; 𝛼 &lt;/em>;，&lt;em>; &lt;em>;ε）&lt;/em em）&lt;/em em） >;如果&lt;a href=&quot;https://en.wikipedia.org/wiki/r%C3%A9NYI_ENTROPY&quot;>;rényidivergence &lt;/a>; forder &lt;em>; 𝛼 &lt;/em>;，最多是&lt;em>; &lt;em>; &lt;em>; ε&lt;/em>;（其中&lt;em>;ε&lt;/em>;是一个小的正值）。在这三个定义中，&lt;em>;ε&lt;/em>;不是可互换的，而是直观地传达了相同的概念。 &lt;em>;ε&lt;/em>;的较大值意味着两个分布之间或更少的隐私之间的差异，因为两个分布更容易区分。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>; dp-auditorium &lt;/h2>; &lt;p>; dp-auditorium包括两个主要组件：属性测试仪和数据集查找器。物业测试人员从在特定数据集评估的机制中获取样本作为输入，旨在确定所提供数据集中违反隐私保证的行为。数据集查找器建议隐私保证可能失败的数据集。通过将这两个组件组合在一起，DP原告可以实现（1）对不同机制和隐私定义的自动测试以及（2）检测隐私机制中的错误。我们实施了各种私人和非私人机制，包括计算记录平均值和更复杂机制的简单机制，例如不同的SVT和&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/stochastic_gradastic_gradient_descent &quot;>;渐变下降&lt;/a>;机制变体。 &lt;/p>; &lt;p>; &lt;strong>;属性测试仪&lt;/strong>;确定是否存在拒绝两个概率分布之间给定差异的假设，即&lt;em>; p &lt;/em>;和&lt;em>; q &lt;/em>; ，由预先指定的预算限制，该预算由DP保证进行了测试。他们从&lt;em>; p &lt;/em>; q，&lt;/em>;中从样品中计算出一个下限，如果下限值超过预期差异，则拒绝该属性。如果结果确实有限，则无法提供保证。为了测试一系列隐私保证，DP-Auditorium介绍了三个新颖的测试人员：（1）Hockeystickpropertytester，（2）Rényipropertytester和（3）Mmdpropertytester。与其他方法不同，这些测试人员不依赖于测试分布的显式直方图近似。他们依靠曲棍球刺激性差异，rényiDivergence和&lt;a href=&quot;https://jmlr.csail.mit.mit.edu/papers/v13/gretton12a.html&quot;>;最大平均差异（（ MMD）可以通过优化功能空间来估计差异。作为基准，我们实施了&lt;a href=&quot;https://arxiv.org/abs/1806.06427&quot;>; Histogropropertytester &lt;/a>;，通常使用的近似DP测试仪。虽然我们的三个测试人员遵循类似的方法，但对于简洁起见，我们在这篇文章中专注于曲棍球史密斯果属。 &lt;/p>; &lt;p>;给定两个相邻数据集，&lt;em>; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;，hockeystickpropertytester找到了一个下限，&lt;i>; &lt;span style =“ bottom：bottom：9px;左：9px;位置：相对;转换：比例（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp;对于&lt;em>; m &lt;/em>;之间的曲棍球 - 粘性差异（&lt;em>;） d）&lt;/em>;和&lt;em>; m &lt;/em>;（&lt;em>; d&#39;&lt;/em>;），具有很高的可能性。曲棍球 - 粘性分歧强制执行两个分布&lt;em>; m &lt;/em>;（&lt;em>; d）&lt;/em>;和&lt;em>; m &lt;/em>;（&lt;em>; d&#39;&lt;/em>;）大约DP保证。因此，如果隐私保证声称曲棍球 - 粘性差异最多是&lt;em>;Δ&lt;/em>;，而&lt;i>; &lt;span style =”底部：9px; left：9px; 9px; 9px; position; tosis：相对; transfrom：scale（scale from：scale（ 4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp; >; &lt;em>;Δ&lt;/em>;，然后具有很高的差异，差异高于&lt;em>; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;上所承诺的差异，并且该机制无法满足给定的近似DP保证。下限&lt;i>; &lt;span style =“底部：9px;左：9px;位置：相对; Transfrom：scale（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp;被计算为曲棍球 - 粘性散射的变分配方的经验和可拖动的对应物（请参阅&lt;a href=&quot;https://arxiv.org/pdf/2307.05608.pdf&quot;>; &lt;a 。 &lt;i>; &lt;>; &lt;span style =“底部：9px;左：9px;位置：相对; Transfrom：scale（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp;随着从机制中得出的样品数量增加，但是随着变化公式的简化而减小。我们平衡了这些因素，以确保&lt;i>; &lt;span style =“底部：9px;左：9px;位置：相对;转换：比例（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>; &amp;nbsp;既准确又易于计算。 &lt;/p>; &lt;p>; &lt;strong>;数据集查找器&lt;/strong>;使用&lt;a href=&quot;https://arxiv.org/pdf/2207.13676.pdf&quot;>; black-box优化&lt;/a>;查找数据集&lt;em >; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;最大化&lt;i>; &lt;span style =“底部：9px; left：9px; position; tocation;相对; transfrom：scale（4,0.5）;“>;^>;^ &lt;/span>;Δ&lt;/i>;，差异值的下限&lt;em>;Δ&lt;/em>;。请注意，黑框优化技术是专门为设置设计的，在这些设置中，在目标函数可能是不切实际甚至不可能的情况下。这些优化技术在探索阶段和剥削阶段之间振荡，以估计目标函数的形状，并预测物镜可以具有最佳值的领域。相比之下，完整的探索算法，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/hyperparameter_optimization#grid_search&quot;>;网格搜索方法&lt;/a>; em>; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;。 DP-Auditorium通过开放源的黑框优化库实现了不同的数据集查找器&lt;a href=&quot;https://github.com/google/google/vizier/vizier&quot;>; Vizier &lt;/a>;。 &lt;/p>; &lt;p>;在新机制上运行现有组件仅需要将机制定义为python函数，该函数采用了一系列数据&lt;em>; d &lt;/em>;和所需数量的样品&lt;em>; n &lt;/em >;通过在&lt;em>; d &lt;/em>;上计算的机制输出。此外，我们还为测试人员和数据集查找器提供灵活的包装器，使从业者可以实施自己的测试和数据集搜索算法。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>;具有不同输出空间的九种非私有机制。对于每个属性测试仪，我们使用&lt;em>;ε&lt;/em>;的不同值在固定数据集上重复测试，并报告每个测试仪标识隐私错误的次数。尽管没有测试仪始终胜过其他测试人员，但我们确定了以前技术（Histogroperpertytester）所遗漏的错误。请注意，组合植物的组织不适用于SVT机制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s785/image22.png “ style =”边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-Foriginal-height =“ 409” data-eriginal-width =“ 785” SRC =“ https：// blogger.googleusercontent.com/img/b/r29vz2xl/avvxsehllyauj1cew8xcqnynmvggkz2bd55uhlzudlxvdn_tw4zbdw4zbdw4 zbbbbbbbbbbbbbbbbbbbbb i6 zvvvvvvvvvvvvvpzjjjjjjjjquznj jjjjjjjjjjjjjjjjjpazxpaz x.4-swpzpazxpazxpazxpazxpazxpazxpazxpaZ 5GHLAGS3AIUQHNS-GNJFA_AT9LMAMVITLRDEP5OJHPRFRA6OLDJRY6YOST66LZ8Z8ZSSIF8WIWIW6UHKFA4PKN7/S16000/s16000/image22.png字幕“ style =”文本align：中心;“>;每个属性测试仪的次数违反了经过测试的非私有机制的隐私行为。 NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Laplace_Mechanism&quot;>;Laplace&lt;/a>; and &lt;a href=&quot;https://en.wikipedia. org/wiki/addistil_noise_differential_privacy_mechanisms＃gaussian_mechanism“>;高斯&lt;/a>;计算平均值的机制&lt;a href=&quot;https://github.com/tensorflow/privacy/blob/blob/master/tensorflow_privacy/privacy/privacy/privacy/pptimizers/dp_optimizer_keras.keras.keras.keras.py&quot;>; DP梯度渐变discent algorithM &lt;/gorithm &lt;/a>;（DP-GD）私人数据上的损失功能。为了保留隐私，DP-GD采用剪裁机制来绑定&lt;a href=&quot;https://mathworld.wolfram.com/l2-norm.html&quot;>; l2-norm &lt;/a>;梯度的价值&lt;/a>; em>; g &lt;/em>;，然后增加高斯噪声。该实现错误地假设添加的噪声具有&lt;em>; g &lt;/em>;的比例，而实际上，量表为&lt;em>; sg &lt;/em>;，其中&lt;em>; s &lt;/em>;是一个正面的标量。这种差异导致近似的DP保证，仅适用于大于或等于1的值，我们评估了属性测试人员在检测此错误中的有效性Hockeystickpropertytester和RényiPropertytester在识别侵犯隐私，表现优于MMDPropertytester和Histogroperpertytester方面表现出色。值得注意的是，这些测试人员即使在&lt;em>; s &lt;/em>;的值高达0.6的值中也检测到错误。 It is worth highlighting that &lt;em>;s &lt;/em>;= 0.5 corresponds to a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/308cbda4db6ccad5d1e7d56248727274e4c0c79e/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py#L445C1 -l446c1“>;常见错误&lt;/a>;在考虑隐私预算&lt;em>;ε&lt;/em>;的文献中涉及丢失两个因子。 DP原告成功捕获了此错误，如下所示。有关更多详细信息，请参见第5.6节&lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;在这里&lt;/a>;。 &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;“>; &lt;tbody>; &lt;trody>; &lt;try>; &lt;tr>; &lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg&quot; imageanchor=&quot; 1“ style =”保证金左：自动;边缘右：auto;“>; &lt;img border =“ 0” data-Original-height =“ 332” data-Original-width =“ 836” src =“ https：/ https：/ https：/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>;估计的差异和测试阈值&lt;em>; s &lt;/em>;的不同值&lt;em>; s &lt;/em>;使用Histogroperpertytester（&lt;strong>;左>; &lt;/strong>;）和Hockeystickpropertytester（&lt;strong>;右&lt;/strong>;）。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” 0“” class =“ tr-caption-container”样式=“边距 - 左：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s828/image20.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto;边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 333” data-eriginal-width =“ 828” src =“ https://blogger.googleusercontent.com/img/img/b/ R29VZ2XL/AVVXSEIBBCE0TFNWCNJ4COXPVVYUZRJA_3JJTNBJSZZA7IG-NIBA14JHOHHOHLN9BGCDO_N4HSUIHLRRN4HSU_N4HSU_N4HSU_HSU_HSU_HSU_HSU_HSU_HSU_HSU_HSU_HHSU_HSUFFFTEMN KNZ0FLBP2QOYPALWVM-NZO118OA67ZGDI_VGC72TAZGKAYPGOWIL6P_LJD/s16000/image20.jpg“ ;“>;用rényipropertytester（&lt;strong>; left &lt;/strong>;）和mmdpropertytester（&lt;strong>;右>; &lt;/strong>;右>; &lt;/strong>;） ）&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;b>;要测试数据集查找器，我们在发现隐私侵犯之前探索了探索的数据集数量。平均而言，大多数错误是在少于10个调用数据集查找器的电话中发现的。随机和探索/剥削方法比网格搜索更有效地查找数据集。有关更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;纸&lt;/a>;。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; dp是数据保护最强大的框架之一。但是，正确实施DP机制可能具有挑战性，并且容易使用传统的单元测试方法来轻易检测到的错误。统一的测试框架可以帮助审计师，监管机构和学者确保私人机制确实是私人的。 &lt;/p>; &lt;p>; dp-auditorium是通过功能空间优化DP测试DP的一种新方法。我们的结果表明，这种基于功能的估计始终优于先前的黑盒访问测试仪。最后，我们证明这些基于函数的估计器与直方图估计相比，可以更好地发现隐私错误的发现率。由&lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/main/python/dp_auditorium&quot;>;开放源&lt;/a>; DP-Auditorium，我们旨在为端到 - 最终测试新的差异私有算法。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;此处描述的工作是与AndrésMuñoz共同完成的麦地那，威廉·孔和乌马尔·赛义德。我们感谢Chris Dibak和Vadym Doroshenko为我们的图书馆提供了有益的工程支持和界面建议。&lt;/em>; &lt;/p>; &lt;/pents>; &lt;link href =“ http://blog.research.google/feeds/feeds/593333333333333333333333333333333333333333333333333333333460125094774/comments /默认值“ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/dp-auditorium-flexible- Library-for.html＃comment-form“ rel =” reply =“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626262626262626262626/posts /default/5933365460125094774&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;self “ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/dp-auditorium-flexible-library-library-for.html“ rel =” re =“替代” title = “ DP-ADITORIUM：一个灵活的库库” &lt;/uri>; &lt;email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https：/https：/ /img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail Height =“ 72” url =“ https：//blogger.googleusercorcercontent 。 9KrdyZQXN38JKVH6XCMLV6FZ1G0UFAXTKORHTTY0WUJEXLPQV6P2C9RPDG_W_W_W_5ZP/s72-c/hire.jpg“ /“>; &lt;/媒体：缩略图>; &lt;thr：thr>; 0 &lt;/thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-3264646469415710647310 &lt;/id>; &lt;Ebtumed>; 2024-02-06T11：17：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-02-06T11：17：53.968-08：00 .blogger.com/atom/ns＃“ term =“图形挖掘”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” >; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“机器学习”>; &lt;/category>; &lt;category>; &lt;类别spece =“ http://wwwww.blogger.com/atom/ns ＃“ term =” tensorflow“>; &lt;/category>; &lt;title type =“ text”>; tensorflow中的图形神经网络&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>; dustin Zelle，软件工程师，Google Research和Arno Eigenwillig，软件工程师，Coreml &lt;/span>; &lt;img src =“ htttps://blogger.google.googleusercercercorcercontent.com/img/r29vz2x2xxl/r29vz2xl/ G4MUM5I-7T3ZDLWUWN5DCCUQI5FKQ-C3EIBPNUQFOLUKFUSX -i3ovim1teps_jkikzh7xqghupnsoa2y3peugwcpnyg4ziqa2_kqwxjpflo0wm6gnwm6gnw8txg5edndiwx_dkk _dkk/s1600/s1600/tfgnn％/tfgnn％20HERO.GIF />; &lt;p>;对象及其关系在我们周围的世界中无处不在，对于理解对象的关系与孤立观看的自己的属性一样重要 - 以运输网络，生产网络，知识图或社交网络为例。离散的数学和计算机科学在形式化&lt;em>; &lt;em>; &lt;a>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/graph_(Discrete_mathematics fornation等网络方面有悠久的历史。由&lt;em>; nodes &lt;/em>;组成&lt;em>; edges &lt;/em>;以各种不规则方式连接。然而，大多数机器学习（ML）算法仅允许输入对象之间的常规和统一关系，例如像素的网格，一系列单词或根本没有关系。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;图形神经网络&lt;/a>;简而言之，已经成为一种强大的技术来利用图形的连接性（如较旧的算法&lt;a href=&quot;http://perozzi.net/projects/projects/deepwalk/&quot;>; deepwalk &lt;/a>; “ https://snap.stanford.edu/node2vec/&quot;>; node2vec &lt;/a>;）和各种节点和边缘上的输入功能。 GNNS可以对整个图表进行预测（该分子以某种方式反应？），单个节点（鉴于该文档的引用是什么主题？）或潜在的边缘（该产品是可能一起购买的，使用该产品？）。除了对图表进行预测外，GNN是一种强大的工具，用于桥接鸿沟，以弥补更典型的神经网络用例。他们用A &lt;em>;连续&lt;/em>;的方式编码图形的&lt;em>;离散&lt;/em>;，&lt;em>;关系&lt;/em>;信息，以便自然地将其包含在另一个深度学习系统中。 &lt;/p>; &lt;p>;我们很高兴宣布发布&lt;a href=&quot;https://github.com/tensorflow/tensorflow/gnn&quot;>; tensorflow gnn 1.0 &lt;/a>;（tf-gnn），一种生产测试的用于在大尺度上建造GNN的库。它支持Tensorflow中的建模和训练，以及从大型数据存储中提取输入图。 TF-gnn是从头开始构建的，以用于异质图，其中对象和关系的类型由不同的节点和边缘组表示。现实世界中的对象及其关系发生在不同的类型中，而TF-GNN的异质焦点使代表它们很自然。 &lt;/p>; &lt;p>;在TensorFlow中，此类图由类型&lt;code>; tfgnn.graphtensor &lt;/code>;的对象表示。这是一种复合张量类型（一个python类中的张量），被接受为&lt;a href=&quot;https://en.wikipedia.org/wiki/first-class_citizen&quot;>; &lt;code>; tf.data.dataset &lt;/code>;，&lt;code>; tf.function &lt;/code>;等。它同时存储图形结构及其附加到节点，边缘和整个图形的功能。图形传感器的可训练转换可以定义为高级&lt;a href=&quot;https://www.tensorflow.org/guide/keras&quot;>; keras api &lt;/a>;或直接使用&lt;code>; tfgnnn， .graphtensor &lt;/code>;原始。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; gnns：在上下文中对对象进行预测&lt;/h2>; &lt;p>;用于插图，让我们看看在TF-GNN的一个典型应用中：预测通过巨大数据库的交叉引用表定义的图表中某种类型的节点的属性。例如，具有一对一引用的计算机科学（CS）ARXIV论文的引用数据库和多一的引用关系，我们想预测每篇论文的主题领域。 &lt;/p>; &lt;p>;像大多数神经网络一样，在许多标记示例的数据集（约百万）的数据集上，GNN受到培训，但每个培训步骤仅包含少量较小的培训示例（例如，数百个）。为了扩展到数百万，GNN在底层图的相当小子图表上进行了训练。每个子图都包含足够的原始数据来计算其中心标记节点的GNN结果并训练模型。这个过程（通常称为子图抽样）对于GNN培训非常重要。大多数现有的工具都以批处理方式完成采样，从而生成静态子图进行培训。 TF-GNN提供了通过动态和互动进行采样来改进这一点的工具。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 600” data-Original-width =“ 800” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “文本 -  align：中心;”>;如图所示，从较大的图形中对小的，可拖动的子图进行采样以创建用于GNN训练的输入示例。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/tbody>; &lt;/tbody>; &lt;/tbody>; >; &lt;p>; TF-GNN 1.0首次亮相灵活的Python API，以在所有相关尺度上配置动态或批次子图抽样：在COLAB笔记本中进行互动性（例如&lt;a href =” https://colab.research.research.google.com/github.com/github.com/github.ghithub.com/github /tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>; this One &lt;/a>;），用于有效地采样存储在单个训练主机的主要存储器中的小数据集=“ https://beam.apache.org/”>; apache beam &lt;/a>;用于存储在网络文件系统上的巨大数据集（最多数亿个节点和数十亿个边缘）。有关详细信息，请参阅我们的用户指南以获取&lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/main/main/tensorflow_gnn/docs/guide/guide/inmemory_sampler.md&quot;>; in-memory &lt;/a>; and-memory &lt;/a>; andmemory &lt;/a>; &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/main/tensorflow_gnn/docs/guide/geide/beam_sampler.md&quot;>;基于beam的&lt;/a>;分别采样。 &lt;/p>; &lt;p>;在那些相同的采样子图上，GNN的任务是计算根节点处的隐藏状态（或潜在）状态；隐藏状态聚集并编码根节点邻域的相关信息。一种经典的方法是&lt;a href=&quot;https://research.google/pubs/neural-message-passing-for-quantum-chemistry/&quot;>; message-pass-passing神经网络&lt;/a>;。在每一轮消息传递中，节点都会从传入的边缘接收邻居的消息，并从他们那里更新自己的隐藏状态。在&lt;em>; n &lt;/em>;圆圈之后，根节点的隐藏状态反映了&lt;em>; n &lt;/em>;边缘中所有节点的汇总信息（如下图所示，用于&lt;em>; n &lt;/em>; = 2） 。消息和新的隐藏状态由神经网络的隐藏层计算。在异质图中，使用单独训练的隐藏层用于不同类型的节点和边缘&lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ traption -Container“ style =” Margin-Left：auto; Margin-Right：auto;“>; &lt;tbody>; &lt;try>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= &quot;511&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td >; &lt;/tr>; &lt;tr>; &lt;td class =“ tr caption” style =“ text-align：center;”>;如图所示，一个简单的消息串起的神经网络，在每个步骤中，节点状态都从外部传播到将其合并以计算新节点状态的内部节点。一旦达到根节点，就可以做出最终预测。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;训练设置是通过将输出层放在GNN隐藏状态的顶部来完成的对于标记的节点，计算&lt;em>;损失&lt;/em>;（以测量预测误差），并像往常一样通过反向传播来更新模型权重。 &lt;/p>; &lt;p>;除了有监督的培训之外（即最大程度地减少标签定义的损失），GNNS还可以以无监督的方式进行培训（即没有标签）。这使我们能够计算节点及其特征的&lt;em>;离散图结构的&lt;em>;连续&lt;/em>;表示（或&lt;em>;嵌入&lt;/em>;）。然后，这些表示通常在其他ML系统中使用。这样，图形编码的离散关系信息可以包含在更典型的神经网络用例中。 TF-GNN支持无与伦比图的无监督目标的细粒度规范。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;建筑物gnn Architectures &lt;/h2>; &lt;p>; TF-GNN库支持建筑和培训GNNS各种水平的抽象。 &lt;/p>; &lt;p>;在最高级别上，用户可以将任何预定义的模型与在keras层中表达的库捆绑在一起的任何预定义型号。除了从研究文献中收集的少量模型外，TF-GNN还配备了高度可配置的模型模板，该模型提供了精选的模型选择，我们发现这些选择可为许多内部问题提供强大的基线。这些模板实现了GNN层；用户只需要初始化KERAS层即可。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1 。 src =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsejmfb8qox14uu1geamffop0ca__zxa_zxa_mkzvsvsvsvsvsvsijoo ZXCMIFENVF6DCH4OPWHVQJHR-MLGCFONU_WPET5H1PZRDXMHJCYKGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGBGWVVLUZCCQNAO5QTONVP5KNJRAD/s16000/s16000/tfgnn％20Code1.pode1.png &lt;/>; &lt;/>; &lt;/>; &lt;/tr /tbody>; &lt;/table>; &lt;p>;在最低级别上，用户可以根据围绕图形传递数据的原始内容来从头开始编写GNN模型，例如将数据从节点传播到所有传出的边缘或将数据汇总到来自其所有传入边缘的节点（例如，计算传入消息的总和）。当涉及功能或隐藏状态时，TF-GNN的图形数据模型平均处理节点，边缘和整个输入图，这使得不仅表达了以上讨论的MPNN等节点中心模型，还可以表达出更通用的&lt;a href = a href =的更通用形式。 “ https://arxiv.org/abs/1806.01261&quot;>; Graphnets &lt;/a>;。这可以（但不需要）用凯拉斯（Keras）作为核心张力量顶部的建模框架。有关建模的更多详细信息和中间级别，请参见TF-GNN &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/main/main/tensorflow_gnn/docs/guide/guide/ghn_modeling.md&quot;>;用户指南&lt; /a>;和&lt;a href=&quot;https://github.com/tensorflow/gnn/tree/main/main/tensorflow_gnn/models&quot;>;模型集合&lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;训练编排&lt;/h2>; &lt;p>;虽然高级用户可以自由进行自定义模型培训，但&lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/main/tensorflow_gnn/docs/guide/runner.md&quot;>; tf-gnn Runner &lt;/a>;海角模型在常见情况下。一个简单的调用可能看起来像这样：&lt;/p>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-Left：auto; auto; margin-right; margin-right : auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD -zltqccgnlghpedfrj2vs-d5-rpzfwxaarpojit-mh3n8tj7nzy-sfxtjxjdrhqy_hvua3-_c8_xqjffrwblo -LEFT：自动;边缘右：自动;“>; &lt;img border =” 0&quot; data-original-height=&quot;508&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt -mh3n8tj7nzy-sfxtjjxjjdrhhqy_hvua3-_c8_xqjfrwblo-dzcfzgul6wynmwjhum7z_mykvf/s16000/s16000/tfgnn％20code％20code2.pode2.png&#39; ML疼痛的解决方案，例如分布式培训和&lt;code>; tfgnn.graphtensor &lt;/code>;填充云TPU上的固定形状。除了对一项任务进行培训（如上所述），它还支持对多个（两个或多个）任务的联合培训。例如，无监督的任务可以与监督的任务混合，以告知最终的连续表示（或嵌入）具有特定应用的归纳偏见。呼叫者只需要用任务的映射来替换任务参数：&lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container” style =“ margin-left：margin-left：自动; Margin-Right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleusercontent.com/img/r29vz2xl/ AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original -height =“ 392” data-Original-width =“ 1400” src =“ https://blogger.googleusercentent.com/img/b/r29vz2xl/avvz2xl/avvxseg4gg4ggpfpfzib5pfpfziB5 ESNZ3I24LEBCNSALR2NU_MWI7M_S47P2BX-AVIAUKY_DXDEKZNDNYMI_52JCEMNKENKENKYKYKYJRQDFYE3_PHAWJZZ7MAQ1LVWWWWW-OYPUWPOOYPSAFBRUNU5Q4M2Q4M2Q4M2/S16000/S16000/TFFFFY3％20. png“/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;此外，TF-gnn Runner还包括&lt;a href =“ https：// www的实现。 tensorflow.org/tutorials/interpretability/integratiencation_gradients&quot;>; integrated梯度&lt;/a>;用于模型属性。集成梯度输出是一种与观察到的GraphTensor相同连接性的GraphTensor，但其功能被梯度值替换，其中较大的值在GNN预测中贡献了较小的值。用户可以检查梯度值，以查看其GNN使用最多的功能。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;简而GNN在Tensorflow中的应用，并在该领域进行进一步的创新。如果您想了解更多信息，请尝试我们的&lt;a href=&quot;https://colab.sandbox.google.com/github/tensorflow/tensorflow/gnn/blob/master/master/master/examples/notebooks/ogbn_mag_mag_e2e.ipynb&quot;>; colab colab ch演示&lt;/a>;具有流行的OGBN-MAG基准测试（在您的浏览器中，无需安装），浏览我们的其余部分&lt;a href =“ https://github.com/tensorflow/tensorflow/gnn/blob/blob/main/main/tensorflow_gnn/ docs/guide/oferview.md“>;用户指南和colabs &lt;/a>;，或者看我们的&lt;a href=&quot;https://arxiv.org/abs/2207.03522&quot;>; paper &lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>; tf-gnn版本1.0是由a开发的Google Research之间的合作：Sami Abu-el-Haija，Neslihan Bulut，Bahar Fatemi，Johannes Gasteiger，Pedro Gonnet，Jonathan Halcrow，Liangze Jiang，Silvio Lattanzi，Brandon Mayer，Brandon Mayer，Vahab Mirrokni，Bryanni，Bryan Perozzi，Google coret coret coret core core core core core core core core core core core core core core core core core core core core core core core core core core core core corece ML：Arno Eigenwillig，Oleksandr Ferludin，Parth Kothari，Mihir Paradkar，Jan Pfeifer，Rachael Tamakloe和Google DeepMind：&lt;strong>; &lt;/strong>; alvaro Sanchez-Gonzalez和Lisa Wang。 >; &lt;&lt;link href =“ http://blog.research.google/feeds/3264694155710647310/comments/comments/default” rel =“ reply =“ requeies” title =“ post commist” “ http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html#comment-form” rel =“回复” title =“ 0 comment” type =“ type” type =“ text/html”/html“/html”/ >; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/32646469415710647310” /www.blogger.com/feeds/847492633145202626/posts/default/326464694155710647310“ 02/graph-near-networks-in-tensorflow.html“ rel =“替代” title =“ tensorflow中的图形神经网络” type =“ text/html”/html“/>; &lt;unce>; &lt;names>; &lt;names>; google ai &lt;/name>; &lt;/name>; &lt; URI>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri>; &lt;Email>; &lt;Email>; noreply@blogger.com &lt;/email email>; &lt;gd>; &lt;gd：image height =“ 16” .com/g/2005＃缩略图“ src =” https://img1.blogblog.com/img/b16-rounded.gif.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/rutim>; &lt;/rutim>; &lt;媒体：&lt;媒体：缩略图height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif&quot; width=&quot;72 “ XMLNS：Media =” http：//search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr：thr：thr>; thr>; 0 &lt;/thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;entry>; &lt;id>; &lt;id>; tag：tag：tag：tag：tag：tag：tag： Blogger.com，1999年：Blog-8474926331452026626.POST-6956767920612914706 &lt;/id>; &lt;/id>; &lt;出版>; 2024-02-02-02T11：07：00.000-08：008：008：00：00.000-08：00 &lt;/00 08:00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ Google Cloud Platform”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http：//www。 blogger.com/atom/ns#“ term =“机器学习”>; &lt;/actory>; &lt;title type =“ text”>;时间序列预测&lt;/stitle>; &lt;content>; &lt;content type =“ html” >;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Yichen Zhou, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/ s320/hero.jpg“ style =” display：none;” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/time_series&quot;>; Time-Series &quot;>; time-series &lt;/a>;预测在各个领域都无处不在科学。 In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-深度学习的价值“>;提高需求预测准确性&lt;/a>;可以有意义地降低库存成本并增加收入。深度学习（DL）模型已成为一种流行的方法，用于预测丰富的，多变量的时间序列数据，因为它们已被证明在各种环境中都表现良好（例如，DL模型在&lt;a href =“ https：https：https：https：https：https： //www.sciendirect.com/science/article/pii/s0169207021001874&quot;>; m5竞赛&lt;/a>;）。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;同时，用于自然语言处理（NLP）任务的大型基础语言模型（例如&lt;a href = “ https://en.wikipedia.org/wiki/machine_translation&quot;>; translation &quot;>; translation &lt;/a>;，&lt;a href =” -in-ai/“>;检索效果生成&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/intelligent_code_completion&quot;>;代码完成&lt;/a>;。这些模型经过大量&lt;em>; textual &lt;/em>;数据的培训，这些数据来自各种来源，例如&lt;a href=&quot;https://commoncrawl.org/&quot;>; Common Common Crawl &lt;/a>;这使他们可以识别语言中的模式。这使它们非常强大&lt;a href=&quot;https://en.wikipedia.org/wiki/zero-shot_learning&quot;>; Zero-sero-hot &lt;/a>;工具;例如，&lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;与检索配对时。 &lt;/p>; &lt;p>;尽管基于DL的预测者大多&lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;胜过&lt;/a>; &lt;/a>;在&lt;a a href =“ https：https：https：https： //cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>; reducing培训和推理成本&lt;/a>;，他们面临挑战：大多数DL架构都需要&lt;a href =“ https：https：https： //cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>; long and涉及培训和验证周期&lt;/a>;在客户可以在新的时间表上测试模型之前。相比之下，时间序列预测的基础模型可以在没有额外培训的情况下为看不见的时间序列数据提供体面的预先预测，使用户能够专注于精炼实际下游任务的预测，例如&lt; =“ https://en.wikipedia.org/wiki/customer_demand_planning”>;零售需求计划&lt;/a>;。 &lt;/p>; &lt;p>;到了那一端，在“ &lt;a href=&quot;https://arxiv.org/pdf/2310.10688.pdf&quot;>; fime-foright-folly基础模型&lt;/a>;”中我们介绍了TimesFM，这是一种预测模型，以1000亿个现实世界时点的大型时间序列语料库进行了预先培训。与最新的大语言模型（LLM）相比，TimesFM较小得多（200m参数），但我们表明，即使在这样的尺度上，它在各种不同域和时间粒度的各种看不见的数据集上的零拍摄性能也接近在这些数据集上明确培训的最先进的监督方法。今年晚些时Google Cloud顶点AI &lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>;接受了&lt;a href=&quot;https://arxiv.org/pdf/1801.10198.pdf&quot;>; decoder-lyly &lt;/a>;时尚的培训。首先，文本被分解为称为令牌的子字。然后，将令牌送入堆叠的因果&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;变压器&lt;/a>;产生与每个输入令牌相对应的输出的图层（它不能与未来的标记一起出现） 。最后，对应于&lt;em>; i &lt;/em>; th代币的输出总结了以前令牌的所有信息，并预测了（&lt;em>; i &lt;/em>; +1）-th令牌。在推断期间，LLM一次生成输出一个令牌。例如，当提示“法国的首都是什么？”时，它可能会产生令牌“ the”，然后在“法国的首都是什么？ “为了产生下一个令牌的“资本”，依此类推，直到它产生完整的答案：“法国的首都是巴黎”。 &lt;/p>; &lt;p>;预测时间序列的基础模型应适用于可变上下文（我们观察到的）和地平线（我们查询模型以预测的内容）长度，同时有足够的能力从大型预处理中编码所有模式数据集。与LLMS类似，我们使用堆叠的变压器层（自我注意力和&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/feedforward_neural_network&quot;>; feedForward &lt;/a>; layers &lt;/a>;层） 。在时间序列预测的背景下，我们将贴片（一组连续的时间点）视为一个令牌，该令牌被最近的&lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;漫长-Horizo​​n预测工作&lt;/a>;。然后，任务是预测（&lt;em>; i &lt;/em>; +1） - 在堆叠变压器层末尾输出的时间点的贴片。 &lt;/p>; &lt;p>;但是，与语言模型有几个关键区别。首先，我们需要一个&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/multilayer_perceptron&quot;>; Multilayer perceptron &lt;/a>;带有残留连接的块，可以将一整个时间表转换为可以输入的时间表以及&lt;a href=&quot;https://machinelearningmastery.com/a-gentle-indroduction-to-positional-cosedal-transformer-models-models-part-part-1/&quot;>;位置编码&lt;/a>;（（ pe）​​。为此，我们使用的残留块类似于我们在&lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>; long-horizo​​n预测&lt;/a>;中类似的工作。其次，在另一端，来自堆叠变压器的输出令牌可用于预测比输入贴片长度的更长的后续时间点长度，即，输出贴片长度可以大于输入贴片长度。 &lt;/p>; &lt;p>;考虑一个长度为512个时间点的时间序列，用于训练具有输入贴片长度32和输出贴片长度的TimesFM模型128。在训练期间，该模型同时训练以使用第32个时间 - 要预测接下来的128个时间点，这是预测65至192的前64个时间点，这是前96个时间点，预测97至224的时间点等等。在推断期间，假设该模型具有长度256的新时间序列，并负责预测未来的接下来的256个时间点。该模型将首先生成时间点257至384的未来预测，然后在初始256长度输入以及生成的输出以生成时间点385至512的条件。等于输入贴片长度为32，然后对于相同的任务，我们必须经过八代步骤，而不仅仅是上面的两个步骤。这增加了更多错误累积的机会，因此，在实践中，我们看到较长的输出贴片长度可为长马预测&lt;/p>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” 0 “ class =“ tr-caption-container” style =“边距 - 左：auto; margin-right：auto;”>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center; center;”>; &lt;a href = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border= &quot;0&quot; data-original-height=&quot;674&quot; data-original-width=&quot;1084&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3. jpg“/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; timesfm架构。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;预读数据&lt;/h2>; &lt;p>;就像llms一样有了更多的令牌，TimesFM需要大量合法的时间序列数据才能学习和改进。我们花了大量时间创建和评估培训数据集，以下是我们发现的最佳作用：&lt;/p>; &lt;div style =“ Margin-Left：40px;”>; &lt;p>; &lt;strong>;合成数据有助于基础知识。&lt;/strong>;可以使用统计模型或物理模拟生成有意义的合成时间序列数据。这些基本的时间模式可以教授模型的语法时间序列预测。 &lt;/p>; &lt;/div>; &lt;div style =“ margin-left：40px;”>; &lt;p>; &lt;strong>;现实世界中的数据增加了现实世界的风味。&lt;/strong>;我们通过可用的公共时间序列数据集梳理，并有选择地将1000亿个时间点的大型语料组合在一起。在这些数据集中，有&lt;a href=&quot;https://trends.google.com/trends/&quot;>; Google Trends &lt;/a>;和&lt;a href =“ https://meta.wikimedia.org/wiki/wiki/research： page_view“>; wikipedia pageviews &lt;/a>;，它跟踪人们感兴趣的内容，并且很好地反映了许多其他现实世界中时间序列中的趋势和模式。这有助于TimesFM理解更大的情况并在提供培训期间未见领域的环境时更好地概括。 &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;零摄像评估结果&lt;/h2>; &lt;p>;我们评估timesfm零 - 使用流行的时间序列基准在培训期间看不到的数据拍摄。我们观察到，TimesFM的性能要比大多数统计方法更好，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/autoregreseriestion_integrated_moving_avering&quot;>; arima &lt;/a>;，&lt;a href =“ href =” https：//en.wikipedia。 org/wiki/exponential_smoothing“>; eTs &lt;/a>;并且可以匹配或超过强大的DL模型，例如&lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>; deepar &lt;/a>; https://arxiv.org/abs/2211.14730&quot;>; patchtst &lt;/a>;在目标时间序列上已被明确训练&lt;/em>;。 &lt;/p>; &lt;p>;我们使用&lt;a href=&quot;https://huggingface.co/datasets/monash_tsf&quot;>; Monash预测存档&lt;/a>;来评估TimesFM的开箱即用绩效。该档案包含来自各个领域的成千上万个时间序列，例如交通，天气和需求预测，涵盖了几分钟到年度数据的频率。遵循现有文献，我们检查&lt;a href=&quot;https://en.wikipedia.org/wiki/mean_absolute_error&quot;>;是绝对错误&lt;/a>;（mae）&lt;a href =“ https://arxiv.org/ ABS/2310.07820“>;适当缩放&lt;/a>;，以便可以在数据集中平均。我们看到，零射（ZS）TimesFM比大多数监督方法（包括最近的深度学习模型）更好。我们还将TimesFM与&lt;a href=&quot;https://platform.openai.com/docs/models/gpt-3-5&quot;>; gpt-3.5 &lt;/a>;用于使用&lt;href提出的特定提示技术进行预测=“ https://arxiv.org/abs/2310.07820”>; llmtime（zs）&lt;/a>;。我们证明，尽管较小的数量级，但TimesFM的性能比LLMTime（ZS）更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 876” data-Original-width =“ 1476” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ text-align：中心;”>;在Monash数据集上针对其他监督和零射击方法的缩放的MAE（越好）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/tbody>; &lt;/table>; &lt;br>; &lt;p>;大多数Monash数据集是短或中型的，即预测长度不长。我们还测试了最新的最新基线&lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>; patchtst &lt;/a>;（和其他长期长期的长期长期，我们还测试了最新的基线&lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;（和其他长期长期的长期长期）地平线预测基线）。在下图中，我们将MAE绘制在&lt;a href=&quot;https://paperswithcode.com/dataset/ett&quot;>; ett &lt;/a>;数据集中，以预测未来的96和192时间点的任务。该度量已在每个数据集的最后一个测试窗口上计算（如&lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>; llmtime &lt;/a>; paper）。我们看到，TimesFM不仅超过了LLMTime（ZS）的性能，而且还与在相应数据集中明确训练的监督PatchTST模型的性能相匹配。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png “ style =”边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-Original-height =“ 433” data-Original-width =“ 735” SRC =“ https：// blogger.googleusercontent.com/img/b/r29vz2xl/avvxsej0dm32gpo6zkmnirobep2oa92g45b-zsmhgcf-unoj6mhgcf-unoj6mnoj6mnsn7vmfmfmfmfmfmfmfmfmfmfmfmfmfmfmfmfmfmfmf4 sssh5p4 qj6 iq qj6 i.1 qus1q iq qus1 qusy1 qus.1 qus.1 cqi.1 c.quccquccobso cqi cqi y.1 cqiccob tssluki2pohngml7z4bmobsd6nekuvl8wj7nhjdfyhbwaqoxocoxocfxvqxucmxugmxuz3wvqw8z5sabffsi7m85_7im85_7i/s16000/s16000/s16000/s16000/image1.png“字幕“ style =” text-align：中心;“>; timesfm（zs）的最后一个窗口MAE（较低）对llmtime（zs）（zs）和长期horizo​​n预测基线在ETT数据集中。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们训练解码器 - 只有使用大量100B现实世界时间点的大量预处理的时间序列进行预测的基础模型，其中大多数是搜索利息时序列数据来自Wikipedia的Google趋势和pageviews。我们表明，即使是一个相对较小的200m参数预处理模型，它使用我们的TimesFM体系结构也会在来自不同领域和粒度的各种公共基准上显示出令人印象深刻的零击性能。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;这项工作是几个合作的结果跨越Google Research和Google Cloud的个人，包括（按字母顺序排列）：Abhimanyu Das，Weihao Kong，Andrew Leach，Mike Lawrence，Mike Lawrence，Alex Martin，Rajat Sen，Yang Yang Yang Yang Yang，Skander Hannachi，Ivan Kuznetsov和Yichen Zhou。 /p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/6956767920612914706/comments/comments/comments/default/default” rel =“ requies” rel =“ reque” />; &lt;link href =“ http://blog.research.google/2024/02/a-decoder-only-foundation-model-model-for.html#comment-form” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/695679679202920612912914706 &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/ blog.research.google/2024/02/a-decoder-only-foundation-model-for.html“ rel =“备用” title =“一个仅解码器 - 仅解码器基础模型“/>; &lt;ause>; &lt;name>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/12098626265147775266161 &lt;/uri>; &lt;emage>; &lt;emage>; noreply@blogger@blogger.com &lt;/email>;图像高度=“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/img/b16-round.gif.gif.gif” width =“ 16 16 “>; &lt;/gd：image>; &lt;/reute>; &lt;媒体：thumbnail高度=“ 72” url =“ https://blogger.googleusercontent.com/img/img/r29vz2xl/avvz2xl/avvxsegjjlavi4qulzvqfmnhhyshhyshhyshhyshhyshhysrulfsensrulfsensrulfsollsplso VWHMQHCELY6FGW1VEX4JDXENNIJCAJ7TOOMZOLUFUT8RUDXNOFZDRBT0HRIHKCRK7RL6CQ5-KUUWGGROYQIIRQIIRPXNF4VMDAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUPAUP jpg“ width =” 72“ xmlns：媒体=” http://search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr：thr>; &lt;/thr>; 0 &lt;/thr：thr>; &lt;/thr>; &lt;/entry>; &lt;/entry>; &lt;输入>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502&lt;/id>;&lt;published>;2024-02-02T09:49:00.000-08:00&lt;/published>;&lt;updated>;2024-02- 02T09：49：36.211-08：00 &lt;/updated>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” ：//www.blogger.com/atom/ns#“ term =“ icml”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” term =“ ml fairness” >; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“监督学习”>; &lt;/category>; &lt;/cattory>; &lt;title type =“ text”>;介入早期读数以减轻缓解虚假的特征和简单性偏见&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-aTHOR”>;由Rishabh Tiwari发布，博士前研究人员和pradeep Shenoy，研究科学家，Google Research &lt;/span Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Machine learning models in the real world are often trained on limited data that may contain unintended &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_(statistics)&quot;>;statistical biases&lt;/a >;。 For example, in the &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CELEBA&lt;/a>; celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as &lt;a href=&quot;https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging&quot;>;medical diagnosis&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Surprisingly, recent work has also discovered an inherent tendency of deep networks to &lt;em>;amplify such statistical biases&lt;/em>;, through the so-called &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf&quot;>;simplicity bias&lt;/a>; of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. &lt;/p>; &lt;p>; With the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying &lt;em>;early readouts&lt;/em>; and &lt;em>;feature forgetting&lt;/em>; 。 First, in “&lt;a href=&quot;https://arxiv.org/abs/2310.18590&quot;>;Using Early Readouts to Mediate Featural Bias in Distillation&lt;/a>;”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;>;model distillation&lt;/a>;, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;Overcoming Simplicity Bias in Deep Networks using a Feature Sieve&lt;/a>;”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model&#39;s ability to generalize to unseen domains compared to previous approaches. Our &lt;a href=&quot;https://ai.google/responsibility/principles&quot;>;AI Principles&lt;/a>; and our &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;1080&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation comparing hypothetical responses from two models trained with and without the feature sieve.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Early readouts for debiasing distillation&lt;/h2>; &lt;p>; We first illustrate the diagnostic value of &lt;em>;early readouts&lt;/em >; and their application in debiased distillation, ie, making sure that the student model inherits the teacher model&#39;s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the &lt;a href=&quot;https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e&quot;>;cross-entropy loss&lt;/a>; between student outputs and the ground-truth labels) and teacher matching (minimizing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL divergence&lt;/a>; loss between student and teacher outputs for any given input). &lt;/p>; &lt;p>; Suppose one trains a linear decoder, ie, a small auxiliary neural network named as &lt;em>;Aux,&lt;/em>; on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model&#39;s dependence on potentially spurious features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;796&quot; data-original-width=&quot;1128&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrating the usage of early readouts (ie, output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used this signal to modulate the contribution of the teacher in the distillation loss on a per-instance basis, and found significant improvements in the trained student model as a result. &lt;/p>; &lt;p>; We evaluated our approach on standard benchmark datasets known to contain spurious correlations (&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;>;Waterbirds&lt;/a>;, &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CelebA&lt;/a>;, &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/civil_comments&quot;>;CivilComments&lt;/a>;, &lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;>;MNLI&lt;/a>;). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color 。 Thus, a measure of model performance is its &lt;em>;worst group accuracy&lt;/em>;, ie, the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our &lt;a href=&quot;https://arxiv.org/pdf/2310.18590.pdf&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1270&quot; height=&quot;511&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overcoming simplicity bias with a feature sieve&lt;/h2>; &lt;p>; In a second, closely related project, we intervene directly on the information provided by early readouts, to improve &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;feature learning&lt;/a>; and &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/generalization/video-lecture&quot;>;generalization&lt;/a>;. The workflow alternates between &lt;em>;identifying &lt;/em>;problematic features and &lt;em>;erasing identified features&lt;/em>; from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;1098&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Training workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We describe the identification and erasure steps in more detail: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Identifying simple features&lt;/b>;: We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. &lt;/li>;&lt;li>;&lt;b>;Applying the feature sieve&lt;/b>;: We aim to erase the identified features in the early layers of the neural network with the use of a novel &lt;em>;forgetting loss&lt;/em>;,&lt;em>; L&lt;sub>;f &lt;/sub>;&lt;/em>;, which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged. &lt;/li>; &lt;/ul>; &lt;p>; We can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter search&lt;/a>; to maximize validation accuracy, a standard measure of generalization. Since we include “no-forgetting” (ie, the baseline model) in the search space, we expect to find settings that are at least as good as the baseline. &lt;/p>; &lt;p>; Below we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (&lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;) and animal categorization (&lt;a href=&quot;https://arxiv.org/pdf/1906.02899v3.pdf&quot;>;NICO&lt;/a>;). Feature importance was estimated using post-hoc gradient-based importance scoring (&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GRAD-CAM&lt;/a>;), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;850&quot; data-original-width=&quot;1616&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Feature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this ability to learn better, generalizable features, we show substantial gains over a range of relevant baselines on real-world spurious feature benchmark datasets: &lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2104.06885.pdf&quot;>;CelebA Hair&lt;/a>;, &lt;a href=&quot;https://nico.thumedialab.com/&quot;>;NICO&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/imagenet_a&quot;>;ImagenetA&lt;/a>;, by margins up to 11% (see figure below). More details are available in &lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1082&quot; data-original-width=&quot;844&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png&quot; width=&quot;501&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We hope that our work on early readouts and their use in feature sieving for generalization will both spur the development of a new class of adversarial feature learning approaches and help improve the generalization capability and robustness of deep learning systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;The work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at &lt;a href=&quot;https://www.iitb.ac.in/&quot;>;IIT Bombay&lt;/a>;. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2254287928040727502/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html&quot; rel=&quot;alternate&quot; title=&quot;Intervening on early readouts for mitigating spurious features and simplicity bias&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984&lt;/id>;&lt;published>;2024-01 -31T13:59:00.000-08:00&lt;/published>;&lt;updated>;2024-01-31T13:59:36.056-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MobileDiffusion: Rapid text-to-image generation on-device&lt; /stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (eg, &lt;a href=&quot;https://stability.ai/news/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>;, &lt;a href=&quot;https://openai.com/research/dall-e&quot;>;DALL·E&lt;/a>;, and &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;). While recent advancements in inference solutions on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;Android&lt;/a>; via MediaPipe and &lt;a href=&quot;https://github.com/apple/ml-stable-diffusion&quot;>;iOS&lt;/a>; via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices&lt;/a>;”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style =&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rapid text-to-image generation on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires &lt;a href=&quot;https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot;>;iterative denoising&lt;/a>; to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature. &lt;/p>; &lt;p>; The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.00927&quot;>;DPM&lt;/a>;) or distillation techniques (eg, &lt;a href=&quot;https://arxiv.org/abs/2202.00512&quot;>;progressive distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;>;consistency distillation&lt;/a>;), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.&quot;>;Adversarial Diffusion Distillation&lt;/a>;, even reduce to a single necessary step. &lt;/p>; &lt;p>; However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (eg, &lt;a href=&quot;https://snap-research.github.io/SnapFusion/&quot;>;SnapFusion&lt;/a>;). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MobileDiffusion&lt;/h2>; &lt;p>; Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model&#39;s architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion&#39;s &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;UNet architecture&lt;/a>;. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion. &lt;/p>; &lt;p>; The design of MobileDiffusion follows that of &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;latent diffusion models&lt;/a>;. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP-ViT/L14&lt;/a>;, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Diffusion UNet&lt;/h3>; &lt;p>; As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (eg, data, optimizer) to study the effects of different architectures. &lt;/p>; &lt;p>; In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of &lt;a href=&quot;https://arxiv.org/abs/2301.11093&quot;>;UViT&lt;/a>; architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV /s915/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;915&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Convolution blocks, in particular &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;separable convolution&lt;/a>;. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance. &lt;/p>; &lt;p>; In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of &lt;a href=&quot;https://arxiv.org/pdf/2110.12894.pdf&quot;>;FLOPs&lt;/a>; (floating-point operations) and number of parameters. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Comparison of some diffusion UNets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h3>;Image decoder&lt;/h3>; &lt;p>; In addition to the UNet, we also optimized the image decoder. We trained a &lt;a href=&quot;https://arxiv.org/abs/2012.03715&quot;>;variational autoencoder&lt;/a>; (VAE) to encode an &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size. To further enhance efficiency, we design a lightweight decoder architecture by pruning the original&#39;s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our &lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;789&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Decoder&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;#Params (M)&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;PSNR↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;SSIM↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;LPIPS↓&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;SD&lt;/b>; &lt;/td>; &lt;td>;49.5 &lt;/td>; &lt;td>;26.7 &lt;/td>; &lt;td>;0.76 &lt;/td>; &lt;td>;0.037 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours&lt;/b>; &lt;/td>; &lt;td>;39.3 &lt;/td>; &lt;td>;30.0 &lt;/td>; &lt;td>;0.83 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours-Lite&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;9.8 &lt;/td>; &lt;td>;30.2 &lt;/td>; &lt;td>;0.84 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_similarity&quot;>;structural similarity index measure&lt;/a>; (SSIM), and &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;>;Learned Perceptual Image Patch Similarity&lt;/a>; (LPIPS).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;One-step sampling&lt;/h3>; &lt;p>; In addition to optimizing the model architecture, we adopt a &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN hybrid&lt;/a>; to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (eg, &lt;a href=&quot;https://arxiv.org/abs/2301.09515&quot;>;StyleGAN-T&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.05511&quot;>;GigaGAN&lt;/a>;) confront similar complexities, resulting in highly intricate and expensive training. &lt;/p>; &lt;p>; To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training. &lt;/p>; &lt;p>; The figure below illustrates the training procedure.初始化后，将嘈杂的图像发送到发电机以进行一步扩散。结果通过重建损失对地面真理进行评估，类似于扩散模型训练。然后，我们将噪声添加到输出中，并将其发送到歧视器，其结果通过GAN损失进行了评估，有效地采用了GAN来对降级步骤进行建模。通过使用预训练的权重来初始化发电机和鉴别器，训练成为一个微调过程，在小于10k的迭代中收敛。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ -uilkw/s960/image7.jpg“ style =” Margin-Left：auto; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 576” data-Original-width =“ 960 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg&quot; />;&lt;/a>;&lt;/ td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; diffusiongan fine-tuning的插图。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/tbody>; &lt;/表>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>;我们显示了通过使用diffusiongan单步采样的动员生成的示例图像。使用如此紧凑的模型（总共520m参数），迁移式Iffusion可以为各种域生成高质量的不同图像。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>;由我们的动员生成的图像&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;我们测量了我们在这两个方面的动员性能的性能iOS和Android设备，使用不同的运行时优化器。延迟数如下报告。 We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image.这种闪电速度有可能使移动设备上的许多有趣的用例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png “ ImaNeAnchor =” 1“样式=” Margin-Left：Auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 742” data-Original-width =“ 1184” SRC = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>;移动设备上的延迟测量（&lt;b>; s &lt;/b>;）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;在潜伏期和尺寸方面具有较高的效率，移动iffusion有可能成为very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts.我们将确保该技术的任何应用都将与Google的&lt;a href=&quot;https://ai.google/responsibility/Responsible-ai-practices/&quot;>;负责的AI实践&lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.&lt; /em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5966553114967673984/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/ atom+xml“/>; &lt;link href =” http://blog.research.google/2024/01/mobilediffusion-rapid-rapid-text-text-mimage.html#comment-form”注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/5966666666655314967676767676767673984 />; &lt;link href =“ http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/5966553149676767676767673984” //blog.research.google/2024/01/mobilediffusion-rapid-text-tex-image.html“ rel =“替代” title =“ mobilediffusion：快速的文本对图像生成” type type =“ type =” text/text/text/text/text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c /InstantTigo%20Hero.png“ width =” 72“ XMLNS：MEDIA =” http://search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr>; &lt;thr>; &lt;thr：thr>; thr>; 0 &lt;/thr>; 0 &lt;/thr>; thr>; thr>; thr>; &lt;/thr>; &lt;/thr>; &lt;/thr>; &lt;/thr>; &lt;/thr>; &lt;/thr /entry>; &lt;endry>; &lt;id>; tag：blogger.com，1999年：blog-8474926331452026626.post-51449067292534953495/id>; >; 2024-01-26T11：56：23.553-08：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/atom/ns#” scheme =“ http://www.blogger.com/atom/ns#” term =“ deep Learning”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“优化”>; &lt;/actory>; &lt;title type =“ text”>;混合输入矩阵乘法性能优化&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由Manish Gupta发布, Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;display: none ;” />; &lt;p>; AI驱动的技术正在将自己编织成我们日常工作的结构，从而有可能增强我们获得知识并提高我们的整体生产率。这些应用的骨干在于大语言模型（LLMS）。 LLM是内存密集型的，通常需要专业的硬件加速器有效地交付&lt;a href =” https://cloud.google.com/blog/products/compute/compute/the-worlds-worlds-worlds-larges-larges-larges-larges-larges-distributed-distributed-lllm-training-job-job-naind-job-on-job-on-job-on-job-onn. -tpu-v5e“>;计算能力的数十个Exaflops &lt;/a>;。这篇博客文章显示了我们如何通过更有效地利用内存来开始解决计算挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>; llm的内存和计算的大部分被&lt;a href=&quot;https://arxiv.org/pdf/pdf/2005.14165.pdf&quot;>;消费权重&lt;/a>;在&lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;矩阵乘法&lt;/a>;操作中。使用狭窄&lt;em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/primisity_data_type&quot;>;数据类型&lt;/a>; &lt;/a>; &lt;/em>;减少了内存消耗。例如，将重量存储在8位&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/integer_(computer_science)&quot;>; integer &lt;/a>;（即（ie，u8或s8）数据类型中，数据类型都会减少内存的内存相对于&lt;a href=&quot;https://en.wikipedia.org/wiki/single-precision_floation_floating-point_format&quot;>;单点&lt;/a>;（f32）和相对于&lt;a href = &lt;a href = = https://en.wikipedia.org/wiki/Half-precision_floating-point_format&quot;>;half-precision&lt;/a>; (F16) or &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format “>; bfloat16 &lt;/a>;（BF16）。此外，&lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;以前的工作显示&lt;/a>;显示了以S8和&lt;em em>;中的&lt;em>; strightss &lt;em>; stripllications运行矩阵乘法的LLM模型>;input&lt;/em>; in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as &lt;em>;weight-only quantization&lt;/em>; and requires efficient implementation of matrix multiplication with &lt;em>;mixed-inputs&lt;/em>;, eg, half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations. &lt;/p>; &lt;p>; To that end, in this blog we focus on mapping mixed-input matrix multiplication onto the &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/&quot;>;NVIDIA Ampere architecture&lt;/a>;. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; repository. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Memory footprint for an 175B parameter LLM model with various data types formats.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The matrix-multiply-accumulate operation&lt;/h2>; &lt;p>; Modern AI hardware accelerators such as &lt;a href= &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google&#39;s TPU&lt;/a>; and &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA&#39;s GPU&lt;/a>; multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the &lt;em>;matrix-multiply-accumulate&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;) operation. For the rest of the blog the reference to &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation are fixed in hardware 。 This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. &lt;/p>; &lt;p>; The Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation is defined by specifying two input matrices (eg, &lt;em>;A&lt;/em>; &amp;amp; &lt;em>;B&lt;/em>;, shown below) to produce a result matrix, &lt;em>;C&lt;/em>;. &lt;span style =“颜色：＃54863f;”>; &lt;code>; mma &lt;/code>; &lt;/span>;操作本质上支持混合精液。 &lt;em>; &lt;a href=&quot;https://developer.nvidia.com/blog/programpommming-tensor-cores-cuda-9/&quot;>; mixp-procision张量核心&lt;/a>; &lt;/em>;允许混合输入（&lt; em>; a &lt;/em>;和&lt;em>; b &lt;/em>;）数据类型（&lt;em>; c &lt;/em>;）数据类型。 In contrast, &lt;em>;mixed-input &lt;/em>;matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Tensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Challenges of mixed -input matrix multiplication&lt;/h2>; &lt;p>; To simplify the discussion, we restrict to a specific example of mixed-input matrix multiplication: F16 for user input and U8 for the model weights (written as F16 * U8). The techniques described here work for various combinations of mixed-input data types. &lt;/p>; &lt;p>; A GPU programmer can access a &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;hierarchy of memory&lt;/a>;, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a &lt;em>;warp&lt;/em>;. The supported data type &lt;em>;and&lt;/em>; layout within a warp are fixed for an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Data type conversion &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the &lt;em>;mixed-input&lt;/em>; matrix multiplication to hardware-supported &lt;em>;mixed-precision&lt;/em>; Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Layout conformance &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix &lt;em>;B&lt;/em>; of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called &lt;em>;layout conformance&lt;/em>; and needs to be achieved in the software. &lt;/p>; &lt;p>; The figure below shows an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation consuming matrix &lt;em>;A&lt;/em>; and matrix &lt;em>;B&lt;/em>; from registers to produce matrix &lt;em>;C&lt;/em>; in registers, distributed across one warp. The thread &lt;em>;T0&lt;/em>; is highlighted and zoomed in to show the weight matrix &lt;em>;B&lt;/em>; goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). (Original figure source &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100&lt;/a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Software strategies addressing challenges&lt;/h2>; &lt;p>; A typical data type conversion involves a sequence of operations on 32-bit registers, shown below. Each rectangular block represents a register and the adjoining text are the operations. The entire sequence shows the conversion from 4xU8 to 2x(2xF16). The sequence involves roughly 10 operations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; from 4xU8 to 2x(2xF16) in 32-bit registers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There are many ways of achieving layout conformance. Two of the existing solutions are: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Narrower bitwidth shared memory loads&lt;/em>;: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in &lt;em>;two&lt;/em>; 32-bit registers, with each register containing 2xF16 values (shown above for the matrix &lt;em>;B&lt;/em>;&#39;s thread &lt;em>;T0&lt;/em>;). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth. &lt;/li>;&lt;li>;&lt;em>;Pre-processing in global memory&lt;/em>;: An &lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;alternative strategy&lt;/a>; involves rearranging the data within the global memory (one level above the shared memory in &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;memory hierarchy&lt;/a>;), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; adopts this method to effectively address layout conformance challenges. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimized software strategies&lt;/h2>; &lt;p>; To further optimize and reduce the overhead of data type conversion and layout conformance, we have implemented &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a>;&lt;/code>; and &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;&lt;/code>;, respectively. &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. &lt;/p>; &lt;p>; The conversion sequence for &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>; is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. &lt;code>;FastNumericArrayConvertor&lt;/code>; uses the &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt&quot;>;permute byte&lt;/a>;&lt;/code>; to rearrange bytes of 4xU8 into two registers. Additionally, &lt;code>;FastNumericArrayConvertor&lt;/code>; does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in &lt;em>;two&lt;/em>; 32-bit registers containing 2x(2xF16) values. The &lt;code>;FastNumericArrayConvertor&lt;/code>; for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; utilizes &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;permute bytes&lt;/a>;&lt;/code>; and packed arithmetic, reducing the number of instructions in the data type conversion.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations 。 &lt;/p>; &lt;p>; NVIDIA Ampere architecture provides a load matrix instruction (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;). The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the &lt;em>;shape&lt;/em>; and &lt;em>;layout&lt;/em>; that &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>; consume. The use of &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; &lt;em>;reduces&lt;/em>; the number of load instructions and &lt;em>;increases&lt;/em>; the memory bandwidth utilization. Since the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; instruction moves U8 data to registers, the layout after the load conforms with U8*U8 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, and not with F16*F16 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation 。 We implemented &lt;code>;FragmentShuffler&lt;/code>; to rearrange the data within registers using shuffle (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; operations to achieve the layout conformance. &lt;/p>;&lt;p>; The most significant contribution of this work is to achieve layout conformance through register shuffles, avoiding offline pre-processing in global memory or narrower bitwidth shared memory loads. Furthermore, we provide implementations for &lt;code>;FastNumericArrayConvertor&lt;/code>; covering data type conversion from &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8-to-BF16&lt;/a>;, and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8-to-BF16&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance results&lt;/h2>; &lt;p>; We measured the performance of eight mixed-input variants of &lt;em>;our method&lt;/em>; (shown below in blue and red; varying the data types of matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) and two &lt;em>;mixed-precision&lt;/em>; data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Mixed-input matrix multiplication performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape &lt;code>;m=3456, n=4096, k=2048.&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p >; &lt;em>;We would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia. We would like to thank our NVIDIA partners Rawn Henry, Pradeep Ramani, Vijay Thakkar, Haicheng Wu, Andrew Kerr, Matthew Nicely, and Vartika Singh.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:/ /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;Mixed-input matrix multiplication performance optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;published >;2024-01-23T14:27:00.000-08:00&lt;/published>;&lt;updated>;2024-01-23T14:27:09.785-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer: Scaling transformers for graph-structured data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;Graphs&lt;/a>;, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A common approach to learning on graphs are &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;graph neural networks&lt;/a>; (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2&quot;>;message-passing&lt;/a>; framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors. &lt;/p>; &lt;p>; Recently, &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer models&lt;/a>; have emerged as a popular alternative to message-passing GNNs. These models build on the success of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer architectures&lt;/a>; in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism&lt;em>; &lt;/em>;that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see &lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot;>;the first open problem here&lt;/a>;). &lt;/p>; &lt;p>; A natural remedy is to use a &lt;em>;sparse&lt;/em>; interaction graph with fewer edges. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;Many sparse and efficient transformers have been proposed&lt;/a>; to eliminate the quadratic bottleneck for sequences, however, they do not generally extend to graphs in a principled manner. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;spectral graph theory&lt;/a>;, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Expander graphs&lt;/h2>; &lt;p>; A key idea at the heart of Exphormer is the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph&quot;>;expander graphs&lt;/a>;, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, ie, a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes. &lt;/p>; &lt;p>; A common class of expander graphs are &lt;em>;d&lt;/em>;-regular expanders, in which there are &lt;em>;d&lt;/em>; edges from every node (ie, every node has degree &lt;em>;d&lt;/em>;). The quality of an expander graph is measured by its &lt;em>;spectral gap&lt;/em>;, an algebraic property of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;>;adjacency matrix&lt;/a>; (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Ramanujan_graph&quot;>;Ramanujan graphs&lt;/a>; — they achieve a gap of &lt;em>;d&lt;/em>; - 2*√(&lt;em>;d&lt;/em>;-1), which is essentially the best possible among &lt;em>;d&lt;/em>;-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of &lt;em>;d&lt;/em>;. We use a &lt;a href=&quot;https://arxiv.org/abs/cs/0405020&quot;>;randomized expander construction of Friedman&lt;/a>;, which produces near-Ramanujan graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;800&quot; height=&quot;320&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968&quot;>;&lt;span face=&quot;Arial, sans- serif&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Expander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.&lt;/span>;&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse &lt;em>;d&lt;/em>;-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that &lt;em>;d&lt;/em>; is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.&lt;/p>; &lt;br />; &lt;h2>;Exphormer: Constructing a sparse interaction graph&lt;/h2>; &lt;p>; Exphormer combines expander edges with the input graph and virtual nodes. More specifically, the sparse attention mechanism of Exphormer builds an interaction graph consisting of three types of edges: &lt;/p>; &lt;ul>; &lt;li>;Edges from the input graph (&lt;em>;local attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from a constant-degree expander graph (&lt;em>;expander attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from every node to a small set of virtual nodes (&lt;em>;global attention&lt;/em>;) &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3&quot;>;&lt;span face=&quot;Arial, sans-serif&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Exphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.&lt;/span>;&lt;/ span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Each component serves a specific purpose: the edges from the input graph retain the inductive bias from the input graph structure (which typically gets lost in a fully-connected attention module). Meanwhile, expander edges allow good global connectivity and random walk mixing properties (which spectrally approximate the complete graph with far fewer edges). Finally, virtual nodes serve as global “memory sinks” that can directly communicate with every node. While this results in additional edges from each virtual node equal to the number of nodes in the input graph, the resulting graph is still sparse. The degree of the expander graph and the number of virtual nodes are hyperparameters to tune for improving the quality指标。 &lt;/p>; &lt;p>; Furthermore, since we use an expander graph of constant degree and a small constant number of virtual nodes for the global attention, the resulting sparse attention mechanism is linear in the size of the original input graph, ie, it models a number of direct interactions on the order of the total number of nodes and edges. &lt;/p>; &lt;p>; We additionally show that Exphormer is as expressive as the dense transformer and obeys universal approximation properties. In particular, when the sparse attention graph of Exphormer is augmented with self loops (edges connecting a node to itself), it can universally approximate continuous functions [&lt;a href=&quot;https://arxiv.org/abs/1912.10077&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>;2&lt;/a>;]. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Relation to sparse Transformers for sequences&lt;/h3>; &lt;p>; It is interesting to compare Exphormer to sparse attention methods for sequences. Perhaps the architecture most conceptually similar to our approach is &lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>;BigBird&lt;/a>;, which builds an interaction graph by combining different components. BigBird还使用虚拟节点，但是与Expymer不同，它使用窗口的关注和随机关注，来自&lt;a href =” ％a9nyi_model“>;erdős-rényi&lt;/a>;剩余组件的随机图模型。 &lt;/p>; &lt;p>; bigbird中的窗户注意力介绍了一个序列的令牌周围的令牌 - 可以将Exponmer中的当地邻里注意力视为窗口对图的关注的概括。 &lt;/p>; &lt;p>; The Erdős-Rényi graph on &lt;em>;n&lt;/em>; nodes, &lt;em>;G(n, p)&lt;/em>;, which connects every pair of nodes independently with probability &lt;em>;p &lt;/em>;，还可以充当适当高&lt;em>; p &lt;/em>;的扩展器图。但是，需要一个超线性数（ω（&lt;em>; n &lt;/em>; log &lt;em>; n &lt;/em>;）），以确保连接Erdős-rényi图，更不用说良好的扩展器了。另一方面，在解释器中使用的扩展器仅具有&lt;em>;线性&lt;/em>;的边缘数。 &lt;/p>; &lt;br />; &lt;h2>;实验结果&lt;/h2>; &lt;p>;早期的作品显示了在数据集中使用完整的基于图形变压器的模型，该模型的图形大小高达5,000个节点。 To evaluate the performance of Exphormer, we build upon the celebrated &lt;a href=&quot;https://github.com/rampasek/GraphGPS&quot;>;GraphGPS framework&lt;/a>; [&lt;a href=&quot;https://arxiv.org/ ABS/2205.12454“>; 3 &lt;/a>;]，它结合了消息传递和图形变压器，并在许多数据集中实现最先进的性能。我们表明，用GraphGPS框架中图形注意力组件的Exponmer代替密集的注意力，使人们可以实现具有可比或更好的性能的模型，通常具有更少的可训练参数。 &lt;/p>; &lt;p>;此外，Expontormer特别允许Graph Transformer体系结构远远超出上面提到的通常的图形尺寸限制。解释器可以扩展到10,000多个节点图的数据集，例如&lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>; cooauthor dataset &lt;/a>;，甚至是更大的图像-nown &lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>; ogbn-arxiv数据集&lt;/a>;，一个引用网络，该网络由170k节点和110万个Edges组成。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance 。 height=&quot;190&quot; data-original-width=&quot;1522&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;结果将expontormer与五个&lt;a href =&#39;的标准graphgps进行比较与标准graphgps进行比较https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper&#39;s publication.&lt;/td>;&lt;/ tr>; &lt;/tbody>; &lt;/table>; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-Left：auto; margin; margin; margin -right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B -suztayk0dx8udq2Sysx4Teep5erw021KZY8L4FEVNV3Boxal/s1600/exphormerPerformance.png“ style =”示例=“ display：block; margin-left; margin-left; auto; auto; margin-right; margin-right：auto; auto; auto; padding; padding：1em 0px; pade; 1em 0px; text-dext-centrign; 0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png “/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;结果将exponmer与五&lt;a href上的标准graphgps进行比较=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.&lt;/td>;&lt;/tr>; &lt;/tbody>; &lt;/table>;  - >; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container” style =“ margin-Left：auto; auto;边缘右：自动;“>; &lt;tbody>; &lt;tr>; &lt;td align =“ left”>; &lt;strong>;型号＆nbsp; &lt;/strong>; &lt;/stron>; &lt;/td>; &lt;td align =“ center”>; &lt;strong>; &lt;strong>; &lt;strong>;＆nbsp; pascalvoc- sp＆nbsp; &lt;/strong>; &lt;br>;＆nbsp; &lt;font size =“  -  1”>; f1分数&lt;/font>; &lt;strong>;↑&lt;/strong>;＆nbsp; &lt;/td>; &lt;td align =“中心”>; &lt;strong>;＆nbsp; coco-sp＆nbsp; &lt;/strong>; &lt;br>; &lt;br>; &lt;br>; &lt;font size =“  -  1”>; f1 score &lt;/f1 score &lt;/fand>; &lt;strong>; /strong>;＆nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Func&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;AP &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Struct&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MAE &lt;/font>;&lt;strong>;↓&lt;/strong>;&amp;nbsp;在>; &lt;/td>; &lt;/tr>; &lt;tr>;&lt;td colspan=&quot;6&quot;>;&lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />;&lt;/div>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;标准Graphgps＆nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.375 ± 0.011&amp;nbsp; &lt;/td>; &lt;td align =“ center”>;＆nbsp; 0.341±0.004＆nbsp; &lt;/td>; &lt;td align =“中心”>;＆nbsp; &lt;strong>; 0.654±0.004 &lt;/strong>;＆nbsp; &lt;/td>; &lt;td align =“ center”>;＆nbsp; 0.250±0.001＆nbsp; &lt;/td>; &lt;td align =“中心”>;＆nbsp; 0.334±0.001 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;td>; &lt;em>; exponmer（我们的） Align =“ Center”>; &lt;strong>; &lt;em>;＆nbsp; 0.398±0.004＆nbsp; &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/td>; &lt;td>; &lt;td align =“ center”>; &lt;strong>; &lt;strong>; &lt;mont>; &lt;em>; &lt;em>; 0.346±0.001 ” >; &lt;em>;＆nbsp; 0.248±0.001＆nbsp; &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/td>; &lt;td align =“ center”>; &lt;strong>; &lt;strong>; &lt;em>; &lt;em>;＆nbsp; 0.364±0.002 &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>;  - >; &lt;p>;最后，我们观察到通过扩张器创建小直径的覆盖层的Expomper，具有有效学习长期依赖性的能力。 &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;长距离图基准&lt;/a>;＆nbsp;是五个图形学习数据集的套件。结果表明，基于Expostemer的模型的表现优于标准GraphGPS模型（在发布时，在五个数据集中的四分之四是最先进的模型）。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>;图形变压器已成为ML的重要体系结构，该体系结构适应了NLP中使用的非常成功的基于序列的变压器对图形结构化数据。然而，事实证明，可伸缩性在启用具有大图的数据集上使用图形变压器是一个重大挑战。 In this post, we have presented Exphormer, a sparse attention framework that uses expander graphs to improve scalability of graph transformers. Exphormer is shown to have important theoretical properties and exhibit strong empirical performance, particularly on datasets where it is crucial to learn long range dependencies.有关更多信息，我们将读者指向简短的演示&lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;视频&lt;/a>;来自ICML2023。&lt;/p>; &lt;br/ >; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢我们的研究合作者Hamed Shirzad和Danica J. Sutherland的不列颠哥伦比亚大学以及Google Research的Ali Kemal Sinop。特别感谢Tom Small创建了本文中使用的动画。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =” =“回复” title =“ post注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/01/exphormer-scaling-transformer-transformers-for.html# comment-form“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://wwwww.blogger.com/feeds/847492633145202626/ =“ edit” type =“ application/atom+xml”/>; &lt;link href =“ http://www.blogger.com/feeds/feeds/847492633145202626/posts/posts/posts/default/14187365826582601940076 ATOM+XML“/>; &lt;link href =” http://blog.research.google/2024/01/exphormer-scaling-transformers-forers-for.html“ rel =” rel =“替代” title =“ exphormer结构化数据“ type =” text/html”/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147752666161 &lt;/uri>; .com &lt;/email>; &lt;gd：image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” -rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd -7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-220849549986709693&lt;/id>;&lt;published>;2024-01-18T10:03 :00.000-08:00&lt;/published>;&lt;updated>;2024-01-18T10:03:43.608-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =“大语言模型”>; &lt;/category>; &lt;title type =“ text”>;在llms &lt;/stitle &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;发布by Jiefeng Chen, Student Researcher, and Jinsung Yoon, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_- o4spk8wye/s320/aspire％20HERO.jpg“ style =” display：none;” />; &lt;p>;在人工智能的快速发展景观中，大语言模型（LLM）彻底改变了我们与机器互动的方式，将自然语言理解和发电的界限推向了前所未有的高度。然而，高风险决策应用程序的飞跃仍然太宽了，这主要是由于模型预测的固有不确定性。 Traditional LLMs generate responses recursively, yet they lack an intrinsic mechanism to assign a confidence score to these responses.尽管可以通过在序列中概括单个令牌的概率来得出置信度得分，但传统方法通常在可靠地区分正确和错误的答案方面缺乏。但是，如果LLM可以衡量自己的信心，并且只能在确定的情况下做出预测怎么办？在预测&lt;/a>;旨在通过使LLMS和选择分数一起输出答案来做到这一点，这表明答案是正确的概率。通过选择性预测，可以更好地了解部署在各种应用程序中的LLM的可靠性。先前的研究，例如&lt;a href=&quot;https://openreview.net/pdf?id=vd-ytp0dve&quot;>;语义不确定性&lt;/a>;和&lt;a href =“ https://arxiv.org/arxiv.org/pdf/pdf/2207.052211.052211.0522111 .pdf“>;自我评估&lt;/a>;，试图在LLMS中启用选择性预测。一种典型的方法是使用启发式提示，例如“提议的答案是对还是错？” to trigger self-evaluation in LLMs.但是，这种方法在挑战&lt;a href=&quot;https://en.wikipedia.org/wiki/question_answering&quot;>;问题答案&lt;/a>;（QA）任务方面可能不佳。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s1999/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1534&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;OPT-2.7B&lt;/a>; model incorrectly answers a question from the &lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;>;TriviaQA&lt;/a>; dataset: “Which vitamin helps regulate blood clotting?” with “Vitamin C”. Without selective prediction, LLMs may output the wrong answer which, in this case, could lead users to take the wrong vitamin. With selective prediction, LLMs will output an answer along with a selection score. If the selection score is low (0.1), LLMs will further output “I don&#39;t know!” to warn users not to trust it or verify it using other sources.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In &quot;&lt;a href=&quot;https://aclanthology.org/2023.findings-emnlp.345.pdf&quot;>;Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs&lt;/a>;&quot;, presented at &lt;a href=&quot;https://2023.emnlp.org/program/accepted_findings/&quot;>;Findings of EMNLP 2023&lt;/a>;, we introduce ASPIRE — a novel framework meticulously designed to enhance the selective prediction capabilities of LLMs. ASPIRE fine-tunes LLMs on QA tasks via &lt;a href=&quot;https://huggingface.co/blog/peft&quot;>;parameter-efficient fine-tuning, &lt;/a>;and trains them to evaluate whether their generated answers are correct. ASPIRE allows LLMs to output an answer along with a confidence score for that answer. Our experimental results demonstrate that ASPIRE significantly outperforms state-of-the-art selective prediction methods on a variety of QA datasets, such as the &lt;a href=&quot;https://aclanthology.org/Q19-1016.pdf&quot;>;CoQA benchmark &lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The mechanics of ASPIRE&lt;/h2>; &lt;p>; Imagine teaching an LLM to not only answer questions but also evaluate those answers — akin to a student verifying their answers in the back of the textbook. That&#39;s the essence of ASPIRE, which involves three stages: (1) task-specific tuning, (2) answer sampling, and (3) self-evaluation learning. &lt;/p>; &lt;p>; &lt;strong>;Task-specific tuning&lt;/strong>;: ASPIRE performs task-specific tuning to train adaptable parameters (θ&lt;sub>;p&lt;/sub>;) while freezing the LLM. Given a training dataset for a generative task, it fine-tunes the pre-trained LLM to improve its prediction performance. Towards this end, parameter-efficient tuning techniques (eg, &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;soft prompt tuning&lt;/a>; and &lt;a href=&quot;https://openreview.net/forum?id=nZeVKeeFYf9&quot;>;LoRA&lt;/a>;) might be employed to adapt the pre-trained LLM on the task, given their effectiveness in obtaining strong generalization with small amounts of target task data. Specifically, the LLM parameters (θ) are frozen and adaptable parameters (θ&lt;sub>;p&lt;/sub>;) are added for fine-tuning. Only θ&lt;sub>;p&lt;/sub>; are updated to minimize the standard LLM training loss (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_minimization&quot;>;cross-entropy&lt;/a>;). Such fine-tuning can improve selective prediction performance because it not only improves the prediction accuracy, but also enhances the likelihood of correct output sequences. &lt;/p>; &lt;p>; &lt;strong>;Answer sampling&lt;/strong>;: After task-specific tuning, ASPIRE uses the LLM with the learned θ&lt;sub>;p&lt;/sub>; to generate different answers for each training question and create a dataset for self-evaluation learning. We aim to generate output sequences that have a high likelihood. We use &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;>;beam search&lt;/a>; as the decoding algorithm to generate high-likelihood output sequences and the &lt;a href=&quot;https://aclanthology.org/P04-1077/&quot;>;Rouge-L&lt;/a>; metric to determine if the generated output sequence is correct. &lt;/p>; &lt;p>; &lt;strong>;Self-evaluation learning&lt;/strong>;: After sampling high-likelihood outputs for each query, ASPIRE adds adaptable parameters (θ&lt;sub>;s&lt;/sub>;) and only fine-tunes θ&lt;sub>;s&lt;/sub>; for learning self-evaluation. Since the output sequence generation only depends on θ and θ&lt;sub>;p&lt;/sub>;, freezing θ and the learned θ&lt;sub>;p&lt;/sub>; can avoid changing the prediction behaviors of the LLM when learning self-evaluation. We optimize θ&lt;sub>;s&lt;/sub>; such that the adapted LLM can distinguish between correct and incorrect answers on their own. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;933&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The three stages of the ASPIRE framework. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In the proposed framework, θ&lt;sub>;p&lt;/sub>; and θ&lt;sub>;s&lt;/sub>; can be trained using any parameter-efficient tuning approach. In this work, we use &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;soft prompt tuning&lt;/a>;, a simple yet effective mechanism for learning “&lt;a href=&quot; https://blog.research.google/2022/02/guiding-frozen-language-models-with.html&quot;>;soft prompts&lt;/a>;” to condition frozen language models to perform specific downstream tasks more effectively than traditional discrete text提示。 The driving force behind this approach lies in the recognition that if we can develop prompts that effectively stimulate self-evaluation, it should be possible to discover these prompts through soft prompt tuning in conjunction with targeted training objectives. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTErsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s800/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;800&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTErsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Implementation of the ASPIRE framework via soft prompt tuning. We first generate the answer to the question with the first soft prompt and then compute the learned self-evaluation score with the second soft prompt.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After training θ&lt;sub>;p&lt;/sub>; and θ&lt;sub>;s&lt;/sub>;, we obtain the prediction for the query via beam search decoding. We then define a selection score that combines the likelihood of the generated answer with the learned self-evaluation score (ie, the likelihood of the prediction being correct for the query) to make selective predictions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To demonstrate ASPIRE&#39;s efficacy, we evaluate it across three question-answering datasets — &lt;a href=&quot;https://aclanthology.org/Q19-1016/&quot;>;CoQA&lt;/a>;, &lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;>;TriviaQA&lt;/a>;, and &lt;a href=&quot;https://aclanthology.org/D16-1264/&quot;>;SQuAD&lt;/a>; — using various &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;open pre-trained transformer&lt;/a>; (OPT) models. By training θ&lt;sub>;p&lt;/sub>; with soft prompt tuning, we observed a substantial hike in the LLMs&#39; accuracy. For example, the &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;OPT-2.7B&lt;/a>; model adapted with ASPIRE demonstrated improved performance over the larger, pre-trained OPT-30B model using the CoQA and SQuAD datasets. These results suggest that with suitable adaptations, smaller LLMs might have the capability to match or potentially surpass the accuracy of larger models in some scenarios. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz -Ei4BrrW-e9Wzc/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1500&quot; data-original-width= &quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz-Ei4BrrW-e9Wzc/s16000/image4.png&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; When delving into the computation of selection scores with fixed model predictions, ASPIRE received a higher &lt;a href=&quot;https://openreview.net /pdf?id=VD-AYtP0dve&quot;>;AUROC&lt;/a>; score (the probability that a randomly chosen correct output sequence has a higher selection score than a randomly chosen incorrect output sequence) than baseline methods across all datasets. For example, on the CoQA benchmark, ASPIRE improves the AUROC from 51.3% to 80.3% compared to the baselines. &lt;/p>; &lt;p>; An intriguing pattern emerged from the TriviaQA dataset evaluations. While the pre-trained OPT-30B model demonstrated higher baseline accuracy, its performance in selective prediction did not improve significantly when traditional self-evaluation methods — &lt;a href=&quot;https://arxiv.org/abs/2207.05221&quot;>;Self-eval and P(True)&lt;/a>; — were applied. In contrast, the smaller OPT-2.7B model, when enhanced with ASPIRE, outperformed in this aspect. This discrepancy underscores a vital insight: larger LLMs utilizing conventional self-evaluation techniques may not be as effective in selective prediction as smaller, ASPIRE-enhanced models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s1999/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1599&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our experimental journey with ASPIRE underscores a pivotal shift in the landscape of LLMs: The capacity of a language model is not the be-all and end-all of its performance. Instead, the effectiveness of models can be drastically improved through strategic adaptations, allowing for more precise, confident predictions even in smaller models. As a result, ASPIRE stands as a testament to the potential of LLMs that can judiciously ascertain their own certainty and decisively outperform larger counterparts in selective prediction tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In conclusion, ASPIRE is not just another framework; it&#39;s a vision of a future where LLMs can be trusted partners in decision-making. By honing the selective prediction performance, we&#39;re inching closer to realizing the full potential of AI in critical applications. &lt;/p>; &lt;p>; Our research has opened new doors, and we invite the community to build upon this foundation. We&#39;re excited to see how ASPIRE will inspire the next generation of LLMs and beyond. To learn more about our findings, we encourage you to read our paper and join us in this thrilling journey towards creating a more reliable and self-aware AI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the contributions of Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, and Somesh Jha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/220849549986709693/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html&quot; rel=&quot;alternate&quot; title=&quot;Introducing ASPIRE for selective prediction in LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_-o4sPK8wYe/s72-c/ASPIRE%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1800430129205268706&lt;/id>;&lt;published>;2024-01-12T09:04:00.000-08:00&lt;/published>;&lt;updated>;2024-01-16T10:55:15.626-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AMIE: A research AI system for diagnostic medical reasoning and conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alan Karthikesalingam and Vivek Natarajan, Research Leads, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s16000/AMIE.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; The physician-patient conversation is a cornerstone of medicine, in which skilled and intentional communication drives diagnosis, management, empathy and trust. AI systems capable of such diagnostic dialogues could increase availability, accessibility, quality and consistency of care by being useful conversational partners to clinicians and patients alike. But approximating clinicians&#39; considerable expertise is a significant challenge. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Recent progress in large language models (LLMs) outside the medical domain has shown that they can plan, reason, and use relevant context to hold rich conversations. However, there are many aspects of good diagnostic dialogue that are unique to the medical domain. An effective clinician takes a complete “clinical history” and asks intelligent questions that help to derive a differential diagnosis. They wield considerable skill to foster an effective relationship, provide information clearly, make joint and informed decisions with the patient, respond empathically to their emotions, and support them in the next steps of care. While LLMs can accurately perform tasks such as medical summarization or answering medical questions, there has been little work specifically aimed towards developing these kinds of conversational diagnostic capabilities. &lt;/p>; &lt;p>; Inspired by this challenge, we developed &lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot;>;Articulate Medical Intelligence Explorer (AMIE)&lt;/a>;, a research AI system based on a LLM and optimized for diagnostic reasoning and conversations. We trained and evaluated AMIE along many dimensions that reflect quality in real-world clinical consultations from the perspective of both clinicians and patients. To scale AMIE across a multitude of disease conditions, specialties and scenarios, we developed a novel self-play based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE&#39;s diagnostic accuracy and conversation quality. Finally, we tested AMIE prospectively in real examples of multi-turn dialogue by simulating consultations with trained actors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s1200/AMIE%20GIF%201%20v2.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s16000/AMIE%20GIF%201%20v2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;AMIE was optimized for diagnostic conversations, asking questions that help to reduce its uncertainty and improve diagnostic accuracy, while also balancing this with other requirements of effective clinical communication, such as empathy, fostering a relationship, and providing information clearly.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation of conversational diagnostic AI&lt;/h2>; &lt;p>; Besides developing and optimizing AI systems themselves for diagnostic conversations, how to assess such systems is also an open question. Inspired by accepted tools used to measure consultation quality and clinical communication skills in real-world settings, we constructed a pilot evaluation rubric to assess diagnostic conversations along axes pertaining to history-taking, diagnostic accuracy, clinical management, clinical communication skills, relationship fostering and共情。 &lt;/p>; &lt;p>; We then designed a randomized, double-blind crossover study of text-based consultations with validated patient actors interacting either with board-certified primary care physicians (PCPs) or the AI system optimized for diagnostic dialogue. We set up our consultations in the style of an &lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structured_clinical_examination&quot;>;objective structured clinical examination&lt;/a>; (OSCE), a practical assessment commonly used in the real world to examine clinicians&#39; skills and competencies in a standardized and objective way. In a typical OSCE, clinicians might rotate through multiple stations, each simulating a real-life clinical scenario where they perform tasks such as conducting a consultation with a standardized patient actor (trained carefully to emulate a patient with a particular condition). Consultations were performed using a synchronous text-chat tool, mimicking the interface familiar to most consumers using LLMs today. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/s1350/image4.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1350&quot; height=&quot;568&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/w640-h568/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE is a research AI system based on LLMs for diagnostic reasoning and dialogue.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;h2>;AMIE: an LLM-based conversational diagnostic research AI system &lt;/h2>; &lt;p>; We trained AMIE on real-world datasets comprising medical reasoning, medical summarization and real-world clinical conversations. &lt;/p>; &lt;p>; It is feasible to train LLMs using real-world dialogues developed by passively collecting and transcribing in-person clinical visits, however, two substantial challenges limit their effectiveness in training LLMs for medical conversations. First, existing real-world data often fails to capture the vast range of medical conditions and scenarios, hindering the scalability and comprehensiveness. Second, the data derived from real-world dialogue transcripts tends to be noisy, containing ambiguous language (including slang, jargon, humor and sarcasm), interruptions, ungrammatical utterances, and implicit references. &lt;/p>; &lt;p>; To address these limitations, we designed a self-play based simulated learning environment with automated feedback mechanisms for diagnostic medical dialogue in a virtual care setting, enabling us to scale AMIE&#39;s knowledge and capabilities across many medical conditions and contexts 。 We used this environment to iteratively fine-tune AMIE with an evolving set of simulated dialogues in addition to the static corpus of real-world data described. &lt;/p>; &lt;p>; This process consisted of two self-play loops: (1) an “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient simulator; and (2) an “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a virtuous continuous learning cycle. &lt;/p>; &lt;p>; Further, we also employed an inference time chain-of-reasoning strategy which enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s1400/image1.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/s1834/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;br />; &lt;p>; We tested performance in consultations with simulated patients (played by trained actors), compared to those performed by 20 real PCPs using the randomized approach described above. AMIE and PCPs were assessed from the perspectives of both specialist attending physicians and our simulated patients in a randomized, blinded crossover study that included 149 case scenarios from OSCE providers in Canada, the UK and India in a diverse range of specialties and diseases. &lt;/p>; &lt;p>; Notably, our study was not designed to emulate either traditional in-person OSCE evaluations or the ways clinicians usually use text, email, chat or telemedicine. Instead, our experiment mirrored the most common way consumers interact with LLMs today, a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s1999 /image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the randomized study design to perform a virtual remote OSCE with simulated patients via online multi-turn synchronous text chat.&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Performance of AMIE&lt;/h2>; &lt;p>; In this setting, we observed that AMIE performed simulated diagnostic conversations at least as well as PCPs when both were evaluated along multiple clinically-meaningful axes of consultation quality. AMIE had greater diagnostic accuracy and superior performance for 28 of 32 axes from the perspective of specialist physicians, and 24 of 26 axes from the perspective of patient actors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/s1834/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE outperformed PCPs on multiple evaluation axes for diagnostic dialogue in our evaluations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s1999/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Specialist-rated top-k diagnostic accuracy. AMIE and PCPs top-k differential diagnosis (DDx) accuracy are compared across 149 scenarios with respect to the ground truth diagnosis (a) and all diagnoses listed within the accepted differential diagnoses (b). Bootstrapping (n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with p &amp;lt;0.05 after&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/False_discovery_rate&quot;>;false discovery rate&lt;/a>;&amp;nbsp;(FDR) correction.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1864&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic conversation and reasoning qualities as assessed by specialist physicians. On 28 out of 32 axes, AMIE outperformed PCPs while being comparable on the rest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s1892/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1714&quot; data-original-width=&quot;1892&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic conversation and reasoning qualities as assessed by specialist physicians. On 28 out of 32 axes, AMIE outperformed PCPs while being comparable on the rest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;br />; &lt;h2>;Limitations&lt;/h2>; &lt;p>; Our research has several limitations and should be interpreted with appropriate caution. Firstly, our evaluation technique likely underestimates the real-world value of human conversations, as the clinicians in our study were limited to an unfamiliar text-chat interface, which permits large-scale LLM–patient interactions but is not representative of usual clinical practice. Secondly, any research of this type must be seen as only a first exploratory step on a long journey. Transitioning from a LLM research prototype that we evaluated in this study to a safe and robust tool that could be used by people and those who provide care for them will require significant additional research. There are many important limitations to be addressed, including experimental performance under real-world constraints and dedicated exploration of such important topics as health equity and fairness, privacy, robustness, and many more, to ensure the safety and reliability of the technology. &lt;/p>; &lt;br />; &lt;h2>;AMIE as an aid to clinicians&lt;/h2>; &lt;p>; In a &lt;a href=&quot;https://arxiv.org/abs/2312.00164&quot;>;recently released preprint&lt;/a>;, we evaluated the ability of an earlier iteration of the AMIE system to generate a DDx alone or as an aid to clinicians. Twenty (20) generalist clinicians evaluated 303 challenging, real-world medical cases sourced from the &lt;em>;&lt;a href=&quot;https://www.nejm.org/&quot;>;New England Journal of Medicine&lt;/a>;&lt;/em>; (NEJM) &lt;a href=&quot;https://www.nejm.org/case-challenges&quot;>;ClinicoPathologic Conferences&lt;/a>; (CPCs). Each case report was read by two clinicians randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or AMIE assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s1999/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1022&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Assisted randomized reader study setup to investigate the assistive effect of AMIE to clinicians in solving complex diagnostic case challenges from the New England Journal of Medicine.&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% vs. 33.6%, p= 0.04). Comparing the two assisted study arms, the top-10 accuracy was higher for clinicians assisted by AMIE, compared to clinicians without AMIE assistance (24.6%, p&amp;lt;0.01) and clinicians with search (5.45%, p=0.02). Further, clinicians assisted by AMIE arrived at more comprehensive differential lists than those without AMIE assistance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/s1999/image7.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1424&quot; data-original-width=&quot;1999&quot; height=&quot;456&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/w640-h456/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In addition to strong standalone performance, using the AMIE system led to significant assistive effect and improvements in diagnostic accuracy of the clinicians in solving these complex case challenges.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; It&#39;s worth noting that NEJM CPCs are not representative of everyday clinical practice. They are unusual case reports in only a few hundred individuals so offer limited scope for probing important issues like equity or fairness. &lt;/p>; &lt;br />; &lt;h2>;Bold and responsible research in healthcare — the art of the possible &lt;/h2>; &lt;p>; Access to clinical expertise remains scarce around the world. While AI has shown great promise in specific clinical applications, engagement in the dynamic, conversational diagnostic journeys of clinical practice requires many capabilities not yet demonstrated by AI systems. Doctors wield not only knowledge and skill but a dedication to myriad principles, including safety and quality, communication, partnership and teamwork, trust, and professionalism. Realizing these attributes in AI systems is an inspiring challenge that should be approached responsibly and with care. AMIE is our exploration of the “art of the possible”, a research-only system for safely exploring a vision of the future where AI systems might be better aligned with attributes of the skilled clinicians entrusted with our care. It is early experimental-only work, not a product, and has several limitations that we believe merit rigorous and extensive further scientific studies in order to envision a future in which conversational, empathic and diagnostic AI systems might become safe, helpful and accessible. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The research described here is joint work across many teams at Google Research and Google Deepmind. We are grateful to all our co-authors - Tao Tu, Mike Schaekermann, Anil Palepu, Daniel McDuff, Jake Sunshine, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Sara Mahdavi, Karan Sighal, Shekoofeh Azizi, Nenad Tomasev, Yun Liu, Yong Cheng, Le Hou, Albert Webson, Jake Garrison, Yash Sharma, Anupam Pathak, Sushant Prakash, Philip Mansfield, Shwetak Patel, Bradley Green, Ewa Dominowska, Renee Wong, Juraj Gottweis, Dale Webster, Katherine Chou, Christopher Semturs, Joelle Barral, Greg Corrado and Yossi Matias. We also thank Sami Lachgar, Lauren Winer and John Guilyard for their support with narratives and the visuals. Finally, we are grateful to Michael Howell, James Manyika, Jeff Dean, Karen DeSalvo, Zoubin Ghahramani&amp;nbsp;and Demis Hassabis for their support during the course of this project&lt;/em>;. &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1800430129205268706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html&quot; rel=&quot;alternate&quot; title=&quot;AMIE: A research AI system for diagnostic medical reasoning and conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s72-c/AMIE.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7998118785777164574&lt;/id>;&lt;published>;2024-01-11T14:42:00.000-08:00&lt;/published>;&lt;updated>;2024-01-11T14:42:51.944-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Can large language models identify and correct their mistakes?&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Gladys Tyen, Intern, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s1600/Backtracking.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; LLMs are increasingly popular for reasoning tasks, such as &lt;a href=&quot;https://hotpotqa.github.io/&quot;>;multi-turn QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2207.01206&quot;>;task completion&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2304.05128&quot;>;code generation&lt;/a>;, or &lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>;mathematics&lt;/a>;. Yet much like people, they do not always solve problems correctly on the first try, especially on tasks for which they were not trained. Therefore, for such systems to be most useful, they should be able to 1) identify where their reasoning went wrong and 2) backtrack to find another solution. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; This has led to a surge in methods related to &lt;em>;self-correction&lt;/em>;, where an LLM is used to identify problems in its own output, and then produce improved results based on the feedback. Self-correction is generally thought of as a single process, but we decided to break it down into two components, &lt;em>;mistake finding&lt;strong>; &lt;/strong>;&lt;/em>;and &lt;em>;output correction&lt;/em>;. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.08516#:~:text=While%20self%2Dcorrection%20has%20shown,et%20al.%2C%202023).&quot;>;LLMs cannot find reasoning errors, but can correct them!&lt;/a>;”, we test state-of-the-art LLMs on mistake finding and output correction separately. We present &lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake&quot;>;BIG-Bench Mistake&lt;/a>;, an evaluation benchmark dataset for mistake identification, which we use to address the following questions: &lt;/p>; &lt;ol>; &lt;li>;Can LLMs find logical mistakes in &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; (CoT) style reasoning? &lt;/li>;&lt;li>;Can mistake-finding be used as a proxy for correctness? &lt;/li>;&lt;li>;Knowing where the mistake is, can LLMs then be prompted to backtrack and arrive at the correct answer? &lt;/li>;&lt;li>;Can mistake finding as a skill generalize to tasks the LLMs have never seen? &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;About our dataset&lt;/h2>; &lt;p>; Mistake finding is an underexplored problem in natural language processing, with a particular lack of evaluation tasks in this domain. To best assess the ability of LLMs to find mistakes, evaluation tasks should exhibit mistakes that are non-ambiguous. To our knowledge, most current mistake-finding datasets do not go beyond the realm of &lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>;mathematics&lt;/a>; for this reason. &lt;/p>; &lt;p>; To assess the ability of LLMs to reason about mistakes outside of the math domain, we produce a new dataset for use by the research community, called&lt;strong>; &lt;/strong>;&lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake&quot;>;BIG-Bench Mistake&lt;/a>;. This dataset consists of Chain-of-Thought traces generated using &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; on five tasks in &lt;a href=&quot;https://github.com/suzgunmirac/BIG-Bench-Hard&quot;>;BIG-Bench&lt;/a>;. Each trace is annotated with the location of the first logical mistake. &lt;/p>; &lt;p>; To maximize the number of mistakes in our dataset, we sample 255 traces where the answer is incorrect (so we know there is definitely a mistake), and 45 traces where the answer is correct (so there may or may not be a mistake). We then ask human labelers to go through each trace and identify the first mistake step. Each trace has been annotated by at least three labelers, whose answers had &lt;a href=&quot;https://en.wikipedia.org/wiki/Inter-rater_reliability&quot;>;inter-rater reliability&lt;/a>; levels of &amp;gt;0.98 (using &lt;a href=&quot;https://en.wikipedia.org/wiki/Krippendorff%27s_alpha&quot;>;Krippendorff&#39;s α&lt;/a>;). The labeling was done for all tasks except the &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/dyck_languages&quot;>;Dyck Languages task&lt;/a>;, which involves predicting the sequence of closing parentheses for a given input sequence. This task we labeled algorithmically. &lt;/p>; &lt;p>; The logical errors made in this dataset are simple and unambiguous, providing a good benchmark for testing an LLM&#39;s ability to find its own mistakes before using them on harder, more ambiguous tasks.&lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s1354/BBMistakes2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;410&quot; data-original-width=&quot;1354&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s16000/BBMistakes2.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;Core questions about mistake identification&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;1. Can LLMs find logical mistakes in Chain-of-Thought style reasoning?&lt;/h3>; &lt;p>; First, we want to find out if LLMs can identify mistakes independently of their ability to correct them. We attempt multiple prompting methods to test &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>; series models for their ability to locate mistakes (prompts &lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake/tree/main/mistake_finding_prompts&quot;>;here&lt;/a>;) under the assumption that they are generally representative of modern LLM performance. &lt;/p>; &lt;p>; Generally, we found these state-of-the-art models perform poorly, with the best model achieving 52.9% accuracy overall. Hence, there is a need to improve LLMs&#39; ability in this area of reasoning. &lt;/p>; &lt;p>; In our experiments, we try three different prompting methods: direct (trace), direct (step) and CoT (step). In direct (trace), we provide the LLM with the trace and ask for the location step of the mistake or &lt;em>;no mistake&lt;/em>;. In direct (step), we prompt the LLM to ask itself this question for each step it takes. In CoT (step), we prompt the LLM to give its reasoning for whether each step is a mistake or not a mistake. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s1061/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;1061&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;A diagram showing the three prompting methods direct (trace), direct (step) and CoT (step).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our finding is in line and builds upon &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;prior results&lt;/a>;, but goes further in showing that LLMs struggle with even simple and unambiguous mistakes (for comparison , our human raters without prior expertise solve the problem with a high degree of agreement). We hypothesize that this is a big reason why LLMs are unable to self-correct reasoning errors. See &lt;a href=&quot;https://arxiv.org/abs/2311.08516&quot;>;the paper&lt;/a>; for the full results. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;2. Can mistake-finding be used as a proxy for correctness of the answer?&lt;/h3>; &lt;p>; When people are confronted with a problem where we are unsure of the answer, we can work through our solutions step-by-step. If no error is found, we can make the assumption that we did the right thing. &lt;/p>; &lt;p>; While we hypothesized that this would work similarly for LLMs, we discovered that this is a poor strategy. On our dataset of 85% incorrect traces and 15% correct traces, using this method is not much better than the naïve strategy of always labeling traces as incorrect, which gives a weighted average &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1&lt;/a>; of 78. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s698/Self-correcting-LLMs-Tasks.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;698&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s16000/Self-correcting-LLMs-Tasks.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing how well mistake-finding with LLMs can be used as a proxy for correctness of the answer on each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;3. Can LLMs backtrack knowing where the error is?&lt;/h3>; &lt;p>; Since we&#39;ve shown that LLMs exhibit poor performance in finding reasoning errors in CoT traces, we want to know whether LLMs can even correct errors &lt;em>;at all&lt;/em>;, even if they know where the error is. &lt;/p>; &lt;p>; Note that knowing the &lt;em>;mistake location&lt;/em>; is different from knowing &lt;em>;the right answer&lt;/em>;: CoT traces can contain logical mistakes even if the final answer is correct, or反之亦然。 In most real-world situations, we won&#39;t know what the right answer is, but we might be able to identify logical errors in intermediate steps. &lt;/p>; &lt;p>; We propose the following backtracking method: &lt;/p>; &lt;ol>; &lt;li>;Generate CoT traces as usual, at temperature = 0. (Temperature is a parameter that controls the randomness of generated responses, with higher values producing more diverse and creative outputs, usually at the expense of quality.) &lt;/li>;&lt;li>;Identify the location of the first logical mistake (for example with a classifier, or here we just use labels from our dataset). &lt;/li>;&lt;li>;Re-generate the mistake step at temperature = 1 and produce a set of eight outputs. Since the original output is known to lead to incorrect results, the goal is to find an alternative generation at this step that is significantly different from the original. &lt;/li>;&lt;li>;From these eight outputs, select one that is different from the original mistake step. (We just use exact matching here, but in the future this can be something more sophisticated.) &lt;/li>;&lt;li>;Using the new step, generate the rest of the trace as normal at temperature = 0. &lt;/li>; &lt;/ol>; &lt;p>; It&#39;s a very simple method that does not require any additional prompt crafting and avoids having to re-generate the entire trace. We test it using the mistake location data from BIG-Bench Mistake, and we find that it can correct CoT errors. &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;Recent work&lt;/a>; showed that self-correction methods, like &lt;a href=&quot;https://arxiv.org/abs/2303.11366&quot;>;Reflexion&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2303.17491&quot;>;RCI&lt;/a>;, cause deterioration in accuracy scores because there are more correct answers becoming incorrect than vice versa. Our method, on the other hand, produces more gains (by correcting wrong answers) than losses (by changing right answers to wrong answers). &lt;/p>; &lt;p>; We also compare our method with a random baseline, where we randomly assume a step to be a mistake. Our results show that this random baseline does produce some gains, but not as much as backtracking with the correct mistake location, and with more losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s744/image4.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;744&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;A diagram showing the gains and losses in accuracy for our method as well as a random baseline on each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;4. Can mistake finding generalize to tasks the LLMs have never seen?&lt;/h3>; &lt;p>; To answer this question, we fine-tuned a small model on four of the BIG-Bench tasks and tested it on the fifth, held-out task 。 We do this for every task, producing five fine-tuned models in total. Then we compare the results with just zero-shot prompting &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-L-Unicorn&lt;/a>;, a much larger model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s759/image5.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;759&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Bar chart showing the accuracy improvement of the fine-tuned small model compared to zero-shot prompting with PaLM 2-L-Unicorn.&lt; /span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results show that the much smaller fine-tuned reward model generally performs better than zero-shot prompting a large model, even though the reward model has never seen data from the task in the test set. The only exception is logical deduction, where it performs on par with zero-shot prompting. &lt;/p>; &lt;p>; This is a very promising result as we can potentially just use a small fine-tuned reward model to perform backtracking and improve accuracy on any task, even if we don&#39;t have the data for it. This smaller reward model is completely independent of the generator LLM, and can be updated and further fine-tuned for individual use cases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s867/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;867&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;An illustration showing how our backtracking method works.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we created an evaluation benchmark dataset that the wider academic community can use to evaluate future LLMs. We further showed that LLMs currently struggle to find logical errors. However, if they could, we show the effectiveness of backtracking as a strategy that can provide gains on tasks. Finally, a smaller reward model can be trained on general mistake-finding tasks and be used to improve out-of-domain mistake finding, showing that mistake-finding can generalize. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Thank you to Peter Chen, Tony Mak, Hassan Mansoor and Victor Cărbune for contributing ideas and helping with the experiments and data collection. We would also like to thank Sian Gooding and Vicky Zayats for their comments and suggestions on the paper.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7998118785777164574/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and.html&quot; rel=&quot;alternate&quot; title=&quot;Can large language models identify and correct their mistakes?&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s72-c/Backtracking.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7197275876457161088&lt;/id>;&lt;published>;2024-01-08T14:07:00.000-08:00&lt;/published>;&lt;updated>;2024-01-19T09:59:57.076-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: User Experience Team&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ayça Çakmakli, UX Lead, Google Research, Responsible AI and Human Centered Technology Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s16000/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google&#39;s Responsible AI User Experience (Responsible AI UX) team is a product-minded team embedded within Google Research. This unique positioning requires us to apply responsible AI development practices to our user-centered user experience (UX) design process. In this post, we describe the importance of UX design and responsible AI in product development, and share a few examples of how our team&#39;s capabilities and cross-functional collaborations have led to responsible development across Google. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; First, the UX part. We are a multi-disciplinary team of product design experts: designers, engineers, researchers, and strategists who manage the user-centered UX design process from early-phase ideation and problem framing to later-phase user-interface (UI) design, prototyping和改进。 We believe that effective product development occurs when there is clear alignment between significant unmet user needs and a product&#39;s primary value proposition, and that this alignment is reliably achieved via a thorough user-centered UX design process. &lt;/p>; &lt;p>; And second, recognizing generative AI&#39;s (GenAI) potential to significantly impact society, we embrace our role as the primary user advocate as we continue to evolve our UX design process to meet the unique challenges AI poses, maximizing the benefits and minimizing the risks. As we navigate through each stage of an AI-powered product design process, we place a heightened emphasis on the ethical, societal, and long-term impact of our decisions. We contribute to the ongoing development of comprehensive &lt;a href=&quot;https://ai.google/responsibility/ai-governance-operations&quot;>;safety and inclusivity protocols&lt;/a>; that define design and deployment guardrails around key issues like content curation, security, privacy, model capabilities, model access, equitability, and fairness that help mitigate GenAI risks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s1920/image4. png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;Responsible AI UX is constantly evolving its user-centered product design process to meet the needs of a GenAI-powered product landscape with greater sensitivity to the needs of users and society and an emphasis on ethical, societal, and long-term impact.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Responsibility in product design is also reflected in the user and societal problems we choose to address and the programs we resource. Thus, we encourage the &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;prioritization of user problems with significant scale and severity&lt;/a>; to help maximize the positive impact of GenAI technology. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s1920 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; Communication across teams and disciplines is essential to responsible product design. The seamless flow of information and insight from user research teams to product design and engineering teams, and vice versa, is essential to good product development. One of our team&#39;s core objectives is to ensure the practical application of deep user-insight into AI-powered product design decisions at Google by bridging the communication gap between the vast technological expertise of our engineers and the user/societal expertise of our academics, research scientists, and user-centered design research experts. We&#39;ve built a multidisciplinary team with expertise in these areas, deepening our empathy for the communication needs of our audience, and enabling us to better interface between our user &amp;amp; society experts and our technical experts. We create frameworks, guidebooks, prototypes, cheatsheets, and multimedia tools to help bring insights to life for the right people at the right time.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s1920/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Facilitating responsible GenAI prototyping and development &lt;/h2>; &lt;p>; During collaborations between Responsible AI UX, the &lt;a href=&quot;https://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot;>;People + AI Research&lt;/a>; (PAIR) initiative and &lt;a href=&quot;https://labs.google/&quot;>;Labs&lt;/a>;, we identified that &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;prototyping&lt;/a>; can afford a creative opportunity to engage with large language models (LLM), and is often the first step in GenAI product development. To address the need to introduce LLMs into the prototyping process, we explored a range of different prompting designs. Then, we went out into the field, employing various external, first-person UX design research methodologies to draw out insight and gain empathy for the user&#39;s perspective. Through user/designer co-creation sessions, iteration, and prototyping, we were able to bring internal stakeholders, product managers, engineers, writers, sales, and marketing teams along to ensure that the user point of view was well understood and to reinforce alignment across teams. &lt;/p>; &lt;p>; The result of this work was &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a generative AI platform launched at &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;Google I/O 2023&lt;/a>; that enables people, even those without any ML experience, to prototype creatively using LLMs. The team&#39;s first-hand experience with users and understanding of the challenges they face allowed us to incorporate our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; into the MakerSuite product design 。 Product features like &lt;a href=&quot;https://developers.generativeai.google/guide/safety_setting&quot;>;safety filters&lt;/a>;, for example, enable users to manage outcomes, leading to easier and more responsible product development with MakerSuite. &lt;/p>; &lt;p>; Because of our close collaboration with product teams, we were able to adapt text-only prototyping to support multimodal interaction with &lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI Studio&lt;/a>;, an evolution of MakerSuite. Now, Google AI Studio enables developers and non-developers alike to seamlessly leverage Google&#39;s latest &lt;a href=&quot;https://ai.google.dev/&quot;>;Gemini&lt;/a>; model to merge multiple modality inputs, like text and image, in product explorations. Facilitating product development in this way provides us with the opportunity to better use AI to identify &lt;a href=&quot;https://arxiv.org/pdf/2310.15428.pdf&quot;>;appropriateness of outcomes&lt;/a>; and unlocks opportunities for developers and non-developers to play with AI sandboxes. Together with our partners, we continue to actively push this effort in the products we support.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1406&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI studio&lt;/a>; enables developers and non-developers to leverage Google Cloud infrastructure and merge multiple modality inputs in their product explorations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Equitable speech recognition&lt;/h2>; &lt;p>; Multiple &lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.1915768117&quot;>;external studies&lt;/a>;, as well as Google&#39;s &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frai.2021.725911/full&quot;>;own research,&lt;/a>; have identified an unfortunate deficiency in the ability of current speech recognition technology to understand Black speakers on average, relative to White speakers. As multimodal AI tools begin to rely more heavily on speech prompts, this problem will grow and continue to alienate users. To address this problem, the Responsible AI UX team is &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with world-renowned linguists and scientists at Howard University&lt;/a>;, a prominent &lt;a href=&quot;https://en.wikipedia.org/wiki/Historically_black_colleges_and_universities&quot; target=&quot;_blank&quot;>;HBCU&lt;/a>;, to build a high quality African-American English dataset to improve the design of our speech technology products to make them more accessible. Called Project Elevate Black Voices, this effort will allow Howard University to share the dataset with those looking to improve speech technology while establishing a framework for responsible data collection, ensuring the data benefits Black communities. Howard University will retain the ownership and licensing of the dataset and serve as stewards for its responsible use. At Google, we&#39;re providing funding support and collaborating closely with our partners at Howard University to ensure the success of this program. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/t_pdlrU8qhs?si=5xY1AoGc_d2HTzQf&quot; width=&quot;640&quot; youtube-src-id=&quot;5xY1AoGc_d2HTzQf&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Equitable computer vision&lt;/h2>; &lt;p>; The &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; project highlighted that computer vision systems struggle to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. This is largely due to the fact that the datasets used to train these models were not inclusive to a wide range of skin tones. To address this limitation, the Responsible AI UX team has been partnering with sociologist &lt;a href=&quot;https://www.ellismonk.com/&quot;>;Dr. Ellis Monk&lt;/a>; to release the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone Scale&lt;/a>; (MST), a skin tone scale designed to be more inclusive of the spectrum of skin tones around the world. It provides a tool to assess the inclusivity of datasets and model performance across an inclusive range of skin tones, resulting in features and products that work better for everyone. &lt;/p>; &lt;p>; We have integrated MST into a range of &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Google products&lt;/a>;, such as Search, Google Photos, and others. We also open sourced MST, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3632120&quot;>;published our research&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;described our annotation practices&lt;/a>;, and &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;shared an example dataset&lt;/a>; to encourage others to easily integrate it into their products. The Responsible AI UX team continues to collaborate with Dr. Monk, utilizing the MST across multiple product applications and continuing to do international research to ensure that it is globally inclusive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Consulting &amp;amp; guidance&lt;/h2>; &lt;p>; As teams across Google continue to develop products that leverage the capabilities of GenAI models, our team recognizes that the challenges they face are varied and that market competition is significant. To support teams, we develop actionable assets to facilitate a more streamlined and responsible product design process that considers available resources. We act as a product-focused design consultancy, identifying ways to scale services, share expertise, and apply our design principles more broadley. Our goal is to help all product teams at Google connect significant unmet user needs with technology benefits via great responsible product design. &lt;/p>; &lt;p>; One way we have been doing this is with the creation of the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>;, an evolving summative resource of many of the responsible design lessons we&#39;ve learned and recommendations we&#39;ve made for internal and external stakeholders. With its forthcoming, rolling &lt;a href=&quot;https://medium.com/people-ai-research/updating-the-people-ai-guidebook-in-the-age-of-generative-ai-cace6c846db4&quot;>;updates&lt;/a>; focusing specifically on how to best design and consider user needs with GenAI, we hope that our internal teams, external stakeholders, and larger community will have useful and actionable guidance at the most critical milestones in the product development journey. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s1100/image1.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;622&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;The People + AI Guidebook has six chapters, designed to cover different aspects of the product life cycle.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If you are interested in reading more about Responsible AI UX and how we are specifically thinking about designing responsibly with Generative AI, please check out this &lt;a href=&quot;https://medium.com/people-ai-research/meet-ay%C3%A7a-%C3%A7akmakli- googles-new-head-of-responsible-ai-ux-d8f2700df95b&quot;>;Q&amp;amp;A piece&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Shout out to our the Responsible AI UX team members: Aaron Donsbach, Alejandra Molina, Courtney Heldreth, Diana Akrong, Ellis Monk, Femi Olanubi, Hope Neveux, Kafayat Abdul, Key Lee, Mahima Pushkarna, Sally Limb, Sarah Post, Sures Kumar Thoddu Srinivasan, Tesh Goyal, Ursula Lauriston, and Zion Mengesha. Special thanks to Michelle Cohn for her contributions to this work. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7197275876457161088/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: User Experience Team&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;