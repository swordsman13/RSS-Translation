<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-04-25T17:21:24.479-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="datasets"></category><category term="Education"></category><category term="Neural Networks"></category><category term="Health"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="Computer Science"></category><category term="Quantum AI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Self-Supervised Learning"></category><category term="accessibility"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="NeurIPS"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Responsible AI"></category><category term="ACL"></category><category term="Audio"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ML Fairness"></category><category term="video"></category><category term="Awards"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Supervised Learning"></category><category term="User Experience"></category><category term="Collaboration"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="TTS"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Environment"></category><category term="Google Accelerated Science"></category><category term="Speech Recognition"></category><category term="DeepMind"></category><category term="Google Translate"></category><category term="Large Language Models"></category><category term="Video Analysis"></category><category term="statistics"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="UI"></category><category term="Vision Research"></category><category term="Acoustic Modeling"></category><category term="Interspeech"></category><category term="Systems"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Differential Privacy"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="crowd-sourcing"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Art"></category><category term="Biology"></category><category term="Climate"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Graphs"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="Weather"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1352&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1569605132526995799&lt;/id>;&lt;发布>;2024-03-29T11:03:00.000-07:00&lt;/发布>;&lt;更新>;2024-03- 29T11:03:10.261-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Climate&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;天气&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;生成人工智能来量化天气预报的不确定性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Lizao (Larry) Li, Google 研究部软件工程师和研究科学家 Rob Carver&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdl ATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif&quot;样式=&quot;显示：无；” />; &lt;p>; 准确的天气预报可以对人们的生活产生直接影响，从帮助做出日常决策（例如为一天的活动携带什么物品）到通知紧急行动（例如，在恶劣的天气条件下保护人们）。随着气候变化，准确及时的天气预报的重要性只会越来越大。认识到这一点，谷歌一直在投资天气和气候研究，以帮助确保未来的预报技术能够满足对可靠天气信息的需求。我们最近的一些创新包括 &lt;a href=&quot;https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot;>;MetNet-3&lt;/a>;、 Google 对未来长达 24 小时的高分辨率预测，以及 &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global- Weather-forecasting/&quot;>;GraphCast&lt;/a>;，一种可以预测最多未来 10 天天气的天气模型。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 天气本质上是随机的。为了量化不确定性，传统方法依靠基于物理的模拟来生成预测集合。然而，生成大型集合以便准确识别和表征罕见和极端天气事件的计算成本很高。考虑到这一点，我们很高兴地宣布我们旨在加速天气预报进展的最新创新，&lt;a href=&quot;https://www.science.org/doi/10.1126/sciadv.adk4489 &quot;>;可扩展的集成包络扩散采样器&lt;/a>; (SEEDS)，最近发表于&lt;em>;&lt;a href=&quot;https://www.science.org/journal/sciadv&quot;>;科学进展&lt;/a>;&lt;/em >;。 SEEDS 是一种生成式 AI 模型，可以有效地大规模生成天气预报集合，而成本仅为传统基于物理的预报模型的一小部分。这项技术为天气和气候科学开辟了新的机遇，它代表了概率扩散模型在天气和气候预测中的首批应用之一，概率扩散模型是媒体生成最新进展背后的一种生成人工智能技术。 &lt;/p>; &lt;br />; &lt;h2>;概率预测的必要性：蝴蝶效应&lt;/h2>; &lt;p>; 1972 年 12 月，&lt;a href=&quot;https://www.aaas.org/&quot;>;美国科学促进会&lt;/a>;在华盛顿召开会议，麻省理工学院气象学教授&lt;a href=&quot;https://en.wikipedia.org/wiki/Edward_Norton_Lorenz&quot;>;Ed Lorenz&lt;/a>;做了题为“ ，“巴西蝴蝶翅膀的扇动是否会在德克萨斯州引发龙卷风？”这促成了“&lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;蝴蝶效应&lt;/a>;”一词。他在 1963 年发表的具有里程碑意义的论文的基础上进行了研究，在该论文中，他研究了“超远程天气预报”的可行性，并描述了当与数值天气预报模型及时集成时，初始条件的误差如何呈指数级增长。这种指数误差增长（称为混沌）会导致确定性可预测性限制，从而限制了决策中个人预测的使用，因为它们无法量化天气条件固有的不确定性。在预测飓风、热浪或洪水等极端天气事件时，这一点尤其成问题。 &lt;/p>; &lt;p>; 认识到确定性预报的局限性，世界各地的气象机构都发布了概率预报。此类预测基于确定性预测的集合，每个预测都是通过在初始条件中包含合成噪声和物理过程中的随机性来生成的。利用天气模型中快速的误差增长率，集合中的预测有目的地不同：初始不确定性被调整以生成尽可能不同的运行，并且天气模型中的随机过程在模型运行期间引入了额外的差异。通过对集合中的所有预测进行平均来减轻误差的增长，并且预测集合中的可变性量化了天气条件的不确定性。 &lt;/p>; &lt;p>; 虽然有效，但生成这些概率预测的计算成本很高。它们需要在大型超级计算机上多次运行高度复杂的数值天气模型。因此，许多业务天气预报只能为每个预报周期生成约 10-50 个集合成员。对于担心罕见但影响大的天气事件的可能性的用户来说，这是一个问题，这些天气事件通常需要更大的集合来评估几天之后的情况。例如，需要一个由 10,000 名成员组成的集合来预测发生概率为 1% 的事件的可能性，相对误差小于 10%。量化此类极端事件的概率可能有用，例如对于应急管理准备或能源贸易商。 &lt;/p>; &lt;br />; &lt;h2>;SEEDS：人工智能驱动的进步&lt;/h2>; &lt;p>; 在上述&lt;a href=&quot;https://www.science.org/doi/10.1126/sciadv.adk4489&quot;中>;论文&lt;/a>;，我们提出了可扩展集合包络扩散采样器（SEEDS），这是一种用于天气预报集合生成的生成人工智能技术。 SEEDS 基于&lt;a href=&quot;https://blog.research.google/2021/07/high-fidelity-image- Generation-using.html&quot;>;去噪扩散概率&lt;/a>;模型，这是一种状态最先进的生成式人工智能方法部分由谷歌研究院首创。 &lt;/p>; &lt;p>; SEEDS 可以根据运行数值天气预报系统的一到两次预报生成大型集合。生成的集合不仅可以产生可信的类似真实天气的预测，而且在技能指标（例如排名直方图）方面也可以匹配或超过基于物理的集合&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;均方根误差&lt;/a>; (RMSE) 和 &lt;a href= “https://www.tandfonline.com/doi/abs/10.1198/016214506000001437&quot;>;连续排名概率得分&lt;/a>; (CRPS)。特别是，生成的集合为预测分布的尾部分配了更准确的可能性，例如 ±2σ 和 ±3σ 天气事件。最重要的是，与超级计算机进行预测所需的计算时间相比，该模型的计算成本可以忽略不计。它在 Google Cloud TPUv3-32 实例上每 3 分钟具有 256 个集成成员（分辨率为 2°）的吞吐量，并且可以通过部署更多加速器轻松扩展到更高的吞吐量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1 BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;1000&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3 PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;SEEDS 生成更多数量级的样本来填充天气模式的分布。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;生成合理的天气预报&lt;/h2>; &lt;p>;众所周知，生成式人工智能可以生成非常详细的图像和视频。此属性对于生成与可能的天气模式一致的集合预报特别有用，这最终为下游应用带来最大的附加值。正如 Lorenz 指出的那样，“他们制作的[天气预报]地图应该看起来像真实的天气地图。”下图将 SEEDS 的预测与美国正在运行的天气预报系统的预测进行了对比 (&lt;a href=&quot;https:// www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/gefs.php&quot;>;全球集合预报系统&lt;/a>;，GEFS）&lt;a href=&quot;https://en.wikipedia 期间的特定日期.org/wiki/2022_European_heatwaves&quot;>;2022 年欧洲热浪&lt;/a>;。我们还将结果与高斯模型的预测进行了比较，该模型预测每个位置每个大气场的单变量平均值和标准差，这是一种常见且计算高效的方法但不太复杂的数据驱动方法。这种高斯模型旨在表征逐点后处理的输出，它忽略相关性并将每个网格点视为独立的随机变量。相比之下，真实的天气图将具有详细的&lt;em>;。 &lt;/p>; &lt;p>; 由于 SEEDS 直接模拟大气状态的联合分布，因此它可以真实地捕获对流层中部位势与平均海平面压力之间的空间协方差和相关性。密切相关，天气预报员通常使用它们来评估和验证预报。平均海平面压力的梯度是驱动地表风的因素，而对流层中部位势的梯度则产生高层风，从而改变大规模的天气模式。 &lt;/p>; &lt;p>; 下图所示的 SEEDS 生成的样本（Ca-Ch 帧）显示了葡萄牙西部的一个位势槽，其空间结构类似于美国实际预测或基于观测的再分析中发现的空间结构。尽管高斯模型可以充分预测边缘单变量分布，但它无法捕获跨领域或空间相关性。这阻碍了对这些异常现象对来自北非的热空气入侵可能产生的影响的评估，而北非的热空气入侵可能会加剧欧洲的热浪。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVu jZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s1999/image2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1675&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8u FWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;2022 年 7 月 14 日 0:00 UTC 欧洲各地的邮票地图。等值线代表平均海平面压力（虚线标记低于 1010 hPa 的等压线），而热图则描绘 500 hPa 压力水平下的位势高度。 (A)&lt;a href=&quot;https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5&quot;>;ERA5&lt;/a>;重新分析，真实观察的代理。 (Ba-Bb) 2 位成员将美国 7 天的运营预测用作我们模型的种子。 (Ca-Ch) 8 个来自 SEEDS 的样品。 (Da-Dh) 8 个非种子成员来自 7 天的美国业务集合预报。 (Ea-Ed) 来自逐点高斯模型的 4 个样本，该模型由整个美国作战集合的均值和方差参数化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;更准确地报道极端事件&lt;/h2>; &lt;p>; 下面我们展示了极端高温期间里斯本附近2米处的温度和总水汽柱的联合分布活动时间：2022年7月14日，当地时间1:00。我们使用 2022 年 7 月 7 日发布的 7 天预测。对于每个图，我们使用 SEEDS 生成 16,384 个成员的集合。 ERA5 观测到的天气事件用星号表示。还显示了操作系综，其中正方形表示用于为生成的系综提供种子的预测，三角形表示其余系综成员。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k 6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;941&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqk KeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;SEEDS 提供了对 2022 年 7 月 14 日欧洲极端高温事件（以棕色星表示）的更好统计覆盖。每幅图显示了葡萄牙里斯本附近网格点上总柱积分水蒸气 (TCVW) 与温度的关系，这些样本来自我们的模型生成的 16,384 个样本（显示为绿点），以取自 2 个种子（蓝色方块）为条件美国 7 天业务集合预报（由稀疏的棕色三角形表示）。有效预报时间为当地时间1:00。实体轮廓级别对应于 SEEDS 内核密度的等比例，最外层包围质量的 95%，每个级别之间包围 11.875%。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; br />; &lt;p>; 根据美国作战小组的说法，在 7 天前观测到的事件发生的可能性非常小，以至于其 31 名成员都没有预测到近地表温度会像观测到的那样温暖。事实上，根据高斯核密度估计计算出的事件概率低于 1%，这意味着成员少于 100 人的集合不太可能包含像此事件一样极端的预测。相比之下，SEEDS 集合能够从两个播种预测中进行推断，提供可能的天气状态的范围，并更好地统计事件的覆盖范围。这样既可以量化事件发生的概率，又可以对事件发生的天气状况进行采样。具体来说，我们高度可扩展的生成方法可以创建非常大的集合，通过为任何用户定义的诊断提供超过给定阈值的天气状态样本来表征非常罕见的事件。 &lt;/p>; &lt;br />; &lt;h2>;结论和未来展望&lt;/h2>; &lt;p>; SEEDS 利用生成式人工智能的力量来生成与美国正在运行的预报系统相当的集合预报，但速度更快。本文报告的结果只需要来自操作系统的 2 个播种预测，该系统在当前版本中生成 31 个预测。这导致了一种混合预报系统，其中使用基于物理的模型计算的一些天气轨迹来播种扩散模型，该模型可以更有效地生成额外的预报。这种方法提供了当前操作天气预报范例的替代方案，其中统计模拟器节省的计算资源可以分配用于提高基于物理的模型的分辨率或更频繁地发布预报。 &lt;/p>; &lt;p>; 我们相信，SEEDS 只是人工智能在未来几年加速数值天气预报业务进展的众多方式之一。我们希望这一生成式人工智能在天气预报模拟和后处理方面的实用性演示将促进其在气候风险评估等研究领域的应用，在这些领域中，生成大量气候预测集合对于准确量化未来的不确定性至关重要气候。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;所有 SEEDS 作者 Lizao Li、Rob Carver、Ignacio Lopez-Gomez、Fei Sha 和 John Anderson 共同撰写了这篇博文， Carla Bromberg 担任项目负责人。我们还要感谢动画设计者 Tom Small。我们 Google Research 的同事为 SEEDS 工作提供了宝贵的建议。其中，我们感谢 Leonardo Zepeda-Núñez、Zhong Yi Wan、Stephan Rasp、Stephan Hoyer 和 Tapio Schneider 的投入和有益的讨论。我们感谢泰勒·拉塞尔 (Tyler Russell) 提供的额外技术项目管理，以及亚历克斯·梅罗斯 (Alex Merose) 的数据协调和支持。我们还感谢 Cenk Gazen、Shreya Agrawal 和 Jason Hickey 在 SEEDS 工作的早期阶段进行的讨论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1569605132526995799/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html&quot; rel=&quot;alternate&quot; title=&quot;生成人工智能来量化天气预报的不确定性&quot; type=&quot;text/html&quot;/ >;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>; &lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATD KSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-1799535679952845079&lt;/id>;&lt;发布>;2024-03-28T13:53:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-29T12： 00:03.604-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;神经网络&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;statistics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AutoBNN：组合概率时间序列预测贝叶斯神经网络&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Urs Köster&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4trPid -OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s320/AutoBNN.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;时间序列&lt;/a>;问题无处不在，从预测天气和交通模式到了解经济趋势。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_inference&quot;>;贝叶斯&lt;/a>;方法从对数据模式（先验概率）的假设开始，收集证据（例如，新的时间序列数据），并不断更新该假设以形成后验概率分布。传统的贝叶斯方法，例如高斯过程（GP）和&lt;a href=&quot;https://blog.tensorflow.org/2019/03/ Structure-time-series-modeling-in.html&quot;>;结构时间序列&lt;/a>;广泛用于对时间序列数据进行建模，例如常用的&lt;a href=&quot;https://gml.noaa.gov/ccgg /trends/&quot;>;冒纳罗亚二氧化碳&lt;/a>;数据集。然而，他们通常依赖领域专家来精心选择合适的模型组件，并且计算成本可能很高。神经网络等替代方案缺乏可解释性，因此很难理解它们如何生成预测，并且不能产生可靠的置信区间。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入 &lt;a href=&quot;https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn&quot; >;AutoBNN&lt;/a>;，一个用 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; 编写的新开源包。 AutoBNN 自动发现可解释的时间序列预测模型，提供高质量的不确定性估计，并有效扩展以用于大型数据集。我们描述了 AutoBNN 如何将传统概率方法的可解释性与神经网络的可扩展性和灵活性结合起来。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;AutoBNN&lt;/h2>; &lt;p>; AutoBNN 基于 &lt;a href=&quot;https:/ /proceedings.mlr.press/v28/duvenaud13.html&quot;>;行&lt;/a>; &lt;a href=&quot;https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550&quot;>;&lt;/a>; &lt;a href= &quot;https://proceedings.mlr.press/v202/saad23a.html&quot;>;过去十年的研究&lt;/a>;通过使用具有学习能力的 GP 进行时间序列建模，提高了预测准确性&lt;a href=&quot;https:// www.cs.toronto.edu/~duvenaud/cookbook/&quot;>;内核&lt;/a>;结构。 GP 的核函数对有关正在建模的函数的假设进行编码，例如趋势、周期性或噪声的存在。对于学习的 GP 核，核函数是组合定义的：它是一个基本核（例如线性核、二次核、周期性核、周期性核） >;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function&quot;>;Matérn&lt;/a>;&lt;/code>; 或 &lt;code>;ExponentiatedQuadratic&lt;/code>;）或组合两个或使用&lt;code>;加法&lt;/code>;、&lt;code>;乘法&lt;/code>;或&lt;code>;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/170等运算符的多个内核函数.pdf&quot;>;ChangePoint&lt;/a>;&lt;/code>;。这种组合内核结构有两个相关的目的。首先，这很简单，对于数据专家（但不一定是 GP 专家）的用户可以为其时间序列构建合理的先验。其次，诸如&lt;a href=&quot;https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf&quot;>;顺序蒙特卡罗&lt;/a>;之类的技术可用于小型结构上的离散搜索，并可以输出可解释的结果。&lt;/p>; &lt;p>; AutoBNN 改进了这些想法，用&lt;a href=&quot;https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/&quot;>;贝叶斯神经网络&lt; /a>; (BNN)，同时保留组合内核结构。 BNN 是一种具有权重概率分布的神经网络，而不是一组固定的权重。这会导致输出分布，捕获预测中的不确定性。与 GP 相比，BNN 具有以下优势：首先，训练大型 GP 的计算成本很高，并且传统的训练算法按时间序列中数据点数量的立方进行缩放。相反，对于固定宽度，训练 BNN 通常与数据点的数量呈近似线性关系。其次，与 GP 训练操作相比，BNN 更适合 GPU 和 &lt;a href=&quot;https://cloud.google.com/tpu?hl=en&quot;>;TPU&lt;/a>; 硬件加速。第三，组合 BNN 可以轻松地与&lt;a href=&quot;https://arxiv.org/abs/2007.06823&quot;>;传统深度 BNN&lt;/a>; 结合，后者具有特征发现的能力。人们可以想象一种“混合”架构，其中用户指定 &lt;code>;Add&lt;/code>;(&lt;code>;Linear&lt;/code>;, &lt;code>;Periodic&lt;/code>;, &lt;code>;Deep&lt; 的顶级结构/code>;），而深度 BNN 则用来学习潜在高维协变量信息的贡献。 &lt;/p>; &lt;p>; 那么如何将具有组合内核的 GP 转换为 BNN？单层神经网络通常会随着神经元数量（或“宽度”）收敛到 GP &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-1-4612-0745-0_2 ”趋向无穷大&lt;/a>;。最近，研究人员&lt;a href=&quot;https://openreview.net/forum?id=gRwh5HkdaTm&quot;>;发现了&lt;/a>;另一个方向的对应关系——许多受欢迎的全科医生&lt;a href=&quot;https://www .cs.toronto.edu/~duvenaud/cookbook/&quot;>;内核&lt;/a>;（例如 &lt;code>;Matern&lt;/code>;、&lt;code>;ExponentiatedQuadratic&lt;/code>;、&lt;code>;Polynomial&lt;/code>; 或 &lt; code>;Periodic&lt;/code>;）可以通过适当选择激活函数和权重分布作为无限宽度的 BNN 获得。此外，即使宽度远小于无穷大，这些 BNN 仍然接近相应的 GP。例如，下图显示了 &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_matrix#:~:text=In%20probability%20theory%20and%20statistics,of%20a%20given 中的差异%20random%20vector&quot;>;观测值对之间的协方差&lt;/a>;，以及真实 GP 及其对应的&lt;a href=&quot;https://en.wikipedia.org/wiki/Kriging&quot;>;回归&lt;/a>;结果width-10 神经网络版本。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_q F-eGv2C1QkU9oDCAS09FfoCne81yEAqC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s1350/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;598&quot; data-original-width=&quot;1350&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_qF-eGv2C1QkU9oDCAS09FfoCne81yEA qC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;真实 GP 内核（顶行）与其宽度 10 神经网络之间的 &lt;a href=&quot;https://en.wikipedia.org/wiki/Gram_matrix&quot;>;Gram 矩阵&lt;/a>; 的比较网络近似值（底行）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; 样式=“左边距：自动”； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoidYqlAK2J1n4y71Qn- WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux 3AXzUENmJ0NA8Ch9dyICT15/s1328/image4.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;586&quot; data-original-width=&quot;1328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhoidYqlaK2J1n4y71Qn-WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC 2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux3AXzUENmJ0NA8Ch9dyICT15/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;真实 GP 内核（顶行）与其宽度为 10 的神经网络近似值（底行）之间的回归结果比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;最后，通过&lt;code>;加法&lt;/code>;和&lt;code>;乘法&lt;/code>;的&lt;a href=&quot;https://arxiv.org/abs/1905.06076&quot;>;BNN类似物&lt;/a>;完成翻译GP 上的算子，并且通过将组件 BNN 的输出相加来直接产生 BNN 加法，BNN 乘法是通过乘以 BNN 隐藏层的激活然后应用共享密集层来实现的。因此，我们只能将具有相同隐藏宽度的 BNN 相乘。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用 AutoBNN&lt;/h2>; &lt;p>; AutoBNN &lt;a href=&quot;https://github .com/tensorflow/probability/tree/main/spinoffs/autobnn&quot;>;软件包&lt;/a>;在&lt;a href=&quot;https://www.tensorflow.org/probability&quot;>;Tensorflow Probability&lt;/a>;中提供。它在 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; 中实现并使用 &lt;a href=&quot;https://github.com/google/flax&quot;>;flax。 linen&lt;/a>; 神经网络库。它实现了迄今为止讨论的所有基本内核和运算符（&lt;code>;Linear&lt;/code>;、&lt;code>;Quadratic&lt;/code>;、&lt;code>;Matern&lt;/code>;、&lt;code>;ExponentiatedQuadratic&lt;/code>;、&lt; code>;Periodic&lt;/code>;、&lt;code>;Addition&lt;/code>;、&lt;code>;Multiplication&lt;/code>;）加上一个新内核和三个新运算符：&lt;/p>; &lt;ul>; &lt;li>;a &lt;code>;OneLayer &lt;/code>; 内核，单个隐藏层 &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;ReLU&lt;/a>; BNN，&lt;/li>;&lt;li>;a &lt;code >;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/170.pdf&quot;>;ChangePoint&lt;/a>;&lt;/code>; 运算符，允许在两个内核之间平滑切换，&lt;/li>;&lt;li>;一个 &lt;code>;LearnableChangePoint&lt;/code>; 运算符，与 &lt;code>;ChangePoint&lt;/code>; 相同，只是位置和斜率是给定先验分布并且可以从数据中学习，以及 &lt;/li>;&lt;li>;a &lt;code >;WeightedSum&lt;/code>; 运算符。 &lt;/li>; &lt;/ul>; &lt;p>; &lt;code>;WeightedSum&lt;/code>; 将两个或多个 BNN 与可学习的混合权重结合起来，其中可学习的权重遵循 &lt;a href=&quot;https://en.wikipedia.org/ wiki/Dirichlet_distribution&quot;>;狄利克雷先验&lt;/a>;。默认情况下，使用浓度为 1.0 的平坦狄利克雷分布。 &lt;/p>; &lt;p>; &lt;code>;WeightedSums&lt;/code>; 允许结构发现的“软”版本，即一次训练许多可能模型的线性组合。与离散结构的结构发现相比，例如在 AutoGP 中，这允许我们使用标准梯度方法来学习结构，而不是使用昂贵的离散优化。 WeightedSum 允许我们并行评估它们，而不是串联评估潜在的组合结构。 &lt;/p>; &lt;p>; 为了轻松实现探索，AutoBNN 定义了&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py&quot;>;模型数量包含顶级或内部 &lt;code>;WeightedSums&lt;/code>; 的结构&lt;/a>;。这些模型的名称可以用作任何 &lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/estimators.py&quot;>;估计器&lt;中的第一个参数/a>; 构造函数，并包含诸如 &lt;code>;&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py#L133&quot;>;sum_of_stumps&lt;/a 之类的内容>;&lt;/code>;（所有基本内核的 &lt;code>;WeightedSum&lt;/code>;）和 &lt;code>;sum_of_shallow&lt;/code>;（将基本内核与所有运算符的所有可能组合相加）。&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td 样式=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXz CeSCZBFDHIc1qYfK53EwEdngf1KykzCfpPiIg3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s1389/image2.png&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;255&quot; data-original-width=&quot;1389&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXzCesCZBFDHIc1qYfK53EwEdngf1KykzCfpPiI g3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;sum_of_stumps 模型的插图。顶行中的条形显示每个基本内核贡献的量，底行显示基本内核所代表的函数。生成的加权和显示在上。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 下图演示了从&lt;a href=&quot;https://forecasters.org/resources/time-series-data/m3-competition/&quot;>;M3&lt;/a>; 数据集。六个基本结构是 &lt;code>;ExponentiatedQuadratic&lt;/code>; （这是相同的）作为径向基函数内核，或简称为 &lt;a href=&quot;https://en.wikipedia.org/wiki/Radial_basis_function_kernel&quot;>;RBF&lt;/a>;）、&lt;code>;Matern&lt;/code>;、&lt;code>;Linear &lt;/code>;、&lt;code>;Quadratic&lt;/code>;、&lt;code>;OneLayer&lt;/code>; 和 &lt;code>;Periodic&lt;/code>; 内核。该图显示了 32 个粒子集合中它们的权重的 MAP 估计值。所有高似然粒子对周期性分量给予较大权重，对线性分量、二次分量和单层分量给予较低权重>;，以及 &lt;code>;RBF&lt;/code>; 或 &lt;code>;Matern&lt;/code>; 的较大权重。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H 2-JZOmxxRJSm9ExG_PUr6U7iFl8nyp4lEaNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s868/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;868&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H2-JZOmxxRJSm9ExG_PUR6U7iFl8nyp4lE aNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;32 个粒子的基本核权重的 &lt;a href=&quot;https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php&quot;>;MAP&lt;/a>; 估计的平行坐标图。 &lt;code>;sum_of_stumps&lt;/code>; 模型在 M3 数据集的 N374 系列上进行训练（插入蓝色）。较暗的线对应于可能性较高的粒子。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;通过使用&lt;code>;WeightedSums&lt;/code>;作为其他算子的输入，可以表示丰富的组合结构，同时保持模型紧凑和可学习权重的数量较少。例如，我们包含 &lt;code>;sum_of_products&lt;/code>; 模型（如下图所示），它首先创建两个 &lt;code>;WeightedSums&lt;/code>; 的成对乘积，然后计算两个乘积的总和。通过将一些权重设置为零，我们可以创建许多不同的离散结构。该模型中可能的结构总数为 2&lt;sup>;16&lt;/sup>;，因为有 16 个可以打开或关闭的基本内核。所有这些结构都是通过仅训练这一模型来隐式探索的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQ tvzMR_548jt4h2lbTzfnp7ZUeYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s1754/AutoBNN%20illustration.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1754&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQtvzMR_548jt4h2lbTzfnp7ZU eYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s16000/AutoBNN%20illustration.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;“sum_of_products”模型的图示。四个 WeightedSum 中的每一个都具有与“sum_of_stumps”模型相同的结构。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 然而，我们发现某些内核组合（例如， &lt;code>;Periodic&lt;/code>; 与 &lt;code>;Matern&lt;/code>; 或 &lt;code>;ExponentiatedQuadratic&lt;/code>; 的乘积）会导致许多数据集过度拟合。为了防止这种情况，我们定义了 &lt;code>;sum_of_safe_shallow&lt;/code>; 等模型类，在使用 &lt;code>;WeightedSums&lt;/code>; 执行结构发现时排除此类乘积。 &lt;/p>; &lt;p>; 对于训练，AutoBNN 提供了 AutoBnnMapEstimator 和 AutoBnnMCMCEstimator 来分别执行 MAP 和 MCMC 推理。任一估计器都可以与六个&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/likelihoods.py&quot;>;似然函数&lt;/a>;中的任何一个组合，包括四种基于连续数据的具有不同噪声特征的正态分布，两种基于计数数据的负二项式分布。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31Pbx TQF29yH1cTJRdI​​-XkXmnZMR_imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s1076/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;280&quot; data-original-width=&quot;1076&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31PbxTQF29yH1cTJRdI​​-XkXmnZMR _imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 &lt;a href=&quot;https://gml.noaa.gov/ccgg/trends/&quot;>;Mauna Loa CO2&lt;/a 上运行 AutoBNN 的结果>; 我们的示例 &lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb&quot;>;colab&lt;/a>; 中的数据集。该模型捕获数据中的趋势和季节性成分。外推未来，均值预测略微低估了实际趋势，而 95% 置信区间逐渐增大。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 拟合如图所示的模型上面，所需要的只是以下 10 行代码，使用 &lt;a href=&quot;https://scikit-learn.org/stable/&quot;>;scikit-learn&lt;/a>; 启发的估计器接口：&lt;/p>; &lt;pre class=&quot;prettyprint&quot;>;导入 autobnn 作为 ab model = ab.operators.Add( bnns=(ab.kernels.PeriodicBNN(width=50), ab.kernels.LinearBNN(width=50), ab.kernels.MaternBNN (width=50))) estimator = ab.estimators.AutoBnnMapEstimator( model, &#39;normal_likelihood_logistic_noise&#39;, jax.random.PRNGKey(42), period=[12]) estimator.fit(my_training_data_xs, my_training_data_ys) 低、中、高 = estimator.predict_quantiles(my_training_data_xs) &lt;/pre>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; &lt;a href =&quot;https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn&quot;>;AutoBNN&lt;/a>; 为构建复杂的时间序列预测模型提供了强大而灵活的框架。通过将 BNN 和 GP 的优势与组合内核相结合，AutoBNN 为理解和预测复杂数据打开了一个充满可能性的世界。我们邀请社区尝试&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb&quot; target=&quot;_blank&quot;>;colab&lt;/a>;，并且利用该库进行创新并解决现实世界的挑战。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;AutoBNN 由 Colin Carroll、Thomas Colthurst 编写，乌尔斯·科斯特和斯里尼瓦斯·瓦苏代万。我们要感谢 Kevin Murphy、Brian Patton 和 Feras Saad 的建议和反馈。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research .google/feeds/1799535679952845079/comments/default&quot; rel=&quot;replies&quot; title=&quot;帖子评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/ 03/autobnn-probabilistic-time-series.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com /feeds/8474926331452026626/posts/default/1799535679952845079&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 1799535679952845079&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/autobnn-probabilistic-time-series.html&quot; rel=&quot; ternate&quot; title=&quot;AutoBNN：使用组合贝叶斯神经网络进行概率时间序列预测&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4 trPid-OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s72-c/AutoBNN.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7061041222399769838&lt;/ id>;&lt;发布>;2024-03-20T13:54:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-20T13:54:06.249-07:00&lt;/更新>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;健康&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt; /category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;肺癌筛查的计算机辅助诊断&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Atilla Kiraly 和产品经理 Rory Pilgrim &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD7R8_tNqtH6D8X_KiMt ZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s320/PULMA%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 肺癌是全球癌症相关死亡的主要原因 &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/cancer#:~:text= 2020 年报告的%20most%20common%20causes%20of，直肠%20（916%20000%20deaths）%3B&quot;>;180 万人死亡&lt;/a>;。晚期诊断会大大降低生存机会。 &lt;a href=&quot;https://www.cdc.gov/cancer/lung/basic_info/screening.htm&quot;>;肺癌筛查&lt;/a>;通过&lt;a href=&quot;https://www.cancer.gov/about -cancer/diagnosis-staging/ct-scans-fact-sheet#:~:text=indicate%20real%20problems.-,Lung%20cancer,-Low%2Ddose%20CT&quot;>;计算机断层扫描&lt;/a>; (CT),它提供了详细的肺部 3D 图像，已被证明可以通过更早地检测潜在的癌症迹象，将高危人群的死亡率降低至少 20%。在美国，筛查涉及每年一次扫描，一些国家或病例建议或多或少地进行扫描。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/lung-cancer-screening&quot;>;美国预防服务工作组&lt;/a>;最近将肺癌筛查建议扩大了&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;大约80%&lt;/a>;，预计将增加筛查妇女以及少数种族和族裔群体的参与机会。然而，假阳性（即错误地报告无癌症患者的潜在癌症）可能会引起焦虑，并导致患者接受不必要的手术，同时增加医疗保健系统的成本。此外，根据医疗基础设施和放射科医生的可用性，筛查大量个体的效率可能具有挑战性。 &lt;/p>; &lt;p>; 在 Google，我们之前开发了&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;用于肺癌检测的机器学习 (ML) 模型&lt;/ a>;，并评估了它们自动检测和分类显示潜在癌症迹象的区域的能力。事实证明，在检测可能的癌症方面，其性能可与专家相媲美。虽然他们取得了很高的绩效，但要充分发挥他们的潜力，在现实环境中有效地传达发现是必要的。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://pubs.rsna.org/doi/10.1148/ryai.230079&quot;>;肺癌筛查中的辅助人工智能：一项回顾性跨国研究美国和日本&lt;/a>;”，发表于&lt;em>;&lt;a href=&quot;https://pubs.rsna.org/journal/ai&quot;>;放射学人工智能&lt;/a>;&lt;/em>;，我们研究了机器学习如何建模可以有效地将研究结果传达给放射科医生。我们还引入了一个以用户为中心的通用界面，以帮助放射科医生利用此类模型进行肺癌筛查。该系统以 CT 成像作为输入，并使用四个类别（无怀疑、可能良性、可疑、高度可疑）以及相应的感兴趣区域输出癌症怀疑评级。我们通过美国和日本的随机读者研究，使用当地癌症评分系统 (&lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/ RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot;>;Lung-RADS V1.1&lt;/a>; 和 &lt;a href=&quot;https://www.jscts.org/pdf/guideline/gls3rdfig_english130621.pdf&quot;>;仙台分数&lt;/a>;）和模仿现实设置的图像查看器。我们发现，在两项读者研究中，读者特异性随着模型的帮助而增加。为了加快利用机器学习模型进行类似研究的进展，我们提供&lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>;开源代码&lt;/a>;处理 CT 图像并生成与放射科医生使用的&lt;a href=&quot;https://en.wikipedia.org/wiki/Picture_archiving_and_communication_system&quot;>;图片存档和通信系统&lt;/a>; (PACS) 兼容的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开发一个接口来传达模型结果&lt;/h2>; &lt;p>; 将 ML 模型集成到放射科医生工作流程中涉及了解他们任务的细微差别和目标，以便为他们提供有意义的支持。在肺癌筛查方面，医院遵循定期更新的各个国家/地区特定指南。例如，在美国，Lung-RADs V1.1 指定 &lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot; >;字母数字评分&lt;/a>;表示肺癌风险和后续建议&lt;em>;。 &lt;/em>;在评估患者时，放射科医生将 CT 加载到工作站中以读取病例、查找肺部结节或病变，并应用既定指南来确定后续决策。 &lt;/p>; &lt;p>; 我们的第一步是通过额外的训练来改进&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前开发的机器学习模型&lt;/a>;数据和架构改进，包括&lt;a href=&quot;https://research.google/pubs/attention-is-all-you-need/&quot;>;自我注意力&lt;/a>;。然后，我们没有针对特定的指南，而是尝试了一种独立于指南或其特定版本来传达人工智能结果的补充方式。具体来说，系统输出提供怀疑评级和定位（感兴趣区域），供用户结合自己的具体指南进行考虑。该界面可生成与 CT 研究直接相关的输出图像，无需更改用户的工作站。放射科医生只需要查看一小组附加图像。他们的系统或与系统的交互没有其他变化。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug- HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s857/image1.png&quot; style=&quot;mar&quot; gin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original -width=&quot;857&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug-HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6 FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s16000/image1.png&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统输出示例。放射科医生的评估结果在发现可疑病变的 CT 体积位置上进行可视化。总体怀疑显示在 CT 图像的顶部。圆圈突出显示可疑病变，而方形则显示从不同角度（称为矢状视图）对同一病变的渲染。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 辅助肺癌筛查系统包括13 个模型，并具有类似于&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前的工作&lt;/a中使用的端到端系统的高级架构>;。这些模型相互协调，首先对肺部进行分割，获得总体评估，定位三个可疑区域，然后使用该信息为每个区域分配可疑评级。该系统使用 &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;>;Google Kubernetes Engine&lt;/a>; (GKE) 部署在 Google Cloud 上，该引擎提取映像、运行机器学习模型并提供了结果。这样可以实现可扩展性，并直接连接到&lt;a href=&quot;https://cloud.google.com/healthcare-api/docs/concepts/dicom&quot;>;DICOM 存储&lt;/a>;中存储图像的服务器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfk b9mGxEmL191sfpAclKD085x-u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s646/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;394&quot; data-original-width=&quot;646&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9mGxEmL191sfpAclKD085x- u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统的 Google Cloud 部署概述以及提供图像和计算结果的各个组件的定向调用流程。使用 Google Cloud 服务将图像提供给查看者和系统。该系统在 Google Kubernetes Engine 上运行，该引擎提取图像、处理图像并将其写回到 DICOM 存储中。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;读者研究&lt;/h2>; &lt;p>;为了评估系统在改善临床表现方面的效用，我们进行了两项读者研究（即设计的实验）使用预先存在的、去识别化的 CT 扫描来评估临床表现（比较有或没有技术帮助的专家表现）与 12 名放射科医生。我们向 6 位美国放射科医生和 6 位日本放射科医生介绍了 627 个具有挑战性的病例。在实验设置中，读者被分为两组，每个案例阅读两次，有或没有模型的帮助。读者被要求应用他们在临床实践中通常使用的评分指南，并报告他们对每个病例​​对癌症的总体怀疑。然后，我们比较了读者的反馈结果，以衡量模型对他们的工作流程和决策的影响。分数和怀疑水平是根据个人的实际癌症结果来判断的，以衡量敏感性、特异性和&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and -auc#:~:text=AUC%20stands%20for%20%22Area%20under,across%20all%20possible%20classification%20thresholds。&quot;>;ROC 曲线下面积&lt;/a>; (AUC) 值。这些是在有帮助和没有帮助的情况下进行比较的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1y LUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPmwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s794/image3.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;297&quot; data-original-width=&quot;794&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFP mwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >; 多案例多读者研究涉及每个读者对每个案例进行两次审查，一次有机器学习系统协助，一次没有机器学习系统协助。在此可视化中，读者首先在没有帮助的情况下查看 A 组（&lt;strong>;蓝色&lt;/strong>;），然后在冲洗期后在有帮助的情况下查看（&lt;strong>;橙色&lt;/strong>;）。第二个读者组遵循相反的路径，首先在帮助下阅读同一组案例 A 集。读者被随机分配到这些组中，以消除排序的影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;使用相同界面进行这些研究的能力突出了其对完全不同的癌症评分的普遍性系统，以及模型和辅助能力对不同患者群体的推广。我们的研究结果表明，当放射科医生在临床评估中使用该系统时，他们在没有可操作的肺癌发现的情况下正确识别肺部图像的能力（即&lt;em>;特异性&lt;/em>;）比之前提高了绝对 5-7%当他们不使用辅助系统时。这可能意味着每筛查 15-20 名患者，就有可能避免不必要的后续程序，从而减轻他们的焦虑和医疗保健系统的负担。反过来，这可以帮助提高肺癌筛查计划的可持续性，特别是随着&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;更多人有资格接受筛查&lt;/a >;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8 ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoBKt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s1999/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoB Kt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;在美国和日本的读者研究中，通过机器学习模型的帮助，读者的特异性得到了提高。特异性值是根据可操作的发现（发现可疑的东西）与没有可操作的发现的读者评分得出的，并与个体的真实癌症结果进行比较。在模型协助下，读者标记出较少的癌症阴性个体进行后续访问。对癌症阳性个体的敏感性保持不变。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过合作将其转化为现实世界的影响 &lt;/h2>; &lt;p>; 该系统结果表明，有可能减少随访次数、减少焦虑，并降低肺癌筛查的总体成本。为了将这项研究转化为现实世界的临床影响，我们正在与：&lt;a href=&quot;https://deephealth.com/&quot;>;DeepHealth&lt;/a>;，一家领先的人工智能驱动的健康信息学提供商；与印度领先的放射学服务提供商 &lt;a href=&quot;https://apolloradiologyintl.com/&quot;>;Apollo Radiology International&lt;/a>; 合作，探索将该系统纳入未来产品的途径。此外，我们还希望通过 &lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>; 帮助其他研究人员研究如何最好地将 ML 模型结果集成到临床工作流程中开源代码&lt;/a>;用于读者研究并结合本博客中描述的见解。我们希望这将有助于加速医学影像研究人员对其人工智能模型进行读者研究，并促进该领域的转化研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目的主要贡献者包括 Corbin Cunningham、Zaid Nabulsi、Ryan Najafi、Jie Yang、Charles Lau、Joseph R. Ledsam、叶文兴、Diego Ardila、Scott M. McKinney、Rory Pilgrim、Hiroaki Saito、Yasuteru Shimamura、Mozziyar Etemadi、Yun Liu、David Melnick、Sunny Jansen、Nadia Harhen 、David P. Nadich、Mikhail Fomitchev、Ziyad Helali、Shabir Adeel、Greg S. Corrado、Lily Peng、Daniel Tse、Shravya Shetty、Shruthi Prabhakara、Neeral Beladia 和 Krish Eswaran。感谢 Arnav Agharwal 和 Andrew Sellergren 的开源支持，以及 Vivek Natarajan 和 Michael D. Howell 的反馈。还要衷心感谢放射科医生在整个研究过程中通过图像解释和注释工作实现了这项工作，以及 Jonny Wong 和 Carli Sampson 协调读者研究。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt; /content>;&lt;link href=&quot;http://blog.research.google/feeds/7061041222399769838/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/03/computer-aided-diagnosis-for-lung.html&quot; rel=&quot;alternate&quot; title=&quot;肺癌筛查的计算机辅助诊断&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI &lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http: //schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author >;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD 7R8_tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s72-c/PULMA%20hero.jpg&quot;宽度=&quot; 72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-4615278636568583418&lt;/id>;&lt;发布>;2024-03-20T09:06:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-20T09:06:06.7 53 -07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“环境”>;&lt;/类别>;&lt;类别方案=“http://www.blogger” .com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;利用人工智能扩大全球对可靠洪水预报的访问&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt; span class=&quot;byline-author&quot;>;发布者：Yossi Matias，工程副总裁研究和 Gray Nearing，研究科学家，Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX-dCHpcYt nSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s320/洪水%20预测% 20hero%20image.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 洪水是&lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;最常见的自然灾害&lt;/ a>;，并造成全球每年大约&lt;a href=&quot;https://www.swissre.com/risk-knowledge/mitigating-climate-risk/floods.html&quot;>;500亿美元&lt;/a>;的经济损失。 &lt;a href=&quot;https://library.wmo.int/records/item/57630-2021-state-of-climate-services-water?offset=1#:~:text=WMO%2DNo.,1278&amp;amp; text=More%20than%202%20billion%20people,for%20the%20past%2020%20years.&quot;>;自 2000 年以来，与洪水相关的灾害发生率增加了一倍多&lt;/a>;&lt;a href=&quot;https: //www.nature.com/articles/s41598-020-70816-2&quot;>;由于气候变化&lt;/a>;。近 &lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;15 亿人&lt;/a>;，占 19%世界人口面临着严重洪水事件的巨大风险。升级预警系统，让这些人群能够获得准确、及时的信息&lt;a href=&quot;https://elibrary.worldbank.org/doi/abs/10.1596/1813-9450-6058&quot;>;每年可以挽救数千人的生命&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在可靠的洪水预报对全球人们生活的潜在影响的推动下，我们于 2017 年开始了洪水预报工作。通过此 &lt;a href=&quot;https ://blog.google/technology/ai/google-ai-global-flood-forecasting/&quot;>;多年历程&lt;/a>;，多年来，我们在推进研究的同时建立了实时运营系统洪水预报系统，&lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;在 Google 搜索、地图、Android 通知和通过&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;。不过，为了&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;在全球范围内扩展&lt;/a>;，尤其是在某些地方在无法获得准确的当地数据的地方，需要取得更多的研究进展。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;未测量流域的极端洪水全球预测&lt;/a>;”中，发表在&lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;自然&lt;/a>;&lt;/em>;中，我们展示了机器学习 (ML) 技术如何显着改善全球范围&lt;a href= “https://sites.research.google/floodforecasting/&quot;>;洪水预报&lt;/a>;相对于洪水相关数据稀缺的国家当前最先进的技术。借助这些基于人工智能的技术，我们将当前可用的全球临近预报的可靠性平均从零天延长到五天，并将非洲和亚洲地区的预报改进为与欧洲当前可用的预报类似。模型的评估是与欧洲中期天气预报中心 (&lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;) 合作进行的。 &lt;/p>; &lt;p>; 这些技术还使&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;能够提前 7 天提供实时河流预报，&lt;a href =&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;覆盖&lt;/a>; 80 多个国家的河流河段。人们、社区、政府和国际组织可以利用这些信息来采取预期行动，帮助保护弱势群体。 &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot;frameborder=&quot;0&quot; height=&quot;360 “ src=&quot;https://www.youtube.com/embed/ET04pDj-RvM?si=WJJXEtwJqtyMRuC_?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;[ET04pDj-RvM]&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;Google 的洪水预报 &lt;/h2>; &lt;p>; 为 FloodHub 工具提供支持的机器学习模型是与多个合作伙伴（包括学术界、政府、国际组织和机构）合作多年研究的成果。非政府组织。 &lt;/p>; &lt;p>; 2018 年，我们&lt;a href=&quot;https://blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;启动了试点&lt; /a>; 印度恒河-雅鲁藏布江流域的早期预警系统，&lt;a href=&quot;https://arxiv.org/abs/1901.09583&quot;>;假设&lt;/a>;认为机器学习可以帮助解决具有挑战性的问题可靠的大规模洪水预报。第二年，该试点进一步&lt;a href=&quot;https://blog.google/technology/ai/tracking-our-progress-on-flood-forecasting/&quot;>;扩大&lt;/a>;&lt;a href=&quot;https: //ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;通过结合淹没模型、实时水位测量、创建高程图和水文建模。 &lt;/p>; &lt;p>; 在与学者的&lt;a href=&quot;https://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html&quot;>;合作&lt;/a>;中，特别是，我们与&lt;a href=&quot;https://www.jku.at/en/institute-for-machine-learning/&quot;>;JKU 机器学习研究所&lt;/a>;一起探索了基于机器学习的水文模型，表明基于&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;LSTM&lt;/a>;的模型可以&lt;a href=&quot;https://hess. copernicus.org/articles/23/5089/2019/&quot;>;比传统的概念性和基于物理的&lt;a href=&quot;https://en.wikipedia.org/wiki/Hydrological_model&quot;>;水文学产生更准确的模拟&lt;/a>;模型&lt;/a>;。这项研究带来了&lt;a href=&quot;https://blog.research.google/2020/09/the-technology-behind-our-recent.html&quot;>;洪水预报改进&lt;/a>;，实现了&lt;a href= &quot;https://blog.google/technology/ai/flood-forecasts-india-bangladesh/&quot;>;扩大&lt;/a>;我们的预测覆盖范围，将印度和孟加拉国全部覆盖。我们还与耶鲁大学的研究人员合作，测试可增加&lt;a href=&quot;https://egc.yale.edu/about/perspectives/pande-and-coauthors-using-technology-save-lives-during-印度季风季节&quot;>;洪水警报的范围和影响&lt;/a>;。 &lt;/p>; &lt;p>; 我们的水文模型通过处理降水和物理流域信息等公开的天气数据来预测河流洪水。此类模型必须根据来自各个河流的&lt;a href=&quot;https://en.wikipedia.org/wiki/Stream_gauge&quot;>;水流测量站&lt;/a>;的长数据记录进行校准。全球河流流域（流域）拥有流量计的比例较低，流量计价格昂贵，但却是提供相关数据所必需的，并且为水文模拟和预报提供流量计具有挑战性&lt;a href=&quot;https://www.tandfonline.com/doi /full/10.1080/02626667.2013.803183&quot;>;缺乏此基础设施的流域的预测&lt;/a>;。 &lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;国内生产总值&lt;/a>; (GDP) 下降与&lt;a href=&quot;https:// www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;洪水风险的脆弱性&lt;/a>;，并且一个国家的国民生产总值与公开数据量之间存在负相关关系。机器学习通过允许&lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;在所有可用河流数据上训练单个模型&lt;/a>;来帮助解决这个问题适用于&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;没有可用数据&lt;/a>;的未计量盆地。通过这种方式，模型可以在全球范围内进行训练，并且可以对任何河流位置进行预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZr bCKnROTNn_hmOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s1051/Streamflow%20data%20from%20the %20Global%20Runoff%20Data%20Center.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width= “1051”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCKnROTNn_h mOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s16000/Streamflow%20data%20from%20the%20Global%20Runoff%20Data%20Center.jpg&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;公开可用的流量之间存在逆（对数）相关性一个国家的数据和国民生产总值。来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;的水流数据。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的学术合作促成了机器学习研究，该研究开发了&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;估计河流预报的不确定性&lt;/ a>; 并展示了 ML 河流预测模型&lt;a href=&quot;https://hess.copernicus.org/articles/25/2685/2021/hess-25-2685-2021-relations.html&quot;>;如何从多个数据中合成信息来源&lt;/a>;。他们证明，这些模型可以&lt;a href=&quot;https://hess.copernicus.org/articles/26/3377/2022/hess-26-3377-2022.html&quot;>;可靠地模拟极端事件&lt;/a>;，甚至当这些事件不是训练数据的一部分时。为了&lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;为开放科学做出贡献&lt;/a>;，我们将在 2023 年开放-在 &lt;em>;&lt;a href=&quot;https://www.nature.com/articles/s41597-023-01975-w&quot;>;Nature Scientific Data&lt;/a>;&lt;/ 中获取了社区驱动的大样本水文学数据集嗯>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;河流预报模型&lt;/h2>; &lt;p>; 国家和国际机构使用的大多数水文模型洪水预报和河流建模是状态空间模型，仅取决于日常输入（例如降水、温度等）和系统的当前状态（例如土壤湿度、积雪等）。 LSTM 是状态空间模型的一种变体，通过定义表示单个时间步长的神经网络来工作，其中处理输入数据（例如当前天气状况）以生成该时间步长的更新状态信息和输出值（流） 。 LSTM 按顺序应用来进行时间序列预测，从这个意义上说，其行为类似于科学家通常概念化水​​文系统的方式。根据经验，我们发现 &lt;a href=&quot;https://hess.copernicus.org/articles/23/5089/2019/&quot;>;LSTM 在河流预测任务上表现良好&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xx rB87mupNTPpioKT0wRgSSc1FwYDmfCUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s960/image1.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB87mupNTPpioKT0wRgSSc1FwYDmf CUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;LSTM 的示意图，LSTM 是一种按时间顺序运行的神经网络。可以在&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;此处&lt;/a>;找到易于理解的入门读物。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的河流预报模型使用两个连续应用的 LSTM：（1）“后报”LSTM 摄取截至当前时间（或者更确切地说，预报发布时间）的历史天气数据（动态后报特征） ），（2）“预测”LSTM 从后播 LSTM 中提取状态以及预测的天气数据（动态预测特征）来做出未来的预测。将一年的历史天气数据输入到后报 LSTM 中，将 7 天的预报天气数据输入到预报 LSTM 中。静态特征包括流域的地理和地球物理特征，这些特征被输入到后报和预测 LSTM 中，并允许模型学习各种类型流域的不同水文行为和响应。 &lt;/p>; &lt;p>; 预测 LSTM 的输出被输入到使用 &lt;a href=&quot;https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf 的“头”层&quot;>;混合密度网络&lt;/a>;产生概率预测（即，水流概率分布的预测参数）。具体来说，该模型在每次预测时都会预测重尾概率密度函数的混合参数，称为“非对称拉普拉斯分布”时间步。结果是一个混合密度函数，称为&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf&quot;>;非对称拉普拉斯的可数混合&lt;/a>;（ CMAL）分布，表示特定时间特定河流中体积流量的概率预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ 3dm8TRicy_wkTVtM4Xio_mhQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s960/LSTM-based%20river %20forecast%20model.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm8TRicy_wkTVtM4Xio_m hQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s16000/LSTM-based%20river%20forecast%20model.jpeg&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;基于 LSTM 的河流预报模型架构。按顺序应用两个 LSTM，一个摄取历史天气数据，一个摄取预测天气数据。模型输出是每个预测时间步长的水流概率分布参数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;输入和训练数据&lt;/h2>; &lt;p>; 该模型使用三种类型的公开数据输入，大部分来自政府来源：&lt;/p>; &lt;ol>; &lt;li>;&lt; em>;代表地理和地球物理变量的静态流域属性：&lt;/em>;来自 &lt;a href=&quot;https://www.Hydrosheds.org/Hydroatlas&quot;>;HydroATLAS 项目&lt;/a>;，包括长期气候指数等数据（降水量、温度、积雪比例）、土地覆盖和人为属性（例如，作为人类发展指标的夜间灯光指数）。 &lt;/li>;&lt;li>;&lt;em>;历史气象时间序列数据&lt;/em>;：用于在预测发布之前一年启动模型。数据来自&lt;a href=&quot;https://gpm.nasa.gov/data/imerg&quot;>;NASA IMERG&lt;/a>;，&lt;a href=&quot;https://psl.noaa.gov/data/gridded/ data.cpc.globalprecip.html&quot;>;NOAA CPC 全球统一日降水量分析&lt;/a>;，以及 &lt;a href=&quot;https://cds.climate.copernicus.eu/cdsapp#!/dataset/ reanalysis-era5-land?tab=overview&quot;>;ECMWF ERA5-land 再分析&lt;/a>;。变量包括每日总降水量、气温、太阳和热辐射、降雪和表面压力。 &lt;/li>;&lt;li>;&lt;em>;7 天预测范围内的预测气象时间序列&lt;/em>;：用作预测 LSTM 的输入。这些数据与上面列出的气象变量相同，来自 &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/datasets/set-i&quot;>;ECMWF HRES 大气模型&lt;/a>;。 &lt;/li>; &lt;/ol>; &lt;p>; 训练数据是来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;的每日流量值&lt; /a>; 1980 - 2023 年期间。使用来自 5,680 个不同流域水流测量仪（如下所示）的数据训练单个水流预测模型，以改进&lt;a href=&quot;https://eartharxiv.org/repository/view/6363 /&quot;>;准确度&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET 634isx6xzUswrYfDwp8oueoWJ7c3hf0os-RISANrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s1417/gauge_locations_map(1 ).jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;689&quot; data-original-width=&quot;1417&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET634isx6xzUswrYfDwp8oueo WJ7c3hf0os-RIsaNrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s16000/gauge_locations_map(1).jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;5,680 个流量计的位置，为 &lt;a href=&quot;https://www.bafg.de/ 的河流预报模型提供训练数据GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;改进当前最先进的技术&lt;/h2>; &lt;p>;我们将我们的河流预测模型与&lt;a href=&quot;https://www .globalfloods.eu/&quot;>;GloFAS 版本 4&lt;/a>;，当前最先进的全球洪水预报系统。这些实验表明，机器学习可以更早地针对规模更大、影响力更大的事件提供准确的警告。 &lt;/p>; &lt;p>; 下图显示了预测河流位置不同严重程度事件时&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1分数&lt;/a>;的分布世界各地，精确度为正负 1 天。 F1 分数是精确度和召回率的平均值，事件严重性通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Return_period#:~:text=A%20return%20period%2C%20also%20known 来衡量,river%20discharge%20flows%20to%20发生。&quot;>;返回期&lt;/a>;。例如，2 年重现期事件是预计平均每两年超过一次的水流量。我们的模型在长达 4 天或 5 天的交付时间内实现了可靠性评分，平均而言，与 GloFAS 即时预报（0 天交付时间）的可靠性相似或更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe4 6W8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s3908/分布%20of%20F1%20分数%20over %202-year%20.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1844&quot; data-original-width=&quot;3908 “ src =” Q28MTS72ZQJHLJXDGMJSA6VCHLB9ZH3EO8XETWSWSQJ8LNRAEJCAEZFPKGPKGPFPKGPFJRPIL4XMHRPHSDO1WX7BZMVSMVSMVSMVSLFMVSLDFMZOWMZOWMFMFARB5 a>; &lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/F- Score&quot;>;F1 得分&lt;/a>; 2014-2023 年期间全球 2,092 个流域的 2 年重现期事件，由 GloFAS（&lt;strong>;蓝色&lt;/strong>;）和我们的模型（&lt;strong>;橙色&lt;/strong>; >;) 在不同的交货时间。平均而言，我们的模型在统计上与 2 年（显示）以及 1 年、5 年和 10 年事件（未显示）提前 5 天的 GloFAS 即时预报（0 天提前时间）一样准确.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外（未显示），我们的模型在更大和更罕见的极端事件中实现了准确性，在 5 年重现期事件中的精确度和召回率得分在 1 年重现期事件中，与 GloFAS 的精度相似或更好。有关更多信息，请参阅&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;展望未来&lt;/h2>; &lt;p>;洪水预报计划是我们&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;适应和恢复能力工作&lt;/a>;并体现了 Google 的承诺&lt;a href=&quot;https:// /research.google/teams/climate-and-sustainability/&quot;>;应对气候变化&lt;/a>;，同时帮助全球社区增强抵御能力。我们相信人工智能和机器学习将继续在帮助推动气候行动科学研究方面发挥关键作用。 &lt;/p>; &lt;p>; 我们积极&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/4-flood-forecasting-collaboration-case-studies-show-how-ai-can-help-与有需要的社区/&quot;>;与多个国际援助组织（例如人道主义数据中心和红十字会）合作&lt;/a>;，提供可行的洪水预报。此外，与&lt;a href=&quot;https://wmo.int/&quot;>;世界气象组织&lt;/a>; (WMO) 持续合作&lt;a href=&quot;https://blog.google/outreach-initiatives /sustainability/early-warning-system-wmo-google/&quot;>;支持气候灾害预警系统&lt;/a>;，我们正在进行一项研究，以帮助了解人工智能如何帮助解决国家洪水预报机构面临的现实挑战。 &lt;/p>; &lt;p>; 虽然这里介绍的工作表明洪水预报向前迈出了重要一步，但未来的工作还需要进一步将洪水预报覆盖范围扩大到全球更多地点以及其他类型的洪水相关事件和灾害，包括山洪和灾害。城市洪水。我们期待与学术界和专家界、地方政府和业界的合作伙伴继续合作，以实现这些目标。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4615278636568583418/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html&quot; rel=&quot;alternate&quot; title=&quot;利用人工智能扩大全球获得可靠洪水预报的机会&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX- dCHpcYtnSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s72-c/洪水%20forecasting%20hero%2 0image.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-520087429457973735&lt;/id>;&lt;已发布>;2024-03-19T13:15:00.000- 07:00&lt;/发布>;&lt;更新>;2024-03-19T13:15:33.664-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= HCI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;ScreenAI：用于 UI 和视觉情境语言理解的视觉语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Srinivas Sunkara 和Gilles Baechler，Google 研究部软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4 uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s320/ScreenAI%20-%20hero。 jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 屏幕用户界面 (UI) 和信息图表（例如图表、图表和表格）在人类交流和人机交互中发挥着重要作用，因为它们促进了丰富的交互式用户体验。 UI 和信息图表共享相似的设计原则和视觉语言（例如图标和布局），这提供了构建可以理解、推理并与这些界面交互的单一模型的机会。然而，由于其复杂性和不同的呈现格式，信息图表和 UI 提出了独特的建模挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入“&lt;a href=&quot;https://arxiv.org/abs/2402.04615&quot;>;ScreenAI：一种视觉语言UI 和信息图表理解模型&lt;/a>;”。 ScreenAI 通过来自 &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI 架构&lt;/a>;的灵活修补策略进行了改进”pix2结构&lt;/a>;。我们在独特的数据集和任务组合上训练 ScreenAI，其中包括一项新颖的屏幕注释任务，该任务要求模型识别屏幕上的 UI 元素信息（即类型、位置和描述）。这些文本注释为大型语言模型 (LLM) 提供了屏幕描述，使它们能够自动大规模生成问答 (QA)、UI 导航和摘要训练数据集。仅用 5B 参数，ScreenAI 就可以在基于 UI 和信息图表的任务上实现最先进的结果 (&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;WebSRC&lt;/a>;和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;），以及 &lt;a href=&quot;https://github.com/vis-nlp 上一流的性能/ChartQA&quot;>;图表 QA&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>; 和 &lt; a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。我们还发布了三个新数据集：&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#screen-annotation-dataset-details&quot;>;屏幕注释&lt;/ a>; 评估模型的布局理解能力，以及&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa/tree/main?tab=readme-ov-file#short_answers- directory&quot;>;ScreenQA Short&lt;/a>; 和&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#complexqa&quot; target=&quot;_blank&quot;>;复杂 ScreenQA&lt; /a>; 对其 QA 能力进行更全面的评估。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ScreenAI&lt;/h2>; &lt;p>; ScreenAI 的架构基于 &lt;a href=&quot;https:// /arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>;，由多模态编码器块和自回归解码器组成。 PaLI 编码器使用创建图像嵌入的视觉转换器 (ViT) 和将图像和文本嵌入连接起来的多模态编码器作为输入。这种灵活的架构使 ScreenAI 能够解决可以重新转换为文本+图像到文本问题的视觉任务。 &lt;/p>; &lt;p>; 在 PaLI 架构之上，我们采用了 pix2struct 中引入的灵活修补策略。不使用固定网格图案，而是选择网格尺寸以保留输入图像的原始纵横比。这使ScreenAi可以在各种纵横比的图像上良好工作。 &lt;/p>; &lt;p>; ScreenAI模型分为两个阶段：一个预训练阶段，然后是微调阶段。首先，将自我监督的学习应用于自动生成数据标签，然后将其用于训练VIT和语言模型。 VIT在微调阶段被冷冻，其中大多数使用的数据由人类评估者手动标记。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s1600/image6.gif&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 583” data-Original-width =“ 1600” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ text-align：中心;”>; screenai模型体系结构。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br/ >; &lt;/div>; &lt;h2>;数据生成&lt;/h2>; &lt;p>;要为ScreenAI创建预训练数据集，我们首先收集了来自各种设备的大量屏幕截图，包括台式机，移动和平板电脑。这是通过使用&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot; target=&quot;_blank&quot;>;公开访问的网页&lt;/a>;并遵循用于&lt;a href = &lt;a href =“ https://dl.acm.org/doi/10.1145/3126594.3126651“ target =“ _ blank”>; rico数据集&lt;/a>;用于移动应用程序。然后，我们基于&lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; target=&quot;_blank&quot;>; detr &lt;/a>;型号，该型号&lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; &lt;a href=&quot;https://arxiv.org/abs/2005.12872 &lt;/a>;型号（例如，图像，象形图，按钮，文本）及其空间关系。象形图使用&lt;a href=&quot;https://arxiv.org/abs/2210.026663&quot; target=&quot;_blank&quot;>;图标分类器&lt;/a>;能够区分77种不同的图标类型。这种详细的分类对于解释通过图标传达的微妙信息至关重要。对于分类器不涵盖的图标以及信息图表和图像，我们使用Pali Image字幕模型来生成提供上下文信息的描述性字幕。我们还应用&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot; target=&quot;_blank&quot;>;光学角色识别&lt;/a>;（OCR）引擎在屏幕上提取和注释文本内容。我们将OCR文本与先前的注释相结合，以创建每个屏幕的详细说明。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s1747/image2.png&quot; style=&quot;左键：自动右右：auto;“>; &lt;img border =“ 0” data-forminal-height =“ 1055” data-original-width =“ 1747” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;移动应用屏幕截图，带有生成的注释，其中包括UI元素及其描述，例如，&lt;code>; text &lt;/code>;元素还包含来自OCR的文本内容，&lt;code>; image &lt;/code &lt;/code &lt;/code &lt;/code>;元素包含图像字幕，&lt;code>; list_items &lt;/code>;包含其所有子元素。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;基于llm的数据生成&lt;/h3>; &lt;p>;我们使用&lt;a href =“ https://blog.google/technology/google”来增强培训数据的多样性-palm-2-ai-large-Lange-Model/“>; PALM 2 &lt;/a>;在两步过程中生成输入输出对。首先，使用上面概述的技术生成屏幕注释，然后我们围绕该架构制定提示，以为LLM创建合成数据。此过程需要及时的工程和迭代精炼才能找到有效的提示。我们通过针对质量阈值进行人体验证来评估生成的数据的质量。态只会说json。不要写不是JSON的文字。给您以下移动屏幕截图，用文字描述。您能否产生有关屏幕快照的内容以及对它们的相应简短答案的5个问题？答案应尽可能短，仅包含必要的信息。您的答案应如下：问题：[{{问题：问题，答案：答案}}，...] {屏幕架构} &lt;/font>; &lt;/pre>; &lt;br />; &lt;br />; “中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-left：auto; auto; margin-right：auto;”>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;tr>; &lt;td class =&#39; tr-caption“ style =” text-align：center;“>; QA数据生成的示例提示。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;通过组合LLMS的自然语言功能使用结构化的模式，我们模拟了广泛的用户交互和方案，以生成合成，现实的任务。特别是，我们生成三类任务：&lt;/p>; &lt;ul>; &lt;li>; &lt;strong>;问题回答&lt;/strong>;：要求该模型回答有关屏幕截图内容的问题，例如，“什么时候餐厅开放？” &lt;/li>; &lt;li>; &lt;strong>;屏幕导航&lt;/strong>;：要求该模型将自然语言转换为屏幕上的可执行动作，例如，“单击搜索按钮”。 &lt;/li>; &lt;li>; &lt;strong>;屏幕摘要&lt;/strong>;：要求该模型在一个或两个句子中汇总屏幕内容。 &lt;/li>; &lt;/ul>; &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;&#39;&#39; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHdEc-JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7 /S1398/image3.png“ style =”保证金 - 左：自动右：自动; =“ https://blogger.googleusercontent.com/img/r29vz2xl/avvxseiinxxwrvjqr3tzj4-o3ipkdjriuqfwor4i2cfwor4i2cfwor4i2ctr4i2ctrbi2ctrbi2ctrbi2spymiswx6uzjw0gzc75mxmxnpknpknpknpknppm a a4r3v4c-hesnhdec-juux31zzmdhdddddddddqnd0wo6ibt7qbzfayn_yx1myh77k-ruo9fjd33silnp0jlnjlnjlnjlnjlnjlnjlnjlnjlnjlnjlnjlndehbsr7/s16000/s16000/s16000/s16000/image图>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;我们的工作流的框图，用于使用现有的ScreenAI模型和LLMS生成QA，摘要和导航任务的数据。每个任务都使用自定义提示来强调所需方面，例如与计数，涉及推理等有关的问题，等等&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;br />; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-left：auto; margin-right：auto;”>; &lt;tbody>; &lt;trbody>; &lt;tr>; &lt;tr>; &lt;td style =“ text-align：text-align： center;&quot;>;&lt;img height=&quot;540&quot; src=&quot;https://lh7-us.googleusercontent.com/LmUtXBMXK-zy_rMShHQ_Hk4vQeXu2Kpx8zfzjhE3uAREczbkbGTEjZ7OMTbqtB37lD4rF31xJsoWdVXNAXLbbM1Uc_01WZWmOfBg9RwyAUEToPpa1W38Pt117Zj5LrNfnxXqjXoAJDZd-zcAIgU4QSoBaAKsIrSi8_POI14F5hguN1NJL9a2RsrKg6WHz7w&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top ：0px;” width =“ 705”/>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; llm生成的数据。屏幕质量检查，导航和摘要的示例。对于导航，操作边界框在屏幕截图上以红色显示。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt; BR/>; &lt;/div>; &lt;h2>;实验和结果&lt;/h2>; &lt;p>;如前所述，ScreenAI分为两个阶段：预训练和微调。培训前数据标签是使用自我监督学习获得的，微调数据标签来自人类评估者。 &lt;/p>; &lt;p>;我们使用公共质量检查，摘要和导航数据集以及与UIS相关的各种任务进行微调。对于QA，我们在多模式和文档理解字段中使用良好的基准，例如&lt;a href=&quot;https://github.com/vis-nlp/chartqa&quot;>; charticqa &lt;/a>;，&lt;a href =“ https ：//rrc.cvc.uab.es/？ch = 17＆amp; com =评估＆amp; task = 1“>; docvqa &lt;/a>;，&lt;a href =” = 17＆amp; com =任务“>;多页docvqa &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>; infocricovqa &lt;/a>;，&lt;a href =“ -vqa.github.io/&quot;>; ocr vqa &lt;/a>;，&lt;a href=&quot;https://x-lance.github.io/websrc/&quot;>; web src &lt;/a>;和&lt;a href =“ https ：//github.com/google-research-datasets/screen_qa“>; screenqa &lt;/a>;。对于导航，使用的数据集包括&lt;a href=&quot;https://github.com/google-research-datasets/uibert/uibert/tree/main&quot;>; referring表达式&lt;/a>;，&lt;a href =“ https：// github。 com/aburns4/motif“>;主题&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2209.15099 &quot;>; mug &lt;/a>;和&lt;a href =” https://github.com /google-Research/google-research/tree/master/android_in_the_wild>; android在野外&lt;/a>;。最后，我们使用&lt;a href=&quot;https://github.com/google-research-datasets/screen2words&quot;>; screen2words &lt;/a>;用于屏幕摘要和&lt;a href =“ https://paperswithcode.com/paper/paper/小部件限制生成 - 生成自然语言/评论/“>;窗口小部件字幕&lt;/a>;用于描述特定的UI元素。除了微调数据集外，我们还使用三个新颖的基准评估了微调的ScreenAI模型：&lt;/p>; &lt;ol>; &lt;li>;屏幕注释：启用评估模型布局注释和空间理解功能。 &lt;/li>; &lt;li>; ScreenQA简短：ScreenQA的一种变体，其基础真理答案已缩短，仅包含与其他质量检查任务相符的相关信息。 &lt;/li>; &lt;li>;复杂的screenqa：补充ScreenQA简短，带有更困难的问题（计数，算术，比较和不可吻合的问题），并包含具有各种纵横比的屏幕。 &lt;/li>; &lt;/ol>; &lt;p>;微调的Screenai模型在各种基于UI和信息图的任务上获得了最新的结果（&lt;a href =“ https：//x-lance.github。 io/webRC/“>; webRC &lt;/a>;和&lt;a href=&quot;https://github.com/aburns4/motif&quot;>; Motif &lt;/a>;）和&lt;a href =的最佳表现：//github.com/vis-nlp/chartqa“>;图表qa &lt;/a>;，&lt;a href =” https://rrc.cvc.uab.es/?ch=17＆comp = evaluation＆comptulation＆amp = evaluation＆amp; “>; docvqa &lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>; InfographicVQA &lt;/a>;与大小相似的模型相比。 Screenai在Screen2Words和OCR-VQA上实现了竞争性能。此外，我们报告了引入的新基准数据集的结果，以作为进一步研究的基准。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s1183/image2.png “ style =”保证金左：自动右右：自动;”>; &lt;img border =“ 0” data-original-height =“ 1137” data-original-width =“ 1183” blogger.googleusercontent.com/img/b/r29vz2xl/avvxseijjaw824ldvbrfu3c7oerx9ik9ik86dwnuq2nqlilpuzlpuzlpuzlpuzlpuzkkkkkkkkkzzzzsswxswsswsswsswsswsswsswsmfyoswns gwjrdsccj3umxtllya tllya fbnpydws1ya2dhdeyfihwl1mvcytwizdgfblxawoxukwwwwww1vllwfnwmnkq64b8wum5slnkgegdgxxlr7/s16000/s16000/s16000/image2.png2.png“/>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>;字幕“ style =” text-align：中心;“>;将ScreenAi的模型性能与类似大小的最新模型（SOTA）进行比较。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;接下来，我们检查了Screenai的缩放功能，并观察到在所有任务中，增加模型尺寸可改善性能，并且改进的尺寸尚未在最大尺寸的情况下饱和。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s1999/image1 。 //blogger.googleusercontent.com/img/b/r29vz2xl/avvxsejknmvtyz1rhm0wqgn7eagb9lev3 869f7dmyjcthlj6c0clwzmagp8hm9amxdck92d4pl2ujz-ti4czsqolzlecmlgelwbjl9fztj-zwiwata2k/s16000/s16000/image1.png =“ tr caption”样式=“ text-align：center;”>;模型性能随大小增加而增加，即使在5B参数的最大尺寸下，性能也没有饱和。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们与统一一起介绍了Screenai模型使我们能够开发从所有这些域中利用数据的自我监督的学习任务的表示。我们还说明了使用LLM的数据生成的影响，并通过修改训练混合物来研究改善模型性能在特定方面。我们应用所有这些技术来构建经过多任务训练的模型，以在许多公共基准上采用最先进的方法来竞争性能。但是，我们还注意到，我们的方法仍然落后于大型模型，需要进一步的研究来弥合这一差距。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; Wang，Fedir Zubach，Hassan Mansoor，Vincent Etter，Victor Carbune，Jason Lin，Jindong Chen和Abhanshu Sharma。我们感谢Fangyu Liu，Xi Chen，Efi Kokiopoulou，Jesse Berent，Gabriel Barcik，Lukas Zilka，Oriana Riva，Gang Li，Yang Li，Radu Soricut，Radu Soricut和Tania Bedrax-Weiss以及Rahul Aralikatte，Hao，Hao，Hao，以及RAHULAX-WEIS Cheng和Daniel Kim在数据准备方面的支持。我们还要感谢Jay Yagnik，Blaise Aguera Y Arcas，Ewa Dominowska，David Petrou和Matt Sharifi的领导，愿景和支持。我们非常感激能够帮助我们在这篇文章中创建动画。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/5200874294579797373735/comments/comments/comments/comments/default “ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/screenai-viseal-visual-visual-language-model--model-- for-ui.html＃comment-form“ rel =” reply =“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http:/ http://www.blogger.com/feeds/847492626331452026262626262626262626/posts /default/520087429457973735“ rel =“ edit” type =“ application/atom+xml”/>; &lt;link href =” “ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/screenai-visual-visual-visual-visual-language-model-model-for-ui.html“ title =“ screenai：UI和视觉上的语言理解的视觉语言模型” type =“ text/html”/>; &lt;aunder>; &lt;names>; google ai &lt;/name>; &lt;uri>; http：//www.blogger。 com/profile/120986265147775266161 &lt;/uri>; &lt;email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s72-c/ScreenAI%20-%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search 。 -4328167517765145678 &lt;/id>; &lt;/id>; &lt;出版>; 2024-03-19T08：00：00.000-07：00 &lt;/00 &lt;/publined>; &lt;更新>; 2024-03-19T08：00. 00：00.150-07：00.150-07：00 &lt;/opect>; &lt;/00 &lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/ “ http://www.blogger.com/atom/ns#” term =“ crowd-sourcing”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term = “数据集”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“多样性”>; &lt;/category>; &lt;类别>; .com/atom/ns＃“ term =“ health”>; &lt;/category>; &lt;title type =“ text”>; scin：代表性皮肤病学图像的新资源&lt;/stitle>; &lt;content type =“ html”>; =&quot;byline-author&quot;>;Posted by Pooja Rao, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s1600/ scinhero.png“ style =” display：none;” />; &lt;p>;健康数据集在研究和医学教育中起着至关重要的作用，但是创建代表现实世界的数据集可能会很具有挑战性。例如，皮肤病学条件的外观和严重程度各不相同，并且在肤色之间表现出不同的表现。然而，现有的皮肤病学图像数据集通常缺乏日常状况（例如皮疹，过敏和感染）的代表，并且偏向更轻的肤色。此外，种族和种族信息经常缺少，阻碍了我们评估差异或创建解决方案的能力。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;要解决这些限制，我们正在发布&lt;a href=&quot;https://github.com/google-research-research-datasets/scin&quot;>;皮肤状况图像网络（SCIN）数据集&lt;/a>;与医生与&lt;a href=&quot;https://med.stanford.edu/&quot;>; Stanford Medicine合作。我们设计了SCIN，以反映人们在线搜索的广泛关注，并补充了临床数据集中通常发现的条件类型。它包含各种肤色和身体部位的图像，有助于确保未来的AI工具有效地适用于所有人。我们已经制作了&lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>; SCIN数据集&lt;/a>;作为研究人员，教育者和开发人员，以及以及采取仔细的步骤来保护贡献者的隐私。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s1327/image1.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 1118” data-Original-width =“ 1327” src =“ https ：//blogger.googleusercontent.com/img/b/r29vz2xl/avvxsei-lvudxsy1bc8xxerfkgtdyrick25knkkzw2dcvmuyvvzfvzfvzfvzfvzfvm7laur5fdnur5fdnugn gudy6666666645 gudylij lug lij lij lij lij lij lij lij lij lij lij vplnpi1thl_bctcqcglr670v4szxkhlcdvajbp7t8hs7u3asnhh_swhsxokjsssln-1ipupypypysj5erdhaduz5r/ s16000/s16000/image1.png -caption“ style =” text-align：center;“>; scin数据集中的图像和元数据示例。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =” line-height： 40％;“>; &lt;br>; &lt;/div>; &lt;h2>;数据集组成&lt;/h2>; &lt;p>; SCIN数据集当前包含10,000多张皮肤，指甲或头发状况，这是由经历它们的个人直接贡献的。根据机构审查委员会批准的研究，在美国个人的知情同意下，自愿做出了所有贡献。为了提供回顾性皮肤科医生标签的背景，要求贡献者拍摄特写图像，并从稍微远离拍摄图像。他们可以选择自我报告人口统计信息和&lt;a href=&quot;https://en.wikipedia.org/wiki/fitzpatrick_scale&quot;>; tanning Propents &lt;/a>;（自我报告的Fitzpatrick皮肤类型，IE，IE，SFST） ，并描述与它们的关注有关的质地，持续时间和症状。 &lt;/p>; &lt;p>; 一到三名皮肤科医生用最多五个皮肤病学条件标记每个贡献，以及每个标签的置信度得分。 SCIN数据集包含这些单独的标签，以及从中得出的聚集和加权鉴别诊断，可用于模型测试或训练。这些标签是追溯分配的，并不等于临床诊断，但是它们使我们能够将SCIN数据集中皮肤病条件的分布与现有数据集进行比较。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s1851/image2.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 775” data-Original-width =“ 1851” src =“ src =” https ：//blogger.googleusercontent.com/img/b/r29vz2xl/avvxsei7oye7nkevgbaw6sehfgfgfgfzcrhnkqcrhnkqx5w86_7ujhmbpmenobyxccutmenobyxccutgmenbpmenbpmenbpmenbpmen 93I3A69KUFOZY54DIZXH_WCF1PEKDFLRBEE_OZODXSS0N4ZRHSAIKI8ZSLUFFUATW4W-0P0P0ZZD4GRWLQGMPLR6GW/S16000/s16000/image2.png2.png -caption“ style =” text-align：中心;“>; SCIN数据集在很大程度上包含过敏，炎症和传染性状况，而来自临床来源的数据集则集中于良性和恶性&lt;a href &lt;a href =“ https:///en.wikipedia.orgg/ wiki/delasm“>;肿瘤&lt;/a>;。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/take>; &lt;p>;，而许多现有的皮肤病学数据集则集中于恶性肿瘤和良性肿瘤，并旨在帮助皮肤癌诊断，SCIN数据集主要由常见的过敏，炎症和感染状况组成。 SCIN数据集中的大多数图像都显示出早期的问题 - 超过一半的照片在照片前不到一周，而30％的出现在拍摄图像之前的一天不到一天。在卫生系统中很少看到此时间窗口中的条件，因此在现有的皮肤病学数据集中的代表性不足。 &lt;/p>; &lt;p>;我们还获得了菲茨帕特里克皮肤类型（估计的FST或EFST）和外行标记估计&lt;a href=&quot;htttps:///en.wikipedia.org/wiki/wiki/monk_skin_skin_scale &quot;>; monk蒙克肤色&quot;>; monk_skin_scale&quot;>; &lt;/a>;（emst）图像。这样可以将皮肤状况和皮肤类型分布与现有皮肤科数据集中的患者进行比较。尽管我们没有选择性地瞄准任何皮肤类型或肤色，但与临床来源的类似数据集相比，SCIN数据集具有平衡的Fitzpatrick皮肤类型分布（3、4、5和6类型）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s1851 /image3.png“ ImageAnchor =“ 1”样式=“边距左：自动右：自动;”>; &lt;img border =“ 0” data-original-height =“ 620” data-original-width =“ 1851&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;自我报告和皮肤科医生估计的scin数据集中的菲茨帕特里克皮肤类型分布与现有的未增强的皮肤病数据集相比a href =“ https://github.com/mattgroh/fitzpatrick17k”>;（fitzpatrick17k17k &lt;/a>;，&lt;a href=&quot;https://wwwww.fc.up.ppt.ppt.pt.pt/addi/addi/addi/addi/ph2%20database.htmll>; ph²&lt;/a>;，&lt;a href=&quot;https://www.it.pt/automaticpage?id=3459&quot;>; skinl2 &lt;/a>;和&lt;a href =“ https：//www.ncbi.nlm。 nih.gov/pmc/articles/pmc7479321/“>; pad-ufes-20 &lt;/a>;）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>; &lt;a href =“ href =” https：https： //en.wikipedia.org/wiki/fitzpatrick_scale&quot;>; fitzpatrick皮肤类型&lt;/a>;量表最初是作为照相量表开发的，用于测量皮肤类型对UV辐射的响应，并且在皮肤病学研究中广泛使用。和尚肤色量表是一种较新的10遮盖量表，可测量肤色而不是皮肤色谱，从而捕捉到较深的肤色之间的差异更细微的差异。虽然这两种量表都旨在使用图像进行回顾性估算，但这些标签的包含旨在使未来对皮肤病学中皮肤类型和音调表示的研究。例如，SCIN数据集为美国人群中这些皮肤类型和色调的分布提供了初始基准。 &lt;/p>; &lt;p>; SCIN数据集对女性和年轻人的表现很高，可能反映了各种因素的组合。这些可能包括皮肤状况发生率的差异，在线寻求健康信息的倾向以及愿意为跨人口统计研究做出贡献的差异。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;众包方法&lt;/h2>; &lt;p>;为了创建SCIN数据集，我们使用了一种新颖的众库方法，我们在随附的&lt;a href=&quot;https://arxiv.org/abs/2402.18545&quot;>;研究论文&lt;/a>;与调查人员合作，&lt;a href =“ https://med.stanford.edu /“>;斯坦福医学&lt;/a>;。这种方法使个人能够在医疗研究中发挥积极作用。它使我们能够在他们的健康问题的早期阶段接触到他们寻求正式护理之前。至关重要的是，此方法使用网络搜索结果页面上的广告（许多人的健康之旅的起点）与参与者建立联系。 &lt;/p>; &lt;p>;我们的结果表明，众包可以产生低垃圾邮件速率的高质量数据集。超过97.5％的贡献是皮肤状况的真实图像。在执行了进一步的过滤步骤以排除SCIN数据集范围不超出范围并删除重复项的图像之后，我们能够释放在8个月的研究期内获得的近90％的贡献。大多数图像敏锐而曝光良好。大约一半的贡献包括自我报告的人口统计，80％包含与皮肤状况有关的自我报告的信息，例如纹理，持续时间或其他症状。我们发现，皮肤科医生回顾性分配鉴别诊断的能力更多地取决于自我报告的信息的可用性，而不是图像质量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s1999/image4.png&quot; imageanchor =“ 1” style =“边距左：auto; margin-right：auto;”>; &lt;img border =“ 0” data-eriginal-height =“ 1610” data-Original-width =“ 1999” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：中心;“>;皮肤科医生对标签的信心（比例为1-5）取决于自我报告的人口统计和症状信息的可用性。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody >; &lt;/table>; &lt;p>;虽然无法保证完美的图像去识别，但保护贡献图像的个人的隐私是创建SCIN数据集时的重中之重。通过知情同意书，贡献者知道潜在的重新识别风险，并建议避免将图像上载具有识别功能。提交后隐私保护措施包括手动修订或裁剪，以排除潜在的识别区域，反向图像搜索以排除公开可用的副本以及去除元数据或汇总。 scin &lt;a href=&quot;https://github.com/google-research-datasets/scin?tab=license-1-ov-file#readme&quot;>;数据使用许可证&lt;/a>;禁止尝试重新识别贡献者的尝试。 &lt;/p>; &lt;p>;我们希望SCIN数据集将是那些致力于推进包容性皮肤病学研究，教育和AI工具开发的人的有用资源。通过演示传统数据集创建方法的替代方案，Scin在自我报告的数据或回顾性标签是可行的领域为更多代表性的数据集铺平了道路。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>; bensentedgments &lt;/h2>; &lt;p>; &lt;em>;我们感谢所有的共同作者Abbi Ward ，吉米·李（Jimmy Li），朱莉·王（Julie Wang），斯里拉姆·拉克斯米纳（Sriram Lakshminarasimhan），阿什利·卡里克（Ashley Carrick），比尔森·坎帕纳（Bilson Campana），杰伊·哈特福德（Jay Hartford），杰伊·哈特福德（Jay Hartford），普拉德普·库玛（Pradeep Kumar），蒂亚·蒂亚西里索（Tiya Tiyasirisokchai），桑尼·维尔曼尼（Sunny Virmani ），史蒂文·林（Steven Lin）（斯坦福医学），贾斯汀·科（Stanford Ko）（斯坦福医学），艾伦·卡尔西卡扬灵（Alan Karthikesalingam）和克里斯托弗·苏格尔（Christopher Spers）。我们还要感谢Yetunde Ibitoye，Sami Lachgar，Lisa Lehmann，Javier Perez，Margaret Ann Smith（Stanford Medicine），Rachelle Sico，Amit Talreja，Annisah Um&#39;rani和Wayne Westerlind对这项工作的重要贡献。最后，我们感谢Heather Cole-Lewis，Naama Hammel，Ivor Horn，Michael Howell，Yun Liu和Eric Teasley对研究设计和手稿的见解。 &lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/43281675177765145678/comments/comments/default” /atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/scin-new-resource-for-representative.html#comment-form“ 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/43281675177777777765145145678 “/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626/ ：//blog.research.google/2024/03/scin-new-resource-for-merpresentative.html“ rel =“替代” title =“ scin：代表性皮肤病学图像的新资源”类型=“ text/html” />; &lt;ause>; &lt;name>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/emaglogger.com &lt;/email>; height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/img/b16-round.gif.gif” >; &lt;/gd：图像>; &lt;/wurs>; &lt;媒体：Thumbnail Height =“ 72“ url =” pn7oynzo3cdkpargn32b4o-t8zd6xcpxmubte1-spbi6j05y5_urfbwsmtjnpldkyzm3xjxoc0iwu7q_a7ktfi2s1hvhly8uqhly8uq1986 ep c1986-pggejqnelnusubjquubjqnelnususubjquubjqnelnusubj s72 cin7ktfi2s1hvhly 。条目>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.POST-757976001746578714 &lt;/id>; &lt;/id>; &lt;出版>; 2024-03-18T11：41：41：00.000-07：00.000-07：00：00：00：00：00：00 &lt;/00 &lt;/00 &lt;/ipland>; 20224-03 -18T12：01：42.865-07：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MELON：从姿势未知的图像重建 3D 对象&lt;/stitle>;&lt;content type =“ html”>; &lt;span class =“ byline-author”>;由高级软件工程师马克·马修斯（Mark Matthews）和研究科学家Dmitry Lagun发表，Google Research &lt;/span>; &lt;img src =“ https：//blogger.googleusercontent. 。 P_GVAY-MTNOK9IVJDVT_CCZVR6IP_R8YV32MHTW2-FCKCTF4UOFRGMYQ3PCCKZAZ-NYMCE/s320/s320/melon％20HERO.JPG“ />; &lt;p>;一个人的先前经验和对世界的理解通常使他们能够轻松地推断对象的整体外观，即使只看了几张2D图片。但是，只有几个图像，计算机可以在3D中重建对象的形状的能力一直是一个困难的算法问题。这个基本的计算机视觉任务具有从创建电子商务3D模型到自动驾驶汽车导航的应用程序。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;问题的关键部分是如何确定拍摄图像的确切位置，称为&lt;em>; pose推理&lt;/em>;。如果已知相机姿势，则一系列成功的技术 - 例如&lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>; Neural Radiance Fields &lt;/a>;（nerf）或&lt;a href =“ https：https： //repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&quot;>; 3d高斯分裂&lt;/a>;  - 可以在3D中重建一个对象。但是，如果这些姿势不可用，那么我们将面临一个困难的“鸡蛋”问题，如果我们知道3D对象，我们可以确定姿势，但是直到我们知道相机姿势之前，我们才能重建3D对象。伪符号使问题更难解决 - 即，从不同角度看时，许多物体看起来相似。例如，像椅子这样的方形物体倾向于每90°旋转时看起来相似。可以通过从各个角度在转盘上渲染并绘制其光度&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/wiki/selfsimarility&quot;>; self-sim-sim-sim-sim-sim-sim-sim-sim-sim-sim-sim-sim-a >;地图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s1764/image5.png&quot; style=&quot;左键：自动右右：auto;“>; &lt;img border =“ 0” data-forminal-height =“ 923” data-original-width =“ 1764” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;玩具卡车型号的自相似图。 &lt;strong>;左：&lt;/strong>;模型在转盘上以各种&lt;a href=&quot;https://en.wikipedia.org/wiki/Azimuth&quot;>;方位角&lt;/a>;、θ 进行渲染。 &lt;strong>;右：&lt;/strong>;平均&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/norm_(mathematics)#euclidean_norm&quot;>; l2 &lt;/a>; rgb与此相似的RGB相似之处θ*。伪相似性用虚线的红线表示。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;上面的图仅可视化旋转的一个维度。当引入更多的自由度时，它变得更加复杂（很难可视化）。伪符号使问题&lt;em>; &lt;em>; &lt;/em>; &lt;/em>;，幼稚的方法通常会融合到局部最小值。在实践中，这种方法可能会将背景视为对象的正面视图，因为它们共享类似的轮廓。以前的技术（例如&lt;a href=&quot;https://chenhsuanlin.bitbucket.io/bundle-adjusting-nerf/&quot;>; barf &lt;/a>;或“>; Samurai &lt;/a>;）通过依靠最初接近全球最小值的初始姿势估计来进行侧链。但是，如果不可用，我们该如何处理呢？ &lt;/p>; &lt;p>;方法，例如&lt;a href=&quot;https://openaccess.thecvf.com/content/content/iccv2021/papers/papers/menggnerf_gan-gan-neurf_neurf_neural_radiance_field_field_field_pose_camera_camera_iccamer_iccv_2021_2121_ppaper.ppaper.ppaper.ppaper.ppaper.ppaper anf &lt;/gnerf =>; https://dl.acm.org/doi/10.1145/3503161.3548078&quot;>; vmrf &lt;/a>;杠杆（甘斯）克服问题。这些技术具有人为地“扩大”有限数量的培训观点，有助于重建的能力。但是，GAN技术通常具有复杂的，有时不稳定的训练过程，使实践中难以实现稳健而可靠的融合。其他一系列成功的方法，例如&lt;a href=&quot;https://openaccess.thecvf.com/content/content/cvpr2023/html/sinhaha_sparsepose_sparsepose_sparsepose_sparse-view_camera-camera_camera_camera_pose_pose_pose_pose_pose_and_red_refinement_red_cvpr_cvpr_cppr_2023_htpprape.htpprape.htpaper.htppaper &lt;/hre &lt;/hre &lt;/hre &lt;a y &lt;a i &lt;/hre y &lt;a = //rust-paper.github.io/&quot;>; rust &lt;/a>;，可以从有限的数字视图中推断出姿势，但需要在大型图像的大数据集中进行预训练，这些图像并不总是可用，并且可以遭受痛苦从“域间隙”问题中推断出对不同类型的图像的姿势。 &lt;/p>; &lt;p>;在“ &lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>; melon：nerf so（3）&lt;/a>; &lt;/a>;”中，在&lt;a href = “ https://3dvconf.github.io/2024/&quot;>; 3dv 2024 &lt;/a>;，我们提出了一种可以确定对象 - 访问摄像机在3D中重建对象时完全来自scratch的技术。 &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON&lt;/a>; (Modulo Equivalent Latent Optimization of NeRF) is one of the first techniques that can do this without initial pose camera estimates, complex training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. We demonstrate that MELON can reconstruct a NeRF from unposed images with state-of-the-art accuracy while requiring as few as 4–6 images of an object. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MELON&lt;/h2>; &lt;p>; We leverage two key techniques to aid convergence of this ill-posed问题。 The first is a very lightweight, dynamically trained &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) encoder that regresses camera poses from training images. We pass a downscaled training image to a four layer CNN that infers the camera pose. This CNN is initialized from noise and requires no pre-training. Its capacity is so small that it forces similar looking images to similar poses, providing an implicit regularization greatly aiding convergence. &lt;/p>; &lt;p>; The second technique is a &lt;em>;modulo loss&lt;/em>; that simultaneously considers pseudo symmetries of an object. We render the object from a fixed set of viewpoints for each training image, backpropagating the loss only through the view that best fits the training image. This effectively considers the plausibility of multiple views for each image. In practice, we find &lt;em>;N&lt;/em>;=2 views (viewing an object from the other side) is all that&#39;s required in most cases, but sometimes get better results with &lt;em>;N&lt;/em>;=4 for square objects. &lt;/p>; &lt;p>; These two techniques are integrated into standard NeRF training, except that instead of fixed camera poses, poses are inferred by the CNN and duplicated by the modulo loss. Photometric gradients back-propagate through the best-fitting cameras into the CNN. We observe that cameras generally converge quickly to globally optimal poses (see animation below). After training of the neural field, MELON can synthesize novel views using standard NeRF rendering methods. &lt;/p>; &lt;p>; We simplify the problem by using the &lt;a href=&quot;https://github.com/bmild/nerf&quot;>;NeRF-Synthetic&lt;/a>; dataset, a popular benchmark for NeRF research and common in the pose-inference literature. This synthetic dataset has cameras at precisely fixed distances and a consistent “up” orientation, requiring us to infer only the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spherical_coordinate_system&quot;>;polar coordinates&lt;/a>; of相机。 This is the same as an object at the center of a globe with a camera always pointing at it, moving along the surface. We then only need the latitude and longitude (2 degrees of freedom) to specify the camera pose. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s1315/image4.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;395&quot; data-original-width=&quot;1315&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;MELON uses a dynamically trained lightweight CNN encoder that predicts a pose for each image. Predicted poses are replicated by the &lt;em>;modulo loss, &lt;/em>;which only penalizes the smallest L2 distance from the ground truth color. At evaluation time, the neural field can be used to generate novel views.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们计算两个关键指标来评估 MELON 在 NeRF 合成数据集上的性能。 The error in orientation between the ground truth and inferred poses can be quantified as a single angular error that we average across all training images, the pose error. We then test the accuracy of MELON&#39;s rendered objects from novel views by measuring the &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR) against held out test views. We see that MELON quickly converges to the approximate poses of most cameras within the first 1,000 steps of training, and achieves a competitive PSNR of 27.5 dB after 50k steps. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s960/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Convergence of MELON on a toy truck model during optimization. &lt;strong>;Left&lt;/strong>;: Rendering of the NeRF. &lt;strong>;Right&lt;/strong>;: Polar plot of predicted (blue &lt;em>;x&lt;/em>;), and ground truth (red dot) cameras.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MELON achieves similar results for other scenes in the NeRF Synthetic dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s1999/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Reconstruction quality comparison between ground-truth (GT) and MELON on NeRF-Synthetic scenes after 100k training steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Noisy images&lt;/h3>; &lt;p>; MELON also works well when performing &lt;a href=&quot;https://en.wikipedia. org/wiki/View_synthesis&quot;>;novel view synthesis&lt;/a>; from extremely noisy, unposed images. We add varying amounts, &lt;em>;σ&lt;/em>;, of &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise&quot;>;white Gaussian noise&lt;/a>; to the training images. For example, the object in &lt;em>;σ&lt;/em>;=1.0 below is impossible to make out, yet MELON can determine the pose and generate novel views of the object. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s1182/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1182&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Novel view synthesis from noisy unposed 128×128 images. Top: Example of noise level present in training views. Bottom: Reconstructed model from noisy training views and mean angular pose error.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This perhaps shouldn&#39;t be too surprising, given that techniques like &lt;a href=&quot;https://bmild.github.io/rawnerf/&quot;>;RawNeRF&lt;/a>; have demonstrated NeRF&#39;s excellent de-noising capabilities with known camera poses. The fact that MELON works for noisy images of unknown camera poses so robustly was unexpected. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present MELON, a technique that can determine object-centric camera poses to reconstruct objects in 3D without the need for approximate pose initializations, complex GAN training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. Though we only demonstrated MELON on synthetic images we are adapting our technique to work in real world conditions. See the &lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;paper&lt;/a>; and &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON site&lt;/a>; to learn more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our paper co-authors Axel Levy, Matan Sela, and Gordon Wetzstein, as well as Florian Schroff and Hartwig Adam for continuous help in building this technology. We also thank Matthew Brown, Ricardo Martin-Brualla and Frederic Poitevin for their helpful feedback on the paper draft. We also acknowledge the use of the computational resources at the SLAC Shared Scientific Data Facility (SDF).&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/757976001746578714/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html&quot; rel=&quot;alternate&quot; title=&quot;MELON: Reconstructing 3D objects from images with unknown poses&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s72-c/MELON%20HERO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-977556648557231190&lt;/id>;&lt;published>;2024-03-15T11:22:00.000-07:00&lt;/published>;&lt;updated>;2024-03-15T11:22:13.760-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;HEAL: A framework for health equity assessment of machine learning performance&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mike Schaekermann, Research Scientist, Google Research, and Ivor Horn, Chief Health Equity Officer &amp;amp; Director, Google Core&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s1600/HEAL-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Health equity is a major societal concern worldwide with disparities having many causes. These sources include limitations in access to healthcare, differences in clinical treatment, and even fundamental differences in the diagnostic technology. In dermatology for example, skin cancer outcomes are worse for populations such as minorities, those with lower socioeconomic status, or individuals with limited healthcare access. While there is great promise in recent advances in machine learning (ML) and artificial intelligence (AI) to help improve healthcare, this transition from research to bedside must be accompanied by a careful understanding of whether and how they impact health equity. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;em>;Health equity&lt;/em>; is defined by public health organizations as fairness of opportunity for everyone to be as healthy as possible. Importantly, equity may be different from &lt;em>;equality&lt;/em>;. For example, people with greater barriers to improving their health may require more or different effort to experience this fair opportunity. Similarly, equity is not &lt;em>;fairness&lt;/em>; as defined in the AI for healthcare literature. Whereas AI fairness often strives for equal performance of the AI technology across different patient populations, this does not center the goal of prioritizing performance with respect to pre-existing health disparities. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s1999 /image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1609&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Health equity considerations. An intervention (eg, an ML-based tool, indicated in dark blue) promotes health equity if it helps reduce existing disparities in health outcomes (indicated in lighter blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In “&lt;a href=&quot;https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00058-0/fulltext&quot;>;Health Equity Assessment of machine Learning performance (HEAL): a framework and dermatology AI model case study&lt;/a>;”, published in &lt;a href=&quot;https://www.thelancet.com/journals/eclinm/home&quot;>;&lt;i>;The Lancet eClinicalMedicine&lt;/i>;&lt;/a>;, we propose a methodology to quantitatively assess whether ML-based health technologies perform equitably. In other words, does the ML model perform well for those with the worst health outcomes for the condition(s) the model is meant to address? This goal anchors on the principle that health equity should prioritize and measure model performance with respect to disparate health outcomes, which may be due to a number of factors that include structural inequities (eg, demographic, social, cultural, political, economic, environmental and geographic). &lt;/p>; &lt;br />; &lt;h2>;The health equity framework (HEAL)&lt;/h2>; &lt;p>; The HEAL framework proposes a 4-step process to estimate the likelihood that an ML-based health technology performs equitably: &lt;/p>; &lt;ol>; &lt;li>; Identify factors associated with health inequities and define tool performance metrics, &lt;/li>; &lt;li>; Identify and quantify pre-existing health disparities, &lt;/li>; &lt;li>; Measure the performance of the tool for each subpopulation, &lt;/li>; &lt;li>; Measure the likelihood that the tool prioritizes performance with respect to health disparities. &lt;/li>; &lt;/ol>; &lt;p>; The final step&#39;s output is termed the HEAL metric, which quantifies how anticorrelated the ML model&#39;s performance is with health disparities. In other words, does the model perform better with populations that have the worse health outcomes? &lt;/p>; &lt;p>; This 4-step process is designed to inform improvements for making ML model performance more equitable, and is meant to be iterative and re-evaluated on a regular basis.例如，步骤（2）中健康结果数据的可用性可以告知步骤（1）中人口因素和括号的选择，并且该框架可以再次应用新的数据集、模型和人群。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s1999/image1.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1352&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Framework for Health Equity Assessment of machine Learning performance (HEAL).&amp;nbsp;Our guiding principle is to avoid exacerbating health inequities, and these steps help us identify disparities and assess for inequitable model performance to move towards better outcomes for all.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With this work, we take a step towards encouraging explicit assessment of the health equity considerations of AI technologies, and encourage prioritization of efforts during model development to reduce health inequities for subpopulations exposed to structural inequities that can precipitate disparate outcomes. We should note that the present framework does not model causal relationships and, therefore, cannot quantify the actual impact a new technology will have on reducing health outcome disparities. However, the HEAL metric may help identify opportunities for improvement, where the current performance is not prioritized with respect to pre-existing health disparities. &lt;/p>; &lt;br />; &lt;h2>;Case study on a dermatology model&lt;/h2>; &lt;p>; As an illustrative case study, we applied the framework to a dermatology model, which utilizes a convolutional neural network similar to that described in &lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;prior work&lt;/a>;. This example dermatology model was trained to classify 288 skin conditions using a development dataset of 29k cases. The input to the model consists of three photos of a skin concern along with demographic information and a brief structured medical history. The output consists of a ranked list of possible matching skin conditions. &lt;/p>; &lt;p>; Using the HEAL framework, we evaluated this model by assessing whether it prioritized performance with respect to pre-existing health outcomes. The model was designed to predict possible dermatologic conditions (from a list of hundreds) based on photos of a skin concern and patient metadata. Evaluation of the model is done using a top-3 agreement metric, which quantifies how often the top 3 output conditions match the most likely condition as suggested by a dermatologist panel. The HEAL metric is computed via the anticorrelation of this top-3 agreement with health outcome rankings. &lt;/p>; &lt;p>;我们使用了一个5,420个远程表现案例的数据集，富含年龄，性别和种族/种族/种族/种族的多样性，以回顾性地评估模型的治疗度量。 The dataset consisted of “store-and-forward” cases from patients of 20 years or older from primary care providers in the USA and skin cancer clinics in Australia. Based on a review of the literature, we decided to explore race/ethnicity, sex and age as potential factors of inequity, and used sampling techniques to ensure that our evaluation dataset had sufficient representation of all race/ethnicity, sex and age groups. To quantify pre-existing health outcomes for each subgroup we relied on measurements from &lt;a href=&quot;https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates/global-health-estimates-leading-causes-of-dalys&quot;>;public&lt;/a>; &lt;a href=&quot;https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30925-9/fulltext&quot;>;databases&lt;/a>; endorsed by the World Health Organization, such as &lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4427&quot;>;Years of Life Lost&lt;/a>; (YLLs) and &lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158&quot;>;Disability-Adjusted Life Years&lt;/a>; (DALYs; years of life lost plus years lived with disability). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s1511 /Table1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot; 1511&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s16000/Table1.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;HEAL metric for all dermatologic conditions across race/ethnicity subpopulations, including health outcomes (YLLs per 100,000), model performance ( top-3 agreement), and rankings for health outcomes and tool performance.&lt;br />;(* Higher is better; measures the likelihood the model performs equitably with respect to the axes in this table.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s1518/Table2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;316&quot; data-original-width=&quot;1518&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s16000/Table2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;HEAL metric for all dermatologic conditions across sexes, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table &lt;p>; Our analysis estimated that the model was 80.5% likely to perform equitably across race/ethnicity subgroups and 92.1% likely to perform equitably across性别。 &lt;/p>; &lt;p>; However, while the model was likely to perform equitably across age groups for cancer conditions specifically, we discovered that it had room for improvement across age groups for non-cancer conditions. For example, those 70+ have the poorest health outcomes related to non-cancer skin conditions, yet the model didn&#39;t prioritize performance for this subgroup. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl 6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s1508/Table3 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1508&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPn LkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s16000/Table3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;跨年龄组的所有癌症和非癌症皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 DALY）、模型性能（顶部-3 协议），以及健康结果和工具性能的排名。 （* 同上。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;将事物放在上下文中&lt;/h2>; &lt;p>;对于整体评估，不能采用 HEAL 指标隔离中。相反，该指标应与许多其他因素结合起来，从计算效率和数据隐私到道德价值观，以及可能影响结果的方面（例如，选择偏差或不同人口群体的评估数据的代表性差异）。 &lt;/p>; &lt;p>; 作为一个对抗性的例子，可以通过故意降低最有利的子群体的模型性能来人为地改进 HEAL 指标，直到该子群体的性能比所有其他子群体更差。出于说明目的，给定亚群 A 和 B，其中 A 的健康结果比 B 差，请考虑在两个模型之间进行选择：模型 1 (M1) 对亚群 A 的表现比对亚群 B 的表现好 5%。模型 2 (M2) 的表现好 5%子群体 A 比 B 更差。M1 的 HEAL 指标会更高，因为它优先考虑结果更差的子群体的表现。然而，M1 对于亚群 A 和 B 的绝对性能可能分别仅为 75% 和 70%，而 M2 对于亚群 A 和 B 的绝对性能分别为 75% 和 80%。选择 M1 而不是 M2 会导致所有子群体的整体表现更差，因为有些子群体的状况更糟，而没有子群体的状况更好。 &lt;/p>; &lt;p>; 因此，HEAL 指标应与&lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_efficiency&quot;>;帕累托条件&lt;/a>;一起使用（本文将进一步讨论） , which restricts model changes so that outcomes for each subpopulation are either unchanged or improved compared to the status quo, and performance does not worsen for any subpopulation. &lt;/p>; &lt;p>; HEAL 框架目前的形式评估了基于机器学习的模型相对于特定亚群预先存在的健康差异优先考虑亚群表现的可能性。这与了解机器学习是否会减少现实中不同亚人群结果差异的目标不同。具体来说，对结果的改善进行建模需要对使用任何给定模型之前和之后发生的护理旅程中的步骤进行因果理解。需要未来的研究来解决这一差距。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; HEAL 框架可以定量评估健康人工智能技术在健康差异方面优先考虑性能的可能性。该案例研究展示了如何将该框架应用于皮肤病学领域，表明模型性能很可能优先考虑性别和种族/民族之间的健康差异，但也揭示了改善跨年龄非癌症疾病的潜力。该案例研究还说明了应用该框架所有建议方面的能力的局限性（例如，绘制社会背景、数据的可用性），从而凸显了基于机器学习的工具的健康公平考虑的复杂性。 &lt;/p>; &lt;p>; 这项工作是为应对人工智能和健康公平的巨大挑战而提出的方法，不仅可以在模型开发过程中提供有用的评估框架，而且可以在预实施和现实世界监测阶段提供有用的评估框架，例如，以健康公平仪表板的形式。我们认为 HEAL 框架的优势在于其未来在各种人工智能工具和用例中的应用以及在此过程中的完善。最后，我们承认，了解人工智能技术对健康公平影响的成功方法需要的不仅仅是一组指标。这需要代表受模型影响最大的群体的社区商定的一系列目标。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的研究是 Google 多个团队的共同工作。我们感谢所有合著者：Terry Spitz、Malcolm Pyles、Heather Cole-Lewis、Ellery Wulczyn、Stephen R. Pfohl、Donald Martin, Jr.、Ronnachai Jaroensri、Geoff Keeling、Yuan Liu、Stephanie Farquhar、Qinghan Xu、 Jenna Lester、Cían Hughes、Patricia Strachan、Fraser Tan、Peggy Bui、Craig H. Mermel、Lily H. Peng、Yossi Matias、Greg S. Corrado、Dale R. Webster、Sunny Virmani、Christopher Semturs、Yun Liu 和 Po-陈宣.卡梅隆.我们还感谢 Lauren Winer、Sami Lachgar、Ting-An Lin、Aaron Loh、Morgan Du、Jenny Rizk、Renee Wong、Ashley Carrick、Preeti Singh、Annisah Um&#39;rani、Jessica Schrouff、Alexander Brown 和 Anna Iurchenko 对我们的支持此项目。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/977556648557231190/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;edit&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html&quot; rel=&quot;alternate&quot; title=&quot;HEAL：机器学习性能健康公平评估框架&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkEC IxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSaf2h0jETJ1PemIR5I6o9pIIW/s72 -c/HEAL-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7868032799856333119&lt;/id>;&lt;已发布>;2024-03-14T12:38:00.000-07:00&lt;/已发布>; &lt;更新>;2024-03-14T12:38:11.597-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/” ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;Cappy：用小记分器超越并提升大型多任务语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：软件工程师 Yun Zhu 和 Lijuan Liu , Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0E jUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s320/Cappy%20hero.jpg&quot;样式= “显示：无；” />; &lt;p>; 大型语言模型 (LLM) 的进步催生了一种新范式，将各种自然语言处理 (NLP) 任务统一在指令跟踪框架内。这种范式以最近的多任务法学硕士为例，例如 &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;、&lt;a href=&quot;https://arxiv.org/ abs/2210.11416&quot;>;FLAN&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML&lt;/a>;。首先，收集多任务数据，每个任务都遵循特定于任务的模板，其中每个标记的示例都转换为指令（例如，&lt;em>;“&lt;/em>;将概念放在一起形成一个句子：滑雪，山，滑雪者&lt;em>;”&lt;/em>;）与相应的响应配对（例如，&lt;em>;“&lt;/em>;滑雪者滑雪下山&lt;em>;”&lt;/em>;）这些指令-响应对用于训练 LLM，产生一个条件生成模型，该模型将指令作为输入并生成响应。此外，多任务 LLM 表现出卓越的任务泛化能力，因为它们可以通过理解和解决全新指令来解决看不见的任务。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt- ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6u whawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s640/Cappy% 20instruction-following.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;177&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOT NGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s16000/Cappy%20instruction-following.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;多任务LLM（例如FLAN）的指令跟踪预训练演示。该范式下的预训练任务提高了未见任务的性能。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 由于仅使用指令理解和解决各种任务的复杂性，多任务 LLM 的大小通常从数十亿个参数到数千亿个参数（例如，&lt;a href=&quot;https ://arxiv.org/abs/2210.11416&quot;>;FLAN-11B&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0-11B&lt;/a>; 和 &lt;a href= “https://arxiv.org/abs/2212.12017&quot;>;OPT-IML-175B&lt;/a>;）。因此，操作如此庞大的模型带来了巨大的挑战，因为它们需要相当大的计算能力，并对 GPU 和 TPU 的内存容量提出了很高的要求，使得它们的训练和推理成本高昂且效率低下。需要大量存储来为每个下游任务维护唯一的 LLM 副本。此外，最强大的多任务LLM（例如FLAN-PaLM-540B）是闭源的，这使得它们不可能被改编。然而，在实际应用中，利用单个多任务LLM以零样本的方式管理所有可以想象的任务仍然很困难，特别是在处理复杂任务、个性化任务以及无法使用指令简洁定义的任务时。另一方面，如果不结合丰富的先验知识，下游训练数据的大小通常不足以很好地训练模型。因此，长期以来人们一直希望使法学硕士能够接受下游监管，同时绕过存储、内存和访问问题。 &lt;/p>; &lt;p>; 某些&lt;em>;参数高效调整&lt;/em>;策略，包括&lt;a href=&quot;https://aclanthology.org/2021.acl-long.353.pdf&quot;>;提示调整&lt;/em>; a>; 和 &lt;a href=&quot;https://openreview.net/pdf?id=nZeVKeeFYf9&quot;>;适配器&lt;/a>; 大大减少了存储需求，但它们在调整过程中仍然通过 LLM 参数执行反向传播，从而保持高记忆力要求。此外，一些&lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/2301.00234.pdf&quot;>;上下文学习&lt;/a>;&lt;/em>;技术通过集成有限数量的监督示例来规避参数调整进入指令。然而，这些技术受到模型最大输入长度的限制，仅允许少量样本来指导任务解决。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2311.06720&quot;>;Cappy：使用小型记分器超越并提升大型多任务 LM&lt;/a>;”中，发表于&lt; a href=&quot;https://nips.cc/virtual/2023/index.html&quot;>;NeurIPS 2023&lt;/a>;，我们提出了一种新方法，可以提高多任务 LLM 的性能和效率。我们引入了一种轻量级预训练记分器 Cappy，它基于 &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; 的持续预训练，仅包含 3.6 亿个参数。 Cappy 将指令和候选响应作为输入，并产生 0 到 1 之间的分数，指示响应相对于指令的估计正确性。 Cappy 可以独立执行分类任务，也可以作为法学硕士的辅助组件，提高其性能。此外，Cappy 可以有效地实现下游监督，无需任何微调，从而避免了通过 LLM 参数进行反向传播的需要，并减少了内存需求。最后，Cappy 的适配不需要访问 LLM 参数，因为它与闭源多任务 LLM 兼容，例如那些只能通过 WebAPI 访问的参数。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-m fiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s1999/Cappy %20overview.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;975&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1 knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s16000/Cappy%20overview.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy 将指令和响应对作为输入，并输出 0 到 1 之间的分数，表示对响应正确性的估计遵守指示。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;预训练&lt;/h2>; &lt;p>; 我们从相同的数据集集合开始，其中包括使用的来自 &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; 的 39 个不同数据集训练&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;。该集合包含广泛的任务类型，例如问答、情感分析和摘要。每个数据集都与一个或多个模板相关联，这些模板将每个实例从原始数据集转换为与其真实响应配对的指令。 &lt;/p>; &lt;p>; Cappy 的回归建模要求每个预训练数据实例包含一个指令-响应对以及响应的正确性注释，因此我们生成一个具有范围从 0 到 1 的正确性注释的数据集。在生成任务中的实例中，我们利用现有的多任务 LLM 通过采样生成多个响应，以给定的指令为条件。随后，我们利用响应与实例的真实响应之间的相似性，为指令和每个响应形成的对分配一个注释。具体来说，我们采用 &lt;a href=&quot;https://aclanthology.org/W04-1013/&quot;>;Rouge-L&lt;/a>;，这是一种用于衡量整体多任务性能的常用指标，已证明与人类评估，将这种相似性计算为弱监督的一种形式。 &lt;/p>; &lt;p>; 结果，我们获得了包含 1.6 亿个实例的有效回归数据集，并配有正确性分数注释。最终的 Cappy 模型是在 &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; 模型之上使用回归数据集进行连续预训练的结果。 Cappy 的预训练是在 Google 的 &lt;a href=&quot;https://arxiv.org/abs/2304.01433&quot;>;TPU-v4&lt;/a>; 上进行的，使用 &lt;a href=&quot;https://arxiv.org/ pdf/2310.16355.pdf&quot;>;RedCoast&lt;/a>;，一个用于自动化分布式训练的轻量级工具包。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLy ZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s1999/Cappy%20data%20增强.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;438&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z0 3aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdI K_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s16000/Cappy%20data%20augmentation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用多任务LLM进行数据增强，为Cappy的预训练和微调构建弱监督回归数据集。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;应用Cappy&lt;/h2​​>; &lt;p>; Cappy解决实际任务在候选人选择机制内。更具体地说，给定指令和一组候选响应，Cappy 会为每个候选响应生成一个分数。这是通过在每个单独的响应旁边输入指令，然后指定得分最高的响应作为其预测来实现的。在分类任务中，所有候选响应本质上都是预定义的。例如，对于情感分类任务的指令（例如，“根据此评论，用户会推荐该产品吗？：‘即使对于非游戏玩家来说也令人惊叹。’”），候选回答是“是”或“不”。在这种情况下，Cappy 独立运作。另一方面，在生成任务中，候选答案不是预先定义的，需要现有的多任务 LLM 来生成候选答案。在这种情况下，Cappy 充当多任务 LLM 的辅助组件，增强其解码。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;使用 Cappy 调整多任务 LLM &lt;/h3>; &lt;p>; 当有可用的下游训练数据时，Cappy 能够有效且高效地适应下游任务上的多任务 LLM。具体来说，我们对 Cappy 进行微调，将下游任务信息集成到 LLM 预测中。此过程涉及创建特定于下游训练数据的单独回归数据集，并使用与构建预训练数据相同的数据注释过程。因此，经过微调的 Cappy 与多任务 LLM 协作，提高了 LLM 在下游任务上的性能。 &lt;/p>; &lt;p>; 与其他 LLM 调整策略相比，使用 Cappy 调整 LLM 显着降低了对设备内存的高需求，因为它避免了下游任务通过 LLM 参数进行反向传播的需要。此外，Cappy 适配不依赖于对 LLM 参数的访问，使其与闭源多任务 LLM 兼容，例如只能通过 WebAPI 访问的 LLM。与通过将训练示例附加到指令前缀来规避模型调整的上下文学习方法相比，Cappy 不受 LLM 最大输入长度的限制。因此，Cappy 可以合并无限数量的下游训练示例。 Cappy 还可以与其他适应方法一起应用，例如微调和上下文学习，进一步提高其整体性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyn cRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s1999/Cappy%20downstream%20适应.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSX fP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s16000/Cappy%20downstream%20adaptation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy 和依赖于 LLM 参数的方法（例如微调和提示调整）之间的下游适应性比较。 Cappy 的应用程序增强了多任务 LLM。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估了 Cappy 在来自 &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; 的十一项语言理解分类任务中的表现。我们证明，具有 360M 参数的 Cappy 性能优于 OPT-175B 和 OPT-IML-30B，并且与现有最好的多任务 LLM（T0-11B 和 OPT-IML-175B）的准确性相匹配。这些发现凸显了 Cappy 的能力和参数效率，这可以归功于其基于评分的预训练策略，该策略通过区分高质量和低质量响应来整合对比信息。相反，以前的多任务法学硕士完全依赖于仅利用真实答案的&lt;a href=&quot;https://en.wikipedia.org/wiki/Teacher_forcing&quot;>;教师强制培训&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9 ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s1999/Cappy%20精度.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_ 5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s16000/Cappy%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;总体准确度是 PromptSource 十一项测试任务的平均值。 “RM”是指&lt;a href=&quot;https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2&quot;>;预训练的 RLHF 奖励模型&lt;/a>;。 Cappy 与现有多任务 LLM 中最好的匹配。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们还研究了多任务 LLM 与 Cappy 对复杂任务的适应性 &lt;a href =&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;，一组手动策划的任务，被认为超出了许多法学硕士的能力范围。我们专注于所有 45 代 BIG-Bench 任务，特别是那些不提供预先确定的答案选择的任务。我们在每个测试集上使用 Rouge-L 分数（代表模型生成与相应的基本事实之间的总体相似性）来评估性能，报告 45 次测试的平均分数。在这个实验中，FLAN-T5 的所有变体都作为 LLM 的骨干，并且基础 FLAN-T5 模型被冻结。如下所示的这些结果表明，Cappy 大幅提高了 FLAN-T5 模型的性能，始终优于通过使用 LLM 本身的自我评分进行样本选择而实现的最有效基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2f hhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s1999/Cappy%20平均%20Rouge-L%20score.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1625&quot; data-original-width=&quot;1999 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s16000/Cappy%20averaged%20Rouge-L%20score.png&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The averaged Rouge-L score over 45 complex tasks within BIG-Bench. The x-axis refers to FLAN-T5 models of different sizes. Every dashed line represents an approach working on FLAN-T5s. Self-scoring refers to using the cross-entropy of LLM to select responses. Cappy enhances the performance of FLAN-T5 models by a large margin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduce Cappy, a novel approach that enhances the performance and efficiency of multi-task LLMs. In our experiments, we adapt a single LLM to several domains with Cappy. In the future, Cappy as a pre-trained model can potentially be used in other creative ways beyond on single LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢Bowen Tan、Jindong Chen、Lei Meng , Abhanshu Sharma and Ewa Dominowska for their valuable feedback. We would also like to thank Eric Xing and Zhiting Hu for their suggestions. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7868032799856333119/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html&quot; rel=&quot;alternate&quot; title=&quot;Cappy: Outperforming and boosting large multi-task language models with a small scorer&quot; type =&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up -Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s72-c/Cappy%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr: total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6090481872694489715&lt;/id>;&lt;published>;2024-03-12T14:15:00.000 -07:00&lt;/已发布>;&lt;更新>;2024-03-19T09:12:05.568-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”term= &quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Talk like a graph: Encoding graphs for large language models&lt;/stitle>;&lt;content type=&quot;html &quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s1600/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; 想象一下您周围的所有事物 - 您的朋友、厨房里的工具，甚至自行车的零件。 They are all connected in different ways. In computer science, the term &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graph&lt;/a>; &lt;/em>;is used to describe connections between objects. Graphs consist of nodes (the objects themselves) and edges (connections between two nodes, indicating a relationship between them). Graphs are everywhere now. The internet itself is a giant graph of websites linked together.甚至搜索引擎使用的知识也是以类似图表的方式组织的。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Furthermore, consider the remarkable advancements in artificial intelligence — such as chatbots that can write stories in seconds, and even software that can interpret medical reports. This exciting progress is largely thanks to large language models (LLMs). New LLM technology is constantly being developed for different uses. &lt;/p>; &lt;p>; Since graphs are everywhere and LLM technology is on the rise, in “&lt;a href=&quot;https://openreview.net/forum?id=IuXR1CCrSi&quot;>;Talk like a Graph: Encoding Graphs for Large Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/&quot;>;ICLR 2024&lt;/a>;, we present a way to teach powerful LLMs how to better reason with graph information. Graphs are a useful way to organize information, but LLMs are mostly trained on regular text. The objective is to test different techniques to see what works best and gain practical insights. Translating graphs into text that LLMs can understand is a remarkably complex task. The difficulty stems from the inherent complexity of graph structures with multiple nodes and the intricate web of edges that connect them. Our work studies how to take a graph and translate it into a format that an LLM can understand. We also design a benchmark called &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>; to study different approaches on different graph reasoning problems and show how to &lt;em>;phrase&lt;/em>; a graph-related problem in a way that enables the LLM to solve the graph problem. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: 1) the graph encoding method, 2) the nature of the graph task itself, and 3) interestingly, the very structure of the graph considered. These findings give us clues on how to best represent graphs for LLMs. Picking the right method can make the LLM up to 60% better at graph tasks! &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s1600/image7.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Pictured, the process of encoding a graph as text using two different approaches and feeding the text and a question about the graph to the LLM.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Graphs as text&lt;/h2>; &lt;p>; To be able to systematically find out what is the best way to translate a graph to text, we first design a benchmark called &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>;. Think of GraphQA as an exam designed to evaluate powerful LLMs on graph-specific problems. We want to see how well LLMs can understand and solve problems that involve graphs in different setups. To create a comprehensive and realistic exam for LLMs, we don&#39;t just use one type of graph, we use a mix of graphs ensuring breadth in the number of connections. This is mainly because different graph types make solving such problems easier or harder. This way, GraphQA can help expose biases in how an LLM thinks about the graphs, and the whole exam gets closer to a realistic setup that LLMs might encounter in the real world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s1368/image6.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;291&quot; data-original-width=&quot;1368&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Overview of our framework for reasoning with graphs using LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; GraphQA focuses on simple tasks related to graphs, like checking if an edge exists , calculating the number of nodes or edges, finding nodes that are connected to a specific node, and checking for cycles in a graph. These tasks might seem basic, but they require understanding the relationships between nodes and edges. By covering different types of challenges, from identifying patterns to creating new connections, GraphQA helps models learn how to analyze graphs effectively. These basic tasks are crucial for more complex reasoning on graphs, like finding the shortest path between nodes, detecting communities, or identifying influential nodes. Additionally, GraphQA includes generating random graphs using various algorithms like &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;scale-free networks&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabasi-Albert model&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;stochastic block model&lt;/a>;, as well as simpler graph structures like paths, complete graphs, and star graphs, providing a diverse set of data for training. &lt;/p>; &lt;p>;使用图形时，我们还需要找到方法来询问LLMS可以理解的与图形相关的问题。 &lt;em>;Prompting heuristics&lt;/em>; are different strategies for doing this. Let&#39;s break down the common ones: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Zero-shot&lt;/em>;: simply describe the task (&quot;Is there a cycle in this graph?&quot;) and tell the LLM to go为了它。 No examples provided. &lt;/li>;&lt;li>;&lt;em>;Few-shot&lt;/em>;：这就像在真正的交易之前给法学硕士进行一次小型练习测试。 We provide a few example graph questions and their correct answers. &lt;/li>;&lt;li>;&lt;em>;Chain-of-Thought&lt;/em>;: Here, we show the LLM how to break down a problem step-by-step with examples. The goal is to teach it to generate its own &quot;thought process&quot; when faced with new graphs. &lt;/li>;&lt;li>;&lt;em>;Zero-CoT&lt;/em>;: Similar to CoT, but instead of training examples, we give the LLM a simple prompt, like &quot;Let&#39;s think step-by-step,&quot; to trigger its自己解决问题的崩溃。 &lt;/li>;&lt;li>;&lt;em>;BAG（构建图形）&lt;/em>;：这是专门用于图形任务的。 We add the phrase &quot;Let&#39;s build a graph...&quot; to the description, helping the LLM focus on the graph structure. &lt;/li>; &lt;/ul>; &lt;p>; We explored different ways to translate graphs into text that LLMs can work with. Our key questions were: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Node encoding&lt;/em>;: How do we represent individual nodes? Options tested include simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer&quot;>;integers&lt;/a>;, common names (people, characters), and letters. &lt;/li>;&lt;li>;&lt;em>;Edge encoding&lt;/em>;: How do we describe the relationships between nodes? Methods involved parenthesis notation, phrases like &quot;are friends&quot;, and symbolic representations like arrows. &lt;/li>; &lt;/ul>; &lt;p>; Various node and edge encodings were combined systematically. This led to functions like the ones in the following figure: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/s855/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;855&quot; height=&quot;302&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/w640-h302/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of graph encoding functions used to encode graphs via text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Analysis and results&lt;/h2>; &lt;p>; We carried out three key experiments: one to test how LLMs handle graph tasks, and two to understand how the size of the LLM and different graph shapes affected performance. We run all our experiments on GraphQA. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;How LLMs handle graph tasks &lt;/h3>; &lt;p>; In this experiment, we tested how well pre-trained LLMs tackle graph problems like identifying connections, cycles, and node degrees. Here is what we learned: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;LLMs struggle:&lt;/em>; On most of these basic tasks, LLMs did not do much better than a random guess. &lt;/li>;&lt;li>;&lt;em>;Encoding matters significantly&lt;/em>;: How we represent the graph as text has a great effect on LLM performance. The &quot;incident&quot; encoding excelled for most of the tasks in general. &lt;/li>; &lt;/ul>; &lt;p>; Our results are summarized in the following chart. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s1864/image8 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of various graph encoder functions based on their accuracy on different graph tasks. The main conclusion from this figure is that the graph encoding functions matter significantly.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;Bigger is (usually) better &lt;/h3>; &lt;p>; In this experiment, we wanted to see if the size of the LLM (in terms of the number of parameters) affects how well they can handle graph problems 。 For that, we tested the same graph tasks on the XXS, XS, S, and L sizes of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>;. Here is a summary of our findings: &lt;/p>; &lt;ul>; &lt;li>;In general, bigger models did better on graph reasoning tasks. It seems like the extra parameters gave them space to learn more complex patterns. &lt;/li>;&lt;li>;Oddly, size didn&#39;t matter as much for the “edge existence” task (finding out if two nodes in a graph are connected). &lt;/li>;&lt;li>;Even the biggest LLM couldn&#39;t consistently beat a simple baseline solution on the cycle check problem (finding out if a graph contains a cycle or not). This shows LLMs still have room to improve with certain graph tasks. &lt;/li>; &lt;/ul>; &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;&#39;&#39; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/s1227 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;959&quot; data-original-width=&quot;1227&quot; height=&quot; 500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/w640-h500/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;模型容量对PALM 2-XXS，XS，S和L的图形推理任务的影响。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Do different graph shapes confuse LLMs &lt;/h3 >; &lt;p>; We wondered if the &quot;shape&quot; of a graph (how nodes are connected) influences how well LLMs can solve problems on it. Think of the following figure as different examples of graph shapes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s1400/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Samples of graphs generated with different graph generators from GraphQA. ER, BA, SBM, and SFN refers to &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabási–Albert&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;Stochastic Block Model&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;Scale-Free Network&lt;/a>; respectively.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that graph structure has a big impact on LLM performance. For example, in a task asking if a cycle exists, LLMs did great on tightly interconnected graphs (cycles are common there) but struggled on path graphs (where cycles never happen). Interestingly, providing some mixed examples helped it adapt. For instance, for cycle check, we added some examples containing a cycle and some examples with no cycles as few-shot examples in our prompt. Similar patterns occurred with other tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s1864/image5.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Comparing different graph generators on different graph tasks. The main observation here is that graph structure has a significant impact on the LLM&#39;s performance. ER, BA, SBM, and SFN refers to &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabási–Albert&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;Stochastic Block Model&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;Scale-Free Network&lt;/a>; respectively.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In short, we dug deep into how to best represent graphs as text so LLMs can understand them. We found three major factors that make a difference: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;How to translate the graph to text&lt;/em>;: how we represent the graph as text significantly influences LLM performance. The incident encoding excelled for most of the tasks in general.. &lt;/li>;&lt;li>;&lt;em>;Task type&lt;/em>;: Certain types of graph questions tend to be harder for LLMs, even with a good translation from graph to文本。 &lt;/li>;&lt;li>;&lt;em>;Graph structure&lt;/em>;: Surprisingly, the &quot;shape&quot; of the graph that on which we do inference (dense with connections, sparse, etc.) influences how well an LLM does. &lt;/li>; &lt;/ul>; &lt;p>; This study revealed key insights about how to prepare graphs for LLMs. The right encoding techniques can significantly boost an LLM&#39;s accuracy on graph problems (ranging from around 5% to over 60% improvement). Our new benchmark, GraphQA, will help drive further research in this area. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to express our gratitude to our co-author, Jonathan Halcrow, for his valuable contributions to this work. We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. We would also like to extend special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6090481872694489715 /comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like- graph-encoding-graphs-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds /8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html&quot; rel =&quot;alternate&quot; title=&quot;Talk like a graph: Encoding graphs for large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger .com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s72-c/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; width =&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id >;tag:blogger.com,1999:blog-8474926331452026626.post-470840348983280912&lt;/id>;&lt;published>;2024-03-11T12:08:00.000-07:00&lt;/published>;&lt;updated>;2024-03-11T12:13 :03.824-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Chain-of-table: Evolving tables in the reasoning chain for table understanding&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zilong Wang, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNcD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s832/Chain-of-Table.png&quot; style=&quot;display: none;&quot; />; &lt;p>; People use tables every day to organize and interpret complex information in a structured, easily accessible format. Due to the ubiquity of such tables, reasoning over tabular data has long been a central topic in &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP). Researchers in this field have aimed to leverage language models to help users answer questions, verify statements, and analyze data based on tables.然而，语言模型是在大量纯文本上进行训练的，因此表格数据固有的结构化性质可能很难让语言模型完全理解和利用。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Recently, &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;>;large language models&lt;/a>; (LLMs) have achieved outstanding performance across diverse &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_understanding&quot;>;natural language understanding&lt;/a>; (NLU) tasks by generating reliable reasoning chains, as shown in works like &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2205.10625&quot;>;Least-to-Most&lt;/a>;. However, the most suitable way for LLMs to reason over tabular data remains an open question. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2401.04398&quot;>;Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding&lt;/a>;”, we propose a framework to tackle table understanding tasks, where we train LLMs to outline their reasoning step by step, updating a given table iteratively to reflect each part of a thought process, akin to how people solve the table-based problems. This enables the LLM to transform the table into simpler and more manageable segments so that it can understand and analyze each part of the table in depth. This approach has yielded significant improvements and achieved new state-of-the-art results on the &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1909.02164&quot;>;TabFact&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>; benchmarks. The figure below shows the high-level overview of the proposed Chain-of-Table and other methods. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s1478/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1478&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Given a complex table where a cyclist&#39;s nationality and name are in the same cell, (a) generic, multi-step reasoning is unable to provide the correct answer (b) program-aided reasoning generates and executes programs (eg , SQL queries) to deliver the answer, but falls short in accurately addressing the question. In contrast, (c) Chain-of-Table iteratively samples a chain of operations that effectively transform the complex table into a version specifically tailored to the question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chain-of-Table&lt;/h2>; &lt;p>; In Chain-of-Table, we guide LLMs using &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; to iteratively generate operations and to update the table to represent its reasoning chain over tabular data. This enables LLMs to dynamically plan the next operation based on the results of previous ones.表的这种连续演变形成了一个链，该链为给定问题提供了对推理过程的更结构化和清晰的表示，并可以从LLM中进行更准确和可靠的预测。 &lt;/p>; &lt;p>; For example, when asked, “Which actor has the most NAACP image awards?” the Chain-of-Table framework prompts an LLM to generate tabular operations mirroring tabular reasoning processes. It first identifies the relevant columns. Then, it aggregates rows based on shared content. Finally, it reorders the aggregated results to yield a final table that clearly answers the posed question. &lt;/p>; &lt;p>; These operations transform the table to align with the question presented. To balance performance with computational expense on large tables, we construct the operation chain according to a subset of tabular rows.. Meanwhile, the step-by-step operations reveal the underlying reasoning process through the display of intermediate results from the tabular operations, fostering enhanced interpretability and understanding. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;983&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Illustration of the tabular reasoning process in Chain-of-Table. This iterative process involves dynamically planning an operation chain and accurately storing intermediate results in the transformed tables. These intermediate tables serve as a tabular thought process that can guide the LLM to land to the correct answer more reliably.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Chain-of-Table consists of three main stages. In the first stage, it instructs the LLM to dynamically plan the next operation by in-context learning. Specifically, the prompt involves three components as shown in the following figure: &lt;/p>; &lt;ol>; &lt;li>; The question &lt;em>;Q&lt;/em>;: “Which country had the most cyclists finish in the top 3?” &lt;/li>;&lt;li>; The operation history &lt;em>;chain&lt;/em>;: &lt;code>;f_add_col(Country)&lt;/code>; and &lt;code>;f_select_row(1, 2, 3)&lt;/code>;. &lt;/li>;&lt;li>; The latest intermediate table &lt;em>;T&lt;/em>;: the transformed intermediate table. &lt;/li>; &lt;/ol>; &lt;p>; 通过在提示中提供三元组&lt;em>;(T, Q, chain)&lt;/em>;，LLM可以观察之前的表格推理过程，并从操作中选择下一个操作pool to complete the reasoning chain step by step. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s1958/image1.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1233&quot; data-original-width=&quot;1958&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Illustration of how Chain-of-Table selects the next operation from the operation pool and generates the arguments for the operation.(a) Chain-of-Table samples the next operation from the operation pool. (b) It takes the selected operation as input and generates its arguments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After the next operation &lt;em>;f&lt;/em>; is determined, in the second stage, we need to generate the arguments. As above, Chain-of-Table considers three components in the prompt as shown in the figure: (1) the question, (2) the selected operation and its required arguments, and (3) the latest intermediate table. &lt;/p>; &lt;p>; For instance, when the operation &lt;code>;f_group_by&lt;/code>; is selected, it requires a header name as its argument. &lt;/p>; &lt;p>; The LLM selects a suitable header within the table.配备了所选操作和生成的参数，可执行该操作，并为以下推理构建新的中间表。 &lt;/p>; &lt;p>; Chain-of-Table iterates the previous two stages to plan the next operation and generate the required arguments. During this process, we create an operation chain acting as a proxy for the tabular reasoning steps. These operations generate intermediate tables presenting the results of each step to the LLM. Consequently, the output table contains comprehensive information about the intermediate phases of tabular reasoning. In our final stage, we employ this output table in formulating the final query and prompt the LLM along with the question for the final answer. &lt;/p>; &lt;br />; &lt;h2>;实验设置&lt;/h2>; &lt;p>;我们使用&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2-S&lt;/a>;&amp;nbsp ;and&amp;nbsp;&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;&amp;nbsp;as the backbone LLMs and conduct the experiments on three public table understanding benchmarks: &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1909.02164 “>; tabfact &lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>; fetaqa &lt;/a>;。 WikiTQ 和 FeTaQA 是基于表格的问答的数据集。 TabFact is a table-based fact verification benchmark. In this blogpost, we will focus on the results on WikiTQ and TabFact. We compare Chain-of-Table with the generic reasoning methods (eg, End-to-End QA, Few-Shot QA, and &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>;) and the program-aided methods (eg, &lt;a href=&quot;https://arxiv.org/abs/2204.00498&quot;>;Text-to-SQL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot;>;Binder&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Dater&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;More accurate answers&lt;/h3>; &lt;p>; Compared to the generic reasoning methods and program-aided reasoning methods, Chain-of-Table achieves better performance across &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;.这归因于动态采样操作和信息丰富的中间表。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s1018/ChainOfTableUnderstanding.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;755&quot; data-original-width=&quot;1018&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s16000/ChainOfTableUnderstanding.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Understanding results on WikiTQ and TabFact with PaLM 2 and GPT 3.5 compared with various models.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Better robustness on harder questions&lt;/h3>; &lt;p>; In Chain-of-Table, longer operation chains indicate the higher difficulty and complexity of the questions and their corresponding tables. We categorize the test samples according to their operation lengths in Chain-of-Table. We compare Chain-of-Table with Chain-of-Thought and Dater, as representative generic and program-aided reasoning methods. We illustrate this using results from &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; on &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;. &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIaEsHJohvlrSna7/s1548/CoTOpChainLength.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;624&quot; data-original-width=&quot;1548&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIaEsHJohvlrSna7/s16000/CoTOpChainLength.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of Chain-of-Thought, Dater, and the proposed Chain-of-Table on WikiTQ for questions that require an operation chain of varying lengths. Our proposed atomic operations significantly improve performance over generic and program-aided reasoning counterparts.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Notably, Chain-of-Table consistently surpasses both baseline methods across all operation chain lengths, with a significant margin up to 11.6% compared with &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>;, and up to 7.9% compared with &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Dater&lt;/a>;. Moreover, the performance of Chain-of-Table declines gracefully with increasing number of operations compared to other baseline methods, exhibiting only a minimal decrease when the number of operations increases from four to five. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Better robustness with larger tables&lt;/h3>; &lt;p>; We categorize the tables from &lt;a href= &quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>; into three groups based on token number: small (&amp;lt;2000 tokens), medium (2000 to 4000 tokens) and large (&amp;gt;4000 tokens) 。 We then compare Chain-of-Table with &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Dater&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot; >; Binder &lt;/a>;，两个最新，最强的基线。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1008&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Performance of Binder, Dater, and the proposed Chain-of-Table on small (&amp;lt;2000 tokens), medium (2000 to 4000 tokens), and large (&amp;gt;4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.)&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Performance of Binder, Dater, and the proposed Chain-of-Table on small (&amp;lt;2000 tokens), medium (2000 to 4000 tokens), and large (&amp;gt;4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.) &lt;/p>; &lt;p>; As anticipated, the performance decreases with larger input tables, as models are required to reason through longer contexts. Nevertheless, the performance of the proposed Chain-of-Table diminishes gracefully, achieving a significant 10+% improvement over the second best competing method when dealing with large tables. This demonstrates the efficacy of the reasoning chain in handling long tabular inputs. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our proposed Chain-of-Table method enhances the reasoning capability of LLMs by leveraging the tabular structure to express intermediate steps for table-based reasoning.它指示法学硕士根据输入表及其相关问题动态规划操作链。这种不断发展的表格设计为促进法学硕士对表格的理解提供了新的思路。 &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister. Thanks to Chih-Kuan Yeh and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/470840348983280912/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables- in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /470840348983280912&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/470840348983280912&quot; rel=&quot;self&quot; type =&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html&quot; rel=&quot;alternate&quot; title= &quot;Chain-of-table: Evolving tables in the reasoning chain for table understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNcD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s72-c/Chain-of-Table.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376&lt;/id>;&lt;published >;2024-03-08T11:33:00.000-08:00&lt;/published>;&lt;updated>;2024-03-13T09:18:01.747-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Image Classification&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Health-specific embedding tools for dermatology and pathology&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Steiner, Clinical Research Scientist, Google Health, and Rory Pilgrim, Product Manager, Google Research&lt;/span>; &lt;img src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s16000/Path%20+%20Derm%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; There&#39;s a worldwide shortage of access to medical imaging expert interpretation across specialties including &lt;a href=&quot;https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage&quot;>;radiology&lt;/a>;, &lt;a href=&quot;https://www.aad.org/dw/monthly/2021/december/feature-running-dry&quot;>;dermatology&lt;/a>; and &lt;a href=&quot;https://proscia.com/infographic-the-state-of-the-pathology-workforce-2022/&quot;>;pathology&lt;/a>;. Machine learning (ML) technology can help ease this burden by powering tools that enable doctors to interpret these images more accurately and efficiently. However, the development and implementation of such ML tools are often limited by the availability of high-quality data, ML expertise, and computational resources. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One way to catalyze the use of ML for medical imaging is via domain-specific models that utilize deep learning (DL) to capture the information in medical images as compressed numerical vectors (called embeddings). These embeddings represent a type of pre-learned understanding of the important features in an image. Identifying patterns in the embeddings reduces the amount of data, expertise, and compute needed to train performant models as compared to &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;>;working with high-dimensional data&lt;/a>;, such as images, directly. Indeed, these embeddings can be used to perform a variety of downstream tasks within the specialized domain (see animated graphic below). This framework of leveraging pre-learned understanding to solve related tasks is similar to that of a seasoned guitar player quickly learning a new song by ear. Because the guitar player has already built up a foundation of skill and understanding, they can quickly pick up the patterns and groove of a new song. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s1600/Path%20+% 20Derm%20train%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s16000/Path%20+%20Derm%20train%20LP.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation is used to convert a small dataset of (image, label) pairs into (embedding, label) pairs 。 These pairs can then be used to train a task-specific classifier using a linear probe, (ie, a lightweight linear classifier) as represented in this graphic, or other types of models using the embeddings as input.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s1600/Path%20+%20Derm%20-%20evaluate%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s16000/Path%20+%20Derm%20-%20evaluate%20LP.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Once the linear probe is trained, it can be used to make predictions on embeddings from new images. These predictions can be compared to ground truth information in order to evaluate the linear probe&#39;s performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In order to make this type of embedding model available and drive further development of ML tools in medical imaging, we are excited to release two domain-specific tools for research use: &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; and &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/path-foundation&quot;>;Path Foundation&lt;/a>;. This follows on the strong response we&#39;ve already received from researchers using the &lt;a href=&quot;https://blog.research.google/2022/07/simplified-transfer-learning-for-chest.html&quot;>;CXR Foundation&lt;/a>; embedding tool for chest radiographs and represents a portion of our expanding research offerings across multiple medical-specialized modalities. These embedding tools take an image as input and produce a numerical vector (the embedding) that is specialized to the domains of dermatology and digital pathology images, respectively. By running a dataset of chest X-ray, dermatology, or pathology images through the respective embedding tool, researchers can obtain embeddings for their own images, and use these embeddings to quickly develop new models for their applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Path Foundation&lt;/h2>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2310.13259&quot;>;Domain-specific optimization and diverse evaluation of self-supervised models for histopathology&lt;/a>;”, we showed that self-supervised learning (SSL) models for pathology images outperform traditional pre-training approaches and enable efficient training of classifiers for downstream tasks. This effort focused on &lt;a href=&quot;https://en.wikipedia.org/wiki/H%26E_stain&quot;>;hematoxylin and eosin&lt;/a>; (H&amp;amp;E) stained slides, the principal tissue stain in diagnostic pathology that enables pathologists to visualize cellular features under a microscope. The performance of linear classifiers trained using the output of the SSL models matched that of prior DL models trained on orders of magnitude more labeled data. &lt;/p>; &lt;p>; Due to substantial differences between digital pathology images and “natural image” photos, this work involved several pathology-specific optimizations during model training. One key element is that &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/&quot;>;whole-slide images&lt;/a>; (WSIs) in pathology can be 100,000 pixels across (thousands of times larger than typical smartphone photos) and are analyzed by experts at multiple magnifications (zoom levels). As such, the WSIs are typically broken down into smaller tiles or patches for computer vision and DL applications. The resulting images are information dense with cells or tissue structures distributed throughout the frame instead of having distinct semantic objects or foreground vs. background variations, thus creating unique challenges for robust SSL and feature extraction. Additionally, physical (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Microtome&quot;>;cutting&lt;/a>;) and chemical (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fixation_(histology)&quot;>;fixing&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Staining&quot;>;staining&lt;/a>;) processes used to prepare the samples can influence image appearance dramatically. &lt;/p>; &lt;p>; Taking these important aspects into consideration, pathology-specific SSL optimizations included helping the model learn &lt;a href=&quot;https://arxiv.org/abs/2206.12694&quot;>;stain-agnostic features&lt;/a>;, generalizing the model to patches from multiple magnifications, &lt;a href=&quot;https://blog.research.google/2020/02/generating-diverse-synthetic-medical.html&quot;>;augmenting&lt;/a>; the data to mimic scanning and image post processing, and custom data balancing to improve input heterogeneity for SSL training. These approaches were extensively evaluated using a broad set of benchmark tasks involving 17 different tissue types over 12 different tasks. &lt;/p>; &lt;p>; Utilizing the vision transformer (&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;>;ViT-S/16&lt;/a>;) architecture, Path Foundation was selected as the best performing model from the optimization and evaluation process described above (and illustrated in the figure below). This model thus provides an important balance between performance and model size to enable valuable and scalable use in generating embeddings over the many individual image patches of large pathology WSIs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk 8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s1999/路径%20+%20Derm% 20SSL.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1097&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s16000/Path%20+%20Derm%20SSL.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SSL training with pathology-specific optimizations for Path Foundation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The value of domain-specific image representations can also be seen in the figure below, which shows the linear probing performance improvement of Path Foundation (as measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;>; AUROC&lt;/a>;) compared to traditional pre-training on natural images (&lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;ImageNet-21k&lt;/a>;). This includes evaluation for tasks such as &lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2665774&quot;>;metastatic breast cancer detection in lymph nodes&lt;/a>;, &lt;a href=&quot;https:// jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;prostate cancer grading&lt;/a>;, and &lt;a href=&quot;https://www.nature.com/articles/s41523-022-00478-y&quot;>;breast cancer评分&lt;/a>;等。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s1999/Path %20+%20Derm%20embeddings.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;890&quot; data-original-width=&quot; 1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg- FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s16000/Path%20+%20Derm%20embeddings.png&quot;/>;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation embeddings significantly outperform traditional ImageNet embeddings as evaluated by linear probing across multiple evaluation tasks in histopathology .&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Derm Foundation&lt;/ h2>; &lt;p>; &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; is an embedding tool derived from our research in将dl应用于&lt;a href=&quot;https://blog.research.google/2019/09/usise-deep-learning-to-inform.html&quot;>;解释皮肤病学条件的图像&lt;/a>;，并包括我们最近的工作，并包括我们最近的工作adds &lt;a href=&quot;https://arxiv.org/abs/2402.15566&quot;>;improvements to generalize better to new datasets&lt;/a>;.由于其皮肤科特定的预训练，因此对皮肤状况图像中存在的特征具有潜在的理解，可用于快速开发模型以对皮肤状况进行分类。 The model underlying the API is a &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>;BiT ResNet-101x3&lt;/a>; trained in two stages. The first pre-training stage uses contrastive learning, similar to &lt;a href=&quot;https://arxiv.org/abs/2010.00747&quot;>;ConVIRT&lt;/a>;, to train on a large number of image-text pairs &lt;a href=&quot;https://blog.research.google/2017/07/revisiting-unreasonable-effectiveness.html&quot;>;from the internet&lt;/a>;. In the second stage, the image component of this pre-trained model is then fine-tuned for condition classification using clinical datasets, such as those from teledermatology services. &lt;/p>; &lt;p>; Unlike histopathology images, dermatology images more closely resemble the real-world images used to train many of today&#39;s computer vision models. However, for specialized dermatology tasks, creating a high-quality model may still require a large dataset. With Derm Foundation, researchers can use their own smaller dataset to retrieve domain-specific embeddings, and use those to build smaller models (eg, linear classifiers or other small non-linear models) that enable them to validate their research or product ideas. To evaluate this approach, we trained models on a downstream task using teledermatology data. Model training involved varying dataset sizes (12.5%, 25%, 50%, 100%) to compare embedding-based linear classifiers against fine-tuning. &lt;/p>; &lt;p>; The modeling variants considered were: &lt;/p>; &lt;ul>; &lt;li>;A linear classifier on frozen embeddings from &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>;BiT-M&lt;/a>; (a standard pre-trained image model) &lt;/li>;&lt;li>;Fine-tuned version of BiT-M with an extra dense layer for the downstream task &lt;/li>;&lt;li>;A linear classifier on frozen embeddings from the Derm Foundation API &lt;/li>;&lt;li>;Fine-tuned version of the model underlying the Derm Foundation API with an extra layer for the downstream task &lt;/li>; &lt;/ul>; &lt;p>; We found that models built on top of the Derm Foundation embeddings for dermatology-related tasks achieved significantly higher quality than those built solely on embeddings or fine tuned from BiT-M.发现该优势对于较小的培训数据集尺寸最为明显。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s1240/Path%20+%20Derm%20task% 20accuracy.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;842&quot; data-original-width=&quot;1240&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s16000/Path%20+%20Derm%20task%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; TD类=“ Tr-Caption”样式=“ Text-Align：Center;”>;这些结果表明，Derm Foundation Tooi可以作为加速与皮肤相关的建模任务的有用起点。 We aim to enable other researchers to build on the underlying features and representations of dermatology that the model has learned. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; However, there are limitations with this analysis. We&#39;re still exploring how well these embeddings generalize across task types, patient populations, and image settings. Downstream models built using Derm Foundation still require careful evaluation to understand their expected performance in the intended setting. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Access Path and Derm Foundation&lt;/h2>; &lt;p>; We envision that the Derm Foundation and Path Foundation embedding tools will enable a range of use cases, including efficient development of models for diagnostic tasks, quality assurance and pre-analytical workflow improvements, image indexing and curation, and biomarker discovery and validation. We are releasing both tools to the research community so they can explore the utility of the embeddings for their own dermatology and pathology data. &lt;/p>; &lt;p>; To get access, please sign up to each tool&#39;s terms of service using the following Google Forms. &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSe5icNBzU_lO2CwjLLIOwbqIcWnJC-m4Sl7MgvI9Lng3QT6Zg/viewform?resourcekey=0-dahJtiVe2CqYkNEdWPcXgw&quot;>;Derm Foundation Access Form&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/1auyo2VkzlzuiAXavZy1AWUyQHAqO7T3BLK-7ofKUvug/edit?resourcekey=0-Z9pRxjDI-kaDEUIiNfMAWQ#question=1168037695&amp;field=173852432&quot;>;Path Foundation Access Form&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>; After gaining access to each tool, you can use the API to retrieve embeddings from dermatology images or digital pathology images stored in Google Cloud. Approved users who are just curious to see the model and embeddings in action can use the provided example Colab notebooks to train models using public data for classifying &lt;a href=&quot;https://github.com/Google-Health/imaging-research/blob/master/derm-foundation/derm_foundation_demo.ipynb&quot;>;six common skin conditions&lt;/a>; or identifying tumors in &lt;a href=&quot;https://github.com/Google-Health/imaging-research/blob/master/path-foundation/linear-classifier-demo.ipynb&quot;>;histopathology patches&lt;/a>;. We look forward to seeing the range of use-cases these tools can unlock. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the many collaborators who helped make this work possible including Yun Liu, Can Kirmizi, Fereshteh Mahvar, Bram Sterling, Arman Tajback, Kenneth Philbrik, Arnav Agharwal, Aurora Cheung, Andrew Sellergren, Boris Babenko, Basil Mustafa, Jan Freyberg, Terry Spitz, Yuan Liu, Pinal Bavishi,阿尤什·杰恩、阿米特·塔尔雷贾、拉吉夫·里克耶、艾比·沃德、杰里米·赖、法鲁克·艾哈迈德、苏普里亚·维贾伊、蒂亚姆·贾罗恩斯里、杰西卡·卢、Saurabh Vyawahare、萨洛尼·阿加瓦尔、埃勒里·乌尔钦、乔纳森·克劳斯、法亚兹·贾米尔、汤姆·斯莫尔、安妮莎·乌姆拉尼、 Lauren Winer、Sami Lachgar、Yossi Matias、Greg Corrado 和 Dale Webster。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1106624361649572376/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools- for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /1106624361649572376&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1106624361649572376&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools-for.html&quot; rel=&quot;alternate&quot; title=&quot;健康-皮肤病学和病理学专用嵌入工具&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9 Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s72-c/Path%20+%20Derm%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/ mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-1765359719068432739&lt;/id >;&lt;发布>;2024-03-07T10:15:00.000-08:00&lt;/发布>;&lt;更新>;2024-03-07T10:19:31.177-08:00&lt;/更新>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器智能&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;社交学习：协作学习大语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究实习生 Amirkeivan Mohtashami 和软件工程师 Florian Hartmann&lt;/span>; &lt;img src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp 8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; Large language models (LLMs) have significantly improved the state of the art for solving tasks specified using natural language, often reaching performance close to that of people.随着这些模型越来越多地支持辅助代理，他们有效地相互学习可能是有益的，就像人们在社交环境中所做的那样，这将使基于 LLM 的代理能够提高彼此的表现。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 讨论人类、班杜拉和沃尔特斯的学习过程&lt;a href=&quot;https://books.google.ch/books/about/Social_Learning_Theory .html?id=IXvuAAAAMAAJ&amp;amp;redir_esc=y&quot;>;在 1977 年描述了&lt;/a>;&lt;em>;社会学习&lt;/em>;的概念，概述了人们使用的观察学习的不同模型。向他人学习的一种常见方法是通过描述如何参与特定行为的&lt;em>;口头指导&lt;/em>;（例如，来自老师）。或者，学习可以通过模仿行为的活生生例子的&lt;em>;实时模型&lt;/em>;来进行。 &lt;/p>; &lt;p>; 鉴于法学硕士模仿人类交流的成功，在我们的论文“&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;社交学习：利用大型语言模型进行协作学习&lt;/”中a>;”，我们调查法学硕士是否能够利用社交学习相互学习。为此，我们概述了一个社会学习框架，其中法学硕士使用自然语言以隐私意识方式相互分享知识。我们评估了我们的框架在各种数据集上的有效性，并提出了在这种情况下衡量隐私的定量方法。与之前的协作学习方法相比，例如常见的&lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;联合学习&lt;/a>;方法通常依赖于关于梯度，在我们的框架中，智能体纯粹使用自然语言互相教学。 &lt;/p>; &lt;br />; &lt;h2>;法学硕士的社交学习&lt;/h2>; &lt;p>;为了将社交学习扩展到语言模型，我们考虑这样的场景：法学硕士学生应该学习解决来自多个教师实体的任务，这些教师实体已经知道那个任务。在我们的论文中，我们评估了学生在各种任务上的表现，例如短短信中的&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2034691.2034742&quot;>;垃圾邮件检测&lt;/a>;（ SMS），解决&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;小学数学问题&lt;/a>;，以及&lt;a href=&quot;https://arxiv.org/abs/1905.10044&quot;>;根据给定的文本回答问题&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1 jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s900/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;381&quot; data-original-width=&quot;900&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5t DBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visualization of the social learning process: A teacher model provides instructions or few-shot examples to a student model without sharing its private data.&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 语言模型在仅给出少数示例的情况下就显示出执行任务的非凡能力 - 这个过程称为 &lt;a href=&quot;https://arxiv.org/abs/2005.14165 ”少样本学习&lt;/a>;。考虑到这一点，我们提供了人工标记的任务示例，使教师模型能够将其教授给学生。当由于隐私问题等原因而无法直接与学生共享这些示例时，就会出现社交学习的主要用例之一。 &lt;/p>; &lt;p>; 为了说明这一点，让我们看一个垃圾邮件检测任务的假设示例。教师模型位于设备上，一些用户自愿将他们收到的传入消息标记为“垃圾邮件”或“非垃圾邮件”。这是有用的数据，可以帮助训练学生模型区分垃圾邮件和非垃圾邮件，但与其他用户共享个人消息是侵犯隐私的行为，应该避免。为了防止这种情况，社交学习过程可以将知识从教师模型转移到学生，这样学生就可以了解垃圾邮件的样子，而无需共享用户的个人短信。 &lt;/p>; &lt;p>; 我们通过与我们上面讨论的已建立的人类社会学习理论进行类比来研究这种社会学习方法的有效性。在这些实验中，我们使用 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-S&lt;/a>; 模型老师和学生。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y 3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1117&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8yt HsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;社会学习的系统观点：在培训时，多名教师教授学生。在推理时，学生正在使用从老师那里学到的知识。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;综合示例&lt;/h3>; &lt;p>;作为对应物针对传统社会学习描述的实时教学模型，我们提出了一种学习方法，教师为任务生成新的综合示例并与学生分享。这样做的动机是，人们可以创建一个与原始示例完全不同但同样具有教育意义的新示例。事实上，我们观察到我们生成的示例与真实示例有很大不同，可以保护隐私，同时仍然能够实现与使用原始示例实现的性能相当的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkP eKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s1456/image5.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPF hsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;对于多项任务，生成的 8 个示例的性能与原始数据一样好（请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们通过任务套件上的综合示例来评估学习的效果。特别是当示例数量足够多时，例如，n = 16，我们观察到对于大多数任务来说，共享原始数据和通过社交学习使用合成数据进行教学之间没有统计上的显着差异，这表明隐私改进不一定会出现以模型质量为代价。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHR8sU3WkHsPKVlsQm VnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s1456/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHR8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEiror XE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;生成 16 个示例（而不是仅 8 个示例）进一步缩小了相对于原始示例的性能差距。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;p>; 一个例外是垃圾邮件检测，使用合成数据进行教学的准确性较低。这可能是因为当前模型的训练过程使它们偏向于仅生成非垃圾邮件示例。在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;中，我们还研究了用于选择要使用的良好示例子集的聚合方法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;综合指令&lt;/h3>; &lt;p>;鉴于语言模型在以下指令中的成功，言语通过让教师为任务生成指令，教学模型也可以自然地适应语言模型。我们的实验表明，提供这样的生成指令可以有效地提高零样本提示的性能，达到与原始示例的少样本提示相当的准确性。然而，我们确实发现教师模型可能无法在某些任务上提供良好的指导，例如由于输出的复杂格式要求。 &lt;/p>; &lt;p>;对于&lt;a href=&quot;https://arxiv.org/abs/1606.06031&quot;>;兰巴达&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>; GSM8k&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;随机插入&lt;/a>;，提供合成示例比提供生成的指令表现更好，而在其他任务中生成的指令获得更高的准确度。这一观察结果表明，教学模式的选择取决于手头的任务，就像最有效的教学方法因任务而异一样。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJe jJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s1451/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1451&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEH y8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据任务的不同，生成指令比生成新示例效果更好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;记忆私人例子&lt;/h2>; &lt;p>;我们希望社会学习中的教师在不透露原始数据细节的情况下教授学生。 To quantify how prone this process is to leaking information, we used &lt;a href=&quot;https://research.google/pubs/the-secret-sharer-evaluating-and-testing-unintended-memorization-in-neural-networks/ &quot;>;Secret Sharer&lt;/a>;，一种流行的方法，用于量化模型记忆其训练数据的程度，并将其适应社交学习环境。我们选择此方法是因为它之前&lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for.html&quot;>;用于&lt;/a>;评估联邦学习中的记忆情况。 &lt;/p>; &lt;p>; 为了将秘密分享者方法应用于社交学习，我们设计了“金丝雀”数据点，以便我们可以具体测量训练过程记住了多少它们。这些数据点包含在教师用来生成新示例的数据集中。社会学习过程完成后，我们可以衡量学生对老师使用的秘密数据点的信心，与那些甚至没有与老师分享的类似数据点相比。 &lt;/p>; &lt;p>; 在我们的分析中（在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;中详细讨论），我们使用包含名称和代码的金丝雀示例。我们的结果表明，学生对老师使用的金丝雀的信心仅稍高一些。相反，当直接与学生共享原始数据点时，对包含的金丝雀的置信度远高于对保留集的置信度。这支持了这样的结论：教师确实使用其数据进行教学，而不是简单地复制它。 &lt;/p>; &lt;br />; &lt;h2>;结论和后续步骤&lt;/h2>; &lt;p>;我们引入了一个社会学习框架，该框架允许能够访问私有数据的语言模型通过文本通信传输知识，同时维护该数据的隐私。数据。在此框架中，我们将共享示例和共享指令确定为基本模型，并在多个任务上对其进行评估。此外，我们将秘密共享者指标调整到我们的框架中，提出了衡量数据泄漏的指标。 &lt;/p>; &lt;p>; 接下来，我们正在寻找改进教学过程的方法，例如通过添加反馈循环和迭代。此外，我们希望研究将社交学习用于文本以外的方式。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们谨向共同作者 Matt Sharifi、Sian Gooding、Lukas Zilka 和 Blaise Aguera y Arcas 表示感谢。在纸上。此外，我们还要感谢 Victor Cărbune、Zachary Garrett、Tautvydas Misiunas、Sofia Neata 和 John Platt 的反馈，这些反馈极大地改进了本文。我们还要感谢 Tom Small 创作了这个动画人物。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1765359719068432739/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html #comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;self&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html&quot; rel=&quot;alternate&quot; title=&quot;社交学习：协作学习large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@ blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/ b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEicN2GYOP9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyy pTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8393293208018757284&lt;/id>;&lt;发布>;2024-03-06T10：26：00.000-08： 00&lt;/published>;&lt;updated>;2024-03-06T14:44:03.387-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot; >;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/” atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Croissant：用于 ML 就绪数据集的元数据格式&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;发布者：Google 研究部软件工程师 Omar Benjelloun 和 Google Core ML 软件工程师兼 MLCommons 协会主席 Peter Mattson&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ IMG/B/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyA AN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s1600/CroissantHero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 机器学习 (ML) 从业者希望重用现有数据集来训练 ML 模型，通常会花费大量时间来理解数据、理解其组织或弄清楚要使用哪个子集作为特征。事实上，机器学习领域的进展长期以来一直受到一个根本障碍的阻碍：数据表示的多样性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML 数据集涵盖广泛的内容类型，从文本和结构化数据到图像、音频和视频。即使在涵盖相同类型内容的数据集中，每个数据集也具有独特的文件和数据格式&lt;em>;临时&lt;/em>;排列。这一挑战降低了整个机器学习开发过程（从查找数据到训练模型）的生产力。它还阻碍了急需的数据集工具的开发。 &lt;/p>; &lt;p>; 数据集有通用元数据格式，例如 &lt;a href=&quot;http://schema.org/Dataset&quot;>;schema.org&lt;/a>; 和 &lt;a href=&quot;https:// www.w3.org/TR/vocab-dcat-3/&quot;>;DCAT&lt;/a>;。然而，这些格式是为数据发现而设计的，而不是为了满足机器学习数据的特定需求，例如从结构化和非结构化源中提取和组合数据的能力，以包含能够实现的元数据.google/responsibility/responsible-ai-practices/&quot;>;负责任地使用数据&lt;/a>;，或描述 ML 使用特征，例如定义训练、测试和验证集。 &lt;/p>; &lt;p>; 今天，我们推出 &lt;a href=&quot;https://mlcommons.org/croissant&quot;>;Croissant&lt;/a>;，这是一种适用于 ML 就绪数据集的新元数据格式。 Croissant was developed collaboratively by a community from industry and academia, as part of the &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; effort. The Croissant format doesn&#39;t change how the actual data is represented (eg, image or text file formats) — it provides a standard way to describe and organize it. Croissant builds upon &lt;a href=&quot;https://schema.org/&quot;>;schema.org&lt;/a>;, the de facto standard for publishing structured data on the Web, which is already used by over 40M datasets. Croissant augments it with comprehensive layers for ML relevant metadata, data resources, data organization, and default ML semantics. &lt;/p>; &lt;p>; In addition, we are announcing support from major tools and repositories: Today, three widely used collections of ML datasets — &lt;a href=&quot;http://www.kaggle.com/datasets&quot;>;Kaggle&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets?other=croissant&amp;amp;sort=trending&quot;>;Hugging Face&lt;/a>;, and &lt;a href=&quot;https://openml.org/search?type=data&quot;>;OpenML&lt;/a>; — will begin supporting the Croissant format for the datasets they host; the &lt;a href=&quot;http://g.co/datasetsearch&quot;>;Dataset Search&lt;/a>; tool lets users search for Croissant datasets across the Web; and popular ML frameworks, including &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, and &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;, can load Croissant datasets easily using the &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; TensorFlow数据集&lt;/a>;（TFDS）软件包。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Croissant&lt;/h2>; &lt;p>; This 1.0 release of Croissant includes a complete &lt;a href=&quot;https://mlcommons.org/croissant/1.0&quot;>;specification&lt;/a>; of the format, a set of &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/datasets&quot;>;example datasets&lt;/a>;, an open source &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/python/mlcroissant&quot;>;Python library&lt;/a>; to validate, consume and generate Croissant metadata, and an open source &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>;visual editor&lt;/a>; to load, inspect and create Croissant dataset descriptions in an intuitive way. &lt;/p>; &lt;p>; Supporting Responsible AI (RAI) was a key goal of the Croissant effort from the start. We are also releasing the first version of the &lt;a href=&quot;https://mlcommons.org/croissant/RAI/1.0&quot;>;Croissant RAI vocabulary&lt;/a>; extension, which augments Croissant with key properties needed to describe important RAI use cases such as data life cycle management, data labeling, participatory data, ML safety and fairness evaluation, explainability, and compliance. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Why a shared format for ML data?&lt;/h2>; &lt;p>; The majority of ML work is actually数据工作。 The training data is the “code” that determines the behavior of a model. Datasets can vary from a collection of text used to train a large language model (LLM) to a collection of driving scenarios (annotated videos) used to train a car&#39;s collision avoidance system.但是，开发ML模型的步骤通常遵循相同的以数据为中心的过程：（1）查找或收集数据，（2）清洁和完善数据，（3）在数据上训练模型，（4）测试有关更多数据的模型，（5）发现该模型不起作用，（6）分析数据以找出原因，（7）重复直到实现可行的模型。 Many steps are made harder by the lack of a common format. This “data development burden” is especially heavy for resource-limited research and early-stage entrepreneurial efforts. &lt;/p>; &lt;p>; The goal of a format like Croissant is to make this entire process easier.例如，可以通过搜索引擎和数据集存储库来利用元数据，以使找到正确的数据集变得更加容易。数据资源和组织信息使开发清洁，完善和分析数据的工具变得更加容易。 This information and the default ML semantics make it possible for ML frameworks to use the data to train and test models with a minimum of code. Together, these improvements substantially reduce the data development burden. &lt;/p>; &lt;p>; Additionally, dataset authors care about the discoverability and ease of use of their datasets. Adopting Croissant improves the value of their datasets, while only requiring a minimal effort, thanks to the available creation tools and support from ML data platforms. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What can Croissant do today?&lt;/h2>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left ：auto; auto-right：auto;“>; &lt;img border =“ 0” data-Forginal-height =“ 540” data-eriginal-width =“ 908” src =“ /b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style =“ Text-Align：Center;”>;羊角面包生态系统：用户可以搜索羊角面包数据集，从主要存储库中下载它们，然后轻松地将其加载到他们喜欢的ML Frameworks中。他们可以使用羊角面包编辑器。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;今天，用户可以在：&lt;/p>; &lt;ul>; &lt;&lt;/p>; &lt;/pdbody>; &lt;p>;上创建，检查和修改羊角面包元数据。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>; li>; Google &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;>;数据集搜索&lt;/a>;，提供了一个羊角面过滤器。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://huggingface.co/datasets？other = croissant＆amp; sort = trending&quot;>; huggingface &lt;/a>; //kaggle.com/datasets&quot;>; kaggle &lt;/a>; &lt;/li>; &lt;li>; &lt;a href=&quot;https://openml.org/search?type=data&quot;>; opentml &lt;/a>; &lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>;使用羊角面包数据集，可以通过&lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; tensorflow轻松获取数据。数据集&lt;/a>;用于在流行的ML框架中使用，例如&lt;a href=&quot;https://www.tensorflow.org/&quot;>; tensorflow &lt;/a>;，&lt;a href=&quot;https://pytorch.org/&quot;>; pytorch &lt;/a>;和&lt;a href=&quot;https://github.com/google/jax&quot;>; jax &lt;/a>;。 &lt;/li>; &lt;li>;使用&lt;a href=&quot;https://huggingface.co/spaces/mlcommons/croissant-editor&quot;>; Croissant Editor ui &lt;/a>;（&lt;a href =“ https） ：//github.com/mlcommons/croissant/tree/main/editor“>; github &lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;p>;要发布羊角面包数据集，用户可以：&lt;/p>; &lt;ul>; &lt;li>;使用&lt;a href =“ https://huggingface.co/spaces/mlcommons/croissant -Editor“>;牛角编辑UI &lt;/a>;（&lt;a href=&quot;https://github.com/mlcommons/croissant/croissant/croissant/tree/main/editor&quot;>; github &lt;/a>;）生成大部分杂色元数据通过分析用户提供的数据并填充重要的元数据字段（例如RAI属性）来自动自动。 &lt;/li>; &lt;li>;作为其数据集网页的一部分发布羊角面信息，以使其可发现和重复使用。 &lt;/li>; &lt;li>;在支持羊角面包的一个存储库中发布其数据，例如Kaggle，Huggingface和OpenML，并自动生成羊角面包元数据。 &lt;/li>; &lt;/ul>; &lt;div style =“线路高：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;未来方向&lt;/h2>; &lt;p>;我们对羊角面包的帮助ML感到兴奋从业者，但是使这种格式真正有用需要社区的支持。我们鼓励数据集创建者考虑提供羊角面包元数据。我们鼓励托管数据集的平台提供羊角面包文件，以便在数据集网页中下载并嵌入羊角面包元数据，以便可以通过数据集搜索引擎发现它们。帮助用户使用ML数据集的工具，例如标签或数据分析工具，也应考虑支持羊角面包数据集。我们可以一起减轻数据开发负担，并实现ML研发的更丰富的生态系统。 &lt;/p>; &lt;p>;我们鼓励社区参加&lt;a href=&quot;http://mlcommons.org/croissant&quot;>;加入我们&lt;/a>;为这项工作做出了贡献。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>; benspectledgments &lt;/h2>; &lt;p>; &lt;p>; &lt;em>; Croissant由&lt;a a href =“ https开发：//datasetsearch.research.google.com/“>; dataset search &lt;/a>;，&lt;a href=&quot;https://www.kaggle.com/&quot;>; kaggle &lt;/a>;和&lt;a href =“ https：https：https： //www.tensorflow.org/datasets&quot;>; tensorflow数据集&lt;/a>; Google的团队，作为&lt;a href=&quot;http://mlcommons.org&quot;>; mlcommons &lt;/a>;社区工作组的一部分。包括来自这些组织的贡献者：拜耳，Ctuning Foundation，Dans-knaw，Dotphoton，Harvard，Husgaft，Hugging Face，Kings College London，List，Meta，NASA，NASA，NASA，NASA，NASA，北卡罗来纳州立大学，开放数据研究所，加泰罗尼亚大学开放大学，Sage Bionetworks和Sage Bionetworks和Sage Bionetworks和tu eindhoven。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/8393293208018757284/comments/comments/comments/comments/default/default/default” =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/03/croissant-metadata-format-format-for-ml-ml-ml-ready.html#comment-comment-form” rel =“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/839329329329329320801875757284应用程序/atom+xml“/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626/posts/posts/posts/default/8393293293293208018757284 link href =“ http://blog.research.google/2024/03/croissant-metadata-format-format-for-ml-ml-ready.html” rel =“替代” title =“ croissant：obsoissant：ml-Ready DataSets的元数据格式“ type =” text/html”/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777775266161 &lt;/uri>; &lt;/email>; &lt;gd：Image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” 。 -T1O5PSACF-EKVHIEIWHMR7RGFT7O3A2RK94GWE8WBOO3DULXRQT1XZ9X4I2AMKJXCUTUKBBSIA8XVYAANAN_LJJJJJJJJJYMABXRXRYON_ uau2p4srnp/s72-c/croissanthero.png“ width =“ 72” xmlns：媒体=“ http：//search.yahoo.com/mrss/ THR：THR>; 0 &lt;/thr：Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-POST-2754526782497247497 &lt;/id>; ：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-03-05T08：40：45.490-08：00 &lt;/updated>; &lt;category scheme =&#39; term =“会议”>; &lt;/category>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” term =“ conferences”>; &lt;/caterory>; &lt;category>; &lt;category scheme =“ http：// http：// wwwww .blogger.com/atom/ns＃“ term =“ physics”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” >; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“量子计算”>; &lt;/category>; &lt;/cattory>; &lt;title type =“ text”>; google at aps 2024 &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由凯特·韦伯（Kate Weber）和香农·莱昂（Shannon Leon）发表，Google Research，Quantum AI Team &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img /b/r29vz2xl/avvxsejy2hfq3rn4qrujcsmupiau4ueiicq219mdvfu4fnj9kf5pbmui0x4kf5pbmui us ds5fywxigjex4nxmpoib2je1z2qxdlnzlkfm075wstfjd777777 xvns2t9hckwzylf/s1600/s1600/lockup_googperleresearch_fullcolor_fullcolor_hero.jper.jpg&#39; />; &lt;p>;今天&lt;a href=&quot;https://www.aps.org/meetings/meeting/meeting.cfm?name=mar24&quot;>; 2024 3月会议&lt;/a>; /www.aps.org/&quot;>;美国物理社会&lt;/a>;（APS）在明尼苏达州明尼阿波利斯（Minneapolis）开幕。 APS 2024跨物理和相关领域的主题大会汇集了研究人员，学生和行业专业人员分享他们的发现并建立合作伙伴关系，以实现与物理相关科学和技术的基本进步。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;今年，Google在APS上拥有强大的影响力，由Google &lt;a href =“ https://quantumai.google/”托管的展位。 >; Quantum ai &lt;/a>;团队，在整个会议期间进行了50多次谈判，并参加会议组织活动，特殊会议和活动。亲自参加APS 2024？快来访问Google的量子AI展位，以了解有关我们为解决该领域最有趣的挑战所做的激动人心的工作的更多信息。 &lt;！ - 访问&lt;a href =“ https://twitter.com/googleai”>; @googleai &lt;/a>; x（twitter）帐户以了解有关Google Booth Activity（例如，demos and q＆amp; a sessions） 。-->; &lt;/p>; &lt;p>;您可以了解有关我们在会议上介绍的最新尖端工作以及下面的展位活动的更多信息（&lt;strong>; Bold &lt;/strong>;中列出的Googles）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p >; Session Chairs include: &lt;strong>;Aaron Szasz&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Booth Activities&lt; /h2>; &lt;div style =“边距左：20px;”>; &lt;p>; &lt;em>;此时间表可能会更改。请访问Google Quantum AI展位以获取更多信息。&lt;/em>; &lt;/p>; &lt;p>;崩溃：一种可视化QEC电路的原型交互式工具&lt;br />;主持人：&lt;strong>; Matt McEwen &lt;/strong>; &lt;br/ >;星期二，3月5日| CST上午11:00 &lt; /p>; &lt;p>; Qualtran：一个开源库，用于有效的资源估计容宽算法&lt;br />;主持人：&lt;strong>; tanuj khattar &lt; /strong>; &lt;br />; | 2:30 pm CST &lt; /p>; &lt;p>; Qualtran：一个开源库，用于有效资源估计容宽算法的资源&lt;br />;主持人：&lt;strong>; tanuj khattar &lt; /strong>; &lt;br />; | 11:00 AM CST &lt; /p>; &lt;p>; $ 5M Xprize /Google Quantum AI竞赛，以加速量子应用Q＆amp; a &lt;br />;主持人：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />; | 11:00 AM CST &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;谈话&lt;/h2>; &lt;h3>;星期一&lt;/h3>; >; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/a45.1&quot;>;从少数少数人身上认识高度enterge单量测量&lt;/a>; &lt;br />;主持人：&lt;strong>; hsin-yuan huang &lt; /strong>; &lt;br />;作者：&lt;strong>; hsin-yuan huang &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; A45：机器学习量子物理学的新边界&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.ops.org/meeting/mar24/session/session/a51.2&quot;>;用超导码头的模拟量子模拟&lt;/a>; &lt;br />;主持人：&lt;strong>; trond andersen &lt;/strong>; &lt;br />;作者：&lt;strong>; trond i andersen &lt;/strong>;，&lt;strong>; xiao mi &lt;/strong >;，&lt;strong>; Amir H Karamlou &lt;/strong>;，&lt;strong>; Nikita Astrakhantsev &lt;/strong>;，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; Julia Berndtsson &lt;/strong>; strong>;，&lt;strong>; dmitry abanin &lt;/strong>;，&lt;strong>; lev b ioffe &lt;/strong>;，&lt;strong>; yu chen &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; pedram roushan &lt; /strong>; &lt;br />; &lt;em>;会话A51：嘈杂的量子硬件上的应用程序i &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/sessision /B50.6&quot;>;在表面代码电路上下文中的电路错误&lt;/a>; &lt;br />;主持人：&lt;strong>; dripto m debroy &lt;/strong>; &lt;br />;作者：&lt;strong>; dripto m debroy &lt;/strong &lt;/strong &lt;/strong &lt;/strong >;，&lt;strong>; Jonathan A Gross &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>;，&lt;strong>; Zhang jiang &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session b50：用qCVV技术表征噪声&lt;/em em em em em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/b51.6&quot;>;量子计算惯性融合目标设计的停止功率量量经典算法的限制&lt;/a>; &lt;br />;主持人：Andrew D. Baczewski &lt;br />;作者：&lt;strong>; Nicholas C. Rubin &lt; /strong>;，Dominic W. Berry，Alina Kononov，&lt;strong>; fionn D. Malone &lt;/strong>;，&lt;strong>; Tanuj Khattar &lt;/strong>;，Alec White，&lt;strong>; Joonho Lee &lt;/strong>;，&lt;strong>; Hartmut Neven &lt;/strong>;，&lt;strong>; Ryan Babbush &lt;/strong>;，Andrew D. BACZEWSKI &lt;br />; &lt;em>; session B51：量子应用的异构设计&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2308.12355.pdf&quot;>;链接到纸张&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/b51.7&quot;>;以及经典算法的极限&lt;/a>; &lt;br />;主持人：&lt;strong>; nicholas C. rubin &lt;/strong>; &lt;br />;作者：&lt;strong>; nicholas C. Rubin &lt;/strong>;，Dominic W. Berry，Dominic W. Berry， Alina Kononov，&lt;strong>; Fionn D. Malone &lt;/strong>;，&lt;strong>; Tanuj Khattar &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Joonho Lee &lt;/strong>;，&lt;strong>; hartmut neven neven &lt;/strong>;，&lt;strong &lt;strong >; Ryan Babbush &lt;/strong>;，Andrew D. Baczewski &lt;br />; &lt;em>; session B51：量子应用的异质设计&lt;/em>; &lt;br />; &lt;br />; 2308.12352.pdf“>;链接到纸张&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/b56.4&quot;>; nisq to Fart bolance &lt;/a>; &lt;br />;主持人：&lt;strong>; sabrina s hong &lt; /strong>; &lt;br />;作者：&lt;strong>; sabrina s hong &lt; /strong>; &lt;br />; &lt;br />;从NISQ到FART耐受&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/b31.9&quot;>; /a>; &lt;br />;主持人：&lt;strong>; Ramis Movassagh &lt; /strong>; &lt;br />;作者：Alireza Seif，Yu-Xin Wang，&lt;strong>; Ramis Movassagh &lt; /strong>;，Aashish A. clerk &lt;br />; &lt;em>;会议B31：多体系统的测量引起的关键性&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2310.18305.pdf&quot;>;链接到纸张/p>; &lt;p>; &lt;a href=&quot;https://meetings.ops.org/meeting/mar24/session/b52.9&quot;>;有效的量子量，保真度和计算成本的噪音量子处理实验&lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt; BR/>;主持人：&lt;strong>; Salvatore Mandra &lt;/strong>; &lt;br />;作者：&lt;strong>; Kostyantyn Kechedzhi &lt;/strong>;，&lt;strong>; Sergei v isakov v isakov &lt;/strong>;，&lt;strong>; salvatore mandra &lt;/strong &lt;/strong>; ，&lt;strong>;本杰明·维拉隆加（Benjamin Villalonga）&lt;/strong>;，&lt;strong>; x。 mi &lt;/strong>;，&lt;strong>; sergio boixo &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session b52：量子算法和复杂性&lt;/em>; &lt;br />; =“ https://arxiv.org/pdf/2306.15970.pdf”>;链接到纸张&lt;/a>; &lt;/a>; &lt;/p>; &lt;p>; &lt;a href =“ Session /D60.4“>;使用机器学习相互作用电位和原子位置的协方差&lt;/a>; &lt;br />;演示者：MGCINI K PHUTHI &lt;br />;作者：Mgcini K Phuthi，Yang Huang，Michael Widom，Michael Widom ，&lt;strong>; ekin d cubuk &lt;/strong>;，Venkat Viswanathan &lt;br />; &lt;em>;会话D60：分子和材料的机器学习：化学空间和动力学&lt;/em>; &lt;/em>; &lt;/p>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/div>; &lt;h3>;星期二&lt;/h3>; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;p>; &lt;a href =“ https：https： //meetings.aps.org/meeting/mar24/session/f50.4&quot;>; in-situ脉冲信封特征技术（Inspect）&lt;/a>; &lt;br />; &lt;br />;主持人：&lt;strong>; Zhang jiang &lt;/strong>; &lt;/strong>; &lt;br />;作者：&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>; Jonathan a Gross &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session f50：高级随机基准和门校准态BR/>;主持人：&lt;strong>; Jonathan A Gross &lt;/strong>; &lt;br />;作者：&lt;strong>; Jonathan a Gross &lt;/strong>;，&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>;éliegenois，dripto m debroy &lt;/strong>;，ze-pei cian*，&lt;strong>; wojciech mruczkiewicz &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session f50：高级随机基准测试和门校准&lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.org/meeting/mar24/session/eee01.2”>;带有二次模型的回归统计物理&lt;/a>; &lt;br />;主持人：Blake Bordelon &lt;br />; Bordelon，Cengiz Pehlevan，&lt;strong>; Yasaman Bahri &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session EE01：V：统计和非线性物理学II​​ &lt;/em>; &lt;/em>; &lt;/em>; &lt;/p &lt;/p &lt;p>; &lt;a href = a href =“ https：/ /Meetings.aps.org/meeting/mar24/session/g51.2&quot;>;改进的状态准备电子结构的先定制模拟&lt;/a>; &lt;br />;主持人：&lt;strong>; William J Huggins &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/ Br/>;作者：&lt;strong>; William J Huggins &lt;/strong>;，&lt;strong>; Oskar Leimkuhler &lt;/strong>;，&lt;strong>; Torin f stetina &lt;/strong>;，&lt;strong>; birgitta whaley &lt;/strong>; &lt;br />; &lt;em>; session G51：汉密尔顿模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/g30.2&quot;>;控制大型超导量子处理器&lt;/a>; &lt;br />;主持人：&lt;strong>; Paul V. Klimov &lt;/strong>; &lt;br />;作者：&lt;strong>; Paul V. Klimov &lt;/strong>;，&lt;strong>; Andreas Bengtsson &lt;/strong>;，&lt; Strong>; Chris Quintana &lt;/strong>;，&lt;strong>; Alexandre Bourassa &lt;/strong>;，&lt;strong>; Sabrina Hong &lt;/strong>;，&lt;strong>; Andrew Dunsworth &lt;/strong>;，&lt;strong>; Kevin J. Satzinger &lt;/strong>; ，&lt;strong>; William P. Livingston &lt;/strong>;，&lt;strong>; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Murphy Y. Niu &lt;/strong>;，&lt;strong>; Trond I. Andersen I. Yaxing Zhang &lt;/strong>;，&lt;strong>; desmond Chik &lt;/strong>;，&lt;strong>; Zijun Chen &lt;/strong>;，&lt;strong>; Charles Neill &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; catherine Erickson &lt;/strong>; Alejandro Grajales dau &lt;/strong>;，&lt;strong>; Anthony Megrant &lt;/strong>;，&lt;strong>; Pedram Roushan &lt;/strong>;，&lt;strong>; Alexander N. Korotkov &lt;/strong>;，&lt;strong>; Julian Kelly &lt;/strong>;，&lt;strong>; &lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; yu chen &lt;/strong>;，&lt;strong>; hartmut neven neven &lt;/strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session g30：量子计算的商业应用&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.02321.pdf&quot;>;链接到纸张&lt;/a>; &lt;/a>; &lt;/p>; &lt;p>; &lt;a href =“ /meeting/mar24/session/g50.5&quot;>; gaussian玻色子采样：确定量子优势&lt;/a>; &lt;br />;主持人：Peter D Drummond &lt;br />;作者：Peter d Drummond，Alex Dlummond，Alex Dellios，Alex Dellios，Ned Goodman，Ned Goodman，Margaret D里德（Reid），&lt;strong>; ben villalonga &lt;/strong>; &lt;br />; &lt;em>; session g50：量子表征，验证和验证ii &lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https：// https：// 。 Yiqing Zhou，Yichen Xu，Chao Wan，Jin Zhou，&lt;strong>; Yuri d Lensky &lt;/strong>;，Jesse Hoke，&lt;strong>; Pedram Roushan &lt;/strong>;，Kilian Q Weinberger，Eun-Ah Kim kim &lt;br />; >;会议G50：量子表征，验证和验证II &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/k48.10&quot;>;平衡超导电路中的耦合&lt;/a>; &lt;br />;主持人：&lt;strong>; Daniel T Sank &lt;/strong>; &lt;br />;作者：&lt;strong>; Daniel T Sank &lt;/strong>;，&lt;strong>; Sergei v Isakov &lt;/strong >;，&lt;strong>; Mostafa Khezri &lt;/strong>;，&lt;strong>; Juan Atalaya &lt;/strong>; &lt;br />; &lt;em>; session K48：强烈驱动的超导系统&lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;p>; &lt;p>; &lt;a href = a href = “ https://meetings.ops.org/meeting/mar24/session/k49.12&quot;>; resource使用qᴜᴀʟᴛʀᴀɴ&lt;/a>; &lt;br />;主持人：&lt;strong>; tanuj khattar &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/>; br/>;作者：&lt;strong>; tanuj khattar &lt;/strong>;，&lt;b>; Matthew Harrigan &lt;/b>;，&lt;b>; Fionn D. Malone &lt;/b>;，&lt;b>; nour yosri &lt;/b>;，&lt;b>; Nicholas C. rubin &lt;/b>; &lt;br />; &lt;em>;会话K49：近期量子计算机上的算法和实现;“>; &lt;br />; &lt;/div>; &lt;h3>;星期三&lt;/h3>; &lt;div style =” Margin-Left：20px;“>; &lt;p>; &lt;a href =” https://meetings.aps.aps.org/会议/mar24/session/m24.1“>;用超导量子台发现新颖的量子动态&lt;/a>; &lt;br />; &lt;br />;主持人：&lt;strong>; pedram roushan &lt;/strong>; &lt;br />;作者：&lt;strong>; pedram roushan &lt;/强>; &lt;br />; &lt;em>;会话M24：跨平台的模拟量子模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.orp.org/meeting/mar24/mar24/session/session/m27 .7“>;三阴性乳腺癌中的解剖肿瘤异质性：动态细胞细胞和细胞矩阵相互作用的关键作用，Celeste Nelson，Molly Brennan，&lt;strong>; Mohak Patel &lt; /strong>;，Christian Franck，Sophia Martinez，Joe Tien，Lena Gamboa，Thomas Valentin，Amanda Khoo，Amanda Khoo，Evelyn K Williams &lt;br />;细胞和组织II &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/n48.2&quot;>;朝着实施受保护的电荷-Parity Qubits &lt; /a>; &lt;br />;主持人：Abigail Shearrow &lt;br />;作者：Abigail Shearrow，Matthew Snyder，Bradley G Cole，Kenneth R Dodge，Yebin Liu，Andrey Klots，&lt;strong>; Plourde，Robert McDermott &lt;br />; &lt;em>; session N48：非常规超导码头&lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ .3“>;电子电容隧道连接处的受保护电荷 - 偏向码头&lt;/a>; &lt;br />;主持人：Bradley G Cole &lt;br />;作者：Bradley G Cole，Kenneth R Dodge，Yebin Liu，Yebin Liu，Abigail Shearrow，Matthew Snyder Snyder Snyder Snyder Snyder Snyder Snyder Snyder Snyder Snyder ，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; lev b ioffe &lt;/strong>;，Robert McDermott，Blt Plourde &lt;br />; &lt;br />; &lt;em>; session n48：非常规的超导速度Qubits &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/p &lt;p &lt;p >; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/n51.7&quot;>;克服量子错误校正中的泄漏&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;/strong>; &lt;br />;作者：&lt;strong>; Kevin C. Miao &lt;/strong>;，&lt;strong>; Matt McEwen &lt;/strong>;，&lt;strong>; Juan Atalaya &lt;/strong>;，&lt;strong>; dvir kafri &lt;/strong &lt;/strong >;，&lt;strong>; Leonid P. Pryadko &lt;/strong>;，&lt;strong>; Andreas Bengtsson &lt;/strong>;，&lt;strong>; Alex Opremcak &lt;/strong>;，&lt;strong>; Kevin J. Satzinger J. Satzinger J. Satzinger &lt;/strong>;，&lt;strong>; Zijun Chen &lt;/strong>;，&lt;strong>; Paul V. Klimov &lt;/strong>;，&lt;strong>; Chris Quintana &lt;/strong>;，&lt;strong>; Rajeev Acharya &lt;/strong>;，&lt;strong>; Kyle Anderson &lt;/strong &lt;/strong &lt;/strong >; Markus Ansmann &lt;/strong>;，&lt;strong>; Frank Arute &lt;/strong>;，&lt;strong>; Kunal Arya &lt;/strong>;，&lt;strong>; Abraham Asfaw &lt;/strong>;，&lt;strong>; Joseph C. Bardin &lt;/strong>;， &lt;strong>; Alexandre Bourassa &lt;/strong>;，&lt;strong>; Jenna Bovaird &lt;/strong>;，&lt;strong>; Leon Brill &lt;/strong>;，&lt;strong>; Bob B. B. B. Buckley &lt;/strong>;，&lt;strong>; David A. Buell A. Buell &lt; /strong>;，&lt;strong>; Tim Burger &lt;/strong>;，&lt;strong>; Brian Burkett &lt;/strong>;，&lt;strong>; Nicholas Bushnell &lt;/strong>;，&lt;strong>; Juan Campero &lt;/strong>;，&lt;strong>; Ben Chiaro &lt; /strong>;，&lt;strong>; Roberto Collins &lt;/strong>;，&lt;strong>; Paul Conner &lt;/strong>;，&lt;strong>; Alexander L. Crook &lt;/strong>;，&lt;strong>; Ben Curtin &lt;/strong &lt;/strong>;，&lt;strong>; dripto M. Debroy &lt;/strong>;，&lt;strong>; Sean DeMura &lt;/strong>;，&lt;strong>; Andrew Dunsworth &lt;/strong>;，&lt;strong>; Catherine Erickson &lt;/strong>;，&lt;strong>; reza fatemi &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong >; Vinicius S. ferreira &lt;/strong>;，&lt;strong>; Leslie Flores burgos &lt;/strong>;，&lt;strong>; ebrahim forati &lt;/strong>;，&lt;strong>; Austin G. Fowler &lt;/strong>;，&lt;strong>; Brooks Foxen &lt;/ Strong>;，&lt;strong>; Gonzalo Garcia &lt;/strong>;，&lt;strong>; William Giang &lt;/strong>;，&lt;strong>; Craig Gidney &lt;/strong>;，&lt;strong>; Marissa Giustina &lt;/strong>;，&lt;strong>; raja gosula &lt;/strong>; Strong>;，&lt;strong>; Alejandro Grajales dau &lt;/strong>;，&lt;strong>; Jonathan A. Gross &lt;/strong>;，&lt;strong>; Michael C. Hamilton &lt;/strong>;，&lt;strong>; Sean D. Harrington &lt;/strong>;， &lt;strong>; Paula Heu &lt;/strong>;，&lt;strong>; Jeremy Hilton &lt;/strong>;，&lt;strong>; Markus R. Hoffmann &lt;/strong>;，&lt;strong>; Sabrina Hong &lt;/strong>;，&lt;strong>; Trent Huang &lt;/strong &lt;/strong >;, &lt;strong>;Ashley Huff&lt;/strong>;, &lt;strong>;Justin Iveland&lt;/strong>;, &lt;strong>;Evan Jeffrey&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong >;，&lt;strong>; Julian Kelly &lt;/strong>;，&lt;strong>; Seon Kim &lt;/strong>;，&lt;strong>; Fedor Kostritsa &lt;/strong>;，&lt;strong>; John Mark Kreikebaum &lt;/strong>;，&lt;strong>; David Landhuis &lt;/ strong>;，&lt;strong>; pavel laptev &lt;/strong>;，&lt;strong>;百合法&lt;/strong>;，&lt;strong>; kenny Lee &lt;/strong>;，&lt;strong>; Brian J. Lester &lt;/strong>;，&lt;strong>; Alexander t 。 Anthony Megrant&lt;/strong>;, &lt;strong>;Xiao Mi&lt;/strong>;, &lt;strong>;Shirin Montazeri&lt;/strong>;, &lt;strong>;Alexis Morvan&lt;/strong>;, &lt;strong>;Ofer Naaman&lt;/strong>;, &lt;strong>; Matthew Neeley &lt;/strong>;，&lt;strong>; Charles Neill &lt;/strong>;，&lt;strong>; ani nersisyan &lt;/strong>;，&lt;strong>; Michael Newman &lt;/strong>;，&lt;strong>; >; Anthony Nguyen &lt;/strong>;，&lt;strong>; Murray nguyen &lt;/strong>;，&lt;strong>; Rebecca Potter &lt;/strong>;，&lt;strong>; Charles Rocque &lt;/strong>;，&lt;strong>; Pedram Roushan &lt;/strong>;，&lt;strong &lt;strong >; Kannan Sankaragomathi &lt;/strong>;，&lt;strong>; Christopher Schuster &lt;/strong>;，&lt;strong>; Michael J. Shearn &lt;/strong>;，&lt;strong>; Aaron Shorter &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; &lt;strong>; Vladimir Shvarts &lt;/strong>;，&lt;strong>; Jindra Skruzny &lt;/strong>;，&lt;strong>; w。 Clarke Smith &lt;/strong>;，&lt;strong>; George Sterling &lt;/strong>;，&lt;strong>; Marco Szalay &lt;/strong>;，&lt;strong>; Douglas Thor &lt;/strong>;，&lt;strong>; Alfredo Torres &lt;/strong>;，&lt;strong>; Theodore White&lt;/strong>;, &lt;strong>;Bryan WK Woo&lt;/strong>;, &lt;strong>;Z. Jamie Yao &lt;/strong>;，&lt;strong>; ping yeh &lt;/strong>;，&lt;strong>; juhwan yoo &lt;/strong>;，&lt;strong>; grayson Young &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>; Ningfeng Zhu &lt;/strong>;，&lt;strong>; nicholas Zobrist &lt;/strong>;，&lt;strong>; hartmut neven &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; andre petukhov &lt;/strong>;，&lt;strong>;，&lt;strong>; Alexander N. Korotkov &lt;/strong>;，&lt;strong>; Daniel Sank &lt;/strong>;，&lt;strong>; Yu Chen &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session n51：量子错误校正代码性能和实现i &lt;/em>; &lt;br />; &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;链接到纸张：//meetings.aps.org/meeting/mar24/session/n51.11“>;对表面代码的性能进行建模，以非均匀误差分布：第1部分&lt;/a>; &lt;br />;主持人：&lt;strong>; yuri：&lt;strong>; yuri D Lensky&lt;/strong>; &lt;br />; Authors: &lt;strong>;Yuri D Lensky&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Igor Aleiner&lt;/强>; &lt;br />; &lt;em>;会话N51：量子错误校正代码性能和实现I &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/ Session/N51.12“>;对表面代码的性能进行非均匀错误分布进行建模：第2部分&lt;/a>; &lt;br />; &lt;br />;主持人：&lt;strong>; Volodymyr Sivak &lt;/strong>; &lt;br />;作者：&lt;strong >; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Michael Newman &lt;/strong>;，&lt;strong>; Cody Jones &lt;/strong>;，&lt;strong>; Henry Schurkus &lt;/strong>;，&lt;strong>; dvir kafri &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong >; Yuri d Lensky &lt;/strong>;，&lt;strong>; Paul Klimov &lt;/strong>;，&lt;strong>; Kostyantyn Kechedzhi &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>; &lt;br/>;校正代码性能和实施I &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/q51.7&quot;>;高度优化的张量网络网络收缩&lt;/a>; &lt;br />;主持人的模拟&lt;/a>; &lt;strong>;本杰明·维拉隆加（Benjamin Villalonga）&lt; /strong>; &lt;br />;作者：&lt;strong>; benjamin villalonga &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session Q51： Co-evolution of Quantum Classical Algorithms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q61.7&quot;>;Teaching modern quantum computing concepts using各个级别的动手开源软件&lt;/a>; &lt;br />;主持人：&lt;strong>; Abraham Asfaw &lt; /strong>; &lt;br />;作者：&lt;strong>; Abraham Asfaw &lt; /strong>; &lt;br />; &lt;br />; >;会议Q61：在所有级别II &lt;/em>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;星期四&lt; /h3>; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/s51.1&quot;>;颜色代码的源解码器&lt;/a>; &lt;br />;主持人：&lt;strong>; craig gidney &lt;/strong>; &lt;br />;作者：&lt;strong>; craig gidney &lt;/strong>;，&lt;strong>; cody jones &lt;/strong>; &lt;br />; &lt;em>;会话S51：量子错误校正代码性能和实现II &lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2312.08813.pdf&quot;>;链接到纸张&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/s18.2&quot;>;用大语言模型进行Hartree-Fock多体物理学计算/a>; &lt;br />;主持人：&lt;strong>; eun-ah kim &lt;/strong>; &lt;br />;作者：&lt;strong>; eun-ah kim kim &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strand pan，&lt;strong>; nayantara mudur &lt;/strong>; ，威廉·塔兰托（William Taranto），&lt;strong>; subhashini venugopalan &lt;/strong>;，&lt;strong>; yasaman bahri &lt;/strong>;，&lt;strong>; Michael P Brenner &lt;/strong>; &lt;br />;物理学学习i &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/session/s51.5&quot;>;新的方法将资源降低水平的新方法代码&lt;/a>; &lt;br />;主持人：&lt;strong>;迈克尔·纽曼Peter Brooks&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;Session S51: Quantum Error Correction Code Performance and Implementation II&lt;/em>; &lt;br />; &lt;a href=&quot;https:/ /arxiv.org/pdf/2312.04522.pdf&quot;>; link to Paper &lt;/a>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://meetings.aps.orp.org/meeting/mar24/mar24/session/session/session/s49.10 “>;将量子计算机应用于药物设计的挑战和机会, Leticia Gonzalez, Elica Kyoseva, Nikolaj Moll, Markus Oppel, Robert M. Parrish, &lt;strong>;Nicholas C. Rubin&lt;/strong>;, Michael Streif, Christofer S. Tautermann, Horst Weiss, Nathan Wiebe, Clemens Utschig-Utschig &lt;br />; &lt;em>;会话S49：近期应用程序的量子算法进步a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/t45.1&quot;>;从Google的新应用程序中的超级Quadratic量子优势中调度&lt;/ a>; &lt;br />;主持人：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />;作者：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session t45：量子算法的最新进展&lt; /em em em em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/t48.11&quot;>; qubit作为反射计&lt;/a>; presenter：&lt;br />; &lt;br />; &lt;br />; &lt;strong >; yaxing Zhang &lt;/strong>; &lt;br />;作者：&lt;strong>; yaxing Zhang &lt;/strong>;，&lt;strong>; benjamin chiaro &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session t48：超导制造，包装，包装等；验证&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.ops.org/meeting/mar24/mar24/session/session/w14.3&quot;>;随机测量诱导的随机-MATRIX在非局限性的相位过渡floquet量子电路&lt;/a>; &lt;br />;主持人：Aleksei Khindanov &lt;br />;作者：Aleksei Khindanov，&lt;strong>; lara faoro &lt;/strong>;，&lt;strong>; lev ioffe &lt;/strong>;，&lt;strong>; /strong>; &lt;br />; &lt;em>; session W14：测量引起的相变&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.ops.org/meeting/mar24/session/ W58.5“>;有限密度的连续限量多体基接地状态&lt;/a>; &lt;br />;主持人：Subhayan Sahu &lt;br />;作者：Subhayan Sahu，&lt;strong>;GuifréVidalvidal>; &lt; /strong>; &lt; /strong>; &lt;br / >; &lt;em>;会话W58：流体动力学和相关学科中的极端计算科学发现II &lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.orp.org/meeting/mar24/ session/w50.8“>;在海森伯格自旋链中无限温度下磁化的动力学&lt;/a>; &lt;br />;主持人：&lt;strong>; Eliott Rosenberg &lt;/strong>; &lt;br />;作者：&lt;strong>; Eliott Rosenberg &lt;/ strong>;, &lt;strong>;Trond Andersen&lt;/strong>;, Rhine Samajdar, &lt;strong>;Andre Petukhov&lt;/strong>;, Jesse Hoke*,&lt;strong>; Dmitry Abanin&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>; Ilya drozdov &lt;/strong>;，&lt;strong>;凯瑟琳·埃里克森（Catherine Erickson）&lt;/strong>;，&lt;strong>; Paul Klimov &lt;/strong>;，&lt;strong>; xiao mi &lt;/strong>;，&lt;strong>; Alexis Morvan &lt;/strong>;， &lt;strong>; Matthew Neeley &lt;/strong>;，&lt;strong>; Charles Neill &lt;/strong>;，&lt;strong>; Rajeev Acharya &lt;/strong>;，&lt;strong>; Richard Allen &lt;/strong>;，&lt;strong>; Kyle Anderson &lt;/strong>;， &lt;strong>; Markus Ansmann &lt;/strong>;，&lt;strong>; Frank Arute &lt;/strong>;，&lt;strong>; Kunal Arya &lt;/strong>;，&lt;strong>; Abraham Asfaw &lt;/strong>;，&lt;strong>; Juan Atalaya &lt;/strong>;， &lt;strong>; Joseph Bardin &lt;/strong>;，&lt;strong>; a。 Bilmes &lt;/strong>;，&lt;strong>; Gina Bortoli &lt;/strong>;，&lt;strong>; Alexandre Bourassa &lt;/strong>;，&lt;strong>; Jenna Bovaird &lt;/strong>;，&lt;strong>; Leon Brill &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Michael Broughton &lt;/strong>;，&lt;strong>; Bob B. Buckley &lt;/strong>;，&lt;strong>; David Buell &lt;/strong>;，&lt;strong>; Tim Burger &lt;/strong>;，&lt;strong>; Brian Burkett &lt;/strong>;，&lt;strong >; Nicholas Bushnell &lt;/strong>;，&lt;strong>; Juan Campero &lt;/strong>;，&lt;strong>; Hung-Shen Chang &lt;/strong>;，&lt;strong>; Zijun Chen &lt;/strong>;，&lt;strong>; benjamin chiaro &lt;/strong &lt;/strong &lt;/strong>;， &lt;strong>; Desmond Chik &lt;/strong>;，&lt;strong>; Josh Cogan &lt;/strong>;，&lt;strong>; Roberto Collins &lt;/strong>;，&lt;strong>; Paul Conner &lt;/strong>;，&lt;strong>; William Courtney &lt;/strong>;， &lt;strong>; Alexander Crook &lt;/strong>;，&lt;strong>; Ben Curtin &lt;/strong>;，&lt;strong>; Dripto Debroy &lt;/strong>;，&lt;strong>; Alexander del Toro Barba &lt;/strong>;，&lt;strong>; Sean Demura &lt;/strong >;，&lt;strong>; Agustin di Paolo &lt;/strong>;，&lt;strong>; Andrew Dunsworth &lt;/strong>;，&lt;strong>; Clint Earle &lt;/strong>;，&lt;strong>; e。 farhi &lt;/strong>;，&lt;strong>; reza fatemi &lt;/strong>;，&lt;strong>; vinicius ferreira &lt;/strong>;，&lt;strong>; leslie flores &lt;/strong>;，&lt;strong>; ebrahim forati &lt;/strong>;，&lt;strong>; austin Fowler &lt;/strong>;，&lt;strong>; brooks foxen &lt;/strong>;，&lt;strong>; gonzalo garcia &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>;，&lt;strong>; william giang &lt;/strong>;，&lt;strong>; craig Gidney &lt;/strong>;，&lt;strong>; dar Gilboa &lt;/strong>;，&lt;strong>; Marissa Giustina &lt;/strong>;，&lt;strong>; raja gosula &lt;/strong>;，&lt;strong>; alejandro grajales dau &lt;/strong>;，&lt;strong>; Jonathan Gross&lt;/strong>;, &lt;strong>;Steve Habegger&lt;/strong>;, &lt;strong>;Michael Hamilton&lt;/strong>;, &lt;strong>;Monica Hansen&lt;/strong>;, &lt;strong>;Matthew Harrigan&lt;/strong>;, &lt;strong>; Sean Harrington &lt;/strong>;，&lt;strong>; Paula Heu &lt;/strong>;，&lt;strong>; Gordon Hill &lt;/strong>;，&lt;strong>; Markus Hoffmann &lt;/strong>;，&lt;strong>; Sabrina Hong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Trent Huang &lt;/strong>;，&lt;strong>; Ashley Huff &lt;/strong>;，&lt;strong>; William Huggins &lt;/strong>;，&lt;strong>; lev ioffe &lt;/strong>;，&lt;strong>; Sergei Isakov &lt;/strong>;，&lt;strong>; Justin Iveland &lt;/strong>;，&lt;strong>; Evan Jeffrey &lt;/strong>;，&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>; Cody Jones &lt;/strong>;，&lt;strong>; Pavol Juhas &lt;/strong>;，&lt;strong>; D . kafri &lt;/strong>;，&lt;strong>; tanuj khattar &lt;/strong>;，&lt;strong>; mostafa khezri &lt;/strong>;，&lt;strong>;máriaKieferová&lt;/strong>;，&lt;strong>; seon kim &lt;/strong>;，&lt;strong>; alexei alexei Kitaev &lt;/strong>;，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; Alexander Korotkov &lt;/strong>;，&lt;strong>; Fedor Kostritsa &lt;/strong>;，&lt;strong>; John Mark Kreikebaum &lt;/strong>;，&lt;strong>; David Landhuis &lt;/strong>;，&lt;strong>; Pavel Laptev &lt;/strong>;，&lt;strong>; Kim Ming Lau &lt;/strong>;，&lt;strong>; Lily Laws &lt;/strong>;，&lt;strong>; Joonho Lee &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong &lt;strong >; Kenneth Lee &lt;/strong>;，&lt;strong>; Yuri Lensky &lt;/strong>;，&lt;strong>; Brian Lester &lt;/strong>;，&lt;strong>; Alexander Lill &lt;/strong>;，&lt;strong>; Wayne Liu &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong &lt;strong >; William P. Livingston &lt;/strong>;，&lt;strong>; a。 locharla &lt;/strong>;，&lt;strong>; SalvatoreMandrà&lt;/strong>;，&lt;strong>; Orion Martin &lt;/strong>;，&lt;strong>; Steven Martin &lt;/strong>;，&lt;strong>; Jarrod McClean &lt;/strong>;，&lt;strong>; Matthew McEwen &lt;/strong>;，&lt;strong>; Seneca Meeks &lt;/strong>;，&lt;strong>; Kevin Miao &lt;/strong>;，&lt;strong>; Amanda Mieszala &lt;/strong>;，&lt;strong>; Shirin Montazeri &lt;/strong>;，&lt;strong>; Ramis Movassagh &lt;/strong>;，&lt;strong>; wojciech mruczkiewicz &lt;/strong>;，&lt;strong>; ani nersisyan &lt;/strong>;，&lt;strong>; Michael Newman &lt;/strong &lt;/strong>;，&lt;strong>; Anthony Nguyen &lt;/strong>;，&lt;strong>; Murray Nguyen &lt;/strong>;，&lt;strong>; m。 Niu &lt;/strong>;，&lt;strong>; Thomas O&#39;Brien &lt;/strong>;，&lt;strong>; Seun Omonije &lt;/strong>;，&lt;strong>; Alex Opremcak &lt;/strong>;，&lt;strong>; rebecca Potter &lt;/strong>;，&lt;strong >;Leonid Pryadko&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;David Rhodes&lt;/strong>;, &lt;strong>;Charles Rocque&lt;/strong>;, &lt;strong>;N.鲁宾&lt;/strong>;，&lt;strong>; negar saei &lt;/strong>;，&lt;strong>;丹尼尔·桑克&lt;/strong>;，&lt;strong>; kannan sankaragomathi &lt;/strong>;，&lt;strong>; kevin kevin satzinger &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; henry Schurkus &lt;/strong>;，&lt;strong>; Christopher Schuster &lt;/strong>;，&lt;strong>; Michael Shearn &lt;/strong>;，&lt;strong>; Aaron Shorter &lt;/strong>;，&lt;strong>; Shvarts &lt;/strong>;，&lt;strong>; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Jindra Skruzny &lt;/strong>;，&lt;strong>; Clarke Smith &lt;/strong>;，&lt;strong>; Rolando Somma &lt;/strong &lt;/strong>; Sterling &lt;/strong>;，&lt;strong>; Doug菌株&lt;/strong>;，&lt;strong>; Marco Szalay &lt;/strong>;，&lt;strong>; Douglas Thor &lt;/strong>;，&lt;strong>; Alfredo Torres &lt;/strong>;，&lt;strong>; guifre vidal &lt;/strong>;，&lt;strong>;本杰明·维拉隆加（Benjamin villalonga）&lt;/strong>;，&lt;strong>;凯瑟琳·沃尔格拉夫·海德威勒（Catherine Vollgraff Heidweeller）&lt;/strong>;，&lt;strong>; theodore white &lt;/strong>;，&lt;strong>; bryan woo &lt;/strong>;，&lt;strong>; Cheng Xing &lt;/strong>;，&lt;strong>; Jamie Yao &lt;/strong>;，&lt;strong>; ping yeh &lt;/strong>;，&lt;strong>; juhwan yoo &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>; Adam Zalcman &lt;/strong>;，&lt;strong>; yaxing Zhang &lt;/strong>;，&lt;strong>; ningfeng Zhu &lt;/strong>;，&lt;strong>; nicholas Zobrist &lt;/strong>;，&lt;strong>; Ryan Babbush &lt;/strong>;，&lt;strong>; Dave Bacon &lt;/strong>;，&lt;strong>; Sergio Boixo &lt;/strong>;，&lt;strong>; Jeremy Hilton &lt;/strong>;，&lt;strong>; Erik Lucero &lt;/strong>;，&lt;strong>; Anthony Megrant &lt;/strong>;，&lt;strong>; Julian Kelly &lt;/strong>;，&lt;strong>; Yu Chen &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，Vedika Khemani，Sarang Gopalakrishnan，&lt;strong>; toma toma toma tomatomažprosen&lt;/ strong>;，&lt;strong>; pedram roushan &lt;/strong>; &lt;br />; &lt;em>; session W50：多体物理学的量子模拟&lt;/em>; &lt;br />; &lt;br />; &lt;a href =“ https://arxiv.org/ pdf/2306.09333.pdf“>;链接到纸&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.orp.org/meeting/mar24/mar24/sessive/w50.13&quot;>;量子计算机上的方法&lt;/a>; &lt;br />;主持人：Kianna Wan &lt;br />;作者：Kianna Wan，Dominic W Berry，&lt;strong>; Ryan Babbush &lt; /strong>; &lt;br />;多体物理学的仿真&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/div>; &lt;h3>;星期五&lt;/h3>; &lt;div style =“ Margin-Left：20px;”>; &lt;p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/y43.1&quot;>;量子计算行业和保护国家安全：什么工具会起作用吗？&lt;/a>; &lt;br />;主持人：&lt;strong>; kate Weber &lt; /strong>; &lt;br />;作者：&lt;strong>; Kate Weber &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; y43：行业，创新和国家安全：找到正确的余额&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/session/y46.3&quot;>;新颖的充电效果在Fluxonium Qubit &lt;/a>; &lt;br />;主持人：&lt;strong>; Agustin di paolo &lt;/strong>; &lt;br />;作者：&lt;strong>; agustin di paolo &lt;/strong>;，Kyle Serniak，Andrew J Kerman，Andrew J Kerman，&lt;strong，&lt;strong &lt;strong >; William D Oliver &lt;/strong>; &lt;br />; &lt;em>;会话y46：基于Fluxonium的超导sigibits &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ /mar24/session/z46.3&quot;>;超导电路中参数相互作用的microwave工程&lt;/a>; &lt;br />;主持人：&lt;strong>; ofer naaman &lt;/strong>; &lt;br />;作者：&lt;strong>;强>; &lt;br />; &lt;em>;会话Z46：宽带参数放大器和循环器&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/mar24/session/session/z62 .3“>;使用内核多项式方法的大磁单元细胞的线性自旋波理论&lt;/a>; &lt;br />;主持人：Harry Lane &lt;br />;作者：Harry Lane，Hao Zhang，David A Dahlbom，Sam Quinn，&lt; Strong>; Rolando d Somma &lt;/strong>;，Martin P Murigal，Cristian D Batista，Kipton Barros &lt;br />; &lt;em>; secess Z62：合作现象，理论&lt;/em>; &lt;/em>; &lt;/em>; &lt;/p>; &lt;/div>; &lt;！ - 脚注 - >; &lt;hr width =“ 80％” />; &lt;p>; &lt;span class =“苹果式式启动” style =“ font-size：x-small;”>; &lt;sup>; &lt;b>;*&lt; />; b>; &lt;/sup>;在Google &lt;/span>; &lt;/p>; &lt;/penter>; &lt;link href =“ http://blog.research.google/feeds/2754526782497247497/comments/comments/comments/default/default =” &quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html#comment-form “ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/default/275452678249724972472477777777.7545267824972497&#39;R =“” &quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml “/>; &lt;link href =” http://blog.research.google/2024/03/google-at-aps-2024.html“ rel =“替代” title =“ google at aps 2024” type =“ type =” type =“文本/ html“/>; &lt;aunder>; &lt;名称>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777526611 &lt;/uri>; &lt;emage>; &lt;Email>; ：Image Height =“ 16” RER =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded.gif.gif.gif” width width =“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg “ width =” 72“ XMLNS：媒体=” http://search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr：thr>; &lt;thr：thr>; 0 &lt;/thr>; 0 &lt;/thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999年：Blog-8474926331452026626.POST-16952642777638670894 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2024-02-222T12：05：00.000-08：00.000-08：00：00：00：00：00：00 &lt;/00 &lt;/00 ：07：07：08.500-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ Machine Intelligence”>; &lt;/category>; &lt;category>; &lt;category>; &lt;类别方案=“ http：http：http：http：http：http： //www.blogger.com/atom/ns#“ term =“ Machine Conception”>; &lt;/cattory>; &lt;title type =“ text”>; videoprism：视频理解的基础视觉编码&lt;/stitle>; &lt;content>; &lt;content type type =&#39; HTML“>; &lt;span class =” Byline-aTHOTOR“>;由Long Zhao，高级研究科学家Long Zhao和Google Spotch Software Engresser的Ting Liu发表&lt;/span>; &lt;img src =” com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;网络上提供了一个惊人的视频，涵盖了人们共享的日常时刻到历史时刻到科学观察的各种内容，每个观察都包含世界上独特的记录。正确的工具可以帮助研究人员分析这些视频，改变我们如何了解周围的世界。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;视频提供的动态视觉内容比静态图像更丰富，捕获移动，变化和实体之间的动态关系。分析这种复杂性，以及公开可公开的视频数据的巨大多样性，需要超越传统图像理解的模型。因此，许多在视频理解上表现最好的方法仍然依赖于针对特定任务量身定制的专业模型。最近，使用视频基础模型（VIFM）在该领域取得了令人兴奋的进步://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;, and &lt;a href=&quot;https: //arxiv.org/abs/2303.16058&quot;>; umt &lt;/a>;。但是，构建处理视频数据多样性的VIFM仍然是一个挑战。 &lt;/p>; &lt;p>;目的是建立一个用于通用视频理解的单个模型，我们介绍了“ &lt;a href=&quot;https://arxiv.org/abs/2402.13217&quot;>; videoprism：videoprism：一种基础视觉编码器：视频理解&lt;/a>;”。 Videoprism是一种VIFM，旨在处理各种视频理解任务，包括分类，本地化，检索，字幕和问题答案（QA）。我们在训练数据和建模策略中都提出了创新。我们在大量和多样化的数据集上预先培训视频：3600万个高质量的视频文本对和5.82亿个带有嘈杂或机器生成的并行文本的视频剪辑。我们的培训预培训方法是为这种混合数据设计的，可以从视频文本对和视频本身学习。摄影非常容易适应新的视频理解挑战，并使用单个冷冻模型来实现最先进的性能。 &lt;/p>; &lt;p>; &lt;/p>; &lt;video autoplay =“” loop =“”“ muted =”“ playsinline =”“ width =“ 100％”>; &lt;source src =“ https://github.com/github.com/garyzhao /videoprism-blog/raw/main/teaser.mp4“ type =“ video/mp4”>; &lt;/source>; &lt;/source>; &lt;/video>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr =” tr -caption-container“ style =”保证金左：自动右：auto;“>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center; center;通用视频编码器，可以通过从单个冷冻模型中产生视频表示形式来实现广泛的视频理解任务，包括分类，本地化，检索，字幕和问题答案。&lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>;预训练数据&lt;/h2>; &lt;p>;强大的VIFM需要大量的视频来训练，类似于其他视频基础模型（FMS），例如大型语言模型（LLMS）。理想情况下，我们希望预培训数据成为世界上所有视频的代表性样本。尽管这些视频中的大多数自然没有完美的字幕或描述，但即使是不完美的文本也可以提供有关视频语义内容的有用信息。 &lt;/p>; &lt;p>;为了给我们的模型提供最佳的起点，我们将大量的预训练语料库组成，该语料库由几个公共和私人数据集组成，包括&lt;a href =“ https://rowanzellers.com/merlot/ “>; yt-temoral-180m &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2307.06942&quot;>; internvid &lt;/a>;，&lt;a href =“ /2204.00679&quot;>; videocc &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2007.14937&quot;>; wts-70m &lt;/a>;等。 ，再加上另外5.82亿个片段，具有不同水平的嘈杂文字（例如自动生成的成绩单）。据我们所知，这是同类视频培训语料库最大，最多样化的视频培训语料库。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxssegrhfnm1r-xb3ztydwwc0m7zochlpiqulpiqury quly quly quly qullandtyttytytym Qulllandtyttyttyttyttyttytyty VEE1NU3GNRKJR7PE-YFAIVRC1AL-BXZECSOSO0AOJXFZSDHFV45OZOOBEYAA93IINECGDNURYH4HLC3W7QR2PXPX0PPX0FY6-4QFMTKMTKBORA_PFMTKBORA_PFHSPPPHSPPP7NRR1OW0 NRR1NRR1199999999999999999999999999999. 。 src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>; &lt;td class =“ tr-caption”样式=“ text-align：center;”>;视频文本预训练数据上的统计信息。 ＆nbsp; &lt;a href=&quot;https://arxiv.org/abs/2104.14806&quot;>;剪辑相似性得分&lt;/a>;＆nbsp;（越高，越高，越高）证明了我们的预训练的多样化字幕质量数据，这是用于收获文本的各种方法的副产品。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;h2>;两阶段训练&lt;/h2>; &lt;p>;视频抗流模型体系结构源于标准&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;视觉变压器&lt;/a>;（VIT），其分解设计，该设计依次编码&lt;href之后的空间和时间信息=“ https://arxiv.org/abs/2103.15691”>; vivit &lt;/a>;。我们的培训方法利用了上述嘈杂文本的高质量视频文本数据和视频数据。首先，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/self-supervise_learning#contrastive_self-supervise_learning&quot;>;对比度学习&lt;/a>;同时最大程度地提高负面视频文本对之间的距离），以教我们的模型与自己的文本说明（包括不完美的文本描述）匹配视频。这为将语义语言内容与视觉内容匹配的基础奠定了基础。 &lt;/p>; &lt;p>;在视频文本对比培训后，我们利用没有文本说明的视频收集。在这里，我们建立在&lt;a href=&quot;https://arxiv.org/abs/2212.04500&quot;>;掩盖视频建模框架&lt;/a>;以进行一些改进的视频中预测蒙面的补丁。我们训练该模型，以预测第一阶段模型的视频级全局嵌入和令牌嵌入，以有效利用该阶段中获得的知识。然后，我们将预测的令牌随机洗牌以防止模型学习快捷方式。 &lt;/p>; &lt;p>; Videoprism设置的独特之处在于，我们使用两个互补的预训练信号：文本说明和视频中的视觉内容。 Text descriptions often focus on what things look like, while the video content provides information about movement and visual dynamics.这使视频杂志能够在需要了解外观和运动的任务中表现出色。 &lt;/p>; &lt;br />; &lt;h2>;结果&lt;/h2>; &lt;p>;我们对视频理解任务的四个广泛类别的视频症进行了广泛的评估，包括视频分类和本地化，视频文本检索，视频字幕，问题回答问题和科学视频理解。 Videoprism在33个视频理解基准中的30个中实现了最先进的性能，这一切都以最小的单个冷冻模型适应。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999 /Image20.png“ ImageAnchor =“ 1”样式=“边距 - 左：自动右：自动;”>; &lt;img border =“ 0” data-Original-height =“ 1999”数据 - 1959&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png&quot; width=&quot;628 “/>;” td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;分类和本地化&lt;/h3>; &lt;p>;我们在现有的大规模视频理解基准（&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>; vievoglue &lt;/a>;）上评估视频概论。我们发现（1）录像带的表现优于所有其他最先进的FM，并且（2）没有其他单个模型一致地排在第二位。这告诉我们，视频杂志已经学会了有效地将各种视频信号包装成一个编码器 - 从不同的粒度语义到外观和运动提示 - 并且在各种视频源中都可以很好地运行。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：中心;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvz2xl/avvxsehnnyg_lnnynnynnynnynnynnynlnlfwdisjelqfwlkjleb1quzor4 xqubf_bknmym to lmccw3ek9qt6nn4lrvff45wu8j2ylcqi4hpe-rfowzmguv8ii6nq8hilemnrs1lmwcuohtvngs04dsxc7yvztamc7yvztamcu0srvumumuhhnnn4unuhhn4u94uikepnikepnikepnike/s1166 g8166.prrywow.prrywow.prrywowwo&#39;&#39;181818frywowwo&#39;1181818frywo&#39;11818frywo&#39;11816.1818frywo&#39; =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 742” data-Original-width =“ 1816” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ Text-Align：Center;”>; Videoprism优于最先进的方法（包括&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>; clip &lt;/a>;，&lt;a href =&#39; https://arxiv.org/abs/2104.11178&quot;>; vatt &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>; internvideo &lt;/a>; ：//arxiv.org/abs/2303.16058“>; umt &lt;/a>;）在&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;视频理解基准&lt;/a>;。在此图中，我们显示了与以前的最佳模型相比的绝对得分差异，以突出视频的相对改进。在&lt;a href=&quot;http://vuchallenge.org/charades.html&quot;>; charades &lt;/a>;，&lt;a href=&quot;http://activitive-net.org/&quot;>; activitynet &lt;/a>; href =“ https://research.google.com/ava/”>; ava &lt;/a>;和&lt;a href=&quot;https://research.google.com/ava/&quot;>; ava-k &lt;/a>; , we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision&quot;>;mean average precision&lt;/a>; (mAP) as the evaluation metric.在其他数据集上，我们报告Top-1精度。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;与LLMS &lt;/h3>; &lt;p>;结合使用，我们进一步探索了将视频与LLMS结合在一起，以解锁其处理各种视频语言任务的能力。特别是，当与文本编码器配对时（以下&lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;>; lit &lt;/a>;）或语言解码器（例如&lt;a href =” https：/https：/ /arxiv.org/abs/2305.10403&quot;>;PaLM-2&lt;/a>;), VideoPrism can be utilized for video-text retrieval, video captioning, and video QA tasks.我们比较了一组广泛而具有挑战性的视觉语言基准测试的组合模型。 Videoprism在大多数基准测试中设定了新的艺术状态。从视觉结果中，我们发现Videoprism能够理解视频中的复杂动作和外观（例如，模型可以在下面的视觉示例中识别窗口上旋转对象的不同颜色）。这些结果表明，录像带与语言模型非常兼容。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png “ style =”保证金左：自动右右：自动;”>; &lt;img border =“ 0” data-foriginal-height =“ 932” data-Original-width =“ 1028”高度=“ 580” SRC = “ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsejd7v86xym18_i3s0aemjiiyxajeiyxajeiyxajeiyxajeiyxajeiyxajeiyicq5kkkreicq5vkk3qnwtr96hkvssobsosobso qurin0 qurin0 qrin0f24Qrin0f23jprintimtt23jprintimtimtt3jprin 9UCV42RZXZ7CRKR21NUCR0ZWALKSUX9FXIBJWVMLQGB19Y5J8J8J8VT_ROKY4DB1SKUKKKKKKKBBC9FACCCCCCC6TC-XLUMHK5P65_UR与最新方法相比arxiv.org/abs/2212.04979&quot;>; videococa &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>; umt &lt;/a>; and umt &lt;/a>;和&lt;a a href =“ https：// arxiv。多个视频 - 文本检索（顶部）和视频字幕和视频QA（底部）基准测试的org/abs/2204.14198“>; flamingo &lt;/a>;）。我们还显示了与先前的最佳模型相比，绝对得分差异，以突出视频爆发的相对改进。我们在&lt;a href =“ https://www.microsoft.com/en-us/research/publication/msr-vtt-a-a-large-video-video-description-description-dataset-for-bridgor-bridging-video-video-video-video-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-video- and-language/“>; masrvtt &lt;/a>;，&lt;a href=&quot;https://eric--xw.github.io/vatex-website/index.html&quot;>; vatex &lt;/a>;，and &lt;a href =” https://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;>; activitivitivitynet &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>; a href =“ https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-video-description-datasetr-dataset-for-brid-bridgor-bridging-video-andlanguage/ cap &lt;/a>;，&lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>; vatex-cap &lt;/a>;和&lt;a href =“ youcook2.eecs.umich.edu/&quot;>; youucook2 &lt;/a>;，&lt;a href=&quot;https://github.com/xudejing/video-question-andwering&quot;>; msrvttt-qa &lt;/a>; and &lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSVD-QA&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/cmp-lg /9406033“>; wups索引&lt;/a>; on &lt;a href=&quot;https://doc-doc.github.io/docs/nextqa.html&quot;>; next-qa &lt;/a>;。&lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/tr >; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;video autoplay =“” loop =“”“ muted =”“ playSinline =”“ width =“ 100％”>; &lt;source src =“ https://github.com /garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4“ type =“ video/mp4”>; &lt;/source>; &lt;/source>; &lt;/video>; &lt;video autoplay =“ loop =”“ loop =”“ =“ 100％”>; &lt;源src =“ https://github.com/garyzhao/videoprism-blog/raw/main/main/spin_roller_skating.mp4” type =“ video/mp4”视频autoplay =“” loop =“” muted =“” playsInline =“” width =“ 100％”>; &lt;source src =“ https://github.com/garyzhao/garyzhao/garyoprism-blog/raw/raw/raw/main/making_ice_ice_ice_cream_cream_ski_ski_lifting.mp4 “ type =“ video/mp4”>; &lt;/source>; &lt;/video>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show qualitative results using VideoPrism with a text encoder for video-text retrieval (第一行），并适用于视频质量质量图（第二行和第三行）的语言解码器。 /tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;科学应用跨领域的科学家，包括伦理学，行为神经科学和生态学等领域。 caltech.edu/records/zrznw-w7386&quot;>; fly vs. fly &lt;/a>;，&lt;a href=&quot;https://data.calta.caltech.edu/records/s0vdx-0k302&quot;>; calms21 &lt;/a>; href =“ https://shirleymaxx.github.io/chimpact/”>; chimpact &lt;/a>;和&lt;a href=&quot;https://dirtmaxim.github.io/kabr/kabr//qubr/&quot;>; kabr &lt;/a>;。摄影不仅表现出色，而且实际上超过了专门为这些任务设计的模型。这表明诸如Videoprism之类的工具具有改变科学家在不同领域分析视频数据的潜力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/r29vz2xl/avvxsei3v-c36gwup8ckacvqfvqfvqfvvvvvvvvvvvvvvvvvvvvcco999999999999999999999999999999999999999999999999999. vjejxsqdb4zbgbkyazv-_i9qe5u7kus_z8qwlvqfzxx0jfelsdpfgj9v4qqhumwx_ekypm-vg7pdymxn0kj1 -s98izjl3u8cpvqoohyasuwxivt7m4_/s1200/image5.png“ style =” margin-left：auto; auto; &quot; height=&quot;397&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png&quot; width =“ 640”/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; videoprism优于各种科学基准的域专家。我们展示了绝对得分的差异，以突出视频的相对改进。我们报告了所有数据集的平均平均精度（MAP），但KABR除了使用类平均top-1精度的KABR。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;br />; &lt;h2>;结论&lt; /h2>; &lt;p>;使用视频，我们介绍了一个功能强大且通用的视频编码器，为通用视频理解设置了新标准。通过我们的广泛评估，我们强调建立大量和多样化的预培训数据集以及创新的建模技术。视频爆发不仅始终超过强大的基线，而且它可以使其概括地位的独特能力可以很好地解决一系列现实世界应用程序。由于其潜在的广泛使用，我们致力于在我们的&lt;a href=&quot;http://ai.google/principles&quot;>; AI原则的指导下继续在该领域继续进行进一步负责任的研究&lt;/a>;。我们希望Videoprism在AI和视频分析的交集中为未来的突破铺平了道路，从而有助于实现跨科学发现，教育和医疗保健等领域的VIFM的潜力。 &lt;/p>; &lt;br />; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;此博客文章是代表所有录像带作者发表的：Long Zhao，Nitesh B. Gundavarapu，Liangzhe Yuan，Hao Zhou，Hao Zhou，Shen，Shen Yan，Jennifer J. Sun，Luke Friedman，Rui Qian，Tobias Weyand，Yue Zhao，Rachel Hornung，Florian Schroff，Ming-Hsuan Yang，David A. Ross，Huisheng Wang，Hartwig Adam，Mikhail Sirotenko，Ting Liuu和Boqing Gong Gong Gong Gong Gong Gong Gong Gong Gong Gong Gog Gog Gong Gong Gong Gong Gong Gong Gong Gong Gong 。我们衷心感谢David Hendon的产品管理工作，Alex Siegman，Ramya Ganeshan和Victor Gomes的计划和资源管理工作。 We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill马克，阿尔沙·纳格拉尼，卡罗琳·潘托法鲁，苏珊特·普拉卡什，科迪莉亚·施密特，布莱恩·塞伯德，布莱恩·塞耶伯德，莫伊塔巴·塞耶德斯塞尼，阿曼达·萨德勒，里夫·A·萨鲁斯，雷切尔·斯蒂格勒，瑞秋·斯蒂格勒，雷切尔·斯蒂格勒，保罗·沃伊格（Paul voigtlaender）支持和反馈极大地为这项工作做出了贡献。我们感谢Jay Yagnik，Rahul Sukthankar和Tomas Izo对这个项目的热情支持。最后，我们感谢Tom Small，Jennifer J. Sun，Hao Zhou，Nitesh B. Gundavarapu，Luke Friedman和Mikhail Sirotenko在制作此博客文章方面提供了巨大帮助。&lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;/p &lt;/p >;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1695264277638670894/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href =“ http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html#comment-form” “/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/1695264264277638670894“ ://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/02/videoprism-foundational-visual-encoder.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPrism: A foundational visual encoder for video understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt; /name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http:/ /schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>; &lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif&quot; width=&quot;72&quot; xmlns:media =&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com, 1999:blog-8474926331452026626.post-4343235509909091741&lt;/id>;&lt;published>;2024-02-21T12:15:00.000-08:00&lt;/published>;&lt;updated>;2024-02-21T12:15:36.694-08:00&lt; /updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Gboard&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;category scheme =&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advances in private training for production on-device language models&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zheng Xu, Research Scientist, and Yanxiang Zhang, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Language models (LMs) trained to predict the next word given input text are the key technology for many applications [&lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;2&lt;/a>;]. In &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;Gboard&lt;/a>;, LMs are used to improve users&#39; typing experience by supporting features like &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;next word prediction&lt;/a>; (NWP), &lt;a href=&quot;https:// support.google.com/gboard/answer/7068415&quot;>;Smart Compose&lt;/a>;,&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>; smart completion&lt;/a>; and &lt; a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;suggestion&lt;/a>;, &lt;a href=&quot;https://support.google.com/gboard/answer/2811346&quot;>;slide to type&lt;/a>;&lt;span style=&quot;text-decoration: underline;&quot;>;,&lt;/span>; and &lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;proofread&lt;/一个>;。 Deploying models on users&#39; devices rather than enterprise servers has advantages like lower latency and better privacy for model usage. While training on-device models directly from user data effectively improves the utility performance for applications such as NWP and &lt;a href=&quot;https://blog.research.google/2021/11/predicting-text-selections-with.html&quot;>;smart text selection&lt;/a>;, protecting the privacy of user data for model training is important. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49 -Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/image45.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “1600”数据原始宽度=“1996”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXu OhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Gboard features powered by on-device language models.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In this blog we discuss how years of research advances now power the private training of Gboard LMs, since the proof-of-concept development of &lt;a href=&quot;https:// blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;federated learning&lt;/a>; (FL) in 2017 and formal &lt;a href=&quot;https://blog.research.google/2022/02 /federated-learning-with-formal.html&quot;>;differential privacy&lt;/a>; (DP) guarantees in 2022. &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative. html&quot;>;FL&lt;/a>; enables mobile phones to collaboratively learn a model while keeping all the training data on device, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;DP&lt;/a >; 提供数据匿名化的可量化衡量标准。 Formally, DP is often characterized by (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) with smaller values representing stronger guarantees. Machine learning (ML) models are considered to have &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable DP guarantees for ε=10 and strong DP guarantees for ε=1&lt;/a>; when &lt;em>;δ&lt;/em>; is small. &lt;/p>; &lt;p>; As of today, all NWP neural network LMs in Gboard are trained with FL with formal DP guarantees, and all future launches of Gboard LMs trained on user data require DP.这 30 多个 Gboard 设备上 LM 以 7 种以上语言和 15 多个国家/地区推出，并满足小 &lt;em>;δ&lt; 的 (&lt;em>;ɛ&lt;/em>;, &lt;em>;δ&lt;/em>;)-DP 保证/em>; of 10&lt;sup>;-10&lt;/sup>; and ɛ between 0.994 and 13.69. To the best of our knowledge, this is the largest known deployment of user-level DP in production at Google or anywhere, and the first time a strong DP guarantee of &lt;em>;ɛ&lt;/em>; &amp;lt; 1 is announced for models trained directly on user data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Privacy principles and practices in Gboard&lt;/h2>; &lt;p>; In “&lt;a href=&quot;https ://arxiv.org/abs/2306.14793&quot;>;Private Federated Learning in Gboard&lt;/a>;”, we discussed how different &lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; >;privacy principles&lt;/a>; are currently reflected in production models, including: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Transparency and user control&lt;/em>;: We provide disclosure of what data is used, what purpose it is used for, how it is processed in various channels, and how Gboard users can easily &lt;a href=&quot;https://support.google.com/gboard/answer/12373137&quot;>;configure&lt;/a>; the data usage in learning楷模。 &lt;/li>;&lt;li>;&lt;em>;Data minimization&lt;/em>;: FL immediately aggregates only focused updates that improve a specific model. &lt;a href=&quot;https://eprint.iacr.org/2017/281.pdf&quot;>;Secure aggregation&lt;/a>; (SecAgg) is an encryption method to further guarantee that only aggregated results of the ephemeral updates can be accessed. &lt;/li>;&lt;li>;&lt;em>;Data anonymization&lt;/em>;: DP is applied by the server to prevent models from memorizing the unique information in individual user&#39;s training data. &lt;/li>;&lt;li>;&lt;em>;Auditability and verifiability&lt;/em>;: We have made public the key algorithmic approaches and privacy accounting in open-sourced code (&lt;a href=&quot;https://github.com/tensorflow/ federated/blob/main/tensorflow_federated/python/aggregators/differential_privacy.py&quot;>;TFF aggregator&lt;/a>;, &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/dp_query /tree_aggregation_query.py&quot;>;TFP DPQuery&lt;/a>;、&lt;a href=&quot;https://github.com/google-research/federated/blob/master/dp_ftrl/blogpost_supplemental_privacy_accounting.ipynb&quot;>;DP 会计&lt;/a>;、 and &lt;a href=&quot;https://github.com/google/federated-compute&quot;>;FL system&lt;/a>;). &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A brief history&lt;/h3>; &lt;p>; In recent years, FL has become the default method for training &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;Gboard on-device LMs&lt;/a>; from user data. In 2020, a DP mechanism that &lt;a href=&quot;https://arxiv.org/abs/1710.06963&quot;>;clips and adds noise&lt;/a>; to model updates was used to &lt;a href=&quot;https://arxiv.org/abs/2009.10031&quot;>;prevent memorization&lt;/a>; for training the Spanish LM in Spain, which satisfies finite DP guarantees (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 3&lt;/a>; described in “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML“&lt;/a>; guide). In 2022, with the help of the &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-Follow-The-Regularized-Leader (DP-FTRL) algorithm&lt;/a>;, the Spanish LM became the first production neural network trained directly on user data announced with &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;a formal DP guarantee of (ε=8.9, δ=10&lt;sup>;-10&lt;/sup>;)-DP&lt;/a>; (equivalent to the reported &lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;ρ=0.81&lt;/a>;&lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1605.02065&quot;>;zero-Concentrated-Differential-Privacy&lt;/a>;), and therefore satisfies &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable privacy guarantees&lt;/a>; (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 2&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Differential privacy by default in federated learning &lt;/h2>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>;”, we announced that all the NWP neural network LMs in Gboard have DP guarantees, and all future launches of Gboard LMs trained on user data require DP guarantees. DP is enabled in FL by applying the following practices: &lt;/p>; &lt;ul>; &lt;li>;Pre-train the model with the &lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;>;multilingual&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;C4&lt;/a>; dataset. &lt;/li>;&lt;li>;Via simulation experiments on public datasets, find a large DP-noise-to-signal ratio that allows for high utility. Increasing the number of clients contributing to one round of model update improves privacy while keeping the noise ratio fixed for good utility, up to the point the DP target is met, or the maximum allowed by the system and the size of the population. &lt;/li>; &lt;li>;配置参数以限制每个客户可以根据计算预算和估计的人群&lt;a href =&#39;https://arxiv.org/abs/1902.0101046，限制每个客户可以贡献的频率（例如，每隔几天一次） &quot;>;the FL system&lt;/a>;. &lt;/li>;&lt;li>;运行&lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;训练，并限制通过&lt;a href选择的每台设备更新的幅度=&quot;https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e2b3a90e103368b6b6&quot;>;自适应裁剪&lt;/a>;，或根据经验进行修复。 &lt;/li>; &lt;/ul>; &lt;p>; SecAgg can be additionally applied by adopting the &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;advances in improving computation and communication for scales and sensitivity&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N /s1600/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1600&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Federated learning with differential privacy and (SecAgg).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reporting DP guarantees&lt;/h3>; &lt;p>; The DP guarantees of launched Gboard NWP LMs are visualized in the barplot below 。 The &lt;em>;x&lt;/em>;-axis shows LMs labeled by language-locale and trained on corresponding populations;当&lt;em>; y &lt;/em>;轴显示&lt;em>;ε&lt;/em>;值时，&lt;em>;Δ&lt;/em>;固定在&lt;&lt;&lt;em>;Δ a href=&quot;https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf&quot;>;(ε, δ)-DP&lt;/a>; (lower is better). The utility of these models are either significantly better than previous non-neural models in production, or comparable with previous LMs without DP, measured based on user-interactions metrics during A/B testing. For example, by applying the best practices, the DP guarantee of the Spanish model in Spain is improved from &lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;ε=8.9&lt;/a>;&lt;/em>; to &lt;em>;ε&lt;/em>;=5.37. SecAgg is additionally used for training the Spanish model in Spain and English model in the US. More details of the DP guarantees are reported in &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;the appendix &lt;/a>;following the &lt;a href=&quot;https://blog.research.google/ 2023/05/making-ml-models-differentially-private.html&quot;>;guidelines outlined&lt;/a>; in “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML&lt; /a>;”。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Towards stronger DP guarantees&lt;/h2>; &lt;p>; The &lt;em>;ε&lt;/em>;~10 DP guarantees of many launched LMs are already considered &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable&lt;/a>; for ML models in practice, while the journey of DP FL in Gboard continues for improving user typing experience while protecting data privacy. We are excited to announce that, for the first time, production LMs of Portuguese in Brazil and Spanish in Latin America are trained and launched with a DP guarantee of &lt;em>;ε&lt;/em>; ≤ 1, which satisfies &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 1 strong privacy guarantees&lt;/a>;. Specifically, the (&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP guarantee is achieved by running the advanced &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;Matrix Factorization DP-FTRL&lt;/a>; (MF-DP-FTRL) algorithm, with 12,000+ devices participating in every training round of server model update larger than the &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;common setting of 6500+ devices&lt;/a>;, and a carefully configured policy to restrict each client to at most participate twice in the total 2000 rounds of training in 14 days in the large Portuguese user population of Brazil. Using a similar setting, the es-US Spanish LM was trained in a large population combining multiple countries in Latin America to achieve (&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP. The &lt;em>;ε&lt;/em>; ≤ 1 es-US model significantly improved the utility in many countries, and launched in Colombia, Ecuador, Guatemala, Mexico, and Venezuela. For the smaller population in Spain, the DP guarantee of es-ES LM is improved from &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;ε=5.37&lt;/a>;&lt;/em>; to &lt;em>;ε&lt;/em>;=3.42 by only replacing &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>; without increasing the number of devices participating every round. More technical details are disclosed in the &lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/federated/blob/master/mf_dpftrl_matrices/privacy_accounting.ipynb&quot;>;colab&lt;/a>; for privacy会计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;709&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP guarantees for Gboard NWP LMs (the purple bar represents the first es-ES launch of ε=8.9; cyan bars represent privacy improvements for models trained with &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>;; &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;tiers &lt;/a>;are from “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML&lt;/a>;“ guide; en-US* and es-ES* are additionally trained with SecAgg).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Discussion and next steps&lt;/h2>; &lt;p>; Our experience suggests that DP can be achieved in practice through system algorithm co-design on client participation, and that both privacy and utility can be strong when populations are large &lt;em>;and&lt;/em>; a large number of devices&#39; contributions are aggregated. Privacy-utility-computation trade-offs can be improved by &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;using public data&lt;/a>;, the &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;new MF-DP-FTRL algorithm&lt;/a>;, &lt;a href=&quot;https://github.com/google/differential-privacy&quot;>;and tightening accounting&lt;/a>;. With these techniques, a strong DP guarantee of &lt;em>;ε&lt;/em>; ≤ 1 is possible but still challenging. Active research on empirical privacy auditing [&lt;a href=&quot;https://arxiv.org/abs/2302.03098&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2305.08846&quot;>;2&lt;/a>;] suggests that DP models are potentially more private than the worst-case DP guarantees imply. While we keep pushing the frontier of algorithms, which dimension of privacy-utility-computation should be prioritized? &lt;/p>; &lt;p>; We are actively working on all privacy aspects of ML, including extending DP-FTRL to &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;distributed DP&lt;/a>; and improving &lt;a href=&quot;https://arxiv.org/abs/2306.14793&quot;>;auditability and verifiability&lt;/a>;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;Trusted Execution Environment&lt;/a>; opens the opportunity for substantially increasing the model size with verifiable privacy. The recent &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;breakthrough in large LMs&lt;/a>; (LLMs) motivates us to &lt;a href=&quot;https://arxiv.org/abs/2305.12132&quot;>;rethink&lt;/a>; the usage of &lt;a href=&quot;https://arxiv.org/abs/2212.06470&quot;>;public&lt;/a>; information in private training and more future interactions between LLMs, on-device LMs, and Gboard production. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;作者衷心感谢 Peter Kairouz、Brendan McMahan, and Daniel Ramage for their early feedback on the blog post itself, Shaofeng Li and Tom Small for helping with the animated figures, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The collaborators below directly contribute to the presented results:&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Research and algorithm development: Galen Andrew, Stanislav Chiknavaryan, Christopher A. Choquette-Choo, Arun Ganesh, Peter Kairouz, Ryan McKenna, H. Brendan McMahan, Jesse Rosenstock, Timon Van Overveldt, Keith Rush, Shuang Song, Thomas Steinke, Abhradeep Guha Thakurta, Om Thakkar, and Yuanbo Zhang.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Infrastructure, production and leadership support: Mingqing Chen, Stefan Dierauf, Billy Dou, Hubert Eichner, Zachary Garrett, Jeremy Gillula, Jianpeng Hou, Hui Li, Xu Liu, Wenzhi Mao, Brett McLarnon, Mengchen Pei, Daniel Ramage, Swaroop Ramaswamy, Haicheng Sun, Andreas Terzis, Yun Wang, Shanshan Wu, Yu Xiao, and Shumin Zhai.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4343235509909091741/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;Advances in private training for production on-device language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025&lt;/id>;&lt;published>;2024-02-14T10:32:00.000-08:00&lt;/published>;&lt;updated>;2024-02-14T10:32:25.557-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Learning the importance of training data under concept drift&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Nishant Jain, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s1600/temporalreweightinghero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The constantly changing nature of the world around us poses a significant challenge for the development of AI models. Often, models are trained on longitudinal data with the hope that the training data used will accurately represent inputs the model may receive in the future. More generally, the default assumption that all training data are equally relevant often breaks in practice. For example, the figure below shows images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; nonstationary learning benchmark, and it illustrates how visual features of objects evolve significantly over a 10 year span (a phenomenon we refer to as &lt;em>;slow concept drift&lt;/em>;), posing a challenge for object categorization models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw -_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= &quot;662&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png&quot; />;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Sample images from the CLEAR benchmark. (Adapted from Lin et al&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;.&lt;/a>;)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Alternative approaches, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Online_machine_learning&quot;>;online&lt;/a>; and &lt;a href=&quot;https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning&quot;>;continual learning&lt;/a>;, repeatedly update a model with small amounts of recent data in order to keep it current. This implicitly prioritizes recent data, as the learnings from past data are gradually erased by subsequent updates. However in the real world, different kinds of information lose relevance at different rates, so there are two key issues: 1) By design they focus &lt;em>;exclusively&lt;/em>; on the most recent data and lose any signal from older data that is erased. 2) Contributions from data instances decay &lt;em>;uniformly over time&lt;/em>; irrespective of the contents of the data. &lt;/p>; &lt;p>; In our recent work, “&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;Instance-Conditional Timescales of Decay for Non-Stationary Learning&lt;/a>;”, we propose to assign each instance an importance score during training in order to maximize model performance on future data. To accomplish this, we employ an auxiliary model that produces these scores using the training instance as well as its age. This model is jointly learned with the primary model. We address both the above challenges and achieve significant gains over other robust learning methods on a range of benchmark datasets for nonstationary learning. For instance, on a &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;recent large-scale benchmark&lt;/a>; for nonstationary learning (~39M photos over a 10 year period), we show up to 15% relative accuracy gains through learned reweighting of training data. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;The challenge of concept drift for supervised learning&lt;/h2>; &lt;p>; To gain quantitative insight into slow concept drift, we built classifiers on a &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;recent photo categorization task&lt;/a>;, comprising roughly 39M photographs sourced from social media websites over a 10 year period.我们比较了离线培训，该培训在所有训练数据中以随机顺序进行了多次迭代，并持续培训，这些培训在每个月的数据中以顺序（时间）顺序进行了多次迭代。 We measured model accuracy both during the training period and during a subsequent period where both models were frozen, ie, not updated further on new data (shown below). At the end of the training period (left panel, x-axis = 0), both approaches have seen the same amount of data, but show a large performance gap. This is due to &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368&quot;>;catastrophic forgetting&lt;/a>;, a problem in continual learning where a model&#39;s knowledge of data from early on in the training sequence is diminished in an uncontrolled manner. On the other hand, forgetting has its advantages — over the test period (shown on the right), the continual trained model degrades much less rapidly than the offline model because it is less dependent on older data. The decay of both models&#39; accuracy in the test period is confirmation that the data is indeed evolving over time, and both models become increasingly less relevant. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1554&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparing offline and continually trained models on the photo classification task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt; div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-sensitive reweighting of training data&lt;/h2>; &lt;p>; We design a method combining the benefits of offline learning (the flexibility of effectively reusing all available data) and continual learning (the ability to downplay older data) to address slow concept drift. We build upon offline learning, then add careful control over the influence of past data and an optimization objective, both designed to reduce model decay in the future. &lt;/p>; &lt;p>; Suppose we wish to train a model, &lt;em>;M&lt;/em>;,&lt;em>; &lt;/em>;given some training data collected over time. We propose to also train a helper model that assigns a weight to each point based on its contents and age. This weight scales the contribution from that data point in the training objective for &lt;em>;M&lt;/em>;. The objective of the weights is to improve the performance of &lt;em>;M&lt;/em>; on future data. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;our work&lt;/a>;, we describe how the helper model can be &lt;em>;meta-learned, &lt;/em>;ie, learned alongside &lt;em>;M&lt;/em>; in a manner that helps the learning of the model &lt;em>;M&lt;/em>; itself. A key design choice of the helper model is that we separated out instance- and age-related contributions in a factored manner. Specifically, we set the weight by combining contributions from multiple different fixed timescales of decay, and learn an approximate “assignment” of a given instance to its most suited timescales. We find in our experiments that this form of the helper model outperforms many other alternatives we considered, ranging from unconstrained joint functions to a single timescale of decay (exponential or linear), due to its combination of simplicity and expressivity. Full details may be found in the &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Instance weight scoring&lt;/h2>; &lt;p>; The top figure below shows that our learned helper model indeed up-weights more modern-looking objects in the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR object recognition challenge&lt;/a>;; older-looking objects are correspondingly down-weighted. On closer examination (bottom figure below, gradient-based &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;feature importance&lt;/a>; assessment), we see that the helper model focuses on the primary object within the image, as opposed to, eg, background features that may spuriously be correlated with instance age. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Sample images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; benchmark (camera &amp;amp; computer categories) assigned the highest and lowest weights respectively by our helper model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 &quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin -right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;Feature importance analysis of our helper model on sample images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; benchmark.&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Gains on large-scale data &lt;/h3>; &lt;p>; We first study the large-scale &lt;a href=&quot;https://arxiv.org/abs/ 2108.09020&quot;>;photo categorization task&lt;/a>; (PCAT) on the &lt;a href=&quot;https://arxiv.org/abs/1503.01817&quot;>;YFCC100M dataset&lt;/a>; discussed earlier, using the first five years of data for training and the next five years as test data. Our method (shown in red below) improves substantially over the no-reweighting baseline (black) as well as many other robust learning techniques. Interestingly, our method deliberately trades off accuracy on the distant past (training data unlikely to reoccur in the future) in exchange for marked improvements in the test period. Also, as desired, our method degrades less than other baselines in the test period. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;Comparison of our method and relevant baselines on the PCAT dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h3>;Broad applicability&lt;/h3>; &lt;p>; We validated our findings on a wide range of nonstationary learning challenge datasets sourced from the academic literature (see &lt;a href=&quot;https://arxiv .org/abs/2108.09020&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;2&lt;/a>;, &lt;a href=&quot;https://arxiv.org /abs/2211.14238&quot;>;3&lt;/a>;, &lt;a href=&quot;https://proceedings.mlr.press/v206/awasthi23b/awasthi23b.pdf&quot;>;4&lt;/a>; for details) that spans data sources and modalities (photos, satellite images, social media text, medical records, sensor readings, tabular data) and sizes (ranging from 10k to 39M instances). We report significant gains in the test period when compared to the nearest published benchmark method for each dataset (shown below). Note that the previous best-known method may be different for each dataset. These results showcase the broad applicability of our approach. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s765/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;765&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= &quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance gain of our method on a variety of tasks studying natural concept drift. Our reported gains are over the previous best-known method for each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Extensions to continual learning&lt;/h3>; &lt;p>; Finally, we consider an interesting extension of our work. The work above described how offline learning can be extended to handle concept drift using ideas inspired by continual learning.但是，有时离线学习是不可行的 - 例如，如果可用的培训数据数量太大而无法维护或处理。 We adapted our approach to continual learning in a straightforward manner by applying temporal reweighting &lt;em>;within the context of &lt;/em>;each bucket of data being used to sequentially update the model. This proposal still retains some limitations of continual learning, eg, model updates are performed only on most-recent data, and all optimization decisions (including our reweighting) are only made over that data. Nevertheless, our approach consistently beats regular continual learning as well as a wide range of other continual learning algorithms on the photo categorization benchmark (see below). Since our approach is complementary to the ideas in many baselines compared here, we anticipate even larger gains when combined with them. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Results of our method adapted to continual learning, compared to the latest baselines.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We addressed the challenge of data drift in learning by combining the strengths of previous approaches — offline learning with its effective reuse of data, and continual learning with its emphasis on more recent data.我们希望我们的工作有助于改善实践中概念漂移的模型鲁棒性，并在解决缓慢的概念漂移的无处不在问题方面产生越来越多的兴趣和新想法。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;我们感谢迈克·莫泽尔（Mike Mozer）在早期的早期进行了许多有趣的讨论这项工作的阶段，以及在开发过程中非常有用的建议和反馈。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/56059333333329926261025/comments/默认值“ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/learning-importance-of-raining-ogning-importance-of-training -data.html＃comment-form“ rel =” reply =“ title =“ 0 comment” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/847492626331452026262626/posts/ default/5605933033299261025&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;self&quot; type =“ application/application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/02/learning-importance-of-training-data.hta.html” rel =“替代”在概念漂移下学习培训数据的重要性“ type =” text/html“/>; &lt;uname>; &lt;名称>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri &lt;/uri &lt;/uri >;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1. blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr：thr>; 0 &lt;/thr：total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999年：Blog-8474926331452026626.POST-5933333365460125094774 &lt;/id>; 11:00.000-08:00&lt;/published>;&lt;updated>;2024-02-13T14:11:49.258-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns# “ term =“差异隐私”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ pancys ai”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http：http：http：http：http：http：http：http：http：http： //www.blogger.com/atom/ns#“ term =“安全与隐私”>; &lt;/category>; &lt;title type =“ text”>; dp-auditorium：一个灵活的库库，用于审核差异隐私&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由MónicaRiberoDíaz发表，研究科学家，Google Research &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img/b/b/ R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/differential_privacy&quot;>;差异隐私&lt;/a>;（DP）是限制任何个人用户信息的随机机制的属性处理和分析数据。 DP提供了一种强大的解决方案，以解决对数据保护的日益关注，启用技术&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-learning-with-formal.html&quot;>;跨越&lt;/a>; &lt;a href=&quot;https://www.apple.com/privacy/docs/differential_privacy_overview.pdf&quot;>; Industries &lt;/a>;和政府应用程序（例如程序 - 策略/年十年级/十年/2020/计划管理/process/procutsion-discluse-avidence/dindialial-privacy.html“>;美国人口普查&lt;/a>;），而不会损害个人用户身份。随着其采用率的增加，重要的是要确定具有错误实施的机制的潜在风险。研究人员最近发现了私人机制的数学证据及其实施的错误。例如，&lt;a href=&quot;https://arxiv.org/pdf/1603.01699.pdf&quot;>;研究人员比较&lt;/a>;六个稀疏矢量技术（SVT）变化，发现六个实际上只遇到了六个的私密性保证。即使数学证明是正确的，实现该机制的代码也容易受到人为错误的影响。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;但是，实际和有效的DP审核主要是由于机制的固有随机性以及测试保证的概率性质。此外，存在一系列保证类型，（例如，&lt;a href=&quot;https://dl.acm.org/doi/10.1007/11681878_14&quot;>; pure dp &lt;/a>;，&lt;a href =“ /link.springer.com/chapter/10.1007/11761679_29&quot;>; approximate dp &lt;/a>;，&lt;a href=&quot;https://arxiv.org/arxiv.org/abs/1702.07476&quot;>; =&quot;https://arxiv.org/pdf/1603.01887.pdf&quot;>;concentrated DP&lt;/a>;), and this diversity contributes to the complexity of formulating the auditing problem. Further, debugging mathematical proofs and code bases is an intractable task given the volume of proposed mechanisms. While &lt;em>;ad hoc&lt;/em>; testing techniques exist under specific assumptions of mechanisms, few efforts have been made to develop an extensible tool for testing DP mechanisms. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;DP-Auditorium: A Large Scale Library for Auditing Differential Privacy&lt;/a>;”, we introduce an &lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/python/dp_auditorium&quot;>;open source library&lt;/a>; for auditing DP guarantees with only black-box access to a mechanism (ie, without any knowledge of the mechanism&#39;s internal properties). DP-Auditorium is implemented in Python and provides a flexible interface that allows contributions to continuously improve its testing capabilities. We also introduce new testing algorithms that perform divergence optimization over function spaces for Rényi DP, pure DP, and approximate DP. We demonstrate that DP-Auditorium can efficiently identify DP guarantee violations, and suggest which tests are most suitable for detecting particular bugs under various privacy guarantees. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP guarantees&lt;/h2>; &lt;p>; The output of a DP mechanism is a sample drawn from a probability distribution (&lt;em>;M&lt;/em>; (&lt;em>;D&lt;/em>;)) that satisfies a mathematical property ensuring the privacy of user data. A DP guarantee is thus tightly related to properties between pairs of probability distributions. A mechanism is differentially private if the probability distributions determined by &lt;i>;M&lt;/i>; on dataset &lt;em>;D&lt;/em>; and a neighboring dataset &lt;em>;D&#39;&lt;/em>;, which differ by only one record, are &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_indistinguishability&quot;>;indistinguishable&lt;/a>;&lt;/em>; under a given divergence metric. &lt;/p>; &lt;p>; For example, the classical &lt;a href=&quot;https://software.imdea.org/~federico/pubs/2013.ICALP.pdf&quot;>;approximate DP&lt;/a>; definition states that a mechanism is approximately DP with parameters (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) if the &lt;a href=&quot;https://arxiv.org/pdf/1508.00335.pdf&quot;>;hockey-stick divergence&lt;/a>; of order &lt;em>;e&lt;sup>;ε&lt;/sup>;&lt;/em>;, between &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;), is at most &lt;em>;δ&lt;/em>;. Pure DP is a special instance of approximate DP where &lt;em>;δ = 0&lt;/em>;. Finally, a mechanism is considered &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>; with parameters (&lt;em>;𝛼&lt;/em>;, &lt;em>;ε)&lt;/em>; if the &lt;a href=&quot;https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy&quot;>;Rényi divergence&lt;/a>; of order &lt;em>;𝛼&lt;/em>;, is at most &lt;em>;ε&lt;/em>; (where &lt;em>;ε&lt;/em>; is a small positive value). In these three definitions, &lt;em>;ε &lt;/em>;is not interchangeable but intuitively conveys the same concept; larger values of &lt;em>;ε&lt;/em>; imply larger divergences between the two distributions or less privacy, since the two distributions are easier to distinguish. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-Auditorium&lt;/h2>; &lt;p>; DP-Auditorium comprises two main components: property testers and dataset finders. Property testers take samples from a mechanism evaluated on specific datasets as input and aim to identify privacy guarantee violations in the provided datasets. Dataset finders suggest datasets where the privacy guarantee may fail. By combining both components, DP-Auditorium enables (1) automated testing of diverse mechanisms and privacy definitions and, (2) detection of bugs in privacy-preserving mechanisms. We implement various private and non-private mechanisms, including simple mechanisms that compute the mean of records and more complex mechanisms, such as different SVT and &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;gradient descent&lt;/a>; mechanism variants. &lt;/p>; &lt;p>; &lt;strong>;Property testers&lt;/strong>; determine if evidence exists to reject the hypothesis that a given divergence between two probability distributions, &lt;em>;P&lt;/em>; and &lt;em>;Q&lt;/em>;, is bounded by a prespecified budget determined by the DP guarantee being tested. They compute a lower bound from samples from &lt;em>;P&lt;/em>; and &lt;em>;Q,&lt;/em>; rejecting the property if the lower bound value exceeds the expected divergence. No guarantees are provided if the result is indeed bounded. To test for a range of privacy guarantees, DP-Auditorium introduces three novel testers: (1) HockeyStickPropertyTester, (2) RényiPropertyTester, and (3) MMDPropertyTester. Unlike other approaches, these testers don&#39;t depend on explicit histogram approximations of the tested distributions. They rely on variational representations of the hockey-stick divergence, Rényi divergence, and &lt;a href=&quot;https://jmlr.csail.mit.edu/papers/v13/gretton12a.html&quot;>;maximum mean discrepancy&lt;/a>; (MMD) that enable the estimation of divergences through optimization over function spaces. As a baseline, we implement &lt;a href=&quot;https://arxiv.org/abs/1806.06427&quot;>;HistogramPropertyTester&lt;/a>;, a commonly used approximate DP tester. While our three testers follow a similar approach, for brevity, we focus on the HockeyStickPropertyTester in this post. &lt;/p>; &lt;p>; Given two neighboring datasets, &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>;, the HockeyStickPropertyTester finds a lower bound,&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp;for the hockey-stick divergence between &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) that holds with high probability. Hockey-stick divergence enforces that the two distributions &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) are close under an approximate DP guarantee. Therefore, if a privacy guarantee claims that the hockey-stick divergence is at most &lt;em>;δ&lt;/em>;, and&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; >; &lt;em>;δ&lt;/em>;, then with high probability the divergence is higher than what was promised on &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>; and the mechanism cannot satisfy the given approximate DP guarantee 。 The lower bound&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; is computed as an empirical and tractable counterpart of a variational formulation of the hockey-stick divergence (see &lt;a href=&quot;https://arxiv.org/pdf/2307.05608.pdf&quot;>;the paper&lt;/a>; for more details) 。 The accuracy of&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; increases with the number of samples drawn from the mechanism, but decreases as the variational formulation is simplified. We balance these factors in order to ensure that&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp; is both accurate and easy to compute. &lt;/p>; &lt;p>; &lt;strong>;Dataset finders&lt;/strong>; use &lt;a href=&quot;https://arxiv.org/pdf/2207.13676.pdf&quot;>;black-box optimization&lt;/a>; to find datasets &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>; that maximize&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;, a lower bound on the divergence value &lt;em>;δ&lt;/em>;. Note that black-box optimization techniques are specifically designed for settings where deriving gradients for an objective function may be impractical or even impossible. These optimization techniques oscillate between exploration and exploitation phases to estimate the shape of the objective function and predict areas where the objective can have optimal values. In contrast, a full exploration algorithm, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&quot;>;grid search method&lt;/a>;, searches over the full space of neighboring datasets &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>;. DP-Auditorium implements different dataset finders through the open sourced black-box optimization library &lt;a href=&quot;https://github.com/google/vizier&quot;>;Vizier&lt;/a>;. &lt;/p>; &lt;p>; Running existing components on a new mechanism only requires defining the mechanism as a Python function that takes an array of data &lt;em>;D&lt;/em>; and a desired number of samples &lt;em>;n&lt;/em>; to be output by the mechanism computed on &lt;em>;D&lt;/em>;. In addition, we provide flexible wrappers for testers and dataset finders that allow practitioners to implement their own testing and dataset search algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key results&lt;/h2>; &lt;p>; We assess the effectiveness of DP-Auditorium on five private and nine non-private mechanisms with diverse output spaces. For each property tester, we repeat the test ten times on fixed datasets using different values of &lt;em>;ε&lt;/em>;, and report the number of times each tester identifies privacy bugs. While no tester consistently outperforms the others, we identify bugs that would be missed by previous techniques (HistogramPropertyTester). Note that the HistogramPropertyTester is not applicable to SVT mechanisms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s785/image22.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;409&quot; data-original-width=&quot;785&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s16000/image22.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Number of times each property tester finds the privacy violation for the tested non-private mechanisms. NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Laplace_Mechanism&quot;>;Laplace&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Gaussian_Mechanism&quot;>;Gaussian&lt;/a>; mechanisms for computing the mean.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also analyze the implementation of a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py&quot;>;DP gradient descent algorithm&lt;/a>; (DP-GD) in TensorFlow that computes gradients of the loss function on private data. To preserve privacy, DP-GD employs a clipping mechanism to bound the &lt;a href=&quot;https://mathworld.wolfram.com/L2-Norm.html&quot;>;l2-norm&lt;/a>; of the gradients by a value &lt;em>;G&lt;/em>;, followed by the addition of Gaussian noise. This implementation incorrectly assumes that the noise added has a scale of &lt;em>;G&lt;/em>;, while in reality, the scale is &lt;em>;sG&lt;/em>;, where &lt;em>;s&lt;/em>; is a positive scalar 。 This discrepancy leads to an approximate DP guarantee that holds only for values of &lt;em>;s&lt;/em>; greater than or equal to 1. &lt;/p>; &lt;p>; We evaluate the effectiveness of property testers in detecting this bug and show that HockeyStickPropertyTester and RényiPropertyTester exhibit superior performance in identifying privacy violations, outperforming MMDPropertyTester and HistogramPropertyTester. Notably, these testers detect the bug even for values of &lt;em>;s&lt;/em>; as high as 0.6. It is worth highlighting that &lt;em>;s &lt;/em>;= 0.5 corresponds to a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/308cbda4db6ccad5d1e7d56248727274e4c0c79e/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py#L445C1-L446C1&quot;>;common error&lt;/a>; in literature that involves missing a factor of two when accounting for the privacy budget &lt;em>;ε&lt;/em>;. DP-Auditorium successfully captures this bug as shown below. For more details see section 5.6 &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;here&lt;/a>;. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;836&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Estimated divergences and test thresholds for different values of &lt;em>;s&lt;/em>; when testing DP-GD with the HistogramPropertyTester (&lt;strong>;left&lt;/strong>;) and the HockeyStickPropertyTester (&lt;strong>;right&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s828/image20.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;333&quot; data-original-width=&quot;828&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s16000/image20.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Estimated divergences and test thresholds for different values of &lt;em>;s&lt;/em>; when testing DP-GD with the RényiPropertyTester (&lt;strong>;left&lt;/strong>;) and the MMDPropertyTester (&lt;strong>;right&lt;/strong>;)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To test dataset finders, we compute the number of datasets explored before finding a privacy violation. On average, the majority of bugs are discovered in less than 10 calls to dataset finders. Randomized and exploration/exploitation methods are more efficient at finding datasets than grid search. For more details, see the &lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DP is one of the most powerful frameworks for data protection. However, proper implementation of DP mechanisms can be challenging and prone to errors that cannot be easily detected using traditional unit testing methods. A unified testing framework can help auditors, regulators, and academics ensure that private mechanisms are indeed private. &lt;/p>; &lt;p>; DP-Auditorium is a new approach to testing DP via divergence optimization over function spaces. Our results show that this type of function-based estimation consistently outperforms previous black-box access testers. Finally, we demonstrate that these function-based estimators allow for a better discovery rate of privacy bugs compared to histogram estimation. By &lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/python/dp_auditorium&quot;>;open sourcing&lt;/a>; DP-Auditorium, we aim to establish a standard for end-to-end testing of new differentially private algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described here was done jointly with Andrés Muñoz Medina, William Kong and Umar Syed. We thank Chris Dibak and Vadym Doroshenko for helpful engineering support and interface suggestions for our library.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5933365460125094774/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html&quot; rel=&quot;alternate&quot; title=&quot;DP-Auditorium: A flexible library for auditing differential privacy&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3264694155710647310&lt;/id>;&lt;published>;2024-02-06T11:17:00.000-08:00&lt;/published>;&lt;updated>;2024-02-06T11:17:53.968-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graph Mining&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Graph neural networks in TensorFlow&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s1600/TFGNN%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks 。 Discrete mathematics and computer science have a long history of formalizing such networks as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graphs&lt;/a>;&lt;/em>;, consisting of &lt;em>;nodes&lt;/em>; connected by &lt;em>;edges&lt;/em>; in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;Graph neural networks&lt;/a>;, or GNNs for short, have emerged as a powerful technique to leverage both the graph&#39;s connectivity (as in the older algorithms &lt;a href=&quot;http://perozzi.net/projects/deepwalk/&quot;>;DeepWalk&lt;/a>; and &lt;a href=&quot;https://snap.stanford.edu/node2vec/&quot;>;Node2Vec&lt;/a>;) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What&#39;s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph&#39;s &lt;em>;discrete&lt;/em>;, &lt;em>;relational&lt;/em>; information in a &lt;em>;continuous&lt;/em>; way so that it can be included naturally in another deep learning system. &lt;/p>; &lt;p>; We are excited to announce the release of &lt;a href=&quot;https://github.com/tensorflow/gnn&quot;>;TensorFlow GNN 1.0&lt;/a>; (TF-GNN), a production-tested library for building GNNs at large scales. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs, where types of objects and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types, and TF-GNN&#39;s heterogeneous focus makes it natural to represent them. &lt;/p>; &lt;p>; Inside TensorFlow, such graphs are represented by objects of type &lt;code>;tfgnn.GraphTensor&lt;/code>;. This is a composite tensor type (a collection of tensors in one Python class) accepted as a &lt;a href=&quot;https://en.wikipedia.org/wiki/First-class_citizen&quot;>;first-class citizen&lt;/a>; in &lt;code>;tf.data.Dataset&lt;/code>;, &lt;code>;tf.function&lt;/code>;, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level &lt;a href=&quot;https://www.tensorflow.org/guide/keras&quot;>;Keras API&lt;/a>;, or directly using the &lt;code>;tfgnn.GraphTensor&lt;/code>; primitive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;GNNs: Making predictions for an object in context&lt;/h2>; &lt;p>; For illustration, let&#39;s look at one typical application of TF-GNN: predicting a property of a certain type of node in a graph defined by cross-referencing tables of a huge database. For example, a citation database of Computer Science (CS) arXiv papers with one-to-many cites and many-to-one cited relationships where we would like to predict the subject area of each paper. &lt;/p>; &lt;p>; Like most neural networks, a GNN is trained on a dataset of many labeled examples (~millions), but each training step consists only of a much smaller batch of training examples (say, hundreds). To scale to millions, the GNN gets trained on a stream of reasonably small subgraphs from the underlying graph. Each subgraph contains enough of the original data to compute the GNN result for the labeled node at its center and train the model. This process — typically referred to as subgraph sampling — is extremely consequential for GNN training. Most existing tooling accomplishes sampling in a batch way, producing static subgraphs for training. TF-GNN provides tooling to improve on this by sampling dynamically and interactively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Pictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; TF-GNN 1.0 debuts a flexible Python API to configure dynamic or batch subgraph sampling at all relevant scales: interactively in a Colab notebook (like &lt;a href=&quot;https://colab.research.google.com/github /tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;this one&lt;/a>;), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by &lt;a href =&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md&quot;>;in-memory&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md&quot;>;beam-based&lt;/a>; sampling, respectively. &lt;/p>; &lt;p>; On those same sampled subgraphs, the GNN&#39;s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node&#39;s neighborhood. One classical approach is &lt;a href=&quot;https://research.google/pubs/neural-message-passing-for-quantum-chemistry/&quot;>;message-passing neural networks&lt;/a>;. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After &lt;em>;n&lt;/em>; rounds, the hidden state of the root node reflects the aggregate information from all nodes within &lt;em>;n&lt;/em>; edges (pictured below for &lt;em>;n&lt;/em>; = 2) 。 The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;511&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The training setup is completed by placing an output layer on top of the GNN&#39;s hidden state for the labeled nodes, computing the &lt;em>;loss &lt;/em>;(to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training. &lt;/p>; &lt;p>; Beyond supervised training (ie, minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (ie, without labels). This lets us compute a &lt;em>;continuous&lt;/em>; representation (or &lt;em>;embedding&lt;/em>;) of the &lt;em>;discrete&lt;/em>; graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building GNN architectures&lt;/h2>; &lt;p>; The TF-GNN library supports building and training GNNs at various levels of abstraction. &lt;/p>; &lt;p>; At the highest level, users can take any of the predefined models bundled with the library that are expressed in Keras layers. Besides a small collection of models from the research literature, TF-GNN comes with a highly configurable model template that provides a curated selection of modeling choices that we have found to provide strong baselines on many of our in-house problems. The templates implement GNN layers; users need only to initialize the Keras layers. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;865&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s16000/TFGNN%20code1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; At the lowest level, users can write a GNN model from scratch in terms of primitives for passing data around the graph, such as broadcasting data from a node to all its outgoing edges or pooling data into a node from all its incoming edges (eg, computing the sum of incoming messages). TF-GNN&#39;s graph data model treats nodes, edges and whole input graphs equally when it comes to features or hidden states, making it straightforward to express not only node-centric models like the MPNN discussed above but also more general forms of &lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;GraphNets&lt;/a>;. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md&quot;>;user guide&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models&quot;>;model collection&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training orchestration&lt;/h2>; &lt;p>; While advanced users are free to do custom model training, the &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md&quot;>;TF-GNN Runner&lt;/a>; also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s1400/TFGNN%20code2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;508&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s16000/TFGNN%20code2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The Runner provides ready-to-use solutions for ML pains like distributed training and &lt;code>;tfgnn.GraphTensor&lt;/code>; padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;392&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s16000/TFGNN%20code3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Additionally, the TF-GNN Runner also includes an implementation of &lt;a href=&quot;https://www.tensorflow.org/tutorials/interpretability/integrated_gradients&quot;>;integrated gradients&lt;/a>; for use in model attribution. Integrated gradients output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In short, we hope TF-GNN will be useful to advance the application of GNNs in TensorFlow at scale and fuel further innovation in the field. If you&#39;re curious to find out more, please try our &lt;a href=&quot;https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;Colab demo&lt;/a>; with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/overview.md&quot;>;user guides and Colabs&lt;/a>;, or take a look at our &lt;a href=&quot;https://arxiv.org/abs/2207.03522&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The TF-GNN release 1.0 was developed by a collaboration between Google Research: Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle, Google Core ML: Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe, and Google DeepMind:&lt;strong>; &lt;/strong>;Alvaro Sanchez-Gonzalez and Lisa Wang.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3264694155710647310/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html&quot; rel=&quot;alternate&quot; title=&quot;Graph neural networks in TensorFlow&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6956767920612914706&lt;/id>;&lt;published>;2024-02-02T11:07:00.000-08:00&lt;/published>;&lt;updated>;2024-02-07T16:05:00.722-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A decoder-only foundation model for time-series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Yichen Zhou, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time-series&lt;/a>; forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural科学。 In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&quot;>;improving demand forecasting accuracy&lt;/a>; can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (eg, DL models performed well in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0169207021001874&quot;>;M5 competition&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;translation&lt;/a>;, &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/&quot;>;retrieval-augmented generation&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligent_code_completion&quot;>;code completion&lt;/a>;. These models are trained on massive amounts of &lt;em>;textual &lt;/em>;data derived from a variety of sources like &lt;a href=&quot;https://commoncrawl.org/&quot;>;common crawl&lt;/a>; and open-source code that allows them to identify patterns in languages. This makes them very powerful &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-shot_learning&quot;>;zero-shot&lt;/a>; tools; for instance, &lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;when paired with retrieval&lt;/a>;, they can answer questions about and summarize current events 。 &lt;/p>; &lt;p>; Despite DL-based forecasters largely &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;outperforming&lt;/a>; traditional methods and progress being made in &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;reducing training and inference costs&lt;/a>;, they face challenges: most DL architectures require &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;long and involved training and validation cycles&lt;/a>; before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like &lt;a href=&quot;https://en.wikipedia.org/wiki/Customer_demand_planning&quot;>;retail demand planning&lt;/a>;. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/pdf/2310.10688.pdf&quot;>;A decoder-only foundation model for time-series forecasting&lt;/a>;”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>;Google Cloud Vertex AI&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A decoder-only foundation model for time-series forecasting&lt;/h2>; &lt;p>; LLMs are usually trained in a &lt;a href=&quot;https://arxiv.org/pdf/1801.10198.pdf&quot;>;decoder-only&lt;/a>; fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformer&lt;/a>; layers that produce an output corresponding to each input token (it cannot attend to future tokens) 。 Finally, the output corresponding to the &lt;em>;i&lt;/em>;-th token summarizes all the information from previous tokens and predicts the (&lt;em>;i&lt;/em>;+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”. &lt;/p>; &lt;p>; A foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining数据集。 Similar to LLMs, we use stacked transformer layers (self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward&lt;/a>; layers) as the main building blocks for the TimesFM model 。 In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;long-horizon forecasting work&lt;/a>;. The task then is to forecast the (&lt;em>;i&lt;/em>;+1)-th patch of time-points given the &lt;em>;i&lt;/em>;-th output at the end of the stacked transformer layers. &lt;/p>; &lt;p>; However, there are several key differences from language models. Firstly, we need a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with &lt;a href=&quot;https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/&quot;>;positional encodings&lt;/a>; (PE). For that, we use a residual block similar to our prior work in &lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;long-horizon forecasting&lt;/a>;. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, ie, the output patch length can be larger than the input patch length. &lt;/p>; &lt;p>; Consider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 &quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border= &quot;0&quot; data-original-height=&quot;674&quot; data-original-width=&quot;1084&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3. jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TimesFM architecture.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Pretraining data&lt;/h2>; &lt;p>; Just like LLMs get better with more tokens, TimesFM requires a large volume of legitimate time series data to learn and improve. We have spent a great amount of time creating and assessing our training datasets, and the following is what we have found works best: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Synthetic data helps with the basics.&lt;/strong>; Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting. &lt;/p>;&lt;/div>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Real-world data adds real-world flavor.&lt;/strong>; We comb through available public time series datasets, and selectively put together a large corpus of 100 billion time-points. Among these datasets there are &lt;a href=&quot;https://trends.google.com/trends/&quot;>;Google Trends&lt;/a>; and &lt;a href=&quot;https://meta.wikimedia.org/wiki/Research:Page_view&quot;>;Wikipedia Pageviews&lt;/a>;, which track what people are interested in, and that nicely mirrors trends and patterns in many other real-world time series. This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training. &lt;/p>;&lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Zero-shot evaluation results&lt;/h2>; &lt;p>; We evaluate TimesFM zero-shot on data not seen during training using popular time-series benchmarks. We observe that TimesFM performs better than most statistical methods like &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot;>;ARIMA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_smoothing&quot;>;ETS&lt;/a>; and can match or outperform powerful DL models like &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; that have been &lt;em>;explicitly trained&lt;/em>; on the target time-series. &lt;/p>; &lt;p>; We used the &lt;a href=&quot;https://huggingface.co/datasets/monash_tsf&quot;>;Monash Forecasting Archive&lt;/a>; to evaluate TimesFM&#39;s out-of-the-box performance. This archive contains tens of thousands of time-series from various domains like traffic, weather, and demand forecasting covering frequencies ranging from few minutes to yearly data. Following existing literature, we inspect the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE) &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;appropriately scaled&lt;/a>; so that it can be averaged across the datasets. We see that zero-shot (ZS) TimesFM is better than most supervised approaches, including recent deep learning models. We also compare TimesFM to &lt;a href=&quot;https://platform.openai.com/docs/models/gpt-3-5&quot;>;GPT-3.5&lt;/a>; for forecasting using a specific prompting technique proposed by &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime(ZS)&lt;/a>;. We demonstrate that TimesFM performs better than llmtime(ZS) despite being orders of magnitude smaller. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;876&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Scaled MAE (the lower the better) of TimesFM(ZS) against other supervised and zero-shot approaches on Monash datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Most of the Monash datasets are short or medium horizon, ie, the prediction length is not too long. We also test TimesFM on popular benchmarks for long horizon forecasting against a recent state-of-the-art baseline &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; (and other long-horizon forecasting baselines). In the next figure, we plot the MAE on &lt;a href=&quot;https://paperswithcode.com/dataset/ett&quot;>;ETT&lt;/a>; datasets for the task of predicting 96 and 192 time-points into the future. The metric has been calculated on the last test window of each dataset (as done by the &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime&lt;/a>; paper). We see that TimesFM not only surpasses the performance of llmtime(ZS) but also matches that of the supervised PatchTST model explicitly trained on the respective datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;433&quot; data-original-width=&quot;735&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Last window MAE (the lower the better) of TimesFM(ZS) against llmtime(ZS) and long-horizon forecasting baselines on ETT datasets.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We train a decoder- only foundation model for time-series forecasting using a large pretraining corpus of 100B real world time-points, the majority of which was search interest time-series data derived from Google Trends and pageviews from Wikipedia. We show that even a relatively small 200M parameter pretrained model that uses our TimesFM architecture displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov and Yichen Zhou.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6956767920612914706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html&quot; rel=&quot;alternate&quot; title=&quot;A decoder-only foundation model for time-series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502&lt;/id>;&lt;published>;2024-02-02T09:49:00.000-08:00&lt;/published>;&lt;updated>;2024-02-02T09:49:36.211-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML Fairness&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Intervening on early readouts for mitigating spurious features and simplicity bias&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Machine learning models in the real world are often trained on limited data that may contain unintended &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_(statistics)&quot;>;statistical biases&lt;/a >;。 For example, in the &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CELEBA&lt;/a>; celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as &lt;a href=&quot;https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging&quot;>;medical diagnosis&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Surprisingly, recent work has also discovered an inherent tendency of deep networks to &lt;em>;amplify such statistical biases&lt;/em>;, through the so-called &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf&quot;>;simplicity bias&lt;/a>; of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. &lt;/p>; &lt;p>; With the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying &lt;em>;early readouts&lt;/em>; and &lt;em>;feature forgetting&lt;/em>; 。 First, in “&lt;a href=&quot;https://arxiv.org/abs/2310.18590&quot;>;Using Early Readouts to Mediate Featural Bias in Distillation&lt;/a>;”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;>;model distillation&lt;/a>;, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;Overcoming Simplicity Bias in Deep Networks using a Feature Sieve&lt;/a>;”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model&#39;s ability to generalize to unseen domains compared to previous approaches. Our &lt;a href=&quot;https://ai.google/responsibility/principles&quot;>;AI Principles&lt;/a>; and our &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;1080&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation comparing hypothetical responses from two models trained with and without the feature sieve.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Early readouts for debiasing distillation&lt;/h2>; &lt;p>; We first illustrate the diagnostic value of &lt;em>;early readouts&lt;/em >; and their application in debiased distillation, ie, making sure that the student model inherits the teacher model&#39;s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the &lt;a href=&quot;https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e&quot;>;cross-entropy loss&lt;/a>; between student outputs and the ground-truth labels) and teacher matching (minimizing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL divergence&lt;/a>; loss between student and teacher outputs for any given input). &lt;/p>; &lt;p>; Suppose one trains a linear decoder, ie, a small auxiliary neural network named as &lt;em>;Aux,&lt;/em>; on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model&#39;s dependence on potentially spurious features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;796&quot; data-original-width=&quot;1128&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrating the usage of early readouts (ie, output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used this signal to modulate the contribution of the teacher in the distillation loss on a per-instance basis, and found significant improvements in the trained student model as a result. &lt;/p>; &lt;p>; We evaluated our approach on standard benchmark datasets known to contain spurious correlations (&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;>;Waterbirds&lt;/a>;, &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CelebA&lt;/a>;, &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/civil_comments&quot;>;CivilComments&lt;/a>;, &lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;>;MNLI&lt;/a>;). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color 。 Thus, a measure of model performance is its &lt;em>;worst group accuracy&lt;/em>;, ie, the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our &lt;a href=&quot;https://arxiv.org/pdf/2310.18590.pdf&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1270&quot; height=&quot;511&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overcoming simplicity bias with a feature sieve&lt;/h2>; &lt;p>; In a second, closely related project, we intervene directly on the information provided by early readouts, to improve &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;feature learning&lt;/a>; and &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/generalization/video-lecture&quot;>;generalization&lt;/a>;. The workflow alternates between &lt;em>;identifying &lt;/em>;problematic features and &lt;em>;erasing identified features&lt;/em>; from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;1098&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Training workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We describe the identification and erasure steps in more detail: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Identifying simple features&lt;/b>;: We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. &lt;/li>;&lt;li>;&lt;b>;Applying the feature sieve&lt;/b>;: We aim to erase the identified features in the early layers of the neural network with the use of a novel &lt;em>;forgetting loss&lt;/em>;,&lt;em>; L&lt;sub>;f &lt;/sub>;&lt;/em>;, which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged. &lt;/li>; &lt;/ul>; &lt;p>; We can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter search&lt;/a>; to maximize validation accuracy, a standard measure of generalization. Since we include “no-forgetting” (ie, the baseline model) in the search space, we expect to find settings that are at least as good as the baseline. &lt;/p>; &lt;p>; Below we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (&lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;) and animal categorization (&lt;a href=&quot;https://arxiv.org/pdf/1906.02899v3.pdf&quot;>;NICO&lt;/a>;). Feature importance was estimated using post-hoc gradient-based importance scoring (&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GRAD-CAM&lt;/a>;), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;850&quot; data-original-width=&quot;1616&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Feature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this ability to learn better, generalizable features, we show substantial gains over a range of relevant baselines on real-world spurious feature benchmark datasets: &lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2104.06885.pdf&quot;>;CelebA Hair&lt;/a>;, &lt;a href=&quot;https://nico.thumedialab.com/&quot;>;NICO&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/imagenet_a&quot;>;ImagenetA&lt;/a>;, by margins up to 11% (see figure below). More details are available in &lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1082&quot; data-original-width=&quot;844&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png&quot; width=&quot;501&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We hope that our work on early readouts and their use in feature sieving for generalization will both spur the development of a new class of adversarial feature learning approaches and help improve the generalization capability and robustness of deep learning systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;The work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at &lt;a href=&quot;https://www.iitb.ac.in/&quot;>;IIT Bombay&lt;/a>;. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2254287928040727502/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html&quot; rel=&quot;alternate&quot; title=&quot;Intervening on early readouts for mitigating spurious features and simplicity bias&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984&lt;/id>;&lt;published>;2024-01 -31T13:59:00.000-08:00&lt;/published>;&lt;updated>;2024-01-31T13:59:36.056-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MobileDiffusion: Rapid text-to-image generation on-device&lt; /stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (eg, &lt;a href=&quot;https://stability.ai/news/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>;, &lt;a href=&quot;https://openai.com/research/dall-e&quot;>;DALL·E&lt;/a>;, and &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;). While recent advancements in inference solutions on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;Android&lt;/a>; via MediaPipe and &lt;a href=&quot;https://github.com/apple/ml-stable-diffusion&quot;>;iOS&lt;/a>; via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices&lt;/a>;”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style =&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rapid text-to-image generation on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires &lt;a href=&quot;https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot;>;iterative denoising&lt;/a>; to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature. &lt;/p>; &lt;p>; The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.00927&quot;>;DPM&lt;/a>;) or distillation techniques (eg, &lt;a href=&quot;https://arxiv.org/abs/2202.00512&quot;>;progressive distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;>;consistency distillation&lt;/a>;), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.&quot;>;Adversarial Diffusion Distillation&lt;/a>;, even reduce to a single necessary step. &lt;/p>; &lt;p>; However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (eg, &lt;a href=&quot;https://snap-research.github.io/SnapFusion/&quot;>;SnapFusion&lt;/a>;). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MobileDiffusion&lt;/h2>; &lt;p>; Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model&#39;s architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion&#39;s &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;UNet architecture&lt;/a>;. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion. &lt;/p>; &lt;p>; The design of MobileDiffusion follows that of &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;latent diffusion models&lt;/a>;. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP-ViT/L14&lt;/a>;, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Diffusion UNet&lt;/h3>; &lt;p>; As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (eg, data, optimizer) to study the effects of different architectures. &lt;/p>; &lt;p>; In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of &lt;a href=&quot;https://arxiv.org/abs/2301.11093&quot;>;UViT&lt;/a>; architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV /s915/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;915&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Convolution blocks, in particular &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;separable convolution&lt;/a>;. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance. &lt;/p>; &lt;p>; In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of &lt;a href=&quot;https://arxiv.org/pdf/2110.12894.pdf&quot;>;FLOPs&lt;/a>; (floating-point operations) and number of parameters. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Comparison of some diffusion UNets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h3>;Image decoder&lt;/h3>; &lt;p>; In addition to the UNet, we also optimized the image decoder. We trained a &lt;a href=&quot;https://arxiv.org/abs/2012.03715&quot;>;variational autoencoder&lt;/a>; (VAE) to encode an &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size. To further enhance efficiency, we design a lightweight decoder architecture by pruning the original&#39;s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our &lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;789&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Decoder&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;#Params (M)&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;PSNR↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;SSIM↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;LPIPS↓&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;SD&lt;/b>; &lt;/td>; &lt;td>;49.5 &lt;/td>; &lt;td>;26.7 &lt;/td>; &lt;td>;0.76 &lt;/td>; &lt;td>;0.037 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours&lt;/b>; &lt;/td>; &lt;td>;39.3 &lt;/td>; &lt;td>;30.0 &lt;/td>; &lt;td>;0.83 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours-Lite&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;9.8 &lt;/td>; &lt;td>;30.2 &lt;/td>; &lt;td>;0.84 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_similarity&quot;>;structural similarity index measure&lt;/a>; (SSIM), and &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;>;Learned Perceptual Image Patch Similarity&lt;/a>; (LPIPS).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;One-step sampling&lt;/h3>; &lt;p>; In addition to optimizing the model architecture, we adopt a &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN hybrid&lt;/a>; to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (eg, &lt;a href=&quot;https://arxiv.org/abs/2301.09515&quot;>;StyleGAN-T&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.05511&quot;>;GigaGAN&lt;/a>;) confront similar complexities, resulting in highly intricate and expensive training. &lt;/p>; &lt;p>; To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training. &lt;/p>; &lt;p>; The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ -UILKw/s960/image7.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;960 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of DiffusionGAN fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling 。 With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1728&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images generated by our MobileDiffusion&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1184&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency measurements (&lt;b>;s&lt;/b>;) on mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;responsible AI practices&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.&lt; /em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5966553114967673984/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html&quot; rel=&quot;alternate&quot; title=&quot;MobileDiffusion: Rapid text-to-image generation on-device&quot; type=&quot;text/ html“/>; &lt;aunder>; &lt;名称>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777526611 &lt;/uri>; &lt;emage>; &lt;Email>; ：Image Height =“ 16” RER =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded.gif.gif.gif” width width =“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c /InstantTIGO%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5144906729109253495&lt;/id>;&lt;published>;2024-01-26T11:56:00.000-08:00&lt;/published>;&lt;updated >;2024-01-26T11:56:23.553-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Mixed-input matrix multiplication performance optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Manish Gupta , Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;display: none ;” />; &lt;p>; AI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs). LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver &lt;a href=&quot;https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e&quot;>;tens of exaflops&lt;/a>; of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The bulk of an LLM&#39;s memory and compute are consumed by &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;>;weights&lt;/a>; in &lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;matrix multiplication&lt;/a>; operations. Using narrower &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Primitive_data_type&quot;>;data types&lt;/a>;&lt;/em>; reduces memory consumption. For example, storing weights in the 8-bit &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_(computer_science)&quot;>;integer&lt;/a>; (ie, U8 or S8) data type reduces the memory footprint by 4× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;>;single-precision&lt;/a>; (F32) and 2× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Half-precision_floating-point_format&quot;>;half-precision&lt;/a>; (F16) or &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format&quot;>;bfloat16&lt;/a>; (BF16). Furthermore, &lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;previous work has&lt;/a>; shown that LLM models running matrix multiplications with &lt;em>;weights&lt;/em>; in S8 and &lt;em>;input&lt;/em>; in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as &lt;em>;weight-only quantization&lt;/em>; and requires efficient implementation of matrix multiplication with &lt;em>;mixed-inputs&lt;/em>;, eg, half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations. &lt;/p>; &lt;p>; To that end, in this blog we focus on mapping mixed-input matrix multiplication onto the &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/&quot;>;NVIDIA Ampere architecture&lt;/a>;. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; repository. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Memory footprint for an 175B parameter LLM model with various data types formats.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;矩阵 - 元素 - 蓄积操作&lt;/h2>; &lt;p>;现代AI硬件加速器，例如&lt;a href = &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google&#39;s TPU&lt;/a>; and &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA&#39;s GPU&lt;/a>; multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the &lt;em>;matrix-multiply-accumulate&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;) operation. For the rest of the blog the reference to &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation are fixed in hardware 。 This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. &lt;/p>; &lt;p>; The Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation is defined by specifying two input matrices (eg, &lt;em>;A&lt;/em>; &amp;amp; &lt;em>;B&lt;/em>;, shown below) to produce a result matrix, &lt;em>;C&lt;/em>;. The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation natively supports mixed-precision. &lt;em>;&lt;a href=&quot;https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/&quot;>;Mixed-precision Tensor Cores&lt;/a>;&lt;/em>; allow mixing input (&lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) data type with the result (&lt;em>;C&lt;/em>;) data type. In contrast, &lt;em>;mixed-input &lt;/em>;matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Tensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Challenges of mixed -input matrix multiplication&lt;/h2>; &lt;p>; To simplify the discussion, we restrict to a specific example of mixed-input matrix multiplication: F16 for user input and U8 for the model weights (written as F16 * U8). The techniques described here work for various combinations of mixed-input data types. &lt;/p>; &lt;p>; gpu程序员可以访问&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-cuda-cuda-cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;记忆的层次&lt;/a>;, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a &lt;em>;warp&lt;/em>;. The supported data type &lt;em>;and&lt;/em>; layout within a warp are fixed for an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Data type conversion &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the &lt;em>;mixed-input&lt;/em>; matrix multiplication to hardware-supported &lt;em>;mixed-precision&lt;/em>; Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Layout conformance &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix &lt;em>;B&lt;/em>; of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called &lt;em>;layout conformance&lt;/em>; and needs to be achieved in the software. &lt;/p>; &lt;p>; The figure below shows an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation consuming matrix &lt;em>;A&lt;/em>; and matrix &lt;em>;B&lt;/em>; from registers to produce matrix &lt;em>;C&lt;/em>; in registers, distributed across one warp. The thread &lt;em>;T0&lt;/em>; is highlighted and zoomed in to show the weight matrix &lt;em>;B&lt;/em>; goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). （原始图来源&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;开发CUDA内核，以将张量芯推向NVIDIA A100 A100 &lt;a100 &lt; /a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Software strategies addressing challenges &lt;/h2>; &lt;p>; A typical data type conversion involves a sequence of operations on 32-bit registers, shown below. Each rectangular block represents a register and the adjoining text are the operations. The entire sequence shows the conversion from 4xU8 to 2x(2xF16). The sequence involves roughly 10 operations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; from 4xU8 to 2x(2xF16) in 32-bit registers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There are many ways of achieving layout conformance. Two of the existing solutions are: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Narrower bitwidth shared memory loads&lt;/em>;: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in &lt;em>;two&lt;/em>; 32-bit registers, with each register containing 2xF16 values (shown above for the matrix &lt;em>;B&lt;/em>;&#39;s thread &lt;em>;T0&lt;/em>;). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth. &lt;/li>;&lt;li>;&lt;em>;Pre-processing in global memory&lt;/em>;: An &lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;alternative strategy&lt;/a>; involves rearranging the data within the global memory (one level above the shared memory in &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;memory hierarchy&lt;/a>;), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; adopts this method to effectively address layout conformance challenges. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimized software strategies&lt;/h2>; &lt;p>; To further optimize and reduce the overhead of data type conversion and layout conformance, we have implemented &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a>;&lt;/code>; and &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;&lt;/code>;, respectively. &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. &lt;/p>; &lt;p>; The conversion sequence for &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>; is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. &lt;code>;FastNumericArrayConvertor&lt;/code>; uses the &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt&quot;>;permute byte&lt;/a>;&lt;/code>; to rearrange bytes of 4xU8 into two registers. Additionally, &lt;code>;FastNumericArrayConvertor&lt;/code>; does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in &lt;em>;two&lt;/em>; 32-bit registers containing 2x(2xF16) values. The &lt;code>;FastNumericArrayConvertor&lt;/code>; for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; utilizes &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;permute bytes&lt;/a>;&lt;/code>; and packed arithmetic, reducing the number of instructions in the data type conversion.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations 。 &lt;/p>; &lt;p>; NVIDIA Ampere architecture provides a load matrix instruction (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;). The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the &lt;em>;shape&lt;/em>; and &lt;em>;layout&lt;/em>; that &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>; consume. The use of &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; &lt;em>;reduces&lt;/em>; the number of load instructions and &lt;em>;increases&lt;/em>; the memory bandwidth utilization. Since the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; instruction moves U8 data to registers, the layout after the load conforms with U8*U8 &lt;span style=&quot;color: ＃54863f;“>; &lt;code>; MMA &lt;/code>; &lt;/span>;操作，而不是F16*F16 &lt;span style =“颜色：＃54863f;”>; &lt;code>; MMA &lt;/code>; MMA &lt;/code>; &lt;/span>; &lt;/span>;操作。 We implemented &lt;code>;FragmentShuffler&lt;/code>; to rearrange the data within registers using shuffle (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; operations to achieve the layout conformance. &lt;/p>;&lt;p>; The most significant contribution of this work is to achieve layout conformance through register shuffles, avoiding offline pre-processing in global memory or narrower bitwidth shared memory loads. Furthermore, we provide implementations for &lt;code>;FastNumericArrayConvertor&lt;/code>; covering data type conversion from &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8-to-BF16&lt;/a>;, and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8-to-BF16&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance results&lt;/h2>; &lt;p>; We measured the performance of eight mixed-input variants of &lt;em>;our method&lt;/em>; (shown below in blue and red; varying the data types of matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) and two &lt;em>;mixed-precision&lt;/em>; data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Mixed-input matrix multiplication performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape &lt;code>;m=3456, n=4096, k=2048.&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p >; &lt;em>;We would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia.我们要感谢我们的Nvidia Partners Rawn Henry，Pradeep Ramani，Vijay Thakkar，Haicheng Wu，Andrew Kerr，Matthew Nicely和Vartika Singh。 /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;Mixed-input matrix multiplication performance optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;published >;2024-01-23T14:27:00.000-08:00&lt;/published>;&lt;updated>;2024-01-23T14:27:09.785-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer: Scaling transformers for graph-structured data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot;样式=“显示：无；” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;Graphs&lt;/a>;, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A common approach to learning on graphs are &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;graph neural networks&lt;/a>; (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2&quot;>;message-passing&lt;/a>; framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors. &lt;/p>; &lt;p>; Recently, &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer models&lt;/a>; have emerged as a popular alternative to message-passing GNNs. These models build on the success of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer architectures&lt;/a>; in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism&lt;em>; &lt;/em>;that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see &lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot;>;the first open problem here&lt;/a>;). &lt;/p>; &lt;p>; A natural remedy is to use a &lt;em>;sparse&lt;/em>; interaction graph with fewer edges. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;已经提出了许多稀疏和高效的变压器&lt;/a>;以消除序列的四边形瓶颈，但是它们通常不会扩展到序列以原则性的方式图形。 &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;spectral graph theory&lt;/a>;, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Expander graphs&lt;/h2>; &lt;p>; A key idea at the heart of Exphormer is the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph&quot;>;expander graphs&lt;/a>;, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, ie, a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes. &lt;/p>; &lt;p>; A common class of expander graphs are &lt;em>;d&lt;/em>;-regular expanders, in which there are &lt;em>;d&lt;/em>; edges from every node (ie, every node has degree &lt;em>;d&lt;/em>;). The quality of an expander graph is measured by its &lt;em>;spectral gap&lt;/em>;, an algebraic property of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;>;adjacency matrix&lt;/a>; (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Ramanujan_graph&quot;>;Ramanujan graphs&lt;/a>; — they achieve a gap of &lt;em>;d&lt;/em>; - 2*√(&lt;em>;d&lt;/em>;-1), which is essentially the best possible among &lt;em>;d&lt;/em>;-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of &lt;em>;d&lt;/em>;. We use a &lt;a href=&quot;https://arxiv.org/abs/cs/0405020&quot;>;randomized expander construction of Friedman&lt;/a>;, which produces near-Ramanujan graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;800&quot; height=&quot;320&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968&quot;>;&lt;span face=&quot;Arial, sans- serif&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Expander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.&lt;/span>;&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse &lt;em>;d&lt;/em>;-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that &lt;em>;d&lt;/em>; is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.&lt;/p>; &lt;br />; &lt;h2>;Exphormer: Constructing a sparse interaction graph&lt;/h2>; &lt;p>; Exphormer combines expander edges with the input graph and virtual nodes. More specifically, the sparse attention mechanism of Exphormer builds an interaction graph consisting of three types of edges: &lt;/p>; &lt;ul>; &lt;li>;Edges from the input graph (&lt;em>;local attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from a constant-degree expander graph (&lt;em>;expander attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from every node to a small set of virtual nodes (&lt;em>;global attention&lt;/em>;) &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3&quot;>;&lt;span face=&quot;Arial, sans-serif&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Exphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.&lt;/span>;&lt;/ span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Each component serves a specific purpose: the edges from the input graph retain the inductive bias from the input graph structure (which typically gets lost in a fully-connected attention module). Meanwhile, expander edges allow good global connectivity and random walk mixing properties (which spectrally approximate the complete graph with far fewer edges). Finally, virtual nodes serve as global “memory sinks” that can directly communicate with every node. While this results in additional edges from each virtual node equal to the number of nodes in the input graph, the resulting graph is still sparse. The degree of the expander graph and the number of virtual nodes are hyperparameters to tune for improving the quality指标。 &lt;/p>; &lt;p>; Furthermore, since we use an expander graph of constant degree and a small constant number of virtual nodes for the global attention, the resulting sparse attention mechanism is linear in the size of the original input graph, ie, it models a number of direct interactions on the order of the total number of nodes and edges. &lt;/p>; &lt;p>; We additionally show that Exphormer is as expressive as the dense transformer and obeys universal approximation properties. In particular, when the sparse attention graph of Exphormer is augmented with self loops (edges connecting a node to itself), it can universally approximate continuous functions [&lt;a href=&quot;https://arxiv.org/abs/1912.10077&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>;2&lt;/a>;]. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Relation to sparse Transformers for sequences&lt;/h3>; &lt;p>; It is interesting to compare Exphormer to sparse attention methods for sequences. Perhaps the architecture most conceptually similar to our approach is &lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>;BigBird&lt;/a>;, which builds an interaction graph by combining different components. BigBird also uses virtual nodes, but, unlike Exphormer, it uses window attention and random attention from an &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>; random graph model for the remaining components. &lt;/p>; &lt;p>; Window attention in BigBird looks at the tokens surrounding a token in a sequence — the local neighborhood attention in Exphormer can be viewed as a generalization of window attention to graphs. &lt;/p>; &lt;p>; The Erdős-Rényi graph on &lt;em>;n&lt;/em>; nodes, &lt;em>;G(n, p)&lt;/em>;, which connects every pair of nodes independently with probability &lt;em>;p&lt;/em>;, also functions as an expander graph for suitably high &lt;em>;p&lt;/em>;. However, a superlinear number of edges (Ω(&lt;em>;n&lt;/em>; log &lt;em>;n&lt;/em>;)) is needed to ensure that an Erdős-Rényi graph is connected, let alone a good expander. On the other hand, the expanders used in Exphormer have only a &lt;em>;linear&lt;/em>; number of edges. &lt;/p>; &lt;br />; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; Earlier works have shown the use of full graph Transformer-based models on datasets with graphs of size up to 5,000 nodes. To evaluate the performance of Exphormer, we build upon the celebrated &lt;a href=&quot;https://github.com/rampasek/GraphGPS&quot;>;GraphGPS framework&lt;/a>; [&lt;a href=&quot;https://arxiv.org/abs/2205.12454&quot;>;3&lt;/a>;], which combines both message passing and graph transformers and achieves state-of-the-art performance on a number of datasets. We show that replacing dense attention with Exphormer for the graph attention component in the GraphGPS framework allows one to achieve models with comparable or better performance, often with fewer trainable parameters. &lt;/p>; &lt;p>; Furthermore, Exphormer notably allows graph transformer architectures to scale well beyond the usual graph size limits mentioned above. Exphormer can scale up to datasets of 10,000+ node graphs, such as the &lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>;Coauthor dataset&lt;/a>;, and even beyond to larger graphs such as the well-known &lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>;ogbn-arxiv dataset&lt;/a>;, a citation network, which consists of 170K nodes and 1.1 million edges. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance .png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original- height=&quot;190&quot; data-original-width=&quot;1522&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot; https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper&#39;s publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td align=&quot;left&quot;>;&lt;strong>;Model&amp;nbsp;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PascalVOC-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;COCO-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Func&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;AP &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Struct&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MAE &lt;/font>;&lt;strong>;↓&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PCQM-Contact&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MRR &lt;/font>;&lt;strong>;↑&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>;&lt;td colspan=&quot;6&quot;>;&lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />;&lt;/div>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;Standard GraphGPS&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.375 ± 0.011&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.341 ± 0.004&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;&lt;strong>;0.654 ± 0.004&lt;/strong>; &amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.250 ± 0.001&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.334 ± 0.001 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Exphormer (ours)&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.398 ± 0.004&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.346 ± 0.001 &amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;em>;&amp;nbsp;0.653 ± 0.004&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong >;&lt;em>;&amp;nbsp;0.248 ± 0.001&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.364 ± 0.002&lt;/em>;&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>;-->; &lt;p>; Finally, we observe that Exphormer, which creates an overlay graph of small diameter via expanders, exhibits the ability to effectively learn long-range dependencies 。 The &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>;&amp;nbsp;is a suite of five graph learning datasets designed to measure the ability of models to capture long-range interactions 。 Results show that Exphormer-based models outperform standard GraphGPS models (which were previously state-of-the-art on four out of five datasets at the time of publication). &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Graph transformers have emerged as an important architecture for ML that adapts the highly successful sequence-based transformers used in NLP to graph-structured data. Scalability has, however, proven to be a major challenge in enabling the use of graph transformers on datasets with large graphs. In this post, we have presented Exphormer, a sparse attention framework that uses expander graphs to improve scalability of graph transformers. Exphormer is shown to have important theoretical properties and exhibit strong empirical performance, particularly on datasets where it is crucial to learn long range dependencies. For more information, we point the reader to a short presentation &lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;video&lt;/a>; from ICML 2023. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our research collaborators Hamed Shirzad and Danica J. Sutherland from The University of British Columbia as well as Ali Kemal Sinop from Google Research. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1418736582601940076/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html&quot; rel=&quot;alternate&quot; title=&quot;Exphormer: Scaling transformers for graph-structured data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;