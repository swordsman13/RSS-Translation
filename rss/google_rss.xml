<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com,1999:blog-8474926331452026626</id><updated> 2023-05-24T19:16:43.514-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Publications"></category><category term="Research"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="conference"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="NLP"></category><category term="CVPR"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="Multimodal Learning"></category><category term="Computer Science"></category><category term="On-device Learning"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="Self-Supervised Learning"></category><category term="AutoML"></category><category term="Quantum AI"></category><category term="Hardware"></category><category term="NeurIPS"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="Android"></category><category term="Awards"></category><category term="Structured Data"></category><category term="TPU"></category><category term="ACL"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Image Processing"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="video"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="DeepMind"></category><category term="Earth Engine"></category><category term="Google Maps"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Responsible AI"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="market algorithms"></category><category term="Acoustic Modeling"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Semantic Models"></category><category term="Systems"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Augmented Reality"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Recommender Systems"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Social Networks"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="RAI-HCT Highlights"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新消息。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1230&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5648346519353613408&lt;/id>;&lt;published>;2023-05-23T10:51:00.004-07:00&lt;/published>;&lt;updated>;2023-05- 24T14:28:39.289-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;语义模型&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger. com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;用机器学习解决代码审查评论&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;由 Alexander Frömmgen，高级软件工程师和 Lera Kharatyan，Core Systems &amp;amp; 高级软件工程师发布经验&lt;/span>; ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s2000/comments2code.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 代码更改审查是大规模软件开发过程的关键部分，&lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;花费&lt;/a>;大量代码作者和代码审查者的时间。作为此过程的一部分，审阅者检查建议的代码并通过用自然语言编写的注释向作者询问代码更改。在 Google，我们每年看到&lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;数百万条审稿意见&lt;/a>;，作者平均需要约 60 分钟&lt;a href=&quot; https://research.google/pubs/pub49446/&quot;>;在发送更改以供审核和最终提交更改之间的活跃牧羊时间&lt;/a>;。在我们的测量中，代码作者必须完成的处理审阅者评论所需的活动时间几乎与评论数量呈线性增长。然而，通过机器学习 (ML)，我们有机会自动化和简化代码审查过程，例如，根据评论文本提出代码更改建议。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今天，我们描述了在实际环境中应用大型序列模型的最新进展，以在日常开发中自动解决代码审查评论Google 的工作流程（即将出版）。截至今天，谷歌的代码更改作者通过应用机器学习建议的编辑来处理大量审阅者的评论。我们预计这将在 Google 规模上每年将花在代码审查上的时间减少数十万小时。未经请求的非常积极的反馈突出表明，ML 建议的代码编辑的影响提高了 Google 员工的工作效率，并使他们能够专注于更具创造性和更复杂的任务。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;预测代码编辑&lt;/h2>; &lt;p>; 我们首先训练一个预测代码编辑的模型需要解决审稿人的意见。该模型针对各种编码任务和相关的开发人员活动（例如，重命名变量、修复损坏的构建、编辑文件）进行了预训练。然后通过审查的代码更改、审阅者的评论以及作者为解决这些评论而执行的编辑，针对此特定任务对其进行微调。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6z fXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s1523/figure1.gif”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1523&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv 4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s16000/figure1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式= &quot;text-align: center;&quot;>;ML 建议的重构代码示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Google 使用 &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2854146&quot;>;monorepo&lt;/a>;，它是所有软件工件的单一存储库，它允许我们的训练数据集包含所有用于构建谷歌最新的软件，以及以前的版本。 &lt;/p>; &lt;p>; 为了提高模型质量，我们对训练数据集进行了迭代。例如，我们将每个文件只有一个评论者评论的数据集的模型性能与每个文件有多个评论的数据集进行了比较，并试验了分类器，以根据小型精选数据集清理训练数据，以选择具有最佳离线效果的模型精度和召回指标。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;服务基础架构和用户体验&lt;/h2>; &lt;p>; 我们在经过训练的模型，侧重于整体用户体验和开发人员效率。作为其中的一部分，我们通过一系列用户研究探索了不同的用户体验 (UX) 替代方案。然后，我们根据来自内部测试版（即开发中的功能测试）的见解改进了该功能，包括用户反馈（例如，建议编辑旁边的“这有用吗？”按钮）。 &lt;/p>; &lt;p>; 最终模型针对目标&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&quot;>;精度&lt;/ a >; 50%。也就是说，我们调整了模型和建议过滤，以便我们的评估数据集上 50% 的建议编辑是正确的。一般来说，增加目标精度会减少显示的建议编辑的数量，而降低目标精度会导致更多不正确的建议编辑。不正确的建议编辑会占用开发人员的时间并降低开发人员对该功能的信任。我们发现 50% 的目标精度提供了良好的平衡。 &lt;/p>; &lt;p>; 在高层次上，对于每一个新的审阅者评论，我们都会以与训练、查询模型和生成建议的代码编辑相同的格式生成模型输入。如果模型对预测有信心并且满足一些额外的启发式方法，我们会将建议的编辑发送到下游系统。下游系统，即代码审查前端和集成开发环境 (IDE)，向用户公开建议的编辑并记录用户交互，例如预览和应用事件。专用管道收集这些日志并生成聚合见解，例如，本博文中报告的总体接受率。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRud NVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s1250/figure%202c.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1250&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOY yeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s16000/figure%202c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;ML 建议编辑基础架构的架构。我们处理来自多个服务的代码和基础设施，获得模型预测并在代码审查工具和 IDE 中显示预测。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 开发人员与 ML 交互- 建议在代码审查工具和 IDE 中进行编辑。根据用户研究的见解，集成到代码审查工具中最适合简化审查体验。 IDE 集成提供了额外的功能，并支持 ML 建议的编辑（下图中左侧）的 3 向合并，以防在审查代码状态（右侧）之上的本地更改与合并结果（中心）发生冲突。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cV z88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s958/figure%203 %20(25%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot; 958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8 h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s16000/figure%203%20(25%25).gif&quot; />;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;IDE 中的三向合并用户体验。&lt;/td>;&lt;/tr>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; 离线评估表明该模型处理了 52% 的评论目标精度为 50%。 Beta 的在线指标和完整的内部发布确认了这些离线指标，即我们看到模型建议高于我们目标模型置信度的所有相关审阅评论的大约 50%。所有预览的建议编辑中有 40% 到 50% 由代码作者应用。 &lt;/p>; &lt;p>; 我们在测试期间使用“没有帮助”的反馈来识别模型的反复出现的故障模式。我们实施了服务时间启发式算法来过滤这些，从而减少显示的不正确预测的数量。通过这些变化，我们以数量换取质量，并观察到现实世界的接受率有所提高。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YUR Kz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s1753/图%204 %20(50%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot; 1753&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAV oBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s16000/figure%204%20(50%25).gif&quot; />;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;代码审查工具 UX。该建议显示为评论的一部分，可以预览、应用并评定为有帮助或无帮助。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们的测试版发布显示了 &lt;em>;可发现性挑战&lt;/em>;：代码作者仅预览了约 20% 的所有生成的建议编辑。我们修改了 UX 并在评论者评论旁边引入了一个突出的“显示 ML-edit”按钮（见上图），导致发布时的整体预览率约为 40%。我们还发现，由于作者在审查过程中所做的更改相互冲突，代码审查工具中建议的编辑通常不适用。我们通过代码审查工具中的一个按钮解决了这个问题，该按钮可以在合并视图中打开 IDE 以进行建议的编辑。我们现在观察到，其中超过 70% 的应用在代码审查工具中，不到 30% 的应用在 IDE 中。所有这些更改使我们能够将通过 ML 建议的编辑解决的审稿人评论的总体比例增加 2 倍，从测试版到完整的内部发布。在谷歌范围内，这些结果有助于每年自动解决数十万条评论。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altT mAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9 -qaZEZkZyw/s1920/figure%207.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;887&quot; 数据-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9I zkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s16000/figure%207.png &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;建议过滤漏斗。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>; 我们看到 ML 建议的编辑解决了生产中广泛的审阅者评论。这包括简单的本地化重构和散布在代码中的重构，如上面博文中的示例所示。该功能解决了需要代码生成、重构和导入的较长且措辞不太正式的注释。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_8 6gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s1479/figure%205c.gif&quot; 样式=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;651&quot; data-original-width=&quot;1479&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZ JMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s16000/figure%205c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;文本对齐: center;&quot;>;关于需要代码生成、重构和导入的较长且措辞不太正式的评论的建议示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 模型也可以响应复杂的注释并产生大量的代码编辑（如下所示）。生成的测试用例遵循现有的单元测试模式，同时更改评论中描述的细节。此外，编辑建议为反映测试语义的测试提供一个综合名称。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDU Yxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s1497/figure%206c .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1497&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mav hLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s16000/figure%206c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模型响应复杂评论和生成大量代码编辑的能力示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>; 在这篇文章中，我们引入了 ML 辅助功能以减少时间花费在与代码审查相关的更改上。目前，在谷歌，大量关于支持语言的可操作代码审查评论都是通过应用机器学习建议的编辑来解决的。一项针对所有 Google 开发人员的为期 12 周的 A/B 实验将进一步衡量该功能对整体开发人员生产力的影响。 &lt;/p>; &lt;p>; 我们正在努力改进整个堆栈。这包括提高模型的质量和召回率，以及在整个审查过程中提高可发现性，为开发人员构建更简化的体验。作为其中的一部分，我们正在研究在审阅者起草评论时向他们显示建议编辑的选项，并将该功能扩展到 IDE 中，以使代码更改作者能够获得针对自然语言命令的建议代码编辑。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这是 Google Core 中许多人的工作系统与体验团队、Google Research 和 DeepMind。我们要特别感谢 Peter Choy 召集合作，感谢我们所有团队成员的重要贡献和有用的建议，包括 Marcus Revaj、Gabriela Surita、Maxim Tabachnyk、Jacob Austin、Nimesh Ghelani、Dan Zheng、Peter Josling , Mariana Stariolo, Chris Gorgolewski, Sascha Varkevisser, Katja Grünwedel, Alberto Elizondo, Tobias Welp, Paige Bailey, Pierre-Antoine Manzagol, Pascal Lamblin, Chenjie Gu, Petros Maniatis, Henryk Michalewski, Sara Wiltberger, Ambar Murillo, Satish Chandra, Madhura Dudhgaonkar , Niranjan Tulpule Zoubin Ghahramani , Brett Wiltshire, Laurent Le Brun, Mingpan Guo, Hermann Loose, Jonas Mattes, Savinee Dancs。感谢 John Guilyard 在这篇博文中创建图形。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5648346519353613408/comments/default&quot; rel=&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot; rel=&quot;alternate&quot; title=&quot;Resolving使用 ML 进行代码审查评论&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11 YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s72-c/comments2code.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3261601760457492933&lt;/id>;&lt;published>;2023-05-19T09:59:00.002- 07:00&lt;/published>;&lt;updated>;2023-05-19T10:01:19.784-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;安全和隐私&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;制作 ML差分隐私模型：最佳实践和开放挑战&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google Research 高级软件工程师 Natalia Ponomareva 和 Alex Kurakin 发布&lt;/span>; &lt; img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35 T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s320/DPfy%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; 大型机器学习 (ML) 模型在现代应用程序中无处不在：来自 &lt;a href=&quot;https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails- spam-filters&quot;>;垃圾邮件过滤器&lt;/a>;到&lt;a href=&quot;https://developers.google.com/machine-learning/recommendation/overview/types&quot;>;推荐系统&lt;/a>;和&lt;a href=&quot; https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;虚拟助手&lt;/a>;。这些模型取得了显着的性能，部分原因是可用的训练数据丰富。然而，这些数据有时可能包含私人信息，包括个人身份信息、版权材料等。因此，保护​​训练数据的隐私对于实际应用 ML 至关重要。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>; (DP) 是其中之一允许以正式方式对数据匿名化进行推理的最广泛接受的技术之一。在 ML 模型的上下文中，DP 可以保证每个用户的贡献不会导致明显不同的模型。模型的隐私保证以元组 (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) 为特征，其中两者的值越小，表示 DP 保证越强，隐私性越好。 &lt;/p>; &lt;p>; 虽然有成功的&lt;a href=&quot;https://research.google/pubs/pub52351/&quot;>;例子&lt;/a>; &lt;a href=&quot;https://ai.googleblog.com /2022/02/federated-learning-with-formal.html&quot;>;使用 DP 保护训练数据&lt;/a>;，使用差分私有 ML (DP-ML) 技术获得良好的效用可能具有挑战性。首先，固有的隐私/计算权衡可能会限制模型的实用性。此外，DP-ML 模型通常需要架构和&lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;超参数&lt;/a>;调整，而有关如何有效执行此操作的指南有限或难以实现寻找。最后，不严格的隐私报告使得比较和选择最佳 DP 方法具有挑战性。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy&lt;/a>;”中，到出现在 &lt;em>;&lt;a href=&quot;https://www.jair.org/index.php/jair&quot;>;Journal of Artificial Intelligence Research&lt;/a>;&lt;/em>; 上，我们讨论了 DP-机器学习研究。我们概述了获取 DP-ML 模型的常用技术，并讨论了研究、工程挑战、缓解技术和当前未解决的问题。我们将在 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>; 和 &lt;a href=&quot;https://kdd.org/kdd2023/&quot; 上展示基于这项工作的教程>;KDD 2023&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-ML方法&lt;/h2>; &lt;p>; DP可以在ML模型开发过程中引入在三个地方：(1) 在输入数据级别，(2) 在训练期间，或 (3) 在推理阶段。每个选项都在 ML 开发过程的不同阶段提供隐私保护，最弱的是在预测级别引入 DP 时，最强的是在输入级别引入时。使输入数据差分隐私意味着任何基于此数据训练的模型也将具有 DP 保证。在训练期间引入 DP 时，只有该特定模型具有 DP 保证。 DP在预测层面是指只保护模型的预测，而模型本身没有差分隐私。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2 hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s1821/image2.png&quot; style=&quot;margin-左：自动；右边距：自动；”>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;1821&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQ gf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;引入 DP 的任务从左到右逐渐变得容易。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; DP 通常在训练期间引入（DP-training）。 &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;梯度噪声注入&lt;/a>;方法，例如&lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP- SGD&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;，及其扩展是目前在大型等复杂模型中实现 DP 保证的最实用方法深度神经网络。 &lt;/p>; &lt;p>; DP-SGD 建立在&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;随机梯度下降&lt;/a>; (SGD) 优化器的基础上，有两个修改：( 1) 每个示例的梯度被裁剪到一定的范数以限制敏感性（单个示例对整体模型的影响），这是一个缓慢且计算密集的过程，以及 (2) 噪声梯度更新是通过采用聚合形成的梯度并添加与敏感度和隐私保证强度成正比的噪声。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJ HKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s1439/DP-ML.gif”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;494&quot; data-original-width=&quot;1439&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTR vLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s16000/DP-ML.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;DP-SGD 是对 SGD 的一种修改，它涉及 a) 剪裁每个示例的梯度以限制灵敏度，以及 b) 添加噪声，根据灵敏度和隐私保证进行校准，以聚合梯度，在梯度更新步骤之前。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;现有的DP训练挑战&lt;/h2>; &lt;p>; 梯度噪声注入方法通常表现出：(1) 效用损失，(2) 训练速度变慢，以及 (3) &lt;a href=&quot;https://en.wikipedia.org/wiki/ Memory_footprint&quot;>;内存占用量&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;效用损失&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;减少效用下降的最佳方法是使用更多计算。使用更大的批量大小和/或更多的迭代是提高模型性能的最突出和最实用的方法之一。超参数调整也非常重要，但经常被忽视。 DP 训练模型的效用对添加的噪声总量敏感，这取决于超参数，如裁剪范数和批量大小。此外，应该重新调整其他超参数（如学习率）以考虑有噪声的梯度更新。 &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;另一种选择是获取更多数据或使用类似分布的公共数据。这可以通过利用公开可用的检查点来完成，例如 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/pdf/ 1910.10683.pdf&quot;>;T5&lt;/a>;，并使用私有数据对其进行微调。 &lt;/p>; &lt;p>; &lt;strong>;较慢的训练&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;大多数梯度噪声注入方法通过剪裁每个示例的梯度来限制灵敏度，显着减慢向下&lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;>;反向传播&lt;/a>;。这可以通过选择一个高效的 DP 框架来解决，该框架可以有效地实现每个示例的裁剪。 &lt;/p>; &lt;p>; &lt;strong>;内存占用增加&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;DP 训练需要大量内存来计算和存储每个示例的梯度。此外，它需要大得多的批次才能获得更好的效用。增加计算资源（例如，加速器的数量和大小）是满足额外内存需求的最简单解决方案。或者，&lt;a href=&quot;https://arxiv.org/abs/2109.12298&quot;>;一些&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2201.12328&quot;>;作品&lt;/a>;提倡梯度累积，其中在应用梯度更新之前组合较小的批次以模拟较大的批次。此外，一些算法（例如，&lt;a href=&quot;https://arxiv.org/pdf/2110.05679.pdf&quot;>;ghost clipping&lt;/a>;，它基于 &lt;a href=&quot;https://arxiv.org /abs/1510.01799&quot;>;本文&lt;/a>;) 完全避免每个示例的梯度裁剪。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;最佳实践&lt;/h2>; &lt;p>; 以下最佳实践可以达到严格的DP保证，最好模型效用可能。 &lt;/p>; &lt;p>; &lt;strong>;选择正确的隐私单元：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;首先，我们应该清楚模型的隐私保证。这是通过选择代表相邻数据集概念（即只有一行不同的数据集）的“隐私单元”来编码的。示例级保护是研究文献中的常见选择，但如果单个用户向训练数据集贡献了多个记录，则对于用户生成的数据可能并不理想。对于这种情况，用户级保护可能更合适。对于文本和序列数据，单元的选择更加困难，因为在大多数应用中，单个训练示例与文本中嵌入的语义不一致。 &lt;/p>; &lt;p>; &lt;strong>;选择隐私保障：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;我们概述了隐私保障的三大层级，并鼓励从业者选择最低的以下可能的层级：&lt;/p>; &lt;ul style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;层级 1 — 强大的隐私保证：&lt;/em>; 选择 &lt;em>;ε&lt;/em>; ≤ 1 提供一个强大的隐私保证，但经常导致大型模型的效用显着下降，因此可能只适用于较小的模型。 &lt;/li>;&lt;li>;&lt;em>;第 2 层 — 合理的隐私保证&lt;/em>;：我们提倡当前未记录但仍被广泛使用的 DP-ML 模型目标，以实现 &lt;em>;ε&lt;/em>; ≤ 10。&lt;/li>;&lt;li>;&lt;em>;第 3 层 — 弱隐私保证&lt;/em>;：任何有限 &lt;em>;ε&lt;/em>; 都是对没有正式隐私保证的模型的改进。然而，对于 &lt;em>;ε&lt;/em>; >; 10，单独的 DP 保证不能作为数据匿名化的充分证据，可能需要额外的措施（例如，经验隐私审计）来确保模型保护用户数据。 &lt;/li>; &lt;/ul>; &lt;p>; &lt;strong>;超参数调整&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;选择超参数需要优化三个​​相互依赖的目标：1 ) 模型效用，2) 隐私成本 &lt;em>;ε&lt;/em>;，以及 3) 计算成本。常见的策略以三者中的两个为约束，重点优化第三个。我们提供的方法可以通过有限次数的试验来最大化效用，例如，根据隐私和计算约束进行调整。 &lt;/p>; &lt;p>; &lt;strong>;报告隐私保证：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;许多关于 DP 的工作仅用于 ML 报告&lt;em>;ε&lt; /em>; 和可能的 &lt;em>;δ &lt;/em>; 训练过程的值。但是，我们认为从业者应该提供模型保证的全面概述，包括：&lt;/p>; &lt;ol style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;DP 设置：&lt;/em>; 结果是假设中央 DP 与受信任的服务提供商、&lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;本地 DP&lt;/a>; 或其他设置？ &lt;/li>;&lt;li>;&lt;em>;实例化 DP 定义：&lt;/em>; &lt;ol type=&quot;a&quot;>; &lt;li>;&lt;em>;涵盖的数据访问：&lt;/em>;DP 保证是否（仅）适用于单个训练运行或还涵盖超参数调整等。&lt;/li>;&lt;li>;&lt;em>;最终机制的输出&lt;/em>;：隐私保证涵盖的内容可以公开发布（例如，模型检查点、完整序列私有化梯度等）&lt;/li>;&lt;li>;&lt;em>;隐私单元&lt;/em>;：选择的“隐私单元”（示例级、用户级等）&lt;/li>;&lt;li>; DP“相邻”数据集的&lt;em>;邻接定义&lt;/em>;：描述相邻数据集的不同之处（例如，添加或删除、替换一、零出一）。 &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;隐私核算细节：&lt;/em>;提供核算细节，例如组成和放大，对于方法之间的正确比较很重要，应包括：&lt;ol 类型=&quot;a&quot;>; &lt;li>;使用的会计类型，例如&lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;会计、PLD会计等&lt;/ li>;&lt;li>;会计假设及其是否成立（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_regression#Maximum_likelihood-based_pa​​rameter_estimation&quot;>;Poisson&lt;/a>; 抽样假设用于隐私放大，但训练中使用了数据改组）。 &lt;/li>;&lt;li>;模型和调整过程的正式 DP 声明（例如，特定的&lt;em>;ε&lt;/em>;、&lt;em>;δ&lt;/em>;-DP 或&lt;em>;&lt;a href=&quot;https ://arxiv.org/pdf/1605.02065.pdf&quot;>;ρ-zCDP&lt;/a>;&lt;/em>; 值）。 &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;透明度和可验证性：&lt;/em>;如果可能，使用标准 DP 库的关键机制实现和会计组件的完整开源代码。 &lt;/li>; &lt;/ol>; &lt;p>; &lt;strong>;注意所有使用的组件：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;通常，DP-training 是一个直接应用 DP-SGD 或其他算法。然而，ML 模型中经常使用的一些组件或损失（例如，&lt;a href=&quot;https://www.baeldung.com/cs/contrastive-learning#:~:text=Contrastive%20Loss,and%20dissimilar% 20samples%20far%20apart.&quot;>;contrastive losses&lt;/a>;，&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;图形神经网络&lt;/a>;层）应该被检查以确保隐私不违反保证。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开放式问题&lt;/h2>; &lt;p>; 虽然 DP-ML 是一个活跃的研究领域，但我们强调有改进空间的广泛领域。 &lt;/p>; &lt;p>; &lt;strong>;开发更好的会计方法&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;我们目前对 DP 训练的理解 &lt;em>;ε&lt;/em >;，&lt;em>;δ&lt;strong>; &lt;/strong>;&lt;/em>;保证&lt;strong>; &lt;/strong>;依赖于多种技术，例如 Rényi DP 组合和隐私放大。我们相信，现有算法的更好计算方法将证明 ML 模型的 DP 保证实际上比预期的要好。 &lt;/p>; &lt;p>; &lt;strong>;开发更好的算法：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;使用梯度噪声注入进行 DP 训练的计算负担来自需要使用更大的批次并限制每个示例的灵敏度。开发可以使用较小批次或确定其他方式（除了每个示例裁剪）来限制灵敏度的方法将是 DP-ML 的突破。 &lt;/p>; &lt;p>; &lt;strong>;更好的优化技术&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;直接应用相同的 DP-SGD 配方被认为不是自适应的最佳选择优化器，因为为梯度私有化而添加的噪声可能会在学习率计算中累积。设计基于理论的 DP 自适应优化器仍然是一个活跃的研究课题。另一个潜在方向是更好地理解 DP 损失的表面，因为对于标准（非 DP）ML 模型，平坦区域已被证明可以&lt;a href=&quot;https://arxiv.org/abs/2010.01412&quot;>;泛化&lt;/ a>; &lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;>;更好&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;识别对噪声更稳健的架构&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;有机会更好地了解我们是否需要在引入 DP 时调整现有模型的架构。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们的&lt;a href=&quot;https://arxiv.org /abs/2303.00654&quot;>;调查报告&lt;/a>; 总结了与 ML 模型 DP 相关的当前研究，并提供了有关如何实现最佳隐私与实用程序权衡的实用技巧。我们希望这项工作能够为想要有效地将 DP 应用于复杂 ML 模型的从业者提供参考点。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢 Hussein Hazimeh、Zheng Xu 和 Carson Denison 、H. Brendan McMahan、Sergei Vassilvitskii、Steve Chien 和 Abhradeep Thakurta、Badih Ghazi、Chiyuan Zhang 帮助准备这篇博文、论文和教程内容。感谢 John Guilyard 在这篇博文中制作图片，感谢 Ravi Kumar 的评论。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3261601760457492933/comments /default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models- differentially-private.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/3261601760457492933&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;self &quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot; rel=&quot;alternate&quot; title= “使 ML 模型具有差异化隐私：最佳实践和公开挑战” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161 &lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:/ /img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS 5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s72-c/DPfy%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-234098392689663352&lt;/id>;&lt;已发布>;2023-05-18T14:08:00.000-07:00&lt;/published>;&lt;更新>;2023-05-18T14:08:39.899-07:00&lt;/更新>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;联合视频和图像视觉转换器的稀疏视频管&lt;/ stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google 研究科学家 AJ Piergiovanni 和 Anelia Angelova 发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANDX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDej tp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s320/TubeViT%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 视频理解是一个具有挑战性的问题，需要对空间信息（例如，对于场景中的对象，包括它们的位置和关系）和 &lt;a href=&quot;https://en.wikipedia.org/wiki /Temporal_information_retrieval#:~:text=Temporal%20information%20retrieval%20(T%2DIR,of%20the%20user%20information%20needs.&quot;>;视频中显示的活动或事件的时间信息&lt;/a>;。有很多视频理解应用程序和任务，例如&lt;a href=&quot;https://cloud.google.com/video-intelligence&quot;>;理解网络视频的语义内容&lt;/a>;和&lt;a href=&quot;https://ai .googleblog.com/2022/02/robot-see-robot-do.html&quot;>;机器人感知&lt;/a>;。但是，当前的工作，例如 &lt;a href=&quot;https://arxiv.org/abs/2103.15691 &quot;>;ViViT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2102.05095&quot;>;TimeSFormer&lt;/a>;，对视频进行密集处理并需要大量计算，尤其是在模型大小加上视频长度和分辨率时&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;重新思考视频 ViT：用于联合的稀疏视频管图像和视频学习&lt;/a>;”，将在 &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; 上展示，我们介绍了一种简单的技术，可以将 &lt; href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer&lt;/a>; (ViT) 将图像编码器模型转换为使用稀疏视频管（视频样本的可学习视觉表示）的高效视频主干，以减少模型的计算需求。这种方法可以无缝地处理图像和视频，这使得它可以在训练期间利用图像和视频数据源。这种训练进一步使我们的稀疏管 ViT 模型能够将图像和视频骨干结合在一起，根据输入充当图像或视频骨干（或两者）的双重角色。我们证明该模型具有可扩展性，无需完全微调即可适应大型预训练 ViT，并在许多视频分类基准测试中取得了最先进的结果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o 7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s860/image5.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;219&quot; data-original-width=&quot;860&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5P J823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用稀疏视频管对视频进行采样，并结合标准的 ViT 编码器，产生可以与图像输入无缝共享的高效视觉表示。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;构建联合图像-视频主干&lt;/h2>; &lt;p>; 我们的稀疏管 ViT 使用标准 ViT 主干，由一堆 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network. html&quot;>;Transformer&lt;/a>; 层，处理视频信息。以前的方法，例如 ViViT，对视频进行密集标记，然后应用&lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;分解注意力&lt;/a>;，即单独计算每个标记的注意力权重对于时间和空间维度。在标准的 ViT 架构中，self-attention 是在整个 token 序列上计算的。当使用视频作为输入时，令牌序列变得很长，这会使计算变慢。相反，在我们提出的方法中，视频是使用 &lt;em>;视频管&lt;/em>; 进行稀疏采样的，视频管是视频中各种形状和大小的 3D 可学习视觉表示（在下面有更详细的描述）。这些管用于使用&lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/strided-convolutions-wfUhx&quot;>;大时间跨度&lt;/a>;对视频进行稀疏采样，即，当管内核仅应用于视频中的几个位置，而不是每个像素。 &lt;/p>; &lt;p>; 通过对视频管进行稀疏采样，我们可以使用相同的全局自注意力模块，而不是像 ViViT 那样分解注意力。我们通过实验表明，由于未初始化的权重，添加分解注意力层可能会损害性能。 ViT 骨干网中的这种单一变压器层堆栈还可以更好地共享权重并提高性能。稀疏视频管采样是通过使用在固定网格上选择标记的大空间和时间步长来完成的。大步幅减少了整个网络中的令牌数量，同时仍然捕获空间和时间信息并实现所有令牌的高效处理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;稀疏视频管&lt;/h2>; &lt;p>; 视频管是基于 3D 网格的长方体，可以有不同的形状或类别，并以可以重叠的步幅和起始位置捕获不同的信息。在模型中，我们使用三种不同的管状来捕获：(1) 仅空间信息（产生一组 2D 图像块），(2) 长时间信息（在小空间区域），以及 (3) 空间信息和时间信息一样。仅捕获空间信息的管可以应用于图像和视频输入。捕获长时间信息或同时捕获时间和空间信息的管仅适用于视频输入。根据输入视频的大小，将三个管形多次应用于模型以生成标记。 &lt;/p>; &lt;p>; 固定位置嵌入，它捕获每个管相对于所有其他管的全局位置（包括任何步幅、偏移量等），应用于视频管。与之前学习的位置嵌入不同，这个固定的嵌入更好地实现了稀疏、重叠的采样。捕获管子的全局位置有助于模型了解每个管子的来源，这在管子重叠或从远处的视频位置采样时特别有用。接下来，将管特征连接在一起以形成一组 &lt;em>;N&lt;/em>; 标记。这些令牌由标准的 ViT 编码器处理。最后，我们应用注意力池将所有标记压缩为单个表示，并输入到全连接 (FC) 层以进行分类（例如，踢足球、游泳等）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP 1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr- Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s474/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;474&quot; data-original-width=&quot;346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC 3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s16000/image2 .png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的视频 ViT 模型通过对稀疏视频管进行采样来工作从视频（显示在底部），使图像或视频输入中的一个或两个能够被无缝处理。这些管子具有不同的形状并捕捉不同的视频特征。管 1（&lt;strong>;黄色&lt;/strong>;）仅捕获空间信息，从而生成一组可应用于图像输入的 2D 色块。管 2（&lt;strong>;红色&lt;/strong>;）捕获时间信息和一些空间信息，管 3（&lt;strong>;绿色&lt;/strong>;）同样捕获时间和空间信息（即管的空间大小 &lt;em >;x&lt;/em>; 和&lt;em>;y&lt;/em>; 与帧数相同&lt;em>;t&lt;/em>;）。管 2 和 3 只能应用于视频输入。位置嵌入被添加到所有管特征中。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;Scaling video ViTs&lt;/h2>; &lt;p>; 构建视频主干的过程需要大量计算，但我们的稀疏管 ViT 模型可以利用先前训练的图像主干，实现视频模型的计算高效缩放。由于图像主干可以适应视频主干，因此大型图像主干可以变成大型视频主干。更具体地说，可以将学习到的视频特征表示从一个小管 ViT 转移到一个大的预训练图像 ViT，并且只需几个步骤就可以用视频数据训练生成的模型，而不是从头开始进行完整的训练。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27 B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s899/image6 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;899&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0 XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的方法能够以更有效的方式缩放稀疏管 ViT。具体来说，可以将来自小型视频 ViT（&lt;strong>;top 网络&lt;/strong>;）的视频特征转移到大型预训练图像 ViT（&lt;strong>;bottom 网络&lt;/strong>;），并进一步微调.这需要更少的训练步骤来实现大型模型的强大性能。这是有益的，因为从头开始训练大型视频模型可能会非常昂贵。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们使用&lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;>;动力学评估我们的稀疏管 ViT 方法-400&lt;/a>;（如下所示）、Kinetics-600 和 Kinetics-700 数据集，并将其性能与一长串现有方法进行比较。我们发现我们的方法优于所有先前的方法。重要的是，它优于所有在图像+视频数据集上联合训练的最先进方法。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhw style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBV bXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;与流行的 Kinetics-400 视频数据集上的几个先前作品相比的性能。我们的稀疏管 ViT 优于最先进的方法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外，我们在 &lt;a href=&quot;https ://developer.qualcomm.com/software/ai-datasets/something-something&quot;>;Something-Something V2&lt;/a>; 数据集，通常用于评估更多动态活动，并且还报告说它优于所有先前的状态-最先进的方法。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBX Lwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s600/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD2 0X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;在 Something-Something V2 视频数据集上的表现。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可视化一些已学习的内核&lt;/h2>; &lt;p>; 了解所提出的模型正在学习什么样的基本特征很有趣。我们在下面将它们可视化，显示了为图像和视频以及视频管共享的 2D 补丁。这些可视化显示投影层捕获的 2D 或 3D 信息。例如，在 2D 贴片中，检测到各种常见特征，如边缘和颜色，而 3D 管捕捉基本形状以及它们如何随时间变化。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-Jio R5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s549/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;285&quot; data-original-width=&quot;549&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu 33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;补丁和管的可视化学习了稀疏管 ViT 模型。顶行是 2D 补丁，其余两行是学习视频管的快照。管显示应用了它们的 8 或 4 帧的每个补丁。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了一种新的稀疏管 ViT，它可以将 ViT 编码器转变为高效的视频模型，并且可以无缝地处理图像和视频输入。我们还展示了大型视频编码器可以从小型视频编码器和纯图像 ViTs 中引导。我们的方法在几个流行的视频理解基准测试中优于之前的方法。我们相信，这种简单的表示可以促进更有效地学习输入视频，无缝地结合图像或视频输入，并有效地消除图像和视频模型的分叉，以促进未来的多模态理解。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作由 AJ Piergiovanni、Weicheng Kuo 进行和现在在谷歌 DeepMind 工作的 Anelia Angelova。我们感谢 Abhijit Ogale、周洛薇、Claire Cui 和我们在 Google Research 的同事们的有益讨论、评论和支持。&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http:/ /ai.googleblog.com/feeds/234098392689663352/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog. com/2023/05/sparse-video-tubes-for-joint-video-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href= &quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger. com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video -tubes-for-joint-video-and.html&quot; rel=&quot;alternate&quot; title=&quot;用于联合视频和图像视觉转换器的稀疏视频管&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt; /name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http:/ /schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>; &lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANDX6MfoZp-jzVsETmH6kgy_Tvni i7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s72-c/TubeViT%20hero.jpg&quot; width= &quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>; tag:blogger.com,1999:blog-8474926331452026626.post-2749680625311121514&lt;/id>;&lt;published>;2023-05-18T10:12:00.006-07:00&lt;/published>;&lt;updated>;2023-05-18T16:14: 50.453-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT 亮点&quot;>;&lt;/category>;&lt;category scheme=&quot;http:/ /www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google Research 的 Responsible AI：PAIR&lt;/stitle>;&lt;content type=&quot;html&quot;>; &lt;span class=&quot;byline-author&quot;>;由 Google Research PAIR 联合负责人 Lucas Dixon 和 Michael Terry 发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DSHetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Q l5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; PAIR（人 + 人工智能研究）于 2017 年&lt;a href=&quot;https://blog.google/technology/ai/pair-people-ai-research-initiative/&quot;>;推出&lt;/a>;相信“如果我们在流程开始时就以人为本来构建系统，人工智能可以走得更远——对我们所有人更有用。”我们继续专注于让 AI 更易于理解、解释、有趣，并为世界各地更多的人所用。鉴于 &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;生成式 AI&lt;/a>; 和 &lt;a href=&quot;https://en.wikipedia .org/wiki/Chatbot&quot;>;聊天机器人&lt;/a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 如今，PAIR 是&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;负责任的 AI 的一部分，并且Google Research 以人为本的技术&lt;/a>;团队，我们的工作跨越这个更大的研究空间：我们推进&lt;a href=&quot;https://pair.withgoogle.com/research/&quot;>;基础研究&lt;/a>;人机交互 (HAI) 和机器学习 (ML)；我们发布教育材料，包括 &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;PAIR Guidebook&lt;/a>; 和 &lt;a href=&quot;https://pair.withgoogle.com/explorables/ &quot;>;Explorables&lt;/a>;（例如最近的 Explorable 着眼于&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;模型有时如何以及为何自信地做出错误预测&lt;/a >;);我们开发了&lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; 等软件工具来帮助人们理解和调试 ML 行为。我们今年的灵感是“改变人们对 &lt;em>;他们&lt;/em>; 可以用 AI 做什么的看法。”这一愿景的灵感来自迅速兴起的生成式 AI 技术，例如支持 &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; 等聊天机器人的大型语言模型 (LLM)，以及新的生成媒体模型，例如 Google 的 &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;、&lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/ a>; 和 &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;。在这篇博文中，我们回顾了最近正在改变方式的 PAIR 工作&lt;/p>; &lt;br />; &lt;h2>;生成式 AI 研究&lt;/h2>; &lt;p>; 生成式 AI 正在引起人们的极大兴趣，PAIR 参与了一系列相关研究，从 &lt;a href =&quot;https://arxiv.org/abs/2304.03442&quot;>;使用语言模型创建生成代理&lt;/a>;&amp;nbsp;研究艺术家如何采用生成图像模型，如 &lt;a href=&quot;https://imagen.research. google/&quot;>;Imagen&lt;/a>; 和 &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;。后者的“文本到图像”模型让用户输入模型生成图像的基于文本的描述（例如，“卡通风格的森林中的姜饼屋”）。在即将发表的题为“&lt;a href=&quot;https://arxiv.org/abs/2303.12253&quot;>;The Prompt Artists&lt;/a>;”的论文中（出现在 &lt;a href=&quot;https://cc.acm.org /2023/&quot;>;Creativity and Cognition 2023&lt;/a>;)，我们发现生成图像模型的用户不仅努力创造美丽的图像，而且努力创造独特、创新的风格。为了帮助实现这些风格，有些人甚至会寻找独特的词汇来帮助发展他们的视觉风格。例如，他们可能会访问建筑博客以了解他们可以采用哪些特定领域的词汇来帮助制作独特的建筑图像。 &lt;/p>; &lt;p>; 我们还在研究即时创作者所面临挑战的解决方案，这些创作者使用生成式 AI，本质上是在不使用编程语言的情况下进行编程。例如，我们开发了&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;新方法&lt;/a>;从自然语言提示。我们已将这些结构应用于提示编辑器，以提供类似于其他编程环境中的功能，例如语义突出显示、自动建议和结构化数据视图。 &lt;/p>; &lt;p>; 生成式 LLM 的发展也开辟了解决长期存在的重要问题的新技术。 &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;敏捷分类器&lt;/a>;是我们利用 LLM 的语义和句法优势来解决与更安全的在线话语相关的分类问题的一种方法，例如，在新类型的有毒语言在网上发展时尽快灵活地阻止它。这里的一大进步是能够从非常小的数据集（小至 80 个示例）开发高质量的分类器。这表明在线讨论的美好未来和更好的节制：与其收集数百万个示例以尝试在数月或数年内为所有用例创建通用安全分类器，不如由个人或小型组织创建更灵活的分类器并针对他们的具体用例，并在一天的时间跨度内进行迭代和调整（例如，阻止收到一种新的骚扰或纠正模型中的意外偏见）。作为实用性的一个例子，这些方法最近 &lt;a href=&quot;https://www.aclweb.org/portal/content/semeval-2023-task-10-explainable-detection-online-sexism-edos&quot;>;赢得了SemEval 竞赛&lt;/a>; 以识别和解释性别歧视。 &lt;/p>; &lt;p>; 我们还开发了&lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>;新的最先进的可解释性方法&lt;/a>;来确定训练的作用有关模型行为和不当行为的数据。通过&lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;将训练数据归因方法与敏捷分类器相结合&lt;/a>;，我们还发现我们可以识别错误标记的训练示例。这使得减少训练数据中的噪声成为可能，从而显着提高模型精度。 &lt;/p>; &lt;p>; 总的来说，这些方法对于帮助科学界改进生成模型至关重要。他们提供快速有效的内容审核技术和对话安全方法，帮助支持内容是生成模型惊人结果基础的创作者。此外，它们还提供直接工具来帮助调试模型错误行为，从而生成更好的模型。 &lt;/p>; &lt;br />; &lt;h2>;可视化和教育&lt;/h2>; &lt;p>; 为了降低理解 ML 相关工作的障碍，我们定期设计和发布高度可视化、交互式的在线文章，称为 &lt;a href=&quot;https ://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;，提供可访问的实践方法来了解 ML 中的关键思想。例如，我们最近发布了关于模型置信度和意外偏差主题的新 AI Explorables。在我们最新的 Explorable“&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;”中，我们讨论了模型置信度问题：模型有时可能对他们的预测&lt;em>;非常&lt;/em>;有信心……但完全不正确。为什么会发生这种情况，对此可以做些什么？我们的 Explorable 通过交互式示例解决了这些问题，并展示了我们如何使用一种名为 &lt;a href=&quot;https://ai.googleblog.com/2021/11/model- ensembles-are-faster-than-you.html&quot;>;ensembling&lt;/a>;，它通过平均多个模型的输出来工作。另一个 Explorable，“&lt;a href=&quot;https://pair.withgoogle.com/explorables/saliency/&quot;>;Searching for Unintended Biases with Saliency&lt;/a>;”，展示了虚假相关如何导致意外偏差——以及技术如何例如显着性图可以检测数据集中的一些偏差，但需要注意的是，当偏差在训练集中更加微妙和零散时，很难看到偏差。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DSHetIblRhW0hvTaPvDmK2f0SDu9XcLCVg 0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;543&quot; data-original-width=&quot;724&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DSHetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5 xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;PAIR 设计并发布 AI Explorables、关于及时主题和 ML 研究新方法的互动文章，例如“&lt;a href=&quot;https://pair.withgoogle.com/ explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;&quot;，它着眼于模型如何以及为何以高置信度提供不正确的预测，以及“集成”许多模型的输出如何帮助避免这种情况。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;透明度和数据卡片剧本&lt;/h2 >; &lt;p>; 为了继续推进我们帮助人们理解 ML 的目标，我们提倡透明的文档。过去，PAIR 和 Google Cloud 开发了&lt;a href=&quot;http://modelcards.withgoogle.com&quot;>;模型卡&lt;/a>;。最近，我们在 &lt;a href=&quot;https://facctconference.org/ 2022/&quot;>;ACM FAccT&#39;22&lt;/a>; 并开源了&lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;，这是与 &lt; a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;技术、人工智能、社会和文化团队&lt;/a>; (TASC)。 &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>; 是参与式活动和框架的工具包，可帮助团队和组织在建立透明度工作时克服障碍。它是使用一种迭代的、多学科的方法创建的，该方法植根于谷歌 20 多个团队的经验，并带有四个模块：询问、检查、回答和审计。这些模块包含各种资源，可帮助您根据组织的需要自定义数据卡：&lt;/p>; &lt;ul>; &lt;li>;18 基础：任何人都可以在任何数据集类型上使用的可扩展框架&lt;/li>;&lt;li>;19透明度模式：大规模制作高质量数据卡的循证指南&lt;/li>;&lt;li>;33 参与式活动：应对团队透明度挑战的跨职能研讨会&lt;/li>;&lt;li>;互动实验室：生成互动数据来自浏览器中 markdown 的卡片 &lt;/li>; &lt;/ul>; &lt;p>; Data Cards Playbook 可作为初创公司、大学和其他研究小组的学习途径。 &lt;/p>; &lt;br />; &lt;h2>;软件工具&lt;/h2>; &lt;p>; 我们的团队致力于创建工具、工具包、库和可视化，以扩展对 ML 模型的访问并提高理解。 &lt;a href=&quot;https://knowyourdata.withgoogle.com/&quot;>;Know Your Data&lt;/a>; 就是这样一种资源，它允许研究人员通过对他们可以使用的数据集进行交互式定性探索来测试模型在各种场景下的性能找到并修复意外的数据集偏差。 &lt;/p>; &lt;p>; 近日，PAIR 发布了新版&lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT)，用于模型调试和理解。 LIT v0.5 提供对图像和表格数据的支持、表格特征属性的新解释器、用于多面数据探索的“Dive”可视化以及允许 LIT 扩展到 100k 数据集条目的性能改进。您可以找到&lt;a href=&quot;https://github.com/PAIR-code/lit/blob/main/RELEASE.md&quot;>;发行说明&lt;/a>;和&lt;a href=&quot;https://pair- GitHub 上的 code.github.io/lit/&quot;>;code&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaG pKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s1920 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ8 2aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR 的&lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;学习可解释性工具&lt;/a>; (LIT)，一个用于可视化和理解 ML 模型的开源平台。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; PAIR 也为 &lt;a href=&quot;https:// developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;，一种使用提示编程的 LLM 快速原型制作工具。 MakerSuite 以我们早期对 &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;PromptMaker&lt;/a>; 的研究为基础，后者在 &lt;a href=&quot;https: //chi2022.acm.org/&quot;>;CHI 2022。&lt;/a>;MakerSuite 通过扩大可以创作这些原型的人员类型并将制作原型模型所花费的时间从几个月缩短到几分钟，降低了制作 ML 应用程序原型的障碍。&amp;nbsp ;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody >;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZ w7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp- xIMif0rhtASefVZZXOSQ/s1216/reversedictionary.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;918&quot; data-original-width=&quot;1216&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP _7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s16000/reversedictionary.png&quot; />;&lt;/a>;&lt;/td>; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MakerSuite 的屏幕截图，这是一种使用基于提示的编程快速构建新 ML 模型原型的工具，它源于 PAIR 的及时进行编程研究。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;正在进行的工作&lt;/h2>; &lt;p>; 随着 AI 世界的快速发展，PAIR 很高兴继续开发新的工具、研究和教育材料，以帮助改变人们对他们可以用人工智能做什么的看法。 &lt;/p>; &lt;p>; 例如，我们最近与五位设计师进行了&lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;一项探索性研究&lt;/a>;（在&lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI&lt;/a>; 今年）研究了没有 ML 编程经验或培训的人如何使用提示编程来快速制作功能用户界面模型的原型。这种原型制作速度可以帮助设计人员了解如何将 ML 模型集成到产品中，并使他们能够在产品设计过程中更快地进行用户研究。 &lt;/p>; &lt;p>; 基于这项研究，PAIR 的研究人员构建了 &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser&lt;/a>;，这是一个用于编写 LLM 的设计工具插件-注入模型。该插件引入了两种新颖的 LLM 交互：输入输出，使内容具有交互性和动态性，以及框架变化，它根据用户的自然语言输入将用户引导到不同的框架。结果是更紧密地集成了 UI 和 ML 原型，所有这些都在一个界面中。 &lt;/p>; &lt;p>; AI 的最新进展代表了研究人员为他们的研究目的和目标定制和控制模型的容易程度的重大转变。这些能力正在改变我们思考与 AI 交互的方式，他们创造为研究界提供了许多新机会。 PAIR 对我们如何利用这些功能让更多人更容易使用 AI 感到很兴奋。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 PAIR 中的每个人，感谢 Reena Jana 和我们所有的合作者。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2749680625311121514/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html#comment-form&quot; rel=&quot;replies&quot; title =&quot;0 评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;edit&quot; type=&quot;application/atom +xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href= &quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research 的 Responsible AI：PAIR&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd :image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot; 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DSHetIblRhW0hvTaPvDmK2f 0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s72 -c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2478354033845809100&lt;/id>;&lt;published>;2023-05-16T12:22:00.001-07:00&lt;/published>;&lt;更新>;2023-05-16T12:23:42.794-07:00&lt;/更新>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;强化学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;在开放式对话中使用强化学习进行动态规划&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author &quot;>;Google Research 研究员 Deborah Cohen 和首席科学家 Craig Boutilier 发表&lt;/span>; lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX -7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s1650/rlxtalk.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 随着虚拟助手变得无处不在，用户越来越多地与他们互动以了解新主题或获得建议，并期望他们提供超越一两轮狭隘对话的能力。动态规划，即根据对话的流程进行展望和重新规划的能力，是进行具有用户期望的更深入、开放式交互的引人入胜的对话的基本要素。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 虽然大型语言模型 (LLM) 现在在许多自然语言处理基准测试中击败了最先进的方法，但它们通常经过训练以输出下一个最佳响应，而不是提前计划，这是多回合交互所必需的。然而，在过去几年中，&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;>;强化学习&lt;/a>; (RL) 在解决涉及动态规划的特定问题时取得了令人难以置信的成果，例如作为赢得比赛和蛋白质折叠。 &lt;/p>; &lt;p>; 今天，我们将分享我们在&lt;a href=&quot;https://arxiv.org/abs/2208.02294&quot;>;人机对话动态规划&lt;/a>;方面的最新进展，其中我们使助手能够针对目标规划多轮对话，并通过采用基于 RL 的方法实时调整该计划。在这里，我们看看如何通过应用 RL 来根据从信誉良好的来源中提取的信息来撰写答案，而不是依赖于语言模型生成的内容，从而改善长时间的交互。我们希望这项工作的未来版本可以在多轮对话中结合 LLM 和 RL。由于建模的复杂性、巨大的状态和动作空间以及设计奖励函数的显着微妙性，在大规模对话系统中“在野外”部署 RL 被证明是一项艰巨的挑战。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;什么是动态规划？&lt;/h2>; &lt;p>; 许多类型的对话，从收集信息到提供信息建议，需要一种灵活的方法，并能够根据其流程修改对话的原始计划。这种在谈话过程中换档的能力被称为&lt;em>;动态计划&lt;/em>;，与&lt;em>;静态计划&lt;/em>;相反，后者指的是一种更固定的方法。例如，在下面的对话中，目标是通过分享有关酷动物的有趣事实来吸引用户。首先，助理通过声音测验将话题引向鲨鱼。鉴于用户对鲨鱼不感兴趣，助手随后制定了更新计划并将对话转向海狮、狮子，然后是猎豹。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3 imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;908&quot; data-original-width=&quot;890&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0y BjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;助手动态修改其原定计划以谈论鲨鱼并分享有关其他动物的事实。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;动态组合&lt;/h2>; &lt;p>; 为了应对对话探索的挑战，我们将辅助响应的生成分为两部分：1) &lt; em>;内容生成&lt;/em>;，从信誉良好的来源中提取相关信息，&lt;em>; &lt;/em>;和 2) &lt;em>;将此类内容灵活组合&lt;/em>;到辅助响应中。我们将这种分为两部分的方法称为&lt;em>;&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;动态组合&lt;/a>;&lt;/em>;。与 LLM 方法不同，此方法使助手能够完全控制其可能提供的内容的来源、正确性和质量。同时，它可以通过学习对话管理器选择和组合最合适的内容来实现灵活性。 &lt;/p>; &lt;p>; 在较早的一篇论文“&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;Dynamic Composition for Conversational Domain Exploration&lt;/a>;”中，我们描述了一种新颖的方法，它包括：(1) 内容提供者的集合，提供来自不同来源的候选人，例如新闻片段、&lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;知识图谱&lt;/a>;事实和问题； (2) 对话管理器； (3) 句子融合模块。每个助理响应都由对话管理器逐步构建，对话管理器选择内容提供者提出的候选者。然后将选定的话语序列融合成一个连贯的响应。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;使用 RL 进行动态规划&lt;/h2>; &lt;p>; 辅助响应组合循环的核心是使用 &lt;em>;off-policy RL&lt;/em>; 训练的对话管理器，即一种评估和改进与代理使用的策略不同的策略的算法（在我们的例子中，后者基于监督模型）。将 RL 应用于对话管理提出了几个挑战，包括大的状态空间（因为状态代表对话状态，需要考虑整个对话历史）和有效无界的动作空间（可能包括自然界中所有现有的单词或句子）语言）。 &lt;/p>; &lt;p>; 我们使用一种新颖的 RL 结构来应对这些挑战。首先，我们利用强大的监督模型——具体来说，&lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;>;递归神经网络&lt;/a>; (RNN) 和 &lt;a href=&quot;https:// ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; — 提供简洁有效的对话状态表示。这些状态编码器被输入对话历史，由一系列用户和助手回合组成，并以 &lt;a href=&quot;https://en.wikipedia.org/wiki/ Latent_space&quot;>;潜在向量&lt;/a>;。 &lt;/p>; &lt;p>; 其次，我们利用这样一个事实，即内容提供者可以在每个对话轮次生成一组相对较小的合理候选话语或动作，并将动作空间限制在这些范围内。虽然动作空间通常在 RL 设置中是固定的，因为所有状态共享相同的动作空间，但我们的空间是一个非标准空间，其中候选动作可能因每个状态而异，因为内容提供者根据对话上下文生成不同的动作。这使我们进入了随机动作集的领域，该框架将每个状态中可用的动作集由外生随机过程控制的情况形式化，我们使用 &lt;a href=&quot;https://www.ijcai. org/proceedings/2018/0650.pdf&quot;>;Stochastic Action Q-Learning&lt;/a>;，&lt;a href=&quot;https://en.wikipedia.org/wiki/Q-learning&quot;>;Q-learning 的变体&lt;/a>; 方法。 Q-learning 是一种流行的 off-policy RL 算法，它不需要环境模型来评估和改进策略。我们在使用监督对话管理器获得的人群计算评级对话语料库上训练我们的模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8 hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s1500/image4 .gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2 g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;给定当前的对话历史和新的用户查询，内容提供者生成候选者，助手从中选择一个。此过程循环运行，最后将选定的话语融合成一个连贯的响应。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h2>;强化学习模型评估&lt;/h2>; &lt;p>; 我们在一项使用 Google Assistant 的实验中将我们的 RL 对话管理器与已启动的监督转换器模型进行了比较，该模型与用户就动物进行了对话。当用户通过询问与动物相关的问题（例如，“狮子的声音如何？”）触发体验时，对话就开始了。该实验是使用&lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B 测试&lt;/a>; 协议进行的，其中随机抽取了一小部分 Google 助理用户与我们基于 RL 的助手交互，而其他用户与标准助手交互。 &lt;/p>; &lt;p>; 我们发现 RL 对话管理器进行的对话时间更长、参与度更高。它将对话长度增加了 30%，同时提高了用户参与度指标。我们看到对助理问题的合作回答增加了 8%——例如，“告诉我关于狮子的事”，以回答“你接下来想听哪种动物？”尽管名义上“不合作”的回答（例如，“不”，作为对提出额外内容的问题的回答，例如“你想听更多吗？”）也有很大增加，但这是预期的RL 智能体通过提出关键问题来承担更多风险。虽然用户可能对助手提出的对话方向不感兴趣（例如，转向另一只动物），但用户通常会继续参与有关动物的对话。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s552/image3.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;534&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;From the non-cooperative user response in the 3rd turn (“No.”) and the query “Make a dog sound,” in the 5th turn, the assistant recognizes that the user is mostly interested in animal sounds and modifies its plan, providing sounds and sound quizzes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In addition, some user queries contain explicit positive (eg, “Thank you, Google,” or “I&#39;m happy.”) or negative (eg, “Shut up,” or “Stop.”) feedback. While an order of magnitude fewer than other queries, they offer a direct measure of user (dis)satisfaction. The RL model increases explicit positive feedback by 32% and reduces negative feedback by 18%. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Learned dynamic planning characteristics and strategies&lt;/h2>; &lt;p>; We observe several characteristics of the (unseen) RL plan to improve user engagement while conducting longer conversations. First, the RL-based assistant ends 20% more turns in questions, prompting the user to choose additional content. It also better harnesses content diversity, including facts, sounds, quizzes, yes/no questions, open questions, etc. On average, the RL assistant uses 26% more distinct content providers per conversation than the supervised model. &lt;/p>; &lt;p>; Two observed RL planning strategies are related to the existence of sub-dialogues with different characteristics. Sub-dialogues about animal sounds are poorer in content and exhibit entity pivoting at every turn (ie, after playing the sound of a given animal, we can either suggest the sound of a different animal or quiz the user about other animal sounds). In contrast, sub-dialogues involving animal facts typically contain richer content and have greater conversation depth. We observe that RL favors the richer experience of the latter, selecting 31% more fact-related content. Lastly, when restricting analysis to fact-related dialogues, the RL assistant exhibits 60% more focus-pivoting turns, that is, conversational turns that change the focus of the dialogue. &lt;/p>; &lt;p>; Below, we show two example conversations, one conducted by the supervised model (left) and the second by the RL model (right), in which the first three user turns are identical. With a supervised dialogue manager, after the user declined to hear about “today&#39;s animal”, the assistant pivots back to animal sounds to maximize the immediate user satisfaction. While the conversation conducted by the RL model begins identically, it exhibits a different planning strategy to optimize the overall user engagement, introducing more diverse content, such as fun facts. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s1539/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1539&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the left conversation, conducted by the supervised model, the assistant maximizes the immediate user satisfaction. The right conversation, conducted by the RL model, shows different planning strategies to optimize the overall user engagement.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future research and challenges&lt;/h2>; &lt;p>; In the past few years, LLMs trained for language understanding and generation have demonstrated impressive results across multiple tasks, including dialogue. We are now exploring the use of an RL framework to empower LLMs with the capability of dynamic planning so that they can dynamically plan ahead and delight users with a more engaging experience. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias, Idan Szpektor and Gal Elidan. We would like to thank: Roee Aharoni, Moran Ambar, John Anderson, Ido Cohn, Mohammad Ghavamzadeh, Lotem Golany, Ziv Hodak, Adva Levin, Fernando Pereira, Shimi Salant, Shachar Shimoni, Ronit Slyper, Ariel Stolovich, Hagai Taitelbaum, Noam Velan, Avital Zipori and the CrowdCompute team led by Ashwin Kakarla. We thank Sophie Allweis for her feedback on this blogpost and Tom Small for the visualization.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2478354033845809100/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Using reinforcement learning for dynamic planning in open-ended conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s72-c/rlxtalk.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-14165165832846745&lt;/id>;&lt;published>;2023-05-15T13:59:00.001-07:00&lt;/published>;&lt;updated>;2023-05-15T14:40:42.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Larger language models do in-context learning differently&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s800/SULICL.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There have recently been tremendous advances in language models, partly because they can perform tasks with strong performance via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>; (ICL), a process whereby models are prompted with a few examples of input-label pairs before performing the task on an unseen evaluation example. In general, models&#39; success at in-context learning is enabled by: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;ul>; &lt;li>;Their use of semantic prior knowledge from pre-training to predict labels while following the format of in-context examples (eg, seeing examples of movie reviews with “positive sentiment” and “negative sentiment” as labels and performing &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>; using prior knowledge). &lt;/li>;&lt;li>;Learning the input-label mappings in context from the presented examples (eg, finding a pattern that positive reviews should be mapped to one label, and negative reviews should be mapped to a different label). &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.03846&quot;>;Larger language models do in-context learning differently&lt;/a>;”, we aim to learn about how these two factors (semantic priors and input-label mappings) interact with each other in ICL settings, especially with respect to the scale of the language model that&#39;s used.我们研究了两种设置来研究这两个因素——带有翻转标签的 ICL（翻转标签 ICL）和带有语义无关标签的 ICL（SUL-ICL）。在翻转标签 ICL 中，上下文示例的标签被翻转，因此语义先验和输入标签映射彼此不一致。在 SUL-ICL 中，上下文示例的标签被替换为语义上与上下文中呈现的任务无关的词。我们发现覆盖先验知识是模型规模的一种新兴能力，在上下文中学习与语义无关的标签的能力也是如此。我们还发现，指令调优加强了先验知识的使用，而不是增加了学习输入标签映射的能力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s625/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;625&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of flipped-label ICL and semantically-unrelated label ICL (SUL-ICL), compared with regular ICL, for a sentiment analysis task.翻转标签 ICL 使用翻转标签，强制模型覆盖语义先验以遵循上下文中的示例。 SUL-ICL uses labels that are not semantically related to the task, which means that models must learn input-label mappings in order to perform the task because they can no longer rely on the semantics of natural language labels.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiment design&lt;/h2>; &lt;p>; For a diverse dataset mixture, we experiment on seven &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) tasks that have been widely used: &lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;sentiment analysis&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/SetFit/subj&quot;>;subjective/objective classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/trec&quot;>;question classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/glue/viewer/qqp/validation&quot;>;duplicated-question recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/super_glue/viewer/rte/test&quot;>;entailment recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/financial_phrasebank&quot;>;financial sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://huggingface.co/datasets/ethos&quot;>;hate speech detection&lt;/a>;. We test five language model families, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;>;InstructGPT&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In this experiment, labels of in-context examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override their priors.在这种情况下，能够覆盖先验知识并在上下文中学习输入标签映射的模型应该会出现性能下降（因为地面实况评估标签没有翻转）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s1036/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1036&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The ability to override semantic priors when presented with flipped in-context example labels emerges with model scale. Smaller models cannot flip predictions to follow flipped labels (performance only decreases slightly), while larger models can do so (performance decreases to well below 50%).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that when no labels are flipped, larger models have better performance than smaller models (as expected).但是当我们翻转越来越多的标签时，小型模型的性能保持相对平稳，但大型模型的性能大幅下降，远低于随机猜测（例如，code-davinci-002 为 90% → 22.5%）。 &lt;/p>; &lt;p>; These results indicate that large models can override prior knowledge from pre-training when contradicting input-label mappings are presented in-context.小型模型无法做到这一点，使这种能力成为模型规模的新兴现象。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Semantically-unrelated labels&lt;/h2>; &lt;p>; In this experiment, we replace labels with semantically-irrelevant ones (eg, for sentiment analysis, we use “foo/bar” instead of “negative/positive”), which means that the model can only perform ICL by learning from input-label mappings.如果一个模型主要依赖于 ICL 的先验知识，那么在这种变化之后它的性能应该会下降，因为它将不再能够使用标签的语义来进行预测。另一方面，可以在上下文中学习输入-标签映射的模型将能够学习这些语义无关的映射，并且性能不会出现重大下降。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s1049/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;1049&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Small models rely more on semantic priors than large models do, as indicated by the greater decrease in performance for small models than for large models when using semantically-unrelated labels (ie, targets) instead of natural language labels. For each plot, models are shown in order of increasing model size (eg, for GPT-3 models, &lt;em>;a&lt;/em>; is smaller than &lt;em>;b&lt;/em>;, which is smaller than &lt;em>;c&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Indeed, we see that using semantically-unrelated labels results in a greater performance drop for small models.这表明较小的模型主要依赖于它们的 ICL 语义先验，而不是从提供的输入标签映射中学习。另一方面，当标签的语义特性被移除时，大型模型能够在上下文中学习输入标签映射。 &lt;/p>; &lt;p>; We also find that including more in-context examples (ie, exemplars) results in a greater performance improvement for large models than it does for small models, indicating that large models are better at learning from in-context examples than small models are. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s825/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;417&quot; data-original-width=&quot;825&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the SUL-ICL setup, larger models benefit more from additional examples than smaller models do.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Instruction tuning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a popular technique for improving model performance, which involves tuning models on various NLP tasks that are phrased as instructions (eg, “Question: What is the sentiment of the following sentence, &#39;This movie is great.&#39; Answer: Positive”). Since the process uses natural language labels, however, an open question is whether it improves the ability to learn input-label mappings or whether it strengthens the ability to recognize and apply semantic prior knowledge. Both of these would lead to an improvement in performance on standard ICL tasks, so it&#39;s unclear which of these occur. &lt;/p>; &lt;p>; We study this question by running the same two setups as before, only this time we focus on comparing standard language models (specifically, PaLM) with their instruction-tuned variants (Flan-PaLM). &lt;/p>; &lt;p>; First, we find that Flan-PaLM is better than PaLM when we use semantically-unrelated labels.这种效果在小型模型中非常突出，因为 Flan-PaLM-8B 比 PaLM-8B 高出 9.6%，几乎赶上了 PaLM-62B。这种趋势表明指令调优增强了学习输入标签映射的能力，这并不特别令人惊讶。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s573/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;301&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned language models are better at learning input–label mappings than pre-training–only language models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More interestingly, we saw that Flan-PaLM is actually worse than PaLM at following flipped labels, meaning that the instruction tuned models were unable to override their prior knowledge (Flan-PaLM models don&#39;t reach below random guessing with 100% flipped labels, but PaLM models without instruction tuning can reach 31% accuracy in the same setting).这些结果表明，指令调整必须增加模型在可用时依赖语义先验的程度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s1016/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;1016&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned models are worse than pre-training–only models at learning to override semantic priors when presented with flipped labels in-context.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Combined with the previous result, we conclude that although instruction tuning improves the ability to learn input-label mappings, it strengthens the usage of semantic prior knowledge more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We examined the extent to which language models learn in-context by utilizing prior knowledge learned during pre-training versus input-label mappings presented in-context. &lt;/p>; &lt;p>; We first showed that large language models can learn to override prior knowledge when presented with enough flipped labels, and that this ability emerges with model scale.然后我们发现使用语义无关的标签成功地进行 ICL 是模型规模的另一种新兴能力。最后，我们分析了指令调优语言模型，发现指令调优提高了学习输入标签映射的能力，但也进一步加强了语义先验知识的使用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; These results underscore how the ICL behavior of language models can change depending on their scale, and that larger language models have an emergent ability to map inputs to many types of labels, a form of reasoning in which input-label mappings can potentially be learned for arbitrary symbols.未来的研究可能有助于深入了解为什么这些现象会在模型规模方面发生。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. We would like to thank Sewon Min and our fellow collaborators at Google Research for their advice and helpful discussions.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/14165165832846745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Larger language models do in-context learning differently&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s72-c/SULICL.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4751112643381771806&lt;/id>;&lt;published>;2023-05-15T10:16:00.003-07:00&lt;/published>;&lt;updated>;2023-05-15T10:24:09.250-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Consensus and subjectivity of skin tone annotation for ML fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Candice Schumann, Software Engineer, and Gbolahan O. Olanubi, User Experience Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s888/MST-E.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Skin tone is an observable characteristic that is subjective, perceived differently by individuals (eg, depending on their location or culture) and thus is complicated to annotate. That said, the ability to reliably and accurately annotate skin tone is highly important in computer vision. This became apparent in 2018, when the &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; study highlighted that computer vision systems struggled to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. The study highlights the importance for computer researchers and practitioners to evaluate their technologies across the full range of skin tones and at intersections of identities. Beyond evaluating model performance on skin tone, skin tone annotations enable researchers to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3375627.3375832&quot;>;measure diversity&lt;/a>; and representation in &lt;a href=&quot;https://arxiv.org/abs/1901.10265&quot;>;image retrieval systems&lt;/a>;, &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;dataset collection&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2202.04053&quot;>;image generation&lt;/a>;. For all of these applications, a collection of meaningful and inclusive skin tone annotations is key. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s1196/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Monk Skin Tone (MST) Scale See more at &lt;a href=&quot;http://skintone.google&quot;>;skintone.google&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Last year, in a step toward more inclusive computer vision systems, Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team in Research partnered with &lt;a href=&quot;https://sociology.fas.harvard.edu/people/ellis-monk&quot;>;Dr. Ellis Monk&lt;/a>; to openly release the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone&lt;/a>; (MST) Scale, a skin tone scale that captures a broad spectrum of skin tones. In comparison to an industry standard scale like the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK481857/table/chapter6.t1/&quot;>;Fitzpatrick Skin-Type Scale&lt;/a>; designed for dermatological use, the MST offers a more inclusive representation across the range of skin tones and was designed for a broad range of applications, including computer vision. &lt;/p>; &lt;p>; Today we&#39;re announcing the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples&lt;/a>; (MST-E) dataset to help practitioners understand the MST scale and train their human annotators. This dataset has been made publicly available to enable practitioners everywhere to create more consistent, inclusive, and meaningful skin tone annotations. Along with this dataset, we&#39;re providing a set of recommendations, noted below, around the MST scale and MST-E dataset so we can all create products that work well for all skin tones. &lt;/p>; &lt;p>; Since we launched the MST, we&#39;ve been using it to improve Google&#39;s computer vision systems to make &lt;a href=&quot;https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/&quot;>;equitable image tools for everyone&lt;/a>; and to &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;improve representation of skin tone in Search&lt;/a>;. Computer vision researchers and practitioners outside of Google, like the curators of &lt;a href=&quot;https://ai.facebook.com/blog/casual-conversations-v2-dataset-measure-fairness/&quot;>;MetaAI&#39;s Casual Conversations&lt;/a>; dataset, are recognizing the value of MST annotations to provide additional insight into diversity and representation in datasets. Incorporation into widely available datasets like these are essential to give everyone the ability to ensure they are building more inclusive computer vision technologies and can test the quality of their systems and products across a wide range of skin tones. &lt;/p>; &lt;p>; Our team has continued to conduct research to understand how we can continue to advance our understanding of skin tone in computer vision. One of our core areas of focus has been skin tone annotation, the process by which human annotators are asked to review images of people and select the best representation of their skin tone. MST annotations enable a better understanding of the inclusiveness and representativeness of datasets across a wide range of skin tones, thus enabling researchers and practitioners to evaluate quality and fairness of their datasets and models. To better understand the effectiveness of MST annotations, we&#39;ve asked ourselves the following questions: &lt;/p>; &lt;ul>; &lt;li>;How do people think about skin tone across geographic locations? &lt;/li>;&lt;li>;What does global consensus of skin tone look like? &lt;/li>;&lt;li>;How do we effectively annotate skin tone for use in inclusive machine learning (ML)? &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;The MST-E dataset&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>; contains 1,515 images and 31 videos of 19 subjects spanning the 10 point MST scale, where the subjects and images were sourced through &lt;a href=&quot;https://tonl.co/&quot;>;TONL&lt;/a>;, a stock photography company focusing on diversity. The 19 subjects include individuals of different ethnicities and gender identities to help human annotators decouple the concept of skin tone from race. The primary goal of this dataset is to enable practitioners to train their human annotators and test for consistent skin tone annotations across various environment capture conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/s1072/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;950&quot; data-original-width=&quot;1072&quot; height=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/w640-h568/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MST-E image set contains 1,515 images and 31 videos featuring 19 models taken under various lighting conditions and facial expressions. Images by TONL. Copyright TONL.CO 2022 ALL RIGHTS RESERVED. Used with permission.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; All images of a subject were collected in a single day to reduce variation of skin tone due to seasonal or other temporal effects. Each subject was photographed in various poses, facial expressions, and lighting conditions. In addition, Dr. Monk annotated each subject with a skin tone label and then selected a “golden” image for each subject that best represents their skin tone. In our research we compare annotations made by human annotators to those made by Dr. Monk, an academic expert in social perception and inequality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Terms of use&lt;/h3>; &lt;p>; Each model selected as a subject provided consent for their images and videos to be released. TONL has given permission for these images to be released as part of MST-E and used for research or human-annotator-training purposes only. The images are not to be used to train ML models. &lt;/p>; &lt;br />; &lt;h2>;Challenges with forming consensus of MST annotations&lt;/h2>; &lt;p>; Although skin tone is easy for a person to see, it can be challenging to systematically annotate across multiple people due to issues with technology and the complexity of human social perception. &lt;/p>; &lt;p>; On the technical side, things like the pixelation, lighting conditions of an image, or a person&#39;s monitor settings can affect how skin tone appears on a screen. You might notice this yourself the next time you change the display setting while watching a show. The hue, saturation, and brightness could all affect how skin tone is displayed on a monitor. Despite these challenges, we find that human annotators are able to learn to become invariant to lighting conditions of an image when annotating skin tone. &lt;/p>; &lt;p>; On the social perception side, aspects of a person&#39;s life like their location, culture, and lived experience may affect how they annotate various skin tones. We found some evidence for this when we asked photographers in the United States and photographers in India to annotate the same image. The photographers in the United States viewed this person as somewhere between MST-5 &amp;amp; MST-7. However, the photographers in India viewed this person as somewhere between MST-3 &amp;amp; MST-5. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s1594/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;1594&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The distribution of Monk Skin Tone Scale annotations for this image from a sample of 5 photographers in the US and 5 photographers in India.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Continuing this exploration, we asked trained annotators from five different geographical regions (India, Philippines, Brazil, Hungary, and Ghana) to annotate skin tone on the MST scale. Within each market each image had 5 annotators who were drawn from a broader pool of annotators in that region. For example, we could have 20 annotators in a market, and select 5 to review a particular image. &lt;/p>; &lt;p>; With these annotations we found two important details. First, annotators within a region had similar levels of agreement on a single image. Second, annotations between regions were, on average, significantly different from each other. (&lt;em>;p&amp;lt;0.05&lt;/em>;). This suggests that people from the same geographic region may have a similar mental model of skin tone, but this mental model is not universal. &lt;/p>; &lt;p>; However, even with these regional differences, we also find that the consensus between all five regions falls close to the MST values supplied by Dr. Monk. This suggests that a geographically diverse group of annotators can get close to the MST value annotated by an MST expert. In addition, after training, we find no significant difference between annotations on well-lit images, versus poorly-lit images, suggesting that annotators can become invariant to different lighting conditions in an image — a non-trivial task for ML models. &lt;/p>; &lt;p>; The MST-E dataset allows researchers to study annotator behavior across curated subsets controlling for potential confounders. We observed similar regional variation when annotating much larger datasets with many more subjects. &lt;/p>; &lt;br />; &lt;h2>;Skin Tone annotation recommendations&lt;/h2>; &lt;p>; Our research includes four major findings. First, annotators within a similar geographical region have a consistent and shared mental model of skin tone. Second, these mental models differ across different geographical regions. Third, the MST annotation consensus from a geographically diverse set of annotators aligns with the annotations provided by an expert in social perception and inequality. And fourth, annotators can learn to become invariant to lighting conditions when annotating MST. &lt;/p>; &lt;p>; Given our research findings, there are a few recommendations for skin tone annotation when using the MST. &lt;/p>; &lt;ol>; &lt;li>;Having a geographically diverse set of annotators is important to gain accurate, or close to ground truth, estimates of skin tone. &lt;/li>;&lt;li>;Train human annotators using the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>;, which spans the entire MST spectrum and contains images in a variety of lighting conditions. This will help annotators become invariant to lighting conditions and appreciate the nuance and differences between the MST points. &lt;/li>;&lt;li>;Given the wide range of annotations we suggest having at least two annotators in at least five different geographical regions (10 ratings per image). &lt;/li>; &lt;/ol>; &lt;p>; Skin tone annotation, like other subjective annotation tasks, is difficult but possible. These types of annotations allow for a more nuanced understanding of model performance, and ultimately help us all to create products that work well for every person across the broad and diverse spectrum of skin tones. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;em>;We wish to thank our colleagues across Google working on fairness and inclusion in computer vision for their contributions to this work, especially Marco Andreetto, Parker Barnes, Ken Burke, Benoit Corda, Tulsee Doshi, Courtney Heldreth, Rachel Hornung, David Madras, Ellis Monk, Shrikanth Narayanan, Utsav Prabhu, Susanna Ricco, Sagar Savla, Alex Siegman, Komal Singh, Biao Wang, and Auriel Wright. We also would like to thank Annie Jean-Baptiste, Florian Koenigsberger, Marc Repnyek, Maura O&#39;Brien, and Dominique Mungin and the rest of the team who help supervise, fund, and coordinate our data collection.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4751112643381771806/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot; rel=&quot;alternate&quot; title=&quot;Consensus and subjectivity of skin tone annotation for ML fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s72-c/MST-E.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6793814472941708824&lt;/id>;&lt;published>;2023-05-12T13:56:00.000-07:00&lt;/published>;&lt;updated>;2023-05-12T13:56:21.572-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;F-VLM: Open-vocabulary object detection upon frozen vision and language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Weicheng Kuo and Anelia Angelova, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s320/F-VLM%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_detection&quot;>;Detection&lt;/a>; is a fundamental vision task that aims to localize and recognize objects in an image. However, the data collection process of manually annotating bounding boxes or instance masks is tedious and costly, which limits the modern detection vocabulary size to roughly 1,000 object classes. This is orders of magnitude smaller than the vocabulary people use to describe the visual world and leaves out many categories. Recent vision and language models (VLMs), such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>;, have demonstrated improved open-vocabulary visual recognition capabilities through learning from Internet-scale image-text pairs. These VLMs are applied to zero-shot classification using frozen model weights without the need for fine-tuning, which stands in stark contrast to the existing paradigms used for retraining or fine-tuning VLMs for &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection tasks&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Intuitively, to align the image content with the text description during training, VLMs may learn region-sensitive and discriminative features that are transferable to object detection. Surprisingly, features of a frozen VLM contain rich information that are both region sensitive for describing object shapes (second column below) and discriminative for region classification (third column below). In fact, feature grouping can nicely delineate object boundaries without any supervision. This motivates us to explore the use of frozen VLMs for open-vocabulary object detection with the goal to expand detection beyond the limited set of annotated categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s1999/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1238&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We explore the potential of frozen vision and language features for open-vocabulary detection. The &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;>;K-Means&lt;/a>; feature grouping reveals rich semantic and region-sensitive information where object boundaries are nicely delineated (column 2). The same frozen features can classify groundtruth (GT) regions well without fine-tuning (column 3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2209.15639&quot;>;F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;ICLR 2023&lt;/a>;, we introduce a simple and scalable open-vocabulary detection approach built upon frozen VLMs. F-VLM reduces the training complexity of an open-vocabulary detector to below that of a standard detector, obviating the need for &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;knowledge distillation&lt;/a>;, detection-tailored pre-training, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_supervision&quot;>;weakly supervised learning&lt;/a>;. We demonstrate that by preserving the knowledge of pre-trained VLMs completely, F-VLM maintains a similar philosophy to &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;>;ViTDet&lt;/a>; and decouples detector-specific learning from the more task-agnostic vision knowledge in the detector backbone. We are also releasing the F-VLM &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm&quot;>;code&lt;/a>; along with a demo on our &lt;a href=&quot;https://sites.google.com/corp/view/f-vlm/home&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning upon frozen vision and language models&lt;/h2>; &lt;p>; We desire to retain the knowledge of pretrained VLMs as much as possible with a view to minimize effort and cost needed to adapt them for open-vocabulary detection. We use a frozen VLM image encoder as the detector backbone and a text encoder for caching the detection text embeddings of offline dataset vocabulary. We take this VLM backbone and attach a &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;detector head&lt;/a>;, which predicts object regions for localization and outputs detection scores that indicate the probability of a detected box being of a certain category. The detection scores are the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; of region features (a set of bounding boxes that the detector head outputs) and category text embeddings. The category text embeddings are obtained by feeding the category names through the text model of pretrained VLM (which has both image and text models)r. &lt;/p>; &lt;p>; The VLM image encoder consists of two parts: 1) a &lt;a href=&quot;https://github.com/openai/CLIP/blob/main/clip/model.py#L139-L151&quot;>;feature extractor&lt;/a>; and 2) a feature &lt;a href=&quot;https://github.com/openai/CLIP/blob/a9b1bf5920416aaeaec965c25dd9e8f98c864f16/clip/model.py#LL152C8-L152C8&quot;>;pooling layer&lt;/a>;. We adopt the feature extractor for detector head training, which is the only step we train (on standard detection &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;data&lt;/a>;), to allow us to directly use frozen weights, inheriting rich semantic knowledge (eg, long-tailed categories like martini, fedora hat, pennant) from the VLM backbone. The detection losses include &lt;a href=&quot;https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/&quot;>;box regression&lt;/a>; and classification losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At training time, F-VLM is simply a detector with the last classification layer replaced by base-category text embeddings.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Region-level open-vocabulary recognition&lt;/h2>; &lt;p>; The ability to perform open-vocabulary recognition at region level (ie, bounding box level as opposed to image level) is integral to F-VLM. Since the backbone features are frozen, they do not overfit to the training categories (eg, donut, zebra) and can be directly cropped for region-level classification. F-VLM performs this open-vocabulary classification only at test time. To obtain the VLM features for a region, we apply the feature pooling layer on the cropped backbone output features. Because the pooling layer requires fixed-size inputs, eg, 7x7 for &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet50&lt;/a>; (R50) &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; backbone, we crop and resize the region features with the &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;ROI-Align layer&lt;/a>; (shown below). Unlike existing open-vocabulary detection &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approaches&lt;/a>;, we do not crop and resize the RGB image regions and cache their embeddings in a separate offline process, but train the detector head in one stage. This is simpler and makes more efficient use of disk storage space.. In addition, we do not crop VLM region features during training because the backbone features are frozen. &lt;/p>; &lt;p>; Despite never being trained on regions, the cropped region features maintain good open-vocabulary recognition capability. However, we observe the cropped region features are not sensitive enough to the localization quality of the regions, ie, a loosely vs. tightly localized box both have similar features. This may be good for classification, but is problematic for detection because we need the detection scores to reflect localization quality as well. To remedy this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geometric_mean&quot;>;geometric mean&lt;/a>; to combine the VLM scores with the detection scores for each region and category. The VLM scores indicate the probability of a detection box being of a certain category according to the pretrained VLM. The detection scores indicate the class probability distribution of each box based on the similarity of region features and input text embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At test time, F-VLM uses the region proposals to crop out the top-level features of the VLM backbone and compute the VLM score per region. The trained detector head provides the detection boxes and masks, while the final detection scores are a combination of detection and VLM scores.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We apply F-VLM to the popular &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, the best F-VLM achieves 32.8 &lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>; (AP) on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;APr&lt;/a>;), which outperforms the state of the art by 6.5 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask APr&lt;/a>; and many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision. F-VLM shows strong scaling property with frozen model capacity, while the number of trainable parameters is fixed. Moreover, F-VLM generalizes and scales well in the transfer detection tasks (eg, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets) by simply replacing the vocabularies without fine-tuning the model. We test the LVIS-trained models on the popular &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; datasets and demonstrate that the model can work very well without training on in-domain detection data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM outperforms &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;the state of the art&lt;/a>; (SOTA) on LVIS open-vocabulary detection benchmark and transfer object detection. On the x-axis, we show the LVIS metric mask AP on rare categories (APr), and the Objects365 (O365) metric box AP on all categories. The sizes of the detector backbones are as follows: Small(R50), Base (R50x4), Large(R50x16), Huge(R50x64). The naming follows CLIP convention.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We visualize F-VLM on open-vocabulary detection and transfer detection tasks (shown below). On LVIS and Objects365, F-VLM correctly detects both novel and common objects. A key benefit of open-vocabulary detection is to test on out-of-distribution data with categories given by users on the fly. See the F-VLM paper for more visualization on &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>;, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1148&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM open-vocabulary and transfer detections. &lt;strong>;Top:&lt;/strong>; Open-vocabulary detection on LVIS. We only show the novel categories for clarity. &lt;strong>;Bottom:&lt;/strong>; Transfer to &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; dataset shows accurate detection of many categories. Novel categories detected: fedora, martini, pennant, football helmet (LVIS); slide (Objects365).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training efficiency&lt;/h2>; &lt;p>; We show that F-VLM can achieve top performance with much less computational resources in the table below. Compared to the state-of-the-art &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approach&lt;/a>;, F-VLM can achieve better performance with 226x fewer resources and 57x faster wall clock time. Apart from training resource savings, F-VLM has potential for substantial memory savings at training time by running the backbone in inference mode. The F-VLM system runs almost as fast as a standard detector at inference time, because the only addition is a single attention pooling layer on the detected region features. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost &lt;br />;(per-core-hour)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost Savings&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;SOTA&lt;/a>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;26.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;460 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;8,000 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;1x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;118 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;565 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;71 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;113x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;27.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;226x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We provide additional results using the shorter &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;>;Detectron2&lt;/a>; training recipes (12 and 36 epochs), and show similarly strong performance by using a frozen backbone. The default setting is marked in gray. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Backbone&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;&lt;a href=&quot;https://arxiv.org/abs/2012.07177&quot;>;Large Scale Jitter&lt;/a>;&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;#Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Batch Size &lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present F-VLM – a simple open-vocabulary detection method which harnesses the power of frozen pre-trained large vision-language models to provide detection of novel objects. This is done without a need for knowledge distillation, detection-tailored pre-training, or weakly supervised learning. Our approach offers significant compute savings and obviates the need for image-level labels. F-VLM achieves the new state-of-the-art in open-vocabulary detection on the LVIS benchmark at system level, and shows very competitive transfer detection on other datasets. We hope this study can both facilitate further research in novel-object detection and help the community explore frozen VLMs for a wider range of vision tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;i>;This work is conducted by Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. We would like to thank our colleagues at Google Research for their advice and helpful discussions. &lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6793814472941708824/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot; rel=&quot;alternate&quot; title=&quot;F-VLM: Open-vocabulary object detection upon frozen vision and language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s72-c/F-VLM%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1014951953767598848&lt;/id>;&lt;published>;2023-05-12T10:03:00.003-07:00&lt;/published>;&lt;updated>;2023-05-12T10:03:43.317-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling conversational interaction on mobile with LLMs&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bryan Wang, Student Researcher, and Yang Li, Research Scientist, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s320/LLM4Mobile%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Intelligent assistants on mobile devices have significantly advanced language-based interactions for performing simple daily tasks, such as setting a timer or turning on a flashlight. Despite the progress, these assistants still face limitations in supporting conversational interactions in mobile user interfaces (UIs), where many user tasks are performed. For example, they cannot answer a user&#39;s question about specific information displayed on a screen. An agent would need to have a computational understanding of &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_user_interface&quot;>;graphical user interfaces&lt;/a>; (GUIs) to achieve such capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Prior research has investigated several important technical building blocks to enable conversational interaction with mobile UIs, including &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3472749.3474765&quot;>;summarizing a mobile screen&lt;/a>; for users to quickly understand its purpose, &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;mapping language instructions to UI actions&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.11103&quot;>;modeling GUIs &lt;/a>;so that they are more amenable for language-based interaction. However, each of these only addresses a limited aspect of conversational interaction and requires considerable effort in curating large-scale datasets and training dedicated models. Furthermore, there is a broad spectrum of conversational interactions that can occur on mobile UIs. Therefore, it is imperative to develop a lightweight and generalizable approach to realize conversational interaction. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/pdf/2209.08655.pdf&quot;>;“Enabling Conversational Interaction with Mobile UI using Large Language Models”&lt;/a>;, presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI 2023&lt;/a>;, we investigate the viability of utilizing large language models (LLMs) to enable diverse language-based interactions with mobile UIs. Recent pre-trained LLMs, such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have demonstrated abilities to adapt themselves to various downstream language tasks when being prompted with a handful of examples of the target task. We present a set of prompting techniques that enable interaction designers and developers to quickly prototype and test novel language interactions with users, which saves time and resources before investing in dedicated datasets and models. Since LLMs only take text tokens as input, we contribute a novel algorithm that generates the text representation of mobile UIs. Our results show that this approach achieves competitive performance using only two data examples per task. More broadly, we demonstrate LLMs&#39; potential to fundamentally transform the future workflow of conversational interaction design.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s1559/LLM4Mobile%201.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;553&quot; data-original-width=&quot;1559&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s16000/LLM4Mobile%201.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing our work on enabling various conversational interactions with mobile UI using LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prompting LLMs with UIs&lt;/h2>; &lt;p>; LLMs support in-context few-shot learning via prompting — instead of fine-tuning or re-training models for each new task, one can prompt an LLM with a few input and output data exemplars from the target task. For many natural language processing tasks, such as question-answering or translation, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompting&lt;/a>; performs competitively with &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;benchmark approaches&lt;/a>; that train a model specific to each task. However, language models can only take text input, while mobile UIs are multimodal, containing text, image, and structural information in their &lt;a href=&quot;https://developer.android.com/topic/performance/rendering/optimizing-view-hierarchies&quot;>;view hierarchy&lt;/a>; data (ie, the structural data containing detailed properties of UI elements) and screenshots. Moreover, directly inputting the view hierarchy data of a mobile screen into LLMs is not feasible as it contains excessive information, such as detailed properties of each UI element, which can exceed the input length limits of LLMs. &lt;/p>; &lt;p>; To address these challenges, we developed a set of techniques to prompt LLMs with mobile UIs. We contribute an algorithm that generates the text representation of mobile UIs using &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth-first_search&quot;>;depth-first search&lt;/a>; traversal to convert the Android UI&#39;s view hierarchy into HTML syntax. We also utilize &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain of thought prompting&lt;/a>;, which involves generating intermediate results and chaining them together to arrive at the final output, to elicit the reasoning ability of the LLM.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s1486/LLM4Mobile%202.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1486&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s16000/LLM4Mobile%202.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing the process of few-shot prompting LLMs with mobile UIs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our prompt design starts with a preamble that explains the prompt&#39;s purpose. The preamble is followed by multiple exemplars consisting of the input, a chain of thought (if applicable), and the output for each task. Each exemplar&#39;s input is a mobile screen in the HTML syntax. Following the input, chains of thought can be provided to elicit logical reasoning from LLMs. This step is not shown in the animation above as it is optional. The task output is the desired outcome for the target tasks, eg, a screen summary or an answer to a user question. Few-shot prompting can be achieved with more than one exemplar included in the prompt. During prediction, we feed the model the prompt with a new input screen appended at the end. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We conducted comprehensive experiments with four pivotal modeling tasks: (1) screen question-generation, (2) screen summarization, (3) screen question-answering, and (4) mapping instruction to UI action. Experimental results show that our approach achieves competitive performance using only two data examples per task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1402&quot; data-original-width=&quot;1999&quot; height=&quot;449&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/w640-h449/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 1: Screen question generation&lt;/h3>; &lt;p>; Given a mobile UI screen, the goal of screen question-generation is to synthesize coherent, grammatically correct natural language questions relevant to the UI elements requiring user input. &lt;/p>; &lt;p>; We found that LLMs can leverage the UI context to generate questions for relevant information. LLMs significantly outperformed the heuristic approach (template-based generation) regarding question quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s1999/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;977&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example screen questions generated by the LLM. The LLM can utilize screen contexts to generate grammatically correct questions relevant to each input field on the mobile UI, while the template approach falls short.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also revealed LLMs&#39; ability to combine relevant input fields into a single question for efficient communication. For example, the filters asking for the minimum and maximum price were combined into a single question: “What&#39;s the price range? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1234&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We observed that the LLM could use its prior knowledge to combine multiple related input fields to ask a single question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In an evaluation, we solicited human ratings on whether the questions were grammatically correct (Grammar) and relevant to the input fields for which they were generated (Relevance). In addition to the human-labeled language quality, we automatically examined how well LLMs can cover all the elements that need to generate questions (Coverage &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1&lt;/a>;). We found that the questions generated by LLM had almost perfect grammar (4.98/5) and were highly relevant to the input fields displayed on the screen (92.8%). Additionally, LLM performed well in terms of covering the input fields comprehensively (95.8%). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;Template &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Grammar &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;3.6 (out of 5) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;4.98 (out of 5)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Relevance &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;84.1% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;92.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Coverage F1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;95.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 2: Screen summarization&lt;/h3>; &lt;p>; Screen summarization is the automatic generation of descriptive language overviews that cover essential functionalities of mobile screens. The task helps users quickly understand the purpose of a mobile UI, which is particularly useful when the UI is not visually accessible. &lt;/p>; &lt;p>; Our results showed that LLMs can effectively summarize the essential functionalities of a mobile UI. They can generate more accurate summaries than the &lt;a href=&quot;https://arxiv.org/abs/2108.03353&quot;>;Screen2Words&lt;/a>; benchmark model that we previously introduced using UI-specific text, as highlighted in the colored text and boxes below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s1999/image10.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1421&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example summary generated by 2-shot LLM. We found the LLM is able to use specific text on the screen to compose more accurate summaries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Interestingly, we observed LLMs using their prior knowledge to deduce information not presented in the UI when creating summaries. In the example below, the LLM inferred the subway stations belong to the London Tube system, while the input UI does not contain this information. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1450&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM uses its prior knowledge to help summarize the screens.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Human evaluation rated LLM summaries as more accurate than the benchmark, yet they scored lower on metrics like &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>;. The mismatch between perceived quality and metric scores echoes &lt;a href=&quot;https://arxiv.org/abs/2209.12356&quot;>;recent work&lt;/a>; showing LLMs write better summaries despite automatic metrics not reflecting it. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;994&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1013&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>;: Screen summarization performance on automatic metrics. &lt;strong>;Right&lt;/strong>;: Screen summarization accuracy voted by human evaluators.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 3: Screen question-answering&lt;/h3>; &lt;p>; Given a mobile UI and an open-ended question asking for information regarding the UI, the model should provide the correct answer. We focus on factual questions, which require answers based on information presented on the screen. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1689&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example results from the screen QA experiment. The LLM significantly outperforms the off-the-shelf QA baseline model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We report performance using four metrics: Exact Matches (identical predicted answer to ground truth), Contains GT (answer fully containing ground truth), Sub-String of GT (answer is a sub-string of ground truth), and the &lt;a href=&quot;https://arxiv.org/abs/1911.03347&quot;>;Micro-F1&lt;/a>; score based on shared words between the predicted answer and ground truth across the entire dataset. &lt;/p>; &lt;p>; Our results showed that LLMs can correctly answer UI-related questions, such as &quot;what&#39;s the headline?&quot;. The LLM performed significantly better than baseline QA model &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;>;DistillBERT&lt;/a>;, achieving a 66.7% fully correct answer rate. Notably, the 0-shot LLM achieved an exact match score of 30.7%, indicating the model&#39;s intrinsic question answering capability. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Exact Matches&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Contains GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Sub-String of GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Micro-F1&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;30.7% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;6.5% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;5.6% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.2% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;65.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;10.0% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;62.9% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;66.7%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;12.6%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;5.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;64.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;DistillBERT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;36.0%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;8.5%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;9.9%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;37.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 4: Mapping instruction to UI action&lt;/h3>; &lt;p>; Given a mobile UI screen and natural language instruction to control the UI, the model needs to predict the ID of the object to perform the instructed action. For example, when instructed with &quot;Open Gmail,&quot; the model should correctly identify the Gmail icon on the home screen. This task is useful for controlling mobile apps using language input such as voice access. We introduced this &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;benchmark task&lt;/a>; previously. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1131&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example using data from the &lt;a href=&quot;https://github.com/google-research-datasets/seq2act&quot;>;PixelHelp dataset&lt;/a>;. The dataset contains interaction traces for common UI tasks such as turning on wifi. Each trace contains multiple steps and corresponding instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We assessed the performance of our approach using the Partial and Complete metrics from the &lt;a href=&quot;https://arxiv.org/abs/2005.03776&quot;>;Seq2Act&lt;/a>; paper. Partial refers to the percentage of correctly predicted individual steps, while Complete measures the portion of accurately predicted entire interaction traces. Although our LLM-based method did not surpass the benchmark trained on massive datasets, it still achieved remarkable performance with just two prompted data examples. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Partial&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Complete&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.29 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.69 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;31.67 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;2-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;75.28 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;34.44 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (in-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;40.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM (in-app)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;80.36&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;45.00&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Seq2Act&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;89.21&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;70.59&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Takeaways and conclusion&lt;/h2>; &lt;p>; Our study shows that prototyping novel language interactions on mobile UIs can be as easy as designing a data exemplar. As a result, an interaction designer can rapidly create functioning mock-ups to test new ideas with end users. Moreover, developers and researchers can explore different possibilities of a target task before investing significant efforts into developing new datasets and models. &lt;/p>; &lt;p>; We investigated the feasibility of prompting LLMs to enable various conversational interactions on mobile UIs. We proposed a suite of prompting techniques for adapting LLMs to mobile UIs. We conducted extensive experiments with the four important modeling tasks to evaluate the effectiveness of our approach. The results showed that compared to traditional machine learning pipelines that consist of expensive data collection and model training, one could rapidly realize novel language-based interactions using LLMs while achieving competitive performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We thank our paper co-author Gang Li, and appreciate the discussions and feedback from our colleagues Chin-Yi Cheng, Tao Li, Yu Hsiao, Michael Terry and Minsuk Chang. Special thanks to Muqthar Mohammad and Ashwin Kakarla for their invaluable assistance in coordinating data collection. We thank John Guilyard for helping create animations and graphics in the blog.&lt;/i>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1014951953767598848/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling conversational interaction on mobile with LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s72-c/LLM4Mobile%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-538722811879850400&lt;/id>;&lt;published>;2023-05-10T08:03:00.004-07:00&lt;/published>;&lt;updated>;2023-05-12T11:51:35.963-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Building better pangenomes to improve the equity of genomics&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Andrew Carroll, Product Lead, and Kishwar Shafin, Research Scientist, Genomics &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s320/image4.png&quot; style=&quot;display: none;&quot; />; &lt;p>; For decades, researchers worked together to assemble a complete copy of the molecular instructions for a human — a map of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_genome&quot;>;human genome&lt;/a>;. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Genome_Project&quot;>;first draft&lt;/a>; was finished in 2000, but with several missing pieces. Even when a complete &lt;a href=&quot;https://en.wikipedia.org/wiki/Reference_genome&quot;>;reference genome&lt;/a>; was &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;achieved in 2022&lt;/a>;, their work was not finished. A single reference genome can&#39;t incorporate known genetic variations, such as the variants for the gene determining whether a person has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Blood_type&quot;>;blood type&lt;/a>; A, B, AB or O. Furthermore, the reference genome &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-8674(19)30231-4&quot;>;didn&#39;t represent the vast diversity of human ancestries&lt;/a>;, making it less useful for detecting disease or finding cures for people from some backgrounds than others. For the past three years, we have been part of an international collaboration with 119 scientists across 60 institutions, called the &lt;a href=&quot;https://www.genome.gov/about-genomics/new-human-pangenome-reference&quot;>;Human Pangenome Research Consortium&lt;/a>;, to address these challenges by creating a new and more representative map of the human genome, a &lt;em>;&lt;a href=&quot;https://www.youtube.com/watch?v=swNtGe9QWAQ&quot;>;pangenome&lt;/a>;&lt;/em>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to share that today, in “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05896-x&quot;>;A draft human pangenome reference&lt;/a>;”, published in &lt;em>;Nature&lt;/em>;, this group is announcing the completion of the first human pangenome reference. The pangenome combines 47 individual genome reference sequences and better represents the genomic diversity of global populations. Building on Google&#39;s deep learning technologies and &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;past advances in genomics&lt;/a>;, we used tools based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; to tackle the challenges of building accurate pangenome sequences and using them for genome analysis. These contributions helped the consortium build an information-rich resource for geneticists, researchers and clinicians around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s1600/Pangenome.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s16000/Pangenome.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Using graphs to build pangenomes &lt;/h2>; &lt;p>; In the typical analysis workflow for high-throughput DNA sequencing, a &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_sequencer&quot;>;sequencing instrument&lt;/a>; reads millions of short pieces of an individual&#39;s genome, and a program called a &lt;a href=&quot;https://en.wikibooks.org/wiki/Next_Generation_Sequencing_(NGS)/Alignment&quot;>;mapper or aligner&lt;/a>; then estimates where those pieces best fit relative to the single, linear human reference sequence. Next, variant caller software identifies the unique parts of the individual&#39;s sequence relative to the reference. &lt;/p>; &lt;p>; But because humans carry a diverse set of sequences, sections that are present in an individual&#39;s DNA but are not in the reference genome can&#39;t be analyzed. One study of 910 African individuals found that a total of &lt;a href=&quot;https://www.theatlantic.com/science/archive/2018/11/human-genome-300-million-missing-letters-dna/576481/&quot;>;300 million DNA base pairs&lt;/a>; — 10% of the roughly three billion base pair reference genome — are not present in the previous linear reference but occur in at least one of the 910 individuals. &lt;/p>; &lt;p>; To address this issue, the consortium used &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph data structures&lt;/a>;, which are powerful for genomics because they can represent the sequences of many people simultaneously, which is needed to create a pangenome. Nodes in a graph genome contain the known set of sequences in a population, and paths through those nodes compactly describe the unique sequences of an individual&#39;s DNA. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s1920/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1081&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Schematic of a graph genome. Each color represents the sequence path of a different individual. Multiple paths passing through the same node indicate multiple individuals share that sequence, but some paths also show a &lt;a href=&quot;https://www.garvan.org.au/research/kinghorn-centre-for-clinical-genomics/learn-about-genomics/for-gp/genetics-refresher-1/types-of-variants&quot;>;single nucleotide variant&lt;/a>; (SNV), insertions, or deletions. Illustration credit Darryl Leja, &lt;a href=&quot;https://www.genome.gov/&quot;>;National Human Genome Research Institute&lt;/a>; (NHGRI).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/s1172/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1172&quot; data-original-width=&quot;1158&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/w632-h640/image1.png&quot; width=&quot;632&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Actual graph genome for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Major_histocompatibility_complex&quot;>;major histocompatibility complex&lt;/a>; (MHC) region of the genome. Genes in MHC regions are essential to immune function and are associated with a person&#39;s resistance and susceptibility to infectious disease and autoimmune disorders (eg, &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fgene.2021.671682/full&quot;>;ankylosing spondylitis&lt;/a>; and &lt;a href=&quot;https://www.hindawi.com/journals/jir/2012/584374/&quot;>;lupus)&lt;/a>;. The graph shows the linear human genome reference (green) and different individual person&#39;s sequence (gray). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Using graphs creates numerous challenges. They require reference sequences to be highly accurate and the development of new methods that can use their data structure as an input. However, new sequencing technologies (such as &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/519025v1&quot;>;consensus sequencing&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2008.01237&quot;>;phased assembly methods&lt;/a>;) have driven exciting progress towards solving these problems. &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.nature.com/articles/s41592-022-01730-w&quot;>;Long-read sequencing technology&lt;/a>;, which reads larger pieces of the genome (10,000 to millions of DNA characters long) at a time, are essential to the creation of high quality reference sequences because larger pieces can be stitched together into assembled genomes more easily than the short pieces read out by earlier technologies. &lt;a href=&quot;https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/short-read-sequencing/&quot;>;Short read sequencing&lt;/a>; reads pieces of the genome that are only 100 to 300 DNA characters long, but has been the highly scalable basis for high-throughput sequencing methods developed in the 2000s. Though long-read sequencing is newer and has advantages for reference genome creation, many informatics methods for short reads hadn&#39;t been developed for long read technologies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evolving DeepVariant for error correction&lt;/h2>; &lt;p>; Google initially developed &lt;a href=&quot;https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html&quot;>;DeepVariant&lt;/a>;, an open-source CNN variant caller framework that analyzes the short-read sequencing evidence of local regions of the genome. However, we were able to re-train DeepVariant to yield &lt;a href=&quot;https://www.nature.com/articles/s41587-019-0217-9&quot;>;accurate analysis of Pacific Bioscience&#39;s long-read data&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s640/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training and evaluation schematic for DeepVariant.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We next teamed up with researchers at the University of California, Santa Cruz (UCSC) &lt;a href=&quot;https://genomics.ucsc.edu/&quot;>;Genomics Institute&lt;/a>; to participate in a &lt;a href=&quot;https://precision.fda.gov/challenges/10&quot;>;United States Food and Drug Administration competition&lt;/a>; for another &lt;a href=&quot;https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html&quot;>;long-read sequencing technology from Oxford Nanopore&lt;/a>;. Together, we won the award for highest accuracy in the &lt;a href=&quot;https://nanoporetech.com/&quot;>;nanopore&lt;/a>; category, with a single nucleotide variants (SNVs) accuracy that matched short-read sequencing. This work has been used to &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;detect and treat genetic diseases in critically ill newborns&lt;/a>;. The use of DeepVariant on long-read technologies provided the foundation for the consortium&#39;s use of DeepVariant for error correction of pangenomes. &lt;/p>; &lt;p>; DeepVariant&#39;s ability to use multiple long-read sequencing modalities proved useful for error correction in the &lt;a href=&quot;https://sites.google.com/corp/ucsc.edu/t2tworkinggroup&quot;>;Telomere-to-Telomere (T2T) Consortium&lt;/a>;&#39;s effort that generated &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;the first complete assembly of a human genome&lt;/a>;. Completing this first genome set the stage to build the multiple reference genomes required for pangenomes, and T2T was already working closely with the &lt;a href=&quot;https://humanpangenomeproject.org/&quot;>;Human Pangenome Project&lt;/a>; (with many shared members) to scale those practices. &lt;/p>; &lt;p>; With a set of high-quality human reference genomes on the horizon, developing methods that could use those assemblies grew in importance. We worked to adapt DeepVariant to use the pangenome developed by the consortium. In partnership with UCSC, we built an end-to-end analysis workflow for &lt;a href=&quot;https://genome.cshlp.org/content/32/5/893.short&quot;>;graph-based variant detection&lt;/a>;, and demonstrated &lt;a href=&quot;https://www.science.org/doi/abs/10.1126/science.abg8871&quot;>;improved accuracy across several thousand samples&lt;/a>;. The use of the pangenome allows many previously missed variants to be correctly identified. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;258&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualization of variant calls in the &lt;a href=&quot;https://medlineplus.gov/genetics/gene/kcne1/&quot;>;KCNE1 gene&lt;/a>; (a gene with variants associated with cardiac arrhythmias and &lt;a href=&quot;https://www.ahajournals.org/doi/pdf/10.1161/JAHA.117.007837&quot;>;sudden death&lt;/a>;) using a pangenome reference versus the prior linear reference. Each dot represents a variant call that is either correct (&lt;strong>;blue dot&lt;/strong>;), incorrect (&lt;strong>;green dot&lt;/strong>;) — when a variant is identified but is not really there —or a missed variant call (&lt;strong>;red dot&lt;/strong>;). The top box shows variant calls made by DeepVariant using the pangenome reference while the bottom shows variant calls made by using the linear reference. Figure adapted from &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.07.09.499321v1&quot;>;A Draft Human Pangenome Reference.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improving pangenome sequences using transformers &lt;/h2>; &lt;p>; Just as new sequencing technologies enabled new pangenome approaches, new informatics technologies enabled improvements for sequencing methods. Google adapted transformer architectures from analysis of human language to genome sequences to develop &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.08.31.458403v1&quot;>;DeepConsensus&lt;/a>;. A key enabler for this was the development of a differentiable loss function that could handle the insertions and deletions common in sequencing data. This enabled us to have high accuracy without needing a decoder, allowing the speed required to keep up with terabytes of sequencer output. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/s940/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;647&quot; data-original-width=&quot;940&quot; height=&quot;441&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/w640-h441/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer architecture for DeepConsensus. DeepConsensus takes as input the repeated sequence of the DNA molecule, measured from fluorescent light detected by the addition of each base. DeepConsensus also uses as input the more detailed information about the sequencing process, including the duration of the light pulse (referred to here as pulse width or PW), the time between pulses (IP) the signal-to-noise ratio (SN) and which side of the double helix is being measured (strand).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;794&quot; data-original-width=&quot;1999&quot; height=&quot;254&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/w640-h254/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of alignment loss function in training evaluation of model output. Better accounting of insertions and deletions by a differentiable alignment function enables the model training process to better estimate errors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; DeepConsensus &lt;a href=&quot;https://blog.google/technology/health/a-new-genome-sequencing-tool-powered-with-our-technology/&quot;>;improves the yield and accuracy of instrument&lt;/a>; data. Because PacBio sequencing provides the primary sequence information for the 47 genome assemblies, we could apply DeepConsensus to improve those assemblies. With application of DeepConsensus, consortium members &lt;a href=&quot;https://www.nature.com/articles/s41587-023-01662-6&quot;>;built a genome assembler&lt;/a>; that was able to reach 99.9997% assembly base-level accuracies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We developed multiple new approaches to improve genetic sequencing methods, which we then used to construct pangenome references that enable more robust genome analysis. &lt;/p>; &lt;p>; But this is just the beginning of the story. In the next stage, a larger, worldwide group of scientists and clinicians will use this pangenome reference to study genetic diseases and make new drugs. And future pangenomes will represent even more individuals, realizing a vision summarized this way in a recent Nature story: “&lt;a href=&quot;https://www.nature.com/articles/d41586-023-01300-w&quot;>;Every base, everywhere, all at once&lt;/a>;.” &lt;em>;Read our post on the &lt;a href=&quot;https://blog.google/technology/health/first-pangenome-reference-nature-paper-ai/&quot;>;Keyword Blog&lt;/a>; to learn more about the human pangenome reference announcement.&lt;/em>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;Many people were involved in creating the pangenome reference, including 119 authors across 60 organizations, with the Human Pangenome Reference Consortium. This blog post highlights Google&#39;s contributions to the broader work. We thank the research groups at UCSC Genomics Institute (GI) under Professors Benedict Paten and Karen Miga, genome polishing efforts of Arang Rhie at National Institute of Health (NIH), Genome Assembly and Polishing of Adam Phillipy&#39;s group, and the standards group at National Institute of Standards and Technology (NIST) of Justin Zook. We thank Google contributors: Pi-Chuan Chang, Maria Nattestad, Daniel Cook, Alexey Kolesnikov, Anastaysia Belyaeva, and Gunjan Baid. We thank John Guilyard for his illustrative animation, and Lizzie Dorfman, Elise Kleeman, Erika Hayden, Cory McLean, Shravya Shetty, Greg Corrado, Katherine Chou, and Yossi Matias for their support, coordination, and leadership. Last but not least, thanks to the research participants that provided their DNA to help build the pangenome resource.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/538722811879850400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html&quot; rel=&quot;alternate&quot; title=&quot;Building better pangenomes to improve the equity of genomics&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s72-c/image4.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7102791933497880989&lt;/id>;&lt;published>;2023-05-04T14:59:00.004-07:00&lt;/published>;&lt;updated>;2023-05-04T14:59:57.911-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s800/MAMMUT.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Vision-language foundational models are built on the premise of a single pre-training followed by subsequent adaptation to multiple downstream tasks. Two main and disjoint training scenarios are popular: a &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;-style contrastive learning and next-token prediction. Contrastive learning trains the model to predict if image-text pairs correctly match, effectively building visual and text representations for the corresponding image and text inputs, whereas next-token prediction predicts the most likely next text token in a sequence, thus learning to generate text, according to the required task. &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;Contrastive learning&lt;/a>; enables &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval tasks&lt;/a>;, such as finding the image that best matches a certain description, and next-token learning enables text-generative tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation#Image_captioning&quot;>;Image Captioning&lt;/a>; and &lt;a href=&quot;https://visualqa.org/&quot;>;Visual Question Answering&lt;/a>; (VQA). While both approaches have demonstrated powerful results, when a model is pre-trained contrastively, it typically does not fare well on text-generative tasks and vice-versa. Furthermore, adaptation to other tasks is often done with complex or inefficient methods. For example, in order to extend a vision-language model to videos, some models need to do inference for each video frame separately. This limits the size of the videos that can be processed to only a few frames and does not fully take advantage of motion information available across frames. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Motivated by this, we present “&lt;a href=&quot;https://arxiv.org/abs/2303.16839&quot;>;A Simple Architecture for Joint Learning for MultiModal Tasks&lt;/a>;”, called MaMMUT, which is able to train jointly for these competing objectives and which provides a foundation for many vision-language tasks either directly or via simple adaptation. MaMMUT is a compact, 2B-parameter multimodal model that trains across contrastive, text generative, and localization-aware objectives. It consists of a single image encoder and a text decoder, which allows for a direct reuse of both components. Furthermore, a straightforward adaptation to video-text tasks requires only using the image encoder once and can handle many more frames than prior work. In line with recent language models (eg, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;>;GLaM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;GPT3&lt;/a>;), our architecture uses a decoder-only text model and can be thought of as a simple extension of language models. While modest in size, our model outperforms the state of the art or achieves competitive performance on &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (VideoQA), &lt;a href=&quot;https://ai.googleblog.com/2022/06/end-to-end-generative-pre-training-for.html&quot;>;video captioning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>;, and &lt;a href=&quot;https://visualqa.org/&quot;>;VQA&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s1182/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;1182&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s980/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s1120/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MaMMUT model enables a wide range of tasks such as image-text/text-image retrieval (&lt;b>;top left&lt;/b>; and &lt;b>;top right&lt;/b>;), VQA (&lt;b>;middle left&lt;/b>;), open-vocabulary detection (&lt;b>;middle right&lt;/b>;), and VideoQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder-only model architecture&lt;/h2>; &lt;p>; One surprising finding is that a single language-decoder is sufficient for all these tasks, which obviates the need for both complex constructs and training procedures presented before. For example, our model (presented to the left in the figure below) consists of a single visual encoder and single text-decoder, connected via &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;cross attention&lt;/a>;, and trains simultaneously on both contrastive and text-generative types of losses. Comparatively, prior work is either not able to handle image-text retrieval tasks, or applies only some losses to only some parts of the model. To enable multimodal tasks and fully take advantage of the decoder-only model, we need to jointly train both contrastive losses and text-generative captioning-like losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s1191/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1191&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT architecture (&lt;b>;left&lt;/b>;) is a simple construct consisting of a single vision encoder and a single text decoder. Compared to other popular vision-language models — eg, &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; (&lt;b>;middle&lt;/b>;) and &lt;a href=&quot;https://arxiv.org/abs/2107.07651&quot;>;ALBEF&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot;>;CoCa&lt;/a>; (&lt;b>;right&lt;/b>;) — it trains jointly and efficiently for multiple vision-language tasks, with both contrastive and text-generative losses, fully sharing the weights between the tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder two-pass learning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1801.10198&quot;>;Decoder-only models&lt;/a>; for language learning show clear advantages in performance with smaller model size (almost half the parameters). The main challenge for applying them to multimodal settings is to unify the contrastive learning (which uses unconditional sequence-level representation) with captioning (which optimizes the likelihood of a token conditioned on the previous tokens). We propose a two-pass approach to jointly learn these two conflicting types of text representations within the decoder. During the first pass, we utilize cross attention and &lt;a href=&quot;https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer&quot;>;causal masking&lt;/a>; to learn the caption generation task — the text features can attend to the image features and predict the tokens in sequence. On the second pass, we disable the cross-attention and causal masking to learn the contrastive task. The text features will not see the image features but can attend bidirectionally to all text tokens at once to produce the final text-based representation. Completing this two-pass approach within the same decoder allows for accommodating both types of tasks that were previously hard to reconcile. While simple, we show that this model architecture is able to provide a foundation for multiple multimodal tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s670/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;670&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT decoder-only two-pass learning enables both contrastive and generative learning paths by the same model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Another advantage of our architecture is that, since it is trained for these disjoint tasks, it can be seamlessly applied to multiple applications such as image-text and text-image retrieval, VQA, and captioning. &lt;/p>; &lt;p>; Moreover, MaMMUT easily adapts to video-language tasks. Previous approaches used a vision encoder to process each frame individually, which required applying it multiple times. This is slow and restricts the number of frames the model can handle, typically to only 6–8. With MaMMUT, we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>; for lightweight adaptation directly via the spatio-temporal information from the video. Furthermore, adapting the model to &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;Open-Vocabulary Detection&lt;/a>; is done by simply training to detect bounding-boxes via an object-detection head. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s632/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;204&quot; data-original-width=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Adaptation of the MaMMUT architecture to video tasks (&lt;b>;left&lt;/b>;) is simple and fully reuses the model. This is done by generating a video “tubes” feature representation, similar to image patches, that are projected to lower dimensional tokens and run through the vision encoder. Unlike prior approaches (&lt;b>;right&lt;/b>;) that need to run multiple individual images through the vision encoder, we use it only once.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Our model achieves excellent zero-shot results on image-text and text-image retrieval without any adaptation, outperforming all previous state-of-the-art models. The results on VQA are competitive with state-of-the-art results, which are achieved by much larger models. The &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI model&lt;/a>; (17B parameters) and the &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo model&lt;/a>; (80B) have the best performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>;, but MaMMUT (2B) has the same accuracy as the 15B PaLI. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s600/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the state of the art (SOTA) on Zero-Shot Image-Text (I2T) and Text-Image (T2I) retrieval on both &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; (&lt;b>;top&lt;/b>;) and &lt;a href=&quot;https://ieeexplore.ieee.org/document/7410660&quot;>;Flickr&lt;/a>; (&lt;b>;bottom&lt;/b>;) benchmarks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>; is competitive but does not outperform large models such as Flamingo-80B and PalI-17B. Performance is evaluated in the more challenging open-ended text generation setting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MaMMUT also outperforms the state-of-the-art on VideoQA, as shown below on the &lt;a href=&quot;https://ieeexplore.ieee.org/document/7780940&quot;>;MSRVTT-QA&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3123266.3123427&quot;>;MSVD-QA&lt;/a>; datasets. Note that we outperform much bigger models such as &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo&lt;/a>;, which is specifically designed for image+video pre-training and is pre-trained with both image-text and video-text data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s600/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s600/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the SOTA models on VideoQA tasks (MSRVTT-QA dataset, &lt;b>;top&lt;/b>;, MSVD-QA dataset,&lt;b>; bottom&lt;/b>;), outperforming much larger models, eg, the 5B GIT2 or Flamingo, which uses 80B parameters and is pre-trained for both image-language and vision-language tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results outperform the state-of-the-art on open-vocabulary detection fine-tuning as is also shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MAMMUT &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>; results on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; dataset compared to state-of-the-art methods. We report the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;average precisions for rare classes&lt;/a>; (APr) as is previously adopted in the literature.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key ingredients&lt;/h2>; &lt;p>; We show that joint training of both contrastive and text-generative objectives is not an easy task, and in our ablations we find that these tasks are served better by different design choices. We see that fewer cross-attention connections are better for retrieval tasks, but more are preferred by VQA tasks. Yet, while this shows that our model&#39;s design choices might be suboptimal for individual tasks, our model is more effective than more complex, or larger, models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s600/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s16000/image11.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s600/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Ablation studies showing that fewer cross-attention connections (1-2) are better for retrieval tasks (&lt;b>;top&lt;/b>;), whereas more connections favor text-generative tasks such as VQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented MaMMUT, a simple and compact vision-encoder language-decoder model that jointly trains a number of conflicting objectives to reconcile contrastive-like and text-generative tasks. Our model also serves as a foundation for many more vision-language tasks, achieving state-of-the-art or competitive performance on image-text and text-image retrieval, videoQA, video captioning, open-vocabulary detection and VQA. We hope it can be further used for more multimodal applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, and Anelia Angelova. We would like to thank Mojtaba Seyedhosseini, Vijay Vasudevan, Priya Goyal, Jiahui Yu, Zirui Wang, Yonghui Wu, Runze Li, Jie Mei, Radu Soricut, Qingqing Huang, Andy Ly, Nan Du, Yuxin Wu, Tom Duerig, Paul Natsev, Zoubin Ghahramani for their help and support. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7102791933497880989/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot; rel=&quot;alternate&quot; title=&quot;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s72-c/MAMMUT.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3611317650594898640&lt;/id>;&lt;published>;2023-05-03T10:22:00.000-07:00&lt;/published>;&lt;updated>;2023-05-03T10:22:54.056-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Joanne Truong, Student Researcher, and Wenhao Yu, Research Scientist, Robotics at Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s320/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Teaching mobile robots to navigate in complex outdoor environments is critical to real-world applications, such as delivery or &lt;a href=&quot;https://www.nasa.gov/stem-ed-resources/robotic-search-and-rescue-challenge.html&quot;>;search and rescue&lt;/a>;. However, this is also a challenging problem as the robot needs to perceive its surroundings, and then explore to identify feasible paths towards the goal. Another common challenge is that the robot needs to overcome uneven terrains, such as stairs, curbs, or rockbed on a trail, while avoiding obstacles and pedestrians. In our prior work, we investigated the second challenge by teaching a quadruped robot to tackle challenging &lt;a href=&quot;https://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html&quot;>;uneven obstacles&lt;/a>; and &lt;a href=&quot;https://research.google/pubs/pub51639/&quot;>;various outdoor terrains&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/pdf/2305.01098.pdf&quot;>;IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience&lt;/a>;”, we present our recent work to tackle the robotic challenge of reasoning about the perceived surroundings to identify a viable navigation path in outdoor environments. We introduce a learning-based indoor-to-outdoor transfer algorithm that uses deep reinforcement learning to train a navigation policy in simulated indoor environments, and successfully transfers that same policy to real outdoor environments. We also introduce Context-Maps (maps with environment observations created by a user), which are applied to our algorithm to enable efficient long-range navigation. We demonstrate that with this policy, robots can successfully navigate hundreds of meters in novel outdoor environments, around previously unseen outdoor obstacles (trees, bushes, buildings, pedestrians, etc.), and in different weather conditions (sunny, overcast, sunset). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/teaser.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;PointGoal navigation&lt;/h2>; &lt;p>; User inputs can tell a robot where to go with commands like “go to the Android statue”, pictures showing a target location, or by simply picking a point on a map. In this work, we specify the navigation goal (a selected point on a map) as a relative coordinate to the robot&#39;s current position (ie, “go to ∆x, ∆y”), this is also known as the &lt;a href=&quot;https://arxiv.org/abs/1807.06757&quot;>;PointGoal Visual Navigation&lt;/a>; (PointNav) task. PointNav is a general formulation for navigation tasks and is one of the standard choices for indoor navigation tasks. However, due to the diverse visuals, uneven terrains and long distance goals in outdoor environments, training PointNav policies for outdoor environments is a challenging task. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Indoor-to-outdoor transfer&lt;/h2>; &lt;p>; Recent successes in training wheeled and legged robotic agents to navigate in &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf&quot;>;indoor&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1911.00357.pdf&quot;>;environments&lt;/a>; were enabled by the development of fast, scalable simulators and the availability of large-scale datasets of &lt;a href=&quot;https://aihabitat.org/datasets/hm3d/&quot;>;photorealistic&lt;/a>; &lt;a href=&quot;https://svl.stanford.edu/igibson/&quot;>;3D scans&lt;/a>; &lt;a href=&quot;https://niessner.github.io/Matterport/&quot;>;of indoor&lt;/a>; &lt;a href=&quot;https://ai2thor.allenai.org/&quot;>;environments&lt;/a>;. To leverage these successes, we develop an indoor-to-outdoor transfer technique that enables our robots to learn from simulated indoor environments and to be deployed in real outdoor environments. &lt;/p>; &lt;p>; To overcome the differences between simulated indoor environments and real outdoor environments, we apply &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;kinematic control&lt;/a>; and image augmentation techniques in our learning system. When using kinematic control, we assume the existence of a reliable low-level &lt;a href=&quot;https://dev.bostondynamics.com/docs/concepts/autonomy/graphnav_and_robot_locomotion&quot;>;locomotion controller&lt;/a>; that can control the robot to precisely reach a new location. This assumption allows us to directly move the robot to the target location during simulation training through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler_method&quot;>;forward Euler integration&lt;/a>; and relieves us from having to explicitly model the underlying robot dynamics in simulation, which drastically improves the throughput of simulation data generation. &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;Prior work&lt;/a>; has shown that kinematic control can lead to better sim-to-real transfer compared to a &lt;a href=&quot;https://arxiv.org/pdf/2008.11867.pdf&quot;>;dynamic control approach&lt;/a>;, where full robot dynamics are modeled and a low-level locomotion controller is required for moving the robot. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/kin_v_dyn.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>; Kinematic control; &lt;strong>;Right&lt;/strong>;: Dynamic control&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We created an outdoor maze-like environment using objects found indoors for initial experiments, where we used Boston Dynamics&#39; &lt;a href=&quot;https://www.bostondynamics.com/products/spot&quot;>;Spot robot&lt;/a>; for test navigation. We found that the robot could navigate around novel obstacles in the new outdoor environment. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_short_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Spot robot successfully navigates around obstacles found in indoor environments, with a policy trained entirely in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; However, when faced with unfamiliar outdoor obstacles not seen during training, such as a large slope, the robot was unable to navigate the slope. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The robot is unable to navigate up slopes, as slopes are rare in indoor environments and the robot was not trained to tackle it.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to walk up and down slopes, we apply an image augmentation technique during the simulation training. Specifically, we randomly tilt the simulated camera on the robot during training. It can be pointed up or down within 30 degrees. This augmentation effectively makes the robot perceive slopes even though the floor is level. Training on these perceived slopes enables the robot to navigate slopes in the real-world. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By randomly tilting the camera angle during training in simulation, the robot is now able to walk up and down slopes.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Since the robots were only trained in simulated indoor environments, in which they typically need to walk to a goal just a few meters away, we find that the learned network failed to process longer-range inputs — eg, the policy failed to walk forward for 100 meters in an empty space. To enable the policy network to handle long-range inputs that are common for outdoor navigation, we normalize the goal vector by using the log of the goal distance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Context-Maps for complex long-range navigation &lt;/h2>; &lt;p>; Putting everything together, the robot can navigate outdoors towards the goal, while walking on uneven terrain, and avoiding trees, pedestrians and other outdoor obstacles. However, there is still one key component missing: the robot&#39;s ability to plan an efficient long-range path. At this scale of navigation, taking a wrong turn and backtracking can be costly. For example, we find that the local exploration strategy learned by standard PointNav policies are insufficient in finding a long-range goal and usually leads to a dead end (shown below). This is because the robot is navigating without context of its environment, and the optimal path may not be visible to the robot from the start. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_long_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Navigation policies without context of the environment do not handle complex long-range navigation goals.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to take the context into consideration and purposefully plan an efficient path, we provide a Context-Map (a binary image that represents a top-down occupancy map of the region that the robot is within) as additional observations for the robot. An example Context-Map is given below, where the black region denotes areas occupied by obstacles and white region is walkable by the robot. The green and red circle denotes the start and goal location of the navigation task. Through the Context-Map, we can provide hints to the robot (eg, the narrow opening in the route below) to help it plan an efficient navigation route. In our experiments, we create the Context-Map for each route guided by &lt;a href=&quot;https://www.google.com/maps&quot;>;Google Maps satellite&lt;/a>; images. We denote this variant of PointNav with environmental context, as &lt;em>;Context-Guided PointNav&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/s1113/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1113&quot; height=&quot;350&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/w640-h350/image12.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the Context-Map (&lt;strong>;right&lt;/strong>;) for a navigation task (&lt;strong>;left&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; It is important to note that the Context-Map does not need to be accurate because it only serves as a rough outline for planning. During navigation,&lt;strong>; &lt;/strong>;the robot still needs to rely on its onboard cameras to identify and adapt its path to pedestrians, which are absent on the map. In our experiments, a human operator quickly sketches the Context-Map from the satellite image, masking out the regions to be avoided. This Context-Map, together with other onboard sensory inputs, including depth images and relative position to the goal, are fed into a neural network with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention&lt;/a>; models (ie, &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>;), which are trained using &lt;a href=&quot;https://arxiv.org/abs/1911.00357&quot;>;DD-PPO&lt;/a>;, a distributed implementation of &lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;>;proximal policy optimization&lt;/a>;, in large-scale simulations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/s782/image15.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;782&quot; height=&quot;442&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/w640-h442/image15.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Context-Guided PointNav architecture consists of a 3-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) to process depth images from the robot&#39;s camera, and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) to process the goal vector. The features are passed into a &lt;a href=&quot;https://arxiv.org/abs/1406.1078&quot;>;gated recurrent unit&lt;/a>; (GRU). We use an additional CNN encoder to process the context-map (top-down map). We compute the &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;scaled dot product attention&lt;/a>; between the map and the depth image, and use a second GRU to process the attended features (Context Attn., Depth Attn.). The output of the policy are linear and angular velocities for the Spot robot to follow. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our system across three long-range outdoor navigation tasks. The provided Context-Maps are rough, incomplete environment outlines that omit obstacles, such as cars, trees, or chairs. &lt;/p>; &lt;p>; With the proposed algorithm, our robot can successfully reach the distant goal location 100% of the time, without a single collision or human intervention. The robot was able to navigate around pedestrians and real-world clutter that are not present on the context-map, and navigate on various terrain including dirt slopes and grass. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 1&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s1999/image14.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s16000/image14.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 2&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s1999/image16.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;697&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 3&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s1999/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work opens up robotic navigation research to the less explored domain of diverse outdoor environments. Our indoor-to-outdoor transfer algorithm uses zero real-world experience and does not require the simulator to model predominantly-outdoor phenomena (terrain, ditches, sidewalks, cars, etc). The success in the approach comes from a combination of a robust locomotion control, low sim-to-real gap in depth and map sensors, and large-scale training in simulation. We demonstrate that providing robots with approximate, high-level maps can enable long-range navigation in novel outdoor environments. Our results provide compelling evidence for challenging the (admittedly reasonable) hypothesis that a new simulator must be designed for every new scenario we wish to study. For more information, please see our &lt;a href=&quot;https://indoorsim2outdoorreal.github.io/&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We would like to thank Sonia Chernova, Tingnan Zhang, April Zitkovich, Dhruv Batra, and Jie Tan for advising and contributing to the project. We would also like to thank Naoki Yokoyama, Nubby Lee, Diego Reyes, Ben Jyenis, and Gus Kouretas for help with the robot experiment setup.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3611317650594898640/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot; rel=&quot;alternate&quot; title=&quot;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s72-c/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6812586756904863062&lt;/id>;&lt;published>;2023-04-30T23:37:00.004-07:00&lt;/published>;&lt;updated>;2023-05-17T15:37:33.732-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICLR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaXU0Qnf7lWC70SIqFUj3KcinepmKmqVwu369RYcajoFZI0UGtej6ZGJD7c0GsXJk_Wjg08CnBLtE7lLlz__azqzUNCgflaTp3F9mZyIxX7HBzJEdTN19WMlfVsu4mTww-LLhvZ4hhWqdVCQqjQbrOk6VumBCH6dA2YwdATT6I_QLKLHSUz_Upnw6zlg/s1200/Google%20Rwanda%20Logo-02.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;Eleventh International Conference on Learning Representations&lt;/a>; (ICLR 2023) is being held this week as a hybrid event in Kigali, Rwanda. We are proud to be a &lt;a href=&quot;https://iclr.cc/Conferences/2023/Sponsors&quot;>;Diamond Sponsor&lt;/a>; of ICLR 2023, a premier conference on deep learning, where Google researchers contribute at all levels. This year we are presenting over 100 papers and are actively involved in organizing and hosting a number of different events, including workshops and interactive sessions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you&#39;re registered for ICLR 2023, we hope you&#39;ll visit the Google booth to learn more about the exciting work we&#39;re doing across topics spanning representation and reinforcement learning, theory and optimization, social impact, safety and privacy, and applications from generative AI to speech and robotics. Continue below to find the many ways in which Google researchers are engaged at ICLR 2023, including workshops, papers, posters and talks (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;p>; Board Members include: &lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Senior Program Chairs include: &lt;strong>;&lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Workshop Chairs include: &lt;strong>;&lt;em>;Aisha Walcott-Bryant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rose Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Diversity, Equity &amp;amp; Inclusion Chairs include: &lt;strong>;&lt;em>;Rosanne Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outstanding Paper awards&lt;/h2>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lTt4KjHSsyl&quot;>;Emergence of Maps in the Memories of Blind Navigation Agents&lt;/a>; &lt;br />; &lt;em>;Erik Wijmans&lt;/em>;, &lt;em>;Manolis Savva&lt;/em>;, &lt;strong>;&lt;em>;Irfan Essa&lt;/em>;&lt;/strong>;, &lt;em>;Stefan Lee&lt;/em>;, &lt;em>;Ari S. Morcos&lt;/em>;, &lt;em>;Dhruv Batra&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FjNys5c7VyY&quot;>;DreamFusion: Text-to-3D Using 2D Diffusion&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>;, &lt;em>;Ajay Jain&lt;/em>;, &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ben Mildenhall&lt;/em>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Keynote speaker&lt;/h2>; &lt;p>; &lt;a href=&quot;https://iclr.cc/virtual/2023/invited-talk/14236&quot;>;Learned Optimizers: Why They&#39;re the Future, Why They&#39;re Hard, and What They Can Do Now&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jascha Sohl-Dickstein &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://www.kaggle.com/iclr-2023&quot;>;Kaggle@ICLR 2023: ML Solutions in Africa&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Julia Elliott&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Phil Culliton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ray Harvey &lt;/i>;&lt;/b>;&lt;br />; Facilitators: &lt;b>;&lt;i>;Julia Elliot&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Walter Reade &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://reincarnating-rl.github.io/&quot;>;Reincarnating Reinforcement Learning&lt;/a>; (Reincarnating RL) &lt;br />; Organizers include: &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ted Xiao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Max Schwarzer &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;br />; Panelists include: &lt;b>;&lt;i>;Marc G. Bellemare&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://rtml-iclr2023.github.io/organizers.html&quot;>;Trustworthy and Reliable Large-Scale Machine Learning Models&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Sanmi Koyejo &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Nicholas Carlini &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://physics4ml.github.io/organizers&quot;>;Physics for Machine Learning&lt;/a>; (Physics4ML) &lt;br />; Speakers include: &lt;b>;&lt;i>;Yasaman Bahri &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4abm.org/workshop_iclr2023/&quot;>;AI for Agent-Based Modelling Community&lt;/a>; (AI4ABM) &lt;br />; Organizers include: &lt;b>;&lt;i>;Pablo Samuel Castro &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/me-fomo2023&quot;>;Mathematical and Empirical Understanding of Foundation Models&lt;/a>; (ME-FoMo) &lt;br />; Organizers include: &lt;b>;&lt;i>;Mathilde Caron&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tengyu Ma&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanie Sedghi &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Yasaman Bahri&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yann Dauphin &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/&quot;>;Neurosymbolic Generative Models 2023&lt;/a>; (NeSy-GeMs) &lt;br />; Organizers include: &lt;b>;&lt;i>;Kevin Ellis &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Daniel Tarlow&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tuan Anh Le &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://domaingen.github.io/&quot;>;What Do We Need for Successful Domain Generalization?&lt;/a>; &lt;br />; Panelists include: &lt;b>;&lt;i>;Boqing Gong &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://pml4dc.github.io/iclr2023/&quot;>;The 4th Workshop on Practical ML for Developing Countries: Learning Under Limited/Low Resource Settings&lt;/a>; &lt;br />; Keynote Speaker: &lt;b>;&lt;i>;Adji Bousso Dieng &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://nasaharvest.github.io/ml-for-remote-sensing/iclr2023/#schedule&quot;>;Machine Learning for Remote Sensing&lt;/a>; &lt;br />; Speakers include: &lt;b>;&lt;i>;Abigail Annkah &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://mrl-workshop.github.io/iclr-2023/&quot;>;Multimodal Representation Learning (MRL): Perks and Pitfalls&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Petra Poklukar &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Arsha Nagrani &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/trustml-unlimited/home&quot;>;Pitfalls of Limited Data and Computation for Trustworthy ML&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Praneeth Netrapalli &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/trustml-unlimited/home&quot;>;Sparsity in Neural Networks: On Practical Limitations and Tradeoffs Between Sustainability and Efficiency&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Trevor Gale&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Utku Evci &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Aakanksha Chowdhery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeff Dean &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/tsrl4h-iclr2023/home&quot;>;Time Series Representation Learning for Health&lt;/a>; &lt;br />; Speakers include: &lt;b>;&lt;i>;Katherine Heller &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl4c.github.io/&quot;>;Deep Learning for Code&lt;/a>; (DL4C) &lt;br />; Organizers include: &lt;b>;&lt;i>;Gabriel Orlanski &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Alex Polozov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Tarlow &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://mlgh-2023.netlify.app/&quot;>;Machine Learning and Global Health&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Mercy Asiedu &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Joelle Barral&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Affinity Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://iclr.cc/virtual/2023/affinity-workshop/13838&quot;>;Tiny Papers Showcase Day&lt;/a>; (a DEI initiative) &lt;br />; Organizers include: &lt;b>;&lt;i>;Rosanne Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Papers&lt;/h2>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Z4s7sJYQM&quot;>;Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics for Advection-Dominated Systems&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhong Yi Wan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Leonardo Zepeda-Nunez&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anudhyan Boral&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fei Sha &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TatRHT_1cK&quot;>;Quantifying Memorization Across Neural Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daphne Ippolito&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Jagielski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katherine Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Florian Tramer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lTt4KjHSsyl&quot;>;Emergence of Maps in the Memories of Blind Navigation Agents&lt;/a>; (&lt;em>;Outstanding Paper Award&lt;/em>;) &lt;br />;&lt;i>; Erik Wijmans&lt;/i>;, &lt;i>;Manolis Savva&lt;/i>;, &lt;b>;&lt;i>;Irfan Essa&lt;/i>;&lt;/b>;, &lt;i>;Stefan Lee&lt;/i>;, &lt;i>;Ari S. Morcos&lt;/i>;, &lt;i>;Dhruv Batra &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4-k7kUavAj&quot;>;Offline Q-Learning on Diverse Multi-task Data Both Scales and Generalizes&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Aviral Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;i>;Xingyang Geng&lt;/i>;, &lt;b>;&lt;i>;George Tucker&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=WE_vluYUL-X&quot;>;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;i>; Shunyu Yao&lt;/i>;*, &lt;b>;&lt;i>;Jeffrey Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dian Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nan Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Izhak Shafran&lt;/i>;&lt;/b>;, &lt;i>;Karthik R. Narasimhan&lt;/i>;, &lt;b>;&lt;i>;Yuan Cao &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=_CDixzkzeyb&quot;>;Prompt-to-Prompt Image Editing with Cross-Attention Control&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Amir Hertz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ron Mokady&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jay Tenenbaum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Kfir Aberman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yael Pritch&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Cohen-Or &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FjNys5c7VyY&quot;>;DreamFusion: Text-to-3D Using 2D Diffusion&lt;/a>; (&lt;em>;Outstanding Paper Award&lt;/em>;) &lt;br />;&lt;i>;&lt;b>; Ben Poole&lt;/b>;&lt;/i>;, &lt;i>;Ajay Jain&lt;/i>;, &lt;b>;&lt;i>;Jonathan T. Barron&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ben Mildenhall &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HcUf-QwZeFh&quot;>;A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Hiroki Furuta&lt;/i>;&lt;/b>;, &lt;i>;Yusuke Iwasawa&lt;/i>;, &lt;i>;Yutaka Matsuo&lt;/i>;, &lt;b>;&lt;i>;Shixiang Shane Gu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OpC-9aBBVJe&quot;>;Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier&lt;/a>; &lt;br />;&lt;i>; Pierluca D&#39;Oro&lt;/i>;, &lt;b>;&lt;i>;Max Schwarzer&lt;/i>;&lt;/b>;, &lt;i>;Evgenii Nikishin&lt;/i>;, &lt;i>;Pierre-Luc Bacon&lt;/i>;, &lt;b>;&lt;i>;Marc G Bellemare&lt;/i>;&lt;/b>;, &lt;i>;Aaron Courville &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=DEGjDDV22pI&quot;>;Dichotomy of Control: Separating What You Can Control from What You Cannot&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Sherry Yang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Pieter Abbeel&lt;/i>;, &lt;b>;&lt;i>;Ofir Nachum &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=7JsGYvjE88d&quot;>;Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search&lt;/a>; &lt;br />;&lt;i>; Michał Zawalski&lt;/i>;, &lt;i>;Michał Tyrolski&lt;/i>;, &lt;i>;Konrad Czechowski&lt;/i>;, &lt;i>;Tomasz Odrzygóźdź&lt;/i>;, &lt;i>;Damian Stachura&lt;/i>;, &lt;i>;Piotr Piekos&lt;/i>;, &lt;b>;&lt;i>;Yuhuai Wu&lt;/i>;&lt;/b>;, &lt;i>;Łukasz Kucinski&lt;/i>;, &lt;i>;Piotr Miłos &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rvsbw2YthH_&quot;>;The Trade-Off Between Universality and Label Efficiency of Representations from Contrastive Learning&lt;/a>; &lt;br />;&lt;i>; Zhenmei Shi&lt;/i>;, &lt;i>;Jiefeng Chen&lt;/i>;, &lt;i>;Kunyang Li&lt;/i>;, &lt;i>;Jayaram Raghuram&lt;/i>;, &lt;b>;&lt;i>;Xi Wu&lt;/i>;&lt;/b>;, &lt;i>;Yingyu Liang&lt;/i>;, &lt;i>;Somesh Jha &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=yHY9NbQJ5BP&quot;>;Sparsity-Constrained Optimal Transport&lt;/a>; &lt;br />;&lt;i>; Tianlin Liu&lt;/i>;*, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mathieu Blondel &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=xSsW2Am-ukZ&quot;>;Unmasking the Lottery Ticket Hypothesis: What&#39;s Encoded in a Winning Ticket&#39;s Mask?&lt;/a>; &lt;br />;&lt;i>; Mansheej Paul&lt;/i>;, &lt;i>;Feng Chen&lt;/i>;, &lt;i>;Brett W. Larsen&lt;/i>;, &lt;i>;Jonathan Frankle&lt;/i>;, &lt;i>;Surya Ganguli&lt;/i>;, &lt;b>;&lt;i>;Gintare Karolina Dziugaite &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SJ0Lde3tRL&quot;>;Extreme Q-Learning: MaxEnt RL without Entropy&lt;/a>; &lt;br />;&lt;i>; Divyansh Garg&lt;/i>;, &lt;i>;Joey Hejna&lt;/i>;, &lt;b>;&lt;i>;Matthieu Geist,&lt;/i>;&lt;/b>; &lt;i>;Stefano Ermon &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SMa9EAovKMC&quot;>;Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs&lt;/a>; &lt;br />;&lt;i>; Albert Qiaochu Jiang&lt;/i>;, &lt;i>;Sean Welleck&lt;/i>;, &lt;b>;&lt;i>;Jin Peng Zhou&lt;/i>;&lt;/b>;, &lt;i>;Timothee Lacroix&lt;/i>;, &lt;i>;Jiacheng Liu&lt;/i>;, &lt;i>;Wenda Li&lt;/i>;, &lt;i>;Mateja Jamnik&lt;/i>;, &lt;i>;Guillaume Lample&lt;/i>;, &lt;b>;&lt;i>;Yuhuai Wu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>; &lt;br />;&lt;i>; Yuzhe Yang&lt;/i>;, &lt;i>;Xin Liu&lt;/i>;, &lt;i>;&lt;b>;Jiang Wu&lt;/b>;&lt;/i>;, &lt;i>;&lt;b>;Silviu Borac&lt;/b>;&lt;/i>;, &lt;i>;Dina Katabi&lt;/i>;, &lt;b>;&lt;i>;Ming-Zher Poh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel McDuff &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G2Q2Mh3avow&quot;>;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Andy Zeng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Maria Attarian&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Brian Ichter&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krzysztof Marcin Choromanski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adrian Wong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Stefan Welker&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Federico Tombari&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aveek Purohit&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael S. Ryoo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vikas Sindhwani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Johnny Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vincent Vanhoucke&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pete Florence &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=0g0X4H8yN4I&quot;>;What Learning Algorithm Is In-Context Learning? Investigations with Linear Models&lt;/a>; &lt;br />;&lt;i>; Ekin Akyurek&lt;/i>;*, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Jacob Andreas&lt;/i>;, &lt;i>;Tengyu Ma&lt;/i>;*, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Peot1SFDX0&quot;>;Preference Transformer: Modeling Human Preferences Using Transformers for RL&lt;/a>; &lt;br />;&lt;i>; Changyeon Kim&lt;/i>;, &lt;i>;Jongjin Park&lt;/i>;, &lt;i>;Jinwoo Shin&lt;/i>;, &lt;i>;Honglak Lee&lt;/i>;, &lt;i>;Pieter Abbeel&lt;/i>;, &lt;b>;&lt;i>;Kimin Lee &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=QCrw0u9LQ7&quot;>;Iterative Patch Selection for High-Resolution Image Recognition&lt;/a>; &lt;br />;&lt;i>; Benjamin Bergner&lt;/i>;, &lt;i>;Christoph Lippert&lt;/i>;, &lt;b>;&lt;i>;Aravindh Mahendran &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=MIMwy4kh9lf&quot;>;Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Weicheng Kuo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yin Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuye Gu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;AJ Piergiovanni&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Anelia Angelova &lt;/b>;&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JLg5aHHv7j&quot;>;(Certified!!) Adversarial Robustness for Free!&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Florian Tramér&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krishnamurthy (Dj) Dvijotham&lt;/i>;&lt;/b>;, &lt;i>;Leslie Rice&lt;/i>;, &lt;i>;Mingjie Sun&lt;/i>;, &lt;i>;J. Zico Kolter &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gU5sJ6ZggcX&quot;>;REPAIR: REnormalizing Permuted Activations for Interpolation Repair&lt;/a>; &lt;br />;&lt;i>; Keller Jordan&lt;/i>;, &lt;b>;&lt;i>;Hanie Sedghi&lt;/i>;&lt;/b>;, &lt;i>;Olga Saukh&lt;/i>;, &lt;i>;Rahim Entezari&lt;/i>;, &lt;b>;&lt;i>;Behnam Neyshabur &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=VM8batVBWvg&quot;>;Discrete Predictor-Corrector Diffusion Models for Image Synthesis&lt;/a>; &lt;br />;&lt;b>;&lt;i>; José Lezama&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tim Salimans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lu Jiang&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Huiwen Chang&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Jonathan Ho&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Irfan Essa &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=zH9GcZ3ZGXu&quot;>;Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Sravanti Addepalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anshul Nasery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Praneeth Netrapalli&lt;/i>;&lt;/b>;, &lt;i>;Venkatesh Babu R.&lt;/i>;, &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=-CoNloheTs&quot;>;An Exact Poly-time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Amit Daniely&lt;/i>;&lt;/b>;, &lt;i>;Elad Granot &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=fR3wGCk-IXp&quot;>;Language Models Are Multilingual Chain-of-Thought Reasoners&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Freda Shi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mirac Suzgun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Markus Freitag&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;i>;Suraj Srivats&lt;/i>;, &lt;i>;Soroush Vosoughi&lt;/i>;, &lt;b>;&lt;i>;Hyung Won Chung&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sebastian Ruder&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dipanjan Das&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JxpBP1JM15-&quot;>;Scaling Forward Gradient with Local Losses&lt;/a>; &lt;br />;&lt;i>; Mengye Ren&lt;/i>;*, &lt;b>;&lt;i>;Simon Kornblith&lt;/i>;&lt;/b>;, &lt;i>;Renjie Liao&lt;/i>;, &lt;b>;&lt;i>;Geoffrey Hinton &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=DWn1TEb2fK&quot;>;Treeformer: Dense Gradient Trees for Efficient Attention Computation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Lovish Madaan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srinadh Bhojanapalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Himanshu Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=NVZvalzCLg&quot;>;LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification&lt;/a>; &lt;br />;&lt;i>; Sharath Girish&lt;/i>;, &lt;i>;Kamal Gupta&lt;/i>;, &lt;b>;&lt;i>;Saurabh Singh&lt;/i>;&lt;/b>;, &lt;i>;Abhinav Shrivastava &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nG9RF9z1yy3&quot;>;DiffusER: Diffusion via Edit-Based Reconstruction&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Machel Reid&lt;/i>;&lt;/b>;, &lt;i>;Vincent J. Hellendoorn&lt;/i>;, &lt;i>;Graham Neubig &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ORp91sAbzI&quot;>;Leveraging Unlabeled Data to Track Memorization&lt;/a>; &lt;br />;&lt;i>; Mahsa Forouzesh&lt;/i>;, &lt;b>;&lt;i>;Hanie Sedghi&lt;/i>;&lt;/b>;, &lt;i>;Patrick Thiran &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4FBUihxz5nm&quot;>;A Mixture-of-Expert Approach to RL-Based Dialogue Management&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yinlam Chow&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aza Tulepbergenov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ofir Nachum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dhawal Gupta&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Moonkyung Ryu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Ghavamzadeh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Craig Boutilier &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rSUCajhLsQ&quot;>;Easy Differentially Private Linear Regression&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Kareem Amin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Joseph&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Monica Ribero&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergei Vassilvitskii &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals&lt;/a>; &lt;br />;&lt;i>; Sandeep Silwal&lt;/i>;*, &lt;b>;&lt;i>;Sara Ahmadian&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew Nystrom&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew McCallum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Deepak Ramachandran&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mehran Kazemi &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=sIoED-yPK9l&quot;>;Massively Scaling Heteroscedastic Classifiers&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Mark Collier&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rodolphe Jenatton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jesse Berent&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Effrosyni Kokiopoulou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TJ2nxciYCk-&quot;>;The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zonglin Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chong You&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srinadh Bhojanapalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daliang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sashank J. Reddi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ke Ye&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Chern&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruiqi Guo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gJW8hSGBys8&quot;>;Compositional Semantic Parsing with Large Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Andrew Drozdov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathanael Scharli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ekin Akyurek&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathan Scales&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xinying Song&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xinyun Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Bousquet&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ndYXTEL6cZz&quot;>;Extremely Simple Activation Shaping for Out-of-Distribution Detection&lt;/a>; &lt;br />;&lt;i>; Andrija Djurisic&lt;/i>;, &lt;i>;Nebojsa Bozanic&lt;/i>;, &lt;i>;Arjun Ashok&lt;/i>;, &lt;b>;&lt;i>;Rosanne Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=5MkYIYCbva&quot;>;Long Range Language Modeling via Gated State Spaces&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Harsh Mehta&lt;/i>;&lt;/b>;, &lt;i>;Ankit Gupta&lt;/i>;, &lt;i>;Ashok Cutkosky&lt;/i>;, &lt;i>;Behnam Neyshabur &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=sSt9fROSZRO&quot;>;Investigating Multi-task Pretraining and Generalization in Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Adrien Ali Taiga&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jesse Farebrother&lt;/i>;&lt;/b>;, &lt;i>;Aaron Courville&lt;/i>;, &lt;b>;&lt;i>;Marc G. Bellemare &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=k9CF4h3muD&quot;>;Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets&lt;/a>; &lt;br />;&lt;i>; Edo Cohen-Karlik&lt;/i>;, &lt;i>;Itamar Menuhin-Gruman&lt;/i>;, &lt;i>;Raja Giryes&lt;/i>;, &lt;i>;Nadav Cohen&lt;/i>;, &lt;b>;&lt;i>;Amir Globerson &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=CL-sVR9pvF&quot;>;Weighted Ensemble Self-Supervised Learning&lt;/a>; &lt;br />;&lt;i>; Yangjun Ruan&lt;/i>;*, &lt;b>;&lt;i>;Saurabh Singh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Warren Morningstar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alexander A. Alemi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Ioffe&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ian Fischer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joshua V. Dillon &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=0qSOodKmJaN&quot;>;Calibrating Sequence Likelihood Improves Conditional Language Generation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yao Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Misha Khalman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Joshi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shashi Narayan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Saleh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OIe3kpwl40D&quot;>;SMART: Sentences as Basic Units for Text Evaluation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Reinald Kim Amplayo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yao Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shashi Narayan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=9Nj_gNdvqYf&quot;>;Leveraging Importance Weights in Subset Selection&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Gui Citovsky&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Giulia DeSalvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srikumar Ramalingam&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Afshin Rostamizadeh&lt;/i>;&lt;/b>;, &lt;i>;Yunjuan Wang&lt;/i>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=oGDKSt9JrZi&quot;>;Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks&lt;/a>; &lt;br />;&lt;i>;&lt;b>;Jesse Farebrother&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Joshua Greaves&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;i>;Charline Le Lan&lt;/i>;, &lt;b>;&lt;i>;Ross Goroshin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pablo Samuel Castro&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Marc G. Bellemare &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=n70oyIlS4g&quot;>;An Extensible Multi-modal Multi-task Object Dataset with Materials&lt;/a>; &lt;br />;&lt;i>; Trevor Standley&lt;/i>;, &lt;i>;Ruohan Gao&lt;/i>;, &lt;b>;&lt;i>;Dawn Chen&lt;/i>;&lt;/b>;, &lt;i>;Jiajun Wu&lt;/i>;, &lt;i>;Silvio Savarese &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=7bJizxLKrR&quot;>;Measuring Forgetting of Memorized Training Examples&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Matthew Jagielski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Om Thakkar&lt;/i>;&lt;/b>;, &lt;i>;Florian Tramér&lt;/i>;, &lt;b>;&lt;i>;Daphne Ippolito&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katherine Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;i>;Eric Wallace&lt;/i>;, &lt;b>;&lt;i>;Shuang Song&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Abhradeep Thakurta&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nicolas Papernot&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=wCFB37bzud4&quot;>;Bidirectional Language Models Are Also Few-Shot Learners&lt;/a>; &lt;br />;&lt;i>; Ajay Patel&lt;/i>;, &lt;i>;Bryan Li&lt;/i>;, &lt;i>;Mohammad Sadegh Rasooli&lt;/i>;, &lt;b>;&lt;i>;Noah Constant&lt;/i>;&lt;/b>;, &lt;i>;Colin Raffel&lt;/i>;, &lt;i>;Chris Callison-Burch &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=xE-LtsE-xx&quot;>;Is Attention All That NeRF Needs?&lt;/a>; &lt;br />;&lt;i>; Mukund Varma T.&lt;/i>;, &lt;i>;Peihao Wang&lt;/i>;, &lt;i>;Xuxi Chen&lt;/i>;, &lt;i>;Tianlong Chen&lt;/i>;, &lt;b>;&lt;i>;Subhashini Venugopalan&lt;/i>;&lt;/b>;, &lt;i>;Zhangyang Wang &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfptQCEKVW4&quot;>;Automating Nearest Neighbor Search Configuration with Constrained Optimization&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Philip Sun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruiqi Guo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lLp-C5nTdJG&quot;>;Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; David Bieber&lt;/i>;&lt;/b>;, &lt;i>;Rishab Goel&lt;/i>;, &lt;b>;&lt;i>;Daniel Zheng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hugo Larochelle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Tarlow &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gmwDKo-4cY&quot;>;Composing Ensembles of Pre-trained Models via Iterative Consensus&lt;/a>; &lt;br />;&lt;i>; Shuang Li&lt;/i>;, &lt;i>;Yilun Du&lt;/i>;, &lt;i>;Joshua B. Tenenbaum&lt;/i>;, &lt;i>;Antonio Torralba&lt;/i>;, &lt;b>;&lt;i>;Igor Mordatch &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=oztkQizr3kk&quot;>;Λ-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection Among Cells&lt;/a>; &lt;br />;&lt;i>; Sajad Movahedi&lt;/i>;, &lt;i>;Melika Adabinejad&lt;/i>;, &lt;i>;Ayyoob Imani&lt;/i>;, &lt;i>;Arezou Keshavarz&lt;/i>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;i>;Azadeh Shakery&lt;/i>;, &lt;i>;Babak N. Araabi &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OjDkC57x5sz&quot;>;Blurring Diffusion Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Emiel Hoogeboom&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tim Salimans &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=bAMTaeqluh4&quot;>;Part-Based Models Improve Adversarial Robustness&lt;/a>; &lt;br />;&lt;i>; Chawin Sitawarin&lt;/i>;, &lt;i>;Kornrapat Pongmala&lt;/i>;, &lt;i>;Yizheng Chen&lt;/i>;, &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;i>;David Wagner &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=z0_V5O9cmNw&quot;>;Learning in Temporally Structured Environments&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Matt Jones&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tyler R. Scott&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mengye Ren&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gamaleldin ElSayed&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Katherine Hermann&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;David Mayo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael C. Mozer &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TFbwV6I0VLg&quot;>;SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models&lt;/a>; &lt;br />;&lt;i>; Ziyi Wu&lt;/i>;, &lt;i>;Nikita Dvornik&lt;/i>;, &lt;b>;&lt;i>;Klaus Greff&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Thomas Kipf&lt;/i>;&lt;/b>;, &lt;i>;Animesh Garg &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=I29Kt0RwChs&quot;>;Robust Algorithms on Adaptive Inputs from Bounded Adversaries&lt;/a>; &lt;br />;&lt;i>; Yeshwanth Cherapanamjeri&lt;/i>;, &lt;i>;Sandeep Silwal&lt;/i>;, &lt;i>;David P. Woodruff&lt;/i>;, &lt;i>;Fred Zhang&lt;/i>;, &lt;b>;&lt;i>;Qiuyi (Richard) Zhang&lt;/i>;&lt;/b>;, &lt;i>;Samson Zhou &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=EnrY5TOrbQ&quot;>;Agnostic Learning of General ReLU Activation Using Gradient Descent&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Pranjal Awasthi&lt;/i>;&lt;/b>;, &lt;i>;Alex Tang&lt;/i>;, &lt;i>;Aravindan Vijayaraghavan &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=3itjR9QxFw&quot;>;Analog Bits: Generating Discrete Data Using Diffusion Models with Self-Conditioning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ting Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruixiang Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Geoffrey Hinton &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lEkl0jdSb7B&quot;>;Any-Scale Balanced Samplers for Discrete Space&lt;/a>; &lt;br />;&lt;i>; Haoran Sun&lt;/i>;*, &lt;b>;&lt;i>;Bo Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Charles Sutton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanjun Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kPPVmUF6bM_&quot;>;Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation&lt;/a>; &lt;br />;&lt;i>; Ziqi Wang&lt;/i>;*, &lt;b>;&lt;i>;Yuexin Wu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frederick Liu&lt;/i>;&lt;/b>;, &lt;i>;Daogao Liu&lt;/i>;, &lt;b>;&lt;i>;Le Hou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hongkun Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Li&lt;/i>;&lt;/b>;, &lt;i>;Heng Ji &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=pOyi9KqE56b&quot;>;Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD&lt;/a>; &lt;br />;&lt;i>; Konstantinos E. Nikolakakis&lt;/i>;, &lt;i>;Farzin Haddadpour&lt;/i>;, &lt;b>;&lt;i>;Amin Karbasi&lt;/i>;&lt;/b>;, &lt;i>;Dionysios S. Kalogerias &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Ha2MnQM9Ph&quot;>;Causal Estimation for Text Data with (Apparent) Overlap Violations&lt;/a>; &lt;br />;&lt;i>; Lin Gui&lt;/i>;, &lt;b>;&lt;i>;Victor Veitch &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=AjC0KBjiMu&quot;>;Contrastive Learning Can Find an Optimal Basis for Approximately View-Invariant Functions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Daniel D. Johnson&lt;/i>;&lt;/b>;, &lt;i>;Ayoub El Hanchi&lt;/i>;, &lt;i>;Chris J. Maddison &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=j1zQGmQQOX1&quot;>;Differentially Private Adaptive Optimization with Delayed Preconditioners&lt;/a>; &lt;br />;&lt;i>; Tian Li&lt;/i>;, &lt;i>;Manzil Zaheer&lt;/i>;, &lt;i>;Ziyu Liu&lt;/i>;, &lt;b>;&lt;i>;Sashank Reddi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Brendan McMahan&lt;/i>;&lt;/b>;, &lt;i>;Virginia Smith &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=3KUfbI9_DQE&quot;>;Distributionally Robust Post-hoc Classifiers Under Prior Shifts&lt;/a>; &lt;br />;&lt;i>; Jiaheng Wei&lt;/i>;*, &lt;b>;&lt;i>;Harikrishna Narasimhan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ehsan Amid&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Wen-Sheng Chu&lt;/i>;&lt;/b>;, &lt;i>;Yang Liu&lt;/i>;, &lt;b>;&lt;i>;Abhishek Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ReDQ1OUQR0X&quot;>;Human Alignment of Neural Network Representations&lt;/a>; &lt;br />;&lt;i>; Lukas Muttenthaler&lt;/i>;, &lt;i>;Jonas Dippel&lt;/i>;, &lt;i>;Lorenz Linhardt&lt;/i>;, &lt;i>;Robert A. Vandermeulen&lt;/i>;, &lt;b>;&lt;i>;Simon Kornblith &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JpbLyEI5EwW&quot;>;Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data&lt;/a>; &lt;br />;&lt;i>; Spencer Frei&lt;/i>;, &lt;i>;Gal Vardi&lt;/i>;, &lt;b>;&lt;i>;Peter Bartlett&lt;/i>;&lt;/b>;, &lt;i>;Nathan Srebro&lt;/i>;, &lt;i>;Wei Hu &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kUmdmHxK5N&quot;>;Koopman Neural Operator Forecaster for Time-Series with Temporal Distributional Shifts&lt;/a>; &lt;br />;&lt;i>; Rui Wang&lt;/i>;*, &lt;b>;&lt;i>;Yihe Dong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sercan Ö. Arik&lt;/i>;&lt;/b>;, &lt;i>;Rose Yu &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=mQpmZVzXK1h&quot;>;Latent Variable Representation for Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tongzheng Ren&lt;/i>;&lt;/b>;, &lt;i>;Chenjun Xiao&lt;/i>;, &lt;b>;&lt;i>;Tianjun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;i>;Zhaoran Wang&lt;/i>;, &lt;i>;Sujay Sanghavi&lt;/i>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=WZH7099tgfM&quot;>;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathanael Scharli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Le Hou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathan Scales&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Claire Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Bousquet&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quoc Le&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ed Chi &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4rXMRuoJlai&quot;>;Mind&#39;s Eye: Grounded Language Model Reasoning Through Simulation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ruibo Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shixiang Shane Gu&lt;/i>;&lt;/b>;, &lt;i>;Te-Yen Wu&lt;/i>;, &lt;i>;Soroush Vosoughi&lt;/i>;, &lt;b>;&lt;i>;Claire Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew M. Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=H0HGljkxQFN&quot;>;MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models&lt;/a>; &lt;br />;&lt;i>; Chenglin Yang&lt;/i>;*, &lt;b>;&lt;i>;Siyuan Qiao&lt;/i>;&lt;/b>;, &lt;i>;Qihang Yu&lt;/i>;, &lt;i>;Xiaoding Yuan&lt;/i>;, &lt;b>;&lt;i>;Yukun Zhu&lt;/i>;&lt;/b>;, &lt;i>;Alan Yuille&lt;/i>;, &lt;b>;&lt;i>;Hartwig Adam&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Liang-Chieh Chen &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HtoA0oT30jC&quot;>;Novel View Synthesis with Diffusion Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Daniel Watson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;William Chan&lt;/i>;&lt;/b>;,&lt;b>;&lt;i>;&amp;nbsp;Ricardo&lt;/i>;&lt;/b>;&amp;nbsp;&lt;b>;&lt;i>;Martin-Brualla&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jonathan Ho&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrea Tagliasacchi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Norouzi&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=fYzLpCsGZVf&quot;>;On Accelerated Perceptrons and Beyond&lt;/a>; &lt;br />;&lt;i>; Guanghui Wang&lt;/i>;, &lt;i>;Rafael Hanashiro&lt;/i>;, &lt;i>;Etash Guha&lt;/i>;, &lt;b>;&lt;i>;Jacob Abernethy &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rJcLocAJpA6&quot;>;On Compositional Uncertainty Quantification for Seq2seq Graph Parsing&lt;/a>; &lt;br />;&lt;i>; Zi Lin&lt;/i>;*, &lt;b>;&lt;i>;Du Phan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Panupong Pasupat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeremiah Liu&lt;/i>;&lt;/b>;, &lt;i>;Jingbo Shang &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=jbIYfq4Tr-&quot;>;On the Robustness of Safe Reinforcement Learning Under Observational Perturbations&lt;/a>; &lt;br />;&lt;i>; Zuxin Liu&lt;/i>;, &lt;i>;Zijian Guo&lt;/i>;, &lt;i>;Zhepeng Cen&lt;/i>;, &lt;i>;Huan Zhang&lt;/i>;, &lt;b>;&lt;i>;Jie Tan&lt;/i>;&lt;/b>;, &lt;i>;Bo Li&lt;/i>;, &lt;i>;Ding Zhao &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=47KG_AvNqeZ&quot;>;Online Low Rank Matrix Completion&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Prateek Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Soumyabrata Pal &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kJUS5nD0vPB&quot;>;Out-of-Distribution Detection and Selective Generation for Conditional Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jie Ren&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Jiaming Luo&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Yao Zhao&lt;/i>;&lt;/b>;, &lt;i>;Kundan Krishna&lt;/i>;*, &lt;b>;&lt;i>;Mohammad Saleh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Balaji Lakshminarayanan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=mWVoBz4W0u&quot;>;PaLI: A Jointly-Scaled Multilingual Language-Image Model&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Xi Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiao Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Soravit Changpinyo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;AJ Piergiovanni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Piotr Padlewski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Salz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sebastian Goodman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adam Grycner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lucas Beyer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alexander Kolesnikov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nan Ding&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Keran Rong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hassan Akbari&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gaurav Mishra&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Linting Xue&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ashish V. Thapliyal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;James Bradbury&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Weicheng Kuo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mojtaba Seyedhosseini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chao Jia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Burcu Karagol Ayan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carlos Riquelme Ruiz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andreas Peter Steiner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anelia Angelova&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiaohua Zhai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Radu Soricut &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=vOEXS39nOF&quot;>;Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ruben Villegas&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Babaeizadeh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pieter-Jan Kindermans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hernan Moraldo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Han Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Taghi Saffar&lt;/i>;&lt;/b>;, &lt;i>;Santiago Castro&lt;/i>;*, &lt;i>;Julius Kunze&lt;/i>;*, &lt;b>;&lt;i>;Dumitru Erhan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gmL46YMpu2J&quot;>;Promptagator: Few-Shot Dense Retrieval from 8 Examples&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhuyun Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vincent Y. Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ji Ma&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Luan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jianmo Ni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Lu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anton Bakalov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Kelvin Guu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Keith B. Hall&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ming-Wei Chang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=MofT9KEF0kw&quot;>;Pushing the Accuracy-Group Robustness Frontier with Introspective Self-Play&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jeremiah Zhe Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krishnamurthy Dj Dvijotham&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jihyeon Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quan Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Balaji Lakshminarayanan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Deepak Ramachandran &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=XSEBx0iSjFQ&quot;>;Re-Imagen: Retrieval-Augmented Text-to-Image Generator&lt;/a>; &lt;brp>;&lt;b>;&lt;i>; Wenhu Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hexiang Hu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chitwan Saharia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;William W. Cohen &lt;/i>;&lt;/b>;&lt;/brp>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.01296.pdf&quot;>;Recitation-Augmented Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhiqing Sun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;i>;Yiming Yang&lt;/i>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=h9O0wsmL-cT&quot;>;Regression with Label Differential Privacy&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Badih Ghazi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pritish Kamath&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ravi Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ethan Leeman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pasin Manurangsi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Avinash Varadarajan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SNgLnzFQeiD&quot;>;Revisiting the Entropy Semiring for Neural Speech Recognition&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Oscar Chang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dongseong Hwang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Siohan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ALDM5SN2r7M&quot;>;Robust Active Distillation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Cenk Baykal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Khoa Trinh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fotis Iliopoulos&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gaurav Menghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Erik Vee &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=BYWWwSY2G5s&quot;>;Score-Based Continuous-Time Discrete Diffusion Models&lt;/a>; &lt;br />;&lt;i>; Haoran Sun&lt;/i>;*, &lt;b>;&lt;i>;Lijun Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanjun Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=1PL1NIMMrw&quot;>;Self-Consistency Improves Chain of Thought Reasoning in Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quoc Le&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ed H. Chi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sharan Narang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aakanksha Chowdhery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Ubc74gTVo3&quot;>;Self-Supervision Through Random Segments with Autoregressive Coding (RandSAC)&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tianyu Hua&lt;/i>;&lt;/b>;, &lt;i>;Yonglong Tian&lt;/i>;, &lt;i>;Sucheng Ren&lt;/i>;, &lt;b>;&lt;i>;Michalis Raptis&lt;/i>;&lt;/b>;, &lt;i>;Hang Zhao&lt;/i>;, &lt;i>;Leonid Sigal &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=T-qVtA3pAxG&quot;>;Serving Graph Compression for Graph Neural Networks&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Si Si&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;i>;Cho-Jui Hsieh&lt;/i>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TTLLGx3eet&quot;>;Sequential Attention for Feature Selection&lt;/a>; &lt;br />;&lt;i>; Taisuke Yasuda&lt;/i>;*, &lt;b>;&lt;i>;MohammadHossein Bateni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lin Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Fahrbach&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gang Fu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vahab Mirrokni &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=T5nUQDrM4u&quot;>;Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints&lt;/a>; &lt;br />;&lt;i>; Aran Komatsuzaki&lt;/i>;*, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;James Lee-Thorp&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carlos Riquelme&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joshua Ainslie&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FBMLeaXpZN&quot;>;Spectral Decomposition Representation for Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tongzheng Ren&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tianjun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lisa Lee&lt;/i>;&lt;/b>;, &lt;i>;Joseph Gonzalez&lt;/i>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=9yE2xEj0BH7&quot;>;Spotlight: Mobile UI Understanding Using Vision-Language Models with a Focus&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/02/a-vision-language-approach-for.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Gang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yang Li &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=8jU7wy7N7mA&quot;>;Supervision Complexity and Its Role in Knowledge Distillation&lt;/a>; &lt;br />;&lt;i>; Hrayr Harutyunyan&lt;/i>;*, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aditya Krishna Menon&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Seungyeon Kim&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=GVSf7Z7DbYL&quot;>;Teacher Guided Training: An Efficient Framework for Knowledge Transfer&lt;/a>; &lt;br />;&lt;i>; Manzil Zaheer&lt;/i>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Seungyeon Kim&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chong You&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Himanshu Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andreas Veit&lt;/i>;&lt;/b>;, &lt;i>;Rob Fergus&lt;/i>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gSHyqBijPFO&quot;>;TEMPERA: Test-Time Prompt Editing via Reinforcement Learning&lt;/a>; &lt;br />;&lt;i>; Tianjun Zhang&lt;/i>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Joseph E. Gonzalez &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6ruVLB727MC&quot;>;UL2: Unifying Language Learning Paradigms&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vinh Q. Tran&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xavier Garcia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hyung Won Chung&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dara Bahri&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tal Schuster&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Steven Zheng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Donald Metzler &lt;/i>;&lt;/b>;&lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; * Work done while at Google&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6812586756904863062/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-iclr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6812586756904863062&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6812586756904863062&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-iclr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICLR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaXU0Qnf7lWC70SIqFUj3KcinepmKmqVwu369RYcajoFZI0UGtej6ZGJD7c0GsXJk_Wjg08CnBLtE7lLlz__azqzUNCgflaTp3F9mZyIxX7HBzJEdTN19WMlfVsu4mTww-LLhvZ4hhWqdVCQqjQbrOk6VumBCH6dA2YwdATT6I_QLKLHSUz_Upnw6zlg/s72-c/Google%20Rwanda%20Logo-02.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1064513940498249945&lt;/id>;&lt;published>;2023-04-27T13:24:00.001-07:00&lt;/published>;&lt;updated>;2023-04-27T13:27:36.601-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An ML-based approach to better characterize lung diseases&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Babak Behsaz, Software Engineer, and Andrew Carroll, Product Lead, Genomics&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjukGQ3XqcJ6pKHbbMe80-OO_hrGogO7FUPtRlCGq7GH5ba83-6_MnzX5CS7MecI2rz4fbFuY1sHp4hb9mpV_zU95zSxA4mfucMRj_d-LtPNNKUAEEHwuE_Cqj1nvVDJN86RLZU2byBsemz-GiVEDIHVusSQHTerZVr0dPytu4HwGA5_9IYXIeiQ3pprg/s1400/copdgenomics.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The combination of the environment an individual experiences and their genetic predispositions determines the majority of their risk &lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/nejmsa073350&quot;>;for various diseases&lt;/a>;. Large national efforts, such as &lt;a href=&quot;https://www.ukbiobank.ac.uk/&quot;>;the UK Biobank&lt;/a>;, have created large, public resources to better understand the links between environment, genetics, and disease. This has the potential to help individuals better understand how to stay healthy, clinicians to treat illnesses, and scientists to develop new medicines. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One challenge in this process is how we make sense of the vast amount of clinical measurements — the UK Biobank has many petabytes of imaging, metabolic tests, and medical records spanning 500,000 individuals. To best use this data, we need to be able to represent the information present as succinct, informative labels about meaningful diseases and traits, a process called &lt;a href=&quot;https://en.wikipedia.org/wiki/Phenotype&quot;>;phenotyping&lt;/a>;. That is where we can use the ability of ML models to pick up on subtle intricate patterns in large amounts of data. &lt;/p>; &lt;p>; We&#39;ve previously demonstrated the ability to use ML models to &lt;a href=&quot;https://www.cell.com/ajhg/fulltext/S0002-9297(21)00188-9&quot;>;quickly phenotype&lt;/a>; at scale for retinal diseases. Nonetheless, these models were trained using labels from clinician judgment, and access to clinical-grade labels is a limiting factor due to the time and expense needed to create them. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41588-023-01372-4&quot;>;Inference of chronic obstructive pulmonary disease with deep learning on raw spirograms identifies new genetic loci and improves risk models&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/ng/&quot;>;Nature Genetics&lt;/a>;&lt;/em>;, we&#39;re excited to highlight a method for training accurate ML models for genetic discovery of diseases, even when using noisy and unreliable labels. We demonstrate the ability to train ML models that can phenotype directly from raw clinical measurement and unreliable medical record information. This reduced reliance on medical domain experts for labeling greatly expands the range of applications for our technique to a panoply of diseases and has the potential to improve their prevention, diagnosis, and treatment. We showcase this method with ML models that can better characterize lung function and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chronic_obstructive_pulmonary_disease&quot;>;chronic obstructive pulmonary disease&lt;/a>; (COPD). Additionally, we show the usefulness of these models by demonstrating a better ability to identify genetic variants associated with COPD, improved understanding of the biology behind the disease, and successful prediction of outcomes associated with COPD. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;ML for deeper understanding of exhalation &lt;/h2>; &lt;p>; For this demonstration, we focused on COPD, the &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death&quot;>;third leading cause of worldwide death in 2019&lt;/a>;, in which airway inflammation and impeded airflow can progressively reduce lung function. Lung function for COPD and other diseases is measured by recording an individual&#39;s exhalation volume over time (the record is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spirometer&quot;>;spirogram&lt;/a>;; see an example below). Although there are guidelines (called &lt;a href=&quot;https://bestpractice.bmj.com/topics/en-us/7/criteria&quot;>;GOLD&lt;/a>;) for determining COPD status from exhalation, these use only a few, specific data points in the curve and apply fixed thresholds to those values. Much of the rich data from these spirograms is discarded in this analysis of lung function. &lt;/p>; &lt;p>; We reasoned that ML models trained to classify spirograms would be able to use the rich data present more completely and result in more accurate and comprehensive measures of lung function and disease, similar to what we have seen in other classification tasks like &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6&quot;>;mammography&lt;/a>; or &lt;a href=&quot;https://ai.googleblog.com/2023/03/learning-from-deep-learning-case-study.html&quot;>;histology&lt;/a>;. We trained ML models to predict whether an individual has COPD using the full spirograms as inputs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw_vNOsI6b6xOptAAEdrfeY1biM7ZgLO6-bGD7ICW2681H_4foD5Gms6AHGDlS1cmWjx2iR8paxAvY8lLAT29uwrijzI-W7uQVmRtHSts0yBA46zJhw8mYRz_H4NZ7rwvMQWOQBGUAvpfsBHHj9kBlVzCc3fU2rUsh7sM_Dn5wNVgPeu2XmR7ERMbJGw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;664&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw_vNOsI6b6xOptAAEdrfeY1biM7ZgLO6-bGD7ICW2681H_4foD5Gms6AHGDlS1cmWjx2iR8paxAvY8lLAT29uwrijzI-W7uQVmRtHSts0yBA46zJhw8mYRz_H4NZ7rwvMQWOQBGUAvpfsBHHj9kBlVzCc3fU2rUsh7sM_Dn5wNVgPeu2XmR7ERMbJGw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spirometry and COPD status overview. Spirograms from lung function test showing a forced expiratory volume-time spirogram (&lt;b>;left&lt;/b>;), a forced expiratory flow-time spirogram (&lt;b>;middle&lt;/b>;), and an interpolated forced expiratory flow-volume spirogram (&lt;b>;right&lt;/b>;). The profile of individuals w/o COPD is different.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The common method of training models for this problem, &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;supervised learning&lt;/a>;, requires samples to be associated with labels. Determining those labels can require the effort of very time-constrained experts. For this work, to show that we do not necessarily need medically graded labels, we decided to use a variety of widely available sources of medical record information to create those labels without medical expert review. These labels are &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233028/&quot;>;less reliable and noisy&lt;/a>; for two reasons. First, there are gaps in the medical records of individuals because they use multiple health services. Second, COPD is often undiagnosed, meaning many with the disease will not be labeled as having it even if we compile the complete medical records. Nonetheless, we trained a model to predict these noisy labels from the spirogram curves and treat the model predictions as a quantitative COPD liability or risk score. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlWGiBTaBqut9XGz_wip5AXFwRVX2gLetKEdaPY0L75-Kz8eow-NIZdwJmjqbZ7bZMlpTzHgWqMya9HVQHlS2HnPaVWnl8vh0NiIlYM5IyEkM6tKLfSsKpSVCBDEXXue6UoHgMvz8ZMAE0_wQcK78tFbTDWmY3RWKhKi7phODzQU1aZ6wJUjIidLOlvQ/s1787/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1787&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlWGiBTaBqut9XGz_wip5AXFwRVX2gLetKEdaPY0L75-Kz8eow-NIZdwJmjqbZ7bZMlpTzHgWqMya9HVQHlS2HnPaVWnl8vh0NiIlYM5IyEkM6tKLfSsKpSVCBDEXXue6UoHgMvz8ZMAE0_wQcK78tFbTDWmY3RWKhKi7phODzQU1aZ6wJUjIidLOlvQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Noisy COPD status labels were derived using various medical record sources (clinical data). A COPD liability model is then trained to predict COPD status from raw flow-volume spirograms.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Predicting COPD outcomes&lt;/h2>; &lt;p>; We then investigated whether the risk scores produced by our model could better predict a variety of binary COPD outcomes (for example, an individual&#39;s COPD status, whether they were hospitalized for COPD or died from it). For comparison, we benchmarked the model relative to expert-defined measurements required to diagnose COPD, specifically &lt;a href=&quot;https://goldcopd.org/gold-spirometry-guide/&quot;>;FEV1/FVC&lt;/a>;, which compares specific points on the spirogram curve with a simple mathematical ratio. We observed an improvement in the ability to predict these outcomes as seen in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision-recall&lt;/a>; curves below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyIWs-H_se4QVdvrrr-8iJ3x2FmWcaGId2LeRr30Sw9trq_03KB94o05zDYAg8l0OW7idCf7G8sbs5PldkZI6WnaDJH3X6z7yv9IEjxLCM9T1Od8HKOtDraAAsOWfZyXCrs860q7iqrVbk4-N6bKniXSjQd64RKIJ_-5_eZKsmivu0qCYhBq1Z0b6xOA/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;728&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyIWs-H_se4QVdvrrr-8iJ3x2FmWcaGId2LeRr30Sw9trq_03KB94o05zDYAg8l0OW7idCf7G8sbs5PldkZI6WnaDJH3X6z7yv9IEjxLCM9T1Od8HKOtDraAAsOWfZyXCrs860q7iqrVbk4-N6bKniXSjQd64RKIJ_-5_eZKsmivu0qCYhBq1Z0b6xOA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Precision-recall curves for COPD status and outcomes for our ML model (green) compared to traditional measures. Confidence intervals are shown by lighter shading.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also observed that separating populations by their COPD model score was predictive of all-cause mortality. This plot suggests that individuals with higher COPD risk are more likely to die earlier from any causes and the risk probably has implications beyond just COPD. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4e3BTXxn3jtqNdnW3xh9mNLiMXRBqCWMfgKdM_TsWOPTbeY4AXm71cVOhvFDY7YR0wvIn2AzvtXnsyn94LFYqY35vVdPIZkF6aVa5zvvRxidRTTHAfe0S3KryH27fWS1VUyKRETqZfRsWwsBHP7c2khartsF3zWyBVQ7AtTK7_6X4TzgrM4M001ttQg/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;809&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4e3BTXxn3jtqNdnW3xh9mNLiMXRBqCWMfgKdM_TsWOPTbeY4AXm71cVOhvFDY7YR0wvIn2AzvtXnsyn94LFYqY35vVdPIZkF6aVa5zvvRxidRTTHAfe0S3KryH27fWS1VUyKRETqZfRsWwsBHP7c2khartsF3zWyBVQ7AtTK7_6X4TzgrM4M001ttQg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Survival analysis of a cohort of UK Biobank individuals stratified by their COPD model&#39;s predicted risk &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartile&quot;>;quartile&lt;/a>;. The decrease of the curve indicates individuals in the cohort dying over time. For example, p100 represents the 25% of the cohort with greatest predicted risk, while p50 represents the 2nd quartile.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Identifying the genetic links with COPD&lt;/h2>; &lt;p>; Since the goal of large scale biobanks is to bring together large amounts of both phenotype and genetic data, we also performed a test called a &lt;a href=&quot;https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies&quot;>;genome-wide association study&lt;/a>; (GWAS) to identify the genetic links with COPD and genetic predisposition. A GWAS measures the strength of the statistical association between a given genetic variant — a change in a specific position of DNA — and the observations (eg, COPD) across a cohort of cases and controls. Genetic associations discovered in this manner can inform drug development that modifies the activity or products of a gene, as well as expand our understanding of the biology for a disease. &lt;/p>; &lt;p>; We showed with our ML-phenotyping method that not only do we rediscover almost all known COPD variants found by manual phenotyping, but we also find many novel genetic variants significantly associated with COPD. In addition, we see good agreement on the effect sizes for the variants discovered by both our ML approach and the manual one (&lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;>;R&lt;sup>;2&lt;/sup>;&lt;/a>;=0.93), which provides strong evidence for validity of the newly found variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvSvAcIMwQ-L_1esTkw6ChFzo_sAbd1ESskmHs0i_9si-Gh91P1IVVnl8kRGUyTtLRSjw6jPbE4bPjUzUpyXawimkXhayASxac-ywLjcPIXOVTKf1fNnO9XZZsCX_a_1BmFFYrtH9eOcAHodEaiS51my3kpol0TvTHK7_Ov7ACF5jOyNbionY1nUVxmw/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;883&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvSvAcIMwQ-L_1esTkw6ChFzo_sAbd1ESskmHs0i_9si-Gh91P1IVVnl8kRGUyTtLRSjw6jPbE4bPjUzUpyXawimkXhayASxac-ywLjcPIXOVTKf1fNnO9XZZsCX_a_1BmFFYrtH9eOcAHodEaiS51my3kpol0TvTHK7_Ov7ACF5jOyNbionY1nUVxmw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left&lt;/b>;: A plot comparing the statistical power of genetic discovery using the &lt;em>;labels for our ML model&lt;/em>; (y-axis) with the statistical power of the &lt;em>;manual labels from a traditional study&lt;/em>; (&lt;em>;x&lt;/em>;-axis). A value above the &lt;em>;y &lt;/em>;= &lt;em>;x&lt;/em>; line indicates greater statistical power in our method. Green points indicate significant findings in our method that are not found using the traditional approach. Orange points are significant in the traditional approach but not ours. Blue points are significant in both. &lt;b>;Right&lt;/b>;: Estimates of the association effect between our method (&lt;em>;y&lt;/em>;-axis) and traditional method (&lt;em>;x&lt;/em>;-axis). Note that the relative values between studies are comparable but the absolute numbers are not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Finally, our collaborators at Harvard Medical School and Brigham and Women&#39;s Hospital further examined the plausibility of these findings by providing insights into the possible biological role of the novel variants in development and progression of COPD (you can see more discussion on these insights in the &lt;a href=&quot;https://www.nature.com/articles/s41588-023-01372-4&quot;>;paper&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We demonstrated that our earlier methods for phenotyping with ML can be expanded to a wide range of diseases and can provide novel and valuable insights. We made two key observations by using this to predict COPD from spirograms and discovering new genetic insights. First, domain knowledge was not necessary to make predictions from raw medical data. Interestingly, we showed the raw medical data is probably underutilized and the ML model can find patterns in it that are not captured by expert-defined measurements. Second, we do not need medically graded labels; instead, noisy labels defined from widely available medical records can be used to generate clinically predictive and genetically informative risk scores. We hope that this work will broadly expand the ability of the field to use noisy labels and will improve our collective understanding of lung function and disease. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;br />;&lt;/h2>; &lt;p>; &lt;em>;This work is the combined output of multiple contributors and institutions. We thank all contributors: Justin Cosentino, Babak Alipanahi, Zachary R. McCaw, Cory Y. McLean, Farhad Hormozdiari (Google), Davin Hill (Northeastern University), Tae-Hwi Schwantes-An and Dongbing Lai (Indiana University), Brian D. Hobbs and Michael H. Cho (Brigham and Women&#39;s Hospital, and Harvard Medical School). We also thank Ted Yun and Nick Furlotte for reviewing the manuscript, Greg Corrado and Shravya Shetty for support, and Howard Yang, Kavita Kulkarni, and Tammi Huynh for helping with publication logistics.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1064513940498249945/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1064513940498249945&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1064513940498249945&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html&quot; rel=&quot;alternate&quot; title=&quot;An ML-based approach to better characterize lung diseases&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjukGQ3XqcJ6pKHbbMe80-OO_hrGogO7FUPtRlCGq7GH5ba83-6_MnzX5CS7MecI2rz4fbFuY1sHp4hb9mpV_zU95zSxA4mfucMRj_d-LtPNNKUAEEHwuE_Cqj1nvVDJN86RLZU2byBsemz-GiVEDIHVusSQHTerZVr0dPytu4HwGA5_9IYXIeiQ3pprg/s72-c/copdgenomics.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8846807972710427001&lt;/id>;&lt;published>;2023-04-26T13:32:00.000-07:00&lt;/published>;&lt;updated>;2023-04-26T13:32:51.664-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Robust and efficient medical imaging with self-supervision&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shekoofeh Azizi, Senior Research Scientist, and Laura Culp, Senior Research Engineer, Google Research&lt;/span>;&lt;div>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvMtAszRzkUmNz3DliRRKOPMA0hi-EplY9vfJ9cWCZhFBUgnblKjVoObTYAfybreL7HEKh11g2unV5oR_KOWfkPxpe3Hzo6qWSWq4YxA9RuhkeGXdDJST2XvW2Nfl3L6kDpV1hA2b_vpBMVaw2_pxt2vWD2n92S9fDGAZp0a4c2dV2Ylf3XrUSLQxAoQ/w640-h640/REMEDIS%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;Despite recent progress in the field of &lt;a href=&quot;https://www.nature.com/articles/s41746-020-00376-2&quot;>;medical artificial intelligence&lt;/a>; (AI), most existing models are &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6.epdf?author_access_token=V_LKV2xpSv9G1dhANYeWM9RgN0jAjWel9jnR3ZoTv0M5zwPVx5jT4z_z-YkUZTBT6_1AtRXi8QouJM7xB-oSN-cVBoH7f_QTgx-yQN3UBEVfkvO1_5urNT-CZHGCEQNGlCuO69tMQYak4SmdoDqyzg%3D%3D&quot;>;narrow&lt;/a>;, &lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2588763&quot;>;single-task&lt;/a>; systems that require large quantities of labeled data to train. Moreover, these models cannot be easily reused in new clinical contexts as they often require the collection, &lt;a href=&quot;https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html&quot;>;de-identification&lt;/a>; and annotation of site-specific data for every new deployment environment, which is both &lt;a href=&quot;https://pubs.rsna.org/doi/full/10.1148/radiol.2020192224&quot;>;laborious and expensive&lt;/a>;. This problem of &lt;em>;data-efficient generalization &lt;/em>;(a model&#39;s ability to generalize to new settings using minimal new data) continues to be a key translational challenge for medical machine learning (ML) models and has in turn, prevented their broad uptake in real world healthcare settings. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;foundation models&lt;/a>; offers a significant opportunity to rethink development of medical AI to make it more performant, safer, and equitable. These models are trained using data at scale, often by self-supervised learning. This process results in generalist models that can rapidly be adapted to new tasks and environments with less need for supervised data. With foundation models, it may be possible to safely and efficiently deploy models across various clinical contexts and environments. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2205.09723&quot;>;Robust and Efficient MEDical Imaging with Self-supervision&lt;/a>;” (REMEDIS), to be published in &lt;em>;&lt;a href=&quot;https://www.nature.com/natbiomedeng/&quot;>;Nature Biomedical Engineering&lt;/a>;&lt;/em>;, we introduce a unified large-scale self-supervised learning framework for building foundation medical imaging models. This strategy combines large scale &lt;a href=&quot;https://arxiv.org/abs/2101.05913&quot;>;supervised transfer learning&lt;/a>; with &lt;a href=&quot;https://ai.googleblog.com/2021/10/self-supervised-learning-advances.html&quot;>;self-supervised learning&lt;/a>; and requires minimal task-specific customization. REMEDIS shows significant improvement in data-efficient generalization across medical imaging tasks and modalities with a 3–100x reduction in site-specific data for adapting models to new clinical contexts and environments. Building on this, we are excited to announce &lt;a href=&quot;https://github.com/google-research/medical-ai-research-foundations&quot;>;Medical AI Research Foundations&lt;/a>; (hosted by &lt;a href=&quot;https://doi.org/10.13026/psq3-vj24&quot;>;PhysioNet&lt;/a>;), an expansion of the public release of &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;chest X-ray Foundations&lt;/a>; in 2022. Medical AI Research Foundations is a collection of open-source non-diagnostic models (starting with REMEDIS models), APIs, and resources to help researchers and developers accelerate medical AI research. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large scale self-supervision for medical imaging&lt;/h2>; &lt;p>; REMEDIS uses a combination of natural (non-medical) images and unlabeled medical images to develop strong medical imaging foundation models. Its pre-training strategy consists of two steps. The first involves supervised representation learning on a large-scale dataset of labeled natural images (pulled from &lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;Imagenet 21k&lt;/a>; or &lt;a href=&quot;http://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html&quot;>;JFT&lt;/a>;) using the &lt;a href=&quot;https://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html&quot;>;Big Transfer&lt;/a>; (BiT) method. &lt;/p>; &lt;p>; The second step involves intermediate self-supervised learning, which does not require any labels and instead, trains a model to learn medical data representations independently of labels. The specific approach used for pre-training and learning representations is &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>;. The method works by maximizing agreement between differently augmented views of the same training example via a contrastive loss in a hidden layer of a feed-forward neural network with &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) outputs. However, REMEDIS is equally compatible with other contrastive self-supervised learning methods. This training method is applicable for healthcare environments as many hospitals acquire raw data (images) as a routine practice. While processes would have to be implemented to make this data usable within models (ie, patient consent prior to gathering the data, de-identification, etc.), the costly, time-consuming, and difficult task of labeling that data could be avoided using REMEDIS. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4_m0T9kVxqldLFRTdlqSqYTieFr_4z2BIwAmjDqk3tDO9keGFVIMWMFgJmEGdHT4boqPCUYsi66ekSGKQXeY_3X-s95_bjsgugMquMjfMzRg5TcsO35SBzCv9AleHuEYYCS_gYNHsvRZw07oI_ZPtRynbqxYNF_F8iTLSesAZMHWbhB54Nlhoz9nPvA/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;368&quot; data-original-width=&quot;1600&quot; height=&quot;147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4_m0T9kVxqldLFRTdlqSqYTieFr_4z2BIwAmjDqk3tDO9keGFVIMWMFgJmEGdHT4boqPCUYsi66ekSGKQXeY_3X-s95_bjsgugMquMjfMzRg5TcsO35SBzCv9AleHuEYYCS_gYNHsvRZw07oI_ZPtRynbqxYNF_F8iTLSesAZMHWbhB54Nlhoz9nPvA/w640-h147/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS leverages large-scale supervised learning using natural images and self-supervised learning using unlabeled medical data to create strong foundation models for medical imaging.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given ML model parameter constraints, it is important that our proposed approach works when using both small and large model architecture sizes. To study this in detail, we considered two ResNet architectures with commonly used depth and width multipliers, &lt;a href=&quot;https://colab.research.google.com/github/yashclone999/ResNet_MODEL/blob/master/ResNet50.ipynb&quot;>;ResNet-50&lt;/a>; (1×) and &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152&quot;>;ResNet-152&lt;/a>; (2×) as the backbone encoder networks. &lt;/p>; &lt;p>; After pre-training, the model was fine-tuned using labeled task-specific medical data and evaluated for in-distribution task performance. In addition, to evaluate the data-efficient generalization, the model was also optionally fine-tuned using small amounts of out-of-distribution (OOD) data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ895lMLLchi5kiP38Gi16aBaaLNEgOxOkKft4ZnMVthFW_NRS_6ZcU8mIgMO2QuvJ3jU0TTH_yE-UjPhO64MX2sMLCvo9qt-reXkYY5zQtYkgunAEittRQru0Zvzi8BT_MhzBnSF9PHqEmD0EEmMErJiw-dXbfX2uWWCEyDVq0pGjSuK3EhVqh9y-gQ/s2209/REMEDIS%201200x2000.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2209&quot; data-original-width=&quot;1215&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ895lMLLchi5kiP38Gi16aBaaLNEgOxOkKft4ZnMVthFW_NRS_6ZcU8mIgMO2QuvJ3jU0TTH_yE-UjPhO64MX2sMLCvo9qt-reXkYY5zQtYkgunAEittRQru0Zvzi8BT_MhzBnSF9PHqEmD0EEmMErJiw-dXbfX2uWWCEyDVq0pGjSuK3EhVqh9y-gQ/w352-h640/REMEDIS%201200x2000.png&quot; width=&quot;352&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS starts with representations initialized using large-scale natural image pretraining following the Big Transfer (BiT) method. We then adapt the model to the medical domain using intermediate contrastive self-supervised learning without using any labeled medical data. Finally, we fine-tune the model to specific downstream medical imaging tasks. We evaluate the ML model both in an in-distribution (ID) setting and in an out-of-distribution (OOD) setting to establish the data-efficient generalization performance of the model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation and results&lt;/h2>; &lt;p>; To evaluate the REMEDIS model&#39;s performance, we simulate realistic scenarios using retrospective de-identified data across a broad range of medical imaging tasks and modalities, including &lt;a href=&quot;https://ai.googleblog.com/2019/09/using-deep-learning-to-inform.html&quot;>;dermatology&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2018/12/improving-effectiveness-of-diabetic.html&quot;>;retinal imaging&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html&quot;>;chest X-ray interpretation&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/learning-from-deep-learning-case-study.html&quot;>;pathology&lt;/a>; and &lt;a href=&quot;https://blog.google/technology/health/improving-breast-cancer-screening&quot;>;mammography&lt;/a>;. We further introduce the notion of data-efficient generalization, capturing the model&#39;s ability to generalize to new deployment distributions with a significantly reduced need for expert annotated data from the new clinical setting. In-distribution performance is measured as (1) improvement in zero-shot generalization to OOD settings (assessing performance in an OOD evaluation set, with zero access to training data from the OOD dataset) and (2) significant reduction in the need for annotated data from the OOD settings to reach performance equivalent to clinical experts (or threshold demonstrating clinical utility). REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strongly supervised baseline. &lt;/p>; &lt;p>; More importantly, our strategy leads to data-efficient generalization of medical imaging models, matching strong supervised baselines resulting in a 3–100x reduction in the need for retraining data. While SimCLR is the primary self-supervised learning approach used in the study, we also show that REMEDIS is compatible with other approaches, such as &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo-V2&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.07922&quot;>;RELIC&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.03230&quot;>;Barlow Twins&lt;/a>;. Furthermore, the approach works across model architecture sizes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIyKlK_o3J1b62jj1RPy7HS2MvM4qrcXCHWI0iqckQt2yYgLRzqM4K2s3Xz3qqH4u6Mle1E5Ixvw9RDVuBLIQDNT9a3XJYKZAdrNoTjhn_FkVakBVVxLmDUlLBJ7Ap3IHzzOVtUTfQt3hyUCF9Y0ntbFKFHSfy5n8op0d2kYYszsv7DhnX3HxHQxWwg/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIyKlK_o3J1b62jj1RPy7HS2MvM4qrcXCHWI0iqckQt2yYgLRzqM4K2s3Xz3qqH4u6Mle1E5Ixvw9RDVuBLIQDNT9a3XJYKZAdrNoTjhn_FkVakBVVxLmDUlLBJ7Ap3IHzzOVtUTfQt3hyUCF9Y0ntbFKFHSfy5n8op0d2kYYszsv7DhnX3HxHQxWwg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS outperformed the supervised baseline pre-trained on JFT-300M for various medical tasks and demonstrated improved data-efficient generalization, reducing data needs by 3–100x for adapting models to new clinical settings. This could potentially translate to significant reduction in clinician hours saved annotating data and cost of developing robust medical imaging systems.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1Q1VS1l1Xd4SqEqJySE7xjxr1L9X_g0lWXY3c8IHBVTsOWQ1X2gwVMbFeAKXHGo6kH3J1PUDsG8OPWKV1zC0dtGNwa6jETWEl9G2affZxyEtzmwOkwxoiO6MntRwrMxRo425fKMT4Q775O-YzAQmW-7k3McZmFq1qmW1i0UwA1vygRADWOJP62psERA/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1683&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1Q1VS1l1Xd4SqEqJySE7xjxr1L9X_g0lWXY3c8IHBVTsOWQ1X2gwVMbFeAKXHGo6kH3J1PUDsG8OPWKV1zC0dtGNwa6jETWEl9G2affZxyEtzmwOkwxoiO6MntRwrMxRo425fKMT4Q775O-YzAQmW-7k3McZmFq1qmW1i0UwA1vygRADWOJP62psERA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS is compatible with &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo-V2&lt;/a>;,&lt;a href=&quot;https://arxiv.org/pdf/2010.07922.pdf&quot;>; RELIC&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.03230&quot;>;Barlow Twins&lt;/a>; as alternate self-supervised learning strategies. All the REMEDIS variants lead to data-efficient generalization improvements over the strong supervised baseline for dermatology condition classification (&lt;strong>;T1&lt;/strong>;), diabetic macular edema classification (&lt;strong>;T2&lt;/strong>;), and chest X-ray condition classification (&lt;strong>;T3&lt;/strong>;). The gray shaded area indicates the performance of the strong supervised baseline pre-trained on JFT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Medical AI Research Foundations&lt;/h2>; &lt;p>; Building on REMEDIS, we are excited to announce &lt;a href=&quot;https://doi.org/10.13026/psq3-vj24&quot;>;Medical AI Research Foundations&lt;/a>;, an expansion of the public release of &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;chest X-ray Foundations&lt;/a>; in 2022. Medical AI Research Foundations is a repository of open-source medical foundation models hosted by PhysioNet. This expands the previous API-based approach to also encompass non-diagnostic models, to help researchers and developers accelerate their medical AI research. We believe that REMEDIS and the release of the Medical AI Research Foundations are a step toward building medical models that can generalize across healthcare settings and tasks. &lt;/p>; &lt;p>; We are seeding Medical AI Research Foundations with REMEDIS models for chest X-ray and pathology (with related&lt;a href=&quot;https://github.com/google-research/medical-ai-research-foundationshttps://github.com/google-research/medical-ai-research-foundations&quot;>; code&lt;/a>;). Whereas the existing chest X-ray Foundation approach focuses on providing frozen embeddings for application-specific fine tuning from a model trained on several large private datasets, the REMEDIS models (trained on public datasets) enable users to fine-tune end-to-end for their application, and to run on local devices. We recommend users test different approaches based on their unique needs for their desired application. We expect to add more models and resources for training medical foundation models such as datasets and benchmarks in the future. We also welcome the medical AI research community to contribute to this. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; These results suggest that REMEDIS has the potential to significantly accelerate the development of ML systems for medical imaging, which can preserve their strong performance when deployed in a variety of changing contexts. We believe this is an important step forward for medical imaging AI to deliver a broad impact. Beyond the experimental results presented, the approach and insights described here have been integrated into several of &lt;a href=&quot;https://health.google/health-research/&quot;>;Google&#39;s medical imaging research projects&lt;/a>;, such as dermatology, mammography and radiology among others. We&#39;re using a similar self-supervised learning approach with our non-imaging foundation model efforts, such as &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; and &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;Med-PaLM 2&lt;/a>;. &lt;/p>; &lt;p>; With REMEDIS, we demonstrated the potential of foundation models for medical imaging applications. Such models hold exciting possibilities in medical applications with the opportunity of multimodal representation learning. The practice of medicine is inherently multimodal and incorporates information from images, electronic health records, sensors, wearables, genomics and more. We believe ML systems that leverage these data at scale using self-supervised learning with careful consideration of privacy, safety, fairness and ethics will help lay the groundwork for the next generation of learning health systems that scale world-class healthcare to everyone. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;This work involved extensive collaborative efforts from a multidisciplinary team of researchers, software engineers, clinicians, and cross-functional contributors across Google Health AI and Google Brain. In particular, we would like to thank our first co-author Jan Freyberg and our lead senior authors of these projects, Vivek Natarajan, Alan Karthikesalingam, Mohammad Norouzi and Neil Houlsby for their invaluable contributions and support. We also thank Lauren Winer, Sami Lachgar, Yun Liu and Karan Singhal for their feedback on this post and Tom Small for support in creating the visuals. Finally, we also thank the PhysioNet team for their support on hosting Medical AI Research Foundations. Users with questions can reach out to medical-ai-research-foundations at google.com.&lt;/i>; &lt;/p>; &lt;/div>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8846807972710427001/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8846807972710427001&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8846807972710427001&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html&quot; rel=&quot;alternate&quot; title=&quot;Robust and efficient medical imaging with self-supervision&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvMtAszRzkUmNz3DliRRKOPMA0hi-EplY9vfJ9cWCZhFBUgnblKjVoObTYAfybreL7HEKh11g2unV5oR_KOWfkPxpe3Hzo6qWSWq4YxA9RuhkeGXdDJST2XvW2Nfl3L6kDpV1hA2b_vpBMVaw2_pxt2vWD2n92S9fDGAZp0a4c2dV2Ylf3XrUSLQxAoQ/s72-w640-h640-c/REMEDIS%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1427927967438262189&lt;/id>;&lt;published>;2023-04-25T10:20:00.002-07:00&lt;/published>;&lt;updated>;2023-04-25T15:21:23.219-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AutoML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Compression&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Neural Networks&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;LayerNAS: Neural architecture search in polynomial complexity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yicheng Fan and Dana Alon, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s1200/image4.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Every byte and every operation matters when trying to build a faster model, especially if the model is to run on-device. &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;>;Neural architecture search&lt;/a>; (NAS) algorithms design sophisticated model architectures by searching through a larger model-space than what is possible manually. Different NAS algorithms, such as &lt;a href=&quot;https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html&quot;>;MNasNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2008.06120&quot;>;TuNAS&lt;/a>;, have been proposed and have discovered several efficient model architectures, including &lt;a href=&quot;https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html&quot;>;MobileNetV3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html&quot;>;EfficientNet&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Here we present &lt;a href=&quot;https://arxiv.org/abs/2304.11517&quot;>;LayerNAS&lt;/a>;, an approach that reformulates the multi-objective NAS problem within the framework of &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization&lt;/a>; to greatly reduce the complexity, which results in an order of magnitude reduction in the number of model candidates that must be searched, less computation required for multi-trial searches, and the discovery of model architectures that perform better overall. Using a search space built on backbones taken from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetV2&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html&quot;>;MobileNetV3&lt;/a>;, we find models with top-1 accuracy on &lt;a href=&quot;https://image-net.org/&quot;>;ImageNet&lt;/a>; up to 4.9% better than current state-of-the-art alternatives. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Problem formulation&lt;/h2>; &lt;p>; NAS tackles a variety of different problems on different search spaces. To understand what LayerNAS is solving, let&#39;s start with a simple example: You are the owner of GBurger and are designing the flagship burger, which is made up with three layers, each of which has four options with different costs. Burgers taste differently with different mixtures of options. You want to make the most delicious burger you can that comes in under a certain budget. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGEuvaD4YhxpcPiekUL6LnVFXI7h6V_iq5geFR6O5N3zv5SjRu36Vn4pxvI2WJM5cZgolJM1MOasbbe6HtsTf6TIfjtmDZrNGCP5Q946iaLygvyj8RIxbLVq3l9EiwmzfitnubP34jsuybOjiUxZ0ptJ6H_rKfIZa0rOYqE0pOew2yWYymObM7IPy0gg/s1194/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1194&quot; height=&quot;261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGEuvaD4YhxpcPiekUL6LnVFXI7h6V_iq5geFR6O5N3zv5SjRu36Vn4pxvI2WJM5cZgolJM1MOasbbe6HtsTf6TIfjtmDZrNGCP5Q946iaLygvyj8RIxbLVq3l9EiwmzfitnubP34jsuybOjiUxZ0ptJ6H_rKfIZa0rOYqE0pOew2yWYymObM7IPy0gg/w400-h261/image3.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Make up your burger with different options available for each layer, each of which has different costs and provides different benefits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Just like the architecture for a neural network, the search space for the perfect burger follows a layerwise pattern, where each layer has several options with different changes to costs and performance. This simplified model illustrates a common approach for setting up search spaces. For example, for models based on convolutional neural networks (CNNs), like &lt;a href=&quot;https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html&quot;>;MobileNet&lt;/a>;, the NAS algorithm can select between a different number of options — filters, strides, or kernel sizes, etc. — for the convolution layer. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Method&lt;/h2>; &lt;p>; We base our approach on search spaces that satisfy two conditions: &lt;/p>; &lt;ul>; &lt;li>;An optimal model can be constructed using one of the model candidates generated from searching the previous layer and applying those search options to the current layer. &lt;/li>;&lt;li>;If we set a &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOP&lt;/a>; constraint on the current layer, we can set constraints on the previous layer by reducing the FLOPs of the current layer. &lt;/li>; &lt;/ul>; &lt;p>; Under these conditions it is possible to search linearly, from layer 1 to layer &lt;em>;n&lt;/em>; knowing that when searching for the best option for layer &lt;em>;i&lt;/em>;, a change in any previous layer will not improve the performance of the model. We can then bucket candidates by their cost, so that only a limited number of candidates are stored per layer. If two models have the same FLOPs, but one has better accuracy, we only keep the better one, and assume this won&#39;t affect the architecture of following layers. Whereas the search space of a full treatment would expand exponentially with layers since the full range of options are available at each layer, our layerwise cost-based approach allows us to significantly reduce the search space, while being able to rigorously reason over the polynomial complexity of the algorithm. Our experimental evaluation shows that within these constraints we are able to discover top-performance models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NAS as a combinatorial optimization problem&lt;/h2>; &lt;p>; By applying a layerwise-cost approach, we reduce NAS to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization problem&lt;/a>;. Ie, for layer &lt;em>;i&lt;/em>;, we can compute the cost and reward after training with a given component &lt;em>;S&lt;sub>;i&lt;/sub>;&lt;/em>; . This implies the following combinatorial problem: &lt;em>;How can we get the best reward if we select one choice per layer within a cost budget?&lt;/em>; This problem can be solved with many different methods, one of the most straightforward of which is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming&quot;>;dynamic programming&lt;/a>;, as described in the following pseudo code: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;font-size: small; text-align: left;&quot;>; &lt;pre class=&quot;”prettyprint”&quot; margin-right:40px=&quot;&quot; pre-wrap=&quot;&quot; style=&quot;margin-left: 40px;&quot; white-space:=&quot;&quot;>;&lt;span style=&quot;color: blue;&quot;>;while True&lt;/span>;: &lt;span style=&quot;color: mediumorchid;&quot;>;# select a candidate to search in Layer i&lt;/span>; candidate = select_candidate(layeri) &lt;span style=&quot;color: blue;&quot;>;if&lt;/span>; searchable(candidate): &lt;span style=&quot;color: mediumorchid;&quot;>;# Use the layerwise structural information to generate the children.&lt;/span>; children = generate_children(candidate) reward = train(children) bucket = bucketize(children) &lt;span style=&quot;color: blue;&quot;>;if&lt;/span>; memorial_table[i][bucket] &amp;lt; reward: memorial_table[i][bucket] = children move to &lt;span style=&quot;color: blue;&quot;>;next&lt;/span>; layer &lt;/pre>; &lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pseudocode of LayerNAS.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s1200/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the LayerNAS approach for the example of trying to create the best burger within a budget of $7–$9. We have four options for the first layer, which results in four burger candidates. By applying four options on the second layer, we have 16 candidates in total. We then bucket them into ranges from $1–$2, $3–$4, $5–$6, and $7–$8, and only keep the most delicious burger within each of the buckets, ie, four candidates. Then, for those four candidates, we build 16 candidates using the pre-selected options for the first two layers and four options for each candidate for the third layer. We bucket them again, select the burgers within the budget range, and keep the best one.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; When comparing NAS algorithms, we evaluate the following metrics: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Quality&lt;/em>;: What is the most accurate model that the algorithm can find? &lt;/li>;&lt;li>;&lt;em>;Stability&lt;/em>;: How stable is the selection of a good model? Can high-accuracy models be consistently discovered in consecutive trials of the algorithm? &lt;/li>;&lt;li>;&lt;em>;Efficiency&lt;/em>;: How long does it take for the algorithm to find a high-accuracy model? &lt;/li>; &lt;/ul>; &lt;p>; We evaluate our algorithm on the standard benchmark &lt;a href=&quot;https://arxiv.org/abs/2009.00437&quot;>;NATS-Bench&lt;/a>; using 100 NAS runs, and we compare against other NAS algorithms, previously described in the NATS-Bench paper: random search, &lt;a href=&quot;https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html&quot;>;regularized evolution&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;>;proximal policy optimization&lt;/a>;. Below, we visualize the differences between these search algorithms for the metrics described above. For each comparison, we record the average accuracy and variation in accuracy (variation is noted by a shaded region corresponding to the 25% to 75% &lt;a href=&quot;https://en.wikipedia.org/wiki/Interquartile_range&quot;>;interquartile range&lt;/a>;). &lt;/p>; &lt;p>; NATS-Bench size search defines a 5-layer CNN model, where each layer can choose from eight different options, each with different channels on the convolution layers. Our goal is to find the best model with 50% of the FLOPs required by the largest model. LayerNAS performance stands apart because it formulates the problem in a different way, separating the cost and reward to avoid searching a significant number of irrelevant model architectures. We found that model candidates with fewer channels in earlier layers tend to yield better performance, which explains how LayerNAS discovers better models much faster than other algorithms, as it avoids spending time on models outside the desired cost range. Note that the accuracy curve drops slightly after searching longer due to the lack of correlation between validation accuracy and test accuracy, ie, some model architectures with higher validation accuracy have a lower test accuracy in NATS-Bench size search. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;262&quot; data-original-width=&quot;393&quot; height=&quot;266&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbQb-b4kQHp_93-t0brOV-7GNKfTv4ZDQ5XAxJOrmWwsRHkgsl3u2H14Dp0nRwiNoHoiR5Fd5CBozSj85iQ27OniH5BtW_AIhA4VDRcSVP8BGp6rJfPaNJfOKgQ6T-lvWMY5eHCVUqYyspU0i7imUUIGaZA6pO2RbnLxQ5IDeDOjtQSDANcSoWa9DFBw/w400-h266/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;400&quot; />;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFDH_4dHxfYjdc-8AfWxfYSdzXHxesQzUF3qm4G17PONWVmxGKH9qcxL7dc0Ql7-sHvAhllUXXic3Xea3wOMmmKR0XbV364VpHZE7LTQAzDQ18jeD0JWj970Sj4hOecUqezf128NGm6IQkbQXfSHpYnDBTa5qBOpIUunr7z9DW1_wZa8FZqY59E9PrxA/s393/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;266&quot; data-original-width=&quot;393&quot; height=&quot;271&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFDH_4dHxfYjdc-8AfWxfYSdzXHxesQzUF3qm4G17PONWVmxGKH9qcxL7dc0Ql7-sHvAhllUXXic3Xea3wOMmmKR0XbV364VpHZE7LTQAzDQ18jeD0JWj970Sj4hOecUqezf128NGm6IQkbQXfSHpYnDBTa5qBOpIUunr7z9DW1_wZa8FZqY59E9PrxA/w400-h271/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9xD2WsoSrTSEyxCRd15iFc5I16zYF55rotV0URUcULNzl1GUkOh4b89vV_r147CP8xK3b_0GH5BfgGW24cGDVaiXTXjCrH4qsnsZ8jXNl3LEZR9cNrMocEHvuovNTeQI4OJaUQXocxxRhbeJspl9X7Bgp5AFCXtSqf5bz1ejCgWGGnhpPKHr_ysMQkQ/s393/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;266&quot; data-original-width=&quot;393&quot; height=&quot;271&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9xD2WsoSrTSEyxCRd15iFc5I16zYF55rotV0URUcULNzl1GUkOh4b89vV_r147CP8xK3b_0GH5BfgGW24cGDVaiXTXjCrH4qsnsZ8jXNl3LEZR9cNrMocEHvuovNTeQI4OJaUQXocxxRhbeJspl9X7Bgp5AFCXtSqf5bz1ejCgWGGnhpPKHr_ysMQkQ/w400-h271/image6.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Top:&lt;/b>; NATS-Bench size search test accuracy on &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;>;Cifar10&lt;/a>;; &lt;b>;Middle:&lt;/b>; On &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;>;Cifar100&lt;/a>;; &lt;b>;Bottom:&lt;/b>; On &lt;a href=&quot;https://image-net.org/&quot;>;ImageNet16-120&lt;/a>;. Average on 100 runs compared with random search (random), &lt;a href=&quot;https://arxiv.org/abs/1802.01548&quot;>;Regularized Evolution&lt;/a>; (evolution), and &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;>;Proximal Policy Optimization&lt;/a>; (PPO). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We construct search spaces based on MobileNetV2, MobileNetV2 1.4x, MobileNetV3 Small, and MobileNetV3 Large and search for an optimal model architecture under different #MADDs (number of multiply-additions per image) constraints. Among all settings, LayerNAS finds a model with better accuracy on ImageNet. See the &lt;a href=&quot;https://arxiv.org/abs/2304.11517&quot;>;paper&lt;/a>; for details. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsxOl7N7q8-V8N4obUKVTDRjjIkmArrMUF_r4kZUF6-Cl0aJ5CUVgf78Bx4w3XfuGaevnZktn6zJ3EITTZF-vDRp5Yln_tYTIJcFqlK0JrSy2JDcORstzU7ipYeaKJgkNdtNXhpgWgViuYTYtc8jSPeCvlw7GL06MqgcbYDxxzSEb6QCM0pOiw3ociyw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1333&quot; data-original-width=&quot;1999&quot; height=&quot;426&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsxOl7N7q8-V8N4obUKVTDRjjIkmArrMUF_r4kZUF6-Cl0aJ5CUVgf78Bx4w3XfuGaevnZktn6zJ3EITTZF-vDRp5Yln_tYTIJcFqlK0JrSy2JDcORstzU7ipYeaKJgkNdtNXhpgWgViuYTYtc8jSPeCvlw7GL06MqgcbYDxxzSEb6QCM0pOiw3ociyw/w640-h426/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison on models under different #MAdds.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this post, we demonstrated how to reformulate NAS into a combinatorial optimization problem, and proposed LayerNAS as a solution that requires only polynomial search complexity. We compared LayerNAS with existing popular NAS algorithms and showed that it can find improved models on NATS-Bench. We also use the method to find better architectures based on MobileNetV2, and MobileNetV3. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Jingyue Shen, Keshav Kumar, Daiyi Peng, Mingxing Tan, Esteban Real, Peter Young, Weijun Wang, Qifei Wang, Xuanyi Dong, Xin Wang, Yingjie Miao, Yun Long, Zhuo Wang, Da-Cheng Juan, Deqiang Chen, Fotis Iliopoulos, Han-Byul Kim, Rino Lee, Andrew Howard, Erik Vee, Rina Panigrahy, Ravi Kumar and Andrew Tomkins for their contribution, collaboration and advice.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1427927967438262189/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/layernas-neural-architecture-search-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1427927967438262189&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1427927967438262189&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/layernas-neural-architecture-search-in.html&quot; rel=&quot;alternate&quot; title=&quot;LayerNAS: Neural architecture search in polynomial complexity&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s72-c/image4.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-190478125834380745&lt;/id>;&lt;published>;2023-04-23T19:26:00.001-07:00&lt;/published>;&lt;updated>;2023-04-23T19:30:30.342-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Google at CHI 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWsabyBZRJDH9o64PiYE4YHaRa21wHoI4lgm0SGzcL5dqNK132QoCw0K-rUSrxAG2lfIO2Z8F8rR8OfotsxiJ-hXUAt3G2mtZ_jOsI_84r59EYcW_q6wZ-kR3Hi3H8yH1mZA8ReGI_jfkr7wGUc1A4BrMcWB7D7X8GjSUEc818RoY3X7QZ3G7wCVWeFQ/w640-h640/Google%20Germany%20Sticker%20Art%20Vecto%20PO%2013031.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://chi2023.acm.org/&quot;>;Conference on Human Factors in Computing Systems&lt;/a>; (CHI 2023) is being held in Hamburg, Germany. We are proud to be a &lt;a href=&quot;https://chi2023.acm.org/for-sponsors-exhibitors-and-recruiters/sponsoring/chi-2023-sponsors/&quot;>;Hero Sponsor&lt;/a>; of CHI 2023, a premier conference on human-computer interaction, where Google researchers contribute at all levels. This year we are presenting over 30 papers and are actively involved in organizing and hosting a number of different events across workshops, courses, and interactive sessions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you&#39;re registered for CHI 2023, we hope you&#39;ll visit the Google booth to learn more about the exciting work across various topics, including language interactions, causal inference, question answering and more. Take a look below to learn more about the Google research being presented at CHI 2023 (Google affiliations in bold). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;p>; Technical Program Chairs include: &lt;strong>;&lt;i>;Tesh Goyal&lt;/i>;&lt;/strong>; &lt;/p>; &lt;p>; Case Studies Chairs include: &lt;strong>;&lt;i>;Frank Bentley&lt;/i>;&lt;/strong>; &lt;/p>; &lt;p>; Keynotes Chairs include: &lt;strong>;&lt;i>;Elizabeth Churchill&lt;/i>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Best Paper Award&lt;/h2>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/98030d5f96d6f383c90e40fcbf79425919f178d3.pdf&quot;>;Infrastructuring Care: How Trans and Non-Binary People Meet Health and Well-Being Needs through Technology&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Renee Shelby&lt;/i>;&lt;/b>;, &lt;i>;Rajesh Veeraraghavan&lt;/i>;, &lt;i>;Oliver Haimson&lt;/i>;, &lt;b>;&lt;i>;Gabriela Erickson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Turken&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Beka Gulotta &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.04219.pdf&quot;>;NewsComp: Facilitating Diverse News Reading through Comparative Annotation&lt;/a>; &lt;br />; &lt;i>;Md Momen Bhuiyan&lt;/i>;, &lt;i>;Sang Won Lee&lt;/i>;, &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;, &lt;i>;Tanushree Mitra&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://www.shuminzhai.com/_files/ugd/62dd15_d0a29020b5e5488f9b9adb1e43b470b9.pdf&quot;>;WordGesture-GAN: Modeling Word-Gesture Movement with Generative Adversarial Network&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Jeremy Chu&lt;/i>;, &lt;i>;Dongsheng An&lt;/i>;, &lt;i>;Yan Ma&lt;/i>;, &lt;i>;Wenzhe Cui&lt;/i>;, &lt;b>;&lt;i>;Shumin Zhai&lt;/i>;&lt;/b>;, &lt;i>;Xianfeng David Gu&lt;/i>;, &lt;i>;Xiaojun Bi&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c02ad27506f5ce120cedfcd26246f782aa525625.pdf&quot;>;“The less I type, the better”: How AI Language Models can Enhance or Impede Communication for AAC Users&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Stephanie Valencia&lt;/i>;&lt;/b>;, &lt;i>;Richard Cave&lt;/i>;, &lt;b>;&lt;i>;Krystal Kallarackal&lt;/i>;&lt;/b>;, &lt;i>;Katie Seaver&lt;/i>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shaun Kane&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00164v2.pdf&quot;>;A Mixed-Methods Approach to Understanding User Trust after Voice Assistant Failures&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Amanda Baughan&lt;/i>;*, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ariel Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Allison Mercurio&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jilin Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiao Ma&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08057.pdf&quot;>;&quot;There&#39;s so much responsibility on users right now:&quot; Expert Advice for Staying Safer From Hate and Harassment&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Miranda Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sunny Consolvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Patrick Gage Kelley&lt;/i>;&lt;/b>;, &lt;i>;Tadayoshi Kohno&lt;/i>;, &lt;i>;Franziska Roesner&lt;/i>;, &lt;b>;&lt;i>;Kurt Thomas&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/20779824f83ae742db71b4711af67e389c189768.pdf&quot;>;ThingShare: Ad-Hoc Digital Copies of Physical Objects for Sharing Things in Video Meetings&lt;/a>; &lt;br />; &lt;i>;Erzhen Hu&lt;/i>;, &lt;i>;Jens Emil Sloth Grønbæk&lt;/i>;, &lt;i>;Wen Ying&lt;/i>;, &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;, &lt;i>;Seongkook Heo&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dfreed.me/wp-content/uploads/2023/02/Digital_Experiences_of_Teens__CHI__23_-18.pdf&quot;>;Understanding Digital-Safety Experiences of Youth in the US&lt;/a>; &lt;br />; &lt;i>;Diana Freed&lt;/i>;, &lt;i>;Natalie N. Bazarova&lt;/i>;, &lt;i>;Sunny Consolvo&lt;/i>;, &lt;i>;Eunice Han&lt;/i>;, &lt;b>;&lt;i>;Patrick Gage Kelley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>; Kurt Thomas&lt;/i>;&lt;/b>;, &lt;i>;Dan Cosley&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/87b73c53996c05a5f9cc67b1f67e3c9e2a49b28a.pdf&quot;>;Slide Gestalt: Automatic Structure Extraction in Slide Decks for Non-Visual Access&lt;/a>; &lt;br />; &lt;i>;Yi-Hao Peng&lt;/i>;*, &lt;b>;&lt;i>;Peggy Chi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anjuli Kannan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Meredith Ringel Morris&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Irfan Essa&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.04711.pdf&quot;>;Using Logs Data to Identify When Engineers Experience Flow or Focused Work&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Adam Brown&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sarah D&#39;Angelo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ben Holtz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ciera Jaspan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Collin Green&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.08655.pdf&quot;>;Enabling Conversational Interaction with Mobile UI Using Large Language Models&lt;/a>; &lt;br />; &lt;i>;Bryan Wang&lt;/i>;*, &lt;b>;&lt;i>;Gang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yang Li&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.07184.pdf&quot;>;Practicing Information Sensibility: How Gen Z Engages with Online Information&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Amelia Hassoun&lt;/i>;, &lt;i>;Ian Beacock&lt;/i>;, &lt;b>;&lt;i>;Sunny Consolvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Beth Goldberg&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Patrick Gage Kelley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel M. Russell&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/CHI23/857/959cee87-9a41-11ed-a76e-16bb50361d1f/OUT/chi23-857.pdf&quot;>;How Bold Can We Be? The Impact of Adjusting Font Grade on Readability in Light and Dark Polarities&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Hilary Palmen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Gilbert&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dave Crossland&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.12243.pdf&quot;>;Investigating How Practitioners Use Human-AI Guidelines: A Case Study on the People + AI Guidebook&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Nur Yildirim&lt;/i>;*, &lt;b>;&lt;i>;Mahima Pushkarna&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Martin Wattenberg&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fernanda Viegas &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03535.pdf&quot;>;From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Shalaleh Rismani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Renee Shelby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew Smart&lt;/i>;&lt;/b>;, &lt;i>;Edgar W. Jatho&lt;/i>;, &lt;i>;Joshua A. Kroll&lt;/i>;, &lt;i>;AJung Moon&lt;/i>;, &lt;i>;&lt;b>;Negar Rostamzadeh&lt;/b>;&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4045f0e0e61d89b6b1eacad4b861e86631d5e660.pdf&quot;>;Designing Responsible AI: Adaptations of UX Practice to Meet Responsible AI Challenges&lt;/a>; &lt;br />; &lt;i>;Qiaosi Wang&lt;/i>;*, &lt;b>;&lt;i>;Michael Madaio&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shaun Kane&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shivani Kapania&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://www.researchgate.net/publication/368685360_It_is_currently_hodgepodge_Examining_AIML_Practitioners&#39;_Challenges_during_Co-production_of_Responsible_AI_Values&quot;>;“It is currently hodgepodge”: Examining AI/ML Practitioners&#39; Challenges during Co-production of Responsible AI Values&lt;/a>; &lt;br />; &lt;i>;Rama Adithya Varanasi&lt;/i>;, &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c5dbc2c146b7443447d43a344b4a22a359b32b16.pdf&quot;>;A Hunt for the Snark: Annotator Diversity in Data Practices&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;b>;&lt;i>;Shivani Kapania&lt;/i>;&lt;/b>;, &lt;i>;Alex S. Taylor&lt;/i>;, &lt;b>;&lt;i>;Ding Wang&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6dc693d389c226e5c70de5b40633c4ec2ddb6b43.pdf&quot;>;Visual Captions: Augmenting Verbal Communication with On-the-Fly Visuals&lt;/a>; &lt;br />; &lt;i>;Xingyu &quot;Bruce&quot; Liu&lt;/i>;, &lt;b>;&lt;i>;Vladimir Kirilyuk&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuxiu Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peggy Chi&lt;/i>;&lt;/b>;, &lt;i>; Xiang &quot;Anthony&quot; Chen&lt;/i>;, &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/98030d5f96d6f383c90e40fcbf79425919f178d3.pdf&quot;>;Infrastructuring Care: How Trans and Non-Binary People Meet Health and Well-Being Needs through Technology&lt;/a>; (&lt;em>;Best Paper Award&lt;/em>;) &lt;br />; &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Renee Shelby&lt;/i>;&lt;/b>;, &lt;i>;Rajesh Veeraraghavan&lt;/i>;, &lt;i>;Oliver Haimson&lt;/i>;, &lt;b>;&lt;i>;Gabriela Erickson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Turken&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Beka Gulotta&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://drive.google.com/file/d/1meAA_arK9rsR5H_UJBlZYsyh9eunyX14/view&quot;>;Kaleidoscope: Semantically-Grounded, Context-Specific ML Model Evaluation&lt;/a>; &lt;br />; &lt;i>;Harini Suresh&lt;/i>;, &lt;i>;Divya Shanmugam&lt;/i>;, &lt;i>;Tiffany Chen&lt;/i>;, &lt;i>;Annie G. Bryan&lt;/i>;, &lt;b>;&lt;i>;Alexander D&#39;Amour&lt;/i>;&lt;/b>;, &lt;i>;John Guttag&lt;/i>;, &lt;i>;Arvind Satyanarayan&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/405beeea55e7ae39d815e9b23fb6623cabb59ef5.pdf&quot;>;Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;; see &lt;a href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Jin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michelle Carney&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Scott Miles&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Maria Kleiner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuxiu Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yinda Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anuva Kulkarni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xingyu &quot;Bruce&quot; Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ahmed Sabie&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergio Orts-Escolano&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Abhishek Kar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ping Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ram Iyengar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adarsh Kowdle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://frankbentleycom.files.wordpress.com/2023/04/chi23m-sub6725-cam-i61.pdf&quot;>;Exploring Users&#39; Perceptions and Expectations of Shapes for Dialog Designs&lt;/a>; &lt;br />; &lt;i>;Xinghui &quot;Erica&quot; Yan&lt;/i>;, &lt;b>;&lt;i>;Julia Feldman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frank Bentley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammed Khwaja&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Gilbert &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://frankbentleycom.files.wordpress.com/2023/04/chi23m-sub2538-cam-i61.pdf&quot;>;Exploring the Future of Design Tooling: The Role of Artificial Intelligence in Tools for User Experience Professionals&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Tiffany Knearem&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammed Khwaja&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yuling Gao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frank Bentley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Clara E. Kliman-Silver&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/a6855e786038b546c11b6bbc4e55fc8f876ad45d.pdf&quot;>;SpeakFaster Observer: Long-Term Instrumentation of Eye-Gaze Typing for Measuring AAC Communication&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Shanqing Cai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Subhashini Venugopalan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katrin Tomanek&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shaun Kane&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Meredith Ringel Morris&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Richard Cave&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Robert MacDonald&lt;/i>;&lt;/b>;, &lt;i>;Jon Campbell&lt;/i>;, &lt;i>;Blair Casey&lt;/i>;, &lt;i>;Emily Kornman&lt;/i>;, &lt;i>;Daniel E. Vance&lt;/i>;, &lt;i>;Jay Beavers&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3506698&quot;>;Designerly Tele-Experiences: A New Approach to Remote Yet Still Situated Co-design&lt;/a>; &lt;br />; &lt;i>;Ferran Altarriba Bertran&lt;/i>;, &lt;i>;Alexandra Pometko&lt;/i>;, &lt;i>;Muskan Gupta&lt;/i>;, &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Reeta Banerjee&lt;/i>;&lt;/b>;, &lt;i>;Katherine Isbister&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3506698&quot;>;“I Just Wanted to Triple Check . . . They Were All Vaccinated”: Supporting Risk Negotiation in the Context of COVID-19&lt;/a>; &lt;br />; &lt;i>;Margaret E. Morris&lt;/i>;, &lt;i>;Jennifer Brown&lt;/i>;, &lt;i>;Paula Nurius&lt;/i>;, &lt;i>;Savanna Yee&lt;/i>;, &lt;i>;Jennifer C. Mankoff&lt;/i>;, &lt;b>;&lt;i>;Sunny Consolvo &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3544549.3585763&quot;>;Expectation vs Reality in Users&#39; Willingness to Delegate to Digital Assistants&lt;/a>; &lt;br />; &lt;i>;Ekaterina Svikhnushina&lt;/i>;*, &lt;b>;&lt;i>;Marcel Schellenberg&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anna K. Niedbala&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Iva Barisic&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeremy N. Miles&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3544549.3585596&quot;>;Interactive Visual Exploration of Knowledge Graphs with Embedding-Based Guidance&lt;/a>; &lt;br />; &lt;i>;Chao-Wen Hsuan Yuan&lt;/i>;, &lt;i>;Tzu-Wei Yu&lt;/i>;, &lt;b>;&lt;i>;Jia-Yu Pan&lt;/i>;&lt;/b>;, &lt;i>;Wen-Chieh Lin&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09498.pdf&quot;>;Measuring the Impact of Explanation Bias: A Study of Natural Language Justifications for Recommender Systems&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Krisztian Balog&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Filip Radlinski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrey Petrov&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://duruofei.com/papers/Liu_ModelingAndImprovingTextStabilityInLiveCaptions_CHIEA2023.pdf&quot;>;Modeling and Improving Text Stability in Live Captions&lt;/a>; &lt;br />; &lt;i>;Xingyu &quot;Bruce&quot; Liu&lt;/i>;, &lt;b>;&lt;i>;Jun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Leonardo Ferrer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Susan Xu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vikas Bahirwani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Boris Smus&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;Programming without a Programming Language: Challenges and Opportunities for Designing Developer Tools for Prompt Programming&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Alexander J. Fiannaca&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chinmay Kulkarni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carrie J. Cai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser: Bringing User Interface Mock-ups to Life with Large Language Models&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Savvas Petridis&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carrie J. Cai&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://frankbentleycom.files.wordpress.com/2023/04/chiea23-529.pdf&quot;>;Prototypes, Platforms and Protocols: Identifying Common Issues with Remote, Unmoderated Studies and Their Impact on Research Participants&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Steven Schirra&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sasha Volkov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frank Bentley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shraddhaa Narasimha&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/99315&quot;>;Human-Centered Responsible Artificial Intelligence: Current &amp;amp; Future Trends&lt;/a>; &lt;br />; &lt;i>;Mohammad Tahaei&lt;/i>;, &lt;i>;Marios Constantinides&lt;/i>;, &lt;i>;Daniele Quercia&lt;/i>;, &lt;i>;Sean Kennedy&lt;/i>;, &lt;i>;Michael Muller&lt;/i>;, &lt;i>;Simone Stumpf&lt;/i>;, &lt;i>;Q. Vera Liao&lt;/i>;, &lt;i>;Ricardo Baeza-Yates&lt;/i>;, &lt;b>;&lt;i>;Lora Aroyo&lt;/i>;&lt;/b>;, &lt;i>;Jess Holbrook&lt;/i>;, &lt;i>;Ewa Luger&lt;/i>;, &lt;i>;&lt;b>;Michael Madaio&lt;/b>;&lt;/i>;, &lt;i>;Ilana Golbin Blumenfeld&lt;/i>;, &lt;i>;Maria De-Arteaga&lt;/i>;, &lt;i>;Jessica Vitak&lt;/i>;, &lt;i>;Alexandra Olteanu&lt;/i>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Interactive sessions&lt;/h2>; &lt;p>; &lt;a href=&quot;https://duruofei.com/papers/Du_ExperiencingRapidPrototypingOfMachineLearningBasedMultimediaApplicationsInRapsai_CHIEA2023.pdf&quot;>;Experiencing Rapid Prototyping of Machine Learning Based Multimedia Applications in Rapsai&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Jin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michelle Carney&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuxiu Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ram Iyengar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ping Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adarsh Kowdle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://in2writing.glitch.me/&quot;>;The Second Workshop on Intelligent and Interactive Writing Assistants&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Minsuk Chang&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://combatingonlinetoxicity.sites.uu.nl/organizers/&quot;>;Combating Toxicity, Harassment, and Abuse in Online Social Spaces: A Workshop at CHI 2023&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/nd.edu/computational-uichi23/&quot;>;The Future of Computational Approaches for Understanding and Adapting User Interfaces&lt;/a>; &lt;br />; Keynote Speaker: &lt;b>;&lt;i>;Yang Li&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://empathich.com/&quot;>;The EmpathiCH Workshop: Unraveling Empathy-Centric Design&lt;/a>; &lt;br />; Panelists include: &lt;b>;&lt;i>;Cindy Bennett&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://chi-trait.github.io/#/&quot;>;Workshop on Trust and Reliance in AI-Human Teams (TRAIT)&lt;/a>; &lt;br />; Keynote Speakers: &lt;b>;&lt;i>;Carrie J. Cai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>; &lt;br />; Program committee includes: &lt;b>;&lt;i>;Aaron Springer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sar-decision-making/&quot;>;Socially Assistive Robots as Decision Makers: Transparency, Motivations, and Intentions&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Maja Matarić&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Courses&lt;/h2>; &lt;p>; Human-Computer Interaction and AI: What Practitioners Need to Know to Design and Build Effective AI Systems from a Human Perspective (&lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/session/99328&quot;>;Part I&lt;/a>;; &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/session/99350&quot;>;Part II&lt;/a>;) &lt;br />; &lt;i>;Daniel M. Russell&lt;/i>;, &lt;i>;Q. Vera Liao&lt;/i>;, &lt;b>;&lt;i>;Chinmay Kulkarni&lt;/i>;&lt;/b>;, &lt;i>;Elena L. Glassman&lt;/i>;, &lt;i>;Nikolas Martelaro&lt;/i>;&lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; * Work done while at Google&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/190478125834380745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-chi-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/190478125834380745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/190478125834380745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-chi-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CHI 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWsabyBZRJDH9o64PiYE4YHaRa21wHoI4lgm0SGzcL5dqNK132QoCw0K-rUSrxAG2lfIO2Z8F8rR8OfotsxiJ-hXUAt3G2mtZ_jOsI_84r59EYcW_q6wZ-kR3Hi3H8yH1mZA8ReGI_jfkr7wGUc1A4BrMcWB7D7X8GjSUEc818RoY3X7QZ3G7wCVWeFQ/s72-w640-h640-c/Google%20Germany%20Sticker%20Art%20Vecto%20PO%2013031.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6318039050629886850&lt;/id>;&lt;published>;2023-04-21T12:01:00.004-07:00&lt;/published>;&lt;updated>;2023-05-08T15:06:16.289-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Visual Blocks for ML: Accelerating machine learning prototyping with interactive tools&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ruofei Du, Interactive Perception &amp;amp; Graphics Lead, Google Augmented Reality, and Na Li, Tech Lead Manager, Google CoreML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl7alDg3lrJ1OofDgtgrcGJlHpjkoU_DThiHYOUepNt4mEUfRJckufpZD8wExahLSJC1S7XQZrznp0WkApOI8cq4-hPwxolT65igv5zx6wG0o1avhmKE2HQuz_aHAz2mld3ViB5Rc0eZnLK75EEILqsfjqhnLw7m22oK0fGVZadBWG-BRObu8oJpWKiQ/s320/Visual%20blocks%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;span style=&quot;font-size: small;&quot;>;&lt;i>;&lt;b>;Update — 2023/05/08:&lt;/b>; This post has been updated to include open-source details for the &lt;a href=&quot;https://visualblocks.withgoogle.com/#/&quot;>;Visual Blocks framework&lt;/a>;.&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>; Recent deep learning advances have enabled a plethora of high-performance, real-time multimedia applications based on machine learning (ML), such as &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;human body segmentation&lt;/a>; for video and teleconferencing, &lt;a href=&quot;https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793&quot;>;depth estimation&lt;/a>; for 3D reconstruction, &lt;a href=&quot;https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html&quot;>;hand and body tracking&lt;/a>; for interaction, and &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;audio processing&lt;/a>; for remote communication. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, developing and iterating on these ML-based multimedia prototypes can be challenging and costly. It usually involves a cross-functional team of ML practitioners who fine-tune the models, evaluate robustness, characterize strengths and weaknesses, inspect performance in the end-use context, and develop the applications. Moreover, models are frequently updated and require repeated integration efforts before evaluation can occur, which makes the workflow ill-suited to design and experiment. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://duruofei.com/papers/Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf&quot;>;Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming&lt;/a>;”, presented at &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/95967&quot;>;CHI 2023&lt;/a>;, we describe a visual programming platform for rapid and iterative development of end-to-end ML-based multimedia applications. Visual Blocks for ML, formerly called Rapsai, provides a no-code graph building experience through its &lt;a href=&quot;https://en.wikipedia.org/wiki/Node_graph_architecture&quot;>;node-graph editor&lt;/a>;. Users can create and connect different components (nodes) to rapidly build an ML pipeline, and see the results in real-time without writing any code. We demonstrate how this platform enables a better model evaluation experience through interactive characterization and visualization of ML model performance and interactive data augmentation and comparison. We have released the &lt;a href=&quot;https://visualblocks.withgoogle.com/#/&quot;>;Visual Blocks for ML&lt;/a>; framework, along with a &lt;a href=&quot;https://visualblocks.withgoogle.com/#/demo&quot;>;demo&lt;/a>; and &lt;a href=&quot;https://github.com/google/visualblocks&quot;>;Colab examples&lt;/a>;. Try it out yourself today. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0LJIJBaw9SlfR-PPhTerCqPiXkJ_6EakiQKk6LEVPs5fedNI8-OcZBXlHjmlAyo6Io-chL2MQb4tDMu4MzZtvxB3FjIbhJHw70OZ793aIhEFl6oXfOkI_NMENc-9HjlxC11k5zO_h4YPrTWJVE8l990AJXBXvitHPxxoNKa8XQch7-7JcMYojCFtGZg/s800/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;443&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0LJIJBaw9SlfR-PPhTerCqPiXkJ_6EakiQKk6LEVPs5fedNI8-OcZBXlHjmlAyo6Io-chL2MQb4tDMu4MzZtvxB3FjIbhJHw70OZ793aIhEFl6oXfOkI_NMENc-9HjlxC11k5zO_h4YPrTWJVE8l990AJXBXvitHPxxoNKa8XQch7-7JcMYojCFtGZg/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual Blocks uses a &lt;em>;node-graph editor that facilitates rapid prototyping of ML-based multimedia applications.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Formative study: Design goals for rapid ML prototyping&lt;/h2>; &lt;p>; To better understand the challenges of existing rapid prototyping ML solutions (&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2939672.2939778&quot;>;LIME&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/9751204&quot;>;VAC-CNN&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1518701.1518895&quot;>;EnsembleMatrix&lt;/a>;), we conducted a &lt;a href=&quot;https://psycnet.apa.org/record/2006-21649-000&quot;>;formative study&lt;/a>; (ie, the process of gathering feedback from potential users early in the design process of a technology product or system) using a conceptual mock-up interface. Study participants included seven computer vision researchers, audio ML researchers, and engineers across three ML teams. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRS0W-mt37RO8Af_UPkDsjg_XQyWjTTXkU1YViz-FpGIx4TqLONO9VQJftjo5dB0JBn0vIuqm8ZOFXTOTGHf8SPjnZoFe6y-wUErxXTz82-VPvyyUtT1OBWNnsThyE-Of-6yurGHUiRBkQiWuegVMi8rbLjE9FE6133XsD7aJJyFWpTCi0CKl7-nmAFg/s1039/mockup.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;690&quot; data-original-width=&quot;1039&quot; height=&quot;425&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRS0W-mt37RO8Af_UPkDsjg_XQyWjTTXkU1YViz-FpGIx4TqLONO9VQJftjo5dB0JBn0vIuqm8ZOFXTOTGHf8SPjnZoFe6y-wUErxXTz82-VPvyyUtT1OBWNnsThyE-Of-6yurGHUiRBkQiWuegVMi8rbLjE9FE6133XsD7aJJyFWpTCi0CKl7-nmAFg/w640-h425/mockup.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The formative study used a conceptual mock-up interface to gather early insights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this formative study, we identified six challenges commonly found in existing prototyping solutions: &lt;/p>; &lt;ol>; &lt;li>;The input used to evaluate models typically differs from in-the-wild input with actual users in terms of resolution, aspect ratio, or sampling rate. &lt;/li>;&lt;li>;Participants could not quickly and interactively alter the input data or tune the model. &lt;/li>;&lt;li>;Researchers optimize the model with quantitative metrics on a fixed set of data, but real-world performance requires human reviewers to evaluate in the application context. &lt;/li>;&lt;li>;It is difficult to compare versions of the model, and cumbersome to share the best version with other team members to try it. &lt;/li>;&lt;li>;Once the model is selected, it can be time-consuming for a team to make a bespoke prototype that showcases the model. &lt;/li>;&lt;li>;Ultimately, the model is just part of a larger real-time pipeline, in which participants desire to examine intermediate results to understand the bottleneck. &lt;/li>; &lt;/ol>; &lt;p>; These identified challenges informed the development of the Visual Blocks system, which included six design goals: (1) develop a visual programming platform for rapidly building ML prototypes, (2) support real-time multimedia user input in-the-wild, (3) provide interactive data augmentation, (4) compare model outputs with side-by-side results, (5) share visualizations with minimum effort, and (6) provide off-the-shelf models and datasets. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Node-graph editor for visually programming ML pipelines&lt;/h2>; &lt;p>; Visual Blocks is mainly written in JavaScript and leverages &lt;a href=&quot;https://www.tensorflow.org/js&quot;>;TensorFlow.js&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; for ML capabilities and &lt;a href=&quot;https://threejs.org/&quot;>;three.js&lt;/a>; for graphics rendering. The interface enables users to rapidly build and interact with ML models using three coordinated views: (1) a &lt;strong>;Nodes Library&lt;/strong>; that contains over 30 nodes (eg, Image Processing, Body Segmentation, Image Comparison) and a search bar for filtering, (2) a &lt;strong>;Node-graph Editor&lt;/strong>; that allows users to build and adjust a multimedia pipeline by dragging and adding nodes from the Nodes Library, and (3) a &lt;strong>;Preview Panel&lt;/strong>; that visualizes the pipeline&#39;s input and output, alters the input and intermediate results, and visually compares different models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYXWn2G2_Ra1f1gwxbk5GPlkpG5wTnNS0qm-GnfdxsLhc477T_Ig2kbeLI_tOI9Ad0QoAAPc-wCrcebPxjhw0k9L2JlWq2U2Y2aWEcbw95NP2mBu1pCVGmXodS0-fLqDiy4787gZM2REfOmBmfoz1pMQ51iurqksSg-WtvVYaclfpa8X69V22Fq09h6A/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYXWn2G2_Ra1f1gwxbk5GPlkpG5wTnNS0qm-GnfdxsLhc477T_Ig2kbeLI_tOI9Ad0QoAAPc-wCrcebPxjhw0k9L2JlWq2U2Y2aWEcbw95NP2mBu1pCVGmXodS0-fLqDiy4787gZM2REfOmBmfoz1pMQ51iurqksSg-WtvVYaclfpa8X69V22Fq09h6A/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The visual programming interface allows users to quickly develop and evaluate ML models by composing and previewing node-graphs with real-time results.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Iterative design, development, and evaluation of unique rapid prototyping capabilities&lt;/h2>; &lt;p>; Over the last year, we&#39;ve been iteratively designing and improving the Visual Blocks platform. Weekly feedback sessions with the three ML teams from the formative study showed appreciation for the platform&#39;s unique capabilities and its potential to accelerate ML prototyping through: &lt;/p>; &lt;ul>; &lt;li>;Support for various types of input data (image, video, audio) and output modalities (graphics, sound). &lt;/li>;&lt;li>;A library of pre-trained ML models for common tasks (body segmentation, landmark detection, portrait depth estimation) and custom model import options. &lt;/li>;&lt;li>;Interactive data augmentation and manipulation with drag-and-drop operations and parameter sliders. &lt;/li>;&lt;li>;Side-by-side comparison of multiple models and inspection of their outputs at different stages of the pipeline. &lt;/li>;&lt;li>;Quick publishing and sharing of multimedia pipelines directly to the web. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation: Four case studies&lt;/h2>; &lt;p>; To evaluate the usability and effectiveness of Visual Blocks, we conducted four case studies with 15 ML practitioners. They used the platform to prototype different multimedia applications: &lt;a href=&quot;https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html&quot;>;portrait depth with relighting effects&lt;/a>;, &lt;a href=&quot;https://augmentedperception.github.io/depthlab/&quot;>;scene depth with visual effects&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/01/accurate-alpha-matting-for-portrait.html&quot;>;alpha matting for virtual conferences&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2203.15578&quot;>;audio denoising for communication&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJTZjauOp5u2CP6ZCWD7V-E3okn17rDbcHLRRUNAZiygGbZfsTH0KaH5QRYJBj_HoLAKkLrX0Cw6wHrjRR2vTju-xC5GZzCEQp21w8Odwpo7IOyQQUHWusYvr4fzgc5RU1xP388AZuPr0LSQfSXaNl56fMCATZGydrmh6TwrUYC2ZmPRFa9zocWJFL_A/s1890/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1282&quot; data-original-width=&quot;1890&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJTZjauOp5u2CP6ZCWD7V-E3okn17rDbcHLRRUNAZiygGbZfsTH0KaH5QRYJBj_HoLAKkLrX0Cw6wHrjRR2vTju-xC5GZzCEQp21w8Odwpo7IOyQQUHWusYvr4fzgc5RU1xP388AZuPr0LSQfSXaNl56fMCATZGydrmh6TwrUYC2ZmPRFa9zocWJFL_A/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The system streamlining comparison of two &lt;a href=&quot;https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793&quot;>;Portrait Depth&lt;/a>; models, including customized visualization and effects.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With a short introduction and video tutorial, participants were able to quickly identify differences between the models and select a better model for their use case. We found that Visual Blocks helped facilitate rapid and deeper understanding of model benefits and trade-offs: &lt;/p>; &lt;blockquote>;&lt;span style=&quot;font-size: small;&quot;>;&lt;i>;“It gives me intuition about which data augmentation operations that my model is more sensitive [to], then I can go back to my training pipeline, maybe increase the amount of data augmentation for those specific steps that are making my model more sensitive.” (Participant 13)&lt;/i>;&lt;/span>;&lt;/blockquote>; &lt;blockquote>;&lt;span style=&quot;font-size: small;&quot;>; &lt;i>; “It&#39;s a fair amount of work to add some background noise, I have a script, but then every time I have to find that script and modify it. I&#39;ve always done this in a one-off way. It&#39;s simple but also very time consuming. This is very convenient.” (Participant 15)&lt;/i>;&lt;/span>;&lt;/blockquote>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDZJpy0nnMh4O6RuoRbWQ78LzcYWA57nQXwUMy9UtB4mu0pKgnZ4Uo2FDixcgEx6OorTPKBpquX1HGdWL-kPUNSxVpQSwb0PcbX2wMWpghnNbnIFYTSfIVT1oS5LEqmz1inahGAcFNDpb3Jyohu107tuQdLVLNmZ-4kX4CBU25WrlYjV-c_sQC21v-0A/s720/depth.mov.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;389&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDZJpy0nnMh4O6RuoRbWQ78LzcYWA57nQXwUMy9UtB4mu0pKgnZ4Uo2FDixcgEx6OorTPKBpquX1HGdWL-kPUNSxVpQSwb0PcbX2wMWpghnNbnIFYTSfIVT1oS5LEqmz1inahGAcFNDpb3Jyohu107tuQdLVLNmZ-4kX4CBU25WrlYjV-c_sQC21v-0A/s16000/depth.mov.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The system allows researchers to compare multiple &lt;a href=&quot;https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793&quot;>;Portrait Depth&lt;/a>; models at different noise levels, helping ML practitioners identify the strengths and weaknesses of each.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In a post-hoc survey using a seven-point &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale#:~:text=Likert%20scaling%20is%20a%20bipolar,the%20neutral%20option%20is%20removed.&quot;>;Likert scale&lt;/a>;, participants reported Visual Blocks to be more transparent about how it arrives at its final results than Colab (Visual Blocks 6.13 ± 0.88 vs. &lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/vision_transformer/blob/main/lit.ipynb&quot;>;Colab&lt;/a>; 5.0 ± 0.88, 𝑝 &amp;lt; .005) and more collaborative with users to come up with the outputs (Visual Blocks 5.73 ± 1.23 vs. Colab 4.15 ± 1.43, 𝑝 &amp;lt; .005). Although Colab assisted users in thinking through the task and controlling the pipeline more effectively through programming, Users reported that they were able to complete tasks in Visual Blocks in just a few minutes that could normally take up to an hour or more. For example, after watching a 4-minute tutorial video, all participants were able to build a custom pipeline in Visual Blocks from scratch within 15 minutes (10.72 ± 2.14). Participants usually spent less than five minutes (3.98 ± 1.95) getting the initial results, then were trying out different input and output for the pipeline. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaPnqjJDwShP4G-S-QplGrbGpdkevN8ZivhcP8h_8APwFGP0VWLnkwH5MB57qGLVOpE_cyiwJKMHqDWJBp5gr2fryTvO2HZtdRvFINYQfOC1CrsCRo2gUn1zrkXDHOCrLsXLJgpqYhj1b0sFuAmgWND5HOZV_V9HpzOJwES8ecvMWgpmvCTKskgSkvnw/s1511/rapsai-eval.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;483&quot; data-original-width=&quot;1511&quot; height=&quot;205&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaPnqjJDwShP4G-S-QplGrbGpdkevN8ZivhcP8h_8APwFGP0VWLnkwH5MB57qGLVOpE_cyiwJKMHqDWJBp5gr2fryTvO2HZtdRvFINYQfOC1CrsCRo2gUn1zrkXDHOCrLsXLJgpqYhj1b0sFuAmgWND5HOZV_V9HpzOJwES8ecvMWgpmvCTKskgSkvnw/w640-h205/rapsai-eval.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User ratings between Rapsai (initial prototype of Visual Blocks) and Colab across five dimensions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More results in &lt;a href=&quot;https://duruofei.com/papers/Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf&quot;>;our paper&lt;/a>; showed that Visual Blocks helped participants accelerate their workflow, make more informed decisions about model selection and tuning, analyze strengths and weaknesses of different models, and holistically evaluate model behavior with real-world input. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusions and future directions&lt;/h2>; &lt;p>;&lt;a href=&quot;https://visualblocks.withgoogle.com/#/&quot;>;Visual Blocks&lt;/a>; lowers development barriers for ML-based multimedia applications. It empowers users to experiment without worrying about coding or technical details. It also facilitates collaboration between designers and developers by providing a common language for describing ML pipelines. In the future, we plan to open this framework up for the community to contribute their own nodes and integrate it into many different platforms. We expect visual programming for machine learning to be a common interface across ML tooling going forward. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;This work is a collaboration across multiple teams at Google. Key contributors to the project include Ruofei Du, Na Li, Jing Jin, Michelle Carney, Xiuxiu Yuan, Kristen Wright, Mark Sherwood, Jason Mayes, Lin Chen, Jun Jiang, Scott Miles, Maria Kleiner, Yinda Zhang, Anuva Kulkarni, Xingyu &quot;Bruce&quot; Liu, Ahmed Sabie, Sergio Escolano, Abhishek Kar, Ping Yu, Ram Iyengar, Adarsh Kowdle, and Alex Olwal.&lt;/i>; &lt;/p>; &lt;p>; &lt;i>;We would like to extend our thanks to Jun Zhang, Satya Amarapalli and Sarah Heimlich for a few early-stage prototypes, Sean Fanello, Danhang Tang, Stephanie Debats, Walter Korman, Anne Menini, Joe Moran, Eric Turner, and Shahram Izadi for providing initial feedback for the manuscript and the blog post. We would also like to thank our CHI 2023 reviewers for their insightful feedback.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6318039050629886850/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6318039050629886850&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6318039050629886850&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html&quot; rel=&quot;alternate&quot; title=&quot;Visual Blocks for ML: Accelerating machine learning prototyping with interactive tools&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl7alDg3lrJ1OofDgtgrcGJlHpjkoU_DThiHYOUepNt4mEUfRJckufpZD8wExahLSJC1S7XQZrznp0WkApOI8cq4-hPwxolT65igv5zx6wG0o1avhmKE2HQuz_aHAz2mld3ViB5Rc0eZnLK75EEILqsfjqhnLw7m22oK0fGVZadBWG-BRObu8oJpWKiQ/s72-c/Visual%20blocks%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8031022821156747802&lt;/id>;&lt;published>;2023-04-20T13:26:00.001-07:00&lt;/published>;&lt;updated>;2023-04-20T13:30:15.380-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Recent advances in deep long-horizon forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Abhimanyu Das, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjazCNV8O9s-ZwT9g5cBDsMQ_gv2SMf5enK_cMdtOamv-WJGXlB3qzhZme8xVfXhA2xFS0HwaDBlh3d6z3R2FS_qW05ouD2z9jQ5sgIizM4VcE6-i6BSf4MJmin1KmfEhkkrVxfZUj-H2ePsGYjqlnCrMqBtk68uHplGODDSmeu8FABJ_ldthUpR0WF_A/s1600/scalar.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time-series&lt;/a>; forecasting is an important research area that is critical to several scientific and industrial applications, like retail supply chain optimization, energy and traffic prediction, and weather forecasting. In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&quot;>;improving demand forecasting accuracy&lt;/a>; can meaningfully reduce inventory costs and increase revenue. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Modern time-series applications can involve forecasting hundreds of thousands of correlated time-series (eg, demands of different products for a retailer) over long horizons (eg, a quarter or year away at daily granularity). As such, time-series forecasting models need to satisfy the following key criterias: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Ability to handle auxiliary features or covariates:&lt;/em>; Most use-cases can benefit tremendously from effectively using covariates, for instance, in retail forecasting, holidays and product specific attributes or promotions can affect demand. &lt;/li>;&lt;li>;&lt;em>;Suitable for different data modalities:&lt;/em>; It should be able to handle sparse count data, eg, intermittent demand for a product with low volume of sales while also being able to model robust continuous seasonal patterns in traffic forecasting. &lt;/li>; &lt;/ol>; &lt;p>; A number of neural network–based solutions have been able to show good performance on benchmarks and also support the above criterion. However, these methods are typically slow to train and can be expensive for inference, especially for longer horizons. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;Long-term Forecasting with TiDE: Time-series Dense Encoder&lt;/a>;”, we present an all &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) encoder-decoder architecture for time-series forecasting that achieves superior performance on long horizon time-series forecasting benchmarks when compared to &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformer&lt;/a>;-based solutions, while being 5–10x faster. Then in “&lt;a href=&quot;https://openreview.net/forum?id=zrW-LVXj2k1&quot;>;On the benefits of maximum likelihood estimation for Regression and Forecasting&lt;/a>;”, we demonstrate that using a carefully designed training loss function based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;>;maximum likelihood estimation&lt;/a>; (MLE) can be effective in handling different data modalities. These two works are complementary and can be applied as a part of the same model. In fact, they will be available soon in &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>;Google Cloud AI&#39;s Vertex AutoML Forecasting&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;TiDE: A simple MLP architecture for fast and accurate forecasting&lt;/h2>; &lt;p>; Deep learning has shown promise in time-series forecasting, &lt;a href=&quot;https://proceedings.mlr.press/v162/zhou22g.html&quot;>;outperforming traditional statistical methods, especially for large multivariate datasets&lt;/a>;. After the success of &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformers&lt;/a>; in &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP), there have been several works evaluating variants of the Transformer architecture for long horizon (the amount of time into the future) forecasting, such as &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;FEDformer&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;. However, &lt;a href=&quot;https://arxiv.org/pdf/2205.13504.pdf&quot;>;other work&lt;/a>; has suggested that even linear models can outperform these transformer variants on time-series benchmarks. Nonetheless, simple linear models are not expressive enough to handle auxiliary features (eg, holiday features and promotions for retail demand forecasting) and non-&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_function&quot;>;linear&lt;/a>; dependencies on the past. &lt;/p>; &lt;p>; We present a scalable MLP-based encoder-decoder model for fast and accurate multi-step forecasting. Our model encodes the past of a time-series and all available features using an MLP encoder. Subsequently, the encoding is combined with future features using an MLP decoder to yield future predictions. The architecture is illustrated below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAvTJSswOJMJL_hACqexTMAxEV9faTAYLT_rxH_0mT9AlsBC8neR1iumIVwi30vlZK7US4BpLRBgRu-5uzjdhKwpizYWrO_3Cwwh0xjkKkpwkdF0He9CRyidSva8lgznEB7X-e36hEE0E22eXrWf2c2nqbGZPGE0YPFZ8a-5GR2daV4gwBb8dstVdu-g/s850/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;850&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAvTJSswOJMJL_hACqexTMAxEV9faTAYLT_rxH_0mT9AlsBC8neR1iumIVwi30vlZK7US4BpLRBgRu-5uzjdhKwpizYWrO_3Cwwh0xjkKkpwkdF0He9CRyidSva8lgznEB7X-e36hEE0E22eXrWf2c2nqbGZPGE0YPFZ8a-5GR2daV4gwBb8dstVdu-g/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TiDE model architecture for multi-step forecasting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; TiDE is more than 10x faster in training compared to transformer-based baselines while being more accurate on benchmarks. Similar gains can be observed in inference as it only scales linearly with the length of the context (the number of time-steps the model looks back) and the prediction horizon. Below on the left, we show that our model can be 10.6% better than the best transformer-based baseline (PatchTST) on a popular traffic forecasting benchmark, in terms of test &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;mean squared error (MSE)&lt;/a>;. On the right, we show that at the same time our model can have much faster inference latency than PatchTST. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbSEnIDH_ZmJgpxkPTqvTYpaHmNeTOSryb7lgBxWk8zj7OTVeg8NGRkDy70AW3CQBKBPeVO9K7h8DgbylbRcUBZomd1W53L1ZCVrjN7OVcWykXYisEDvlHLZVdF6Rx_3e257_UYFu81xRdrXstStF-4x_VIDiyiRgvhztHpiW_Ts_qPwSO3MMgAI44lw/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;584&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbSEnIDH_ZmJgpxkPTqvTYpaHmNeTOSryb7lgBxWk8zj7OTVeg8NGRkDy70AW3CQBKBPeVO9K7h8DgbylbRcUBZomd1W53L1ZCVrjN7OVcWykXYisEDvlHLZVdF6Rx_3e257_UYFu81xRdrXstStF-4x_VIDiyiRgvhztHpiW_Ts_qPwSO3MMgAI44lw/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left:&lt;/b>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;MSE&lt;/a>; on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets&quot;>;test set&lt;/a>; of a popular traffic forecasting benchmark. &lt;b>;Right:&lt;/b>; inference time of TiDE and PatchTST as a function of the look-back length.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our research demonstrates that we can take advantage of MLP&#39;s linear computational scaling with look-back and horizon sizes without sacrificing accuracy, while transformers scale quadratically in this situation. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Probabilistic loss functions&lt;/h2>; &lt;p>; In most forecasting applications the end user is interested in popular target metrics like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_percentage_error&quot;>;mean absolute percentage error&lt;/a>; (MAPE), &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_percentage_error#WMAPE&quot;>;weighted absolute percentage error&lt;/a>; (WAPE), etc. In such scenarios, the standard approach is to use the same target metric as the loss function while training. In “&lt;a href=&quot;https://openreview.net/forum?id=zrW-LVXj2k1&quot;>;On the benefits of maximum likelihood estimation for Regression and Forecasting&lt;/a>;”, accepted at &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;ICLR&lt;/a>;, we show that this approach might not always be the best. Instead, we advocate using the maximum likelihood loss for a carefully chosen family of distributions (discussed more below) that can capture inductive biases of the dataset during training. In other words, instead of directly outputting point predictions that minimize the target metric, the forecasting neural network predicts the parameters of a distribution in the chosen family that best explains the target data. At inference time, we can predict the statistic from the learned predictive distribution that minimizes the target metric of interest (eg, the mean minimizes the MSE target metric while the median minimizes the WAPE). Further, we can also easily obtain uncertainty estimates of our forecasts, ie, we can provide quantile forecasts by estimating the quantiles of the predictive distribution. In several use cases, accurate quantiles are vital, for instance, in demand forecasting a retailer might want to stock for the 90th &lt;a href=&quot;https://en.wikipedia.org/wiki/Percentile&quot;>;percentile&lt;/a>; to guard against worst-case scenarios and avoid lost revenue. &lt;/p>; &lt;p>; The choice of the distribution family is crucial in such cases. For example, in the context of sparse count data, we might want to have a distribution family that can put more probability on zero, which is commonly known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-inflated_model&quot;>;zero-inflation&lt;/a>;. We propose a mixture of different distributions with learned mixture weights that can adapt to different data modalities. In the paper, we show that using a mixture of zero and multiple negative binomial distributions works well in a variety of settings as it can adapt to sparsity, multiple modalities, count data, and data with &lt;a href=&quot;https://en.wikipedia.org/wiki/Heavy-tailed_distribution&quot;>;sub-exponential tails&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpER0cWQ65hFEvCvFvsD-PrOxgw-6WBOSPzDxM0lZOft7j8KiKfugaBJc2l4kvpI6_6udTYjPdgdRgELPSOhwnwviPImdBY4R6Y8bVpfhkqtY9boDMHqQLiJ0yEiEtWI4Z6QnHy3uSVVltBcoiEfw7SqQEtnF9tqFqj7puNKZmO-BaRuTubpxsD-l4Kw/s960/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpER0cWQ65hFEvCvFvsD-PrOxgw-6WBOSPzDxM0lZOft7j8KiKfugaBJc2l4kvpI6_6udTYjPdgdRgELPSOhwnwviPImdBY4R6Y8bVpfhkqtY9boDMHqQLiJ0yEiEtWI4Z6QnHy3uSVVltBcoiEfw7SqQEtnF9tqFqj7puNKZmO-BaRuTubpxsD-l4Kw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A mixture of zero and two negative binomial distributions. The weights of the three components, a&lt;sub>;1&lt;/sub>;, a&lt;sub>;2&lt;/sub>; and a&lt;sub>;3&lt;/sub>;, can be learned during training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We use this loss function for training Vertex AutoML models on the &lt;a href=&quot;https://www.kaggle.com/c/m5-forecasting-accuracy&quot;>;M5 forecasting competition&lt;/a>; dataset and show that this simple change can lead to a 6% gain and outperform other benchmarks in the competition metric, &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy/overview/evaluation&quot;>;weighted root mean squared scaled error&lt;/a>; (WRMSSE). &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;M5 Forecasting&lt;/b>; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;b>;WRMSSE&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Vertex AutoML &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.639 +/- 0.007 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Vertex AutoML with probabilistic loss&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;b>;0.581 +/- 0.007&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;DeepAR &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.789 +/- 0.025 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;FEDFormer &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.804 +/- 0.033 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:140%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have shown how TiDE, together with probabilistic loss functions, enables fast and accurate forecasting that automatically adapts to different data distributions and modalities and also provides uncertainty estimates for its predictions. It provides state-of-the-art accuracy among neural network–based solutions at a fraction of the cost of previous transformer-based forecasting architectures, for large-scale enterprise forecasting applications. We hope this work will also spur interest in revisiting (both theoretically and empirically) MLP-based deep time-series forecasting models. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Pranjal Awasthi, Dawei Jia, Weihao Kong, Andrew Leach, Shaan Mathur, Petros Mol, Shuxin Nie, Ananda Theertha Suresh, and Rose Yu. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8031022821156747802/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031022821156747802&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031022821156747802&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html&quot; rel=&quot;alternate&quot; title=&quot;Recent advances in deep long-horizon forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjazCNV8O9s-ZwT9g5cBDsMQ_gv2SMf5enK_cMdtOamv-WJGXlB3qzhZme8xVfXhA2xFS0HwaDBlh3d6z3R2FS_qW05ouD2z9jQ5sgIizM4VcE6-i6BSf4MJmin1KmfEhkkrVxfZUj-H2ePsGYjqlnCrMqBtk68uHplGODDSmeu8FABJ_ldthUpR0WF_A/s72-c/scalar.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6000199110548559167&lt;/id>;&lt;published>;2023-04-19T11:12:00.002-07:00&lt;/published>;&lt;updated>;2023-04-19T14:11:04.992-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Technology, AI, Society and Culture&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Lauren Wilcox, Senior Staff Research Scientist, on behalf of the Technology, AI, Society, and Culture Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjojA8Eq3h0gUbJQ6ttpMVcy6mzFucxjbGvEpKL2jnxLoQaAqSYYTwrw4q6GLtCGOeNlThZw4-uj9oQjmYlJyb_vNgEWHoE1jUfHP-bUslCjN6eeJGZvMcfQkaZf-E4RRg06yhmC0giabaqlRupcYOmWi46uZC3VnLmpX1ZQn3C_wtw8gp8zVubx_o0dw/s620/TASC-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google sees &lt;a href=&quot;https://blog.google/technology/ai/why-we-focus-on-ai-and-to-what-end/&quot;>;AI as a foundational and transformational technology&lt;/a>;, with recent advances in generative AI technologies, such as &lt;a href=&quot;https://arxiv.org/abs/2201.08239&quot;>;LaMDA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>;, &lt;a href=&quot;https://research.google/pubs/pub52118/&quot;>;MusicLM&lt;/a>;, and similar machine learning (ML) models, some of which are now being incorporated into &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/generative-ai&quot;>;our products&lt;/a>;. This transformative potential requires us to be responsible not only in how we advance our technology, but also in how we envision which technologies to build, and how we assess the social impact AI and ML-enabled technologies have on the world. This endeavor necessitates fundamental and applied research with an &lt;a href=&quot;https://doi.org/10.17226/26507&quot;>;interdisciplinary lens&lt;/a>; that engages with — and accounts for — the social, cultural, economic, and other contextual dimensions that shape the development and deployment of AI systems. We must also understand the range of possible impacts that ongoing use of such technologies may have on vulnerable communities and broader social systems.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Our team, Technology, AI, Society, and Culture (TASC), is addressing this critical need. Research on the societal impacts of AI is complex and multi-faceted; no one disciplinary or methodological perspective can alone provide the diverse insights needed to grapple with the social and cultural implications of ML technologies. TASC thus leverages the strengths of an interdisciplinary team, with backgrounds ranging from computer science to social science, digital media and urban science. We use a multi-method approach with qualitative, quantitative, and mixed methods to critically examine and shape the social and technical processes that underpin and surround AI technologies. We focus on participatory, culturally-inclusive, and intersectional equity-oriented research that brings to the foreground impacted communities. Our work advances Responsible AI (RAI) in areas such as &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://sites.google.com/corp/view/c3nlp/home&quot;>;natural language processing&lt;/a>;, &lt;a href=&quot;https://aaas.confex.com/aaas/2023/meetingapp.cgi/Session/29974&quot;>;health&lt;/a>;, and general purpose ML models and applications. Below, we share examples of our approach to &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI&lt;/a>; and where we are headed in 2023. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7CrSVLFHgjk2TP-7oA0-suXvnDKTBxOBQjmgUz6QHn9nhiPcPZ19GFXsaU2kl9vlc1j8H6lm2oz0_bcor5vOaMrB3TK2-hKFnTps2yyo_m9QLlNnc75KgiYuj52O1U6bEeuv57eswy3SF1rPsyer3y42KrHOpN8Puzvn-fC_mloWJNh9Pfg2QnOwMHA/s1392/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;675&quot; data-original-width=&quot;1392&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7CrSVLFHgjk2TP-7oA0-suXvnDKTBxOBQjmgUz6QHn9nhiPcPZ19GFXsaU2kl9vlc1j8H6lm2oz0_bcor5vOaMrB3TK2-hKFnTps2yyo_m9QLlNnc75KgiYuj52O1U6bEeuv57eswy3SF1rPsyer3y42KrHOpN8Puzvn-fC_mloWJNh9Pfg2QnOwMHA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual diagram of the various social, technical, and equity-oriented research areas that TASC studies to progress Responsible AI in a way that respects the complex relationships between AI and society.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Theme 1: Culture, communities, &amp;amp; AI&lt;/h2>; &lt;p>; One of our key areas of research is the advancement of methods to make generative AI technologies more inclusive of and valuable to people globally, through &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517716&quot;>;community-engaged&lt;/a>;, and &lt;a href=&quot;https://ai-cultures.github.io/&quot;>;culturally-inclusive&lt;/a>; approaches. Toward this aim, we see communities as experts in their context, recognizing their deep knowledge of how technologies can and should impact their own lives. Our research champions the importance of embedding &lt;a href=&quot;https://arxiv.org/pdf/2211.13069.pdf&quot;>;cross-cultural considerations&lt;/a>; throughout the ML development pipeline. Community engagement enables us to shift how we incorporate knowledge of what&#39;s most important throughout this pipeline, from dataset curation to evaluation. This also enables us to understand and account for the ways in which technologies fail and how specific communities might experience harm. Based on this understanding we have created &lt;a href=&quot;https://aclanthology.org/2022.aacl-main.55/&quot;>;responsible AI evaluation strategies&lt;/a>; that are effective in recognizing and mitigating biases along multiple dimensions. &lt;/p>; &lt;p>; Our work in this area is vital to ensuring that Google&#39;s technologies are safe for, work for, and are useful to a diverse set of stakeholders around the world. For example, our research on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517533&quot;>;user attitudes towards AI&lt;/a>;, &lt;a href=&quot;https://research.google/pubs/pub52061/&quot;>;responsible interaction design&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/pdf/2209.12226.pdf&quot;>;fairness evaluations&lt;/a>; with a focus on the global south demonstrated the cross-cultural differences in the impact of AI and contributed resources that enable culturally-situated evaluations. We are also building cross-disciplinary research communities to examine the relationship between AI, culture, and society, through our recent and upcoming workshops on &lt;a href=&quot;https://ai-cultures.github.io/&quot;>;Cultures in AI/AI in Culture&lt;/a>;, &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision&lt;/a>;, and &lt;a href=&quot;https://sites.google.com/corp/view/c3nlp&quot;>;Cross-Cultural Considerations in NLP&lt;/a>;. &lt;/p>; &lt;p>; Our recent research has also sought out perspectives of particular communities who are known to be less represented in ML development and applications. For example, we have investigated gender bias, both in &lt;a href=&quot;https://blog.google/technology/ai/reducing-gender-based-harms-in-ai-with-sunipa-dev/&quot;>;natural language&lt;/a>; and in contexts such as &lt;a href=&quot;https://research.google/pubs/pub52060/&quot;>;gender-inclusive health&lt;/a>;, drawing on our research to develop more accurate evaluations of bias so that anyone developing these technologies can identify and mitigate harms for people with &lt;a href=&quot;https://facctconference.org/2022/acceptedcraft.html#colab&quot;>;queer and non-binary identities&lt;/a>;. &lt;br />; &lt;/p>; &lt;br />; &lt;h2>;Theme 2: Enabling Responsible AI throughout the development lifecycle&lt;/h2>; &lt;p>; We work to enable RAI at scale, by establishing industry-wide best practices for RAI across the development pipeline, and ensuring our technologies verifiably incorporate that best practice by default. This applied research includes responsible data production and analysis for ML development, and systematically advancing tools and practices that support practitioners in meeting key RAI goals like transparency, fairness, and accountability. Extending earlier work on &lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533231&quot;>;Data Cards&lt;/a>;, &lt;a href=&quot;https://modelcards.withgoogle.com/model-reports&quot;>;Model Cards&lt;/a>; and the &lt;a href=&quot;https://www.tensorflow.org/responsible_ai/model_card_toolkit/guide&quot;>;Model Card Toolkit&lt;/a>;, we released the &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;, providing developers with methods and tools to document appropriate uses and essential facts related to a dataset. Because ML models are often trained and evaluated on human-annotated data, we also advance human-centric research on data annotation. We have developed &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3531146.3534647&quot;>;frameworks to document annotation processes&lt;/a>; and methods to account for &lt;a href=&quot;https://arxiv.org/abs/2110.05719&quot;>;rater disagreement&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2301.09406&quot;>;rater diversity&lt;/a>;. These methods enable ML practitioners to better ensure &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c5dbc2c146b7443447d43a344b4a22a359b32b16.pdf&quot;>;diversity in annotation of datasets&lt;/a>; used to train models, by identifying current barriers and re-envisioning data work practices. &lt;/p>; &lt;br />; &lt;h2>;Future directions&lt;/h2>; &lt;p>; We are now working to further broaden participation in ML model development, through approaches that embed a diversity of cultural contexts and voices into technology design, development, and impact assessment to ensure that AI achieves societal goals. We are also redefining responsible practices that can handle the scale at which ML technologies operate in today&#39;s world. For example, we are developing frameworks and structures that can enable community engagement within industry AI research and development, including community-centered evaluation frameworks, benchmarks, and dataset curation and sharing. &lt;/p>; &lt;p>; In particular, we are furthering our prior work on understanding how &lt;a href=&quot;https://aclanthology.org/2020.acl-main.487.pdf&quot;>;NLP language models &lt;span>;may perpetuate bias against people with disabilities&lt;/span>;&lt;/a>;, extending this research to address other marginalized communities and cultures and including image, video, and other multimodal models. Such models may contain tropes and stereotypes about particular groups or may erase the experiences of specific individuals or communities. Our efforts to identify sources of bias within ML models will lead to better detection of these representational harms and will support the creation of more fair and inclusive systems. &lt;/p>; &lt;p>; TASC is about studying all the touchpoints between AI and people — from individuals and communities, to cultures and society. For AI to be culturally-inclusive, equitable, accessible, and reflective of the needs of impacted communities, we must take on these challenges with inter- and multidisciplinary research that centers the needs of impacted communities. Our research studies will continue to explore the interactions between society and AI, furthering the discovery of new ways to develop and evaluate AI in order for us to develop more robust and culturally-situated AI technologies. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank everyone on the team that contributed to this blog post. In alphabetical order by last name: Cynthia Bennett, Eric Corbett, Aida Mostafazadeh Davani, Emily Denton, Sunipa Dev, Fernando Diaz, Mark Díaz, Shaun Kane, Shivani Kapania, Michael Madaio, Vinodkumar Prabhakaran, Rida Qadri, Renee Shelby, Ding Wang, and Andrew Zaldivar. Also, we would like to thank Toju Duke and Marian Croak for their valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6000199110548559167/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6000199110548559167&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6000199110548559167&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Technology, AI, Society and Culture&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjojA8Eq3h0gUbJQ6ttpMVcy6mzFucxjbGvEpKL2jnxLoQaAqSYYTwrw4q6GLtCGOeNlThZw4-uj9oQjmYlJyb_vNgEWHoE1jUfHP-bUslCjN6eeJGZvMcfQkaZf-E4RRg06yhmC0giabaqlRupcYOmWi46uZC3VnLmpX1ZQn3C_wtw8gp8zVubx_o0dw/s72-c/TASC-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-331432973525797076&lt;/id>;&lt;published>;2023-04-18T13:19:00.000-07:00&lt;/published>;&lt;updated>;2023-04-18T13:19:38.487-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private heatmaps&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Badih Ghazi, Staff Research Scientist, and Nachiappan Valliappan, Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgS24x7QsXnFRdTdq_PJYQOr4XqJSpsFBWcb630Ujz2vOcGLL0FzXcvaDq_xwd72wCVXs3U5oNPYz0JgLJYNDrfyqf_0Fzuri6CJg2s8OzNVwsWRzzlU23NkGtre3XMlhoxf9egBUJRyEfEeIjdsRJ8m2L3gFJYCbNefqY2XNNGotPNJF6lMOM1DrlbPA/s1100/DPheatmaps.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Recently, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) has emerged as a mathematically robust notion of user privacy for data aggregation and machine learning (ML), with practical deployments including the &lt;a href=&quot;https://www.census.gov/about/policies/privacy/statistical_safeguards.html&quot;>;2022 US Census&lt;/a>; and in industry. Over the last few years, we have open-sourced &lt;a href=&quot;https://github.com/google/differential-privacy/&quot;>;libraries&lt;/a>; for privacy-preserving &lt;a href=&quot;https://developers.googleblog.com/2019/09/enabling-developers-and-organizations.html&quot;>;analytics&lt;/a>; and &lt;a href=&quot;https://blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.html&quot;>;ML&lt;/a>; and have been &lt;a href=&quot;https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&quot;>;constantly enhancing&lt;/a>; their capabilities. Meanwhile, new algorithms have been developed by the research community for several analytic tasks involving private aggregation of data. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One such important data aggregation method is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Heat_map&quot;>;heatmap&lt;/a>;. Heatmaps are popular for visualizing aggregated data in two or more dimensions. They are widely used in many fields including computer vision, image processing, spatial data analysis, bioinformatics, and more. Protecting the privacy of user data is critical for many applications of heatmaps. For example, heatmaps for &lt;a href=&quot;https://en.wikipedia.org/wiki/Gene_expression_profiling&quot;>;gene microdata&lt;/a>; are based on private data from individuals. Similarly, a heatmap of popular locations in a geographic area are based on user location check-ins that need to be kept private. &lt;/p>; &lt;p>; Motivated by such applications, in “&lt;a href=&quot;https://arxiv.org/pdf/2211.13454.pdf&quot;>;Differentially Private Heatmaps&lt;/a>;” (presented at &lt;a href=&quot;https://aaai-23.aaai.org/&quot;>;AAAI 2023&lt;/a>;), we describe an efficient DP algorithm for computing heatmaps with provable guarantees and evaluate it empirically. At the core of our DP algorithm for heatmaps is a solution to the basic problem of how to privately aggregate sparse input vectors (ie, input vectors with a small number of non-zero coordinates) with a small error as measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&quot;>;Earth Mover&#39;s Distance&lt;/a>; (EMD). Using a hierarchical partitioning procedure, our algorithm views each input vector, as well as the output heatmap, as a probability distribution over a number of items equal to the dimension of the data. For the problem of sparse aggregation under EMD, we give an efficient algorithm with error asymptotically close to the best possible. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithm description&lt;/h2>; &lt;p>; Our algorithm works by privatizing the aggregated distribution (obtained by averaging over all user inputs), which is sufficient for computing a final heatmap that is private due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Robustness_to_post-processing&quot;>;the post-processing property of DP&lt;/a>;. This property ensures that any transformation of the output of a DP algorithm remains differentially private. Our main contribution is a new privatization algorithm for the aggregated distribution, which we will describe next. &lt;/p>; &lt;p>; The EMD measure, which is a distance-like measure of dissimilarity between two probability distributions originally proposed for computer vision tasks, is well-suited for heatmaps since it takes the underlying metric space into account and considers &quot;neighboring&quot; bins. EMD is used in a variety of applications including &lt;a href=&quot;https://ai.googleblog.com/2021/06/using-variational-transformer-networks.html&quot;>;deep learning&lt;/a>;, spatial analysis, human mobility, image retrieval, face recognition, visual tracking, shape matching, and more. &lt;/p>; &lt;p>; To achieve DP, we need to add noise to the aggregated distribution. We would also like to preserve statistics at different scales of the grid to minimize the EMD error. So, we create a hierarchical partitioning of the grid, add noise at each level, and then recombine into the final DP aggregated distribution. In particular, the algorithm has the following steps: &lt;/p>; &lt;ol>; &lt;li>;&lt;b>;Quadtree construction:&lt;/b>; Our hierarchical partitioning procedure first divides the grid into four cells, then divides each cell into four subcells; it recursively continues this process until each cell is a single pixel. This procedure creates a &lt;a href=&quot;https://en.wikipedia.org/wiki/Quadtree&quot;>;quadtree&lt;/a>; over the subcells where the root represents the entire grid and each leaf represents a pixel. The algorithm then calculates the total probability mass for each tree node (obtained by adding up the aggregated distribution&#39;s probabilities of all leaves in the subtree rooted at this node). This step is illustrated below. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgf3kAHyEgmTCQfW3jRdmr9cVQ4j0FH2eqxwrVlx27nPaKV4n-AUzugBfqaj0mAhSbSNOoq6H_Ta9DRTfIjTx9zYOhkHrE4f65pqqhNOq-JOFRXMPIa9SGuCDTgB0PSu3saaCV1C7YQ1bukiwBveyI9Y_gAgwCHEUVIlVkIZIY_py90aAuBuR8svzXzRQ/s960/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgf3kAHyEgmTCQfW3jRdmr9cVQ4j0FH2eqxwrVlx27nPaKV4n-AUzugBfqaj0mAhSbSNOoq6H_Ta9DRTfIjTx9zYOhkHrE4f65pqqhNOq-JOFRXMPIa9SGuCDTgB0PSu3saaCV1C7YQ1bukiwBveyI9Y_gAgwCHEUVIlVkIZIY_py90aAuBuR8svzXzRQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the first step, we take the (non-private) aggregated distribution (&lt;b>;top left&lt;/b>;) and repeatedly divide it to create a quadtree. Then, we compute the total probability mass is each cell (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;li>;&lt;b>;Noise addition:&lt;/b>; To each tree node&#39;s mass we then add &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_mechanisms#Laplace_Mechanism&quot;>;Laplace noise&lt;/a>; calibrated to the use case. &lt;/li>;&lt;li>;&lt;b>;Truncation:&lt;/b>; To help reduce the final amount of noise in our DP aggregated distribution, the algorithm traverses the tree starting from the root and, at each level, it discards all but the top &lt;em>;w&lt;/em>; nodes with highest (noisy) masses together with their descendants. &lt;/li>;&lt;li>;&lt;b>;Reconstruction:&lt;/b>; Finally, the algorithm solves a &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_programming&quot;>;linear program&lt;/a>; to recover the aggregated distribution. This linear program is inspired by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Compressed_sensing&quot;>;sparse recovery &lt;/a>;literature where the noisy masses are viewed as (noisy) measurements of the data. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizD4PvQcqzf9UyCIF7m2DYgA14QJ7TewHebY0rZSZrpDPHTNdGPKJpNc-aqS4y8MzMQuXisMJxy8nXRdgX7frNu9Xqh9G-YhZMHNet4Df1hz_VM17v3TTFHUCPEzIJH1_WYZ9SYrsUocoeZpqZIqOlq8LGUlyCmPBNybnyrlVBIPL8dBvxI6Aw2LQ33A/s960/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizD4PvQcqzf9UyCIF7m2DYgA14QJ7TewHebY0rZSZrpDPHTNdGPKJpNc-aqS4y8MzMQuXisMJxy8nXRdgX7frNu9Xqh9G-YhZMHNet4Df1hz_VM17v3TTFHUCPEzIJH1_WYZ9SYrsUocoeZpqZIqOlq8LGUlyCmPBNybnyrlVBIPL8dBvxI6Aw2LQ33A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In step 2, noise is added to each cell&#39;s probability mass. Then in step 3, only top-w cells are kept (&lt;b>;green&lt;/b>;) whereas the remaining cells are truncated (&lt;b>;red&lt;/b>;). Finally, in the last step, we write a linear program on these top cells to reconstruct the aggregation distribution, which is now differentially private.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We evaluate the performance of our algorithm in two different domains: real-world location check-in data and image saliency data. We consider as a baseline the ubiquitous &lt;a href=&quot;https://doi.org/10.1007/11681878_14&quot;>;Laplace mechanism&lt;/a>;, where we add Laplace noise to each cell, zero out any negative cells, and produce the heatmap from this noisy aggregate. We also consider a “thresholding” variant of this baseline that is more suited to sparse data: only keep top &lt;em>;t&lt;/em>;% of the cell values (based on the probability mass in each cell) after noising while zeroing out the rest. To evaluate the quality of an output heatmap compared to the true heatmap, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;>;Pearson coefficient&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL-divergence&lt;/a>;, and EMD. Note that when the heatmaps are more similar, the first metric increases but the latter two decrease. &lt;/p>; &lt;p>; The locations dataset is obtained by combining two datasets, &lt;a href=&quot;http://snap.stanford.edu/data/loc-Gowalla.html&quot;>;Gowalla&lt;/a>; and &lt;a href=&quot;http://snap.stanford.edu/data/loc-Brightkite.html&quot;>;Brightkite&lt;/a>;, both of which contain check-ins by users of location-based social networks. We pre-processed this dataset to consider only check-ins in the continental US resulting in a final dataset consisting of ~500,000 check-ins by ~20,000 users. Considering the top cells (from an initial partitioning of the entire space into a 300 x 300 grid) that have check-ins from at least 200 unique users, we partition each such cell into subgrids with a resolution of ∆ × ∆ and assign each check-in to one of these subgrids. &lt;/p>; &lt;p>; In the first set of experiments, we fix ∆ = 256. We test the performance of our algorithm for different values of ε (the &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Definition_of_%CE%B5-differential_privacy&quot;>;privacy parameter&lt;/a>;, where smaller ε means stronger DP guarantees), ranging from 0.1 to 10, by running our algorithms together with the baseline and its variants on all cells, randomly sampling a set of 200 users in each trial, and then computing the distance metrics between the true heatmap and the DP heatmap. The average of these metrics is presented below. Our algorithm (the red line) performs better than all versions of the baseline across all metrics, with improvements that are especially significant when ε is not too large or small (ie, 0.2 ≤ ε ≤ 5). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaJ4ZoMvzmgh-1RtxelE3BPryTE_OsHzQBzO4JAA6gli0XO0AZa_yTocYwqQXP1PY5Jt7nF77PF6j6LqVCvxAhAljvfKfZHb-9WxqYwTLepWEVrnM7bqdE__GBuSc35jGil5G5PrVs0qEUuWEgA7lQy19ok5cmrgKvbezrBMB197dthwGZ9iZkf0tolQ/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;647&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaJ4ZoMvzmgh-1RtxelE3BPryTE_OsHzQBzO4JAA6gli0XO0AZa_yTocYwqQXP1PY5Jt7nF77PF6j6LqVCvxAhAljvfKfZHb-9WxqYwTLepWEVrnM7bqdE__GBuSc35jGil5G5PrVs0qEUuWEgA7lQy19ok5cmrgKvbezrBMB197dthwGZ9iZkf0tolQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Metrics averaged over 60 runs when varying ε for the location dataset. Shaded areas indicate 95% confidence interval.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Next, we study the effect of varying the number &lt;em>;n&lt;/em>; of users. By fixing a single cell (with &amp;gt; 500 users) and ε, we vary &lt;em>;n&lt;/em>; from 50 to 500 users. As predicted by theory, our algorithms and the baseline perform better as &lt;em>;n&lt;/em>; increases. However, the behavior of the thresholding variants of the baseline are less predictable. &lt;/p>; &lt;p>; We also run another experiment where we fix a single cell and ε, and vary the resolution ∆ from 64 to 256. In agreement with theory, our algorithm&#39;s performance remains nearly constant for the entire range of ∆. However, the baseline suffers across all metrics as ∆ increases while the thresholding variants occasionally improve as ∆ increases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlu-390vSrt7mMXlCdyVTKEahZohQKC7U_P7C7poSvE23bODHZLlj9CtjeF3kvF8UxCJDf7O8s9TcT49QH4vbmRhc7An8b8GjhBVMh1tVPM7upoS9dYqtGoPruom5Cd6uKUch-Hhs7fpYdSx17dfF1cYYmDCBFj1U8krV-OCmkwZT-INKWAPrmmWCBcw/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlu-390vSrt7mMXlCdyVTKEahZohQKC7U_P7C7poSvE23bODHZLlj9CtjeF3kvF8UxCJDf7O8s9TcT49QH4vbmRhc7An8b8GjhBVMh1tVPM7upoS9dYqtGoPruom5Cd6uKUch-Hhs7fpYdSx17dfF1cYYmDCBFj1U8krV-OCmkwZT-INKWAPrmmWCBcw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of the number of users and grid resolution on EMD.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also experiment on the &lt;a href=&quot;http://salicon.net/&quot;>;Salicon image saliency dataset (SALICON&lt;/a>;). This dataset is a collection of &lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;saliency annotations&lt;/a>; on the Microsoft &lt;a href=&quot;https://cocodataset.org/#home&quot;>;Common Objects in Context&lt;/a>; image database. We downsized the images to a fixed resolution of 320 × 240 and each [user, image] pair consists of a sequence of coordinates in the image where the user looked. We repeat the experiments described previously on 38 randomly sampled images (with ≥ 50 users each) from SALICON. As we can see from the examples below, the heatmap obtained by our algorithm is very close to the ground truth. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGgnVoDZw3huIVPMGOAeOHGggcZcnd2IWcrlSyMnDS7RixKyBV12dtGF3ofDR8mxYy6CEElatdAyNIcxkyk91jZ2c-7_kSnnZ1omPLuUwWjnl019yQdj1NlmCf0844xwNeFH9uvTbwnmjOEpUszF8nztKpt_TBMiZcaX4f3_rIBZDpxJ1uCYLyO1JJGQ/s930/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;930&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGgnVoDZw3huIVPMGOAeOHGggcZcnd2IWcrlSyMnDS7RixKyBV12dtGF3ofDR8mxYy6CEElatdAyNIcxkyk91jZ2c-7_kSnnZ1omPLuUwWjnl019yQdj1NlmCf0844xwNeFH9uvTbwnmjOEpUszF8nztKpt_TBMiZcaX4f3_rIBZDpxJ1uCYLyO1JJGQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example visualization of different algorithms for two different natural images from SALICON for ε = 10 and &lt;em>;n&lt;/em>; = 50 users. The algorithms from left to right are: original heatmap (no privacy), baseline, and ours.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Additional experimental results, including those on other datasets, metrics, privacy parameters and DP models, can be found in the paper. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented a privatization algorithm for sparse distribution aggregation under the EMD metric, which in turn yields an algorithm for producing privacy-preserving heatmaps. Our algorithm extends naturally to distributed models that can implement the Laplace mechanism, including the secure aggregation model and the &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3132747.3132769&quot;>;shuffle model&lt;/a>;. This does not apply to the more stringent &lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;local DP model&lt;/a>;, and it remains an interesting open question to devise practical local DP heatmap/EMD aggregation algorithms for “moderate” number of users and privacy parameters. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work was done jointly with Junfeng He, Kai Kohlhoff, Ravi Kumar, Pasin Manurangsi, and Vidhya Navalpakkam.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/331432973525797076/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/differentially-private-heatmaps.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/331432973525797076&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/331432973525797076&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private heatmaps&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgS24x7QsXnFRdTdq_PJYQOr4XqJSpsFBWcb630Ujz2vOcGLL0FzXcvaDq_xwd72wCVXs3U5oNPYz0JgLJYNDrfyqf_0Fzuri6CJg2s8OzNVwsWRzzlU23NkGtre3XMlhoxf9egBUJRyEfEeIjdsRJ8m2L3gFJYCbNefqY2XNNGotPNJF6lMOM1DrlbPA/s72-c/DPheatmaps.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5107802027865733485&lt;/id>;&lt;published>;2023-04-14T08:00:00.001-07:00&lt;/published>;&lt;updated>;2023-04-14T08:00:00.171-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Beyond automatic differentiation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Matthew Streeter, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeF_4QxTZliBxybI6a8fpfDt-IC6AG1GkRjM6mQqSLQ64RQTP5BJYq8WcGkzss_Ft-5QF7nU36hT8CahVaq04fVThFV_zWbgbfutAzQU3g8yoghkEmb603znIRbX5jT74LQxBKQ1qKp6sx9GTKVEgcHaWJOpmiipwcuUj6FXC1_hCbqHHX05yMA_SaLw/s660/AutoBound-Crop.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Derivative&quot;>;Derivatives&lt;/a>; play a central role in optimization and machine learning. By locally approximating a &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;training loss&lt;/a>;, derivatives guide an optimizer toward lower values of the loss. Automatic differentiation frameworks such as &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, and &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; are an essential part of modern machine learning, making it feasible to use gradient-based optimizers to train very complex models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; But are derivatives all we need? By themselves, derivatives only tell us how a function behaves on an &lt;a href=&quot;https://en.wikipedia.org/wiki/Infinitesimal&quot;>;infinitesimal&lt;/a>; scale. To use derivatives effectively, we often need to know more than that. For example, to choose a &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;learning rate&lt;/a>; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;gradient descent&lt;/a>;, we need to know something about how the loss function behaves over a small but &lt;em>;finite&lt;/em>; window. A finite-scale analogue of automatic differentiation, if it existed, could help us make such choices more effectively and thereby speed up training. &lt;/p>;&lt;p>;&lt;/p>; &lt;p>; In our new paper &quot;&lt;a href=&quot;https://arxiv.org/pdf/2212.11429.pdf&quot;>;Automatically Bounding The Taylor Remainder Series: Tighter Bounds and New Applications&lt;/a>;&quot;, we present an algorithm called AutoBound that computes polynomial upper and lower bounds on a given function, which are valid over a user-specified &lt;a href=&quot;https://en.wikipedia.org/wiki/Interval_(mathematics)&quot;>;interval&lt;/a>;. We then begin to explore AutoBound&#39;s applications. Notably, we present a meta-optimizer called SafeRate that uses the upper bounds computed by AutoBound to derive learning rates that are guaranteed to monotonically reduce a given loss function, without the need for time-consuming hyperparameter tuning.&amp;nbsp;We are also making AutoBound available as an &lt;a href=&quot;https://github.com/google/autobound&quot;>;open-source library&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;The AutoBound algorithm&lt;/h2>; &lt;p>; Given a function &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>; and a reference point &lt;code>;&lt;em>;x&lt;sub>;0&lt;/sub>;&lt;/em>;&lt;/code>;, AutoBound computes polynomial upper and lower bounds on &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>; that hold over a user-specified interval called a &lt;em>;trust region&lt;/em>;. Like &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;Taylor polynomials&lt;/a>;, the bounding polynomials are equal to &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>; at &lt;code>;&lt;em>;x&lt;sub>;0&lt;/sub>;&lt;/em>;&lt;/code>;. The bounds become tighter as the trust region shrinks, and approach the corresponding Taylor polynomial as the trust region width approaches zero. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjb0WLo5_NiobuGRFacHeB-xYdA4i6kQEFF0BDeF5FkXuqHHTX-ddE1qzK1ekOgtOwu8c3JHknR8FFU14sgobiTj5tx23yn183Yf8KP6ca5O5y6nKRmgaYi-2Vs8ZL4Gt9kxITBlmNSc6Zmm3UAamc84Ry_TR1GqIiexxrkHVCREuKSh9Y5GT7cIBDGZQ/s3091/taylor_enclosure_animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2217&quot; data-original-width=&quot;3091&quot; height=&quot;459&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjb0WLo5_NiobuGRFacHeB-xYdA4i6kQEFF0BDeF5FkXuqHHTX-ddE1qzK1ekOgtOwu8c3JHknR8FFU14sgobiTj5tx23yn183Yf8KP6ca5O5y6nKRmgaYi-2Vs8ZL4Gt9kxITBlmNSc6Zmm3UAamc84Ry_TR1GqIiexxrkHVCREuKSh9Y5GT7cIBDGZQ/w640-h459/taylor_enclosure_animation.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Automatically-derived quadratic upper and lower bounds on a one-dimensional function f, centered at x&lt;sub>;0&lt;/sub>;=0.5. The upper and lower bounds are valid over a user-specified trust region, and become tighter as the trust region shrinks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Like automatic differentiation, AutoBound can be applied to any function that can be implemented using standard mathematical operations. In fact, AutoBound is a generalization of &lt;a href=&quot;https://openreview.net/pdf?id=SkxEF3FNPH&quot;>;Taylor mode automatic differentiation&lt;/a>;, and is equivalent to it in the special case where the trust region has a width of zero. &lt;/p>; &lt;p>; To derive the AutoBound algorithm, there were two main challenges we had to address: &lt;/p>; &lt;ol>; &lt;li>;We had to derive polynomial upper and lower bounds for various elementary functions, given an arbitrary reference point and arbitrary trust region. &lt;/li>;&lt;li>;We had to come up with an analogue of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule&quot;>;chain rule&lt;/a>; for combining these bounds. &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;Bounds for elementary functions&lt;/h2>; &lt;p>; For a variety of commonly-used functions, we derive &lt;em>;optimal&lt;/em>; polynomial upper and lower bounds in closed form. In this context, &quot;optimal&quot; means the bounds are as tight as possible, among all polynomials where only the maximum-&lt;a href=&quot;https://en.wikipedia.org/wiki/Degree_of_a_polynomial&quot;>;degree coefficient&lt;/a>; differs from the Taylor series. Our theory applies to elementary functions, such as &lt;code>;&lt;em>;exp&lt;/em>;&lt;/code>; and &lt;code>;&lt;em>;log&lt;/em>;&lt;/code>;, and common neural network activation functions, such as &lt;code>;&lt;a href=&quot;https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf&quot;>;ReLU&lt;/a>;&lt;/code>; and &lt;code>;&lt;a href=&quot;https://arxiv.org/pdf/1710.05941.pdf&quot;>;Swish&lt;/a>;&lt;/code>;. It builds upon and generalizes &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3129714/pdf/nihms90438.pdf&quot;>;earlier work&lt;/a>; that applied only to quadratic bounds, and only for an unbounded trust region. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5WY6ZTX0Rpwa4dpCoa4pyu5nFBoGDFnDqpQrVqHkOmTAAXivb7AhueTYHGwWFJLUYuvP6daRXsp8A7k_U1kJ8OaSHhW3TyIFAvNeoIOki9ek9GY95DXAfGzZXLqsKlHeb2_8Z4yHMBx1kPYJx4LXU_mQQO0u0JOLgWHaOZqYflTrKd6n9Z4NouU5wAQ/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1446&quot; data-original-width=&quot;1999&quot; height=&quot;463&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5WY6ZTX0Rpwa4dpCoa4pyu5nFBoGDFnDqpQrVqHkOmTAAXivb7AhueTYHGwWFJLUYuvP6daRXsp8A7k_U1kJ8OaSHhW3TyIFAvNeoIOki9ek9GY95DXAfGzZXLqsKlHeb2_8Z4yHMBx1kPYJx4LXU_mQQO0u0JOLgWHaOZqYflTrKd6n9Z4NouU5wAQ/w640-h463/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Optimal quadratic upper and lower bounds on the exponential function, centered at x&lt;sub>;0&lt;/sub>;=0.5 and valid over the interval [0, 2].&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;A new chain rule&lt;/h2>; &lt;p>; To compute upper and lower bounds for arbitrary functions, we derived a generalization of the chain rule that operates on polynomial bounds. To illustrate the idea, suppose we have a function that can be written as &lt;/p>; &lt;div align=&quot;center&quot;>; &lt;pre class=&quot;prettyprint&quot;>;f(x) = g(h(x)) &lt;/pre>; &lt;/div>; &lt;p>; and suppose we already have polynomial upper and lower bounds on &lt;code>;&lt;em>;g&lt;/em>;&lt;/code>; and &lt;code>;&lt;em>;h&lt;/em>;&lt;/code>;. How do we compute bounds on &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>;? &lt;/p>;&lt;p>; The key turns out to be representing the upper and lower bounds for a given function as a &lt;em>;single&lt;/em>; polynomial whose highest-degree coefficient is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Interval_(mathematics)&quot;>;interval&lt;/a>; rather than a scalar. We can then plug the bound for &lt;code>;&lt;em>;h&lt;/em>;&lt;/code>; into the bound for &lt;code>;&lt;em>;g&lt;/em>;&lt;/code>;, and convert the result back to a polynomial of the same form using &lt;a href=&quot;https://epubs.siam.org/doi/book/10.1137/1.9780898717716&quot;>;interval arithmetic&lt;/a>;. Under suitable assumptions about the trust region over which the bound on &lt;code>;&lt;em>;g&lt;/em>;&lt;/code>; holds, it can be shown that this procedure yields the desired bound on &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXlrNk32-9fDM8A8ooF6TnLk3CNpNpo-QRl79dL9PXRNRa4O4l6gX2NxfBH9WxVFoG0Ks7R80E3-HxeORDu_OjSI446cntCytscoKTPODAHUU20h8hkXi3EV0mJLsrm4dCT5iROfgi5_2rLX0aM1S3oMREuYqKAB_yy9B2wo4aV2bY3qxPxzMIKS7c9Q/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;591&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXlrNk32-9fDM8A8ooF6TnLk3CNpNpo-QRl79dL9PXRNRa4O4l6gX2NxfBH9WxVFoG0Ks7R80E3-HxeORDu_OjSI446cntCytscoKTPODAHUU20h8hkXi3EV0mJLsrm4dCT5iROfgi5_2rLX0aM1S3oMREuYqKAB_yy9B2wo4aV2bY3qxPxzMIKS7c9Q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The interval polynomial chain rule applied to the functions h(x) = sqrt(x) and g(y) = exp(y), with x&lt;sub>;0&lt;/sub>;=0.25 and trust region [0, 0.5].&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our chain rule applies to one-dimensional functions, but also to multivariate functions, such as matrix multiplications and convolutions. &lt;/p>; &lt;br />; &lt;h2>;Propagating bounds&lt;/h2>; &lt;p>; Using our new chain rule, AutoBound propagates interval polynomial bounds through a computation graph from the inputs to the outputs, analogous to &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation#Forward_accumulation&quot;>;forward-mode automatic differentiation&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjb3Iw5MwfRFRP6MUo8JdubaBYOMqZqbUI-Xjjla07M8_faGpdxsI_TfRnS3OZjrUGDQrie_mwPCroYO_2xKiutmjUCCSHSOKQHqI8tbEYDRiIE-gD4UmV252Z1IKFsR6wYRVZtT9X1bbHdxrJofymjtFTlm3lrTG8tR28hOI8wNmfUwl_v8DnGluTRYw/s960/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;161&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjb3Iw5MwfRFRP6MUo8JdubaBYOMqZqbUI-Xjjla07M8_faGpdxsI_TfRnS3OZjrUGDQrie_mwPCroYO_2xKiutmjUCCSHSOKQHqI8tbEYDRiIE-gD4UmV252Z1IKFsR6wYRVZtT9X1bbHdxrJofymjtFTlm3lrTG8tR28hOI8wNmfUwl_v8DnGluTRYw/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Forward propagation of interval polynomial bounds for the function f(x) = exp(sqrt(x)). We first compute (trivial) bounds on x, then use the chain rule to compute bounds on sqrt(x) and exp(sqrt(x)).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To compute bounds on a function &lt;code>;&lt;em>;f(x)&lt;/em>;&lt;/code>;, AutoBound requires memory proportional to the dimension of &lt;code>;&lt;em>;x&lt;/em>;&lt;/code>;. For this reason, practical applications apply AutoBound to functions with a small number of inputs. However, as we will see, this does not prevent us from using AutoBound for neural network optimization. &lt;/p>; &lt;br />; &lt;h2>;Automatically deriving optimizers, and other applications&lt;/h2>; &lt;p>; What can we do with AutoBound that we couldn&#39;t do with automatic differentiation alone? &lt;/p>; &lt;p>; Among other things, AutoBound can be used to automatically derive problem-specific, hyperparameter-free optimizers that converge from any starting point. These optimizers iteratively reduce a loss by first using AutoBound to compute an upper bound on the loss that is tight at the current point, and then minimizing the upper bound to obtain the next point. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2JJN-D-nFQ7Bl4BYySe3oe4YBFLAnqROlRvrZuRURTicKO8Luhti9I_kuJ1nJKQe4chKqldyddqGXQKBMB1ltXNWKu06SfDxUEjaG52pNCdgyhgNYpo8DQmGnlr5FoSVJSvZMbi0GtD1w60iQF6hV4eRky-SSmfFY2fCUWLKxTcVcMIwVc8T8QyA-6w/s3074/mm_animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2197&quot; data-original-width=&quot;3074&quot; height=&quot;457&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2JJN-D-nFQ7Bl4BYySe3oe4YBFLAnqROlRvrZuRURTicKO8Luhti9I_kuJ1nJKQe4chKqldyddqGXQKBMB1ltXNWKu06SfDxUEjaG52pNCdgyhgNYpo8DQmGnlr5FoSVJSvZMbi0GtD1w60iQF6hV4eRky-SSmfFY2fCUWLKxTcVcMIwVc8T8QyA-6w/w640-h457/mm_animation.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Minimizing a one-dimensional logistic regression loss using quadratic upper bounds derived automatically by AutoBound.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Optimizers that use upper bounds in this way are called &lt;a href=&quot;https://epubs.siam.org/doi/book/10.1137/1.9781611974409&quot;>;majorization-minimization&lt;/a>; (MM) optimizers. Applied to one-dimensional logistic regression, AutoBound rederives an MM optimizer &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3129714/&quot;>;first published in 2009&lt;/a>;. Applied to more complex problems, AutoBound derives novel MM optimizers that would be difficult to derive by hand. &lt;/p>; &lt;p>; We can use a similar idea to take an existing optimizer such as &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>; and convert it to a hyperparameter-free optimizer that is guaranteed to monotonically reduce the loss (in the full-batch setting). The resulting optimizer uses the same update direction as the original optimizer, but modifies the learning rate by minimizing a one-dimensional quadratic upper bound derived by AutoBound. We refer to the resulting meta-optimizer as SafeRate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgutnTe8tbfFqdlv7G1CWmk5LgRRVRIuw2veGSrIOv3N7k-dKGCWZMmIb8n3drlkWcZSc2dcwd2-BmxCm-9I7y5q3lAcCkB17wC2nMrIeSbZeQeck6P2W_bu4VvQ38QkVKr5CDB62pwIV0zEGmG_n_r8oY9vnv92Jv-Dh-ZS3Qb_20QV2rQTEdmRH1_pg/s1656/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1094&quot; data-original-width=&quot;1656&quot; height=&quot;423&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgutnTe8tbfFqdlv7G1CWmk5LgRRVRIuw2veGSrIOv3N7k-dKGCWZMmIb8n3drlkWcZSc2dcwd2-BmxCm-9I7y5q3lAcCkB17wC2nMrIeSbZeQeck6P2W_bu4VvQ38QkVKr5CDB62pwIV0zEGmG_n_r8oY9vnv92Jv-Dh-ZS3Qb_20QV2rQTEdmRH1_pg/w640-h423/image8.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of SafeRate when used to train a single-hidden-layer neural network on a subset of the &lt;a href=&quot;https://yann.lecun.com/exdb/mnist/&quot;>;MNIST&lt;/a>; dataset, in the full-batch setting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using SafeRate, we can create more robust variants of existing optimizers, at the cost of a single additional forward pass that increases the &lt;a href=&quot;https://en.wikipedia.org/wiki/Elapsed_real_time&quot;>;wall time&lt;/a>; for each step by a small factor (about 2x in the example above). &lt;/p>; &lt;p>; In addition to the applications just discussed, AutoBound can be used for verified &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_integration&quot;>;numerical integration&lt;/a>; and to automatically prove sharper versions of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%27s_inequality&quot;>;Jensen&#39;s inequality&lt;/a>;, a fundamental mathematical inequality used frequently in statistics and other fields. &lt;/p>; &lt;br />; &lt;h2>;Improvement over classical bounds&lt;/h2>; &lt;p>; Bounding the &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor%27s_theorem&quot;>;Taylor remainder term&lt;/a>; automatically is not a new idea. A classical technique produces degree &lt;code>;&lt;em>;k&lt;/em>;&lt;/code>; polynomial bounds on a function &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>; that are valid over a trust region &lt;code>;[&lt;em>;a&lt;/em>;, &lt;em>;b&lt;/em>;]&lt;/code>; by first computing an expression for the &lt;code>;&lt;em>;k&lt;/em>;&lt;/code>;th derivative of &lt;code>;&lt;em>;f&lt;/em>;&lt;/code>; (using automatic differentiation), then evaluating this expression over &lt;code>;[&lt;em>;a&lt;/em>;,&lt;em>;b&lt;/em>;]&lt;/code>; using interval arithmetic. &lt;/p>;&lt;p>; While elegant, this approach has some inherent limitations that can lead to very loose bounds, as illustrated by the dotted blue lines in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgENtTpbXUtRpnOVihpobiGYrw4L_jEUASrWaRvGcPBgTk-HLuy2_gUevqyhtmDMpELAmjYaszDdniJRndog7u8zH-o0Sjp3OrWF5lB4q1nOcyYay8AY2mgE9xGK3ZwRsAfARybfzzO7o2SFIcSWz66Ghyw66tWQYf8rLwxBoG1Zkkagm3I8tQVweYZTg/s1510/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1107&quot; data-original-width=&quot;1510&quot; height=&quot;469&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgENtTpbXUtRpnOVihpobiGYrw4L_jEUASrWaRvGcPBgTk-HLuy2_gUevqyhtmDMpELAmjYaszDdniJRndog7u8zH-o0Sjp3OrWF5lB4q1nOcyYay8AY2mgE9xGK3ZwRsAfARybfzzO7o2SFIcSWz66Ghyw66tWQYf8rLwxBoG1Zkkagm3I8tQVweYZTg/w640-h469/image5.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quadratic upper and lower bounds on the loss of a multi-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Perceptron&quot;>;perceptron&lt;/a>; with two hidden layers, as a function of the initial learning rate. The bounds derived by AutoBound are much tighter than those obtained using interval arithmetic evaluation of the second derivative.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Looking forward&lt;/h2>; &lt;p>; Taylor polynomials have been in use for over three hundred years, and are omnipresent in numerical optimization and scientific computing. Nevertheless, Taylor polynomials have significant limitations, which can limit the capabilities of algorithms built on top of them. Our work is part of a growing literature that recognizes these limitations and seeks to develop a new foundation upon which more robust algorithms can be built. &lt;/p>; &lt;p>; Our experiments so far have only scratched the surface of what is possible using AutoBound, and we believe it has many applications we have not discovered. To encourage the research community to explore such possibilities, we have made AutoBound available as an open-source library built on top of &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;. To get started, visit our &lt;a href=&quot;https://github.com/google/autobound&quot;>;GitHub repo&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This post is based on joint work with Josh Dillon. We thank Alex Alemi and Sergey Ioffe for valuable feedback on an earlier draft of the post.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5107802027865733485/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5107802027865733485&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5107802027865733485&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html&quot; rel=&quot;alternate&quot; title=&quot;Beyond automatic differentiation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeF_4QxTZliBxybI6a8fpfDt-IC6AG1GkRjM6mQqSLQ64RQTP5BJYq8WcGkzss_Ft-5QF7nU36hT8CahVaq04fVThFV_zWbgbfutAzQU3g8yoghkEmb603znIRbX5jT74LQxBKQ1qKp6sx9GTKVEgcHaWJOpmiipwcuUj6FXC1_hCbqHHX05yMA_SaLw/s72-c/AutoBound-Crop.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8521407697493439122&lt;/id>;&lt;published>;2023-04-13T12:31:00.000-07:00&lt;/published>;&lt;updated>;2023-04-13T12:31:16.059-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Robotic deep RL at scale: Sorting waste and recyclables with a fleet of robots&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sergey Levine, Research Scientist, and Alexander Herzog, Staff Research Software Engineer, Google Research, Brain Team&lt;/span>;&lt;div>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEYhP7IrTnpGC_qxqzY9xaShL-JjOujpoWIEqWf-tccTefAx7xKsoD_74wwMq07kbZvvwAgiNPydbCAPMtbDYXz5roJaemUzqPc8AspgjV4lxB0Bln1YSLzzwgxNUT0EA7QDS0SDuwofleEqOgzF_5bIgb_rNnyMWKMTHhYH1OI0W5lDEym8wsP1G44A/s320/Robotic%20RL%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>;Reinforcement learning (RL) can enable robots to learn complex behaviors through trial-and-error interaction, getting better and better over time. Several of our prior works explored how RL can enable intricate robotic skills, such as &lt;a href=&quot;https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html&quot;>;robotic grasping&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html&quot;>;multi-task learning&lt;/a>;, and even playing &lt;a href=&quot;https://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html&quot;>;table tennis&lt;/a>;. Although robotic RL has come a long way, we still don&#39;t see RL-enabled robots in everyday settings. The real world is complex, diverse, and changes over time, presenting a major challenge for robotic systems. However, we believe that RL should offer us an excellent tool for tackling precisely these challenges: by continually practicing, getting better, and learning on the job, robots should be able to &lt;a href=&quot;https://sites.google.com/corp/view/efficient-ft/home&quot;>;adapt to the world as it changes around them&lt;/a>;.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://rl-at-scale.github.io/assets/rl_at_scale.pdf&quot;>;Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators&lt;/a>;”, we discuss how we studied this problem through a recent large-scale experiment, where we deployed a fleet of 23 RL-enabled robots over two years in Google office buildings to sort waste and recycling. Our robotic system combines scalable deep RL from real-world data with bootstrapping from training in simulation and auxiliary object perception inputs to boost generalization, while retaining the benefits of end-to-end training, which we validate with 4,800 evaluation trials across 240 waste station configurations.&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Problem setup&lt;/h2>; &lt;p>; When people don&#39;t sort their trash properly, batches of recyclables can become contaminated and compost can be improperly discarded into landfills. In our experiment, a robot roamed around an office building searching for “waste stations” (bins for recyclables, compost, and trash). The robot was tasked with approaching each waste station to sort it, moving items between the bins so that all recyclables (cans, bottles) were placed in the recyclable bin, all the compostable items (cardboard containers, paper cups) were placed in the compost bin, and everything else was placed in the landfill trash bin. Here is what that looks like: &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/rl-at-scale/rl-at-scale.github.io/blob/main/videos/sort_at_rls_1440x810.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/rl-at-scale/rl-at-scale.github.io/blob/main/videos/sorty_task_animation.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;br />; &lt;p>; This task is not as easy as it looks. Just being able to pick up the vast variety of objects that people deposit into waste bins presents a major learning challenge. Robots also have to identify the appropriate bin for each object and sort them as quickly and efficiently as possible. In the real world, the robots can encounter a variety of situations with unique objects, like the examples from real office buildings below: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnyIwIL3posLhbG6D9ka9eeHp8VJqgDSUWoPabIIkWH-bED_Cq2B3BswAerAuVYcG0mVpwtGW4ZeRHJXAfQ_hQXewszOZSEzg_Nae2z5t_ahJOAivUMJDhVQnCUn7R1vM4b5jVlHwQX9XzNXg7IihWgyv5K_pCjA-Ef9CYarhpAvmPJvwtkeHe1H44Yw/s1917/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1917&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnyIwIL3posLhbG6D9ka9eeHp8VJqgDSUWoPabIIkWH-bED_Cq2B3BswAerAuVYcG0mVpwtGW4ZeRHJXAfQ_hQXewszOZSEzg_Nae2z5t_ahJOAivUMJDhVQnCUn7R1vM4b5jVlHwQX9XzNXg7IihWgyv5K_pCjA-Ef9CYarhpAvmPJvwtkeHe1H44Yw/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning from diverse experience&lt;/h2>; &lt;p>; Learning on the job helps, but before even getting to that point, we need to bootstrap the robots with a basic set of skills. To this end, we use four sources of experience: (1) a set of simple hand-designed policies that have a very low success rate, but serve to provide some initial experience, (2) a simulated training framework that uses &lt;a href=&quot;https://arxiv.org/abs/2011.03148&quot;>;sim-to-real transfer&lt;/a>; to provide some initial bin sorting strategies, (3) “robot classrooms” where the robots continually practice at a set of representative waste stations, and (4) the real deployment setting, where robots practice in real office buildings with real trash.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2P-roxlxTLbkw2aVK5njLi9YtLgi-Vl_aaKe9M2_7AavUymKzykgLImqUtBxG0QnmOwHLS1lq6ikflOoThGU3HyE6-np7sSgI-EK4wv6FuOy_v-62Na5BsNAHZGdmJeo5B6v_8UCl4I8UZqHwn1sYbYJ6Ic3glRETHBisK6z5MJDdNPYUiXk6cMhW0g/s862/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;734&quot; data-original-width=&quot;862&quot; height=&quot;545&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2P-roxlxTLbkw2aVK5njLi9YtLgi-Vl_aaKe9M2_7AavUymKzykgLImqUtBxG0QnmOwHLS1lq6ikflOoThGU3HyE6-np7sSgI-EK4wv6FuOy_v-62Na5BsNAHZGdmJeo5B6v_8UCl4I8UZqHwn1sYbYJ6Ic3glRETHBisK6z5MJDdNPYUiXk6cMhW0g/w640-h545/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram of RL at scale. We bootstrap policies from data generated with a script (&lt;strong>;top-left&lt;/strong>;). We then train a sim-to-real model and generate additional data in simulation (&lt;strong>;top-right&lt;/strong>;). At each deployment cycle, we add data collected in our classrooms (&lt;strong>;bottom-right&lt;/strong>;). We further deploy and collect data in office buildings (&lt;strong>;bottom-left&lt;/strong>;). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Our RL framework is based on &lt;a href=&quot;https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html&quot;>;QT-Opt&lt;/a>;, which we previously applied to learn bin grasping in laboratory settings, as well as &lt;a href=&quot;https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html&quot;>;a range of other skills&lt;/a>;. In simulation, we bootstrap from simple scripted policies and use RL, with a &lt;a href=&quot;https://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html&quot;>;CycleGAN-based transfer method&lt;/a>; that uses &lt;a href=&quot;https://arxiv.org/abs/2011.03148&quot;>;RetinaGAN&lt;/a>; to make the simulated images appear more life-like. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/rl-at-scale/rl-at-scale.github.io/blob/main/videos/sorty_sim_to_real.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br>; &lt;p>; From here, it&#39;s off to the classroom. While real-world office buildings can provide the most representative experience, the throughput in terms of data collection is limited — some days there will be a lot of trash to sort, some days not so much. Our robots collect a large portion of their experience in “robot classrooms.” In the classroom shown below, 20 robots practice the waste sorting task: &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/rl-at-scale/rl-at-scale.github.io/blob/main/videos/classrooms_wide.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br>; &lt;p>; While these robots are training in the classrooms, other robots are simultaneously learning on the job in 3 office buildings, with 30 waste stations: &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/rl-at-scale/rl-at-scale.github.io/blob/main/videos/sorty_wild_collage_2x3.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Sorting performance&lt;/h2>; &lt;p>; In the end, we gathered 540k trials in the classrooms and 32.5k trials from deployment. Overall system performance improved as more data was collected. We evaluated our final system in the classrooms to allow for controlled comparisons, setting up scenarios based on what the robots saw during deployment. The final system could accurately sort about 84% of the objects on average, with performance increasing steadily as more data was added. In the real world, we logged statistics from three real-world deployments between 2021 and 2022, and found that our system could reduce contamination in the waste bins by between 40% and 50% by weight. &lt;a href=&quot;https://rl-at-scale.github.io/assets/rl_at_scale.pdf&quot;>;Our paper&lt;/a>; provides further insights on the technical design, ablations studying various design decisions, and more detailed statistics on the experiments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; Our experiments showed that RL-based systems can enable robots to address real-world tasks in real office environments, with a combination of offline and online data enabling robots to adapt to the broad variability of real-world situations. At the same time, learning in more controlled “classroom” environments, both in simulation and in the real world, can provide a powerful bootstrapping mechanism to get the RL “flywheel” spinning to enable this adaptation. There is still a lot left to do: our final RL policies do not succeed every time, and &lt;a href=&quot;https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html&quot;>;larger and more powerful models&lt;/a>; will be needed to improve their performance and extend them to a broader range of tasks. Other sources of experience, including from other tasks, other robots, and even Internet videos may serve to further supplement the bootstrapping experience that we obtained from simulation and classrooms. These are exciting problems to tackle in the future. Please see the full paper &lt;a href=&quot;https://rl-at-scale.github.io/assets/rl_at_scale.pdf&quot;>;here&lt;/a>;, and the supplementary video materials on the &lt;a href=&quot;https://rl-at-scale.github.io/&quot;>;project webpage&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;This research was conducted by multiple researchers at Robotics at Google and Everyday Robots, with contributions from Alexander Herzog, Kanishka Rao, Karol Hausman, Yao Lu, Paul Wohlhart, Mengyuan Yan, Jessica Lin, Montserrat Gonzalez Arenas, Ted Xiao, Daniel Kappler, Daniel Ho, Jarek Rettinghouse, Yevgen Chebotar, Kuang-Huei Lee, Keerthana Gopalakrishnan, Ryan Julian, Adrian Li, Chuyuan Kelly Fu, Bob Wei, Sangeetha Ramesh, Khem Holden, Kim Kleiven, David Rendleman, Sean Kirmani, Jeff Bingham, Jon Weisz, Ying Xu, Wenlong Lu, Matthew Bennice, Cody Fong, David Do, Jessica Lam, Yunfei Bai, Benjie Holson, Michael Quinlan, Noah Brown, Mrinal Kalakrishnan, Julian Ibarz, Peter Pastor, Sergey Levine and the entire Everyday Robots team.&lt;/i>;&lt;/p>;&lt;/div>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8521407697493439122/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robotic-deep-rl-at-scale-sorting-waste.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8521407697493439122&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8521407697493439122&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robotic-deep-rl-at-scale-sorting-waste.html&quot; rel=&quot;alternate&quot; title=&quot;Robotic deep RL at scale: Sorting waste and recyclables with a fleet of robots&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEYhP7IrTnpGC_qxqzY9xaShL-JjOujpoWIEqWf-tccTefAx7xKsoD_74wwMq07kbZvvwAgiNPydbCAPMtbDYXz5roJaemUzqPc8AspgjV4lxB0Bln1YSLzzwgxNUT0EA7QDS0SDuwofleEqOgzF_5bIgb_rNnyMWKMTHhYH1OI0W5lDEym8wsP1G44A/s72-c/Robotic%20RL%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7559428319650294916&lt;/id>;&lt;published>;2023-04-12T13:04:00.000-07:00&lt;/published>;&lt;updated>;2023-04-12T13:04:05.487-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;UniPi: Learning universal policies via text-guided video generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sherry Yang, Research Scientist, and Yilun Du, Student Researcher, Google Research, Brain Team&lt;/span>;&lt;div>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtLCDewF7ckZRQjY48oqo6-84nJRlwHPLzWG5IolqJba46K106Hz1TuzbptipOrHI57dx5WsVZfGg0r2n7w2EGBSaUuQeZIukOrTFuUbNAwddcKD3t0uNgBHXysdsFCd1Wpg30rHIUeEqJ4GoWLr3Joi7NMruK8zL1uH5OxY2urRsz_4_lf_4Oa9a_sQ/s320/UniPi%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;Building models that solve a diverse set of tasks has become a dominant paradigm in the domains of vision and language. In natural language processing, large pre-trained models, such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/GPT-3&quot;>;GPT-3&lt;/a>; and &lt;a href=&quot;https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval&quot;>;Gopher&lt;/a>;, have demonstrated remarkable zero-shot learning of new language tasks. Similarly, in computer vision, models like &lt;a href=&quot;https://openai.com/blog/clip/&quot;>;CLIP&lt;/a>; and &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>; have shown robust performance on zero-shot classification and object recognition. A natural next step is to use such tools to construct agents that can complete different decision-making tasks across many environments. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, training such agents faces the inherent challenge of environmental diversity, since different environments operate with distinct state action spaces (eg, the joint space and continuous controls in &lt;a href=&quot;https://mujoco.org/&quot;>;MuJoCo&lt;/a>; are fundamentally different from the image space and discrete actions in &lt;a href=&quot;https://en.wikipedia.org/wiki/Atari_2600&quot;>;Atari&lt;/a>;). This environmental diversity hampers knowledge sharing, learning, and generalization across tasks and environments. Furthermore, it is difficult to construct reward functions across environments, as different tasks generally have different notions of success. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/pdf/2302.00111.pdf&quot;>;Learning Universal Policies via Text-Guided Video Generation&lt;/a>;”, we propose a Universal Policy (UniPi) that addresses environmental diversity and reward specification challenges. UniPi leverages text for expressing task descriptions and video (ie, image sequences) as a universal interface for conveying action and observation behavior in different environments. Given an input image frame paired with text describing a current goal (ie, the next high-level step), UniPi uses a novel video generator (trajectory planner) to generate video with snippets of what an agent&#39;s trajectory should look like to achieve that goal. The generated video is fed into an &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_dynamics&quot;>;inverse dynamics model&lt;/a>; that extracts underlying low-level control actions, which are then executed in simulation or by a real robot agent. We demonstrate that UniPi enables the use of language and video as a universal control interface for generalizing to novel goals and tasks across diverse environments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBGUX8lgUGKlOpry7ZEmR2W5ZoIfnJYBIn5n2Da4YTYxP-Jr4YIyYwsCTv1KBjs1oD5sBTP2FeMi59lIj25hMqDllfGkAHN9oyIwwK7V8oFekLnoxE2GNjzVgLnTPc-xX6HFkwHEDpQzJr8zdgUpOPnauYkz_Ia2JQlPR8TQP-mpQK4EIJaZvwhCZIiQ/s1920/image9.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;906&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBGUX8lgUGKlOpry7ZEmR2W5ZoIfnJYBIn5n2Da4YTYxP-Jr4YIyYwsCTv1KBjs1oD5sBTP2FeMi59lIj25hMqDllfGkAHN9oyIwwK7V8oFekLnoxE2GNjzVgLnTPc-xX6HFkwHEDpQzJr8zdgUpOPnauYkz_Ia2JQlPR8TQP-mpQK4EIJaZvwhCZIiQ/s16000/image9.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Video policies generated by UniPi.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgv7g179aDeZtcyKR7y8CZXValvowgPgMe-qNcXIb963bsReKQZKnR9ORdJCMZt1eaFmRLC9a2OBfw14Irf98kKpjQ8_peKO8Z07VCQ4aljCM3lS3e1CjEW0I8f41mpgajiPEkx_JpMGNq378VVC8-TCYumE938Uy2MFXGagbGjh3g2ccwcp1rSUFK4lg/s964/image2.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;550&quot; data-original-width=&quot;964&quot; height=&quot;365&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgv7g179aDeZtcyKR7y8CZXValvowgPgMe-qNcXIb963bsReKQZKnR9ORdJCMZt1eaFmRLC9a2OBfw14Irf98kKpjQ8_peKO8Z07VCQ4aljCM3lS3e1CjEW0I8f41mpgajiPEkx_JpMGNq378VVC8-TCYumE938Uy2MFXGagbGjh3g2ccwcp1rSUFK4lg/w640-h365/image2.jpeg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;UniPi may be applied to downstream multi-task settings that require combinatorial language generalization, long-horizon planning, or internet-scale knowledge. In the bottom example, UniPi takes the image of the white robot arm from the internet and generates video snippets according to the text description of the goal.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;UniPi implementation&lt;/h2>; &lt;p>; To generate a valid and executable plan, a text-to-video model must synthesize a constrained video plan starting at the current observed image. We found it more effective to explicitly constrain a video synthesis model during training (as opposed to only constraining videos at sampling time) by providing the first frame of each video as explicit conditioning context. &lt;/p>; &lt;p>; At a high level, UniPi has four major components: 1) consistent video generation with first-frame tiling, 2) hierarchical planning through &lt;a href=&quot;https://imagen.research.google/video/paper.pdf&quot;>;temporal super resolution&lt;/a>;, 3) flexible behavior synthesis, and 4) task-specific action adaptation. We explain the implementation and benefit of each component in detail below. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Video generation through tiling&lt;/h3>; &lt;p>; Existing text-to-video models like &lt;a href=&quot;https://imagen.research.google/video/&quot;>;Imagen&lt;/a>; typically generate videos where the underlying environment state changes significantly throughout the duration. To construct an accurate trajectory planner, it is important that the environment remains consistent across all time points. We enforce environment consistency in conditional video synthesis by providing the observed image as additional context when denoising each frame in the synthesized video. To achieve context conditioning, UniPi directly concatenates each intermediate frame sampled from noise with the conditioned observed image across sampling steps, which serves as a strong signal to maintain the underlying environment state across time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOud2tz9jLgiSlRVKwDlZU17dRPwRC1ilx30jcQX7V7f5Au0PqkqGgzg8Q6sjrWG9rVnITpLguqfTkWEqP0ijOk9Uc_EA7EXPte0IOtbEDKttF-IXPrOjGZZazYHIdPQO4U-7DOI5Ep6MCi2xpNRy4nd5hndrKX_49rsYO6Dc1J_t7Id7K8ydtM0lZRw/s1009/UniPi-image2.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;1009&quot; height=&quot;376&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOud2tz9jLgiSlRVKwDlZU17dRPwRC1ilx30jcQX7V7f5Au0PqkqGgzg8Q6sjrWG9rVnITpLguqfTkWEqP0ijOk9Uc_EA7EXPte0IOtbEDKttF-IXPrOjGZZazYHIdPQO4U-7DOI5Ep6MCi2xpNRy4nd5hndrKX_49rsYO6Dc1J_t7Id7K8ydtM0lZRw/w640-h376/UniPi-image2.jpeg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Text-conditional video generation enables UniPi to train general purpose policies on a wide range of data sources (simulated, real robots and YouTube).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Hierarchical planning &lt;/h3>; &lt;p>; When constructing plans in high-dimensional environments with long time horizons, directly generating a set of actions to reach a goal state quickly becomes intractable due to the exponential growth of the underlying search space as the plan gets longer. &lt;a href=&quot;https://en.wikipedia.org/wiki/Motion_planning&quot;>;Planning methods&lt;/a>; often circumvent this issue by leveraging a natural hierarchy in planning. Specifically, planning methods first construct coarse plans (the intermediate key frames spread out across time) operating on low-dimensional states and actions, which are then refined into plans in the underlying state and action spaces. &lt;/p>; &lt;p>; Similar to planning, our conditional video generation procedure exhibits a natural temporal hierarchy. UniPi first generates videos at a coarse level by sparsely sampling videos (“abstractions”) of desired agent behavior along the time axis. UniPi then refines the videos to represent valid behavior in the environment by super-resolving videos across time. Meanwhile, coarse-to-fine super-resolution further improves consistency via &lt;a href=&quot;https://en.wikipedia.org/wiki/Motion_interpolation&quot;>;interpolation&lt;/a>; between frames. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnVM_oF992xHblRLdP7NcdDUHPpu5fA55x_y29l_WTHxjRQ1dn_hk9xdL9CO5wjjbOOf9v8m7DBFnt2tq-jNhhgHvtE6HASc0qV-FkERqVUNyE9zy1o2aUqzXvtY1mVrb0lsJ92ezq09fdTps9sbj98eSqkFM9lJHU9ZYEPZOp69RkyppT4-4pKnceZQ/s1146/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;822&quot; data-original-width=&quot;1146&quot; height=&quot;459&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnVM_oF992xHblRLdP7NcdDUHPpu5fA55x_y29l_WTHxjRQ1dn_hk9xdL9CO5wjjbOOf9v8m7DBFnt2tq-jNhhgHvtE6HASc0qV-FkERqVUNyE9zy1o2aUqzXvtY1mVrb0lsJ92ezq09fdTps9sbj98eSqkFM9lJHU9ZYEPZOp69RkyppT4-4pKnceZQ/w640-h459/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an input observation and text instruction, we plan a set of images representing agent behavior. Images are converted to actions using an inverse dynamics model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Flexible behavioral modulation &lt;/h3>; &lt;p>; When planning a sequence of actions for a given sub-goal, one can readily incorporate external constraints to modulate a generated plan. Such test-time adaptability can be implemented by composing a probabilistic prior incorporating properties of the desired plan to specify desired constraints across the synthesized action trajectory, which is also compatible with UniPi. In particular, the prior can be specified using a learned classifier on images to optimize a particular task, or as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;>;Dirac delta distribution&lt;/a>; on a particular image to guide a plan towards a particular set of states. To train the text-conditioned video generation model, we utilize the video diffusion algorithm, where pre-trained language features from the &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;Text-To-Text Transfer Transformer&lt;/a>; (T5) are encoded. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task-specific action adaptation&lt;/h3>; &lt;p>; Given a set of synthesized videos, we train a small task-specific inverse dynamics model to translate frames into a set of low-level control actions. This is independent from the planner and can be done on a separate, smaller and potentially suboptimal dataset generated by a simulator. &lt;/p>; &lt;p>; Given the input frame and text description of the current goal, the inverse dynamics model synthesizes image frames and generates a control action sequence that predicts the corresponding future actions. An agent then executes inferred low-level control actions via &lt;a href=&quot;https://en.wikipedia.org/wiki/Control_theory&quot;>;closed-loop control&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Capabilities and evaluation of UniPi&lt;/h2>; &lt;p>; We measure the task success rate on novel language-based goals, and find that UniPi generalizes well to both seen and novel combinations of language prompts, compared to baselines such as &lt;a href=&quot;https://arxiv.org/abs/2212.06817&quot;>;Transformer BC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.02039&quot;>;Trajectory Transformer&lt;/a>; (TT), and &lt;a href=&quot;https://arxiv.org/abs/2205.09991&quot;>;Diffuser&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOSXl-QvIcytF45iNW4Yt4zFVataJyEfFy9DKK-VrLQJFYOrWAniQxjydgKVxH6Vy1PU4n0p0bk8YfLMGComr7R_Wjz0pCXjQurEMe5kJ6JyuMi7uFaBweHNwYZVl2VGwo8864qGR2kFIFOnT-z-s2Dn4IwkuYPfl4SZuSZTK7eIV8TLvwa7U_SZzBbw/s1200/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOSXl-QvIcytF45iNW4Yt4zFVataJyEfFy9DKK-VrLQJFYOrWAniQxjydgKVxH6Vy1PU4n0p0bk8YfLMGComr7R_Wjz0pCXjQurEMe5kJ6JyuMi7uFaBweHNwYZVl2VGwo8864qGR2kFIFOnT-z-s2Dn4IwkuYPfl4SZuSZTK7eIV8TLvwa7U_SZzBbw/w640-h396/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;UniPi generalizes well to both seen and novel combinations of language prompts in Place (eg, “place X in Y”) and Relation (eg, “place X to the left of Y”) tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Below, we illustrate generated videos on unseen combinations of goals. UniPi is able to synthesize a diverse set of behaviors that satisfy unseen language subgoals: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKOL6MZILf9AXdTi9ohNpgpcKqpzPct1BwnVM-alHrUrZ60ev7xBOwdkHDijCEHI69Wg3hgHYkVw9NXYikogjjCZz4Y7pUDSUpghuwArDcW8BvEFkV6eq9Q8LsntJ_dbNz5SrSjGSctcmbmp3S15u-cIo_X2DLizAbMf-ckqmEvNyL-qnlNz5SkAiqSQ/s1344/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKOL6MZILf9AXdTi9ohNpgpcKqpzPct1BwnVM-alHrUrZ60ev7xBOwdkHDijCEHI69Wg3hgHYkVw9NXYikogjjCZz4Y7pUDSUpghuwArDcW8BvEFkV6eq9Q8LsntJ_dbNz5SrSjGSctcmbmp3S15u-cIo_X2DLizAbMf-ckqmEvNyL-qnlNz5SkAiqSQ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generated videos for unseen language goals at test time.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Multi-environment transfer &lt;/h3>; &lt;p>; We measure the task success rate of UniPi and baselines on novel tasks not seen during training. UniPi again outperforms the baselines by a large margin: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnpmpnrfvcQd7G3zsNCQcmZ89GXcGWwWuHIONOedbUr1pyWsVQOz_0NvG-Mn9tLy-CIcttvAwQbZ_Hknbpaf5x7PsHx775pM8CDThixjuivg7xmnT0AQqKVn5-RSpksKpNivGdwlYxME_5_jwPjP51tkhNCvkloeQRWsPYnjtv-eftuvESSaEwtcmD2A/s1200/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnpmpnrfvcQd7G3zsNCQcmZ89GXcGWwWuHIONOedbUr1pyWsVQOz_0NvG-Mn9tLy-CIcttvAwQbZ_Hknbpaf5x7PsHx775pM8CDThixjuivg7xmnT0AQqKVn5-RSpksKpNivGdwlYxME_5_jwPjP51tkhNCvkloeQRWsPYnjtv-eftuvESSaEwtcmD2A/w640-h396/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;UniPi generalizes well to new environments when trained on a set of different multi-task environments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Below, we illustrate generated videos on unseen tasks. UniPi is further able to synthesize a diverse set of behaviors that satisfy unseen language tasks: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFpw4MIbX3rs2dARAW3Q1ysZtZlAPCXloEylnLGcTl5hlloOyVZWBl3TQxpeJy0sjx6EDbQmG5mhkYl2hkRREdqvf2mABA15ilAVtrppIJl7bjuUoXR0MaKhcQGWCUxSqM7VUnvgPFFElzGjYiD9zEwoeXGN8hiDm4F2jUBE5olkcuR_dVovqHjGAHDQ/s1344/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFpw4MIbX3rs2dARAW3Q1ysZtZlAPCXloEylnLGcTl5hlloOyVZWBl3TQxpeJy0sjx6EDbQmG5mhkYl2hkRREdqvf2mABA15ilAVtrppIJl7bjuUoXR0MaKhcQGWCUxSqM7VUnvgPFFElzGjYiD9zEwoeXGN8hiDm4F2jUBE5olkcuR_dVovqHjGAHDQ/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generated video plans on different new test tasks in the multitask setting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Real world transfer&lt;/h3>; &lt;p>; Below, we further illustrate generated videos given language instructions on unseen real images. Our approach is able to synthesize a diverse set of different behaviors which satisfy language instructions: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhD-ARn2xmugVjVfZTMoPekp6synqc9U2KWOnRUKIFoTL3-Z2nZ38oSnm268yrAvRWMoXKTLJjWd_YhecN-IdkRZMBR5r5quXVqYSF93uEjojFgXsZepgPCc8DbtL4YieAKM2QJ2hIMueg_TKVRu0Qb8507sLkrCwr-LXXSXaBoMMBiqy0caUVqlPW6nw/s1344/image7.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhD-ARn2xmugVjVfZTMoPekp6synqc9U2KWOnRUKIFoTL3-Z2nZ38oSnm268yrAvRWMoXKTLJjWd_YhecN-IdkRZMBR5r5quXVqYSF93uEjojFgXsZepgPCc8DbtL4YieAKM2QJ2hIMueg_TKVRu0Qb8507sLkrCwr-LXXSXaBoMMBiqy0caUVqlPW6nw/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; Using internet pre-training enables UniPi to synthesize videos of tasks not seen during training. In contrast, a model trained from scratch incorrectly generates plans of different tasks: &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCKSQ2Q86mGaBi1Lmept4WF4cklAPfGnH2KKzyh8O5uYYKpfpjBqZLe67TsIx218RWdzieKtwgfy58PiL4Qsy_JnM1XenD5OqQ23wq9gyQTdq8tcMCgyFp3rJFy4A3ojgFHLV02XSeN2n_hAN0SaEliPSBTbrKmpJIuF0OcpMjZtqdPsp4h9NS6aWDkQ/s1440/image8.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;1440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCKSQ2Q86mGaBi1Lmept4WF4cklAPfGnH2KKzyh8O5uYYKpfpjBqZLe67TsIx218RWdzieKtwgfy58PiL4Qsy_JnM1XenD5OqQ23wq9gyQTdq8tcMCgyFp3rJFy4A3ojgFHLV02XSeN2n_hAN0SaEliPSBTbrKmpJIuF0OcpMjZtqdPsp4h9NS6aWDkQ/s16000/image8.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To evaluate the quality of videos generated by UniPi when pre-trained on non-robot data, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;>;Fréchet Inception Distance&lt;/a>; (FID) and &lt;a href=&quot;https://openreview.net/pdf?id=rylgEULtdN&quot;>;Fréchet Video Distance&lt;/a>; (FVD) metrics. We used &lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;Contrastive Language-Image Pre-training&lt;/a>; scores (CLIPScores) to measure the language-image alignment. We demonstrate that pre-trained UniPi achieves significantly higher FID and FVD scores and a better CLIPScore compared to UniPi without pre-training, suggesting that pre-training on non-robot data helps with generating plans for robots. We report the CLIPScore, FID, and VID scores for UniPi trained on &lt;a href=&quot;https://bair.berkeley.edu/blog/2021/11/18/bridge-data/&quot;>;Bridge&lt;/a>; data, with and without pre-training: &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Model (24x40)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;CLIPScore ↑&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;FID ↓&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;FVD ↓&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;No pre-training &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;24.43 ± 0.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;17.75 ± 0.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;288.02 ± 10.45 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Pre-trained &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;24.54&lt;/strong>; ± 0.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;14.54&lt;/strong>; ± 0.57 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;264.66&lt;/strong>; ± 13.64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using existing internet data improves video plan predictions under all metrics considered.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The future of large-scale generative models for decision making&lt;/h2>; &lt;p>; The positive results of UniPi point to the broader direction of using generative models and the wealth of data on the internet as powerful tools to learn general-purpose decision making systems. UniPi is only one step towards what generative models can bring to decision making. Other examples include using generative foundation models to provide photorealistic or linguistic simulators of the world in which artificial agents can be trained indefinitely. Generative models as agents can also learn to interact with complex environments such as the internet, so that much broader and more complex tasks can eventually be automated. We look forward to future research in applying internet-scale foundation models to multi-environment and multi-embodiment settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We&#39;d like to thank all remaining authors of the paper including Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. We would like to thank George Tucker, Douglas Eck, and Vincent Vanhoucke for the feedback on this post and on the original paper.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7559428319650294916/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/unipi-learning-universal-policies-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7559428319650294916&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7559428319650294916&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/unipi-learning-universal-policies-via.html&quot; rel=&quot;alternate&quot; title=&quot;UniPi: Learning universal policies via text-guided video generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtLCDewF7ckZRQjY48oqo6-84nJRlwHPLzWG5IolqJba46K106Hz1TuzbptipOrHI57dx5WsVZfGg0r2n7w2EGBSaUuQeZIukOrTFuUbNAwddcKD3t0uNgBHXysdsFCd1Wpg30rHIUeEqJ4GoWLr3Joi7NMruK8zL1uH5OxY2urRsz_4_lf_4Oa9a_sQ/s72-c/UniPi%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-393682532255114969&lt;/id>;&lt;published>;2023-04-11T11:22:00.001-07:00&lt;/published>;&lt;updated>;2023-04-21T12:37:00.241-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Developing an aging clock using deep learning on retinal images&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sara Ahadi, Research Fellow, Applied Science, and Andrew Carroll, Product Lead, Genomics&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJnv-sATdRuxawiBQHmuRV8smGfYjSveA-ZBCZfCkUOT6ZxvejcTU5ku2SkOkYo94FczvS5bbZywmWt24fq6UGyVX1hYKoSeHgV6arkL5Tm0Y65HsKy2RoOoylxGQADk1IYvXZ3RjNK_Lf9WhkB4jx6P7wLiLQPB5NHdfEpd08DOjl-ZYOOVvJyEQTfA/s850/genomics.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Aging is a process that is characterized by physiological and molecular changes that increase an individual&#39;s risk of developing diseases and eventually dying. Being able to measure and estimate the biological signatures of aging can help researchers identify preventive measures to reduce disease risk and impact. Researchers have developed “aging clocks” based on markers such as blood proteins or &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_methylation&quot;>;DNA methylation&lt;/a>; to measure individuals&#39; &lt;em>;biological&lt;/em>; age, which is distinct from one&#39;s &lt;em>;chronological&lt;/em>; age. These aging clocks help predict the risk of age-related diseases. But because protein and methylation markers require a blood draw, non-invasive ways to find similar measures could make aging information more accessible. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Perhaps surprisingly, the features on our &lt;a href=&quot;https://en.wikipedia.org/wiki/Retina&quot;>;retinas&lt;/a>; reflect a lot about us. Images of the retina, which has vascular connections to the brain, are a valuable source of biological and physiological information. Its features have been linked to several aging-related diseases, including diabetic retinopathy, cardiovascular disease, and Alzheimer&#39;s disease. Moreover, &lt;a href=&quot;https://ai.googleblog.com/2018/02/assessing-cardiovascular-risk-factors.html&quot;>;previous work from Google&lt;/a>; has shown that retinal images can be used to predict age, risk of cardiovascular disease, or even sex or smoking status. Could we extend those findings to aging, and maybe in the process identify a new, useful biomarker for human disease? &lt;/p>; &lt;p>; In a new paper “&lt;a href=&quot;https://elifesciences.org/articles/82364&quot;>;Longitudinal fundus imaging and its genome-wide association analysis provide evidence for a human retinal aging clock&lt;/a>;”, we show that deep learning models can accurately predict biological age from a retinal image and reveal insights that better predict age-related disease in individuals. We discuss how the model&#39;s insights can improve our understanding of how genetic factors influence aging. Furthermore, we&#39;re releasing the &lt;a href=&quot;https://gist.github.com/cmclean/a7e01b916f07955b2693112dcd3edb60&quot;>;code modifications&lt;/a>; for these models, which build on ML frameworks for analyzing retina images that we have &lt;a href=&quot;https://zenodo.org/record/7154413#.ZDCm2ezMKUs&quot;>;previously publicly released&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Predicting chronological age from retinal images&lt;/h2>; &lt;p>; We trained a model to predict chronological age using hundreds of thousands of retinal images from a telemedicine-based blindness prevention program that were captured in primary care clinics and de-identified. A subset of these images has been used in a &lt;a href=&quot;https://www.kaggle.com/competitions/diabetic-retinopathy-detection/&quot;>;competition by Kaggle&lt;/a>; and academic publications, including prior &lt;a href=&quot;https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html&quot;>;Google work with diabetic retinopathy&lt;/a>;. &lt;/p>; &lt;p>; We evaluated the resulting model performance both on a held-out set of 50,000 retinal images and on a separate &lt;a href=&quot;https://www.ukbiobank.ac.uk/&quot;>;UKBiobank&lt;/a>; dataset containing approximately 120,000 images. The model predictions, named eyeAge, strongly correspond with the true chronological age of individuals (shown below; &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;>;Pearson correlation&lt;/a>; coefficient of 0.87). This is the first time that retinal images have been used to create such an accurate aging clock. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWNxNxHrvjqtzKpuLC--s3bZN5WYgsqgVEh1wTDKGShr11iUvKJ-d7S_pcN6dG8rhdzHWGWOJkQSeQPkk3Oh86iXrN67P1fZwfSfqP8e3z9wnxtdADih0-gL4fEKUeoXDEyNafEFHPgHuAqtVvIdO6Pos6_TwvEy92LSp_cRtjALWRR_Xm72mYx3wb7A/s781/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;380&quot; data-original-width=&quot;781&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWNxNxHrvjqtzKpuLC--s3bZN5WYgsqgVEh1wTDKGShr11iUvKJ-d7S_pcN6dG8rhdzHWGWOJkQSeQPkk3Oh86iXrN67P1fZwfSfqP8e3z9wnxtdADih0-gL4fEKUeoXDEyNafEFHPgHuAqtVvIdO6Pos6_TwvEy92LSp_cRtjALWRR_Xm72mYx3wb7A/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left&lt;/b>;: A retinal image showing the macula (dark spot in the middle), optic disc (bright spot at the right), and blood vessels (dark red lines extending from the optic disc). &lt;b>;Right&lt;/b>;: Comparison of an individual&#39;s true chronological age with the retina model predictions, “eyeAge”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Analyzing the predicted and real age gap&lt;/h2>; &lt;p>; Even though eyeAge correlates with chronological age well across many samples, the figure above also shows individuals for which the eyeAge differs substantially from chronological age, both in cases where the model predicts a value much younger or older than the chronological age. This could indicate that the model is learning factors in the retinal images that reflect real biological effects that are relevant to the diseases that become more prevalent with biological age. &lt;/p>; &lt;p>; To test whether this difference reflects underlying biological factors, we explored its correlation with conditions such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Chronic_obstructive_pulmonary_disease&quot;>;chronic obstructive pulmonary disease&lt;/a>; (COPD) and myocardial infarction and other biomarkers of health like systolic blood pressure. We observed that a predicted age higher than the chronological age, correlates with disease and biomarkers of health in these cases. For example, we showed a statistically significant (p=0.0028) correlation between eyeAge and all-cause mortality — that is a higher eyeAge was associated with a greater chance of death during the study. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Revealing genetic factors for aging&lt;/h2>; &lt;p>; To further explore the utility of the eyeAge model for generating biological insights, we related model predictions to genetic variants, which are available for individuals in the &lt;a href=&quot;https://www.ukbiobank.ac.uk/enable-your-research/about-our-data/genetic-data&quot;>;large UKBiobank study&lt;/a>;. Importantly, an individual&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Germline&quot;>;germline genetics&lt;/a>; (the variants inherited from your parents) are fixed at birth, making this measure independent of age. This analysis generated a list of genes associated with accelerated biological aging (labeled in the figure below). The top identified gene from our genome-wide association study is &lt;em>;&lt;a href=&quot;https://www.genecards.org/cgi-bin/carddisp.pl?gene=ALKAL2&quot;>;ALKAL2&lt;/a>;&lt;/em>;, and interestingly the corresponding gene in fruit flies had &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1111/acel.13137&quot;>;previously been shown&lt;/a>; to be involved in extending life span in flies. Our collaborator, Professor Pankaj Kapahi from the &lt;a href=&quot;https://www.buckinstitute.org/&quot;>;Buck Institute for Research on Aging&lt;/a>;, found in laboratory experiments that reducing the expression of the gene in flies resulted in improved vision, providing an indication of &lt;em>;ALKAL2&lt;/em>; influence on the aging of the visual system. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijplgd_DSu6DAim9NZs6zkST-ONy4k66JUYK9Mtn-vj2cTgfkW8tDRmdAj3bDoC-N9mPwxhbS0X6Opa3s6gHwCaeCKHz11C6epW9OrTEiak_qANNy4sT1Rfmy0YsP88eJbCiIez5HKMdFdI5Youkdkckcw_ZWcvZW0ZiD0e-GP3MKTEpH4Xj_C4E1nUQ/s1064/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;1064&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijplgd_DSu6DAim9NZs6zkST-ONy4k66JUYK9Mtn-vj2cTgfkW8tDRmdAj3bDoC-N9mPwxhbS0X6Opa3s6gHwCaeCKHz11C6epW9OrTEiak_qANNy4sT1Rfmy0YsP88eJbCiIez5HKMdFdI5Youkdkckcw_ZWcvZW0ZiD0e-GP3MKTEpH4Xj_C4E1nUQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Manhattan_plot&quot;>;Manhattan plot&lt;/a>; representing significant genes associated with gap between chronological age and eyeAge. Significant genes displayed as points above the dotted threshold line.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Applications&lt;/h2>; &lt;p>; Our eyeAge clock has many potential applications. As demonstrated above, it enables researchers to discover markers for aging and age-related diseases and to identify genes whose functions might be changed by drugs to promote healthier aging. It may also help researchers further understand the effects of lifestyle habits and interventions such as exercise, diet, and medication on an individual&#39;s biological aging. Additionally, the eyeAge clock could be useful in the pharmaceutical industry for evaluating rejuvenation and anti-aging therapies. By tracking changes in the retina over time, researchers may be able to determine the effectiveness of these interventions in slowing or reversing the aging process. &lt;/p>; &lt;p>; Our approach to use retinal imaging for tracking biological age involves collecting images at multiple time points and analyzing them longitudinally to accurately predict the direction of aging. Importantly, this method is non-invasive and does not require specialized lab equipment. Our findings also indicate that the eyeAge clock, which is based on retinal images, is independent from blood-biomarker–based aging clocks. This allows researchers to study aging through another angle, and when combined with other markers, provides a more comprehensive understanding of an individual&#39;s biological age. Also unlike current aging clocks, the less invasive nature of imaging (compared to blood tests) might enable eyeAge to be used for actionable biological and behavioral interventions. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We show that deep learning models can accurately predict an individual&#39;s chronological age using only images of their retina. Moreover, when the predicted age differs from chronological age, this difference can identify accelerated onset of age-related disease. Finally, we show that the models learn insights which can improve our understanding of how genetic factors influence aging. &lt;/p>; &lt;p>; We&#39;ve publicly released the &lt;a href=&quot;https://gist.github.com/cmclean/a7e01b916f07955b2693112dcd3edb60&quot;>;code modifications&lt;/a>; used for these models which build on ML frameworks for analyzing retina images that we have &lt;a href=&quot;https://zenodo.org/record/7154413#.ZDCm2ezMKUs&quot;>;previously publicly released&lt;/a>;. &lt;/p>; &lt;p>; It is our hope that this work will help scientists create better processes to identify disease and disease risk early, and lead to more effective drug and lifestyle interventions to promote healthy aging. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work is the outcome of the combined efforts of multiple groups. We thank all contributors: Sara Ahadi, Boris Babenko, Cory McLean, Drew Bryant, Orion Pritchard, Avinash Varadarajan, Marc Berndl and Ali Bashir (Google Research), Kenneth Wilson, Enrique Carrera and Pankaj Kapahi (Buck Institute of Aging Research), and Ricardo Lamy and Jay Stewart (University of California, San Francisco). We would also like to thank Michelle Dimon and John Platt for reviewing the manuscript, and Preeti Singh for helping with publication logistics.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/393682532255114969/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/developing-aging-clock-using-deep.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/393682532255114969&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/393682532255114969&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/developing-aging-clock-using-deep.html&quot; rel=&quot;alternate&quot; title=&quot;Developing an aging clock using deep learning on retinal images&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJnv-sATdRuxawiBQHmuRV8smGfYjSveA-ZBCZfCkUOT6ZxvejcTU5ku2SkOkYo94FczvS5bbZywmWt24fq6UGyVX1hYKoSeHgV6arkL5Tm0Y65HsKy2RoOoylxGQADk1IYvXZ3RjNK_Lf9WhkB4jx6P7wLiLQPB5NHdfEpd08DOjl-ZYOOVvJyEQTfA/s72-c/genomics.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;