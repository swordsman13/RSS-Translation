<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-03-05T15:12:23.080-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="Quantum AI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="accessibility"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Audio"></category><category term="ICML"></category><category term="Physics"></category><category term="Responsible AI"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ML"></category><category term="ML Fairness"></category><category term="video"></category><category term="Awards"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Supervised Learning"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Speech Recognition"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Differential Privacy"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Large Language Models"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gboard"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graphs"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Generative AI"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1338&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-2754526782497247497&lt;/id>;&lt;发布>;2024-03-04T07:06:00.000-08:00&lt;/发布>;&lt;更新>;2024-03- 05T08：40：45.490-08：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http： //www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt; /category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;量子计算&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 在 APS 2024&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：Kate Weber 和 Shannon Leon，Google 研究中心，量子 AI 团队&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYix ldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s1600/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;显示： 没有任何;” />; &lt;p>; 今天，&lt;a href=&quot;https://www.aps.org/meetings/meeting.cfm?name=MAR24&quot;>;2024 年 3 月会议&lt;/a>; &lt;a href=&quot;https:// /www.aps.org/&quot;>;美国物理学会&lt;/a>; (APS) 在明尼苏达州明尼阿波利斯成立。 APS 2024 是一次涵盖物理学及相关领域主题的顶级会议，汇集了研究人员、学生和行业专业人士，分享他们的发现并建立合作伙伴关系，以实现物理相关科学和技术的根本性进步。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今年，Google 在 APS 上表现强劲，其展位由 Google &lt;a href=&quot;https://quantumai.google/&quot; 主办>;量子AI&lt;/a>;团队，整个会议期间进行了50+次演讲，并参与了会议组织活动、专题会议和活动。亲自参加 APS 2024？欢迎参观 Google 的量子人工智能展位，详细了解我们为解决该领域一些最有趣的挑战而所做的令人兴奋的工作。 &lt;!--访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 帐户了解 Google 展位活动（例如演示和问答环节） .-->; &lt;/p>; &lt;p>; 您可以详细了解我们在会议上展示的最新前沿工作以及下面的展位活动时间表（Google 员工以&lt;strong>;粗体&lt;/strong>;列出）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p >; 会议主席包括：&lt;strong>;Aaron Szasz&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;展位活动&lt; /h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;em>;此时间表可能会发生变化。请访问 Google Quantum AI 展位了解更多信息。&lt;/em>; &lt;/p>; &lt;p>; Crumble：用于可视化 QEC 电路的原型交互工具&lt;br />; 演讲者：&lt;strong>;Matt McEwen&lt;/strong>; &lt;br / >; 3 月 5 日，星期二 |中部标准时间上午 11:00 &lt;/p>; &lt;p>; Qualtran：用于容错算法的有效资源估计的开源库 &lt;br />; 演讲者：&lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; 3 月 5 日，星期二|下午 2:30 CST &lt;/p>; &lt;p>; Qualtran：用于容错算法的有效资源估计的开源库&lt;br />; 演讲者：&lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; 3 月 7 日星期四|上午 11:00 CST &lt;/p>; &lt;p>; 价值 500 万美元的 XPRIZE / Google Quantum AI 竞赛，旨在加速量子应用问答 &lt;br />; 演讲者：&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; 3 月 7 日，星期四|上午 11:00（美国中部标准时间） &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;演讲&lt;/h2>; &lt;h3>;周一&lt;/h3 >; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/A45.1&quot;>;证明少数国家的高度纠缠状态单量子位测量&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Hsin-Yuan Huang&lt;/strong>; &lt;br />; 作者：&lt;strong>;Hsin-Yuan Huang&lt;/strong>; &lt;br />; &lt;em>;会议A45：机器学习量子物理新领域&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/A51.2&quot;>;迈向高保真超导量子位的模拟量子模拟&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Trond Andersen&lt;/strong>; &lt;br />; 作者：&lt;strong>;Trond I Andersen&lt;/strong>;、&lt;strong>;小米&lt;/strong >;、&lt;strong>;阿米尔·H·卡拉姆卢&lt;/strong>;、&lt;strong>;尼基塔·阿斯特拉罕采夫&lt;/strong>;、&lt;strong>;安德烈·克洛茨&lt;/strong>;、&lt;strong>;朱莉娅·伯恩特森&lt;/strong>;、&lt;strong>;安德烈·佩图霍夫&lt;/strong>; &lt;strong>;、&lt;strong>;德米特里·阿巴宁&lt;/strong>;、&lt;strong>;Lev B Ioffe&lt;/strong>;、&lt;strong>;陈宇&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>;、&lt;strong>;Pedram Roushan&lt; /strong>; &lt;br />; &lt;em>;会议 A51：噪声量子硬件上的应用 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session /B50.6&quot;>;测量表面代码电路上下文中的电路错误&lt;/a>; &lt;br />;演讲者：&lt;strong>;Dripto M Debroy&lt;/strong>; &lt;br />;作者：&lt;strong>;Dripto M Debroy&lt;/strong >;、&lt;strong>;Jonathan A Gross&lt;/strong>;、&lt;strong>;Élie Genois&lt;/strong>;、&lt;strong>;张江&lt;/strong>; &lt;br />; &lt;em>;会议 B50：使用 QCVV 技术表征噪声&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.6&quot;>;惯性聚变目标设计的阻止本领的量子计算 I：物理概述和经典算法的局限性&lt;/a>; &lt;br />;演讲者：Andrew D. Baczewski &lt;br />;作者：&lt;strong>;Nicholas C. Rubin&lt;/strong>;、Dominic W. Berry、Alina Kononov、&lt;strong>;Fionn D.马龙、塔努吉·哈塔尔、亚历克·怀特、李俊浩、哈特穆特·内文、瑞安·巴布什、安德鲁D. Baczewski &lt;br />; &lt;em>;会议 B51：量子应用的异构设计&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.12352.pdf&quot;>;论文链接&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.7&quot;>;惯性聚变目标设计停止本领的量子计算二：物理概述以及经典算法的局限性&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Nicholas C. Rubin&lt;/strong>; &lt;br />; 作者：&lt;strong>;Nicholas C. Rubin&lt;/strong>;、Dominic W. Berry、阿丽娜·科诺诺夫、&lt;strong>;菲恩·D·马龙&lt;/strong>;、&lt;strong>;塔努吉·卡塔尔&lt;/strong>;、亚历克·怀特、&lt;strong>;李俊浩&lt;/strong>;、&lt;strong>;哈特穆特·内文&lt;/strong>;、&lt;strong >;Ryan Babbush&lt;/strong>;、Andrew D. Baczewski &lt;br />; &lt;em>;会议 B51：量子应用的异构设计&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/ 2308.12352.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B56.4&quot;>;校准超导量子位：来自NISQ 到容错&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Sabrina S Hong&lt;/strong>; &lt;br />; 作者：&lt;strong>;Sabrina S Hong&lt;/strong>; &lt;br />; &lt;em>;Session B56:从 NISQ 到容错&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B31.9&quot;>;测量和前馈引起的纠缠负性转变&lt; /a>; &lt;br />; 演讲者：&lt;strong>;Ramis Movassagh&lt;/strong>; &lt;br />; 作者：Alireza Seif、Yu-Xin Wang、&lt;strong>;Ramis Movassagh&lt;/strong>;、Aashish A. Clerk &lt;br />; &lt;em>;会议 B31：多体系统中测量引起的临界&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2310.18305.pdf&quot;>;论文链接&lt;/a>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B52.9&quot;>;噪声量子处理实验的有效量子体积、保真度和计算成本&lt;/a>; &lt; br />; 演讲者：&lt;strong>;Salvatore Mandra&lt;/strong>; &lt;br />; 作者：&lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;、&lt;strong>;Sergei V Isakov&lt;/strong>;、&lt;strong>;Salvatore Mandra&lt;/strong>; ，&lt;strong>;本杰明·维拉隆加&lt;/strong>;，&lt;strong>;X。 Mi&lt;/strong>;、&lt;strong>;Sergio Boixo&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;会议 B52：量子算法和复杂性&lt;/em>; &lt;br />; &lt;a href =&quot;https://arxiv.org/pdf/2306.15970.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/ Session/D60.4&quot;>;使用机器学习相互作用势和原子位置协方差绘制准确的固体热力学表&lt;/a>; &lt;br />;演讲者：Mgcini K Phuthi &lt;br />;作者：Mgcini K Phuthi、Yang Huang、Michael Widom , &lt;strong>;Ekin D Cubuk&lt;/strong>;, Venkat Viswanathan &lt;br />; &lt;em>;会议 D60：分子和材料的机器学习：化学空间和动力学&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期二&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https: //meetings.aps.org/Meeting/MAR24/Session/F50.4&quot;>;原位脉冲包络表征技术(INSPECT)&lt;/a>; &lt;br />;主讲人：&lt;strong>;张江&lt;/strong>; &lt;br />; 作者：&lt;strong>;张江&lt;/strong>;、&lt;strong>;Jonathan A Gross&lt;/strong>;、&lt;strong>;Élie Genois&lt;/strong>; &lt;br />; &lt;em>;会议 F50：高级随机基准测试和门校准&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/F50.11&quot;>;利用动态解耦来表征两个量子位门&lt;/a>; &lt; br />; 演讲者：&lt;strong>;Jonathan A Gross&lt;/strong>; &lt;br />; 作者：&lt;strong>;Jonathan A Gross&lt;/strong>;、&lt;strong>;张江&lt;/strong>;、&lt;strong>;Élie Genois、Dripto M Debroy&lt;/strong>;、Ze-Pei Cian*、&lt;strong>;Wojciech Mruczkiewicz&lt;/strong>; &lt;br />; &lt;em>;会议 F50：高级随机基准测试和门校准&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/EE01.2&quot;>;二次模型回归的统计物理&lt;/a>; &lt;br />;演讲者：Blake Bordelon &lt;br />;作者：Blake Bordelon、Cengiz Pehlevan、&lt;strong>;Yasaman Bahri&lt;/strong>; &lt;br />; &lt;em>;会议 EE01：V：统计和非线性物理 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /meetings.aps.org/Meeting/MAR24/Session/G51.2&quot;>;改进电子结构首次量子化模拟的状态准备&lt;/a>; &lt;br />; 演讲者：&lt;strong>;William J Huggins&lt;/strong>; &lt;作者：&lt;strong>;William J Huggins&lt;/strong>;、&lt;strong>;Oskar Leimkuhler&lt;/strong>;、&lt;strong>;Torin F Stetina&lt;/strong>;、&lt;strong>;Birgitta Whaley&lt;/strong>; &lt;br />; &lt;em>;会议 G51：哈密顿模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G30.2&quot;>;控制大型超导量子处理器&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Paul V. Klimov&lt;/strong>; &lt;br />; 作者：&lt;strong>;Paul V. Klimov&lt;/strong>;、&lt;strong>;Andreas Bengtsson&lt;/strong>;、&lt;克里斯·昆塔纳 (Chris Quintana)、亚历山大·布拉萨 (Alexandre Bourassa)、萨布丽娜·洪 (Sabrina Hong)、安德鲁·邓斯沃思 (Andrew Dunsworth)、凯文·J·萨辛格 (Kevin J. Satzinger) ，&lt;strong>;威廉·P·利文斯顿&lt;/strong>;，&lt;strong>;弗拉基米尔·西瓦克&lt;/strong>;，&lt;strong>;墨菲·Y·纽&lt;/strong>;，&lt;strong>;特隆德·I·安德森&lt;/strong>;，&lt;strong>;张亚星&lt;/strong>;、&lt;strong>;德斯蒙德·奇克&lt;/strong>;、&lt;strong>;陈子君&lt;/strong>;、&lt;strong>;查尔斯·尼尔&lt;/strong>;、&lt;strong>;凯瑟琳·埃里克森&lt;/strong>;、&lt;strong>;亚历杭德罗·格拉哈莱斯·道&lt;/strong>;、&lt;strong>;安东尼·梅格兰特&lt;/strong>;、&lt;strong>;佩德拉姆·鲁山&lt;/strong>;、&lt;strong>;亚历山大·科罗特科夫&lt;/strong>;、&lt;strong>;朱利安·凯利&lt;/strong>;、 &lt;strong>;Vadim Smelyanskiy&lt;/strong>;、&lt;strong>;Yu Chen&lt;/strong>;、&lt;strong>;Hartmut Neven&lt;/strong>; &lt;br />; &lt;em>;G30 会议：量子计算的商业应用&lt;/em>;&lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.02321.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org /Meeting/MAR24/Session/G50.5&quot;>;高斯玻色子采样：确定量子优势&lt;/a>; &lt;br />; 演讲者：Peter D Drummond &lt;br />; 作者：Peter D Drummond、Alex Dellios、Ned Goodman、Margaret D Reid，&lt;strong>;Ben Villalonga&lt;/strong>; &lt;br />; &lt;em>;会议 G50：量子表征、验证和验证 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings .aps.org/Meeting/MAR24/Session/G50.8&quot;>;关注复杂性 III：学习随机量子电路状态的复杂性&lt;/a>; &lt;br />; 演讲者：Hyejin Kim &lt;br />; 作者：Hyejin Kim，周一清、徐一辰、万超、周锦、&lt;strong>;Yuri D Lensky&lt;/strong>;、Jesse Hoke、&lt;strong>;Pedram Roushan&lt;/strong>;、Kilian Q Weinberger、Eun-Ah Kim &lt;br />; &lt;em >;会议 G50：量子表征、验证和验证 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/K48.10&quot;>;平衡超导电路中的耦合&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Daniel T Sank&lt;/strong>; &lt;br />; 作者：&lt;strong>;Daniel T Sank&lt;/strong>;、&lt;strong>;Sergei V Isakov&lt;/strong >;, &lt;strong>;Mostafa Khezri&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>; &lt;br />; &lt;em>;K48 会议：强驱动超导系统&lt;/em>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://meetings.aps.org/Meeting/MAR24/Session/K49.12&quot;>;使用 Qᴜᴀʟᴛʀᴀɴ 进行容错算法的资源估计&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Tanuj Khattar&lt;/strong>; &lt; br />; 作者：&lt;strong>;Tanuj Khattar&lt;/strong>;、&lt;b>;Matthew Harrigan&lt;/b>;、&lt;b>;Fionn D. Malone&lt;/b>;、&lt;b>;Nour Yosri&lt;/b>;、&lt;b>; Nicholas C. Rubin&lt;/b>;&lt;br />; &lt;em>;K49 会议：近期量子计算机的算法和实现&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期三&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/ Meeting/MAR24/Session/M24.1&quot;>;用超导量子比特发现新颖的量子动力学&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; 作者：&lt;strong>;Pedram Roushan&lt;/a>; &lt;br />; strong>; &lt;br />; &lt;em>;会议 M24：跨平台的模拟量子模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/M27 .7&quot;>;破译三阴性乳腺癌中的肿瘤异质性：动态细胞-细胞和细胞-基质相互作用的关键作用&lt;/a>; &lt;br />;演讲者：Susan Leggett &lt;br />;作者：Susan Leggett、Ian Wong , Celeste Nelson, Molly Brennan, &lt;strong>;Mohak Patel&lt;/strong>;, Christian Franck, Sophia Martinez, Joe Tien, Lena Gamboa, Thomas Valentin, Amanda Khoo, Evelyn K Williams &lt;br />; &lt;em>;第 M27 节：力学细胞和组织 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48.2&quot;>;实现受保护的电荷奇偶校验量子位&lt; /a>; &lt;br />; 演讲者：Abigail Shearrow &lt;br />; 作者：Abigail Shearrow、Matthew Snyder、Bradley G Cole、Kenneth R Dodge、Yebin Liu、Andrey Klots、&lt;strong>;Lev B Ioffe&lt;/strong>;、Britton L Plourde，Robert McDermott &lt;br />; &lt;em>;第 N48 场会议：非常规超导量子位&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48 .3&quot;>;受保护的电荷奇偶校验量子位的隧道结中的电子电容&lt;/a>; &lt;br />; 演讲者：Bradley G Cole &lt;br />; 作者：Bradley G Cole、Kenneth R Dodge、Yebin Liu、Abigail Shearrow、Matthew Snyder 、&lt;strong>;Andrey Klots&lt;/strong>;、&lt;strong>;Lev B Ioffe&lt;/strong>;、Robert McDermott、BLT Plourde &lt;br />; &lt;em>;第 N48 场会议：非常规超导量子位&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.7&quot;>;克服量子纠错中的泄漏&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Kevin C. Miao &lt;/strong>; &lt;br />; 作者：&lt;strong>;Kevin C. Miao&lt;/strong>;、&lt;strong>;Matt McEwen&lt;/strong>;、&lt;strong>;Juan Atalaya&lt;/strong>;、&lt;strong>;Dvir Kafri&lt;/strong>; >;、&lt;strong>;Leonid P. Pryadko&lt;/strong>;、&lt;strong>;Andreas Bengtsson&lt;/strong>;、&lt;strong>;Alex Opremcak&lt;/strong>;、&lt;strong>;Kevin J. Satzinger&lt;/strong>;、&lt;strong>;子君陈&lt;/strong>;、&lt;strong>;保罗·V·克里莫夫&lt;/strong>;、&lt;strong>;克里斯·昆塔纳&lt;/strong>;、&lt;strong>;拉吉夫·阿查里亚&lt;/strong>;、&lt;strong>;凯尔·安德森&lt;/strong>;、&lt;strong>; >;马库斯·安斯曼&lt;/strong>;、&lt;strong>;弗兰克·阿鲁特&lt;/strong>;、&lt;strong>;库纳尔·艾莉亚&lt;/strong>;、&lt;strong>;亚伯拉罕·阿斯福&lt;/strong>;、&lt;strong>;约瑟夫·C·巴丁&lt;/strong>;、 &lt;strong>;亚历山大·布拉萨&lt;/strong>;、&lt;strong>;珍娜·博瓦伊德&lt;/strong>;、&lt;strong>;莱昂·布里尔&lt;/strong>;、&lt;strong>;鲍勃·B·巴克利&lt;/strong>;、&lt;strong>;大卫·A·布尔&lt; /strong>;、&lt;strong>;蒂姆·伯格&lt;/strong>;、&lt;strong>;布莱恩·伯克特&lt;/strong>;、&lt;strong>;尼古拉斯·布什内尔&lt;/strong>;、&lt;strong>;胡安·坎佩罗&lt;/strong>;、&lt;strong>;本·基亚罗&lt; /strong>;、&lt;strong>;罗伯托·柯林斯&lt;/strong>;、&lt;strong>;保罗·康纳&lt;/strong>;、&lt;strong>;亚历山大·克鲁克&lt;/strong>;、&lt;strong>;本·科廷&lt;/strong>;、&lt;strong>;Dripto M.德布罗伊&lt;/strong>;、&lt;strong>;肖恩·德穆拉&lt;/strong>;、&lt;strong>;安德鲁·邓斯沃斯&lt;/strong>;、&lt;strong>;凯瑟琳·埃里克森&lt;/strong>;、&lt;strong>;雷扎·法特米&lt;/strong>;、&lt;strong>; >;维尼修斯·费雷拉&lt;/strong>;、&lt;strong>;莱斯利·弗洛雷斯·布尔戈斯&lt;/strong>;、&lt;strong>;易卜拉欣·福拉蒂&lt;/strong>;、&lt;strong>;奥斯汀·G·福勒&lt;/strong>;、&lt;strong>;布鲁克斯·福克斯&lt;/strong>; &lt;strong>;、&lt;strong>;贡萨洛·加西亚&lt;/strong>;、&lt;strong>;威廉·江&lt;/strong>;、&lt;strong>;克雷格·吉德尼&lt;/strong>;、&lt;strong>;玛丽莎·朱斯蒂娜&lt;/strong>;、&lt;strong>;Raja Gosula&lt;/strong>; &lt;strong>;、&lt;strong>;Alejandro Grajales Dau&lt;/strong>;、&lt;strong>;乔纳森 A. 格罗斯&lt;/strong>;、&lt;strong>;迈克尔 C. 汉密尔顿&lt;/strong>;、&lt;strong>;肖恩 D. 哈林顿&lt;/strong>;、 &lt;strong>;Paula Heu&lt;/strong>;、&lt;strong>;杰里米·希尔顿&lt;/strong>;、&lt;strong>;Markus R. Hoffmann&lt;/strong>;、&lt;strong>;Sabrina Hong&lt;/strong>;、&lt;strong>;Trent Huang&lt;/strong>; >;、&lt;strong>;阿什利·哈夫&lt;/strong>;、&lt;strong>;贾斯汀·艾夫兰&lt;/strong>;、&lt;strong>;埃文·杰弗里&lt;/strong>;、&lt;strong>;张江&lt;/strong>;、&lt;strong>;科迪·琼斯&lt;/strong>; >;、&lt;strong>;朱利安·凯利&lt;/strong>;、&lt;strong>;Seon Kim&lt;/strong>;、&lt;strong>;费多尔·科斯特里萨&lt;/strong>;、&lt;strong>;约翰·马克·克莱克鲍姆&lt;/strong>;、&lt;strong>;大卫·兰德休斯&lt;/strong>; 、&lt;strong>;帕维尔·拉普捷夫&lt;/strong>;、&lt;strong>;莉莉·劳斯&lt;/strong>;、&lt;strong>;肯尼·李&lt;/strong>;、&lt;strong>;布莱恩·J·莱斯特&lt;/strong>;、&lt;strong>;亚历山大·T . Lill&lt;/strong>;、&lt;strong>;Wayne Liu&lt;/strong>;、&lt;strong>;Aditya Locharla&lt;/strong>;、&lt;strong>;Erik Lucero&lt;/strong>;、&lt;strong>;Steven Martin&lt;/strong>;、&lt;strong>;安东尼·梅格兰特、&lt;strong>;小米&lt;/strong>;、&lt;strong>;希林·蒙塔泽里&lt;/strong>;、&lt;strong>;亚历克西斯·莫万&lt;/strong>;、&lt;strong>;奥弗·纳曼&lt;/strong>;、&lt;strong>;马修·尼利 (Matthew Neeley)、&lt;strong>;查尔斯·尼尔 (Charles Neill)&lt;/strong>;、&lt;strong>;Ani Nersisyan&lt;/strong>;、&lt;strong>;迈克尔·纽曼 (Michael Newman)&lt;/strong>;、&lt;strong>;Jiun How Ng&lt;/strong>;、&lt;strong>; >;安东尼·阮&lt;/strong>;、&lt;strong>;默里·阮&lt;/strong>;、&lt;strong>;丽贝卡·波特&lt;/strong>;、&lt;strong>;查尔斯·罗克&lt;/strong>;、&lt;strong>;佩德拉姆·鲁山&lt;/strong>;、&lt;strong>; >;坎南·桑卡拉戈马蒂&lt;/strong>;、&lt;strong>;克里斯托弗·舒斯特&lt;/strong>;、&lt;strong>;迈克尔·J·谢恩&lt;/strong>;、&lt;strong>;亚伦·肖特&lt;/strong>;、&lt;strong>;诺亚·舒蒂&lt;/strong>;、 &lt;strong>;弗拉基米尔·施瓦茨&lt;/strong>;、&lt;strong>;金德拉·斯克鲁兹尼&lt;/strong>;、&lt;strong>;W.克拉克·史密斯&lt;/strong>;、&lt;strong>;乔治·斯特林&lt;/strong>;、&lt;strong>;马可·萨莱&lt;/strong>;、&lt;strong>;道格拉斯·托尔&lt;/strong>;、&lt;strong>;阿尔弗雷多·托雷斯&lt;/strong>;、&lt;strong>;西奥多·怀特 (Theodore White)、&lt;strong>;布莱恩 WK Woo&lt;/strong>;、&lt;strong>;Z.杰米·姚&lt;/strong>;、&lt;strong>;叶平&lt;/strong>;、&lt;strong>;Juhwan Yoo&lt;/strong>;、&lt;strong>;格雷森·杨&lt;/strong>;、&lt;strong>;亚当·扎尔克曼&lt;/strong>;、&lt;strong>;朱宁峰&lt;/strong>;、&lt;strong>;尼古拉斯·佐布里斯特&lt;/strong>;、&lt;strong>;哈特穆特·内文&lt;/strong>;、&lt;strong>;瓦迪姆·斯梅尔扬斯基&lt;/strong>;、&lt;strong>;安德烈·佩图霍夫&lt;/strong>;、&lt;strong>; Alexander N. Korotkov&lt;/strong>;、&lt;strong>;Daniel Sank&lt;/strong>;、&lt;strong>;Yu Chen&lt;/strong>; &lt;br />; &lt;em>;会议 N51：量子纠错码性能和实现 I&lt;/em>; &lt;br />; &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://meetings.aps.org/Meeting/MAR24/Session/N51.11&quot;>;使用非均匀误差分布对表面代码的性能进行建模：第 1 部分&lt;/a>; &lt;br />; 演示者：&lt;strong>;Yuri D Lensky&lt;/strong>; &lt;br />; 作者：&lt;strong>;Yuri D Lensky&lt;/strong>;、&lt;strong>;Volodymyr Sivak&lt;/strong>;、&lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;、&lt;strong>;Igor Aleiner&lt;/strong>; strong>; &lt;br />; &lt;em>;会议 N51：量子纠错码性能和实现 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/ Session/N51.12&quot;>;使用非均匀误差分布对表面代码的性能进行建模：第 2 部分&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Volodymyr Sivak&lt;/strong>; &lt;br />; 作者：&lt;strong >;弗拉基米尔·西瓦克&lt;/strong>;、&lt;strong>;迈克尔·纽曼&lt;/strong>;、&lt;strong>;科迪·琼斯&lt;/strong>;、&lt;strong>;亨利·舒尔克斯&lt;/strong>;、&lt;strong>;德维尔·卡弗里&lt;/strong>;、&lt;strong>; >;Yuri D Lensky&lt;/strong>;、&lt;strong>;Paul Klimov&lt;/strong>;、&lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;会议 N51：量子误差修正代码性能和实现 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q51.7&quot;>;高度优化的张量网络收缩经典挑战性量子计算的模拟&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; 作者：&lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; &lt;em>;会议 Q51：量子经典算法的共同进化&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q61.7&quot;>;使用现代量子计算概念进行教学各级实践开源软件&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; 作者：&lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; &lt;em >;课程 Q61：各级量子信息教学 II&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期四&lt; /h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.1&quot;>;新电路和开放颜色代码的源解码器&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Craig Gidney&lt;/strong>; &lt;br />; 作者：&lt;strong>;Craig Gidney&lt;/strong>;、&lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;会议 S51：量子纠错码性能和实现 II&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2312.08813.pdf&quot;>;论文链接&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S18.2&quot;>;使用大型语言模型执行 Hartree-Fock 多体物理计算&lt; /a>; &lt;br />; 演讲者：&lt;strong>;Eun-Ah Kim&lt;/strong>; &lt;br />; 作者：&lt;strong>;Eun-Ah Kim&lt;/strong>;、Haining Pan、&lt;strong>;Nayantara Mudur&lt;/strong>; 、 William Taranto、&lt;strong>; Subhashini Venugopalan&lt;/strong>;、&lt;strong>;Yasaman Bahri&lt;/strong>;、&lt;strong>;Michael P Brenner&lt;/strong>; &lt;br />; &lt;em>;会议 S18：数据科学、人工智能和机器物理学习 I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.5&quot;>;减少地表资源开销的新方法代码&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Michael Newman&lt;/strong>; &lt;br />; 作者：&lt;strong>;Craig M Gidney&lt;/strong>;、&lt;strong>;Michael Newman&lt;/strong>;、&lt;strong>; Peter Brooks&lt;/strong>;、&lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;会议 S51：量子纠错码性能和实现 II&lt;/em>; &lt;br />; &lt;a href=&quot;https:/ /arxiv.org/pdf/2312.04522.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S49.10 &quot;>;将量子计算机应用于药物设计的挑战和机遇&lt;/a>; &lt;br />; 演讲者：Raffaele Santagati &lt;br />; 作者：Raffaele Santagati、Alan Aspuru-Guzik、&lt;strong>;Ryan Babbush&lt;/strong>;、Matthias Degroote 、莱蒂西亚·冈萨雷斯、艾丽卡·基瑟娃、尼古拉·莫尔、马库斯·奥佩尔、罗伯特·M·帕里什、&lt;strong>;尼古拉斯·C·鲁宾&lt;/strong>;、迈克尔·斯特雷夫、克里斯托弗·S·陶特曼、霍斯特·韦斯、内森·维贝、克莱门斯·乌奇-乌奇&lt;br />; &lt;em>;会议 S49：近期应用的量子算法进展&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2301.04114.pdf&quot;>;论文链接&lt;/em>; a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T45.1&quot;>;谷歌在新应用中寻找超二次量子优势的报道&lt;/ a>; &lt;br />; 演讲者：&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; 作者：&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;会议 T45：量子算法的最新进展&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T48.11&quot;>;Qubit 作为反射计&lt;/a>; &lt;br />; 演讲者：&lt;strong >;张亚星&lt;/strong>; &lt;br />; 作者：&lt;strong>;张亚星&lt;/strong>;、&lt;strong>;Benjamin Chiaro&lt;/strong>; &lt;br />; &lt;em>;会议T48：超导制造、封装和技术验证&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W14.3&quot;>;非局域测量引起的相变的随机矩阵理论Floquet 量子电路&lt;/a>; &lt;br />; 演讲者：Aleksei Khindanov &lt;br />; 作者：Aleksei Khindanov、&lt;strong>;Lara Faoro&lt;/strong>;、&lt;strong>;Lev Ioffe&lt;/strong>;、&lt;strong>;Igor Aleiner&lt; /strong>; &lt;br />; &lt;em>;第 W14 场会议：测量引起的相变&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/ W58.5&quot;>;MERA 有限密度多体基态的连续极限&lt;/a>; &lt;br />; 演讲者：Subhayan Sahu &lt;br />; 作者：Subhayan Sahu，&lt;strong>;Guifré Vidal&lt;/strong>; &lt;br / >; &lt;em>;W58 会议：流体动力学及相关学科的超大规模计算科学发现 II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/ Session/W50.8&quot;>;海森堡自旋链中无限温度下的磁化动力学&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Eliott Rosenberg&lt;/strong>; &lt;br />; 作者：&lt;strong>;Eliott Rosenberg&lt;/strong>;&lt;/ &lt;strong>;、&lt;strong>;特隆德·安德森&lt;/strong>;、Rhine Samajdar、&lt;strong>;安德烈·佩图霍夫&lt;/strong>;、杰西·霍克*、&lt;strong>;德米特里·阿巴宁&lt;/strong>;、&lt;strong>;Andreas Bengtsson&lt;/strong>;、 &lt;strong>;伊利亚·德罗兹多夫&lt;/strong>;、&lt;strong>;凯瑟琳·埃里克森&lt;/strong>;、&lt;strong>;保罗·克里莫夫&lt;/strong>;、&lt;strong>;小米&lt;/strong>;、&lt;strong>;亚历克西斯·莫万&lt;/strong>;、 &lt;strong>;马修·尼利&lt;/strong>;、&lt;strong>;查尔斯·尼尔&lt;/strong>;、&lt;strong>;拉吉夫·阿查亚&lt;/strong>;、&lt;strong>;理查德·艾伦&lt;/strong>;、&lt;strong>;凯尔·安德森&lt;/strong>;、 &lt;strong>;马库斯·安斯曼&lt;/strong>;、&lt;strong>;弗兰克·阿鲁特&lt;/strong>;、&lt;strong>;库纳尔·艾莉亚&lt;/strong>;、&lt;strong>;亚伯拉罕·阿斯福&lt;/strong>;、&lt;strong>;胡安·阿塔拉亚&lt;/strong>;、 &lt;strong>;约瑟夫·巴尔丁&lt;/strong>;，&lt;strong>;A.比尔梅斯&lt;/strong>;、&lt;strong>;吉娜·博尔托利&lt;/strong>;、&lt;strong>;亚历山大·布拉萨&lt;/strong>;、&lt;strong>;珍娜·博瓦尔德&lt;/strong>;、&lt;strong>;莱昂·布里尔&lt;/strong>;、&lt;strong>;迈克尔布劳顿&lt;/strong>;、&lt;strong>;鲍勃·B·巴克利&lt;/strong>;、&lt;strong>;大卫·布尔&lt;/strong>;、&lt;strong>;蒂姆·伯格&lt;/strong>;、&lt;strong>;布莱恩·伯克特&lt;/strong>;、&lt;strong>; >;尼古拉斯·布什内尔&lt;/strong>;、&lt;strong>;胡安·坎佩罗&lt;/strong>;、&lt;strong>;张洪深&lt;/strong>;、&lt;strong>;陈子君&lt;/strong>;、&lt;strong>;本杰明·基亚罗&lt;/strong>;、 &lt;strong>;德斯蒙德·奇克&lt;/strong>;、&lt;strong>;乔什·科根&lt;/strong>;、&lt;strong>;罗伯托·柯林斯&lt;/strong>;、&lt;strong>;保罗·康纳&lt;/strong>;、&lt;strong>;威廉·考特尼&lt;/strong>;、 &lt;strong>;亚历山大·克鲁克&lt;/strong>;、&lt;strong>;本·科廷&lt;/strong>;、&lt;strong>;德里普托·德布罗伊&lt;/strong>;、&lt;strong>;亚历山大·德尔·托罗·巴尔巴&lt;/strong>;、&lt;strong>;肖恩·德穆拉&lt;/strong>; >;、&lt;strong>;奥古斯丁·迪保罗&lt;/strong>;、&lt;strong>;安德鲁·邓斯沃斯&lt;/strong>;、&lt;strong>;克林特·厄尔&lt;/strong>;、&lt;strong>;E. Farhi&lt;/strong>;、&lt;strong>;雷扎·法特米&lt;/strong>;、&lt;strong>;维尼修斯·费雷拉&lt;/strong>;、&lt;strong>;莱斯利·弗洛雷斯&lt;/strong>;、&lt;strong>;易卜拉欣·福拉蒂&lt;/strong>;、&lt;strong>;奥斯汀福勒、&lt;strong>;布鲁克斯·福克斯&lt;/strong>;、&lt;strong>;贡萨洛·加西亚&lt;/strong>;、&lt;strong>;艾莉·杰诺瓦&lt;/strong>;、&lt;strong>;威廉·江&lt;/strong>;、&lt;strong>;克雷格吉德尼&lt;/strong>;、&lt;strong>;达尔·吉尔博亚&lt;/strong>;、&lt;strong>;玛丽莎·朱斯蒂娜&lt;/strong>;、&lt;strong>;拉贾·戈苏拉&lt;/strong>;、&lt;strong>;亚历杭德罗·格拉哈莱斯·道&lt;/strong>;、&lt;strong>;乔纳森·格罗斯、&lt;strong>;史蒂夫·哈贝格&lt;/strong>;、&lt;strong>;迈克尔·汉密尔顿&lt;/strong>;、&lt;strong>;莫妮卡·汉森&lt;/strong>;、&lt;strong>;马修·哈里根&lt;/strong>;、&lt;strong>;肖恩·哈灵顿、&lt;strong>;宝拉·休&lt;/strong>;、&lt;strong>;戈登·希尔&lt;/strong>;、&lt;strong>;马库斯·霍夫曼&lt;/strong>;、&lt;strong>;萨布丽娜·洪&lt;/strong>;、&lt;strong>;特伦特·黄&lt;/strong>;、&lt;strong>;阿什利·哈夫&lt;/strong>;、&lt;strong>;威廉·哈金斯&lt;/strong>;、&lt;strong>;列夫·约夫&lt;/strong>;、&lt;strong>;谢尔盖·伊萨科夫&lt;/strong>;、&lt;strong>;贾斯汀·艾维兰&lt;/strong>;、&lt;strong>;埃文·杰弗里&lt;/strong>;、&lt;strong>;张江&lt;/strong>;、&lt;strong>;科迪·琼斯&lt;/strong>;、&lt;strong>;帕沃尔·尤哈斯&lt;/strong>;、&lt;strong>; D . Kafri&lt;/strong>;、&lt;strong>;Tanuj Khattar&lt;/strong>;、&lt;strong>;Mostafa Khezri&lt;/strong>;、&lt;strong>;Mária Kieferová&lt;/strong>;、&lt;strong>;Seon Kim&lt;/strong>;、&lt;strong>;Alexei基塔耶夫、&lt;strong>;安德烈·克洛茨&lt;/strong>;、&lt;strong>;亚历山大·科罗特科夫&lt;/strong>;、&lt;strong>;费多尔·科斯特里察&lt;/strong>;、&lt;strong>;约翰·马克·克莱克鲍姆&lt;/strong>;、&lt;strong>;大卫·兰德休斯 (David Landhuis)、&lt;strong>;帕维尔·拉普捷夫 (Pavel Laptev)&lt;/strong>;、&lt;strong>;刘金铭&lt;/strong>;、&lt;strong>;莉莉·劳斯 (Lily Laws)&lt;/strong>;、&lt;strong>;李俊浩&lt;/strong>;、&lt;strong>; >;肯尼思·李&lt;/strong>;、&lt;strong>;尤里·连斯基&lt;/strong>;、&lt;strong>;布莱恩·莱斯特&lt;/strong>;、&lt;strong>;亚历山大·利尔&lt;/strong>;、&lt;strong>;韦恩·刘&lt;/strong>;、&lt;strong>; >;威廉·P·利文斯顿&lt;/strong>;，&lt;strong>;A.洛查拉&lt;/strong>;、&lt;strong>;萨尔瓦托雷·曼德拉&lt;/strong>;、&lt;strong>;猎户座马丁&lt;/strong>;、&lt;strong>;史蒂文·马丁&lt;/strong>;、&lt;strong>;贾罗德·麦克林&lt;/strong>;、&lt;strong>;马修麦克尤恩、&lt;strong>;塞内卡·米克斯&lt;/strong>;、&lt;strong>;苗凯文&lt;/strong>;、&lt;strong>;阿曼达·米斯萨拉&lt;/strong>;、&lt;strong>;希林·蒙塔泽里&lt;/strong>;、&lt;strong>;拉米斯Movassagh&lt;/strong>;、&lt;strong>;Wojciech Mruczkiewicz&lt;/strong>;、&lt;strong>;Ani Nersisyan&lt;/strong>;、&lt;strong>;迈克尔·纽曼&lt;/strong>;、&lt;strong>;Jiun How Ng&lt;/strong>;、&lt;strong>;安东尼·阮&lt;/strong>;、&lt;strong>;默里·阮&lt;/strong>;、&lt;strong>;M. Niu&lt;/strong>;、&lt;strong>;托马斯·奥布莱恩&lt;/strong>;、&lt;strong>;Seun Omonije&lt;/strong>;、&lt;strong>;亚历克斯·奥普莱姆卡&lt;/strong>;、&lt;strong>;丽贝卡·波特&lt;/strong>;、&lt;strong>; >;列昂尼德·普里亚德科&lt;/strong>;、&lt;strong>;克里斯·昆塔纳&lt;/strong>;、&lt;strong>;大卫·罗兹&lt;/strong>;、&lt;strong>;查尔斯·罗克&lt;/strong>;、&lt;strong>;N.鲁宾&lt;/strong>;、&lt;strong>;Negar Saei&lt;/strong>;、&lt;strong>;丹尼尔·桑克&lt;/strong>;、&lt;strong>;卡南·桑卡拉戈马蒂&lt;/strong>;、&lt;strong>;凯文·萨辛格&lt;/strong>;、&lt;strong>;亨利舒尔克斯&lt;/strong>;、&lt;strong>;克里斯托弗·舒斯特&lt;/strong>;、&lt;strong>;迈克尔·谢恩&lt;/strong>;、&lt;strong>;亚伦·肖特&lt;/strong>;、&lt;strong>;诺亚·舒蒂&lt;/strong>;、&lt;strong>;弗拉基米尔施瓦茨&lt;/strong>;、&lt;strong>;弗拉基米尔·西瓦克&lt;/strong>;、&lt;strong>;金德拉·斯克鲁兹尼&lt;/strong>;、&lt;strong>;克拉克·史密斯&lt;/strong>;、&lt;strong>;罗兰多·索玛&lt;/strong>;、&lt;strong>;乔治斯特林、&lt;strong>;道格·斯特兰&lt;/strong>;、&lt;strong>;马可·萨莱&lt;/strong>;、&lt;strong>;道格拉斯·托尔&lt;/strong>;、&lt;strong>;阿尔弗雷多·托雷斯&lt;/strong>;、&lt;strong>;吉弗雷维达尔，&lt;strong>;本杰明·维拉隆加&lt;/strong>;，&lt;strong>;凯瑟琳·沃尔格拉夫·海德韦勒&lt;/strong>;，&lt;strong>;西奥多·怀特&lt;/strong>;，&lt;strong>;布莱恩·吴&lt;/strong>;，&lt;strong>;程星&lt;/strong>;、&lt;strong>;杰米·姚&lt;/strong>;、&lt;strong>;叶平&lt;/strong>;、&lt;strong>;刘周焕&lt;/strong>;、&lt;strong>;格雷森·杨&lt;/strong>;、&lt;strong>; Adam Zalcman&lt;/strong>;、&lt;strong>;张亚星&lt;/strong>;、&lt;strong>;朱宁峰&lt;/strong>;、&lt;strong>;尼古拉斯·佐布里斯特&lt;/strong>;、&lt;strong>;Hartmut Neven&lt;/strong>;、&lt;strong>;瑞安·巴布什&lt;/strong>;、&lt;strong>;戴夫·培根&lt;/strong>;、&lt;strong>;塞尔吉奥·博伊索&lt;/strong>;、&lt;strong>;杰里米·希尔顿&lt;/strong>;、&lt;strong>;埃里克·卢塞罗&lt;/strong>;、&lt;strong>;安东尼·梅格兰特、&lt;strong>;朱利安·凯利&lt;/strong>;、&lt;strong>;陈宇&lt;/strong>;、&lt;strong>;Vadim Smelyanskiy&lt;/strong>;、Vedika Khemani、Sarang Gopalakrishnan、&lt;strong>;托马兹·普罗森&lt;/strong>; strong>;, &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;W50 会议：多体物理的量子模拟&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/ pdf/2306.09333.pdf&quot;>;论文链接&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W50.13&quot;>;快速多极量子计算机上的方法&lt;/a>; &lt;br />; 演讲者：Kianna Wan &lt;br />; 作者：Kianna Wan、Dominic W Berry、&lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;W50 会议：量子多体物理模拟&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;星期五&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y43.1&quot;>;量子计算产业和保护国家安全：什么工具会起作用吗？&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Kate Weber&lt;/strong>; &lt;br />; 作者：&lt;strong>;Kate Weber&lt;/strong>; &lt;br />; &lt;em>;第 Y43 场会议：行业、创新与国家安全：找到适当的平衡&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y46.3&quot;>;新颖的收费效果Fluxium 量子比特&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Agustin Di Paolo&lt;/strong>; &lt;br />; 作者：&lt;strong>;Agustin Di Paolo&lt;/strong>;、Kyle Serniak、Andrew J Kerman、&lt;strong>; >;William D Oliver&lt;/strong>; &lt;br />; &lt;em>;Y46 会议：基于 Fluxium 的超导 Quibits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting /MAR24/Session/Z46.3&quot;>;超导电路参数相互作用的微波工程&lt;/a>; &lt;br />; 演讲者：&lt;strong>;Ofer Naaman&lt;/strong>; &lt;br />; 作者：&lt;strong>;Ofer Naaman&lt;/a>; &lt;br />; strong>; &lt;br />; &lt;em>;会议 Z46：宽带参量放大器和循环器&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Z62 .3&quot;>;使用核多项式方法的大磁性晶胞的线性自旋波理论&lt;/a>; &lt;br />;演讲者：Harry Lane&lt;br />;作者：Harry Lane，Hao Zhang，David A Dahlbom，Sam Quinn，&lt; strong>;Rolando D Somma&lt;/strong>;、Martin P Mourigal、Cristian D Batista、Kipton Barros &lt;br />; &lt;em>;会议 Z62：合作现象、理论&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;b>;*&lt;/ b>;&lt;/sup>;在 Google 期间完成的工作&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2754526782497247497/comments/default&quot; rel=&quot;replies &quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html#comment-form &quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;编辑“ type =“application/atom+xml”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497”rel =“self”type =“application/atom+xml” &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html&quot; rel=&quot;alternate&quot; title=&quot;Google at APS 2024&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixld OZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1695264277638670894&lt;/id>;&lt;发布>;2024-02-22T12:05:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-23T10 :07:08.500-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器感知&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPrism：用于视频理解的基础视觉编码器&lt;/stitle>;&lt;content type=&quot; html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院高级研究科学家 Long Zhao 和高级软件工程师 Ting Liu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mag3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-g JSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 网络上有数量惊人的视频，涵盖了从人们分享的日常生活瞬间到历史时刻再到科学观察的各种内容，每个视频都包含了对世界的独特记录。正确的工具可以帮助研究人员分析这些视频，改变我们理解周围世界的方式。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 视频提供比静态图像丰富得多的动态视觉内容，捕捉实体之间的运动、变化和动态关系。分析这种复杂性以及公开视频数据的巨大多样性，需要超越传统图像理解的模型。因此，许多在视频理解方面表现最好的方法仍然依赖于为特定任务量身定制的专门模型。最近，使用视频基础模型 (ViFM) 在该领域取得了令人兴奋的进展，例如 &lt;a href=&quot;https://arxiv.org/abs/2109.14084&quot;>;VideoCLIP&lt;/a>;、&lt;a href=&quot;https ://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>; 和 &lt;a href=&quot;https: //arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;。然而，构建一个能够处理视频数据多样性的 ViFM 仍然是一个挑战。 &lt;/p>; &lt;p>; 为了构建通用视频理解的单一模型，我们引入了“&lt;a href=&quot;https://arxiv.org/abs/2402.13217&quot;>;VideoPrism：基础视觉编码器视频理解&lt;/a>;”。 VideoPrism 是一种 ViFM，旨在处理广泛的视频理解任务，包括分类、本地化、检索、字幕和问答 (QA)。我们在预训练数据和建模策略方面提出了创新。我们在海量且多样化的数据集上对 VideoPrism 进行预训练：3600 万个高质量视频文本对和 5.82 亿个带有噪声或机器生成的并行文本的视频剪辑。我们的预训练方法是针对这种混合数据而设计的，可以从视频文本对和视频本身中学习。 VideoPrism 非常容易适应新的视频理解挑战，并使用单个冻结模型实现最先进的性能。 &lt;/p>;&lt;p>;&lt;/p>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao /videoprism-blog/raw/main/teaser.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr -caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 是一种通用视频编码器，通过从单个冻结模型生成视频表示，可以在广泛的视频理解任务（包括分类、本地化、检索、字幕和问答）中获得最先进的结果。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;预训练数据&lt;/h2>; &lt;p>;强大的 ViFM 需要大量视频来进行训练 - 与其他视频类似基础模型 (FM)，例如大型语言模型 (LLM)。理想情况下，我们希望预训练数据能够成为世界上所有视频的代表性样本。虽然大多数视频自然没有完美的字幕或描述，但即使不完美的文本也可以提供有关视频语义内容的有用信息。 &lt;/p>; &lt;p>; 为了给我们的模型提供最好的起点，我们整理了一个由多个公共和私人数据集组成的庞大预训练语料库，包括 &lt;a href=&quot;https://rowanzellers.com/merlot/ &quot;>;YT-Temporal-180M&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2307.06942&quot;>;InternVid&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs /2204.00679&quot;>;VideoCC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2007.14937&quot;>;WTS-70M&lt;/a>;等。其中包括3600万个精心挑选的带有高质量字幕的视频，以及额外的 5.82 亿个剪辑，其中包含不同程度的嘈杂文本（例如自动生成的文字记录）。据我们所知，这是同类中最大、最多样化的视频训练语料库。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7 PE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s1999/image18 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;779&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZec sO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;视频文本预训练数据统计。 &lt;a href=&quot;https://arxiv.org/abs/2104.14806&quot;>;CLIP 相似度得分&lt;/a>;的巨大变化（越高越好）证明了我们预训练的多样化字幕质量数据，它是用于获取文本的各种方式的副产品。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;两阶段训练&lt;/h2>; &lt;p>; VideoPrism 模型架构源自标准&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;视觉转换器&lt;/a>; (ViT)，采用分解设计，可按&lt;a href>;顺序编码空间和时间信息=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>;。我们的训练方法利用了高质量的视频文本数据和上述带有噪声文本的视频数据。首先，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning&quot;>;对比学习&lt;/a>;（一种最小化正视频文本对之间的距离的方法）同时最大化负视频-文本对之间的距离）来教我们的模型将视频与其自己的文本描述（包括不完美的文本描述）进行匹配。这为将语义语言内容与视觉内容匹配奠定了基础。 &lt;/p>; &lt;p>; 经过视频-文本对比训练后，我们利用没有文本描述的视频集合。在这里，我们基于&lt;a href=&quot;https://arxiv.org/abs/2212.04500&quot;>;屏蔽视频建模框架&lt;/a>;来预测视频中的屏蔽补丁，并进行了一些改进。我们训练模型来预测第一阶段模型的视频级全局嵌入和令牌明智嵌入，以有效利用该阶段获得的知识。然后，我们随机打乱预测的标记，以防止模型学习捷径。 &lt;/p>; &lt;p>; VideoPrism 设置的独特之处在于我们使用两个互补的预训练信号：文本描述和视频中的视觉内容。文本描述通常关注事物的外观，而视频内容则提供有关运动和视觉动态的信息。这使得 VideoPrism 能够在需要了解外观和运动的任务中表现出色。 &lt;/p>; &lt;br />; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们对 VideoPrism 进行了广泛的评估，涵盖四大类视频理解任务，包括视频分类和本地化、视频文本检索、视频字幕、问答，以及科学的视频理解。 VideoPrism 在 33 个视频理解基准测试中的 30 个上实现了最先进的性能 - 所有这些都对单个冻结模型进行了最小程度的调整。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1 -TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999 /image20.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot; 1959&quot; 高度 = &quot;640&quot; src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNY elYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png&quot; 宽度 = &quot;628 &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 与之前性能最佳的 FM 相比。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />; &lt;/div>; &lt;h3>;分类和本地化&lt;/h3>; &lt;p>; 我们在涵盖分类和本地化任务的现有大规模视频理解基准 (&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;VideoGLUE&lt;/a>;) 上评估 VideoPrism。我们发现 (1) VideoPrism 优于所有其他最先进的 FM，并且 (2) 没有其他单一模型始终位居第二。这告诉我们，VideoPrism 已经学会了如何有效地将各种视频信号打包到一个编码器中（从不同粒度的语义到外观和运动提示），并且它可以在各种视频源上正常工作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4Lr LvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s1816/image12.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1816&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzm GuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;VideoPrism 优于最先进的方法（包括 &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;、&lt;a href=&quot; https://arxiv.org/abs/2104.11178&quot;>;VATT&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>;实习视频&lt;/a>;和&lt;a href=&quot;https ://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;）在&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;视频理解基准&lt;/a>;上。在此图中，我们显示了与之前最佳模型相比的绝对分数差异，以突出 VideoPrism 的相对改进。关于&lt;a href=&quot;http://vuchallenge.org/charades.html&quot;>;猜谜游戏&lt;/a>;、&lt;a href=&quot;http://activity-net.org/&quot;>;ActivityNet&lt;/a>;、&lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA&lt;/a>; 和 &lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA-K&lt;/a>; ，我们使用平均精度（mAP）作为评估指标。在其他数据集上，我们报告 top-1 准确率。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;与法学硕士结合&lt;/h3>; &lt;p>;我们进一步探索将 VideoPrism 与法学硕士结合起来，以释放其处理各种视频语言任务的能力。特别是，当与文本编码器（遵循 &lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;>;LiT&lt;/a>;）或语言解码器（例如 &lt;a href=&quot;https:/ /arxiv.org/abs/2305.10403&quot;>;PaLM-2&lt;/a>;），VideoPrism 可用于视频文本检索、视频字幕和视频 QA 任务。我们在一组广泛且具有挑战性的视觉语言基准上对组合模型进行了比较。 VideoPrism 在大多数基准测试中树立了新的技术水平。从视觉结果中，我们发现VideoPrism能够理解视频中的复杂运动和外观（例如，在下面的视觉示例中，模型可以识别窗口上旋转物体的不同颜色）。这些结果表明VideoPrism与语言模型具有很强的兼容性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx8 0PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1028&quot; height=&quot;580&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42Rz xZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/w640-h580/VideoPrismResults.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 与最先进的方法（包括 &lt;a href=&quot;https:// arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>; 和 &lt;a href=&quot;https://arxiv. org/abs/2204.14198&quot;>;Flamingo&lt;/a>;）在多个视频文本检索（顶部）以及视频字幕和视频质量检查（底部）基准上。我们还显示了与之前最佳模型相比的绝对分数差异，以突出 VideoPrism 的相对改进。我们在 &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video- 上报告了 Recall@1 and-language/&quot;>;MASRVTT&lt;/a>;、&lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX&lt;/a>; 和 &lt;a href=&quot; https://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;>;ActivityNet&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr 得分&lt;/a>; &lt; a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/&quot;>;MSRVTT- Cap&lt;/a>;、&lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX-Cap&lt;/a>; 和 &lt;a href=&quot;http:// youcook2.eecs.umich.edu/&quot;>;YouCook2&lt;/a>;，&lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSRVTT-QA&lt;/a>; 准确率排名第一和&lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSVD-QA&lt;/a>;，以及&lt;a href=&quot;https://arxiv.org/abs/cmp-lg /9406033&quot;>;&lt;a href=&quot;https://doc-doc.github.io/docs/nextqa.html&quot;>;NExT-QA&lt;/a>; 上的 WUPS 索引&lt;/a>;。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://github.com /garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;宽度=&quot;100%&quot;>; &lt;源src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/spin_roller_skating.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/源>; &lt;/视频>; &lt;视频自动播放=“”循环=“”静音=“”playsinline=“”宽度=“100％”>; &lt;source src =“https://github.com/garyzhao/videoprism-blog/raw/main/making_ice_cream_ski_lifting.mp4 &quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用 VideoPrism 和用于视频文本检索的文本编码器显示定性结果 (第一行）并适用于视频 QA 的语言解码器（第二行和第三行）。对于视频文本检索示例，蓝色条表示视频和文本查询之间的嵌入相似性。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;科学应用&lt;/h3>; &lt;p>; 最后，我们在使用的数据集上测试 VideoPrism跨领域的科学家，包括行为学、行为神经科学和生态学等领域。这些数据集通常需要领域专业知识来注释，为此我们利用社区开源的现有科学数据集，包括 &lt;a href=&quot;https://data. caltech.edu/records/zrznw-w7386&quot;>;飞行与飞行&lt;/a>;，&lt;a href=&quot;https://data.caltech.edu/records/s0vdx-0k302&quot;>;CalMS21&lt;/a>;，&lt;a href=&quot;https://shirleymaxx.github.io/ChimpACT/&quot;>;ChimpACT&lt;/a>; 和 &lt;a href=&quot;https://dirtmaxim.github.io/kabr/&quot;>;KABR&lt;/a>;。 VideoPrism 不仅表现出色，而且实际上超越了专门为这些任务设计的模型。这表明像 VideoPrism 这样的工具有可能改变科学家分析不同领域视频数据的方式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkya Zv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1 -s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200 “高度=“397”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE 5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism 在各种科学基准上均优于领域专家。我们显示绝对分数差异以突出 VideoPrism 的相对改进。我们报告了所有数据集的平均精度 (mAP)，但 KABR 除外，它使用类平均 top-1 精度。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论&lt; /h2>; &lt;p>; 通过 VideoPrism，我们推出了一款功能强大且多功能的视频编码器，为通用视频理解树立了新标准。我们对构建庞大且多样化的预训练数据集和创新建模技术的重视已经通过我们的广泛评估得到了验证。 VideoPrism 不仅始终优于强大的基线，而且其独特的泛化能力使其能够很好地处理一系列现实世界的应用程序。由于其潜在的广泛用途，我们致力于在我们的&lt;a href=&quot;http://ai.google/principles&quot;>;人工智能原则&lt;/a>;的指导下，继续在这一领域进行进一步负责任的研究。我们希望 VideoPrism 为人工智能和视频分析交叉领域的未来突破铺平道路，帮助实现 ViFM 在科学发现、教育和医疗保健等领域的潜力。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本博文代表所有 VideoPrism 作者撰写：Long Zhao、Nitesh B. Gundavarapu、Liangzhe Yuan、Hao Zhou、Shen严、詹妮弗·J·孙、卢克·弗里德曼、钱睿、托拜厄斯·韦安德、赵越、雷切尔·霍农、弗洛里安·施罗夫、杨明轩、大卫·A·罗斯、王惠生、哈特维格·亚当、米哈伊尔·西罗滕科、刘婷和龚博清。我们衷心感谢 David Hendon 的产品管理工作，以及 Alex Siegman、Ramya Ganeshan 和 Victor Gomes 的计划和资源管理工作。我们还要感谢 Hassan Akbari、Sherry Ben、Yoni Ben-Meshulam、Chun-Te Chu、Sam Clearwater、Yin Cui、Ilya Figotin、Anja Hauth、Sergey Ioffe、Xuhui Jia、Yeqing Li、Lu Jiang、Zu Kim、Dan Kondratyuk、Bill Mark、Arsha Nagrani、Caroline Pantofaru、Sushant Prakash、Cordelia Schmid、Bryan Seybold、Mojtaba Seyedhosseini、Amanda Sadler、Rif A. Saurous、Rachel Stigler、Paul Voigtlaender、Pingmei Xu、Chachao Yan、Xuan Yang 和 Yukun Zhu 参与讨论，支持和反馈对这项工作做出了巨大贡献。我们感谢 Jay Yagnik、Rahul Sukthankar 和 Tomas Izo 对这个项目的热情支持。最后，我们感谢 Tom Small、Jennifer J. Sun、Hao Zhou、Nitesh B. Gundavarapu、Luke Friedman 和 Mikhail Sirotenko 对撰写这篇博文提供的巨大帮助。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p >;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1695264277638670894/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/02/videoprism-foundational-visual-encoder.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPrism：用于视频理解的基础视觉编码器&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt; /name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http:// /schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>; &lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1 sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif&quot; width=&quot;72&quot; xmlns:media =&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com， 1999：blog-8474926331452026626.post-4343235509909091741&lt;/id>;&lt;发布>;2024-02-21T12:15:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-21T12:15:36.694-08:00 &lt; /updated>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“差异隐私”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;Gboard&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;category schema =&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;生产设备上语言模型的私人培训取得进展&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：谷歌研究科学家郑旭和软件工程师张彦翔&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl 1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 经过训练来预测给定输入文本的下一个单词的语言模型 (LM) 是许多应用程序的关键技术 [&lt;a href=&quot;https://blog.google/technology/ai/google-palm-2- ai-large-language-model/&quot;>;1&lt;/a>;、&lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;2&lt;/a>;]。在 &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;Gboard&lt;/a>; 中，LM用于通过支持诸如&lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;下一个单词预测&lt;/a>; (NWP)、&lt;a href=&quot;https:// support.google.com/gboard/answer/7068415&quot;>;智能撰写&lt;/a>;、&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;智能完成&lt;/a>;和&lt; a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;建议&lt;/a>;、&lt;a href=&quot;https://support.google.com/gboard/answer/2811346&quot;>;幻灯片输入&lt;/a>;&lt;span style=&quot;text-decoration: underline;&quot;>;、&lt;/span>;并&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;校对&lt;/一个>;。在用户设备而不是企业服务器上部署模型具有较低延迟和更好的模型使用隐私等优点。直接根据用户数据训练设备上模型可以有效提高 NWP 和 &lt;a href=&quot;https://blog.research.google/2021/11/predicting-text-selections-with.html&quot; 等应用程序的实用性能>;智能文本选择&lt;/a>;，保护用户数据的隐私对于模型训练很重要。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49 -Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/图像45.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “1600”数据原始宽度=“1996”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP- ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;由设备上语言模型提供支持的 Gboard 功能。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在本博客中，我们讨论了自 &lt;a href=&quot;https:// 的概念验证开发以来，多年来的研究进展如何为 Gboard LM 的私人培训提供动力。 blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;2017 年联邦学习&lt;/a>;（佛罗里达州）和正式&lt;a href=&quot;https://blog.research.google/2022/02 /federated-learning-with-formal.html&quot;>;2022 年的差异化隐私&lt;/a>; (DP) 保证。&lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative。 html&quot;>;FL&lt;/a>; 使手机能够协作学习模型，同时将所有训练数据保留在设备上，而 &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;DP&lt;/a >; 提供数据匿名化的可量化衡量标准。形式上，DP 通常用 (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) 来表征，值越小表示保证越强。机器学习 (ML) 模型被认为对 ε=10 具有&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;合理的 DP 保证，并且当 &lt;em>;δ&lt;/em>; 很小时，强 DP 保证 ε=1&lt;/a>;。 &lt;/p>; &lt;p>; 截至今天，Gboard 中的所有 NWP 神经网络 LM 均通过 FL 进行训练，并提供正式的 DP 保证，并且未来推出的所有基于用户数据训练的 Gboard LM 都需要 DP。这 30 多个 Gboard 设备上 LM 以 7 种以上语言和 15 多个国家/地区推出，并满足小 &lt;em>;δ&lt; 的 (&lt;em>;ɛ&lt;/em>;, &lt;em>;δ&lt;/em>;)-DP 保证/em>; 为 10&lt;sup>;-10&lt;/sup>;，ɛ 介于 0.994 和 13.69 之间。据我们所知，这是 Google 或任何地方在生产环境中已知的最大规模的用户级 DP 部署，并且首次实现 &lt;em>;ɛ&lt;/em>; &lt;em>;ɛ&lt;/em>; &lt;em>; 的强大 DP 保证。 1 已公布，用于直接根据用户数据训练的模型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Gboard 中的隐私原则和实践&lt;/h2>; &lt;p>; 在“&lt;a href=&quot;https”中://arxiv.org/abs/2306.14793&quot;>;Gboard 中的私有联合学习&lt;/a>;”，我们讨论了不同之处&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; >;隐私原则&lt;/a>;目前反映在生产模型中，包括：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;透明度和用户控制&lt;/em>;：我们披露使用哪些数据及其目的用途、在各个渠道中的处理方式以及 Gboard 用户如何轻松&lt;a href=&quot;https://support.google.com/gboard/answer/12373137&quot;>;配置&lt;/a>;学习中的数据使用情况楷模。 &lt;/li>;&lt;li>;&lt;em>;数据最小化&lt;/em>;：FL 立即聚合仅改进特定模型的重点更新。 &lt;a href=&quot;https://eprint.iacr.org/2017/281.pdf&quot;>;安全聚合&lt;/a>; (SecAgg) 是一种加密方法，可进一步保证只能访问临时更新的聚合结果。 &lt;/li>;&lt;li>;&lt;em>;数据匿名化&lt;/em>;：服务器应用DP来防止模型记住单个用户训练数据中的唯一信息。 &lt;/li>;&lt;li>;&lt;em>;可审计性和可验证性&lt;/em>;：我们在开源代码中公开了关键算法方法和隐私核算（&lt;a href=&quot;https://github.com/tensorflow/ federated/blob/main/tensorflow_federated/python/aggregators/ Differential_privacy.py&quot;>;TFF聚合器&lt;/a>;，&lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/dp_query /tree_aggregation_query.py&quot;>;TFP DPQuery&lt;/a>;、&lt;a href=&quot;https://github.com/google-research/federated/blob/master/dp_ftrl/blogpost_supplemental_privacy_accounting.ipynb&quot;>;DP 会计&lt;/a>;、和&lt;a href=&quot;https://github.com/google/federated-compute&quot;>;FL 系统&lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;简史&lt;/h3>; &lt;p>; 近年来，FL 已成为根据用户数据训练 &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;Gboard 设备上 LM&lt;/a>; 的默认方法。 2020 年，&lt;a href=&quot;https://arxiv.org/abs/1710.06963&quot;>;对模型更新进行剪辑并添加噪音&lt;/a>;的 DP 机制被用于&lt;a href=&quot;https://arxiv. org/abs/2009.10031&quot;>;防止在西班牙训练西班牙 LM 时的记忆&lt;/a>;，满足有限 DP 保证 (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml- models- Differentially-private.html&quot;>;“&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;如何 DP-fy ML“&lt;/a>; 指南中描述的第 3 层&lt;/a>;）。 2022 年，在 &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-Follow-The-Regularized-Leader (DP-FTRL) 算法&lt;/a>;的帮助下，西班牙 LM 成为第一个直接在用户数据上训练的生产神经网络宣布&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;正式的 DP 保证 (ε= 8.9, δ=10&lt;sup>;-10&lt;/sup>;)-DP&lt;/a>;（相当于报告的&lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated- Learning-with-formal.html&quot;>;ρ=0.81&lt;/a>;&lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1605.02065&quot;>;零集中差异隐私&lt;/a>;） ，因此满足&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;合理的隐私保证&lt;/a>; (&lt;a href=&quot;https ://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;第 2 层&lt;/a>;）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;联邦学习中默认的差异隐私&lt;/h2>; &lt;p>; In “&lt;a href=&quot; https://arxiv.org/abs/2305.18465&quot;>;具有差异隐私的 Gboard 语言模型的联邦学习&lt;/a>;”，我们宣布 Gboard 中的所有 NWP 神经网络 LM 都具有 DP 保证，并且未来推出的所有 Gboard LM根据用户数据进行训练需要 DP 保证。通过应用以下实践在 FL 中启用 DP：&lt;/p>; &lt;ul>; &lt;li>;使用&lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;>;多语言&lt;/a预训练模型>; &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;C4&lt;/a>; 数据集。 &lt;/li>;&lt;li>;通过对公共数据集的模拟实验，找到具有高实用性的大DP信噪比。增加参与一轮模型更新的客户端数量可以提高隐私性，同时保持噪声比固定以获得良好的实用性，直到满足 DP 目标或系统允许的最大值和人口规模。 &lt;/li>;&lt;li>;根据&lt;a href=&quot;https://arxiv.org/abs/1902.01046中的计算预算和估计人口配置参数以限制每个客户端可以贡献的频率（例如，每隔几天一次） &quot;>;FL 系统&lt;/a>;。 &lt;/li>;&lt;li>;运行&lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;训练，并限制通过&lt;a href选择的每台设备更新的幅度=&quot;https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e2b3a90e103368b6b6&quot;>;自适应裁剪&lt;/a>;，或根据经验进行修复。 &lt;/li>; &lt;/ul>; &lt;p>; SecAgg 还可以通过采用&lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for.html&quot;>;先进技术来应用改善尺度和灵敏度的计算和通信&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gx fKfT4aEv-_2MULO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N /s1600/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mULO5z laWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;具有差异隐私和 (SecAgg) 的联邦学习。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;报告 DP 保证&lt;/h3>; &lt;p>; 已启动的 Gboard NWP LM 的 DP 保证在下面的条形图中可视化。 &lt;em>;x&lt;/em>; 轴显示按语言区域标记并在相应群体上进行训练的 LM； &lt;em>;y&lt;/em>; 轴显示当 &lt;em>;δ&lt;/em>; 固定为较小值 10&lt;sup>;-10&lt;/sup>; 时 &lt;em>;ε&lt;/em>; 值 &lt; a href=&quot;https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf&quot;>;(ε, δ)-DP&lt;/a>;（越低越好）。根据 A/B 测试期间的用户交互指标进行测量，这些模型的实用性要么显着优于之前生产中的非神经模型，要么与之前没有 DP 的 LM 相当。例如，通过应用最佳实践，西班牙模式的DP保证从&lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal .html&quot;>;ε=8.9&lt;/a>;&lt;/em>; 至 &lt;em>;ε&lt;/em>;=5.37。 SecAgg 还用于在西班牙训练西班牙语模型和在美国训练英语模型。有关 DP 保证的更多详细信息，请参阅&lt;a href=&quot;https://blog.research.google/ 后的&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;附录&lt;/a>; 2023/05/making-ml-models- Differentially-private.html&quot;>;“&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;如何 DP-fy ML&lt;”中概述的指南&lt;/a>; /a>;”。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实现更强大的 DP 保证&lt;/h2>; &lt;p>; &lt;em>;ε&lt;/em>;~许多已推出的 LM 的 10 DP 保证对于 ML 模型来说已经被认为是&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;合理&lt;/a>;在实践中，Gboard 中的 DP FL 的旅程仍在继续，以改善用户的打字体验，同时保护数据隐私。我们很高兴地宣布，巴西的葡萄牙语和拉丁美洲的西班牙语生产语言模型首次在 DP 保证 &lt;em>;ε&lt;/em>; ≤ 1 的情况下进行培训和启动，满足 &lt;a href=&quot; https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;第 1 层强大的隐私保证&lt;/a>;。具体来说，(&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP 保证是通过运行高级 &lt;a href=&quot;https ://arxiv.org/abs/2306.08153&quot;>;矩阵分解 DP-FTRL&lt;/a>; (MF-DP-FTRL) 算法，每轮服务器模型更新训练都有超过 12,000 台设备参与，规模大于&lt;a href= “https://arxiv.org/abs/2305.18465&quot;>;6500+台设备的通用设置&lt;/a>;，以及精心配置的策略，限制每个客户在 14 天内最多参加 2000 轮训练的两次巴西庞大的葡萄牙语用户群体。使用类似的设置，es-US 西班牙语 LM 在拉丁美洲多个国家/地区的大量人群中进行了训练，以实现 (&lt;em>;ε&lt;/em>;=0.994，&lt;em>;δ&lt;/em>;=10&lt;sup>; -10&lt;/sup>;)-DP。 &lt;em>;ε&lt;/em>; ≤ 1 es-US 模型显着提高了许多国家的实用性，并在哥伦比亚、厄瓜多尔、危地马拉、墨西哥和委内瑞拉推出。对于人口较少的西班牙，es-ES LM 的 DP 保证从 &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;ε=5.37&lt;/a>;&lt;/em>; 提高到 &lt;em>;ε&lt;/em>;=3.42，只需将 &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; 替换为 &lt;a href=&quot;https://arxiv .org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>;，无需增加每轮参与的设备数量。为了保护隐私，&lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/federated/blob/master/mf_dpftrl_matrices/privacy_accounting.ipynb&quot;>;colab&lt;/a>; 中披露了更多技术细节会计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_A HnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;709&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA- M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP 对 Gboard NWP LM 的保证（紫色条代表 ε=8.9 的首次 es-ES 启动；青色条代表使用 &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>; 训练的模型的隐私改进； &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models- Differentially-private.html&quot;>;层级&lt;/a>;来自“&lt;a href=&quot;https://arxiv .org/abs/2303.00654&quot;>;如何实现 DP-fy ML&lt;/a>;“指南； en-US* 和 es-ES* 还使用 SecAgg 进行训练。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;讨论和后续步骤&lt;/h2>; &lt;p>;我们的经验表明，DP可以在实践中通过客户参与的系统算法协同设计来实现，并且当人群参与时，隐私性和实用性都可以很强。大量&lt;em>;和&lt;/em>;设备的贡献被汇总。隐私-实用性-计算权衡可以通过&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;使用公共数据&lt;/a>;来改善，&lt;a href=&quot;https://arxiv. org/abs/2306.08153&quot;>;新的 MF-DP-FTRL 算法&lt;/a>;，&lt;a href=&quot;https://github.com/google/differential-privacy&quot;>;并加强核算&lt;/a>;。通过这些技术，&lt;em>;ε&lt;/em>; ≤ 1 的强大 DP 保证是可能的，但仍然具有挑战性。实证隐私审计的积极研究 [&lt;a href=&quot;https://arxiv.org/abs/2302.03098&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2305.08846&quot;>;2 &lt;/a>;]表明 DP 模型可能比最坏情况 DP 保证所暗示的更加私密。在我们不断推动算法前沿的同时，隐私-效用-计算的哪个维度应该优先考虑？ &lt;/p>; &lt;p>; 我们正在积极致力于 ML 的所有隐私方面的工作，包括将 DP-FTRL 扩展到 &lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for .html&quot;>;分布式DP&lt;/a>;并提高&lt;a href=&quot;https://arxiv.org/abs/2306.14793&quot;>;可审计性和可验证性&lt;/a>;。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;可信执行环境&lt;/a>;为大幅增加模型大小和可验证隐私提供了机会。最近&lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;大型 LM 的突破&lt;/a>; (LLM) 激励我们&lt;a href=&quot;https:// arxiv.org/abs/2305.12132&quot;>;重新思考&lt;/a>;在私人培训中使用&lt;a href=&quot;https://arxiv.org/abs/2212.06470&quot;>;公共&lt;/a>;信息以及法学硕士之间未来更多的互动、设备上 LM 和 Gboard 制作。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;作者要感谢 Peter Kairouz、Brendan McMahan 和 Daniel Ramage 对博客文章本身的早期反馈，Shafeng Li 和 Tom Small 对动画人物的帮助，以及 Google 团队对算法设计、基础设施实施和生产维护的帮助。以下合作者直接对所呈现的结果做出了贡献：&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;研究和算法开发：Galen Andrew、Stanislav Chiknavaryan、Christopher A. Choquette-Choo、Arun Ganesh、Peter Kairouz、Ryan McKenna 、H. Brendan McMahan、Jesse Rosenstock、Timon Van Overveldt、Keith Rush、Shuang Song、Thomas Steinke、Abhradeep Guha Thakurta、Om Thakkar 和 Yuanbo Zhang。&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;基础设施、生产和领导支持：Mingqing Chen、Stefan Dierauf、Billy Dou、Hubert Eichner、Zachary Garrett、Jeremy Gillula、Jianpeng Hou、Hui Li、Xu Liu、Wenzhi Mao、Brett McLarnon、Mengchen Pei、Daniel Ramage、Swaroop Ramaswamy、Hai Cheng Sun、Andreas Terzis、王云、吴珊珊、肖宇和斋淑敏。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4343235509909091741/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 4343235509909091741&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;self&quot; type= &quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;进展设备上生产语言模型的私人培训” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri >;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1. blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6 MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025&lt;/id>;&lt;发布>;2024-02- 14T10:32:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-14T10:32:25.557-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;监督学习&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;了解概念漂移下训练数据的重要性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：博士前研究员 Nishant Jain 和研究科学家 Pradeep Shenoy , Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmq QFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s1600/temporalreweightinghero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 我们周围世界不断变化的性质对人工智能模型的开发提出了重大挑战。通常，模型是根据纵向数据进行训练的，希望所使用的训练数据能够准确地表示模型将来可能收到的输入。更一般地说，所有训练数据都同等相关的默认假设在实践中经常被打破。例如，下图显示了来自 &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; 非平稳学习基准的图像，它说明了对象的视觉特征如何在 10 年内显着演变。年跨度（我们称之为&lt;em>;缓慢概念漂移&lt;/em>;的现象），对对象分类模型提出了挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw -_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “662”数据原始宽度=“1999”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF- Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png&quot; />;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;来自 CLEAR 基准测试的示例图像。 （改编自 Lin 等人&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;.&lt;/a>;）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt; p>; 替代方法，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Online_machine_learning&quot;>;在线&lt;/a>;和&lt;a href=&quot;https://wiki.continualai.org/the- continualai-wiki/introduction-to-continual-learning&quot;>;持续学习&lt;/a>;，用少量最新数据反复更新模型，以保持最新状态。这隐含地优先考虑最近的数据，因为从过去数据中学到的知识会被后续更新逐渐删除。然而，在现实世界中，不同类型的信息以不同的速率失去相关性，因此存在两个关键问题：1) 在设计上，它们&lt;em>;仅&lt;/em>;关注最新数据，并丢失来自较旧数据的任何信号。被删除。 2) 无论数据内容如何，​​数据实例的贡献都会随着时间的推移均匀衰减。 &lt;/p>; &lt;p>; 在我们最近的工作“&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;非平稳学习的实例条件衰减时间尺度&lt;/a>;”中，我们提出在训练期间为每个实例分配一个重要性分数，以便最大限度地提高模型在未来数据上的性能。为了实现这一目标，我们采用了一个辅助模型，该模型使用训练实例及其年龄来生成这些分数。该模型是与主模型共同学习的。我们解决了上述挑战，并在一系列非平稳学习的基准数据集上比其他强大的学习方法取得了显着的成果。例如，在&lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;最近的非平稳学习大规模基准&lt;/a>;（10 年期间约 3900 万张照片）中，我们显示通过训练数据的学习重新加权，相对准确度提高了 15%。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;监督学习概念漂移的挑战&lt;/h2>; &lt;p>; 获得对缓慢概念的定量洞察为了解决这个问题，我们在&lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;最近的照片分类任务&lt;/a>;上构建了分类器，其中包含 10 年来来自社交媒体网站的大约 3900 万张照片。我们比较了离线训练和连续训练，前者以随机顺序对所有训练数据进行多次迭代，后者以顺序（时间）顺序对每个月的数据进行多次迭代。我们在训练期间和后续期间测量了模型的准确性，其中两个模型都被冻结，即没有进一步更新新数据（如下所示）。在训练期结束时（左图，x 轴 = 0），两种方法都看到了相同数量的数据，但表现出很大的性能差距。这是由于&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368&quot;>;灾难性遗忘&lt;/a>;造成的，这是持续学习中的一个问题，模型从早期开始就对数据有了解。训练序列中的 on 以不受控制的方式减少。另一方面，遗忘也有其优点——在测试期间（如右图所示），持续训练的模型的退化速度比离线模型慢得多，因为它较少依赖于旧数据。测试期间两个模型准确性的下降证实了数据确实随着时间的推移而变化，并且两个模型变得越来越不相关。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyV JBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1554&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2a PHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在照片分类任务上比较离线模型和持续训练模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt; div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;训练数据的时间敏感的重新加权&lt;/h2>; &lt;p>; 我们设计了一种结合离线学习优点的方法（灵活性有效地重用所有可用数据）和持续学习（淡化旧数据的能力）来解决缓慢的概念漂移问题。我们以离线学习为基础，然后仔细控制过去数据的影响和优化目标，两者都旨在减少未来的模型衰减。 &lt;/p>; &lt;p>; 假设我们希望训练一个模型，&lt;em>;M&lt;/em>;，&lt;em>; &lt;/em>;给定一段时间内收集的一些训练数据。我们建议还训练一个辅助模型，根据每个点的内容和年龄为其分配权重。该权重缩放了该数据点在 &lt;em>;M&lt;/em>; 训练目标中的贡献。权重的目标是提高&lt;em>;M&lt;/em>;在未来数据上的性能。 &lt;/p>; &lt;p>; 在&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;我们的工作&lt;/a>;中，我们描述了如何元学习辅助模型，&lt;/em>; em>;ie，以有助于模型&lt;em>;M&lt;/em>;本身学习的方式与&lt;em>;M&lt;/em>;一起学习。辅助模型的一个关键设计选择是我们以分解的方式分离出与实例和年龄相关的贡献。具体来说，我们通过组合多个不同的固定衰减时间尺度的贡献来设置权重，并学习给定实例对其最适合的时间尺度的近似“分配”。我们在实验中发现，这种形式的辅助模型优于我们考虑的许多其他替代方案，从无约束的联合函数到单一时间尺度的衰减（指数或线性），因为它结合了简单性和表现力。完整的详细信息可以在&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;论文&lt;/a>;中找到。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;实例权重评分&lt;/h2>; &lt;p>; 下面的上图显示我们的学习辅助模型确实提高了-在&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR物体识别挑战&lt;/a>;中对外观更现代的物体进行加权；看起来较旧的物体的权重相应降低。经过仔细检查（下图，基于梯度的&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;特征重要性&lt;/a>;评估），我们发现辅助模型侧重于内部的主要对象图像，而不是例如可能与实例年龄虚假相关的背景特征。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVB MBAndaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5o XImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;来自 &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; 基准测试的示例图像（相机和图像）计算机类别）分别由我们的辅助模型分配最高和最低权重。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 &quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWv k-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 自动; 边距-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpK xDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;对 &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; 基准测试中的示例图像进行辅助模型的特征重要性分析。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;大规模数据的收益&lt;/h3>; &lt;p>;我们首先研究大规模&lt;a href=&quot;https://arxiv.org/abs/ 2108.09020&quot;>;前面讨论的 &lt;a href=&quot;https://arxiv.org/abs/1503.01817&quot;>;YFCC100M 数据集&lt;/a>;上的照片分类任务&lt;/a>; (PCAT)，使用前五年的数据训练数据和未来五年的测试数据。我们的方法（如下图红色所示）比无重新加权基线（黑色）以及许多其他稳健的学习技术有了显着的改进。有趣的是，我们的方法故意牺牲了遥远过去的准确性（训练数据不太可能在未来再次出现），以换取测试期间的显着改进。此外，根据需要，我们的方法在测试期间的降解程度低于其他基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZ vhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20 xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;我们的方法与 PCAT 数据集上的相关基线的比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h3>;广泛的适用性&lt;/h3>; &lt;p>; 我们在来自学术文献的各种非平稳学习挑战数据集上验证了我们的发现（请参阅&lt;a href=&quot;https://arxiv .org/abs/2108.09020&quot;>;1&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;2&lt;/a>;，&lt;a href=&quot;https://arxiv.org /abs/2211.14238&quot;>;3&lt;/a>;、&lt;a href=&quot;https://proceedings.mlr.press/v206/awasthi23b/awasthi23b.pdf&quot;>;4&lt;/a>; 了解详细信息），涵盖数据源和模式（照片、卫星图像、社交媒体文本、医疗记录、传感器读数、表格数据）和大小（范围从 10k 到 39M 实例）。与每个数据集最近发布的基准方法相比，我们报告在测试期间取得了显着的进步（如下所示）。请注意，之前最著名的方法对于每个数据集可能有所不同。这些结果展示了我们的方法的广泛适用性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU 0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s765/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;765&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70 y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s16000/image7.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= &quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的方法在研究自然概念漂移的各种任务上的性能增益。我们报告的每个数据集的收益都超过了之前最著名的方法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br >; &lt;/div>; &lt;h3>;持续学习的扩展&lt;/h3>; &lt;p>; 最后，我们考虑我们工作的一个有趣的扩展。上述工作描述了如何使用持续学习启发的想法来扩展离线学习以处理概念漂移。然而，有时离线学习是不可行的——例如，如果可用的训练数据量太大而无法维护或处理。我们通过在用于顺序更新模型的每个数据桶的上下文中应用时间重新加权，以直接的方式调整我们的持续学习方法。该提案仍然保留了持续学习的一些限制，例如，模型更新仅针对最新的数据执行，并且所有优化决策（包括我们的重新加权）仅针对该数据做出。尽管如此，我们的方法在照片分类基准上始终优于常规的持续学习以及各种其他持续学习算法（见下文）。由于我们的方法与此处比较的许多基线中的想法是互补的，因此我们预计与它们结合时会获得更大的收益。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4 g4SPFD3le​​b56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3le​​b56aZHex95MM_yovx 2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;与最新基线相比，我们的方法的结果适应了持续学习。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们通过结合先前方法的优点（离线学习与它有效地重用数据，并持续学习，重点关注最新数据。我们希望我们的工作有助于提高模型在实践中对概念漂移的鲁棒性，并在解决缓慢概念漂移的普遍问题方面产生更多的兴趣和新想法。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢 Mike Mozer 在早期进行的许多有趣的讨论这项工作的各个阶段，以及开发过程中非常有用的建议和反馈。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5605933033299261025/comments/默认&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/learning-importance-of-training -data.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/5605933033299261025&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/learning-importance-of-training-data.html&quot; rel=&quot;alternate&quot; title=&quot;了解概念漂移下训练数据的重要性&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri >;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1. blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIubsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jX aMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5933365460125094774&lt;/id>;&lt;发布>;2024-02-13T14： 11:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-13T14:11:49.258-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns# &quot; term=&quot;差异隐私&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DP-Auditorium：用于审核差异隐私的灵活库&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究科学家 Mónica Ribero Díaz&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCC-8eL3OqA 9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>; (DP) 是一种随机机制的属性，它限制任何单个用户信息的影响，同时处理和分析数据。 DP 提供了强大的解决方案来解决人们日益增长的数据保护问题，支持&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;跨领域&lt;/a>;的技术&lt;a href=&quot;https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf&quot;>;行业&lt;/a>;和政府应用（例如，&lt;a href=&quot;https://www.census.gov/ programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/ Differential-privacy.html&quot;>;美国人口普查&lt;/a>;），而不损害个人用户身份。随着其采用率的增加，识别开发实施错误的机制的潜在风险非常重要。研究人员最近发现私有机制的数学证明及其实现存在错误。例如，&lt;a href=&quot;https://arxiv.org/pdf/1603.01699.pdf&quot;>;研究人员比较了&lt;/a>;六种稀疏矢量技术 (SVT) 变体，发现六种中只有两种真正满足所声称的隐私要求保证。即使数学证明是正确的，实现该机制的代码也很容易出现人为错误。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 然而，实用且高效的 DP 审计具有挑战性，主要是由于机制固有的随机性和测试保证的概率性质。此外，还存在一系列保证类型（例如，&lt;a href=&quot;https://dl.acm.org/doi/10.1007/11681878_14&quot;>;纯 DP&lt;/a>;、&lt;a href=&quot;https:// /link.springer.com/chapter/10.1007/11761679_29&quot;>;近似 DP&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>; 和 &lt;a href =&quot;https://arxiv.org/pdf/1603.01887.pdf&quot;>;集中的DP&lt;/a>;），这种多样性增加了制定审计问题的复杂性。此外，考虑到所提出的机制的数量，调试数学证明和代码库是一项艰巨的任务。虽然在特定的机制假设下存在&lt;em>;临时&lt;/em>;测试技术，但很少有人努力开发用于测试 DP 机制的可扩展工具。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;DP-Auditorium：用于审计差异隐私的大型库&lt;/a>;”中，我们引入一个&lt;a href=&quot;https://github.com/google/ Differential-privacy/tree/main/python/dp_auditorium&quot;>;开源库&lt;/a>;，用于仅通过黑盒访问机制来审核 DP 保证（即，不了解该机制的内部属性）。 DP-Auditorium 采用 Python 实现，并提供灵活的接口，允许贡献者不断提高其测试能力。我们还引入了新的测试算法，可以对 Rényi DP、纯 DP 和近似 DP 的函数空间执行散度优化。我们证明 DP-Auditorium 可以有效地识别 DP 保证违规行为，并建议哪些测试最适合检测各种隐私保证下的特定错误。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP 保证&lt;/h2>; &lt;p>; DP 机制的输出是从满足确保用户数据隐私的数学属性的概率分布 (&lt;em>;M&lt;/em>; (&lt;em>;D&lt;/em>;))。因此，DP 保证与概率分布对之间的属性紧密相关。如果由数据集 &lt;em>;D&lt;/em>; 和相邻数据集 &lt;em>;D&#39;&lt;/em>; 上的 &lt;i>;M&lt;/i>; 确定的概率分布（仅相差一条记录），则该机制是差分隐私的，在给定的差异指标下&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_indistinguishability&quot;>;无法区分&lt;/a>;&lt;/em>;。 &lt;/p>; &lt;p>; 例如，经典的&lt;a href=&quot;https://software.imdea.org/~federico/pubs/2013.ICALP.pdf&quot;>;近似 DP&lt;/a>; 定义指出一种机制如果&lt;a href=&quot;https://arxiv.org/pdf/1508.00335.pdf&quot;>;曲棍球棒散度，则近似为具有参数（&lt;em>;ε&lt;/em>;、&lt;em>;δ&lt;/em>;）的 DP &lt;/a>; 阶 &lt;em>;e&lt;sup>;ε&lt;/sup>;&lt;/em>;，介于 &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>; 和 &lt;em>;M&lt;/em>; 之间>;(&lt;em>;D&#39;&lt;/em>;)，至多为&lt;em>;δ&lt;/em>;。纯 DP 是近似 DP 的特殊实例，其中 &lt;em>;δ = 0&lt;/em>;。最后，考虑一种带有参数（&lt;em>;𝛼&lt;/em>;，&lt;em>;ε）&lt;/em的&lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;机制>; 如果 &lt;a href=&quot;https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy&quot;>;Rényi 散度&lt;/a>; 的阶 &lt;em>;𝛼&lt;/em>; 最多为 &lt;em>; ε&lt;/em>;（其中&lt;em>;ε&lt;/em>;是一个小的正值）。在这三个定义中，&lt;em>;ε&lt;/em>;不可互换，但直观地传达了相同的概念； &lt;em>;ε&lt;/em>; 值越大意味着两个分布之间的差异越大或隐私性越差，因为这两个分布更容易区分。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-Auditorium&lt;/h2>; &lt;p>; DP-Auditorium 包含两个主要组件：属性测试器和数据集查找器。属性测试人员从在特定数据集上评估的机制中获取样本作为输入，旨在识别所提供数据集中的隐私保证违规行为。数据集发现者建议隐私保证可能失败的数据集。通过结合这两个组件，DP-Auditorium 能够 (1) 自动测试不同的机制和隐私定义，以及 (2) 检测隐私保护机制中的错误。我们实现了各种私有和非私有机制，包括计算记录平均值的简单机制和更复杂的机制，例如不同的 SVT 和梯度下降&lt;/a>;机制变体。 &lt;/p>; &lt;p>; &lt;strong>;性能测试人员&lt;/strong>;确定是否存在证据来拒绝两个概率分布 &lt;em>;P&lt;/em>; 和 &lt;em>;Q&lt;/em>; 之间给定分歧的假设，受到预先指定的预算的限制，该预算由正在测试的 DP 保证确定。他们根据 &lt;em>;P&lt;/em>; 和 &lt;em>;Q 的样本计算下界，如果下界值超过预期偏差，则拒绝该属性。如果结果确实有界，则不提供任何保证。为了测试一系列隐私保证，DP-Auditorium 引入了三种新颖的测试仪：(1) HockeyStickPropertyTester、(2) RényiPropertyTester 和 (3) MMDPropertyTester。与其他方法不同，这些测试器不依赖于测试分布的显式直方图近似值。它们依赖于曲棍球棒散度、Rényi 散度和&lt;a href=&quot;https://jmlr.csail.mit.edu/papers/v13/gretton12a.html&quot;>;最大平均差异&lt;/a>;的变分表示（ MMD），可以通过函数空间的优化来估计散度。作为基线，我们实现了 &lt;a href=&quot;https://arxiv.org/abs/1806.06427&quot;>;HistogramPropertyTester&lt;/a>;，一种常用的近似 DP 测试器。虽然我们的三个测试仪遵循类似的方法，但为了简洁起见，我们在本文中重点关注 HockeyStickPropertyTester。 &lt;/p>; &lt;p>; 给定两个相邻数据集 &lt;em>;D&lt;/em>; 和 &lt;em>;D&#39;&lt;/em>;，HockeyStickPropertyTester 找到一个下限，&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px;position:relative;transfrom:scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp;用于 &lt;em>;M&lt;/em>;(&lt;em>; D) &lt;/em>;和 &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) 成立的概率很高。曲棍球棒散度强制两个分布 &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>; 和 &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) 在近似的 DP 保证。因此，如果隐私保证声称曲棍球棒散度至多&lt;em>;δ&lt;/em>;，并且&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px;position:relative; transfrom:scale( 4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; >; &lt;em>;δ&lt;/em>;，那么很有可能散度会高于 &lt;em>;D&lt;/em>; 和 &lt;em>;D&#39;&lt;/em>; 上的承诺，并且该机制无法满足给定的近似 DP 保证。下界&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px;position:relative; transfrom:scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp;被计算为曲棍球棒散度变分公式的经验且易于处理的对应物（有关更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/pdf/2307.05608.pdf&quot;>;论文&lt;/a>;） 。精度&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px;position:relative; transfrom:scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp;随着从机制中抽取的样本数量增加，但随着变分公式的简化而减少。我们平衡这些因素，以确保&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px;position:relative; transfrom:scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp;既准确又易于计算。 &lt;/p>; &lt;p>; &lt;strong>;数据集查找器&lt;/strong>;使用&lt;a href=&quot;https://arxiv.org/pdf/2207.13676.pdf&quot;>;黑盒优化&lt;/a>;来查找数据集&lt;em >;D&lt;/em>; 和 &lt;em>;D&#39;&lt;/em>; 最大化&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px;position:relative; transfrom:scale(4,0.5);&quot;>;^ &lt;/span>;δ&lt;/i>;，散度值&lt;em>;δ&lt;/em>;的下限。请注意，黑盒优化技术是专门为导出目标函数的梯度可能不切实际甚至不可能的设置而设计的。这些优化技术在探索和开发阶段之间振荡，以估计目标函数的形状并预测目标可以具有最佳值的区域。相比之下，完整的探索算法，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&quot;>;网格搜索方法&lt;/a>;，会搜索相邻数据集的整个空间&lt; em>;D&lt;/em>; 和 &lt;em>;D&#39;&lt;/em>;。 DP-Auditorium 通过开源黑盒优化库 &lt;a href=&quot;https://github.com/google/vizier&quot;>;Vizier&lt;/a>; 实现不同的数据集查找器。 &lt;/p>; &lt;p>; 在新机制上运行现有组件只需将该机制定义为 Python 函数，该函数采用数据数组 &lt;em>;D&lt;/em>; 和所需数量的样本 &lt;em>;n&lt;/em>; >; 由在 &lt;em>;D&lt;/em>; 上计算的机制输出。此外，我们为测试人员和数据集查找器提供灵活的包装器，允许从业者实现自己的测试和数据集搜索算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;主要结果&lt;/h2>; &lt;p>; 我们评估了 DP-Auditorium 对五个私人和九个具有不同产出空间的非私人机制。对于每个属性测试人员，我们使用不同的 &lt;em>;ε&lt;/em>; 值在固定数据集上重复测试十次，并报告每个测试人员识别隐私错误的次数。虽然没有测试仪始终优于其他测试仪，但我们发现了以前的技术（HistogramPropertyTester）可能会遗漏的错误。请注意，HistogramPropertyTester 不适用于 SVT 机制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x -iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s785/image22.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;409&quot; data-original-width=&quot;785&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHL AGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s16000/image22.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;每个属性测试人员发现所测试的非私有机制侵犯隐私的次数。 NonDPLaplaceMean 和 NonDPGaussianMean 机制是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_ Differential_privacy_mechanisms#Laplace_Mechanism&quot;>;Laplace&lt;/a>; 和 &lt;a href=&quot;https://en.wikipedia&quot; 的错误实现。 org/wiki/Additive_noise_ Differential_privacy_mechanisms#Gaussian_Mechanism&quot;>;用于计算均值的高斯&lt;/a>;机制。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们还分析了一个TensorFlow 中计算梯度的&lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py&quot;>;DP 梯度下降算法&lt;/a>; (DP-GD)私人数据的损失函数。为了保护隐私，DP-GD 采用裁剪机制将梯度的 &lt;a href=&quot;https://mathworld.wolfram.com/L2-Norm.html&quot;>;l2-norm&lt;/a>; 限制为值 &lt; em>;G&lt;/em>;，然后添加高斯噪声。此实现错误地假设添加的噪声的尺度为 &lt;em>;G&lt;/em>;，而实际上，尺度为 &lt;em>;sG&lt;/em>;，其中 &lt;em>;s&lt;/em>; 是正标量。这种差异导致近似 DP 保证仅适用于大于或等于 1 的 &lt;em>;s&lt;/em>; 值。&lt;/p>; &lt;p>; 我们评估了属性测试人员检测此错误的有效性，并表明： HockeyStickPropertyTester 和 RényiPropertyTester 在识别隐私侵犯方面表现出卓越的性能，优于 MMDPropertyTester 和 HistogramPropertyTester。值得注意的是，即使 &lt;em>;s&lt;/em>; 值高达 0.6，这些测试人员也能检测到该错误。值得强调的是，&lt;em>;s&lt;/em>;= 0.5 对应于 &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/308cbda4db6ccad5d1e7d56248727274e4c0c79e/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py#L445C1 -L446C1&quot;>;文献中的常见错误&lt;/a>;涉及在考虑隐私预算时遗漏了两倍&lt;em>;ε&lt;/em>;。 DP-Auditorium 成功捕获了此错误，如下所示。有关更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;此处&lt;/a>;第 5.6 节。 &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na 87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;836&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV 3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;使用 HistogramPropertyTester 测试 DP-GD 时不同 &lt;em>;s&lt;/em>; 值的估计偏差和测试阈值（&lt;strong>;左&lt;/strong>;）和HockeyStickPropertyTester（&lt;strong>;右&lt;/strong>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig- NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWil 6p_ljD/s828/image20.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;333&quot; data-original-width=&quot;828&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_t knZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s16000/image20.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;使用 RényiPropertyTester（&lt;strong>;左&lt;/strong>;）和 MMDPropertyTester（&lt;strong>;右&lt;/strong>;）测试 DP-GD 时，不同 &lt;em>;s&lt;/em>; 值的估计偏差和测试阈值)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 为了测试数据集查找器，我们计算在发现侵犯隐私之前探索的数据集数量。平均而言，大多数错误是对数据集查找器的调用不到 10 次即可发现。随机和探索/利用方法在查找数据集方面比网格搜索更有效。有关更多详细信息，请参阅 &lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;纸&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; DP 是最强大的数据保护框架之一。然而，正确实现 DP 机制可能具有挑战性，并且容易出现使用传统单元测试方法无法轻松检测到的错误。统一的测试框架可以帮助审计人员、监管机构和学者确保私人机制确实是私人的。 &lt;/p>; &lt;p>; DP-Auditorium 是一种通过函数空间的散度优化来测试 DP 的新方法。我们的结果表明，这种基于函数的估计始终优于以前的黑盒访问测试仪。最后，我们证明与直方图估计相比，这些基于函数的估计器可以更好地发现隐私错误。通过&lt;a href=&quot;https://github.com/google/ Differential-privacy/tree/main/python/dp_auditorium&quot;>;开源&lt;/a>; DP-Auditorium，我们的目标是建立端到端标准结束新的差分隐私算法的测试。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的工作是与 Andrés Muñoz 共同完成的梅迪纳、威廉·孔和奥马尔·赛义德。我们感谢 Chris Dibak 和 Vadym Doroshenko 为我们的库提供了有用的工程支持和界面建议。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5933365460125094774/comments /default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible- library-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/5933365460125094774&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;self “ type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html&quot; rel=&quot;alternate&quot; title= “DP-Auditorium：用于审核差异隐私的灵活库” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161 &lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:// /img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmC c-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss /&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3264694155710647310&lt;/id>; &lt;发布>;2024-02-06T11:17:00.000-08:00&lt;/发布>;&lt;更新>;2024-02-06T11:17:53.968-08:00&lt;/更新>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;图挖掘&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns #&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;TensorFlow 中的图形神经网络&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Dustin 发布Zelle，Google 研究部软件工程师，Arno Eigenwillig，CoreML 软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zd Lwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx -I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s1600/TFGNN%20hero.gif&quot; style=&quot;显示: 无;&quot; />; &lt;p>; 对象及其关系在我们周围的世界中无处不在，对于理解对象而言，关系与孤立地看待对象自身的属性一样重要——例如交通网络、生产网络、知识图谱或社交网络。离散数学和计算机科学在将此类网络形式化为&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;图&lt;/a>;&lt;/em>;方面有着悠久的历史，由以各种不规则方式通过&lt;em>;边&lt;/em>;连接的&lt;em>;节点&lt;/em>;组成。然而，大多数机器学习 (ML) 算法仅允许输入对象之间存在规则且统一的关系，例如像素网格、单词序列或根本没有关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;图神经网络&lt;/a>;，或 GNN简而言之，已经成为一种利用图的连接性的强大技术（如旧算法 &lt;a href=&quot;http://perozzi.net/projects/deepwalk/&quot;>;DeepWalk&lt;/a>; 和 &lt;a href= “https://snap.stanford.edu/node2vec/&quot;>;Node2Vec&lt;/a>;）以及各个节点和边上的输入特征。 GNN 可以对整个图（该分子是否以某种方式反应？）、单个节点（本文档的主题是什么，给出其引文？）或潜在边（该产品是否可能一起购买）进行预测与该产品？）。除了对图进行预测之外，GNN 也是一种强大的工具，用于弥合与更典型的神经网络用例之间的鸿沟。它们以&lt;em>;连续&lt;/em>;的方式对图的&lt;em>;离散&lt;/em>;、&lt;em>;关系&lt;/em>;信息进行编码，以便它可以自然地包含在另一个深度学习系统中。 &lt;/p>; &lt;p>; 我们很高兴地宣布发布 &lt;a href=&quot;https://github.com/tensorflow/gnn&quot;>;TensorFlow GNN 1.0&lt;/a>; (TF-GNN)，这是经过生产测试的用于大规模构建 GNN 的库。它支持 TensorFlow 中的建模和训练，以及从庞大的数据存储中提取输入图。 TF-GNN 是从头开始构建的异构图，其中对象和关系的类型由不同的节点和边集表示。现实世界的对象及其关系以不同的类型出现，而 TF-GNN 的异构焦点使得表征它们变得很自然。 &lt;/p>; &lt;p>; 在 TensorFlow 内部，此类图由 tfgnn.GraphTensor 类型的对象表示。这是一种复合张量类型（一个 Python 类中的张量集合），被接受为&lt;a href=&quot;https://en.wikipedia.org/wiki/First-class_citizen&quot;>;一等公民&lt;/a>; &lt;code>;tf.data.Dataset&lt;/code>;、&lt;code>;tf.function&lt;/code>;等。它存储图结构及其附加到节点、边和整个图的特征。 GraphTensors 的可训练转换可以定义为高层 &lt;a href=&quot;https://www.tensorflow.org/guide/keras&quot;>;Keras API&lt;/a>; 中的 Layers 对象，或者直接使用 tfgnn .GraphTensor&lt;/code>; 原语。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;GNN：对上下文中的对象进行预测&lt;/h2>; &lt;p>; 作为说明，让我们看一下TF-GNN 的一个典型应用是：预测由大型数据库的交叉引用表定义的图中某种类型节点的属性。例如，计算机科学 (CS) arXiv 论文的引文数据库具有一对多引用和多对一引用关系，我们希望在其中预测每篇论文的主题领域。 &lt;/p>; &lt;p>; 与大多数神经网络一样，GNN 在包含许多标记示例（约数百万个）的数据集上进行训练，但每个训练步骤仅包含一小批训练示例（例如数百个）。为了扩展到数百万级，GNN 在底层图中的相当小的子图流上进行训练。每个子图都包含足够的原始数据来计算其中心标记节点的 GNN 结果并训练模型。这个过程（通常称为子图采样）对于 GNN 训练极其重要。大多数现有工具以批量方式完成采样，生成用于训练的静态子图。 TF-GNN 提供了通过动态和交互采样来改进这一点的工具。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOSTu5_mWUEd Ns0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5 NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;如图所示，子图采样的过程，从较大的图中采样小的、易于处理的子图，为 GNN 训练创建输入示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; TF-GNN 1.0 首次推出了灵活的 Python API，用于在所有相关尺度上配置动态或批量子图采样：在 Colab 笔记本中以交互方式（例如 &lt;a href=&quot;https://colab.research.google.com/github /tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;这个&lt;/a>;），用于对存储在单个训练主机主内存中的小数据集进行高效采样，或由 &lt;a href 分发=&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; 用于存储在网络文件系统上的巨大数据集（多达数亿个节点和数十亿条边）。有关详细信息，请参阅我们的&lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md&quot;>;内存&lt;/a>;和分别是&lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md&quot;>;基于波束&lt;/a>;的采样。 &lt;/p>; &lt;p>; 在这些相同的采样子图上，GNN 的任务是计算根节点处的隐藏（或潜在）状态；隐藏状态聚合并编码根节点邻域的相关信息。一种经典方法是&lt;a href=&quot;https://research.google/pubs/neural-message-passing-for-quantum-chemistry/&quot;>;消息传递神经网络&lt;/a>;。在每轮消息传递中，节点沿着传入边缘接收来自邻居的消息，并从中更新自己的隐藏状态。经过 &lt;em>;n&lt;/em>; 轮后，根节点的隐藏状态反映了 &lt;em>;n&lt;/em>; 条边内所有节点的聚合信息（下图为 &lt;em>;n&lt;/em>; = 2） 。消息和新的隐藏状态由神经网络的隐藏层计算。在异构图中，对不同类型的节点和边使用单独训练的隐藏层通常是有意义的 &lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption -container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEu YswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “511”数据原始宽度=“573”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiE hiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif&quot;/>;&lt;/a>;&lt;/td >;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;如图所示，一个简单的消息传递神经网络，其中每一步，节点状态从外部传播到内部节点，在那里它被池化以计算新的节点状态。一旦到达根节点，就可以做出最终预测。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;通过将输出层放置在 GNN 隐藏状态之上来完成训练设置对于标记的节点，计算损失（以测量预测误差），并通过反向传播更新模型权重，就像任何神经网络训练中的惯例一样。 &lt;/p>; &lt;p>; 除了监督训练（即最小化由标签定义的损失）之外，GNN 还可以以无监督方式进行训练（即没有标签）。这让我们可以计算节点及其特征的&lt;em>;离散&lt;/em>;图结构的&lt;em>;连续&lt;/em>;表示（或&lt;em>;嵌入&lt;/em>;）。这些表示通常用于其他机器学习系统。通过这种方式，由图编码的离散关系信息可以包含在更典型的神经网络用例中。 TF-GNN 支持异构图无监督目标的细粒度规范。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;构建 GNN 架构&lt;/h2>; &lt;p>; TF-GNN 库支持在以下位置构建和训练 GNN：不同层次的抽象。 &lt;/p>; &lt;p>; 在最高级别，用户可以采用与库捆绑在一起并以 Keras 层表示的任何预定义模型。除了研究文献中的一小部分模型之外，TF-GNN 还提供了一个高度可配置的模型模板，该模板提供了精选的建模选择，我们发现这些模型选择可以为我们的许多内部问题提供强有力的基线。模板实现了 GNN 层；用户只需要初始化 Keras 层。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE 4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;865&quot; data-original-width=&quot;1400&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4 opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s16000/TFGNN%20code1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 在最底层，用户可以根据在图上传递数据的原语从头开始编写 GNN 模型，例如将数据从节点广播到其所有传出边缘或将数据池化到来自其所有传入边的节点（例如，计算传入消息的总和）。 TF-GNN 的图数据模型在涉及特征或隐藏状态时平等对待节点、边和整个输入图，从而不仅可以直接表达以节点为中心的模型（如上面讨论的 MPNN），还可以表达更通用的形式“https://arxiv.org/abs/1806.01261&quot;>;GraphNet&lt;/a>;。这可以（但不一定）使用 Keras 作为核心 TensorFlow 之上的建模框架来完成。有关更多详细信息和中级建模级别，请参阅 TF-GNN &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md&quot;>;用户指南&lt; /a>; 和&lt;a href=&quot;https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models&quot;>;模型集合&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;训练编排&lt;/h2>; &lt;p>; 虽然高级用户可以自由地进行自定义模型训练，但&lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md&quot;>;TF-GNN Runner&lt;/a>; 还提供了一种简洁的方法来协调训练Keras 对常见情况进行建模。一个简单的调用可能如下所示： &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right : auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TD D -zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s1400/TFGNN%20code2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin -左：自动；右边距：自动；&quot;>;&lt;img border=&quot; 0“数据原始高度=“508”数据原始宽度=“1400”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUF ujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt -MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s16000/TFGNN%20code2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Runner 提供即用型针对 ML 难题的解决方案，例如分布式训练和 Cloud TPU 上固定形状的 tfgnn.GraphTensor 填充。除了对单个任务进行训练（如上所示）之外，它还支持对多个（两个或更多）任务进行联合训练。例如，无监督任务可以与有监督任务混合，以告知具有应用程序特定归纳偏差的最终连续表示（或嵌入）。调用者只需用任务映射替换任务参数： &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_ PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; 数据原始-height =“392”数据原始宽度=“1400”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBC NsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s16000/TFGNN%20code3。 png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外，TF-GNN Runner 还包括 &lt;a href=&quot;https://www.gnn.png&quot; 的实现。 tensorflow.org/tutorials/interpretability/integrated_gradients&quot;>;用于模型归因的集成梯度&lt;/a>;。集成梯度输出是一个 GraphTensor，与观察到的 GraphTensor 具有相同的连接性，但其特征被梯度值取代，其中较大的值比较小的值在 GNN 预测中贡献更多。用户可以检查梯度值，以了解其 GNN 使用最多的特征。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 简而言之，我们希望 TF-GNN 能够有助于推进GNN 在 TensorFlow 中的大规模应用，并推动该领域的进一步创新。如果您想了解更多信息，请尝试我们的 &lt;a href=&quot;https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;Colab演示&lt;/a>;与流行的OGBN-MAG基准（在您的浏览器中，无需安装），浏览我们的其余部分&lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/ docs/guide/overview.md&quot;>;用户指南和 Colab&lt;/a>;，或者查看我们的&lt;a href=&quot;https://arxiv.org/abs/2207.03522&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;TF-GNN 版本 1.0 是由Google 研究部之间的合作：Sami Abu-El-Haija、Neslihan Bulut、Bahar Fatemi、Johannes Gasteiger、Pedro Gonnet、Jonathan Halcrow、Liangze Jiang、Silvio Lattanzi、Brandon Mayer、Vahab Mirrokni、Bryan Perozzi、Anton Tsitsulin、Dustin Zelle、Google Core ML：Arno Eigenwillig、Oleksandr Ferludin、Parth Kothari、Mihir Paradkar、Jan Pfeifer、Rachael Tamakloe 和 Google DeepMind：&lt;strong>; &lt;/strong>;Alvaro Sanchez-Gonzalez 和 Lisa Wang。&lt;/em>; &lt;/p>; &lt;/content >;&lt;link href=&quot;http://blog.research.google/feeds/3264694155710647310/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href= “http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html#comment-form”rel =“replies”title =“0条评论”type =“text/html”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/ 02/graph-neural-networks-in-tensorflow.html&quot; rel=&quot;alternate&quot; title=&quot;TensorFlow 中的图神经网络&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt; uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google .com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx -I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif&quot; width=&quot;72 &quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签： blogger.com，1999：blog-8474926331452026626.post-6956767920612914706&lt;/id>;&lt;已发布>;2024-02-02T11:07:00.000-08:00&lt;/已发布>;&lt;更新>;2024-02-07T16:05:00.722- 08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>; blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;用于时间序列预测的仅解码器基础模型&lt;/stitle>;&lt;content type=&quot;html&quot; >;&lt;span class=&quot;byline-author&quot;>;发布者：Rajat Sen 和 Yichen Zhou，Google 研究中心&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHm qHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/ s320/hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;时间序列&lt;/a>;预测在零售、金融、制造、医疗保健和自然等各个领域无处不在科学。例如，在零售用例中，据观察&lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and- value-of-deep-learning&quot;>;提高需求预测准确性&lt;/a>;可以显着降低库存成本并增加收入。深度学习 (DL) 模型已成为预测丰富、多元、时间序列数据的流行方法，因为它们已被证明在各种设置中都表现良好（例如，DL 模型在 &lt;a href=&quot;https: //www.sciencedirect.com/science/article/pii/S0169207021001874&quot;>;M5 竞赛&lt;/a>;）。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 与此同时，用于自然语言处理（NLP）任务的大型基础语言模型也取得了快速进展，例如&lt;a href= “https://en.wikipedia.org/wiki/Machine_translation&quot;>;翻译&lt;/a>;，&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented- Generation-rag -in-ai/&quot;>;检索增强生成&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligent_code_completion&quot;>;代码完成&lt;/a>;。这些模型接受来自各种来源（例如&lt;a href=&quot;https://commoncrawl.org/&quot;>;常见抓取&lt;/a>;和开源代码）的大量&lt;em>;文本&lt;/em>;数据的训练这使他们能够识别语言中的模式。这使得它们非常强大的&lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-shot_learning&quot;>;零样本&lt;/a>;工具；例如，&lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;与检索配合使用时&lt;/a>;，它们可以回答有关当前事件的问题并进行总结。 &lt;/p>; &lt;p>; 尽管基于深度学习的预测器在很大程度上&lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;表现优于&lt;/a>;传统方法，并且在&lt;a href=&quot;https: //cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;降低训练和推理成本&lt;/a>;，他们面临挑战：大多数深度学习架构需要&lt;a href=&quot;https: //cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;在客户可以在新的时间序列上测试模型之前，需要经过漫长而复杂的培训和验证周期&lt;/a>;。相比之下，时间序列预测的基础模型可以对未见的时间序列数据提供良好的开箱即用的预测，无需额外的训练，使用户能够专注于对实际下游任务的细化预测，例如&lt;a href =&quot;https://en.wikipedia.org/wiki/Customer_demand_planning&quot;>;零售需求规划&lt;/a>;。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://arxiv.org/pdf/2310.10688.pdf&quot;>;用于时间序列预测的仅解码器基础模型&lt;/a>;”中，我们引入了 TimesFM，这是一个在包含 1000 亿个现实世界时间点的大型时间序列语料库上进行预训练的单一预测模型。与最新的大型语言模型（LLM）相比，TimesFM 要小得多（2 亿个参数），但我们表明，即使在这样的规模下，它在不同领域和时间粒度的各种未见过的数据集上的零样本性能也接近在这些数据集上明确训练的最先进的监督方法。今年晚些时候，我们计划在 &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>; 中向外部客户提供此模型Google Cloud Vertex AI&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;用于时间序列预测的仅解码器基础模型&lt;/h2>; &lt;p>; 法学硕士通常是以&lt;a href=&quot;https://arxiv.org/pdf/1801.10198.pdf&quot;>;仅解码器&lt;/a>;方式进行训练，涉及三个步骤。首先，文本被分解为称为标记的子词。然后，这些令牌被送入堆叠的因果层，这些层产生与每个输入令牌相对应的输出（它无法处理未来的令牌） 。最后，对应于第 &lt;em>;i&lt;/em>; 个标记的输出总结了先前标记的所有信息，并预测第 (&lt;em>;i&lt;/em>;+1) 个标记。在推理过程中，LLM 一次生成一个令牌的输出。例如，当提示“法国的首都是什么？”时，它可能会生成标记“The”，然后以“法国的首都是什么？”为条件。 ”生成下一个标记“资本”，依此类推，直到生成完整的答案：“法国的首都是巴黎”。时间序列预测的基础模型应该适应可变的上下文（我们观察到的内容）和范围（我们查询模型预测的内容）长度，同时具有足够的能力来编码大型预训练中的所有模式数据集。与 LLM 类似，我们使用堆叠变压器层（自注意力和&lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;前馈&lt;/a>;层）作为 TimesFM 模型的主要构建块。在时间序列预测的背景下，我们将补丁（一组连续的时间点）视为由最近的&lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;长-地平线预测工作&lt;/a>;。然后，任务是根据堆叠变压器层末尾的第 (&lt;em>;i&lt;/em>;) 个输出来预测第 (&lt;em>;i&lt;/em>;+1) 个时间点补丁。 &lt;/p>; &lt;p>; 然而，它与语言模型有几个关键的区别。首先，我们需要一个具有残差连接的&lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;多层感知器&lt;/a>;块，将时间序列补丁转换为可以输入的令牌到变压器层以及&lt;a href=&quot;https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/&quot;>;位置编码&lt;/a>;（体育）。为此，我们使用类似于我们之前在&lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;长期预测&lt;/a>;中的工作的残差块。其次，在另一端，来自堆叠变压器的输出令牌可以用于预测比输入补丁长度更长的后续时间点的长度，即，输出补丁长度可以大于输入补丁长度。 &lt;/p>; &lt;p>; 考虑使用长度为 512 个时间点的时间序列来训练输入补丁长度为 32 和输出补丁长度为 128 的 TimesFM 模型。在训练期间，模型同时被训练为使用前 32 个时间点- 点预测接下来的128个时间点，前64个时间点预测第65至192个时间点，前96个时间点预测第97至224个时间点，依此类推。在推理过程中，假设给模型一个长度为 256 的新时间序列，并负责预测未来的接下来 256 个时间点。该模型将首先生成时间点 257 到 384 的未来预测，然后以初始 256 长度输入加上生成的输出为条件，生成时间点 385 到 512。另一方面，如果在我们的模型中，输出补丁长度等于输入补丁长度 32，那么对于相同的任务，我们将必须经历八个生成步骤，而不仅仅是上面的两个。这增加了累积更多错误的机会，因此，在实践中，我们发现较长的输出补丁长度可以为长范围预测带来更好的性能&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0 &quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mra Ndd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border= “0”数据原始高度=“674”数据原始宽度=“1084”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0PO ILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3。 jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TimesFM 架构。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;预训练数据&lt;/h2>; &lt;p>; 就像LLM一样代币越多越好，TimesFM 需要大量合法的时间序列数据来学习和改进。我们花费了大量的时间来创建和评估我们的训练数据集，以下是我们发现效果最好的数据集：&lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;合成数据有助于基础知识。&lt;/strong>;可以使用统计模型或物理模拟生成有意义的合成时间序列数据。这些基本的时间模式可以教会模型时间序列预测的语法。 &lt;/p>;&lt;/div>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;真实世界的数据增添了真实世界的味道。&lt;/strong>;我们梳理了可用的公共时间序列数据集，并有选择地汇集了1000亿个时间点的大型语料库。这些数据集包括 &lt;a href=&quot;https://trends.google.com/trends/&quot;>;Google 趋势&lt;/a>; 和 &lt;a href=&quot;https://meta.wikimedia.org/wiki/Research： Page_view&quot;>;维基百科页面浏览量&lt;/a>;，跟踪人们感兴趣的内容，并且很好地反映了许多其他现实世界时间序列的趋势和模式。这有助于 TimesFM 了解更大的情况，并在提供训练期间未见的特定领域上下文时更好地进行概括。 &lt;/p>;&lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;零样本评估结果&lt;/h2>; &lt;p>; 我们评估 TimesFM 零样本使用流行的时间序列基准对训练期间未见过的数据进行拍摄。我们观察到，TimesFM 的表现优于大多数统计方法，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot;>;ARIMA&lt;/a>;、&lt;a href=&quot;https://en.wikipedia. org/wiki/Exponential_smoothing&quot;>;ETS&lt;/a>;，并且可以匹配或超越强大的深度学习模型，例如 &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>;、&lt;a href=&quot; https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; 已在目标时间序列上&lt;em>;显式训练&lt;/em>;。 &lt;/p>; &lt;p>; 我们使用&lt;a href=&quot;https://huggingface.co/datasets/monash_tsf&quot;>;莫纳什预测档案&lt;/a>;来评估 TimesFM 的开箱即用性能。该档案包含来自交通、天气和需求预测等各个领域的数万个时间序列，覆盖频率从几分钟到每年的数据。根据现有文献，我们检查&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;平均绝对误差&lt;/a>; (MAE) &lt;a href=&quot;https://arxiv.org/ abs/2310.07820&quot;>;适当缩放&lt;/a>;，以便可以在数据集中进行平均。我们发现零样本（ZS）TimesFM 优于大多数监督方法，包括最近的深度学习模型。我们还将 TimesFM 与 &lt;a href=&quot;https://platform.openai.com/docs/models/gpt-3-5&quot;>;GPT-3.5&lt;/a>; 进行比较，以使用 &lt;a href 提出的特定提示技术进行预测=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime(ZS)&lt;/a>;。我们证明 TimesFM 的性能优于 llmtime(ZS)，尽管其数量级要小一些。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cbl qgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;876&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0 TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;TimesFM(ZS) 与 Monash 数据集上的其他监督和零样本方法相比的缩放 MAE（越低越好）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; 大多数 Monash 数据集都是短或中水平的，即预测长度不会太长。我们还根据最近最先进的基线 &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;（以及其他长期预测）在流行的长期预测基准上测试了 TimesFM。地平线预测基线）。在下图中，我们在 &lt;a href=&quot;https://paperswithcode.com/dataset/ett&quot;>;ETT&lt;/a>; 数据集上绘制了预测未来 96 和 192 个时间点任务的 MAE。该指标是在每个数据集的最后一个测试窗口上计算的（如 &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime&lt;/a>; 论文所做的那样）。我们看到 TimesFM 不仅超越了 llmtime(ZS) 的性能，而且还与在各自数据集上显式训练的监督 PatchTST 模型相匹配。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6a D5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;433&quot; data-original-width=&quot;735&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLU kI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;TimesFM(ZS) 相对于 llmtime(ZS) 和 ETT 数据集上的长期预测基线的最后一个窗口 MAE（越低越好）。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们训练一个解码器 -使用 100B 个现实世界时间点的大型预训练语料库进行时间序列预测的唯一基础模型，其中大部分是来自 Google 趋势的搜索兴趣时间序列数据和来自维基百科的页面浏览量。我们表明，即使是使用我们的 TimesFM 架构的相对较小的 200M 参数预训练模型，在来自不同领域和粒度的各种公共基准测试上也显示出令人印象深刻的零样本性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是多个团队合作的结果Google 研究院和 Google Cloud 的个人，包括（按字母顺序排列）：Abhimanyu Das、Weihao Kong、Andrew Leach、Mike Lawrence、Alex Martin、Rajat Sen、Yang Yang、Skander Hannachi、Ivan Kuznetsov 和 Yichen Zhou。&lt;/em>; &lt; /p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6956767920612914706/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// blog.research.google/2024/02/a-decoder-only-foundation-model-for.html&quot; rel=&quot;alternate&quot; title=&quot;用于时间序列预测的仅解码器基础模型&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1 vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s72-c/英雄。 jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-2254287928040727502&lt;/id>;&lt;发布>;2024-02-02T09:49:00.000-08:00&lt;/发布>;&lt;更新>;2024-02- 02T09:49:36.211-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML 公平性&quot; >;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;干预早期读数以缓解虚假特征和简单性偏见&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Research 博士前研究员 Rishabh Tiwari 和研究科学家 Pradeep Shenoy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S 3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 现实世界中的机器学习模型通常是根据有限的数据进行训练的，这些数据可能包含意外的&lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_(statistics)&quot;>;统计偏差&lt;/a >;。例如，在 &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CELEBA&lt;/a>; 名人图像数据集中，金发碧眼的女性名人比例不成比例，导致分类器错误地预测“金发”为大多数女性面孔的头发颜色——在这里，性别是预测头发颜色的虚假特征。这种不公平的偏见可能会在&lt;a href=&quot;https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging&quot;>;医疗诊断&lt;/a>;等关键应用中产生严重后果。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 令人惊讶的是，最近的工作还发现了深层网络通过所谓的“放大”这种统计偏差的固有趋势。深度学习的&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf&quot;>;简单性偏差&lt;/a>;。这种偏差是深度网络在训练早期识别弱预测特征的倾向，并继续锚定这些特征，而无法识别更复杂和可能更准确的特征。考虑到上述情况，我们提出了简单而有效的解决方案，通过应用&lt;em>;早期读出&lt;/em>;和&lt;em>;特征遗忘&lt;/em>;来解决虚假特征和简单性偏差的双重挑战。 。首先，在“&lt;a href=&quot;https://arxiv.org/abs/2310.18590&quot;>;使用早期读数调节蒸馏中的特征偏差&lt;/a>;”中，我们展示了从深层网络的早期层进行预测（称为“早期读数”）可以自动发出有关所学习表示的质量问题的信号。特别是，当网络依赖于虚假特征时，这些预测更经常是错误的，而且更肯定是错误的。我们利用这种错误的信心来改善&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;>;模型蒸馏&lt;/a>;的结果，在这种情况下，更大的“老师”模型指导训练较小的“学生”模型。然后在“&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;使用特征筛克服深度网络中的简单性偏差&lt;/a>;”中，我们通过让网络“忘记”来直接干预这些指标信号” 有问题的特征，从而寻找更好、更具预测性的特征。与以前的方法相比，这大大提高了模型泛化到未见过的领域的能力。我们的&lt;a href=&quot;https://ai.google/responsibility/principles&quot;>;人工智能原则&lt;/a>;和我们的&lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;负责任的人工智能实践&lt;/a>;指导我们如何研究和开发这些先进的应用程序，并帮助我们应对统计偏差带来的挑战。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oIC jeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;1080&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3h KVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;动画比较使用和不使用特征筛训练的两个模型的假设响应。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;用于去偏蒸馏的早期读数&lt;/h2>; &lt;p>;我们首先说明&lt;em>;早期读数的诊断价值&lt;/em >; 以及它们在去偏差蒸馏中的应用，即确保学生模型通过蒸馏继承教师模型对特征偏差的弹性。我们从一个标准的蒸馏框架开始，在该框架中，学生接受混合标签匹配的训练（最小化&lt;a href=&quot;https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e&quot;>;交叉熵损失&lt; /a>; 学生输出和真实标签之间）和教师匹配（最小化 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL 散度&lt;/a >; 对于任何给定的输入，学生和教师输出之间的损失）。 &lt;/p>; &lt;p>; 假设有人在学生模型的中间表示之上训练一个线性解码器，即一个名为 &lt;em>;Aux&lt;/em>; 的小型辅助神经网络。我们将该线性解码器的输出称为网络表示的早期读出。我们的发现是，早期读数在包含虚假特征的实例上会产生更多错误，而且，这些错误的置信度高于与其他错误相关的置信度。这表明对早期读数错误的置信度是模型对潜在虚假特征依赖性的相当强的自动指标。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0H cPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;796&quot; data-original-width=&quot;1128&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYk nydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;说明早期读数（即辅助层的输出）在去偏蒸馏中的用法。在早期读数中自信错误预测的实例在蒸馏损失中的权重会增加。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们使用此信号来调节教师在蒸馏损失中的贡献在每个实例的基础上，结果发现训练有素的学生模型有显着改进。 &lt;/p>; &lt;p>; 我们在已知包含虚假相关性的标准基准数据集上评估了我们的方法（&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;>;Waterbirds&lt;/a>;、&lt;a href =&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CelebA&lt;/a>;，&lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/civil_comments&quot; >;公民评论&lt;/a>;，&lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;>;MNLI&lt;/a>;）。每个数据集都包含数据分组，这些数据分组共享一个可能以虚假方式与标签相关的属性。例如，上面提到的 CelebA 数据集包括 {blondmale, blondfemale, non-blondmale,non-blondfemale} 等组，模型在预测头发颜色时通常在 {non-blondfemale} 组上表现最差。因此，模型性能的衡量标准是其&lt;em>;最差组准确度&lt;/em>;，即数据集中存在的所有已知组中准确度最低的。我们改进了所有数据集上学生模型的最差组准确度；此外，我们还提高了四个数据集中三个数据集的整体准确性，这表明我们对任何一组的改进都不会以牺牲其他组的准确性为代价。更多详细信息请参阅我们的&lt;a href=&quot;https://arxiv.org/pdf/2310.18590.pdf&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc -c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1270&quot; height=&quot;511&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5 SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;不同蒸馏技术相对于 Teacher 模型的最差组精度的比较。我们的方法在所有数据集上都优于其他方法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用特征筛克服简单性偏差&lt;/h2>; &lt;p>;在第二个密切相关的项目中，我们直接干预早期读数提供的信息，以改进&lt;a href=&quot;https://en.wikipedia.org/ wiki/Feature_learning&quot;>;功能学习&lt;/a>;和&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/generalization/video-lecture&quot;>;泛化&lt;/a>;。工作流程在&lt;em>;识别&lt;/em>;有问题的特征和&lt;em>;从网络中删除&lt;/em>;已识别的特征之间交替。我们的主要假设是，早期的特征更容易出现简单性偏差，并且通过擦除（“筛选”）这些特征，我们可以学习更丰富的特征表示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMO BIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;1098&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2coo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctAS yDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;使用特征筛训练工作流程。我们交替识别有问题的特征（使用训练迭代）和从网络中删除它们（使用遗忘迭代）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们在更多细节：&lt;/p>; &lt;ul>; &lt;li>;&lt;b>;识别简单特征&lt;/b>;：我们通过前向和反向传播以传统方式训练主要模型和读出模型（上面的 AUX）。请注意，来自辅助层的反馈不会反向传播到主网络。这是为了迫使辅助层从已有的特征中学习，而不是在主网络中创建或强化它们。 &lt;/li>;&lt;li>;&lt;b>;应用特征筛&lt;/b>;：我们的目标是使用一种新颖的&lt;em>;遗忘损失&lt;/em>;来消除神经网络早期层中已识别的特征， &lt;em>; L&lt;sub>;f &lt;/sub>;&lt;/em>;，它只是读数与标签上均匀分布之间的交叉熵。本质上，所有导致重要读数的信息都会从主网络中删除。本步骤中，辅助网络和主网络上层保持不变。 &lt;/li>; &lt;/ul>; &lt;p>; 我们可以通过少量的配置参数来具体控制特征筛选如何应用于给定的数据集。通过改变辅助网络的位置和复杂性，我们控制了识别和擦除特征的复杂性。通过修改学习和遗忘步骤的混合，我们控制模型学习更复杂特征的挑战程度。这些依赖于数据集的选择是通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;超参数搜索&lt;/a>;做出的，以最大限度地提高验证准确性，这是泛化的标准衡量标准。由于我们在搜索空间中包含“不遗忘”（即基线模型），因此我们期望找到至少与基线一样好的设置。 &lt;/p>; &lt;p>; 下面我们展示了基线模型（中行）和我们的模型（底行）在两个基准数据集上学习到的特征 - 偏差活动识别（&lt;a href=&quot;https://github.com/alinlab /BAR&quot;>;BAR&lt;/a>;）和动物分类（&lt;a href=&quot;https://arxiv.org/pdf/1906.02899v3.pdf&quot;>;NICO&lt;/a>;）。使用事后基于梯度的重要性评分（​​&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GRAD-CAM&lt;/a>;）估计特征重要性，光谱的橙红色端表示高重要性，而绿蓝色表示低重要性。如下所示，我们训练的模型侧重于主要感兴趣的对象，而基线模型往往侧重于更简单且与标签虚假相关的背景特征。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTs ljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;850&quot; data-original-width=&quot;1616&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIW taFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;使用 GRAD-CAM 在活动识别 (BAR) 和动物分类 (NICO) 泛化基准上进行特征重要性评分。我们的方法（最后一行）关注图像中的相关对象，而基线（ERM；中间行）依赖于与标签虚假相关的背景特征。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/表>; &lt;p>; 通过这种学习更好的、可概括的特征的能力，我们在现实世界的虚假特征基准数据集上的一系列相关基线上显示出了巨大的进步：&lt;a href=&quot;https://github.com/alinlab/BAR &quot;>;酒吧&lt;/a>;，&lt;a href=&quot;https://arxiv.org/pdf/2104.06885.pdf&quot;>;CelebA Hair&lt;/a>;，&lt;a href=&quot;https://nico.thumedialab.com/ &quot;>;NICO&lt;/a>; 和 &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/imagenet_a&quot;>;ImagenetA&lt;/a>;，利润率高达 11%（见下图）。更多详细信息请参阅&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;我们的论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdz mKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1082&quot; data-original-width=&quot;844&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8 JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png&quot; width=&quot;501&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的特征筛选方法相对于一系列特征泛化基准数据集的最近基线，显着提高了准确性。&lt;/td>;&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们希望我们的工作能够早日实现读数及其在泛化特征筛选中的使用将刺激新型对抗性特征学习方法的开发，并有助于提高深度学习系统的泛化能力和鲁棒性。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;将早期读数应用于去偏蒸馏的工作是与我们的学术伙伴 Durga Sivasubramanian、Anmol Reddy 和 Ganesh Ramakrishnan 教授在 &lt;a href=&quot;https://www.iitb.ac.in/&quot;>;IIT Bombay&lt;/a>; 合作进行。我们衷心感谢 Praneeth Netrapalli 和 Anshul Nasery 的反馈和建议。我们还感谢 Nishant Jain、Shreyas Havaldar、Rachit Bansal、Kartikeya Badola、Amandeep Kaur 以及 Google 印度研究院的全体博士前研究人员参与研究讨论。特别感谢 Tom Small 创建本文中使用的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2254287928040727502/comments/default&quot; rel =&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502 “ rel =“编辑”类型=“application/atom+xml”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502”rel =“self”类型=“ application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html&quot; rel=&quot;alternate&quot; title=&quot;早期干预用于减轻虚假功能和简单性偏差的读数” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ80 7gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984&lt;/id>;&lt;发布>;2024-01 -31T13:59:00.000-08:00&lt;/已发布>;&lt;更新>;2024-01-31T13:59:36.056-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MobileDiffusion：在设备上快速生成文本到图像&lt; /stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Core ML 高级软件工程师 Yang Zhao 和高级软件工程师 Tingbo Hou&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8Kq B_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 文本到图像&lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;扩散模型&lt;/a>;在根据文本提示生成高质量图像方面表现出了卓越的能力。然而，领先的模型具有数十亿个参数，因此运行成本昂贵，需要强大的桌面或服务器（例如，&lt;a href=&quot;https://stability.ai/news/stable-diffusion-public-release&quot;>;稳定扩散&lt; /a>;、&lt;a href=&quot;https://openai.com/research/dall-e&quot;>;DALL·E&lt;/a>; 和 &lt;a href=&quot;https://imagen.research.google/&quot;>;图像&lt;/a>;）。虽然最近通过 MediaPipe 在 &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;Android&lt;/a>; 上的推理解决方案取得了进展和通过 Core ML 的 &lt;a href=&quot;https://github.com/apple/ml-stable-diffusion&quot;>;iOS&lt;/a>; 已经在过去一年中实现了快速（亚秒级）文本到图像的转换移动设备上的一代仍然遥不可及。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，在“&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;MobileDiffusion：亚秒文本到-移动设备上的图像生成&lt;/a>;”，我们介绍了一种新颖的方法，具有在设备上快速生成文本到图像的潜力。 MobileDiffusion 是专为移动设备设计的高效潜在扩散模型。我们还采用 &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; 来实现推理过程中的一步采样，从而微调预训练的扩散模型，同时利用 GAN对去噪步骤进行建模。我们在 iOS 和 Android 高级设备上测试了 MobileDiffusion，它可以在半秒内运行生成 512x512 的高质量图像。其相对较小的模型尺寸（仅 520M 参数）使其特别适合移动部署。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmm YS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5 CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td 样式=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLi DbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif&quot;样式=“左边距：自动”； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZ YXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在设备上快速生成文本到图像。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;背景&lt;/h2>; &lt;p>; 文本相对效率低下-图像扩散模型面临两个主要挑战。首先，扩散模型的固有设计需要&lt;a href=&quot;https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html ”迭代去噪&lt;/a>;来生成图像，需要对模型进行多次评估。其次，文本到图像扩散模型中网络架构的复杂性涉及大量参数，通常达到数十亿，并导致因此，尽管在移动设备上部署生成模型具有潜在的好处，例如增强用户体验和解决新出现的隐私问题，但在当前的文献中仍然相对未经探索。 &lt;/p>; &lt;p>; 文本到图像扩散模型中推理效率的优化一直是一个活跃的研究领域。先前的研究主要集中于解决第一个挑战，寻求减少功能评估（NFE）的数量。利用先进的数值求解器（例如，&lt;a href=&quot;https://arxiv.org/abs/2206.00927&quot;>;DPM&lt;/a>;）或蒸馏技术（例如，&lt;a href=&quot;https://arxiv.org/ abs/2202.00512&quot;>;渐进式蒸馏&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;>;浓稠蒸馏&lt;/a>;），必要的采样步骤数量已从数百次显着减少到个位数。一些最近的技术，例如 &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2311.17042#:~:text =我们%20引入%20Adversarial%20Diffusion%20Distillation，同时%20保持%20high%20image%20quality。&quot;>;对抗性扩散蒸馏&lt;/a>;，甚至减少到单个必要步骤。 &lt;/p>; &lt;p>; 然而，在移动设备上，由于模型架构的复杂性，即使少量的评估步骤也可能很慢。到目前为止，文本到图像扩散模型的架构效率受到的关注相对较少。一些早期的作品简要地涉及了这个问题，涉及删除冗余的神经网络块（例如，&lt;a href=&quot;https://snap-research.github.io/SnapFusion/&quot;>;SnapFusion&lt;/a>;）。然而，这些工作缺乏对模型架构中每个组件的全面分析，从而无法为设计高效架构提供整体指导。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MobileDiffusion&lt;/h2>; &lt;p>; 有效克服移动设备计算能力有限带来的挑战需要对模型的架构效率进行深入、整体的探索。为了实现这一目标，我们的研究对稳定扩散的 &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;UNet 架构&lt;/a>;中的每个组成部分和计算操作进行了详细检查。我们提供了一个全面的指南，用于制作高效的文本到图像扩散模型，最终形成 MobileDiffusion。 &lt;/p>; &lt;p>; MobileDiffusion 的设计遵循&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;潜在扩散模型&lt;/a>;。它包含三个组件：文本编码器、扩散 UNet 和图像解码器。对于文本编码器，我们使用 &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP-ViT/L14&lt;/a>;，这是一个适合移动设备的小型模型（125M 参数）。然后我们将重点转向扩散 UNet 和图像解码器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;扩散UNet&lt;/h3>; &lt;p>; 如下图所示，扩散UNet通常交错变压器块和卷积块。我们对这两个基本组成部分进行了全面调查。在整个研究过程中，我们控制训练管道（例如数据、优化器）来研究不同架构的效果。 &lt;/p>; &lt;p>; 在经典的文本到图像扩散模型中，变压器块由用于建模视觉特征之间的远程依赖性的自注意力层（SA）和用于捕获的交叉注意力层（CA）组成。文本条件和视觉特征之间的相互作用，以及前馈层（FF）来对注意力层的输出进行后处理。这些转换器块在文本到图像的扩散模型中起着关键作用，是负责文本理解的主要组件。然而，考虑到注意力操作的计算费用是序列长度的二次方，它们也带来了重大的效率挑战。我们遵循&lt;a href=&quot;https://arxiv.org/abs/2301.11093&quot;>;UViT&lt;/a>;架构的思想，它将更多的变压器块放置在UNet的瓶颈处。这种设计选择的动机是，由于注意力计算的维度较低，因此在瓶颈处的资源密集程度较低。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP -LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV /s915/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;915&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3W ggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的 UNet 架构在中间包含更多变压器，并在更高分辨率下跳过自注意力 (SA) 层。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 卷积块，特别是 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; 块部署在UNet 的各个级别。虽然这些块对于特征提取和信息流很有帮助，但相关的计算成本，尤其是在高分辨率水平上，可能会很大。在这种情况下，一种经过验证的方法是&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;可分离卷积&lt;/a>;。我们观察到，在 UNet 的较深层部分中用轻量级可分离卷积层替换常规卷积层会产生相似的性能。 &lt;/p>; &lt;p>; 下图中，我们比较了几种扩散模型的UNet。我们的 MobileDiffusion 在 &lt;a href=&quot;https://arxiv.org/pdf/2110.12894.pdf&quot;>;FLOPs&lt;/a>;（浮点运算）和参数数量方面表现出卓越的效率。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7 CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1- 65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;一些扩散UNet的比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h3>;图像解码器&lt;/h3>; &lt;p>; 除了UNet之外，我们还对图像解码器进行了优化。我们训练了一个&lt;a href=&quot;https://arxiv.org/abs/2012.03715&quot;>;变分自动编码器&lt;/a>; (VAE)来编码&lt;a href=&quot;https://en.wikipedia.org/wiki/ RGB_color_model&quot;>;RGB&lt;/a>; 图像转换为 8 通道潜在变量，图像空间尺寸缩小 8 倍。潜在变量可以被解码为图像，并且大小增大 8 倍。为了进一步提高效率，我们通过修剪原始的宽度和深度来设计轻量级解码器架构。由此产生的轻量级解码器可显着提高性能，延迟降低近 50%，质量也更好。有关更多详细信息，请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12 tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;789&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0 IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;VAE重建。我们的 VAE 解码器具有比 SD（稳定扩散）更好的视觉质量。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 &quot; class=&quot;tr-caption-container&quot; style=&quot;text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;解码器&lt;/b>; &lt;/ td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;#Params (M)&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;PSNR↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt; /td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;SSIM↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;LPIPS↓&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td >; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;SD&lt;/b>; &lt;/td>; &lt;td>;49.5 &lt;/td>; &lt;td>;26.7 &lt;/td>; &lt; td>;0.76 &lt;/td>; &lt;td>;0.037 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;我们的&lt;/b>; &lt;/td>; &lt;td>; 39.3 &lt;/td>; &lt;td>;30.0 &lt;/td>; &lt;td>;0.83 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt; b>;我们的精简版&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;9.8 &lt;/td>; &lt;td>;30.2 &lt;/td>; &lt;td>;0.84 &lt;/td>; &lt;td>;0.032 &lt;/ td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left” : auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VAE解码器的质量评估。我们的精简版解码器比 SD 小得多，具有更好的质量指标，包括&lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;峰值信噪比&lt;/a>; (PSNR)、&lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_similarity&quot;>;结构相似性指数测量&lt;/a>; (SSIM) 和&lt;a href=&quot;https://arxiv.org/ abs/1801.03924&quot;>;学习感知图像块相似度&lt;/a>; (LPIPS)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;一步采样&lt;/h3>; &lt;p>;除了优化模型架构之外，我们还采用了&lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN混合&lt;/a>;实现一步采样。训练用于文本到图像生成的 DiffusionGAN 混合模型会遇到一些复杂的问题。值得注意的是，鉴别器（区分真实数据和生成数据的分类器）必须基于纹理和语义进行判断。此外，训练文本到图像模型的成本可能非常高，特别是在基于 GAN 的模型中，其中鉴别器引入了额外的参数。纯基于 GAN 的文本到图像模型（例如，&lt;a href=&quot;https://arxiv.org/abs/2301.09515&quot;>;StyleGAN-T&lt;/a>;、&lt;a href=&quot;https://arxiv. org/abs/2303.05511&quot;>;GigaGAN&lt;/a>;）面临类似的复杂性，导致训练高度复杂且昂贵。 &lt;/p>; &lt;p>; 为了克服这些挑战，我们使用预训练的扩散 UNet 来初始化生成器和鉴别器。这种设计可以使用预先训练的扩散模型进行无缝初始化。我们假设扩散模型中的内部特征包含文本和视觉数据之间复杂相互作用的丰富信息。这种初始化策略显着简化了训练。 &lt;/p>; &lt;p>; 下图说明了训练过程。初始化后，噪声图像被发送到生成器进行一步扩散。结果根据具有重建损失的地面实况进行评估，类似于扩散模型训练。然后，我们向输出添加噪声并将其发送到鉴别器，鉴别器的结果使用 GAN 损失进行评估，从而有效地采用 GAN 来建模去噪步骤。通过使用预训练的权重来初始化生成器和判别器，训练变成了一个微调过程，在不到 10K 次迭代内收敛。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yX VtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ -UILKw/s960/image7.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;960 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pj vw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DiffusionGAN 微调说明。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 下面我们展示了 MobileDiffusion 与 DiffusionGAN 一步采样生成的示例图像。凭借如此紧凑的模型（总共 5.2 亿参数），MobileDiffusion 可以为各个领域生成高质量的多样化图像。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxR xDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1728&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmX zYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;由我们的 MobileDiffusion 生成的图像&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在两个设备上测量了 MobileDiffusion 的性能iOS 和 Android 设备，使用不同的运行时优化器。下面报告了延迟数字。我们看到MobileDiffusion非常高效，可以在半秒内运行生成512x512的图像。这种闪电般的速度可能会在移动设备上实现许多有趣的用例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpe uZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1184&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQ P4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;移动设备上的延迟测量 (&lt;b>;s&lt;/b>;)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 凭借在延迟和大小方面卓越的效率，MobileDiffusion 有潜力成为对于移动部署来说，这是非常友好的选择，因为它能够在输入文本提示时实现快速图像生成体验。我们将确保该技术的任何应用都符合 Google 的&lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;负责任的 AI 实践&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢为我们提供帮助的合作者和贡献者将 MobileDiffusion 引入设备：Zhisheng Shaw、Yanwu Xu、Jiuqiang Tang、Haolin Jia、Lutz Justen、Daniel Fenner、Ronald Wotzlaw、Jianing Wei、Raman Sarokin、Juhyun Lee、Andrei Kulik、Chuo-Ling Chang 和 Matthias Grundmann。&lt; /em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5966553114967673984/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mobileiffusion-rapid-text-to-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html&quot; rel=&quot;alternate&quot; title=&quot;MobileDiffusion：在设备上快速生成文本到图像&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI 1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c /InstantTIGO%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-5144906729109253495&lt;/id>;&lt;已发布>;2024-01-26T11:56:00.000-08:00&lt;/已发布>;&lt;已更新>;2024-01-26T11:56:23.553-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“深度学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;混合输入矩阵乘法性能优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Manish Gupta ，Google 研究部高级软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLp J1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;显示: 无;” />; &lt;p>; 人工智能驱动的技术正在融入我们的日常生活，有可能增强我们获取知识的能力并提高我们的整体生产力。这些应用程序的支柱在于大型语言模型（LLM）。 LLM 需要占用大量内存，通常需要专门的硬件加速器来高效交付&lt;a href=&quot;https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on -tpu-v5e&quot;>;数十亿亿次浮点运算&lt;/a>;的计算能力。这篇博文展示了我们如何通过更有效地利用内存来开始解决计算挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; LLM 的大部分内存和计算均由 &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;>; 消耗&lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;矩阵乘法&lt;/a>;运算中的权重&lt;/a>;。使用较窄的&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Primitive_data_type&quot;>;数据类型&lt;/a>;&lt;/em>;可以减少内存消耗。例如，以 8 位&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_(computer_science)&quot;>;整数&lt;/a>;（即 U8 或 S8）数据类型存储权重会减少内存相对于&lt;a href=&quot;https://en.wikipedia.org/wiki/Single- precision_floating-point_format&quot;>;单精度&lt;/a>; (F32) 占用空间为 4 倍，相对于 &lt;a href=&quot; 占用空间为 2 倍https://en.wikipedia.org/wiki/Half- precision_floating-point_format&quot;>;半精度&lt;/a>; (F16) 或 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format &quot;>;bfloat16&lt;/a>; (BF16)。此外，&lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;之前的工作&lt;/a>;表明，LLM 模型在 S8 和 &lt;em>; 中运行带有&lt;em>;权重&lt;/em>;的矩阵乘法>;F16 中的输入（保留用户输入的较高精度）是一种提高效率的有效方法，同时在精度方面进行了可接受的权衡。这种技术被称为&lt;em>;仅权量化&lt;/em>;，并且需要有效地实现与&lt;em>;混合输入&lt;/em>;的矩阵乘法，例如，半精度输入乘以8位整数。包括 GPU 在内的硬件加速器支持一组固定的数据类型，因此混合输入矩阵乘法需要软件转换来映射到硬件运算。 &lt;/p>; &lt;p>; 为此，在本博客中，我们重点关注将混合输入矩阵乘法映射到 &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-深度/&quot;>;NVIDIA Ampere 架构&lt;/a>;。我们提出了解决数据类型转换和布局一致性的软件技术，以将混合输入矩阵乘法有效地映射到硬件支持的数据类型和布局上。我们的结果表明，软件中额外工作的开销是最小的，并且使性能接近峰值硬件能力。此处描述的软件技术已在开源 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; 存储库中发布。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXgh Ez-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm _K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;具有各种数据类型格式的 175B 参数 LLM 模型的内存占用。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;矩阵乘法累加运算&lt;/h2>; &lt;p>; 现代人工智能硬件加速器，例如&lt;a href= &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google 的 TPU&lt;/a>; 和 &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA 的 GPU&lt;/a>; 通过针对 Tensor Core 在硬件中本地乘法矩阵，Tensor Core 是用于加速矩阵运算的专用处理元素，特别是对于 AI 工作负载。在本博客中，我们重点关注 NVIDIA Ampere Tensor Core，它提供&lt;em>;矩阵乘法累加&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel- thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;）操作。在博客的其余部分中，对 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 的引用适用于 Ampere Tensor Core。 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算的两个输入矩阵（称为操作数）支持的数据类型、形状和数据布局在硬件中是固定的。这意味着通过将问题平铺到硬件支持的数据类型、形状和布局上，可以在软件中实现各种数据类型和更大形状的矩阵乘法。 &lt;/p>; &lt;p>; Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算通过指定两个输入矩阵来定义（例如，&lt;em>;A&lt; /em>; &amp;amp; &lt;em>;B&lt;/em>;，如下所示）以生成结果矩阵 &lt;em>;C&lt;/em>;。 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作本身支持混合精度。 &lt;em>;&lt;a href=&quot;https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/&quot;>;混合精度张量核心&lt;/a>;&lt;/em>;允许混合输入（&lt; em>;A&lt;/em>; 和 &lt;em>;B&lt;/em>;）数据类型与结果（&lt;em>;C&lt;/em>;）数据类型。相比之下，混合输入矩阵乘法涉及混合输入数据类型，硬件不支持，因此需要在软件中实现。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxn VdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTISZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ 68Yr5wve9uLTISZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;M×N-by-K 对 M×K 的输入矩阵 A 和 K×N 的矩阵 B 进行张量核心运算，产生输出矩阵 C M-by-N。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;混合的挑战-输入矩阵乘法&lt;/h2>; &lt;p>; 为了简化讨论，我们只讨论混合输入矩阵乘法的一个具体示例：F16 表示用户输入，U8 表示模型权重（写为 F16 * U8）。这里描述的技术适用于混合输入数据类型的各种组合。 &lt;/p>; &lt;p>; GPU 程序员可以访问&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;内存层次结构&lt;/a>;，包括全局内存、共享内存、寄存器，按照容量递减、速度递增的顺序排列。 NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算消耗寄存器中的输入矩阵。此外，输入和输出矩阵需要符合称为&lt;em>;扭曲&lt;/em>;的一组32个线程内的数据布局。对于 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作，扭曲中支持的数据类型&lt;em>;和&lt;/em>;布局是固定的，因此要实现混合-高效输入乘法，需要解决软件中数据类型转换和布局一致性的挑战。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;数据类型转换&lt;/h3>; &lt;p>; &lt;span style=&quot;color: #54863f; &quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算需要两个具有相同数据类型的输入矩阵。因此，混合输入矩阵乘法（其中一个操作数存储在全局内存中的 U8 中，其他操作数存储在 F16 中）需要从 U8 到 F16 的数据类型转换。该转换将为 F16 带来两个操作数，将&lt;em>;混合输入&lt;/em>;矩阵乘法映射到硬件支持的&lt;em>;混合精度&lt;/em>;张量核心。鉴于权重数量很大，因此存在大量此类操作，我们的技术展示了如何减少其延迟并提高性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;布局一致性&lt;/h3>; &lt;p>; &lt;span style=&quot;color: #54863f;&quot; >;&lt;code>;mma&lt;/code>;&lt;/span>; 操作还要求扭曲寄存器内的两个输入矩阵的布局符合硬件规范。混合输入矩阵乘法（F16 * U8）中U8数据类型的输入矩阵&lt;em>;B&lt;/em>;的布局需要符合转换后的F16数据类型。这称为布局一致性，需要在软件中实现。 &lt;/p>; &lt;p>; 下图显示了一个 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算，消耗了矩阵 &lt;em>;A&lt;/em>; 和矩阵 &lt; em>;B&lt;/em>; 从寄存器生成寄存器中的矩阵 &lt;em>;C&lt;/em>;，分布在一个扭曲上。线程 &lt;em>;T0&lt;/em>; 突出显示并放大，以显示权重矩阵 &lt;em>;B&lt;/em>; 经过数据类型转换，并且需要布局一致性才能映射到硬件支持的 Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-X M6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi 27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;软件中的混合输入 (F32 = F16 * U8) 操作到硬件中原生支持的扭曲级张量核心 (F32 = F16 * F16) 的映射。 （原始图来源&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;开发 CUDA 内核以将 Tensor Core 推向 NVIDIA A100 的绝对极限&lt; /a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;应对挑战的软件策略&lt;/h2>; &lt;p>; 典型的数据类型转换涉及 32 位寄存器上的一系列操作，如下所示。每个矩形块代表一个寄存器，相邻的文本是操作。整个序列显示了从 4xU8 到 2x(2xF16) 的转换。该序列大约涉及 10 个操作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3 KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q 4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; 在 32 位寄存器中从 4xU8 到 2x(2xF16)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 实现布局一致性的方法有很多。现有的两个解决方案是： &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;窄位宽共享内存加载&lt;/em>;：在这种方法中，线程发出窄位宽内存加载，将 U8 数据从共享内存移动到寄存器。这会产生&lt;em>;两个&lt;/em>; 32 位寄存器，每个寄存器包含 2xF16 值（如上所示矩阵 &lt;em>;B&lt;/em>; 的线程 &lt;em>;T0&lt;/em>;）。较窄的共享内存负载直接实现寄存器的布局一致性，而不需要任何洗牌；但是，它没有利用全部共享内存带宽。 &lt;/li>;&lt;li>;&lt;em>;全局内存中的预处理&lt;/em>;：&lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;替代策略&lt;/a>;涉及重新排列全局内存中的数据（内存层次结构中共享内存的上一级） &lt;/a>;），允许更广泛的共享内存负载。这种方法最大限度地提高了共享内存带宽利用率，并确保数据以一致的布局直接加载到寄存器中。虽然重新排列过程可以在 LLM 部署之前离线执行，确保不会影响应用程序性能，但它引入了一个额外的、重要的特定于硬件的预处理步骤，需要额外的程序来重新排列数据。 &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; 采用这种方法来有效解决布局一致性挑战。 &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;优化软件策略&lt;/h2>; &lt;p>;进一步优化并降低开销为了实现数据类型转换和布局一致性，我们实现了 &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a >;&lt;/code>; 和 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;分别。 &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; 在 32 位寄存器中的 4xU8 上运行，无需解包各个 1xU8 值。此外，它使用成本较低的算术运算，从而减少了指令数量并提高了转换速度。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/的转换顺序a>; 如下所示。这些操作使用打包的 32b 寄存器，避免显式解包和打包。 &lt;code>;FastNumericArrayConvertor&lt;/code>; 使用 &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions- prmt&quot;>;permute byte&lt;/a>;&lt;/code>; 将 4xU8 的字节重新排列到两个寄存器中。此外，&lt;code>;FastNumericArrayConvertor&lt;/code>; 不使用昂贵的整数到浮点转换指令，而是采用向量化运算来获取包含 2x(2xF16) 值的&lt;em>;两个&lt;/em>; 32 位寄存器中的打包结果。 U8 到 F16 的 &lt;code>;FastNumericArrayConvertor&lt;/code>; 大约使用六次操作，相对于上面所示的方法减少了 1.6 倍。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGU cpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1 yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; 利用 &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;置换字节&lt;/a>;&lt;/code>;和压缩算术，减少数据类型转换中的指令数量。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; 通过以允​​许使用更宽位宽的加载操作的方式混洗数据来处理布局一致性，从而提高共享内存带宽利用率并减少操作总数。 &lt;/p>; &lt;p>; NVIDIA Ampere 架构提供加载矩阵指令 (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-矩阵指令-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;）。 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; 是一个 warp 级别的操作，其中一个 warp 的 32 个线程将数据从共享内存移动到 &lt;em >;形状&lt;/em>;和&lt;em>;布局&lt;/em>;，&lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>;矩阵&lt;em>;A&lt;/em>;和&lt;em>;B&lt;/em>;消耗。使用&lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>;&lt;em>;减少&lt;/em>;加载指令的数量并&lt;em>;增加&lt;/em>;内存带宽利用率。由于 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; 指令将 U8 数据移至寄存器，因此加载后的布局符合 U8*U8 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作，而不是 F16*F16 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作。我们实现了 FragmentShuffler，以使用 shuffle 重新排列寄存器内的数据（&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html #warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; 操作以实现布局一致性。 &lt;/p>;&lt;p>; 这项工作最重要的贡献是通过寄存器洗牌实现布局一致性，避免全局内存中的离线预处理或较窄位宽的共享内存负载。此外，我们还提供了 &lt;code>;FastNumericArrayConvertor&lt;/code>; 的实现，涵盖了 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot; 的数据类型转换>;U8 到 F16&lt;/a>;、&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8 到 F16&lt;/ a>;、&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8 到 BF16&lt;/a>; 和 &lt;a href= “https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8 到 BF16&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;性能结果&lt;/h2>; &lt;p>; 我们测量了 &lt; 的八种混合输入变体的性能em>;我们的方法&lt;/em>;（如下图蓝色和红色所示；改变矩阵&lt;em>;A&lt;/em>;和&lt;em>;B&lt;/em>;的数据类型）和两个&lt;em>;混合精度&lt;/em>; em>; NVIDIA A100 SXM 芯片上的数据类型（以绿色显示）。性能结果以 &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; 为单位（越高越好）。值得注意的是，前八个矩阵乘法相对于后两个矩阵乘法需要额外的操作，因为混合精度变体直接针对硬件加速的 Tensor Core 操作，不需要数据类型转换和布局一致性。即便如此，我们的方法证明了混合输入矩阵乘法性能仅略低于或与混合精度相当。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4 fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6 IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;NVIDIA A100 40GB SMX4 芯片上计算受限矩阵问题形状的混合输入矩阵乘法性能&lt;code>;m=3456, n=4096, k=2048。&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p >; &lt;em>;我们想提及几位通过技术头脑风暴和改进博客文章做出贡献的人，包括 Quentin Colombet、Jacques Pienaar、Allie Culp、Calin Cascaval、Ashish Gondimalla、Matt Walsh、Marek Kolodziej 和 Aman Bhatia。我们要感谢 NVIDIA 合作伙伴 Rawn Henry、Pradeep Ramani、Vijay Thakkar、Hai Cheng Wu、Andrew Kerr、Matthew Nicely 和 Vartika Singh。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:// /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;混合输入矩阵乘法性能优化&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG 2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png”宽度=“72”xmlns：媒体=“http://search.yahoo.com/mrss/” >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;已发布>;2024-01-23T14:27:00.000-08:00&lt;/已发布>;&lt;更新>;2024-01-23T14:27:09.785-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer：图结构数据的缩放转换器&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Ameya Velingker，Google 研究院研究科学家Balaji Venkatachalam，Google 软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRn N8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot;样式=“显示：无；” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;图&lt;/a>;，其中对象及其关系表示为节点（或顶点）和边节点对之间的连接（或链接）在计算和机器学习 (ML) 中无处不在。例如，社交网络、道路网络以及分子结构和相互作用都是底层数据集具有自然图结构的领域。机器学习可用于学习节点、边或整个图的属性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 图学习的常见方法是&lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;图神经网络网络（GNN），通过对节点、边缘和全局属性应用可优化的转换来对图数据进行操作。最典型的 GNN 类别通过 &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2 进行操作&quot;>;消息传递框架，其中每一层将节点的表示与其直接邻居的表示聚合。 &lt;/p>; &lt;p>; 最近，&lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;图转换器模型&lt;/a>;已成为消息传递 GNN 的流行替代方案。这些模型建立在自然语言处理 (NLP) 领域&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer 架构&lt;/a>;的成功之上，使它们适应图形- 结构化数据。图转换器中的注意力机制可以通过交互图来建模，其中边代表相互关注的节点对。与消息传递架构不同，图转换器具有与输入图分离的交互图。典型的交互图是一个完整的图，这意味着一个完整的注意力机制，可以模拟所有节点对之间的直接交互。&lt;em>; &lt;/em>;然而，这会产生二次计算和内存瓶颈，限制了图转换器对最多具有数千个节点的小图上的数据集的适用性。使图转换器可扩展被认为是该领域最重要的研究方向之一（参见&lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot; >;这里是第一个开放问题&lt;/a>;）。 &lt;/p>; &lt;p>; 一种自然的补救措施是使用边数较少的&lt;em>;稀疏&lt;/em>;交互图。 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;人们提出了许多稀疏且高效的变换器&lt;/a>;来消除序列的二次瓶颈，但是，它们通常不会扩展到有原则地绘制图表。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer：图的稀疏变换器&lt;/a>;”中，介绍于 &lt;a href=&quot;https:// icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;，我们通过引入专为图数据设计的变压器稀疏注意力框架来解决可扩展性挑战。 Exphormer 框架利用扩展图，这是&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;谱图理论&lt;/a>;的强大工具，并且能够在以下方面取得强有力的实证结果：各种各样的数据集。我们的 Exphormer 实现现已在 &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>; 上提供。 &lt;/p>; &lt;br />; &lt;h2>;扩展图&lt;/h2>; &lt;p>; Exphormer 核心的一个关键思想是使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph &quot;>;扩展图&lt;/a>;，它们是稀疏但连接良好的图，具有一些有用的属性 - 1）图的矩阵表示具有与完整图类似的线性代数属性，2）它们表现出快速混合随机游走，即从任何起始节点开始的随机游走中的少量步骤足以确保收敛到图的节点上的“稳定”分布。扩展器已应用于不同领域，例如算法、伪随机性、复杂性理论和纠错码。 &lt;/p>; &lt;p>; 扩展器图的常见类别是 &lt;em>;d&lt;/em>; 正则扩展器，其中每个节点都有 &lt;em>;d&lt;/em>; 条边（即每个节点的度数 &lt; em>;d&lt;/em>;）。扩展图的质量是通过其&lt;em>;谱间隙&lt;/em>;来衡量的，这是其&lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;>;邻接矩阵&lt;/a的代数属性>;（图形的矩阵表示，其中行和列由节点索引，条目指示节点对是否由边连接）。那些最大化光谱间隙的被称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Ramanujan_graph&quot;>;拉马努扬图&lt;/a>; - 它们实现了&lt;em>;d&lt;/em>;的间隙 - 2*√(&lt;em>;d&lt;/em>;-1)，这本质上是&lt;em>;d&lt;/em>;正则图中最好的。多年来，针对各种 &lt;em>;d&lt;/em>; 值，人们提出了拉马努金图的许多确定性和随机构造。我们使用弗里德曼的随机扩展器构造，它生成近拉马努金图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML 9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;800&quot; height=&quot;320&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1P pnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968&quot;>;&lt;spanface=&quot;Arial, sans-衬线&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse:preserve;&quot;>;扩展图是 Exphormer 的核心。一个好的扩展器是稀疏的，但表现出随机游走的快速混合，使其全局连接适合图转换器模型中的交互图。&lt;/span >;&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;Exphormer 用稀疏 &lt;em>;d&lt;/em 的边缘替换标准 Transformer 的密集、全连接交互图>;-正则扩展图。直观上，扩展图的谱近似和混合属性允许在图转换器架构中堆叠多个关注层后，远处的节点能够相互通信，即使节点可能不直接相互关注。此外，通过确保&lt;em>;d&lt;/em>;恒定（与节点数量的大小无关），我们在生成的交互图中获得了线性数量的边。&lt;/p>; &lt;br />; &lt;h2 >;Exphormer：构建稀疏交互图&lt;/h2>; &lt;p>; Exphormer 将扩展器边与输入图和虚拟节点结合起来。更具体地说，Exphormer 的稀疏注意力机制构建了一个由三种类型的边组成的交互图：&lt;/p>; &lt;ul>; &lt;li>;来自输入图的边（&lt;em>;局部注意力&lt;/em>;）&lt;/li>; &lt;li>;来自恒定度扩展图的边（&lt;em>;扩展注意力&lt;/em>;）&lt;/li>;&lt;li>;从每个节点到一小组虚拟节点的边（&lt;em>;全局注意力&lt;/em>; ) &lt;/li>; &lt;/ul>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right:auto; &quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi 1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq- hDAYX8/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzC eytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3&quot;>;&lt;spanface=&quot;Arial , 无衬线&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse:preserve;&quot;>;Exphormer 通过组合三种类型的边来构建交互图。生成的图具有良好的连通性，并保留输入数据集图的归纳偏差，同时仍然保持稀疏。&lt;/span>;&lt;/ span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 每个组件都有特定的用途：输入图的边保留输入图结构的归纳偏差（通常会在全连接注意力模块）。同时，扩展器边缘允许良好的全局连接和随机游走混合属性（在光谱上近似于具有更少边缘的完整图）。最后，虚拟节点充当全局“内存接收器”，可以直接与每个节点通信节点。虽然这会导致每个虚拟节点的附加边等于输入图中的节点数，但生成的图仍然稀疏。扩展器图的度和虚拟节点的数量是用于调整以提高质量的超参数指标。 &lt;/p>; &lt;p>; 此外，由于我们使用恒定度的扩展图和少量恒定数量的虚拟节点来进行全局注意力，因此得到的稀疏注意力机制与原始输入图的大小是线性的，即根据节点和边总数的顺序对许多直接交互进行建模。 &lt;/p>; &lt;p>; 我们还表明，Exphormer 与密集变压器一样具有表现力，并且遵循通用逼近属性。特别是，当 Exphormer 的稀疏注意力图通过自环（将节点连接到自身的边）进行增强时，它可以普遍逼近连续函数 [&lt;a href=&quot;https://arxiv.org/abs/1912.10077&quot;>;1 &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>;2&lt;/a>;]。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;与序列的稀疏 Transformer 的关系&lt;/h3>; &lt;p>; 将 Exphormer 与稀疏进行比较很有趣序列的注意方法。也许在概念上与我们的方法最相似的架构是 &lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>;BigBird&lt;/a>;，它构建了一个交互通过组合不同的组件来绘制图形。 BigBird 也使用虚拟节点，但与 Exphormer 不同，它使用来自 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3 的窗口注意力和随机注意力%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>; 其余组件的随机图模型。 &lt;/p>; &lt;p>; BigBird 中的窗口注意力着眼于序列中标记周围的标记——Exphormer 中的局部邻域注意力可以被视为窗口注意力对图的概括。 &lt;/p>; &lt;p>; &lt;em>;n&lt;/em>; 个节点上的 Erdős-Rényi 图，&lt;em>;G(n, p)&lt;/em>;，以概率 &lt;em>;p 独立连接每对节点&lt;/em>;，还可以用作适当高&lt;em>;p&lt;/em>;的扩展图。然而，需要超线性边数 (Ω(&lt;em>;n&lt;/em>; log &lt;em>;n&lt;/em>;)) 来确保 Erdős-Rényi 图是连通的，更不用说良好的扩展器了。另一方面，Exphormer 中使用的扩展器只有&lt;em>;线性&lt;/em>;数量的边。 &lt;/p>; &lt;br />; &lt;h2>;实验结果&lt;/h2>; &lt;p>; 早期的工作已经展示了在图大小高达 5,000 个节点的数据集上使用基于全图 Transformer 的模型。为了评估 Exphormer 的性能，我们基于著名的 &lt;a href=&quot;https://github.com/rampasek/GraphGPS&quot;>;GraphGPS 框架&lt;/a>; [&lt;a href=&quot;https://arxiv.org/ abs/2205.12454&quot;>;3&lt;/a>;]，它结合了消息传递和图形转换器，并在许多数据集上实现了最先进的性能。我们证明，用 Exphormer 代替 GraphGPS 框架中的图注意力组件的密集注意力可以实现具有可比或更好性能的模型，并且通常具有更少的可训练参数。 &lt;/p>; &lt;p>; 此外，Exphormer 还特别允许图形转换器架构的扩展远远超出上述通常的图形大小限制。 Exphormer 可以扩展到包含 10,000 多个节点图的数据集，例如 &lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>;Coauthor 数据集&lt;/a>;，甚至可以扩展到更大的图，例如井-已知&lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>;ogbn-arxiv数据集&lt;/a>;，一个引文网络，由17万个节点和110万条边组成。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT 8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance .png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original-高度=“190”数据原始宽度=“1522”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0 aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在五个 &lt;a href=&quot; 上比较 Exphormer 与标准 GraphGPS 的结果https://arxiv.org/abs/2206.08164&quot;>;长期图基准&lt;/a>;数据集。我们注意到，在论文发表时，Exphormer 在五个数据集（PascalVOC-SP、COCO-SP、Peptides-Struct、PCQM-Contact）中的四个上取得了最先进的结果。&lt;/td>;&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin -right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwO z0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B -suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot; 0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jV qpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在五个&lt;a href 上比较 Exphormer 与标准 GraphGPS 的结果=&quot;https://arxiv.org/abs/2206.08164&quot;>;长期图基准&lt;/a>;数据集。我们注意到，截至发布时，Exphormer 在五个数据集（PascalVOC-SP、COCO-SP、Peptides-Struct、PCQM-Contact）中的四个上取得了最先进的结果。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;tdalign=&quot;left&quot;>;&lt;strong>;型号&lt;/strong>; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;PascalVOC- SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 得分 &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;&amp;nbsp;COCO-SP&amp;nbsp;&lt;/strong>;&lt;br>;&amp;nbsp;&lt;font size=&quot;-1&quot;>;F1分数&lt;/font>;&lt;strong>;↑&lt; /strong>;&amp;nbsp; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;肽功能&lt;/strong>;&lt;br>;&lt;font size=&quot;-1&quot;>;AP &lt;/font>;&lt;strong>;↑&lt;/强>;&amp;nbsp; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;&amp;nbsp;肽结构&lt;/strong>;&lt;br>;&amp;nbsp;&lt;font size=&quot;-1&quot;>;MAE&lt;/font>;&lt;strong>;↓&lt;/强>;&amp;nbsp; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;PCQM-联系方式&lt;/strong>;&lt;br>;&amp;nbsp;&lt;font size=&quot;-1&quot;>;MRR&lt;/font>;&lt;strong>;↑&lt;/strong >; &lt;/td>; &lt;/tr>; &lt;tr>;&lt;td colspan=&quot;6&quot;>;&lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />;&lt;/div>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;标准图形GPS &lt;/td>; &lt;tdalign=&quot;center&quot;>;0.375 ± 0.011 &lt;/td>; &lt;tdalign=&quot;center&quot;>;0.341 ± 0.004 &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;0.654 ± 0.004&lt;/strong>; &amp;nbsp; &lt;/td>; &lt;tdalign=&quot;center&quot;>;0.250 ± 0.001 &lt;/td>; &lt;tdalign=&quot;center&quot;>;&amp;nbsp;0.334 ± 0.001&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Exphormer（我们的）&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;0.398 ± 0.004&lt;/em>;&lt;/strong>; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;&lt;em>;0.346 ± 0.001 &lt;/em>;&lt;/strong>; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;em>;0.653 ± 0.004&lt;/em>; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong >;&lt;em>;&amp;nbsp;0.248 ± 0.001&lt;/em>;&lt;/strong>; &lt;/td>; &lt;tdalign=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.364 ± 0.002&lt;/em>;&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>;-->; &lt;p>; 最后，我们观察到Exphormer通过扩展器创建小直径的覆盖图，表现出有效学习远程依赖关系的能力。 &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;远程图基准&lt;/a>; 是一套由五个图学习数据集组成的套件，旨在衡量模型捕获远程交互的能力。结果表明，基于 Exphormer 的模型优于标准 GraphGPS 模型（在发布时，该模型在五分之四的数据集上是最先进的）。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 图转换器已成为 ML 的重要架构，它将 NLP 中使用的非常成功的基于序列的转换器适应图结构数据。然而，可扩展性已被证明是在具有大型图的数据集上使用图转换器的主要挑战。在这篇文章中，我们介绍了 Exphormer，一个稀疏注意力框架，它使用扩展图来提高图转换器的可扩展性。 Exphormer 被证明具有重要的理论特性，并表现出强大的经验性能，特别是在学习长期依赖性至关重要的数据集上。如需了解更多信息，我们建议读者观看 ICML 2023 的简短演示&lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;视频&lt;/a>;。&lt;/p>; &lt;br / >; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢不列颠哥伦比亚大学的研究合作者 Hamed Shirzad 和 Danica J. Sutherland 以及 Google Research 的 Ali Kemal Sinop。特别感谢 Tom Small 创建本文中使用的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1418736582601940076/comments/default&quot; rel =&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html# comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel =&quot;编辑&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;self&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html&quot; rel=&quot;alternate&quot; title=&quot;Exphormer：图的缩放转换器结构化数据” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger .com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16 -rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd -7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/ s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-220849549986709693&lt;/id>;&lt;已发布>;2024-01-18T10:03 ：00.000-08:00&lt;/已发布>;&lt;已更新>;2024-01-18T10:03:43.608-08:00&lt;/已更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;引入 ASPIRE 在法学硕士中进行选择性预测&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：Jiefeng Chen（学生研究员）和 Jinsung Yoon（云 AI 团队研究科学家）&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7Nq ezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_- o4sPK8wYe/s320/ASPIRE%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在快速发展的人工智能领域，大型语言模型 (LLM) 彻底改变了我们与机器交互的方式，将自然语言理解和生成的界限推向了前所未有的高度。然而，进入高风险决策应用仍然是一个巨大的鸿沟，这主要是由于模型预测固有的不确定性。传统的法学硕士递归地生成响应，但它们缺乏为这些响应分配置信度分数的内在机制。尽管可以通过总结序列中各个标记的概率来得出置信度分数，但传统方法通常无法可靠地区分正确答案和错误答案。但是，如果法学硕士能够衡量自己的信心并仅在有把握时才做出预测，结果会怎样呢？ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2017/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html&quot;>;选择性预测旨在通过使法学硕士能够输出答案和选择分数来实现这一点，选择分数表明答案正确的概率。通过选择性预测，人们可以更好地了解在各种应用中部署的法学硕士的可靠性。先前的研究，例如&lt;a href=&quot;https://openreview.net/pdf?id=VD-AYtP0dve&quot;>;语义不确定性&lt;/a>;和&lt;a href=&quot;https://arxiv.org/pdf/2207.05221 .pdf&quot;>;自我评估&lt;/a>;，试图在法学硕士中实现选择性预测。典型的方法是使用启发式提示，例如“建议的答案是对还是错？”触发法学硕士的自我评估。但是，这种方法可能不适用于具有挑战性的&lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;问答&lt;/a>; (QA) 任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC 7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s1999/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1534&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlf gERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;OPT-2.7B&lt;/a>; 模型错误地回答了来自&lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;>;TriviaQA&lt;/a>; 数据集：“哪种维生素有助于调节血液凝固？”与“维生素C”。如果没有选择性预测，法学硕士可能会输出错误的答案，在这种情况下，可能会导致用户服用错误的维生素。通过选择性预测，法学硕士将输出答案以及选择分数。如果选择分数较低（0.1），LLM会进一步输出“我不知道！”警告用户不要信任它或使用其他来源验证它。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 在“&lt;a href=&quot;https://aclanthology. org/2023.findings-emnlp.345.pdf&quot;>;适应自我评估以提高法学硕士的选择性预测&lt;/a>;”，发表于 &lt;a href=&quot;https://2023.emnlp.org/program/accepted_findings /&quot;>;EMNLP 2023的研究结果&lt;/a>;，我们引入了ASPIRE——一个精心设计的新颖框架，旨在增强法学硕士的选择性预测能力。 ASPIRE 通过参数高效的微调对 QA 任务上的法学硕士进行微调，并训练他们评估生成的答案是否正确。 ASPIRE 允许法学硕士输出答案以及该答案的置信度分数。我们的实验结果表明，ASPIRE 在各种 QA 数据集上的性能显着优于最先进的选择性预测方法，例如 CoQA 基准&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ASPIRE 的机制&lt;/h2>; &lt;p>; 想象一下，教授法学硕士不仅要回答问题，还要还要评估这些答案——类似于学生在课本后面验证他们的答案。这就是 ASPIRE 的本质，它涉及三个阶段：(1) 针对特定任务的调优，(2) 答案采样，(3) 自我评估学习。 &lt;/p>; &lt;p>; &lt;strong>;特定于任务的调整&lt;/strong>;：ASPIRE 执行特定于任务的调整以训练适应性参数 (θ&lt;sub>;p&lt;/sub>;)，同时冻结 LLM。给定生成任务的训练数据集，它会对预训练的 LLM 进行微调以提高其预测性能。为此，参数高效的调整技术（例如，&lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;软提示调整&lt;/a>;和&lt;a href=&quot;https: //openreview.net/forum?id=nZeVKeeFYf9&quot;>;LoRA&lt;/a>;）可用于调整任务上的预训练 LLM，因为它们可以有效地利用少量目标任务数据获得强泛化能力。具体来说，LLM 参数 (θ) 被冻结，并添加自适应参数 (θ&lt;sub>;p&lt;/sub>;) 进行微调。仅更新 θ&lt;sub>;p&lt;/sub>; 以最小化标准 LLM 训练损失（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_minimization&quot;>;交叉-熵&lt;/a>;）。这种微调可以提高选择性预测性能，因为它不仅提高了预测精度，而且还提高了正确输出序列的可能性。 &lt;/p>; &lt;p>; &lt;strong>;答案采样&lt;/strong>;：在特定于任务的调整之后，ASPIRE 使用 LLM 和学习到的 θ&lt;sub>;p&lt;/sub>; 为每个训练问题生成不同的答案，并创建一个用于自我评估学习的数据集。我们的目标是生成具有高可能性的输出序列。我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;>;束搜索&lt;/a>;作为解码算法来生成高似然输出序列和&lt;a href=&quot;https:// aclanthology.org/P04-1077/&quot;>;Rouge-L&lt;/a>; 度量来确定生成的输出序列是否正确。 &lt;/p>; &lt;p>; &lt;strong>;自评估学习&lt;/strong>;：在对每个查询的高似然输出进行采样后，ASPIRE 添加自适应参数 (θ&lt;sub>;s&lt;/sub>;)，并且仅微调 θ &lt;sub>;s&lt;/sub>; 用于学习自我评估。由于输出序列的生成仅取决于θ和θ&lt;sub>;p&lt;/sub>;，因此冻结θ和学习到的θ&lt;sub>;p&lt;/sub>;可以避免在学习自我评估时改变LLM的预测行为。我们优化了 θ&lt;sub>;s&lt;/sub>;，使得适应后的 LLM 可以自己区分正确和错误的答案。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4 z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;933&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6 LG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ASPIRE 框架的三个阶段。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 在所提出的框架中，可以训练 θ&lt;sub>;p&lt;/sub>; 和 θ&lt;sub>;s&lt;/sub>;使用任何参数有效的调整方法。在这项工作中，我们使用&lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;软提示调整&lt;/a>;，这是一种简单而有效的学习“&lt;a href=&quot; https://blog.research.google/2022/02/guiding-frozen-language-models-with.html&quot;>;软提示&lt;/a>;”来调节冻结语言模型，以比传统离散文本更有效地执行特定的下游任务提示。这种方法背后的驱动力在于认识到，如果我们能够开发出有效激发自我评价的提示，那么应该可以通过结合有针对性的培训目标的软提示调整来发现这些提示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTERsGxKSWWM8 lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTswrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s800/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;800&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTERsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_u CY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过软提示调整实现ASPIRE框架。我们首先使用第一个软提示生成问题的答案，然后使用第二个软提示计算学习到的自我评估分数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p >; 训练 θ&lt;sub>;p&lt;/sub>; 和 θ&lt;sub>;s&lt;/sub>; 后，我们通过波束搜索解码获得查询的预测。然后，我们定义一个选择分数，将生成的答案的可能性与学习的自我评估分数（即，预测对于查询正确的可能性）相结合，以做出选择性预测。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 为了证明 ASPIRE 的功效，我们通过三个问答来评估它数据集 - &lt;a href=&quot;https://aclanthology.org/Q19-1016/&quot;>;CoQA&lt;/a>;、&lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;>;TriviaQA&lt;/a>; >; 和 &lt;a href=&quot;https://aclanthology.org/D16-1264/&quot;>;SQuAD&lt;/a>; — 使用各种&lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;开放预-训练有素的变压器（OPT）模型。通过使用软提示调整训练 θ&lt;sub>;p&lt;/sub>;，我们观察到法学硕士的准确性大幅提高。例如，采用 ASPIRE 的 &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>;OPT-2.7B&lt;/a>; 模型表现出比使用CoQA 和 SQuAD 数据集。这些结果表明，通过适当的调整，较小的法学硕士可能有能力在某些情况下匹配或可能超越较大模型的准确性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcus D7O1FQvyVe9U9C5iCbX-us8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz -Ei4BrrW-e9Wzc/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1500&quot; data-original-width= “1999” src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe 9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz-Ei4BrrW-e9Wzc/s16000/image4.png&quot;/>;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在深入研究固定模型预测的选择分数计算时，ASPIRE 获得了更高的&lt;a href=&quot;https://openreview.net /pdf?id=VD-AYtP0dve&quot;>;AUROC&lt;/a>; 分数（随机选择的正确输出序列比随机选择的错误输出序列具有更高选择分数的概率）高于所有数据集的基线方法。例如，在 CoQA 基准上，与基线相比，ASPIRE 将 AUROC 从 51.3% 提高到 80.3%。 &lt;/p>; &lt;p>; TriviaQA 数据集评估中出现了一个有趣的模式。虽然预训练的 OPT-30B 模型表现出更高的基线精度，但与传统的自我评估方法相比，其选择性预测的性能并没有显着提高 - &lt;a href=&quot;https://arxiv.org/abs/2207.05221&quot;>;自我评估eval 和 P(True)&lt;/a>; — 已应用。相比之下，较小的 OPT-2.7B 模型在使用 ASPIRE 进行增强后，在这方面表现优于其他模型。这种差异强调了一个重要的见解：利用传统自我评估技术的较大法学硕士在选择性预测方面可能不如较小的 ASPIRE 增强模型有效。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKd zZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s1999/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1599&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9 hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们与 ASPIRE 的实验之旅强调了法学硕士格局的关键转变：语言模型并不是其性能的全部和最终目的。相反，模型的有效性可以通过策略调整来大幅提高，即使在较小的模型中也可以进行更精确、更自信的预测。因此，ASPIRE 证明了法学硕士的潜力，他们可以明智地确定自己的确定性，并在选择性预测任务中果断地超越更大的同行。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 总之，ASPIRE 不仅仅是另一个框架；它是一个框架。这是一个未来的愿景，法学硕士可以成为决策中值得信赖的合作伙伴。通过磨练选择性预测性能，我们距离充分发挥人工智能在关键应用中的潜力又近了一步。 &lt;/p>; &lt;p>; 我们的研究打开了新的大门，我们邀请社区在此基础上继续发展。我们很高兴看到 ASPIRE 将如何激励下一代法学硕士及其他人。要了解更多关于我们的发现，我们鼓励您阅读我们的论文，并加入我们这个激动人心的旅程，以创建更可靠和具有自我意识的人工智能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们衷心感谢 Sercan Sayna Ebrahimi 的贡献O Arik、Tomas Pfister 和 Somesh Jha。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/220849549986709693/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html#comment-form&quot; rel=&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;编辑&quot;类型=“application/atom+xml”/>;&lt;link href=“http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693”rel=“self”类型=“application/atom+xml” />;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html&quot; rel=&quot;alternate&quot; title=&quot;引入 ASPIRE 在法学硕士中进行选择性预测&quot; type=&quot; text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =” https://img1.blog1.blogblog.com/img/b16-round.gif =&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_ -o4spk8wye/s72-c/aspire％20HERO.JPG“ width =“ 72” XMLNS：MEDIA =“ http：//search.yahoo.com/mrss/”>; &lt;/Media：thumbnail>; &lt;/thumbnail>; &lt;thr>; &lt;thr>; &lt;thr>; &lt;thr>; thr>; 0 &lt;：thr>; 0 &lt;：thr>; 0 &lt;：thr>; 0 &lt;： /thr：total>; &lt;/entry>; &lt;&lt;id>; &lt;id>; tag：blogger.com，1999年：blog-8474926331452026626.post-1800430129205268706 &lt;post-129205268706&lt;/id>; &lt;/publined>; &lt;更新>; 2024-01-16T10：55：15.626-08：00 &lt;/updated>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ term =” >; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ health”>; &lt;/category>; &lt;类别>; &lt;category scheme =“ http://www.blogger.com/ atom/ns＃“ term =“大语言模型”>; &lt;/category>; &lt;title type =“ text”>; amie：用于诊断医学推理和对话的研究AI系统&lt;/stitle>; &lt;content type type =“ html”>; &lt; span class=&quot;byline-author&quot;>;Posted by Alan Karthikesalingam and Vivek Natarajan, Research Leads, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c /S16000/amie.gif“ style =” display：none;” />; &lt;p>;医师患者的对话是医学的基石，在这种对话中，熟练和故意的沟通驱动诊断，管理，同理心和信任。能够进行此类诊断对话的AI系统可以通过成为临床医生和患者的有用的对话伙伴，可以提高可用性，可访问性，质量和一致性。但是，近似临床医生的大量专业知识是一个重大挑战。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;在医学领域之外的大语言模型（LLMS）中的最新进展表明，他们可以计划，原因和使用相关上下文来进行丰富的对话。但是，良好的诊断对话是医疗领域独有的许多方面。有效的临床医生进行了完整的“临床病史”，并提出了有助于得出鉴别诊断的明智问题。他们具有相当大的技巧来建立有效的关系，清楚地提供信息，与患者做出联合和明智的决定，对自己的情绪做出同情的反应，并在下一步的护理步骤中为他们提供支持。尽管LLM可以准确执行医疗摘要或回答医学问题等任务，但很少有专门针对开发此类对话诊断功能的工作。 &lt;/p>; &lt;p>;受到这一挑战的启发，我们开发了&lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot;>;表达基于基于研究AI系统的医学智能探索器（AMIE） LLM并优化用于诊断推理和对话。我们从临床医生和患者的角度培训和评估了AMIE的许多维度，这些维度反映了现实世界中临床咨询的质量。为了在多种疾病，特色疾病和场景中扩展AMIE，我们开发了一种新型的基于自我播放的诊断对话环境，具有自动反馈机制，以丰富和加速其学习过程。我们还引入了推理时间链策略，以提高AMIE的诊断准确性和对话质量。最后，我们通过模拟与训练有素的演员进行磋商，在多转对话的真实实例中测试了艾米。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s1200/AMIE%20GIF%201%20v2.gif “ style =”边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-foriginal-height =“ 512” data-eriginal-width =“ 1200” 1200“ src =” https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s16000/AMIE%20GIF%201%20v2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“ style =”文本align：中心;>;艾米（Amie关系，并清楚地提供信息。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>;评估对话诊断AI &lt;/h2>; &lt;p>;除了开发和优化AI系统本身之外对于诊断对话，如何评估此类系统也是一个悬而未决的问题。灵感来自用于衡量现实世界中咨询质量和临床沟通技巧的公认工具，我们构建了一个试点评估标题，以评估与历史记录，诊断准确性，临床沟通技巧，临床沟通技巧，关系培养和关系培养和关系培养和关系的诊断对话共情。 &lt;/p>; &lt;p>;然后，我们设计了一项随机，双盲的跨界研究，该研究与经过验证的患者参与者与经过董事会认证的初级保健医生（PCPS）相互作用的经过验证的咨询，或优化用于诊断对话的AI系统。我们以&lt;a href=&quot;https://en.wikipedia.org/wiki/objective_clentive_clinical_examination&quot;>;客观结构化的临床检查&lt;/a>;（OSCE）（OSCE），实际上常用于现实的实际评估，以标准化和客观的方式研究临床医生的技能和能力的世界。在典型的OSCE中，临床医生可能会在多个站点旋转，每个站点模拟了现实生活中的临床场景，他们执行任务，例如与标准化的患者参与者进行咨询（经过培训以仔细训练以模仿具有特定状况的患者）。使用同步的文本聊天工具进行咨询，模仿当今大多数消费者熟悉的界面。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercentent.com/img/img/r29vz2xl/avvxssejbtzbtzb4nx-dnlx-dnlllllrrr quim1g7 CHCZMVY7TUHYETCLVRU8HV3J9GQRD_WGYWORLIYLIYLKNVIILVFO068DAETL3MZJAA2RGU7YZJG2BKFUAS0HQGP9YHQGP9YBP9YBP9YBP9YBP9YBH_WMH_WX3WX3WMCPGFUZ-GFUZ-GFUZ-GFUZ-GR/SC1350 =“边距 - 左：自动;边缘权利：自动;”>; &lt;img border =“ 0” data-Original-height =“ 1200” data-Original-width =“ 1350”高度=“ 568” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/w640-h568/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; amie是一个基于LLMS的研究AI系统，用于诊断推理和对话。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/表>; &lt;br />; &lt;h2>; AMIE：基于LLM的对话诊断研究AI系统&lt; /h2>; &lt;p>;我们在包括医学推理，医学摘要和现实世界中临床对话的现实世界数据集中培训了AMIE。 &lt;/p>; &lt;p>;使用被动收集和转录面对面的临床访问开发的现实世界对话训练LLM是可行的，但是，两个实质性的挑战限制了他们在培训LLMS医疗对话中的有效性。首先，现有的现实数据通常无法捕获各种医疗状况和方案，从而阻碍了可扩展性和全面性。其次，从现实世界对话成绩单中得出的数据往往嘈杂，包含模棱两可的语言（包括语，术语，幽默和讽刺），中断，不语法的话语和隐式引用。 &lt;/p>; &lt;p>;要解决这些限制，我们设计了一个基于自我播放的模拟学习环境，其自动反馈机制在虚拟护理环境中用于诊断医学对话，使我们能够在许多医疗条件和环境中扩展AMIE的知识和能力。除了描述的真实世界数据的静态语料库外，我们还将这种环境与迭代的微调AMIE一起使用。 &lt;/p>; &lt;p>;此过程由两个自我播放循环组成：（1）“内部”自我播放循环，其中Amie利用艾米（Amie）在上下文批评的反馈中以优化其在与AI患者模拟器的模拟对话中的行为； （2）“外部”自我播放循环，其中将一组精制的模拟对话集合到了随后的微调迭代中。然后，由此产生的新版本AMIE可以再次参与内部循环，从而创建一个道德的连续学习周期。 &lt;/p>; &lt;p>;此外，我们还采用了推理时间链策略，使艾米能够逐步完善其在当前对话中的回应，以得出知情和扎根的答复。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s1400/image1.gif “ style =”边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-Original-height =“ 800” data-Original-width =“ 1400” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“ style =”文本align：中心;“>; amie使用新型基于自我的模拟对话学习环境来提高跨多种疾病，特色疾病和患者环境的诊断对话质量。&lt;/td>; &lt;/td>; &lt;/ tr>; &lt;/tbody>; &lt;/table>; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/ s1834/image4.png“ style =”边距 - 左：auto;边缘权利：自动;”>; &lt;img border =“ 0” data-Original-height =“ 1284” data-eriginal-width =“ 1834” height =“ 448” src =“ https：//blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “文本主修：中心；”>;艾米使用新型基于自我的模拟对话学习环境来提高各种疾病，专业和患者环境的诊断对话质量。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/ tbody>; &lt; /table>;  - >; &lt;br />; &lt;p>;我们在与模拟患者（由训练有素的演员扮演）的咨询中测试了性能，与20个真实PCP进行的那些使用上述随机方法相比，AMIE和PCP是在一项随机的，盲人的跨界研究中，从专家就读医生和我们的模拟患者的角度进行评估，其中包括加拿大，英国和印度的OSCE提供者的149个案例场景，包括各种专业和疾病。 &lt;/p>; &lt;p>;值得注意的是，我们的研究并非旨在模仿传统的OSCE评估或临床医生通常使用文本，电子邮件，聊天或远程医疗的方式。取而代之的是，我们的实验反映了当今消费者与LLM互动的最常见方式，这是AI系统进行远程诊断对话的潜在可扩展和熟悉的机制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s1999 /image8.png“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 1296” data-Original-width =“ 1999” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>;随机研究设计概述，通过在线多转变同步文本聊天访问模拟患者的虚拟远程OSCE。&lt;/td>; &lt;/td>; &lt; /tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>; AMIE &lt;/h2>; &lt;p>;的性能，在此设置中，我们观察到AMIE至少在评估两者时至少进行了模拟的诊断对话以及PCP沿着多个临床上的咨询质量轴。从专科医生的角度来看，AMIE在32个轴中的28个轴的诊断准确性和出色的性能，从患者参与者的角度来看，26个轴中有24个轴中的24个轴。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/s1834/image4.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 1284” data-Original-width =“ 1834” height =“ 448” src =“ src =” https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; amie在我们的评估中的诊断对话中的多个评估轴上的pcps。&lt;/td>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/tbody>; &lt;/table >; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s1999/image9.png&quot; style=&quot;margin-left: auto;边缘权利：自动;”>; &lt;img border =“ 0” data-Original-height =“ 752” data-eriginal-width =“ 1999” R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;在149个场景中，就基础真理诊断（a）和公认的鉴别诊断（b）中列出的所有诊断，比较了AMIE和PCP的TOP-K诊断精度。引导（n = 10,000）确认AMIE和PCP DDX精度之间的所有顶级差异都很重要，而p＆lt; 0.05之后，&lt;a href=&quot;https:///en.wikipedia.org/wikipedia.org/wiki/false_discovery_rate &quot;>; false_discovery_rate &quot;>;假发现费率。 &lt;/a>;＆nbsp;（fdr）校正。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption” -Container“ style =”边缘左左右：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW- fqcncyq_tkftpyclm-ev3ivk5ypgydjykxt8-yxqndz4mnjwkop4ys3xvynpcuzvorhm0mrjks5dvfl-u8hgwki6kzavctocto4kzavcto4 kz4jgcntb njgccntb njgcntb/s199/imag.pt.png =：边缘权利：自动;“>; &lt;img border =“ 0” data-Original-height =“ 1864” data-eriginal-width =“ 1999” R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;专家医生评估的诊断对话和推理素质。在32个轴中，有28个轴，艾米在其余部分中都胜过pcps。 ！ -  &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4- 4leka8rjmz7p3qcjqggwta2pkrxymzkzbtrxcfvqqmqrpvggrefjb0qwzofjb0qwzo1gxtknapwhrq1hxhfbumzpr7ffhhbkuylktesg/s1892/s1892/image6.png边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 1714” data-eriginal-width =“ 1892” src =“ https://blogger.googleusercontent.com/img/img/b/b/ R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic专家医生评估的对话和推理素质。在32个轴中28个轴，AMIE的PCP胜过其余的PCP，而其余的则是可比的。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;h2>;限制&lt;/h2>; &lt;p>;我们的研究有几个局限性，应谨慎解释。首先，我们的评估技术可能低估了人类对话的现实世界价值，因为我们研究中的临床医生仅限于一项不熟悉的文本聊天界面，允许大规模的LLM  - 患者相互作用，但不能代表通常的临床实践。其次，任何对这种类型的研究都必须被视为漫长旅程中的第一步。从我们在本研究中评估的LLM研究原型过渡到可以被人们和那些为他们提供照顾的人使用的安全和强大的工具，将需要大量的额外研究。需要解决许多重要的局限性，包括在现实世界中的限制下进行实验性能以及对诸如健康公平和公平，隐私，鲁棒性等重要主题的专门探索，以确保技术的安全性和可靠性。 &lt;/p>; &lt;br />; &lt;h2>; amie作为对临床医生的帮助&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2312.00164&quot;>;最近发布的Preprint &lt;/a >;，我们评估了AMIE系统早期迭代单独生成DDX或作为临床医生的帮助的能力。二十（20）次通才临床医生评估了303个具有挑战性的现实医疗案例，源自&lt;em>; &lt;a href=&quot;https://www.nejm.org/&quot;>;新英格兰医学杂志&lt;/a>; &lt;/em em >;（NEJM）&lt;a href=&quot;https://www.nejm.org/case-challenges&quot;>;临床病理学会议&lt;/a>;（CPCS）。随机将两名临床医生读取为两个辅助条件之一：搜索引擎和标准医疗资源的辅助，或AMIE援助之外的两名临床医生阅读了每个病例报告。在使用各自的辅助工具之前，所有临床医生都提供了基线，无助的DDX。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s1999/image5 。 //blogger.googleusercontent.com/img/b/r29vz2xl/avvxseia-nkw28syak0dht21l2csb18jkummt2sptafggvgvvgvvvgvvvvvvvvnxdtsdtstssfsfppppppppppppppppppbbabba kmwmwmwmhtba k. km n k. km n gm n k. k qultbbba ivauv akqultbauv akqultbbbbu qultbkqultbkqultbkqultbkqultbkqultbkqultbkqultbkqultkem gngm＆ 3en5vf2temhfmotmlcjtfd3pcvmmc8pjsbiyu-iikfu4atfcoba-a5yhtm2tok1wjzpkmbbviouhxz4dc/s16000/s16000/image5.png5.png =“ Tr-Caption”样式=“ Text-Align：Center;”>;辅助随机读取器研究设置，以调查AMIE对临床医生在解决新英格兰医学杂志的复杂诊断病例挑战方面的辅助效应。&lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/td>; &lt;/td /tr>; &lt;/tbody>; &lt;/table>; &lt;p>; amie表现出独立的表现，超过了未辅助的临床医生的表现（前十名精度为59.1％，比33.6％，p = 0.04）。比较了两个辅助研究组，与没有AMIE援助的临床医生相比，AMIE协助的临床医生的前十名精度更高（24.6％，P＆LT; 0.01）和搜索的临床医生（5.45％，P = 0.02）。此外，艾米协助的临床医生比没有艾米协助的临床医生获得了更全面的差异清单。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/s1999/image7.png&quot; style=&quot;左键左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Forminal-height =“ 1424” data-Original-width =“ 1999”高度=“ 456” src =“ https：/ https：/ https：/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/w640-h456/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ Tr-Caption”样式=“ Text-Align：Center;”>;除了强大的独立绩效外，使用AMIE系统带来了显着的辅助效应，并提高了临床医生在解决这些复杂案例挑战方面的诊断准确性。&lt;/ &lt;/ TD>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;值得注意的是，NEJM CPC不能代表日常临床实践。它们仅在几百个人中是不寻常的案例报告，因此提供有限的范围来探测诸如公平或公平性的重要问题。 &lt;/p>; &lt;br />; &lt;h2>;医疗保健领域的大胆和负责任的研究 - 可能&lt;/h2>; &lt;p>;获得临床专业知识的艺术仍然很少。尽管AI在特定的临床应用中表现出了很大的希望，但参与临床实践的动态，对话诊断之旅需要AI系统尚未证明的许多功能。医生不仅具有知识和技巧，还具有对无数原则的奉献精神，包括安全和质量，沟通，伙伴关系和团队合作，信任和专业精神。在AI系统中意识到这些属性是一个鼓舞人心的挑战，应负责任地谨慎对待。艾米（Amie）是我们对“可能的艺术”的探索，这是一种仅研究的系统，用于安全地探索对未来的愿景，在该系统中，AI系统可能会更好地与熟练的临床医生的属性保持一致，并与我们的照顾相符。这是早期的实验性工作，而不是一种产品，并且有几个局限性，我们认为这是值得严格而广泛的进一步的科学研究，以便设想一个未来，在这种情况下，对话，移情和诊断性AI系统可能会变得安全，有用且易于使用。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的研究是Google Research和Google DeepMind的许多团队的共同工作。 We are grateful to all our co-authors - Tao Tu, Mike Schaekermann, Anil Palepu, Daniel McDuff, Jake Sunshine, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Sara Mahdavi, Karan Sighal, Shekoofeh Azizi, Nenad Tomasev, Yun Liu, Yong Cheng, Le Hou, Albert Webson, Jake Garrison, Yash Sharma, Anupam Pathak, Sushant Prakash, Philip Mansfield, Shwetak Patel, Bradley Green, Ewa Dominowska, Renee Wong, Juraj Gottweis, Dale Webster,凯瑟琳·周（Katherine Chou），克里斯托弗（Christopher）词素，乔尔·巴拉尔（Joelle Barral），格雷格·科拉多（Greg Corrado）和Yossi Matias。我们还要感谢Sami Lachgar，Lauren Winer和John Guilyard对叙述和视觉效果的支持。最后，我们感谢迈克尔·豪威尔（Michael Howell），詹姆斯·莫蒂卡（James Manyika），杰夫·迪恩（James Dean），卡伦·德萨尔沃（Karen DeSalvo），Zoubin Ghahramani＆nbsp;以及Demis Hassabis在此项目过程中提供了支持&lt;/em>;。 &lt;/p>; &lt;br />; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/1800430129205268706/comments/comments/default/default” /atom+xml“/>; &lt;link href =” http://blog.research.google/2024/01/amie-research-aie-system-system-for-diarostic_12.html#comment-comment-form” =“ 0注释” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/feeds/847492633145202626/posts/posts/default/1800430043012920292026268706 +xml“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706 “ http://blog.research.google/2024/01/amie-research-ai-system-for-diarostic_12.html” rel =“替代” title =“ amie：用于诊断医学推理和对话的研究AI系统” type =“ text/html”/>; &lt;unam>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777775266161 &lt;/uri>; &lt;emage>; /email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded。 gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/ s72-c/amie.gif“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/“>; &lt;/media：thumbnail>; &lt;thr>; &lt;thr：thr：thr>; 0 &lt;/thr>; 0 &lt;/thr：thr：thr>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999：Blog-8474926331452026626.POST-7999811818787777164574 &lt;/id>; &lt;/id>; &lt;出版>; 2024-01-1114：42：42：00.000-08：008：008：008：008：008：008：00 &lt;/PRAUBL更新>; 2024-01-11T14：42：51.944-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” >; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ ml”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom/ns# “ term =“自然语言理解”>; &lt;/cattory>; &lt;title type =“ text”>;大语言模型可以识别和纠正其错误？&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-- author&quot;>;Posted by Gladys Tyen, Intern, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s1600/Backtracking.jpg “样式=”显示：无;” />; &lt;p>; llms越来越流行于推理任务，例如&lt;a href=&quot;https://hotpotqa.github.io/&quot;>; Multi-Turn QA &lt;/a>;，&lt;a href =“ https：// arxiv.org/abs/2207.01206&quot;>; task完成&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2304.05128&quot;>;代码生成&lt;/a>;或&lt;a href = /GITHUB.com/openai/grade-school-math&quot;>; Mathematics &lt;/a>;。然而，就像人们一样，他们并不总是在第一次尝试中正确解决问题，尤其是在未经培训的任务上。因此，对于此类系统最有用的是，它们应该能够1）确定其推理出现在哪里，2）回溯以找到另一个解决方案。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;这导致与&lt;em>;自我纠正相关的方法激增，其中使用LLM来识别其中的问题自己的输出，然后根据反馈产生改进的结果。通常认为自我纠正是一个单个过程，但我们决定将其分解为两个组成部分，&lt;em>;错误查找&lt;strong>; &lt;/strong>; &lt;/em>; &lt;/em>;和&lt;em>;输出校正&lt;/em>;。 &lt;/p>; &lt;p>;在“ &lt;a href =” https://arxiv.org/abs/2311.08516#:~: text = while%20 self%2dcorrection%20has%20has%20shown shown est%20shown est%20al.%2C%202023） 。”>; llms找不到推理错误，但可以纠正它们！&lt;/a>;”，我们在错误查找和输出校正时测试了未来的llms。我们提出&lt;a href=&quot;https://github.com/whgtyen/big-bench-mistake&quot;>; big-bench Misfer &lt;/a>;，一个评估基准数据集用于错误识别，我们用来解决以下问题： &lt;/p>; &lt;ol>; &lt;li>; llms可以在&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>; chain-of-of-thought &lt;/a>;（cot）样式推理中找到逻辑错误吗？ &lt;/li>; &lt;li>;是否可以将错误找到的方法用作正确性的代理？ &lt;/li>; &lt;li>;知道错误在哪里，可以提示LLMS回溯并得出正确的答案吗？ &lt;/li>; &lt;li>;是否会误认为是一项技能，可以推广到LLM从未见过的任务？ &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;关于我们的数据集&lt;/h2>; &lt;p>;错误发现是自然语言处理中的一个毫无疑问的问题，在该域中尤其缺乏评估任务。为了最好地评估LLM发现错误的能力，评估任务应表现出非歧义的错误。据我们所知，大多数当前的错误遇到数据集并不超出&lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>; Mathematics &lt;/a>;的范围。 &lt;/p>; &lt;p>;评估LLM在数学领域之外推理错误的能力，我们生成了一个新的数据集供研究界使用，称为&lt;strong>; &lt;/strong>; &lt;a href =“ https：https： //github.com/whgtyen/big-bench-mistake&quot;>; big-bench错误&lt;/a>;。该数据集由使用&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>; Palm 2 &lt;/a>;生成的链条痕迹组成，&lt;a href =” https：// github.com/suzgunmirac/big-bench-hard&quot;>; big-bench &lt;/a>;。第一个逻辑错误的位置对每个迹线进行注释。 &lt;/p>; &lt;p>;为了最大化数据集中的错误次数，我们采样了255个痕迹，其中答案是不正确的（因此我们知道肯定有一个错误），而答案是正确的45个迹线（因此可以或可以或可能不是一个错误）。然后，我们要求人类标签浏览每个轨迹并确定第一个错误步骤。每个痕迹都注释了至少三个标签，其答案的答案&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/inter-rater_reliabilition&quot;>; Inter-Rater可靠性&lt;/a>; 0.98（0.98）（使用&lt;a href=&quot;https://en.wikipedia.org/wiki/krippendorff%27s_alpha&quot;>; krippendorff的α&lt;/a>;）。该标签均针对所有任务完成，除了&lt;a href=&quot;https://github.com/google/boogle/big-bench/tree/main/main/main/bigbench/benchmark_tasks/dyck_languages&quot;>; dyck语言&lt;/a>;给定输入序列的闭合括号序列。我们以算法为标记的此任务。 &lt;/p>; &lt;p>;本数据集中犯的逻辑错误简单明了，为测试LLM在更严格，更模棱两可的任务上使用它们之前找到自己的错误的能力提供了一个很好的基准。&lt;/p>; &lt;div类。&lt;/p>; &lt;div类。 =“ siperator”样式=“ clear：clear; text-align：center;“>; &lt;a href =” https：//blogger.googleusercontent.com/img/b/r29vz2xl/avvz2xl/avvxsegvk5zklbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbblpjcc9l-llppjc9l-llq-2 aik2 eimblq2左右R1GIS9FMEXEAFFFFPRNPYPMF_XABOQ7TQPRPEYLBNSLBNSLBWYTNV1BFXLZ5I-ULNM0ZBC7KBC7KBHX2KKDCT5MIEJWDSHKPHU6RRJ4LRJ4LBVD-NA_XUN_XUN_XUN5DDCCY0EKTCYTCYTCYTCYTJ1UYY6/ MISEJWDSHKPHU6RRJ s1354/bbmistakes2.png“ style =”边距 - 左左右：1em; margin-right：1em;“>; &lt;img border =“ 0” data-Original-height =“ 410” data-original-width =“ 1354” src = src = src = src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s16000/BBMistakes2.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;关于错误识别的核心问题&lt;/h2>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>; 1。 LLM可以在经过思考的链条推理中找到逻辑错误吗？&lt;/h3>; &lt;p>;首先，我们想找出LLM是否可以独立于纠正误差的能力来识别错误。我们尝试多种提示方法来测试&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/generative_pre-trained_trasnformer&quot;>; gpt &lt;/a>; gpt &lt;/a>;系列模型，以找到错误的能力（提示：//github.com/whgtyen/big-bench-mistake/tree/main/mistake_finding_prompts“>; there &lt;/a>;）假设它们通常代表了现代LLM性能。 &lt;/p>; &lt;p>;通常，我们发现这些最先进的模型的性能较差，最佳模型总体上可以达到52.9％的精度。因此，有必要在这一推理领域提高LLM的能力。 &lt;/p>; &lt;p>;在我们的实验中，我们尝试了三种不同的提示方法：直接（跟踪），直接（步骤）和COT（步骤）。在直接（跟踪）中，我们为LLM提供了跟踪，并要求错误或&lt;em>; &lt;em>; no Mister &lt;/em>;的位置步骤&lt;/em>;。在直接（步骤）中，我们提示LLM为其采取的每个步骤询问这个问题。在COT（步骤）中，我们提示LLM对每个步骤是否是错误的理由给出了理由。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s1061/image2.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 466” data-Original-width =“ 1061” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “文本align：中心;”>;一个图表显示了三个提示方法直接（跟踪），直接（步骤）和cot（step）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;我们的发现是一致的，并建立在&lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;先前的结果&lt;/a>;上，但进一步表明LLMS甚至与简单且不歧义的错误斗争（用于比较） ，我们没有事先专业知识的人类评估者以高度同意解决了问题）。我们假设这是LLM无法自我纠正推理错误的重要原因。有关完整结果，请参见&lt;a href=&quot;https://arxiv.org/abs/2311.08516&quot;>;纸&lt;/a>;。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/div>; &lt;h3>; 2。是否可以将错误的发现用作答案正确性的代理？&lt;/h3>; &lt;p>;当人们面对一个不确定答案的问题时，我们可以逐步通过解决方案来工作。如果找不到错误，我们可以假设我们做了正确的事情。 &lt;/p>; &lt;p>;虽然我们假设这对LLM的起作用类似，但我们发现这是一个糟糕的策略。在我们的85％轨迹和15％正确跟踪的数据集中，使用此方法并不比始终将轨迹标记为不正确的幼稚策略好得多，这给出了加权平均&lt;a href =“ https：//en.wikipedia。 org/wiki/f-分数“>; f1 &lt;/a>; of78。&lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“边距 - 左：自动;边缘右：自动;“>; &lt;tbody>; &lt;try>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s698/Self-correcting-LLMs-Tasks.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data- original-height=&quot;427&quot; data-original-width=&quot;698&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s16000/Self-correcting-LLMs -tasks.png“/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;一个图表显示了错误找到的良好状态使用LLM可以用作每个数据集上答案的正确性的代理。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br/ >; &lt;/div>; &lt;h3>; 3。 LLMS可以知道错误在哪里吗？&lt;/h3>; &lt;p>;，因为我们表明LLMS在发现COT痕迹中的推理错误时表现出较差的性能，我们想知道LLMS是否可以完全纠正错误&lt;em>; &lt;em>; /em>;，即使他们知道错误在哪里。 &lt;/p>; &lt;p>;请注意，知道&lt;em>;错误位置&lt;/em>;与知道正确的答案&lt;/em>;：即使最终答案正确，或反之亦然。在大多数现实世界中，我们不知道正确的答案是什么，但是我们可能能够在中间步骤中识别逻辑错误。 &lt;/p>; &lt;p>;我们提出以下回溯方法：&lt;/p>; &lt;ol>; &lt;li>;在温度= 0时生成cot痕迹，在温度下= 0。价值产生更多样化和创造性的输出，通常是以质量为代价。）&lt;/li>; &lt;li>;确定第一个逻辑错误的位置（例如，使用分类器，或者在这里我们只使用数据集中的标签）。 &lt;/li>; &lt;li>;在温度= 1处重新生成错误步骤，并产生一组八个输出。由于已知原始输出导致结果不正确，因此目标是在此步骤中找到与原始的替代一代。 &lt;/li>; &lt;li>;从这八个输出中，选择一个与原始错误步骤不同的输出。 （我们只是在这里使用精确的匹配，但是将来可能是更复杂的东西。）&lt;/li>; &lt;li>;使用新步骤，在温度= 0时生成正常的轨迹的其余部分。&lt;/li>; &lt;/li>; &lt;。 /ol>; &lt;p>;这是一种非常简单的方法，不需要任何额外的提示手工制作，并且避免了必须重新生成整个痕迹。我们使用来自大台式错误的错误位置数据对其进行测试，我们发现它可以纠正COT错误。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;最近的工作&lt;/a>;表明自我校正方法，例如&lt;a href =“ https：// arxiv。 org/abs/2303.11366“>;反射&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2303.17491&quot;>; rci &lt;/a>;，准确分数会导致准确分数降低，因为有更多正确的答案变得不正确反之亦然。另一方面，我们的方法比损失（通过将正确的答案更改为错误的答案）产生更多的收益（通过纠正错误的答案）。 &lt;/p>; &lt;p>;我们还将我们的方法与随机基线进行比较，在该方法中，我们随机假设一个步骤是错误。我们的结果表明，这种随机基线确实会产生一些收益，但没有正确的错误位置和更多的损失。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxssei4cqq3mwjgmjmjgmjmjgmjmjgmjmjmwjmwjmwjm jospoizp04vyioj7f4ktdgtan4bkjquftawhy_quqwhy_quqwhi _qqquqwhi _qqquqwhy_quqny flbbbbny flbbbbny flbbbbny flbbbny = ZVUX1W4N3GLQI51FY95LS-GK_RMLQQETYNMVQI7OS7U6XHGXX2CUNHTCWMZRPFWRS1VD01G14GWOWOEXXZDTTTCXDTCXDCXDCXDCCPOIBGTHFXRGTXRFXRGLGLGLWJ222222222REMBER aigry sinf/sinf/simign abrimum abric/simum firams abrimum s74边距 - 左：自动;边缘权利：自动;“>; &lt;img border =“ 0” data-Original-height =“ 480” data-Original-width =“ 744” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：center;“>;图表显示我们方法准确性的收益和损失以及每个数据集中的随机基线。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;/table>; &lt;div style =“ line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;4. Can mistake finding generalize to tasks the LLMs have never seen?&lt;/h3>; &lt;p>; To answer this question, we fine-tuned a small model on four of the BIG-Bench tasks and tested it on the fifth, held-out task 。 We do this for every task, producing five fine-tuned models in total. Then we compare the results with just zero-shot prompting &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-L-Unicorn&lt;/a>;, a much larger model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s759/image5.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;759&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Bar chart showing the accuracy improvement of the fine-tuned small model compared to zero-shot prompting with PaLM 2-L-Unicorn.&lt; /span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results show that the much smaller fine-tuned reward model generally performs better than zero-shot prompting a large model, even though the reward model has never seen data from the task in the test set. The only exception is logical deduction, where it performs on par with zero-shot prompting. &lt;/p>; &lt;p>; This is a very promising result as we can potentially just use a small fine-tuned reward model to perform backtracking and improve accuracy on any task, even if we don&#39;t have the data for it. This smaller reward model is completely independent of the generator LLM, and can be updated and further fine-tuned for individual use cases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s867/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;867&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;An illustration showing how our backtracking method works.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we created an evaluation benchmark dataset that the wider academic community can use to evaluate future LLMs. We further showed that LLMs currently struggle to find logical errors. However, if they could, we show the effectiveness of backtracking as a strategy that can provide gains on tasks. Finally, a smaller reward model can be trained on general mistake-finding tasks and be used to improve out-of-domain mistake finding, showing that mistake-finding can generalize. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Thank you to Peter Chen, Tony Mak, Hassan Mansoor and Victor Cărbune for contributing ideas and helping with the experiments and data collection. We would also like to thank Sian Gooding and Vicky Zayats for their comments and suggestions on the paper.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7998118785777164574/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and.html&quot; rel=&quot;alternate&quot; title=&quot;Can large language models identify and correct their mistakes?&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s72-c/Backtracking.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7197275876457161088&lt;/id>;&lt;published>;2024-01-08T14:07:00.000-08:00&lt;/published>;&lt;updated>;2024-01-19T09:59:57.076-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: User Experience Team&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ayça Çakmakli, UX Lead, Google Research, Responsible AI and Human Centered Technology Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s16000/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google&#39;s Responsible AI User Experience (Responsible AI UX) team is a product-minded team embedded within Google Research. This unique positioning requires us to apply responsible AI development practices to our user-centered user experience (UX) design process. In this post, we describe the importance of UX design and responsible AI in product development, and share a few examples of how our team&#39;s capabilities and cross-functional collaborations have led to responsible development across Google. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; First, the UX part. We are a multi-disciplinary team of product design experts: designers, engineers, researchers, and strategists who manage the user-centered UX design process from early-phase ideation and problem framing to later-phase user-interface (UI) design, prototyping和细化。 We believe that effective product development occurs when there is clear alignment between significant unmet user needs and a product&#39;s primary value proposition, and that this alignment is reliably achieved via a thorough user-centered UX design process. &lt;/p>; &lt;p>; And second, recognizing generative AI&#39;s (GenAI) potential to significantly impact society, we embrace our role as the primary user advocate as we continue to evolve our UX design process to meet the unique challenges AI poses, maximizing the benefits and minimizing the risks. As we navigate through each stage of an AI-powered product design process, we place a heightened emphasis on the ethical, societal, and long-term impact of our decisions. We contribute to the ongoing development of comprehensive &lt;a href=&quot;https://ai.google/responsibility/ai-governance-operations&quot;>;safety and inclusivity protocols&lt;/a>; that define design and deployment guardrails around key issues like content curation, security, privacy, model capabilities, model access, equitability, and fairness that help mitigate GenAI risks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s1920/image4. png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;Responsible AI UX is constantly evolving its user-centered product design process to meet the needs of a GenAI-powered product landscape with greater sensitivity to the needs of users and society and an emphasis on ethical, societal, and long-term impact.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Responsibility in product design is also reflected in the user and societal problems we choose to address and the programs we resource. Thus, we encourage the &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;prioritization of user problems with significant scale and severity&lt;/a>; to help maximize the positive impact of GenAI technology. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s1920 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; Communication across teams and disciplines is essential to responsible product design. The seamless flow of information and insight from user research teams to product design and engineering teams, and vice versa, is essential to good product development. One of our team&#39;s core objectives is to ensure the practical application of deep user-insight into AI-powered product design decisions at Google by bridging the communication gap between the vast technological expertise of our engineers and the user/societal expertise of our academics, research scientists, and user-centered design research experts. We&#39;ve built a multidisciplinary team with expertise in these areas, deepening our empathy for the communication needs of our audience, and enabling us to better interface between our user &amp;amp; society experts and our technical experts. We create frameworks, guidebooks, prototypes, cheatsheets, and multimedia tools to help bring insights to life for the right people at the right time.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s1920/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Facilitating responsible GenAI prototyping and development &lt;/h2>; &lt;p>; During collaborations between Responsible AI UX, the &lt;a href=&quot;https://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot;>;People + AI Research&lt;/a>; (PAIR) initiative and &lt;a href=&quot;https://labs.google/&quot;>;Labs&lt;/a>;, we identified that &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;prototyping&lt;/a>; can afford a creative opportunity to engage with large language models (LLM), and is often the first step in GenAI product development. To address the need to introduce LLMs into the prototyping process, we explored a range of different prompting designs. Then, we went out into the field, employing various external, first-person UX design research methodologies to draw out insight and gain empathy for the user&#39;s perspective. Through user/designer co-creation sessions, iteration, and prototyping, we were able to bring internal stakeholders, product managers, engineers, writers, sales, and marketing teams along to ensure that the user point of view was well understood and to reinforce alignment跨团队。 &lt;/p>; &lt;p>; The result of this work was &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a generative AI platform launched at &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;Google I/O 2023&lt;/a>; that enables people, even those without any ML experience, to prototype creatively using LLMs. The team&#39;s first-hand experience with users and understanding of the challenges they face allowed us to incorporate our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; into the MakerSuite product design 。 Product features like &lt;a href=&quot;https://developers.generativeai.google/guide/safety_setting&quot;>;safety filters&lt;/a>;, for example, enable users to manage outcomes, leading to easier and more responsible product development with MakerSuite. &lt;/p>; &lt;p>; Because of our close collaboration with product teams, we were able to adapt text-only prototyping to support multimodal interaction with &lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI Studio&lt;/a>;, an evolution of MakerSuite. Now, Google AI Studio enables developers and non-developers alike to seamlessly leverage Google&#39;s latest &lt;a href=&quot;https://ai.google.dev/&quot;>;Gemini&lt;/a>; model to merge multiple modality inputs, like text and image, in product explorations. Facilitating product development in this way provides us with the opportunity to better use AI to identify &lt;a href=&quot;https://arxiv.org/pdf/2310.15428.pdf&quot;>;appropriateness of outcomes&lt;/a>; and unlocks opportunities for developers and non-developers to play with AI sandboxes. Together with our partners, we continue to actively push this effort in the products we support.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1406&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI studio&lt;/a>; enables developers and non-developers to leverage Google Cloud infrastructure and merge multiple modality inputs in their product explorations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Equitable speech recognition&lt;/h2>; &lt;p>; Multiple &lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.1915768117&quot;>;external studies&lt;/a>;, as well as Google&#39;s &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frai.2021.725911/full&quot;>;own research,&lt;/a>; have identified an unfortunate deficiency in the ability of current speech recognition technology to understand Black speakers on average, relative to White speakers. As multimodal AI tools begin to rely more heavily on speech prompts, this problem will grow and continue to alienate users. To address this problem, the Responsible AI UX team is &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with world-renowned linguists and scientists at Howard University&lt;/a>;, a prominent &lt;a href=&quot;https://en.wikipedia.org/wiki/Historically_black_colleges_and_universities&quot; target=&quot;_blank&quot;>;HBCU&lt;/a>;, to build a high quality African-American English dataset to improve the design of our speech technology products to make them more accessible. Called Project Elevate Black Voices, this effort will allow Howard University to share the dataset with those looking to improve speech technology while establishing a framework for responsible data collection, ensuring the data benefits Black communities. Howard University will retain the ownership and licensing of the dataset and serve as stewards for its responsible use. At Google, we&#39;re providing funding support and collaborating closely with our partners at Howard University to ensure the success of this program. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/t_pdlrU8qhs?si=5xY1AoGc_d2HTzQf&quot; width=&quot;640&quot; youtube-src-id=&quot;5xY1AoGc_d2HTzQf&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Equitable computer vision&lt;/h2>; &lt;p>; The &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; project highlighted that computer vision systems struggle to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. This is largely due to the fact that the datasets used to train these models were not inclusive to a wide range of skin tones. To address this limitation, the Responsible AI UX team has been partnering with sociologist &lt;a href=&quot;https://www.ellismonk.com/&quot;>;Dr. Ellis Monk&lt;/a>; to release the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone Scale&lt;/a>; (MST), a skin tone scale designed to be more inclusive of the spectrum of skin tones around the world. It provides a tool to assess the inclusivity of datasets and model performance across an inclusive range of skin tones, resulting in features and products that work better for everyone. &lt;/p>; &lt;p>; We have integrated MST into a range of &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Google products&lt;/a>;, such as Search, Google Photos, and others. We also open sourced MST, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3632120&quot;>;published our research&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;described our annotation practices&lt;/a>;, and &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;shared an example dataset&lt;/a>; to encourage others to easily integrate it into their products. The Responsible AI UX team continues to collaborate with Dr. Monk, utilizing the MST across multiple product applications and continuing to do international research to ensure that it is globally inclusive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Consulting &amp;amp; guidance&lt;/h2>; &lt;p>; As teams across Google continue to develop products that leverage the capabilities of GenAI models, our team recognizes that the challenges they face are varied and that market competition is significant. To support teams, we develop actionable assets to facilitate a more streamlined and responsible product design process that considers available resources. We act as a product-focused design consultancy, identifying ways to scale services, share expertise, and apply our design principles more broadley. Our goal is to help all product teams at Google connect significant unmet user needs with technology benefits via great responsible product design. &lt;/p>; &lt;p>; One way we have been doing this is with the creation of the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>;, an evolving summative resource of many of the responsible design lessons we&#39;ve learned and recommendations we&#39;ve made for internal and external stakeholders. With its forthcoming, rolling &lt;a href=&quot;https://medium.com/people-ai-research/updating-the-people-ai-guidebook-in-the-age-of-generative-ai-cace6c846db4&quot;>;updates&lt;/a>; focusing specifically on how to best design and consider user needs with GenAI, we hope that our internal teams, external stakeholders, and larger community will have useful and actionable guidance at the most critical milestones in the product development journey. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s1100/image1.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;622&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;The People + AI Guidebook has six chapters, designed to cover different aspects of the product life cycle.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If you are interested in reading more about Responsible AI UX and how we are specifically thinking about designing responsibly with Generative AI, please check out this &lt;a href=&quot;https://medium.com/people-ai-research/meet-ay%C3%A7a-%C3%A7akmakli- googles-new-head-of-responsible-ai-ux-d8f2700df95b&quot;>;Q&amp;amp;A piece&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Shout out to our the Responsible AI UX team members: Aaron Donsbach, Alejandra Molina, Courtney Heldreth, Diana Akrong, Ellis Monk, Femi Olanubi, Hope Neveux, Kafayat Abdul, Key Lee, Mahima Pushkarna, Sally Limb, Sarah Post, Sures Kumar Thoddu Srinivasan, Tesh Goyal, Ursula Lauriston, and Zion Mengesha. Special thanks to Michelle Cohn for her contributions to this work. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7197275876457161088/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html#comment-form&quot; rel=&quot;replies&quot; title =&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;edit&quot; type=&quot;application/atom +xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href= &quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: User Experience Team&quot; type=&quot; text/html“/>; &lt;aunder>; &lt;名称>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/1209862651477775266161 &lt;/uri>; &lt;Email>; &lt;Email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =” https://img1.blog1.blogblog.com/img/b16-round.gif =&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked -9QDeDkSOCZMBaIYrL/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr :total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8816183473385638131&lt;/id>;&lt;published>;2023-12-22T10:37:00.000-08:00&lt;/ published>;&lt;updated>;2024-01-11T16:01:33.313-08:00&lt;/updated>;&lt;title type=&quot;text&quot;>;2023: A year of groundbreaking advances in AI and computing&lt;/stitle>;&lt;content type=&quot; html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jeff Dean, Chief Scientist, Google DeepMind &amp;amp; Google Research, Demis Hassabis, CEO, Google DeepMind, and James Manyika, SVP, Google Research, Technology &amp;amp; Society&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU12G_p2DZO4gQ-P95aP2IFvtKRHRG5Oik9VzJ4WtT_VznggjUrL4tTzqto-C8yx6ULgvugKgH9usiZxnKGk97pOFyHWnu-1S4sSbR2Jjq5T36tQRbZucTAP7gmXkGw77xN6s39IKxPaxbo5tw_Cq52ZfPWOCBkWL-2XTuzsIh6viRIqgGcdUdNq-pHjzl/s1100/year_in_review-hero.jpg&quot; style=&quot;display ： 没有任何;” />; &lt;p>; This has been a year of incredible progress in the field of Artificial Intelligence (AI) research and its practical applications. &lt;/p>; &lt;p>; As ongoing research pushes AI even farther, we look back to our &lt;a href=&quot;https://ai.google/static/documents/google-why-we-focus-on-ai.pdf&quot;>;perspective&lt;/a>; published in January of this year, titled “Why we focus on AI (and to what end),” where we noted: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; We are committed to leading and setting the standard in developing and shipping useful and beneficial applications, applying ethical principles grounded in human values, and evolving our approaches as we learn from research, experience, users, and the wider community. &lt;/p>; &lt;p>; We also believe that getting AI right — which to us involves innovating and delivering widely accessible benefits to people and society, while mitigating its risks — must be a collective effort involving us and others, including researchers, developers, users (individuals, businesses, and other organizations), governments, regulators, and citizens. &lt;/p>; &lt;p>; We are convinced that the AI-enabled innovations we are focused on developing and delivering boldly and responsibly are useful, compelling, and have the potential to assist and improve lives of people everywhere — this is what compels us. &lt;/p>; &lt;/div>; &lt;p>; In this Year-in-Review post we&#39;ll go over some of Google Research&#39;s and Google DeepMind&#39;s efforts putting these paragraphs into practice safely throughout 2023. &lt;/p>; &lt;br />; &lt;h2>;Advances in products &amp;amp; technologies &lt;/h2>; &lt;p>; This was the year generative AI captured the world&#39;s attention, creating imagery, music, stories, and engaging conversation about everything imaginable, at a level of creativity and a speed almost implausible a few years ago. &lt;/p>; &lt;p>; In February, we &lt;a href=&quot;https://blog.google/technology/ai/bard-google-ai-search-updates/&quot;>;first launched&lt;/a>; &lt;a href=&quot;https://bard.google.com&quot;>;Bard&lt;/a>;, a tool that you can use to explore creative ideas and explain things simply. It can generate text, translate languages, write different kinds of creative content and more. &lt;/p>; &lt;p>; In May, we watched the results of months and years of our foundational and applied work announced on stage &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;at Google I/O&lt;/a>;. Principally, this included &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>;, a large language model (LLM) that brought together compute-optimal scaling, an improved dataset mixture, and model architecture to excel at advanced reasoning tasks. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5R7E2f0BQIsiLZm25kxfR_Ix_Bvm6HlQQCXI12sB42siKUAf8eZvVEDU5bi8EQc22BoG6SV_H8NbC-PKd2pPv7FhC-uBR43ZWpbrgvaGJ7699j-uUctPbFBO9Bf-u81gkfU1OP4oGZhs6KKkub2znNsvcElREn5kwKh2npPJxFJdSVZZUIPZhyphenhyphen3B_jCq4/s1920/lockup_ic_PaLM-2_H_4297x745px_clr_@1x.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;555&quot; data-original-width=&quot;1920&quot; height=&quot;116&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5R7E2f0BQIsiLZm25kxfR_Ix_Bvm6HlQQCXI12sB42siKUAf8eZvVEDU5bi8EQc22BoG6SV_H8NbC-PKd2pPv7FhC-uBR43ZWpbrgvaGJ7699j-uUctPbFBO9Bf-u81gkfU1OP4oGZhs6KKkub2znNsvcElREn5kwKh2npPJxFJdSVZZUIPZhyphenhyphen3B_jCq4/w400-h116/lockup_ic_PaLM-2_H_4297x745px_clr_@1x.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/div>; &lt;p>;By fine-tuning and instruction-tuning PaLM 2 for different purposes, we were able to integrate it into numerous Google products and features, including:&lt;/p>; &lt;ul>; &lt;li>;An update to Bard, which enabled multilingual capabilities. Since its initial launch, Bard is now available in more than &lt;a href=&quot;https://support.google.com/bard/answer/13575153?hl=en&quot;>;40 languages and over 230 countries and territories&lt;/a>;, and &lt;a href=&quot;https://blog.google/products/bard/google-bard-new-features-update-sept-2023/&quot;>;with extensions&lt;/a>;, Bard can find and show relevant information from Google tools used every day — like Gmail, Google Maps, YouTube, and more. &lt;/li>; &lt;li>;&lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;Search Generative Experience&lt;/a>; (SGE), which uses LLMs to reimagine both how to organize information and how to help people navigate through it, creating a more fluid, conversational interaction model for our core Search product. This work extended the search engine experience from primarily focused on information retrieval into something much more — capable of retrieval, synthesis, creative generation and continuation of previous searches — while continuing to serve as a connection point between users and the web content they seek. &lt;/li>; &lt;li>;&lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;, a text-to-music model powered by &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2208.12415&quot;>;MuLAN&lt;/a>;, which can make music from text, humming, images or video and musical accompaniments to singing. &lt;/li>; &lt;li>;Duet AI, our AI-powered collaborator that provides users with assistance when they use Google Workspace and Google Cloud. &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>;Duet AI in Google Workspace&lt;/a>;, for example, helps users write, create images, analyze spreadsheets, draft and summarize emails and chat messages, and summarize meetings. &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-ai-for-google-cloud&quot;>;Duet AI in Google Cloud&lt;/a>; helps users code, deploy, scale, and monitor applications, as well as identify and accelerate resolution of cybersecurity threats. &lt;/li>; &lt;li>;And many &lt;a href=&quot;https://blog.google/technology/developers/google-io-2023-100-announcements/&quot;>;other developments&lt;/a>;. &lt;/li>; &lt;/ul>; &lt;p>; In June, following last year&#39;s release of our text-to-image generation model &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, we released &lt;a href=&quot;https://blog.research.google/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;Imagen Editor&lt;/a>;, which provides the ability to use region masks and natural language prompts to interactively edit generative images to provide much more precise control over the model output. &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU-6oYBC5383noYqno-cYzUboAAOgXvDlWtqwu-zl94M2r02Fsid0jjgLBJl3JCVR4lc8ZVw7-c7q9OlHjzmrRXz5i5H/s1261/image4.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU-6oYBC5383noYqno-cYzUboAAOgXvDlWtqwu-zl94M2r02Fsid0jjgLBJl3JCVR4lc8ZVw7-c7q9OlHjzmrRXz5i5H/s16000/image4.png&quot; />;&lt;/a>;&lt;/div>; &lt;p>; Later in the year, we released Imagen 2, which improved outputs via a specialized image aesthetics model based on human preferences for qualities such as good lighting, framing, exposure, and sharpness. &lt;/p>; &lt;p>; In October, we launched a feature that &lt;a href=&quot;https://blog.research.google/2023/10/google-search-can-now-help-with-english-speaking-practice.html&quot;>;helps people practice speaking and improve their language skills&lt;/a>;. The key technology that enabled this functionality was a novel deep learning model developed in collaboration with the Google Translate team, called Deep Aligner. This single new model has led to dramatic improvements in alignment quality across all tested language pairs, reducing average alignment error rate from 25% to 5% compared to alignment approaches based on &lt;a href=&quot;https://aclanthology.org/C96-2141/&quot;>;Hidden Markov models&lt;/a>; (HMMs). &lt;/p>; &lt;p>; In November, in partnership with &lt;a href=&quot;https://blog.youtube/inside-youtube/ai-and-music-experiment/&quot;>;YouTube&lt;/a>;, we announced &lt;a href=&quot;https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/&quot;>;Lyria&lt;/a>;, our most advanced AI music generation model to date. We released two experiments designed to open a new playground for creativity, DreamTrack and music AI tools, in concert with &lt;a href=&quot;https://blog.youtube/inside-youtube/partnering-with-the-music-industry-on-ai/&quot;>;YouTube&#39;s Principles for partnering with the music industry on AI technology&lt;/a>;. &lt;/p>; &lt;p>; Then in December, we launched &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;Gemini&lt;/a>;, our most capable and general AI模型。 Gemini was built to be multimodal from the ground up across text, audio, image and videos. Our initial family of Gemini models comes in three different sizes, Nano, Pro, and Ultra. Nano models are our smallest and most efficient models for powering on-device experiences in products like Pixel. The Pro model is highly-capable and best for scaling across a wide range of tasks. The Ultra model is our largest and most capable model for highly complex tasks. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://www.youtube.com/watch?v=jV1vkHv4zq8&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/jV1vkHv4zq8&quot; width=&quot;640&quot; youtube-src-id=&quot;jV1vkHv4zq8&quot;>;&lt;/iframe>;&lt;/a>;&lt;/div>; &lt;br />; &lt;p>;In a &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&quot;>;technical report&lt;/a>; about &lt;a href=&quot;https://deepmind.google/technologies/gemini&quot;>;Gemini models&lt;/a>;, we showed that Gemini Ultra&#39;s performance exceeds current state-of-the-art results on 30 of the 32 widely-used academic benchmarks used in LLM research and development. With a score of 90.04%, Gemini Ultra was the first model to outperform human experts on &lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>;MMLU&lt;/a>;, and achieved a state-of-the-art score of 59.4% on the new &lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>;MMMU&lt;/a>; benchmark. &lt;/p>; &lt;p>; Building on &lt;a href=&quot;https://deepmind.google/discover/blog/competitive-programming-with-alphacode/&quot;>;AlphaCode&lt;/a>;, the first AI system to perform at the level of the median competitor in competitive programming, we &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf&quot;>;introduced AlphaCode 2&lt;/a>; powered by a specialized version of Gemini 。 When evaluated on the same platform as the original AlphaCode, we found that AlphaCode 2 solved 1.7x more problems, and performed better than 85% of competition participants &lt;/p>; &lt;p>; At the same time, &lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;Bard got its biggest upgrade&lt;/a>; with its use of the Gemini Pro model, making it far more capable at things like understanding, summarizing, reasoning, coding, and planning. In six out of eight benchmarks, Gemini Pro outperformed GPT-3.5, including in MMLU, one of the key standards for measuring large AI models, and &lt;a href=&quot;https://huggingface.co/datasets/gsm8k&quot;>;GSM8K&lt;/a>;, which measures grade school math reasoning. Gemini Ultra will come to Bard early next year through Bard Advanced, a new cutting-edge AI experience. &lt;/p>; &lt;p>; Gemini Pro is also available on &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/gemini-support-on-vertex-ai&quot;>;Vertex AI&lt;/a>;, Google Cloud&#39;s end-to-end AI platform that empowers developers to build applications that can process information across text, code, images, and video. &lt;a href=&quot;https://blog.google/technology/ai/gemini-api-developers-cloud/&quot;>;Gemini Pro was also made available in AI Studio&lt;/a>; in December. &lt;/p>; &lt;p>; To best illustrate some of Gemini&#39;s capabilities, we produced a &lt;a href=&quot;https://deepmind.google/technologies/gemini/#hands-on&quot;>;series of short videos&lt;/a>; with explanations of how Gemini could: &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=sPiOP_CB54A&quot;>;Unlock insights in scientific literature&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=LvGmVmHv69s&amp;amp;t=1s&quot;>;Excel at competitive programming&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=D64QD7Swr3s&quot;>;Process and understand raw audio&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=K4pX1VAxaAI&quot;>;Explain reasoning in math and physics&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=v5tRc_5-8G4&quot;>;Reason about user intent to generate bespoke experiences&lt;/a>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;ML/AI Research&lt;/h2>; &lt;p>; In addition to our advances in products and technologies, we&#39;ve also made a number of important advancements in the broader fields of machine learning and AI research. &lt;/p>; &lt;p>; At the heart of the most advanced ML models is the Transformer model architecture, &lt;a href=&quot;https://blog.research.google/2017/08/transformer-novel-neural-network.html&quot;>;developed by Google researchers in 2017&lt;/a>;. Originally developed for language, it has proven useful in domains as varied as &lt;a href=&quot;https://blog.research.google/2020/12/transformers-for-image-recognition-at.html&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/&quot;>;audio&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/&quot;>;genomics&lt;/a>;, &lt;a href=&quot;https://deepmind.google/technologies/alphafold/&quot;>;protein folding&lt;/a>;, and more. This year, our work on &lt;a href=&quot;https://blog.research.google/2023/03/scaling-vision-transformers-to-22.html&quot;>;scaling vision transformers&lt;/a>; demonstrated state-of-the-art results across a wide variety of vision tasks, and has also been useful in building &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;more capable robots&lt;/a>;. &lt;/p>; &lt;p>; &lt;/p>; &lt;p>; Expanding the versatility of models requires the ability to perform higher-level and multi-step reasoning. This year, we approached this target following several research tracks. For example, &lt;a href=&quot;https://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot;>;algorithmic prompting&lt;/a>; is a new method that teaches language models reasoning by demonstrating a sequence of algorithmic steps, which the model can then apply in new contexts. This approach improves accuracy on one middle-school mathematics benchmark from 25.9% to 61.1%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s1200/image5.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;By providing algorithmic prompts, we can teach a model the rules of arithmetic via in-context learning.&lt;/em>;&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>; In the domain of visual question answering, in a collaboration with UC Berkeley researchers, we showed how we could &lt;a href=&quot;https://blog.research.google/2023/07 /modular-visual-question-answering-via.html&quot;>;better answer complex visual questions&lt;/a>; (“Is the carriage to the right of the horse?”) by combining a visual model with a language model trained to answer visual questions by synthesizing a program to perform multi-step reasoning. &lt;/p>; &lt;p>; We are now using a &lt;a href=&quot;https://blog.research.google/2023/05/large-sequence-models-for-software.html&quot;>;general model that understands many aspects of the software development life cycle&lt;/a>; to automatically generate code review comments, respond to code review comments, make performance-improving suggestions for pieces of code (by learning from past such changes in other contexts), fix code in response to compilation errors, and more. &lt;/p>; &lt;p>; In a multi-year research collaboration with the Google Maps team, we were able to scale inverse reinforcement learning and apply it to the &lt;a href=&quot;https://blog.research.google/2023/09/world-scale-inverse-reinforcement.html&quot;>;world-scale problem of improving route suggestions&lt;/a>; for over 1 billion users. Our work culminated in a 16–24% relative improvement in global route match rate, helping to ensure that routes are better aligned with user preferences. &lt;/p>; &lt;p>; We also continue to work on techniques to improve the inference performance of machine learning models. In work on &lt;a href=&quot;https://blog.research.google/2023/08/neural-network-pruning-with.html&quot;>;computationally-friendly approaches to pruning connections in neural networks&lt;/a>;, we were able to devise an approximation algorithm to the computationally intractable best-subset selection problem that is able to prune 70% of the edges from an image classification model and still retain almost all of the accuracy of the original. &lt;/p>; &lt;p>; In work on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;accelerating on-device diffusion models&lt;/a>;, we were also able to apply a variety of optimizations to attention mechanisms, convolutional kernels, and fusion of operations to make it practical to run high quality image generation models on-device; for example, enabling “a photorealistic and high-resolution image of a cute puppy with surrounding flowers” to be generated in just 12 seconds on a smartphone. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/s522/image7.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/w208-h400/image7.gif&quot; width=&quot;208&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;p>;Advances in capable language and multimodal models have also benefited our robotics research efforts. We combined separately trained language, vision, and robotic control models into &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>;, an embodied multi-modal model for robotics, and &lt;a href=&quot;https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/&quot;>;Robotic Transformer 2&lt;/a>; (RT-2), a novel vision-language-action (VLA) model that &lt;a href=&quot;https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/&quot;>;learns&lt;/a>; from both web and robotics data, and translates this knowledge into generalized instructions for robotic control.&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s616/image8.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s16000/image8.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;RT-2 architecture and training: We co-fine-tune a pre-trained vision-language model on robotics and web data. The resulting model takes in robot camera images and directly predicts actions for a robot to perform.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore, we showed how &lt;a href=&quot;https://blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot;>;language can also be used to control the gait of quadrupedal robots&lt;/a>; and explored the &lt;a href=&quot;https://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot;>;use of language to help formulate more explicit reward functions&lt;/a>; to bridge the gap between human language and robotic actions. Then, in &lt;a href=&quot;https://blog.research.google/2023/05/barkour-benchmarking-animal-level.html&quot;>;Barkour&lt;/a>; we benchmarked the agility limits of quadrupedal robots.&lt;/p>; &lt;br />; &lt;h2>;Algorithms &amp;amp; optimization&lt;/h2>; &lt;p>; Designing efficient, robust, and scalable algorithms remains a high priority. This year, our work included: applied and scalable algorithms, market algorithms, system efficiency and optimization, and privacy. &lt;/p>; &lt;p>; We introduced &lt;a href=&quot;https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/&quot;>;AlphaDev&lt;/a>;, an AI system that uses reinforcement learning to discover enhanced computer science algorithms. AlphaDev uncovered a faster algorithm for sorting, a method for ordering data, which led to improvements in the LLVM libc++ sorting library that were up to 70% faster for shorter sequences and about 1.7% faster for sequences exceeding 250,000 elements. &lt;/p>; &lt;p>; We developed a novel model to &lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;predict the properties of large graphs&lt;/a>;, enabling estimation of performance for large programs. We released a new dataset, &lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TPUGraphs&lt;/a>;, to accelerate &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime&quot;>;open research in this area&lt;/a>;, and showed how we can use &lt;a href=&quot;https://blog.research.google/2023/12/advancements-in-machine-learning-for.html&quot;>;modern ML to improve ML efficiency&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/s1223/image11.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1042&quot; data-original-width=&quot;1223&quot; height=&quot;341&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/w400-h341/image11.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The TPUGraphs dataset has 44 million graphs for ML program optimization. &lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;We developed a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Load_balancing_(computing)&quot;>; load balancing&lt;/a>; algorithm for distributing queries to a server, called &lt;a href=&quot;https://arxiv.org/abs/2312.10172&quot;>;Prequal&lt;/a>;, which minimizes a combination of requests-in-flight and estimates the latency. Deployments across several systems have saved CPU, latency, and RAM significantly. We also designed a new &lt;a href=&quot;https://arxiv.org/abs/2305.02508&quot;>;analysis framework&lt;/a>; for the classical caching problem with capacity reservations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s1896/image12.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1896&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Heatmaps of normalized CPU usage transitioning to&amp;nbsp;&lt;a href=&quot;Prequal&quot;>;Prequal&lt;/a>;&amp;nbsp;at 08:00.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We improved state-of-the-art in clustering and &lt;a href=&quot;https://en. wikipedia.org/wiki/Graph_neural_network&quot;>;graph algorithms&lt;/a>; by developing new techniques for&lt;a href=&quot;https://arxiv.org/abs/2106.05513&quot;>; computing minimum-cut&lt;/a>;, &lt;a href =&quot;https://arxiv.org/abs/2309.17243&quot;>;approximating correlation clustering&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2308.00503&quot;>;massively parallel graph clustering&lt;/a>; 。 Additionally, we introduced&lt;a href=&quot;https://arxiv.org/abs/2308.03578&quot;>; TeraHAC&lt;/a>;, a novel hierarchical clustering algorithm for trillion-edge graphs, designed a &lt;a href=&quot;https://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot;>;text clustering algorithm&lt;/a>; for better scalability while maintaining quality, and designed the most efficient &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot;>;algorithm for approximating the Chamfer Distance&lt;/a>;, the standard similarity function for multi-embedding models, offering &amp;gt;50× speedups over highly-optimized exact algorithms and scaling to billions of points.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; We continued optimizing Google&#39;s large embedding models (LEMs), which power many of our core products and recommender systems. Some new techniques include &lt;a href=&quot;https://arxiv.org/abs/2305.12102&quot;>;Unified Embedding&lt;/a>; for battle-tested feature representations in web-scale ML systems and &lt;a href=&quot;https://arxiv.org/abs/2209.14881&quot;>;Sequential Attention&lt;/a>;, which uses attention mechanisms to discover high-quality sparse model architectures during training. &lt;/p>; &lt;!--&lt;p>; This year, we also continued our research in market algorithms to design computationally efficient marketplaces and causal inference. First, we remain committed to advancing the rapidly growing interest in ads automation for which our recent work &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3543507.3583416&quot;>;explains the adoption of autobidding mechanisms&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597725&quot;>;examines the effect of different auction formats on the incentives of advertisers&lt;/a>;. In the multi-channel setting, our findings shed light on how the choice between local and global optimizations affects the design of multi-channel &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597707&quot;>;auction systems&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618709&quot;>;bidding systems&lt;/a>;. &lt;/p>;-->; &lt;p>; Beyond auto-bidding systems, we also studied auction design in other complex settings, such as &lt;a href=&quot;https://arxiv.org/abs/2204.01962&quot;>;buy-many mechanisms&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2207.09429&quot;>;auctions for heterogeneous bidders&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2309.10766&quot;>;contract designs&lt;/a>;, and innovated &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618478&quot;>;robust online bidding algorithms&lt;/a>;. Motivated by the application of generative AI in collaborative creation (eg, joint ad for advertisers), we proposed &lt;a href=&quot;https://arxiv.org/abs/2310.10826&quot;>;a novel token auction model &lt;/a>;where LLMs bid for influence in the collaborative AI creation. Finally, we show how to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3580507.3597702&quot;>;mitigate personalization effects in experimental design&lt;/a>;, which, for example, may cause recommendations to随着时间的推移而漂移。 &lt;/p>; &lt;p>; The Chrome Privacy Sandbox, a multi-year collaboration between Google Research and Chrome, has publicly launched several APIs, including for &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#protected-audience&quot;>;Protected Audience&lt;/a>;, &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#topics&quot;>;Topics&lt;/a>;, and &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#attribution-reporting&quot;>;Attribution Reporting&lt;/a>;. This is a major step in protecting user privacy while supporting the open and free web ecosystem. These efforts have been facilitated by fundamental research on &lt;a href=&quot;https://arxiv.org/abs/2304.07210&quot;>;re-identification risk&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2301.05605&quot;>;private streaming computation&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot;>;optimization&lt;/a>; of privacy caps and budgets, &lt;a href=&quot;https://arxiv.org/pdf/2308.13510.pdf&quot;>;hierarchical aggregation&lt;/a>;, and training models with &lt;a href=&quot;https://arxiv.org/pdf/2312.05659.pdf&quot;>;label privacy&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Science and society&lt;/h2>; &lt;p>; In the not too distant future, there is a very real possibility that AI applied to scientific problems can accelerate the rate of discovery in certain domains by 10× or 100×, or more, and lead to major advances in diverse areas including bioengineering, &lt;a href=&quot;https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning&quot;>;materials science&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/&quot;>;weather prediction&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;climate forecasting&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot;>;neuroscience&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/an-ml-based-approach-to-better.html&quot;>;genetic medicine&lt;/a>;, and &lt;a href=&quot;https://health.google/health-research/publications/&quot;>;healthcare&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Sustainability and climate change &lt;/h3>; &lt;p>; In &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-reduce-greenhouse-emissions-project-greenlight/&quot;>;Project Green Light&lt;/a>;, we partnered with 13 cities around the world to help improve traffic flow at intersections and reduce stop-and-go emissions. Early numbers from these partnerships indicate a potential for up to 30% reduction in stops and up to 10% reduction in emissions. &lt;/p>; &lt;p>; In our &lt;a href=&quot;https://sites.research.google/contrails/&quot;>;contrails work&lt;/a>;, we analyzed large-scale weather data, historical satellite images, and past flights 。 We &lt;a href=&quot;https://blog.google/technology/ai/ai-airlines-contrails-climate-change/&quot;>;trained an AI model&lt;/a>; to predict where contrails form and reroute airplanes accordingly. In partnership with American Airlines and Breakthrough Energy, we used this system to demonstrate contrail reduction by 54%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s957/image14.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Contrails detected over the United States using AI and GOES-16 satellite imagery.&lt;/em>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; We are also developing novel technology-driven approaches to &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot; >;help communities with the effects of climate change&lt;/a>;. For example, we have &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;expanded our flood forecasting coverage to 80 countries&lt;/a>;, which directly impacts more than 460 million people. We have initiated a &lt;a href=&quot;https://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot;>;number of research efforts&lt;/a>; to help mitigate the increasing danger of wildfires, including &lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;real-time tracking of wildfire boundaries&lt;/a>; using satellite imagery, and work that &lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot;>;improves emergency evacuation plans&lt;/a>; for communities at risk to rapidly-spreading wildfires. Our &lt;a href=&quot;https://www.americanforests.org/article/american-forests-unveils-updates-for-tree-equity-score-tool-to-address-climate-justice/&quot;>;partnership&lt;/a>; with American Forests puts data from our &lt;a href=&quot;https://insights.sustainability.google/places/ChIJVTPokywQkFQRmtVEaUZlJRA/trees?hl=en-US&quot;>;Tree Canopy&lt;/a>; project to work in their &lt;a href=&quot;https://treeequityscore.org/&quot;>;Tree Equity Score&lt;/a>; platform, helping communities identify and address unequal access to trees.&lt;/p>; &lt;p>; Finally, we continued to develop better models for weather prediction at longer time horizons. Improving on &lt;a href=&quot;https://blog.research.google/2020/03/a-neural-weather-model-for-eight-hour.html&quot;>;MetNet&lt;/a>; and &lt;a href=&quot;https ://blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;MetNet-2&lt;/a>;, in this year&#39;s work on &lt;a href=&quot;https: //blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot;>;MetNet-3&lt;/a>;, we now outperform traditional numerical weather simulations up to twenty-four hours 。 In the area of medium-term, global weather forecasting, our work on &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/&quot;>;GraphCast&lt;/a>; showed significantly better prediction accuracy for up to 10 days compared to &lt;a href=&quot;https://en.wikipedia.org/wiki/Integrated_Forecast_System&quot;>;HRES&lt;/a>;, the most accurate operational deterministic forecast, produced by the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&lt;/a>; (ECMWF). In collaboration with ECMWF, we released &lt;a href=&quot;https://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot;>;WeatherBench-2&lt;/a>;, a benchmark for evaluating the accuracy of weather forecasts in a common framework. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/Q6fOlW-Y_Ss&quot; width=&quot;640&quot; youtube-src-id=&quot;Q6fOlW-Y_Ss&quot;>;&lt;/iframe>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A selection of GraphCast&#39;s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Health and the life sciences&lt;/h3>; &lt;p>; The potential of AI to dramatically improve processes in healthcare is significant. Our initial &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06291-2&quot;>;Med-PaLM&lt;/a>; model was the first model capable of achieving a passing score on the US medical执照考试。 Our more recent &lt;a href=&quot;https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/&quot;>;Med-PaLM 2 model&lt;/a>; improved by a further 19%, achieving an expert-level accuracy of 86.5%. These &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM models&lt;/a>; are language-based, enable clinicians to ask questions and have a dialogue about complex medical conditions, and are &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/introducing-medlm-for-the-healthcare-industry&quot;>;available&lt;/a>; to healthcare organizations as part of &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview&quot;>;MedLM&lt;/a>; through Google Cloud. &lt;/p>; &lt;p>; In the same way our general language models are evolving to handle multiple modalities, we have recently shown research on a &lt;a href=&quot;https://blog.research.google/2023/08/multimodal-medical-ai.html&quot;>;multimodal version of Med-PaLM&lt;/a>; capable of interpreting medical images, textual data, and other modalities, &lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;describing a path&lt;/a>; for how we can realize the exciting potential of AI models to help advance real-world clinical care. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s1600/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same model weights.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;We have also been working on &lt;a href=&quot;https://deepmind.google/discover/blog/codoc -developing-reliable-ai-tools-for-healthcare/&quot;>;how best to harness AI models in clinical workflows&lt;/a>;. We have shown that &lt;a href=&quot;https://blog.research.google/2023/03/learning-from-deep-learning-case-study.html&quot;>;coupling deep learning with interpretability methods&lt;/a>; can yield new insights for clinicians. We have also shown that self-supervised learning, with careful consideration of privacy, safety, fairness and ethics, &lt;a href=&quot;https://blog.research.google/2023/04/robust-and-efficient-medical-imaging.html&quot;>;can reduce the amount of de-identified data needed&lt;/a>; to train clinically relevant medical imaging models by 3×–100×, reducing the barriers to adoption of models in real clinical settings. We also released an &lt;a href=&quot;https://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot;>;open source mobile data collection platform&lt;/a>; for people with chronic disease to provide tools to the community to build their own studies.&lt;/p>; &lt;p>; AI systems can also discover completely new signals and biomarkers in existing forms of medical data. In work on &lt;a href=&quot;https://blog.research.google/2023/03/detecting-novel-systemic-biomarkers-in.html&quot;>;novel biomarkers discovered in retinal images&lt;/a>;, we demonstrated that a number of systemic biomarkers spanning several organ systems (eg, kidney, blood, liver) can be predicted from external eye photos. In other work, we showed that combining &lt;a href=&quot;https://blog.research.google/2023/04/developing-aging-clock-using-deep.html&quot;>;retinal images and genomic information&lt;/a>; helps identify some underlying factors of aging. &lt;/p>; &lt;p>; In the genomics space, we worked with 119 scientists across 60 institutions to create a &lt;a href=&quot;https://blog.research.google/2023/05/building-better-pangenomes-to-improve.html&quot;>;new map of the human genome&lt;/a>;, or pangenome. This more equitable pangenome better represents the genomic diversity of global populations. Building on our ground-breaking &lt;a href=&quot;https://www.nature.com/articles/s41586-021-03819-2&quot;>;AlphaFold&lt;/a>; work, our work on &lt;a href=&quot;https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/&quot;>;AlphaMissense&lt;/a>; this year provides a catalog of predictions for 89% of all 71 million possible &lt;a href=&quot;https://en.wikipedia.org/wiki/Missense_mutation&quot;>;missense variants&lt;/a>; as either likely pathogenic or likely benign. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s1070/image1 .jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1070&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Examples of AlphaMissense predictions overlaid on AlphaFold predicted structures (red – predicted as pathogenic; blue – predicted as benign; grey – uncertain). Red dots represent known pathogenic missense variants, blue dots represent known benign variants.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;HBB protein. Variants in this protein can cause sickle cell anaemia.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;CFTR protein. Variants in this protein can cause cystic fibrosis.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also shared &lt;a href=&quot;https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/&quot;>;an update&lt;/a>; on progress towards the next generation of AlphaFold. Our latest model can now generate predictions for nearly all molecules in the &lt;a href=&quot;https://www.wwpdb.org/&quot;>;Protein Data Bank&lt;/a>; (PDB), frequently reaching atomic accuracy. This unlocks new understanding and significantly improves accuracy in multiple key biomolecule classes, including ligands (small molecules), proteins, nucleic acids (DNA and RNA), and those containing post-translational modifications (PTMs).&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; On the neuroscience front, we &lt;a href=&quot;https://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot;>;announced a new collaboration&lt;/a>; with Harvard, Princeton, the NIH, and others to map an entire mouse brain at synaptic resolution, beginning with a first phase that will focus on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hippocampal_formation&quot;>;hippocampal formation&lt;/a>; — the area of the brain responsible for memory formation, spatial navigation, and other important functions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Quantum computing&lt;/h3>; &lt;p>; Quantum computers have the potential to solve big, real-world problems across science and industry. But to realize that potential, they must be significantly larger than they are today, and they must reliably perform tasks that cannot be performed on classical computers. &lt;/p>; &lt;p>; This year, we took an important step towards the development of a large-scale, useful quantum computer. Our breakthrough is the first demonstration of &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;quantum error correction&lt;/a>;, showing that it&#39;s possible to reduce errors while also increasing the number of qubits. To enable real-world applications, these qubit building blocks must perform more reliably, lowering the error rate from ~1 in 10&lt;sup>;3&lt;/sup>; typically seen today, to ~1 in 10&lt;sup>;8&lt;/sup>; 。 &lt;/p>; &lt;br />; &lt;h2>;Responsible AI research&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Design for Responsibility &lt;/h3>; &lt;p>; Generative AI is having a transformative impact in a wide range of fields including healthcare, education, security, energy, transportation, manufacturing, and entertainment. Given these advances, the importance of designing technologies consistent with our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; remains a top priority. We also recently published case studies of &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;emerging practices in society-centered AI&lt;/a>; 。 And in our annual &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf&quot; target=&quot;_blank&quot;>;AI Principles Progress Update&lt;/a>;, we offer details on how our Responsible AI research is integrated into products and risk management processes. &lt;/p>; &lt;p>; Proactive design for Responsible AI begins with identifying and documenting potential harms. For example, we recently &lt;a href=&quot;https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/&quot;>;introduced&lt;/a>; a &lt;a href=&quot;https://arxiv.org/abs/2310.11986&quot;>;three-layered&lt;/a>; context-based framework for comprehensively evaluating the social and ethical risks of AI systems. During model design, harms can be mitigated with the use of &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;responsible datasets&lt;/a>; 。 &lt;/p>; &lt;p>; We are &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with Howard University&lt;/a>; to build high quality African-American English (AAE) datasets to improve our products and make them work well for more people. Our research on &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3593013.3594016&quot;>;globally inclusive cultural representation&lt;/a>; and our publication of the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone scale&lt;/a>; furthers our commitments to equitable representation of all people. The insights we gain and techniques we develop not only help us improve our own models, they also power &lt;a href=&quot;https://blog.google/intl/en-in/company-news/using-ai-to-study-demographic-representation-in-indian-tv/&quot;>;large-scale studies of representation in popular media&lt;/a>; to inform and inspire more inclusive content creation around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s1196/image9.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Monk Skin Tone (MST) Scale. See more at&amp;nbsp;&lt;a href=&quot;http://skintone.google/&quot;>;skintone.google&lt;/a>;.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With advances in generative image models, &lt;a href=&quot;https://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot;>;fair and inclusive representation of people&lt;/a>; remains a top priority. In the development pipeline, we are working to &lt;a href=&quot;https://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot;>;amplify underrepresented voices and to better integrate social context knowledge&lt;/a>;. We proactively address potential harms and bias using &lt;a href=&quot;https://arxiv.org/pdf/2306.06135.pdf&quot;>;classifiers and filters&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2311.17259.pdf&quot;>;careful dataset analysis&lt;/a>;, and in-model mitigations such as fine-tuning, &lt;a href=&quot;https://arxiv.org/abs/2310.16523&quot;>;reasoning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2306.14308&quot;>;few-shot prompting&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2310.16959&quot;>;data augmentation&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2310.17022&quot;>;controlled decoding&lt;/a>;, and our research showed that generative AI enables &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;higher quality safety classifiers&lt;/a>; to be developed with far less data. We also released &lt;a href=&quot;https://developers.googleblog.com/2023/10/make-with-makersuite-part-2-tuning-llms.html&quot;>;a powerful way to better tune models with less data&lt;/a>; giving developers more control of responsibility challenges in generative AI.&lt;/p>; &lt;p>; We have developed new&lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>; state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements in model accuracy. &lt;/p>; &lt;p>; We initiated several efforts to improve safety and transparency about online content. For example, we introduced &lt;a href=&quot;https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/&quot;>;SynthID&lt;/a>;, a tool for watermarking and identifying AI-生成的图像。 SynthID is imperceptible to the human eye, doesn&#39;t compromise image quality, and allows the watermark to remain detectable, even after modifications like adding filters, changing colors, and saving with various lossy compression schemes. &lt;/p>; &lt;p>; We also launched &lt;a href=&quot;https://blog.google/products/search/google-search-new-fact-checking-features/&quot;>;About This Image&lt;/a>; to help people assess the credibility of images, showing information like an image&#39;s history, how it&#39;s used on other pages, and available metadata about an image. And we &lt;a href=&quot;https://arxiv.org/abs/2210.03535&quot;>;explored safety methods&lt;/a>; that have been developed in other fields, learning from established situations where there is low-risk tolerance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s616/image6 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;346&quot; data-original-width=&quot;616&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;SynthID generates an imperceptible digital watermark for AI-generated images.&lt;/em>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Privacy remains an essential aspect of our commitment to Responsible AI. We continued improving our state-of-the-art privacy preserving learning algorithm &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, developed the DP-Alternating Minimization algorithm (&lt;a href=&quot;https://arxiv.org/pdf/2310.15454.pdf&quot;>;DP-AM&lt;/a>;) to enable personalized recommendations with rigorous privacy protection, and defined a new &lt;a href=&quot;https://blog.research.google/2023/09/differentially-private-median-and-more.html&quot;>;general paradigm&lt;/a>; to reduce the privacy costs for many aggregation and learning tasks. We also proposed a scheme for &lt;a href=&quot;https://openreview.net/pdf?id=q15zG9CHi8&quot;>;auditing differentially private machine learning systems&lt;/a>;.&lt;/p>; &lt;p>; On the applications front we demonstrated that &lt;a href=&quot;https://arxiv.org/pdf/2308.10888.pdf&quot;>;DP-SGD offers a practical solution&lt;/a>; in the large model fine-tuning regime and showed that images generated by DP diffusion models are &lt;a href=&quot;https://arxiv.org/pdf/2302.13861.pdf&quot;>;useful for a range of downstream tasks&lt;/a>;. We &lt;a href=&quot;https://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot;>;proposed&lt;/a>; a new algorithm for DP training of large embedding models that provides efficient training on TPUs without compromising accuracy. &lt;/p>; &lt;p>; We also teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>; to address the scenario in which training images are forgotten to protect the privacy or rights of individuals. We shared a mechanism for &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;extractable memorization&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;participatory systems&lt;/a>; that give users more control over their sensitive data. &lt;/p>; &lt;p>; We continued to expand the world&#39;s largest corpus of atypical speech recordings to &amp;gt;1M utterances in &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/ a>;, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/ a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize atypical speech by 37%&lt;/a>; on real - 世界基准。 &lt;/p>; &lt;p>; We also built an &lt;a href=&quot;https://blog.research.google/2023/08/study-socially-aware-temporally-causal.html&quot;>;audiobook recommendation system&lt;/a>; for students with reading disabilities such as dyslexia. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Adversarial testing&lt;/h3>; &lt;p>; Our work in adversarial testing &lt;a href=&quot;https://blog.research.google/2023/03/responsible-ai-at-google-research.html&quot;>;engaged community voices&lt;/a>; from historically marginalized communities. We partnered with groups such as the &lt;a href=&quot;https://arxiv.org/abs/2303.08177&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR) to ensure we represent the diverse communities who use our models and &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;engage with external users&lt;/a>; to identify potential harms in generative model outputs. &lt;/p>; &lt;p>; We &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;established a dedicated Google AI Red Team&lt;/a>; focused on testing AI models and products for security, privacy, and abuse risks. We showed that attacks such as “&lt;a href=&quot;https://arxiv.org/pdf/2302.10149.pdf?isApp=1&quot;>;poisoning&lt;/a>;” or &lt;a href=&quot;https://arxiv.org/pdf/2306.15447.pdf&quot;>;adversarial examples&lt;/a>; can be applied to production models and surface additional risks such as memorization in both &lt;a href=&quot;https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf&quot;>;image&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;text generative models&lt;/a>;. We also demonstrated that defending against such attacks can be challenging, as merely applying defenses can cause other &lt;a href=&quot;https://arxiv.org/pdf/2309.05610.pdf&quot;>;security and privacy leakages&lt;/a>;. We also introduced model evaluation for &lt;a href=&quot;https://arxiv.org/abs/2305.15324&quot;>;extreme risks&lt;/a>;, such as offensive cyber capabilities or strong manipulation skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Democratizing AI though tools and education&lt;/h3>; &lt;p>; As we advance the state-of-the-art in ML and AI, we also want to ensure people can understand and apply AI to specific problems. We released &lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; (now &lt;a href=&quot;https://makersuite.google.com&quot;>;Google AI Studio&lt;/a>;), a web-based tool that enables AI developers to quickly iterate and build lightweight AI-powered apps. To help AI engineers better understand and debug AI, we released &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;LIT 1.0&lt;/a>;, a state-of-the-art, open-source debugger for machine learning models. &lt;/p>; &lt;p>; &lt;a href=&quot;https://colab.google/&quot;>;Colab&lt;/a>;, our tool that helps developers and students access powerful computing resources right in their web browser, reached over 10 million users 。 We&#39;ve just added &lt;a href=&quot;https://blog.google/technology/ai/democratizing-access-to-ai-enabled-coding-with-colab/&quot;>;AI-powered code assistance&lt;/a>; to all users at no cost — making Colab an even more helpful and integrated experience in data and ML workflows. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s844 /image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;844&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;One of the most used features is “Explain error” — whenever the user encounters an execution error in Colab, the code assistance model provides an explanation along with a potential fix.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To ensure AI produces accurate knowledge when put to use, we also recently introduced &lt;a href=&quot;https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/&quot;>;FunSearch&lt; /a>;, a new approach that generates verifiably true knowledge in mathematical sciences using evolutionary methods and large language models.&lt;/p>; &lt;p>; For AI engineers and product designers, we&#39;re updating the &lt;a href=&quot;https:/ /pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>; with generative AI best practices, and we continue to design &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, which includes &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Community engagement&lt;/h3>; &lt;p>; We continue to advance the fields of AI and computer science by publishing much of our work and participating in and organizing conferences. We have published more than 500 papers so far this year, and have strong presences at conferences like ICML (see the &lt;a href=&quot;https://blog.research.google/2023/07/google-at-icml-2023.html&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/&quot;>;Google DeepMind&lt;/a>; posts), ICLR (&lt;a href=&quot;https://blog.research.google/2023/04/google-at-iclr-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/&quot;>;Google DeepMind&lt;/a>;), NeurIPS (&lt;a href=&quot;https://blog.research.google/2023/12/google-at-neurips-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/&quot;>;Google DeepMind&lt;/a>;), &lt;a href=&quot;https://blog.research.google/2023/10/google-at-iccv-2023.html&quot;>;ICCV&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/06/google-at-cvpr-2023.html&quot;>;CVPR&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/07/google-at-acl-2023.html&quot;>;ACL&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/google-at-chi-2023.html&quot;>;CHI&lt;/a>;, and &lt;a href=&quot;https://blog.research.google/2023/08/google-at-interspeech-2023.html&quot;>;Interspeech&lt;/a>;. We are also working to support researchers around the world, participating in events like the &lt;a href=&quot;https://deeplearningindaba.com/2023/google-outreach-mentorship-programme/&quot;>;Deep Learning Indaba&lt;/a>;, &lt;a href=&quot;https://khipu.ai/khipu2023/khipu-2023-speakers2023/&quot;>;Khipu&lt;/a>;, supporting &lt;a href=&quot;https://blog.google/around-the-globe/google-latin-america/phd-fellowship-research-latin-america/&quot;>;PhD Fellowships in Latin America&lt;/a>;, and more.我们还与来自33个学术实验室的合作伙伴合作，汇总22种不同机器人类型的数据，并创建&lt;a href =“ https://deepmind.google/discover/discover/blog/scaling-up-up-learning-learning-learning-across-many-different-robot--robot -Types/“>;打开X-Embodiment数据集和RT-X模型&lt;/a>;更好地推进负责任的AI开发。 &lt;/p>; &lt;p>; Google率先开发了&lt;a href=&quot;https://mlcommons.org/working-groups/ai-safety/ai-safety/&quot;>; ai安全基准&lt;/a >;在&lt;a href=&quot;https://mlcommons.org/&quot;>; mlCommons &lt;/a>;标准组织中，来自生成AI空间中的几位主要参与者的标准组织，包括Openai，Anthropic，Microsoft，Microsoft，Meta，Hugging Face等。与行业中的其他人一起，我们还&lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-apenai-anthropic-frontier-model-model-model-model-model-model-forum/&quot;>; a>; &lt;a href=&quot;https://www.frontiermodelforum.org/&quot;>; Frontier模型论坛&lt;/a>;（FMF），该论坛的重点是确保Frontier AI模型的安全和负责任的开发。 With our FMF partners and other philanthropic organizations, we launched a $10 million &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum - 执行导演/“>; AI安全基金&lt;/a>;促进对社会工具持续开发的研究，以有效测试和评估最有能力的AI模型。 &lt;/p>; &lt;p>;与&lt;a href=&quot;http://google.org&quot;>; Google.org &lt;/a>;，我们&lt;a href =“ https://blog.google/technology/technology/technology/ai /google-ai-data-un-global-goals/“>;与联合国合作&lt;/a>;建造&lt;a href=&quot;https://unstats.un.org/unsdwebsite/unsdwebsite/undatacommons/undatacommons/sdgs&quot;>; un可持续发展目标的数据共享&lt;/a>;，该工具可以跟踪17 &lt;a href=&quot;https://sdgs.un.org/goals&quot;>;可持续发展目标&lt;/a>;和&lt;href &lt;a href =“ https://globalgoals.withgoogle.com/globalgoals/supported-organizations”>;支持的项目&lt;/a>;来自非政府组织，学术机构和社会企业，&lt;a href =“倡议/google-org/httpsbloggoogletreach-initiativesgoogle-orgunited-nations-global-goals-google-google-ai-/“>;使用AI来加速SDGS上的进度&lt;/a>;。 &lt;/p>; &lt;p>;这篇文章中突出的项目是我们在过去一年所做的研究工作的一小部分。在&lt;a href=&quot;https://blog.research.google/&quot;>; Google Research &lt;/a>;和&lt;a href=&quot;https://deepmind.google/discover/discover/blog/&quot;>; Google DeepMind上找到更多信息。 &lt;/a>;博客，以及我们的&lt;a href=&quot;https://research.google/pubs/&quot;>;出版物列表&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;未来愿景&lt;/h2>; &lt;p>;随着多模型模型变得更加有能力，他们将使人们能够在从科学到教育到全新知识领域的领域取得令人难以置信的进步。 &lt;/p>; &lt;p>;进步持续发展，随着年的进步，我们的产品和研究进步也将为AI找到更多有趣的创意用途。 &lt;/p>; &lt;p>;结束今年的审视，我们从&lt;em>; &lt;a href =“ https://ai.google/static/documents/google-why-we-we-focus-- on-ai.pdf“>;我们为什么要专注于AI（以及到底）&lt;/a>; &lt;/em>;：&lt;/p>; &lt;div style =“ margin-left：40px;”>; &lt;p>;如果要大胆地负责任地，我们认为AI可以成为改变各地人民生活的基础技术 - 这就是激动我们的！态;“”>;今年审视在&lt;a href=&quot;https://blog.creachearch.google/2023/2023 Yar.2023 are-of-of-groundbreaking-adbreaking-Advances-in.html&quot;>; Google Research blo &lt;/a>; g和&lt;a href=&quot;https://deepmind.google/discover/blog/2023-a arear of-of-of-of-of-groundbreaking-Advances-invances-in-ai-and-computing/&quot;>; google DeepMind Blog &lt;/a>;。&lt;/span>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/881618347347347385638131/comments/comments/comments/default/default”发表注释“ type =”应用程序/atom+xml”/>; &lt;link href =” http://blog.research.google/2023/2023 Yarial-of-ground-groundbreaking-rakearking-abrbreaking-advances-in.html#comment-form “ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/88161618161834733855638563813813813131&#39;REL =“” “ type =”应用程序/ATOM+XML“/>; &lt;link href =” http://www.blogger.com/feeds/8474926333145202626/posts/posts/posts/default/88161618161816183473434338563856381381381381381381381381381381381 rel =“ “/>; &lt;link href =” http://blog.research.google/2023/2023 Yar.-of-groundbreaking-andvances-in.html” AI和计算中的进步“ type =” text/html“/>; &lt;unal>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile.com/profile/1209862651477775266161 &lt;/uri>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blog1.blogblog.com/ img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/wustr>; &lt;媒体：缩略图高度=“ 72” url =“ https://blogger.googger.googleusercontent.com/img/img/b/ R29vZ2xl/AVvXsEiU12G_p2DZO4gQ-P95aP2IFvtKRHRG5Oik9VzJ4WtT_VznggjUrL4tTzqto-C8yx6ULgvugKgH9usiZxnKGk97pOFyHWnu-1S4sSbR2Jjq5T36tQRbZucTAP7gmXkGw77xN6s39IKxPaxbo5tw_Cq52ZfPWOCBkWL-2XTuzsIh6viRIqgGcdUdNq-pHjzl/s72-c/year_in_review-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/媒体：缩略图>; &lt;thr：THR>; 0 &lt;/thr：Total>; &lt;/entry>; &lt;entry>; &lt;ID>;标签：Blogger.com，1999年：Blog-8474926331452026626.POST-5516200707034946636207 12-19T13：08：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-01-12T11：04：04：04.629-08：00 &lt;/updateed>; &lt;category scheme =“ http：///www.blogger.com/ atom/ns＃“ term =“大语言模型”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“ Machine Learning”>; &lt;/category>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” term =“ video”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term = “视觉研究”>; &lt;/category>; &lt;title type =“ text”>; videopoet：零摄影视频生成的大语言模型&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author” >;Posted by Dan Kondratyuk and David Ross, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s1600/videopoetpreview .gif“ style =” display：none;” />; &lt;p>;最近的视频生成模型已经突破了现场，在许多情况下，展示了令人惊叹的风景如画的质量。当前视频生成的瓶颈之一是产生连贯大运动的能力。在许多情况下，即使是当前领先的模型也会产生较小的运动，或者当产生较大的运动时，会表现出明显的伪影。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;要探索语言模型在视频生成中的应用，我们介绍Videopoet（&lt;a href =“ http://sites.research.google/videopoet “>;网站&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2312.14125&quot;>;研究论文&lt;/a>;），一种大型语言模型（LLM），能够使用各种视频生成任务，包括文本到视频，图像到视频，视频风格，视频&lt;a href=&quot;https://en.wikipedia.org/wiki/inpainting&quot;>; Inpainting &lt;/a>;和&lt;a href = a href = “ https://paperswithcode.com/task/image-ustpainting&quot;>; outpainting &lt;/a>;和Video-to-to-to-audio。一个值得注意的观察是，领先的视频生成模型几乎完全基于扩散（例如，请参见&lt;a href=&quot;https://imagen.research.google/video/video/&quot;>; Imagen Video &lt;/a>;）。另一方面，由于其在各种模式中的出色学习能力，包括语言，代码和音频（例如，&lt;a href =&#39;https：// https：// Google-Research.github.io/seanet/audiopalm/examples/&quot;>; audiopalm &lt;/a>;）。与该领域的替代模型相比，我们的方法将许多视频生成功能无缝集成在单个法学硕士中，而不是依赖于专门针对每个任务的单独训练的组件。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;概述&lt;/h2>; &lt;p>;下图显示了Videopoet的功能。输入图像可以被动画化以产生运动，并且（可以选择裁剪或遮罩）视频可以被编辑以进行修复或修复。对于风格化，该模型接收代表深度和光流（代表运动）的视频，并在顶部绘制内容以产生文本引导的风格。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s1999 /Image7.png“ style =”左右左右：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 682” data-Original-width =“ 1999” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; videopoet的概述，能够在各种以视频为中心的输入和输出上进行多任务处理。法学硕士可以选择将文本作为输入来指导文本到视频、图像到视频、视频到音频、风格化和绘画任务的生成。使用的资源：&lt;a href=&quot;https://commons.wikimedia.org/wiki/main_page&quot;>; wikimedia commons &lt;/a>;和&lt;a href=&quot;https://davischallenge.org/davis &lt;/a>; 。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;语言模型作为视频生成器&lt;/h2 >; &lt;p>;使用LLM进行培训的一个主要优点是，可以重用现有LLM培训基础设施中引入的许多可扩展效率提高。然而，法学硕士在离散令牌上运行，这可能使视频生成具有挑战性。幸运的是，存在&lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;视频&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>; audio &lt;/a>;用于编码视频和音频剪辑的Tokenizers作为离散令牌的序列（即，整数索引），并且也可以将其转换回原始表示形式。 &lt;/p>; &lt;p>; videopoet训练&lt;a href=&quot;https://en.wikipedia.org/wiki/autoregresceression_model&quot;>;自动性语言模型&lt;/a>;以通过视频，图像，音频和文本方式学习使用多个tokenizer（&lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>; magvit v2 &lt;/a>;用于视频和图像，&lt;a href =“ https://arxiv.org.org.org.org.org.org.org /abs/2107.03312&quot;>; soundstream &lt;/a>;用于音频）。一旦模型生成以某些上下文为条件的标记，就可以使用标记器解码器将它们转换回可查看的表示形式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s2680 /Image21.png“ style =” Margin-Left：auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 824” data-Original-width =“ 2680” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s16000/image21.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption”样式=“ text-align：center;”>;详细介绍了Videopoet任务设计，显示了各种任务的培训和推理输入和输出。使用标记器编码器和解码器将模态与标记进行转换。每种方式都被边界令牌包围，任务令牌表示要执行的任务类型。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/div>; &lt;h2>; videopoet生成的示例&lt;/h2>; &lt;p>;我们的模型生成的一些示例如下所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s1100/image18.gif “样式=” Margin-Left：Auto; Margin-Right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 963” data-Original-width =“ 1100” 1100“ src =” https：/// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s16000/image18.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“ style =” text-align：center;“>;视频中从各种文本提示中生成的视频。有关特定的文本提示，请参阅&lt;a href=&quot;http://sites.research.google/videopoet&quot;>;网站&lt;/a>;。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br/ >; &lt;p>;对于文本到视频，视频输出是可变的长度，可以根据文本内容应用一系列动作和样式。为了确保负责任的做法，我们参考公共领域中的艺术品和样式-container“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：right;“>; &lt;b>; text Input &lt;/b>; &lt;/td>; &lt;td>; &lt;td>;＆nbsp;＆nbsp;＆nbsp; &lt;/td>; &lt;td>; &lt;td>; &lt;td>; =“ center” width =“ 20％”>; &lt;em>;“时代广场上的浣熊跳舞” &lt;/em>; &lt;/td>; &lt;td>;＆nbsp;＆nbsp; &lt;/td>; &lt;td>; &lt;td align =“中心” width =“ “ 20％”>; &lt;em>;“一匹马在van-gogh的&#39;starry Night&#39;&#39;&#39; 20％“>; &lt;em>;”“两个熊猫扑克牌” &lt;/em>; &lt;/td>; &lt;td>;＆nbsp;＆nbsp; &lt;/td>; &lt;td align =“中心” width =“ 20％”>; &lt;em>;“大量的爆炸式飞溅彩虹油漆，带有苹果出现的8k” &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;tr>; &lt;td style =“ text-align：right; comight;”>; &lt;b>; &lt;b>;视频输出&lt; /b>; &lt;/td>; &lt;td>;＆nbsp;＆nbsp; &lt;/td>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/img/b/ r29vz2xl/avvxsehfnvzu_hzvgt7xcva4adbprfzwpov_b8xs6q54no72b88fgthfk7ehfr3uj9acnac0txnac0txnac0txxi6vxi6vxi6vxi6vxi6vxi6c2i8hqwznwws2 vernmiabi yia yiarybiia yiarybiia yia ia y.bhiia和2WQZMTTOBSKIHB0RJIFSK_C7VZBQ685OPUY_UQPC5NJGXQKS3AWJOG-GQGK0V/s448/image6.gif“ style =” Margin-Left：auto;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 256” r29vz2xl/avvxsehfnvzu_hzvgt7xcva4adbprfzwpov_b8xs6q54no72b88fgthfk7ehfr3uj9acnac0txnac0txnac0txxi6vxi6vxi6vxi6vxi6vxi6c2i8hqwznwws2 vernmiabi yia yiarybiia yiarybiia yia ia y.bhiia和2WQZMTTOBSKIHB0RJIFSK_C7VZBQ685OPUY_UQPC5NJGXQKS3AWJOG-GQGK0V/S16000/IMAGE6.GIF“/>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/>; &lt;/>; &lt;/>; &lt;/td>; &lt;td>; &lt;td>; &lt;td>; &lt;td>; &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s448/image11.gif&quot; style=&quot;margin-left: auto;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 256” R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https ：//blogger.googleusercontent.com/img/b/r29vz2xl/avvxsei4yqm_nd4mtu-3d_adfxkkax453hyphenhyphenxe9e9e9e7ehbzz9rvbozzfboizwxfboboizwxfboboizwxf_5flw4xacu3xacu3x1autw4wrck.pkwrcwrcwrcwrc.phig>; k.->; k.->; k.->; yk yk yk yk yk yk yk yk yk yk yk yk yk yk yk yk yk yk yk yk>; yk>; yk>;>; TXDWIWSMIQ59NDH3AXWD0UOIFA8C2AQ0XNSGCV1NSGCV1FC9MXFVISZTG8Z7GPCAKGQTN5HOBBJ-PKS8II1TOUDAR8/s448/s448/image12.gif&#39;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 256” R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453hyphenhyphenXE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxdwiWsMiQ59NDH3axwd0UOIFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s16000/image12.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s448/image17.gif&quot; style=&quot;margin-left: auto;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 256” R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s16000/image17.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;For image-to-video, VideOpoet可以获取输入映像并使用提示。&lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-left：margin-left：” margin-left：汽车; Margin-Right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https：//blogger.googleuserercercontent.com/img/img/r29vz2xxl/r29vz2xl/avvz2xl/avvxsej5kaqccs7gjojr0m_8hmm_8hmm_8hmsysq- Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s1536/image13.gif&quot; style=&quot;margin-left: auto;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 1536” src =“ https://blogger.googleusercontent.com/img/img/b/b/ R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq-Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s16000/image13.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style =“ text-align：center;”>;带有文本提示的图像到视频的一个示例，以指导运动。每个视频都与左图配对。&lt;strong>;左>;左>;：“船在帆布上浏览海洋，雷暴和闪电，动画油”。甘蔗在大风天下俯视下面的漩涡状海雾”。 。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;用于视频样式化，我们在用一些附加的输入文本中进食videopoet之前预测了光流和深度信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s1536/image16 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s16000/image16.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption”样式=“ text-align：center;”>;视频风格化的示例在视频上，文本到视频生成的视频，带有文本提示，深度和光流作为调节。每对中的左侧视频是输入视频，右侧是风格化输出。 &lt;b>;左&lt;/b>;：“袋鼠戴着太阳镜在阳光明媚的海滩上拿着沙滩球。” &lt;b>;中间&lt;/b>;：“泰迪熊在水晶透明的冷冻湖上滑冰。” &lt;b>;右&lt;/b>;：“在伪造的光中咆哮的金属狮子。” &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>; videopoet也能够生成音频。在这里，我们首先从模型生成 2 秒的剪辑，然后尝试在没有任何文本指导的情况下预测音频。这使您可以从单个模型中生成视频和音频。&lt; /p>; &lt;br />; &lt;br />; &lt;br />; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr caption-container” >; &lt;tbody>; &lt;tr>; &lt;td>; &lt;video Controls =“ controls” playSinline =“” width =“ 100％”>; &lt;source src =“ https://storage.googleapis.com/videopoet/videopoet/videot/videos/105_drums_drums_with_with_with_with_with_with_with_mp4 “ type =“ video/mp4”>; &lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;＆nbsp;＆nbsp;＆nbsp; &lt;/td>; &lt;td>; &lt;video Controls =“ controls” playSinline =“ width =” 100 ％“>; &lt;source src =” https://storage.googleapis.com/videopoet/videos/107_cat_piano_with_with_audio.mp4“ type =“ video/mp4”>; &lt;/source>; &lt;/source>; &lt;/source>; &lt;/source>; &lt;/source>; &lt;/tive ;＆nbsp; &lt;/td>; &lt;td>; &lt;video Controls =“ controls” playSinline =“” width =“ 100％”>; &lt;source src =“ https：//storage.googleapis.com/videopoet/videopoet/videoet/videos/108_train_train_train_train_train_train_train_train_with_with_mp4 “ type =“ video/mp4”>; &lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;＆nbsp;＆nbsp;＆nbsp; &lt;/td>; &lt;td>; &lt;video Controls =“ controls” playSinline =“ width =” 100 %&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/104_dog_popcorn_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-Left：auto;边缘权利：自动;“>; &lt;tbody>; &lt;try>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;视频对审计的示例，从没有视频示例中生成音频任何文本输入。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;默认情况下，Videopoet模型在肖像方向上生成视频，以将其输出定制为短形式内容。为了展示其功能，我们，我们制作了一部简短的电影，由Videopoet生成的许多简短片段组成。对于剧本，我们问&lt;a href=&quot;https://bard.google.com/&quot;>; bard &lt;/a>;写一个关于旅行的短篇小说浣熊带有场景分解和随附的提示列表。然后，我们为每个提示生成了视频剪辑，并将所有结果拼接在一起，以制作下面的最终视频。&lt; /p>; &lt;br />; &lt;br />; &lt;div class =“ saparator” style =“ clear：两者; text-align：center;“>; &lt;iframe allayfullscreen =” class =“ blog_video_class” frameborder =“ 0” heptiv =“ 360” src =“ https：//wwwww.youtube.com/embed.com/embed/70wzkfx6ylk “ youtube-src-id =” 70wzkfx6ylk“>; &lt;/iframe>; &lt;/div>; &lt;br />; &lt;p>;当我们开发videopoet时，我们注意到模型功能的一些不错的属性，我们在下面突出显示。&lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;长视频&lt;/h3>; &lt;p>;我们能够仅通过在最后1秒的条件下生成更长的视频视频并预测接下来的1秒。通过反复链接它，我们表明该模型不仅可以很好地扩展视频，而且还可以忠实地保留所有物体的外观，即使在几个迭代中也可以保留所有对象的外观。&lt;/p>; &lt;p>;这是两个示例videopoet从文本输入中生成长视频：&lt;br />; &lt;br />; &lt;br />; &lt; /p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr caption-container”>; &lt;tbody >; &lt;tr>; &lt;td style =“ text-align：right;”>; &lt;b>; text Input &lt;/b>; &lt;/td>; &lt;td>; &lt;td>;＆nbsp;＆nbsp;＆nbsp; &lt;/td>; &lt;td>; &lt;td align =“ center” width =“ width =” “ 35％”>; &lt;em>;“宇航员开始在火星上跳舞。然后在背景中爆炸五颜六色丛林中非常尖锐的精灵城市，有一条灿烂的蓝河，瀑布和陡峭的垂直悬崖面。 ＆nbsp;＆nbsp;＆nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style =“ text-align：recright;”>; &lt;b>;视频输出&lt;/b>; &lt;/td>; &lt;td>; &lt;td>; &lt;td>;＆nbsp;＆nbsp;＆nbsp;＆nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAIV6ZbjeXK5JPtQr8Mb_ZcPKiIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/ s448/image14.gif“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 448” data-Original-width =“ 256” src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAIV6ZbjeXK5JPtQr8Mb_ZcPKiIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp; &amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqWtNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/ s448/image9.gif“ style =”边距 - 左：auto;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 256” R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqWtNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/s16000/image9.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp ; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;p>;也可以交互编辑Videopoet生成的现有视频剪辑。如果我们提供输入视频，我们可以更改运动对象执行不同动作的对象。对象操作可以在第一帧或中间框架中居中，这允许高度编辑控制。&lt;/p>; &lt;p>;输入视频，然后选择所需的下一个剪辑。&lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; Margin-Right：auto;“>; &lt;tbody>; &lt;tr>; &lt;td style =” text-align：center;“>; &lt;a href =” https://blogger.googleusercentent.com/img/img/r29vz2xxl/r29vz2xl/ 8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s1280/image10.gif&quot; style=&quot;margin-left: auto;边缘权利：自动;”>; &lt;img border =“ 0” data-original-height =“ 448” data-eriginal-width =“ 1280” src =“ https://blogger.googleusercontent.com/img/img/b/ R29vZ2xl/AVvXsEg2NFzPadmS-8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;左侧的输入视频被用作调节，以产生四个选择，并在初步提示下：“可爱的生锈的破碎蒸汽朋克机器人被苔藓潮湿和崭露头角的植被覆盖，被高大的草包围。”我们展示了未提倡动议的情况。对于下面的列表中的最后一个视频，我们添加到提示：“在后台上用烟雾供电”以指导动作。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>;要编辑其对所需状态的内容，以文本提示为条件。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q -Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s512/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;512 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q-Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s16000/image5.gif&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animating a painting with different prompts. &lt;b>;Left&lt;/b>;: “A woman turning to look at the camera.” &lt;b>;Right&lt;/b>;: “A woman yawning.” **&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Camera motion&lt;/h3>; &lt;p>; We can also accurately control camera movements by appending the type of desired camera motion to the text prompt. As an example, we generated an image by our model with the prompt, &lt;em>;“Adventure game concept art of a sunrise over a snowy mountain by a crystal clear river”&lt;/em>;.下面的示例附加给定的文本后缀以应用所需的动作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s1536 /image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Prompts from left to right: “Zoom out”, “Dolly zoom”, “Pan left”, “Arc shot”, “Crane shot ”, “FPV drone shot”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation results&lt;/h2>; &lt;p>; We evaluate VideoPoet on text-to-video generation with a variety of benchmarks to compare the results to other approaches.为了确保中立的评估，我们在各种不同的提示下运行了所有模型，没有挑选示例，并要求人们对他们的偏好进行评分。下图以绿色突出显示了 VideoPoet 被选为以下问题的首选选项的时间百分比。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Text fidelity&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IfiFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IfiFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User preference ratings for text fidelity, ie, what percentage of videos are preferred in terms of accurately following a prompt.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion interestingness&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s1999/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;797&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User preference ratings for motion interestingness, ie, what percentage of videos are preferred in terms of producing interesting motion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Based on the above, on average people selected 24–35% of examples from VideoPoet as following prompts better than a competing model vs. 8–11% for competing models. Raters also preferred 41–54% of examples from VideoPoet for more interesting motion than 11–21% for other models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Through VideoPoet, we have demonstrated LLMs&#39; highly-competitive video generation quality across a wide variety of tasks, especially in producing interesting and high quality motions within videos.我们的结果表明法学硕士在视频生成领域的巨大潜力。对于未来的方向，我们的框架应该能够支持“任意到任意”的生成，例如，扩展到文本到音频、音频到视频和视频字幕等。 &lt;/p>; &lt;p>; To view more examples in original quality, see the &lt;a href=&quot;http://sites.research.google/videopoet&quot;>;website demo&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research has been supported by a large body of contributors, including Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, and Lu Jiang.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We give special thanks to Alex Siegman,Victor Gomes, and Brendan Jou for managing computing resources. We also give thanks to Aren Jansen, Marco Tagliasacchi, Neil Zeghidour, John Hershey for audio tokenization and processing, Angad Singh for storyboarding in “Rookie the Raccoon”, Cordelia Schmid for research discussions, David Salesin, Tomas Izo, and Rahul Sukthankar for their support, and Jay Yagnik as architect of the initial concept.&lt;/em>; &lt;/p>; &lt;br />; &lt;p>; &lt;em>;**&lt;/em>; &lt;br />; &lt;em>;(a) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Rembrandt_Christ_in_the_Storm_on_the_Lake_of_Galilee.jpg&quot;>;The Storm on the Sea of Galilee&lt;/a>;, by Rembrandt 1633, public domain.&lt;/em>; &lt;br />; &lt;em>;(b) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Pillars_of_creation_2014_HST_WFC3-UVIS_full-res.jpg&quot;>;Pillars of Creation&lt;/a>;, by NASA 2014, public domain.&lt;/em>; &lt;br />; &lt;em>;(c) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Caspar_David_Friedrich_-_Wanderer_above_the_Sea_of_Fog.jpeg&quot;>;Wanderer above the Sea of Fog&lt;/a>;, by Caspar David Friedrich, 1818, public domain&lt;/em>; &lt;br />; &lt;em>;(d) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg&quot;>;Mona Lisa&lt;/a>;, by Leonardo Da Vinci, 1503, public domain.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5516200703494636207/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5516200703494636207&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5516200703494636207&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPoet: A large language model for zero-shot video generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s72-c/videopoetpreview.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1587965303727576226&lt;/id>;&lt;published>;2023-12-19T08:01:00.000-08:00&lt;/published>;&lt;updated>;2023-12-19T08:15:57.101-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Simulations illuminate the path to post-event traffic flow&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yechen Li and Neha Arora, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s600/TrafficFlow.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Fifteen minutes. That&#39;s &lt;a href=&quot;https://colosseum.tours/interesting-facts#:~:text=THE%20COLOSSEUM%20COULD%20BE%20FILLED,in%20a%20matter%20of%20minutes.&quot;>;how long it took to empty the Colosseum&lt;/a>;, an engineering marvel that&#39;s still standing as the largest amphitheater in the world. Two thousand years later, this design continues to work well to move enormous crowds out of sporting and entertainment venues. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; But of course, exiting the arena is only the first step. Next, people must navigate the traffic that builds up in the surrounding streets. This is an age-old problem that remains unsolved to this day. In Rome, they addressed the issue by prohibiting private traffic on the street that passes directly by the Colosseum. This policy worked there, but what if you&#39;re not in Rome? What if you&#39;re at the Superbowl? Or at a Taylor Swift concert? &lt;/p>; &lt;p>; An approach to addressing this problem is to use simulation models, sometimes called &quot;digital twins&quot;, which are virtual replicas of real-world transportation networks that attempt to capture every detail from the layout of streets and intersections to the flow of vehicles. These models allow traffic experts to mitigate congestion, reduce accidents, and improve the experience of drivers, riders, and walkers alike. Previously, our team used these models to &lt;a href=&quot;https://arxiv.org/abs/2111.03426&quot;>;quantify sustainability impact of routing&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot;>;test evacuation plans&lt;/a>; and show simulated traffic in &lt;a href=&quot;https://blog.google/products/maps/google-maps-immersive-view-routes/&quot;>;Maps Immersive View&lt;/a>;. &lt;/p>; &lt;p>; Calibrating high-resolution traffic simulations to match the specific dynamics of a particular setting is a longstanding challenge in the field. The availability of aggregate mobility data, detailed Google Maps road network data, advances in transportation science (such as understanding the relationship &lt;a href=&quot;https://tristan2022.org/Papers/TRISTAN_2022_paper_3704.pdf&quot;>;between segment demands and speeds&lt;/a>; for road segments with traffic signals), and &lt;a href=&quot;https://research.google/pubs/pub52679/&quot;>;calibration techniques&lt;/a>; which make use of speed data in physics-informed traffic models are paving the way for compute-efficient optimization at a global scale. &lt;/p>; &lt;p>; To test this technology in the real world, Google Research partnered with the Seattle Department of Transportation (SDOT) to develop simulation-based traffic guidance plans. Our goal is to help thousands of attendees of major sports and entertainment events leave the stadium area quickly and safely. The proposed plan reduced average trip travel times by 7 minutes for vehicles leaving the stadium region during large events. We deployed it in collaboration with SDOT using Dynamic Message Signs (DMS) and verified impact over multiple events between August and November, 2023. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s1200/PreImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;518&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s16000/PreImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIcVEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s1200/PostImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;519&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIcVEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s16000/PostImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One policy recommendation we made was to divert traffic from S Spokane St, a major thoroughfare that connects the area to highways I-5 and SR 99, and is often congested after events. Suggested changes improved the flow of traffic through highways and arterial streets near the stadium, and reduced the length of vehicle queues that formed behind traffic signals. (Note that vehicles are larger than reality in this clip for demonstration.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Simulation model&lt;/h2>; &lt;p>; For this project, we created a new simulation model of the area around Seattle&#39;s stadiums. The intent for this model is to replay each traffic situation for a specified day as closely as possible. We use an open-source simulation software, &lt;a href=&quot;https://www.eclipse.org/sumo/&quot;>;Simulation of Urban MObility&lt;/a>; (SUMO). SUMO&#39;s behavioral models help us describe traffic dynamics, for instance, how drivers make decisions, like car-following, lane-changing and speed limit compliance. We also use insights from Google Maps to define the network&#39;s structure and various static segment attributes (eg, number of lanes, speed limit, presence of traffic lights). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s1601/SUMO.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s16000/SUMO.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Overview of the Simulation framework.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Travel demand is an important simulator input. To compute it, we first decompose the road network of a given metropolitan area into zones, specifically level 13 &lt;a href=&quot;http://s2geometry.io/devguide/s2cell_hierarchy.html&quot;>;S2 cells&lt;/a>; with 1.27 km&lt;sup>;2 &lt;/sup>;area per cell. From there, we define the travel demand as the expected number of trips that travel from an origin zone to a destination zone in a given time period. The demand is represented as aggregated origin–destination (OD) matrices. &lt;/p>; &lt;p>; To get the initial expected number of trips between an origin zone and a destination zone, we use aggregated and anonymized mobility statistics. Then we solve the OD calibration problem by combining initial demand with observed traffic statistics, like segment speeds, travel times and vehicular counts, to reproduce event scenarios. &lt;/p>; &lt;p>; We model the traffic around multiple past events in Seattle&#39;s T-Mobile Park and Lumen Field and evaluate the accuracy by computing aggregated and anonymized traffic statistics. Analyzing these event scenarios helps us understand the effect of different routing policies on congestion in the region. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_CqMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s1601/TrafficHeatMap.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;655&quot; data-original-width=&quot;1601&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_CqMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s16000/TrafficHeatMap.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Heatmaps demonstrate a substantial increase in numbers of trips in the region after a game as compared to the same time on a non-game day.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s1999/TrafficFlow.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;931&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s16000/TrafficFlow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;The graph shows observed segment speeds on the x-axis and simulated speeds on the y-axis for a modeled event. The concentration of data points along the red x=y line demonstrates the ability of the simulation to reproduce realistic traffic conditions.&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Routing policies&lt;/h2>; &lt;p>; SDOT and the Seattle Police Department&#39;s (SPD) local knowledge helped us determine the most congested routes that needed improvement: &lt; /p>; &lt;ul>; &lt;li>;Traffic from T-Mobile Park stadium parking lot&#39;s Edgar Martinez Dr. S exit to eastbound I-5 highway / westbound SR 99 highway &lt;/li>; &lt;li>;Traffic through Lumen Field stadium parking lot to northbound Cherry St. I-5 on-ramp &lt;/li>; &lt;li>;Traffic going southbound through Seattle&#39;s SODO neighborhood to S Spokane St. &lt;/li>; &lt;/ul>; &lt;p>; We developed routing policies and evaluated them using the simulation模型。 To disperse traffic faster, we tried policies that would route northbound/southbound traffic from the nearest ramps to further highway ramps, to shorten the wait times. We also experimented with opening HOV lanes to event traffic, recommending alternate routes (eg, SR 99), or load sharing between different lanes to get to the nearest stadium ramps. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s1600/NorthboundCherry.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;840&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s16000/NorthboundCherry.gif&quot; />;&lt;/a>; &lt;br />; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s1600/SouthboundSpokane.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;837&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s16000/SouthboundSpokane.gif&quot; />;&lt;/a>;&lt;/div>; &lt;h2>;Evaluation results&lt;/h2>; &lt;p>; We model multiple events with different traffic conditions, event times, and attendee counts. For each policy, the simulation reproduces post-game traffic and reports the travel time for vehicles, from departing the stadium to reaching their destination or leaving the Seattle SODO area. The time savings are computed as the difference of travel time before/after the policy, and are shown in the below table, per policy, for small and large events. We apply each policy to a percentage of traffic, and re-estimate the travel times. Results are shown if 10%, 30%, or 50% of vehicles are affected by a policy. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s1252/TrafficImprovement.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;916&quot; data-original-width=&quot;1252&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s16000/TrafficImprovement.png&quot; />;&lt;/a>;&lt;/div>; &lt;p>;Based on these simulation results, the feasibility of implementation, and other considerations, SDOT has decided to implement the “Northbound Cherry St ramp” and “Southbound S Spokane St ramp” policies using DMS during large events. The signs suggest drivers take alternative routes to reach their destinations. The combination of these two policies leads to an average of 7 minutes of travel time savings per vehicle, based on rerouting 30% of traffic during large events. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work demonstrates the power of simulations to model, identify, and quantify the effect of proposed traffic guidance policies. Simulations allow network planners to identify underused segments and evaluate the effects of different routing policies, leading to a better spatial distribution of traffic. The offline modeling and online testing show that our approach can reduce total travel time. Further improvements can be made by adding more traffic management strategies, such as optimizing traffic lights. Simulation models have been historically time consuming and hence affordable only for the largest cities and high stake projects. By investing in more scalable techniques, we hope to bring these models to more cities and use cases around the world. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;In collaboration with Alex Shashko, Andrew Tomkins, Ashley Carrick, Carolina Osorio, Chao Zhang, Damien Pierce, Iveel Tsogsuren, Sheila de Guia, and Yi-fan Chen. Visual design by John Guilyard. We would like to thank our SDOT partners Carter Danne, Chun Kwan, Ethan Bancroft, Jason Cambridge, Laura Wojcicki, Michael Minor, Mohammed Said, Trevor Partap, and SPD partners Lt. Bryan Clenna and Sgt. Brian Kokesh.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1587965303727576226/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type =&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/simulations-illuminate-path-to-post.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;edit&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/12/simulations-illuminate-path-to-post.html&quot; rel=&quot;alternate&quot; title=&quot;Simulations illuminate the path to post-event traffic flow&quot; type=&quot; text/html“/>; &lt;aunder>; &lt;名称>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/1209862651477775266161 &lt;/uri>; &lt;Email>; &lt;Email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =” https://img1.blog1.blogblog.com/img/b16-round.gif =&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s72 -c/TrafficFlow.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6088118107306075362&lt;/id>;&lt;published>;2023-12-15T14:34:00.000-08:00&lt;/published>;&lt;updated >;2023-12-15T14:34:10.381-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Kaggle&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advancements in machine learning for machine learning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, Google DeepMind, and Bryan Perozzi, Senior Staff Research Scientist, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s1600/Screenshot%202023-12-15%20at%202.33.10%E2 %80%AFPM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; With the recent and accelerated advances in machine learning (ML), machines can &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&quot;>;understand natural language&lt;/a>;, &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;>;engage in conversations&lt;/a>;, &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview&quot;>;draw images&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;create videos&lt;/a>; and more. Modern ML models are programmed and trained using ML programming frameworks, such as &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, among many others. These libraries provide high-level instructions to ML practitioners, such as linear algebra operations (eg, matrix multiplication, convolution, etc.) and neural network layers (eg, &lt;a href=&quot;https://keras.io/api/layers/convolution_layers/convolution2d/&quot;>;2D convolution layers&lt;/a>;, &lt;a href=&quot;https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/&quot;>;transformer layers&lt;/a>;). Importantly, practitioners need not worry about how to make their models run efficiently on hardware because an ML framework will automatically optimize the user&#39;s model through an underlying &lt;em>;compiler&lt;/em>;. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on heuristics to solve complex optimization problems, often resulting in suboptimal performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post, we present exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Prior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small sub-programs, such as basic blocks or kernels. We introduce “&lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs&lt;/a>;” (presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), which we recently released to fuel more research in ML for program optimization. We hosted a &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime/overview&quot;>;Kaggle competition&lt;/a>; on the dataset, which recently completed with 792 participants on 616 teams from 66 countries. Furthermore, in “&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;Learning Large Graph Property Prediction via Graph Segment Training&lt;/a>;”, we cover a novel method to scale &lt;a href=&quot;https://arxiv.org/abs/2005.03675&quot;>;graph neural network&lt;/a>; (GNN) training to handle large programs represented as graphs. The technique both enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ML compilers&lt;/h2>; &lt;p>; ML compilers are software routines that convert user-written programs (here, mathematical instructions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML program can be represented as a computation graph, where a node represents a tensor operation (such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;matrix multiplication&lt;/a>;), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimization problems, including &lt;em>;graph-level &lt;/em>;and &lt;em>;kernel-level&lt;/em>; optimizations. A graph-level optimization requires the context of the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization transforms one kernel (a fused subgraph) at a time, independently of other kernels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Important optimizations in ML compilers include graph-level and kernel-level optimizations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To provide a concrete example, imagine a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_(mathematics)&quot;>;matrix&lt;/a>; (2D tensor): &lt;/p>; &lt;table align=&quot;center&quot; cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;290&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; It can be stored in computer memory as [ABC abc] or [A a B b C c], known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Row-_and_column-major_order&quot;>;row- and column-major memory layout&lt;/a>;, respectively. One important ML compiler optimization is to assign memory layouts to all intermediate tensors in the program. The figure below shows two different layout configurations for the same program. Let&#39;s assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual operator. However, this layout configuration requires the compiler to insert a &lt;em>;copy&lt;/em>; operation to transform the memory layout between the &lt;em>;add&lt;/em>; and &lt;em>;convolution&lt;/em>;运营。 On the other hand, the right-hand side configuration might be less efficient for each individual operator, but it doesn&#39;t require the additional memory transformation. The layout assignment optimization has to trade off between local computation efficiency and layout transformation overhead. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;343&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;A node represents a tensor operator, annotated with its output tensor shape [&lt;em>;n&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;n&lt;sub>;1&lt;/sub>;&lt;/ em>;, ...], where &lt;em>;n&lt;sub>;i &lt;/sub>;&lt;/em>;is the size of dimension &lt;em>;i&lt;/em>;. Layout {&lt;em>;d&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;d&lt;sub>;1&lt;/sub>;&lt;/em>;, ...} represents minor-to-major ordering in memory. Applied configurations are highlighted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inputs and outputs of influential operators (ie, convolution and reshape). A copy operator is inserted when there is a layout mismatch.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If the compiler makes optimal choices, significant speedups can be made. For example, we have seen &lt;a href=&quot;https://ieeexplore.ieee.org/document/9563030&quot;>;up to a 32% speedup&lt;/a>; when choosing an optimal layout configuration over the default compiler&#39;s configuration in the &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;XLA&lt;/a>; benchmark suite. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TpuGraphs dataset&lt;/h2>; &lt;p>; Given the above, we aim to improve ML model efficiency by improving the ML compiler. Specifically, it can be very effective to equip the compiler&lt;strong>; &lt;/strong>;with a &lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;learned cost model&lt;/a>;&lt;strong>; &lt;/strong>;that takes in an input program and compiler configuration and then outputs the predicted runtime of the program. &lt;/p>; &lt;p>; With this motivation, we &lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;release TpuGraphs&lt;/a>;, a dataset for learning cost models for programs running on Google&#39;s custom &lt;a href=&quot;https://cloud.google.com/tpu/docs/intro-to-tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs). The dataset targets two XLA compiler configurations: &lt;em>;layout&lt;/em>; (generalization of row- and column-major ordering, from matrices, to higher dimension tensors) and &lt;em>;tiling&lt;/em>; (configurations of tile sizes) 。 We provide download instructions and starter code on the &lt;a href=&quot;https://github.com/google-research-datasets/tpu_graphs&quot;>;TpuGraphs GitHub&lt;/a>;. Each example in the dataset contains a computational graph of an ML workload, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source ML programs, featuring popular model architectures, eg, &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;>;EfficientNet&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;Mask R-CNN&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>;. The dataset provides 25× more graphs than the largest (earlier) graph property prediction dataset (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction datasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction task on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1546&quot; data-original-width=&quot;2868&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Scale of TpuGraphs compared to other graph property prediction datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We provide baseline learned cost models with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represented as a graph. Node features, shown in blue below, consist of two parts. The first part is an &lt;em>;opcode id&lt;/em>;, the most important information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id to an &lt;em>;opcode embedding&lt;/em>; via an embedding lookup table. The opcode embedding is then concatenated with the second part, the rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed-size embedding of the graph using a simple graph pooling reduction (ie, sum and mean). The resulting graph embedding is then linearly transformed into the final scalar output by a feedforward layer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1106&quot; data-original-width=&quot;2284&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Our baseline learned cost model employs a GNN since programs can be naturally represented as graphs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore we present &lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;Graph Segment Training&lt;/a>; (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases where the prediction task is on the entire-graph (ie, graph-level prediction). Unlike scaling training for node- or edge-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs can contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN model is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a random subset of segments is selected to update the model; embeddings for the remaining segments are produced without saving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to generate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the historical embedding table to efficiently obtain graph segments&#39; embeddings and segment dropout to mitigate the staleness from historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;790&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Comparing Full Graph Training (typical method) vs Graph Segment Training (our proposed method).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Kaggle competition&lt;/h2>; &lt;p>; Finally, we ran the “&lt;a href=&quot;https://kaggle.com/competitions/predict-ai-model- runtime&quot;>;Fast or Slow? Predict AI Model Runtime&lt;/a>;” competition over the TpuGraph dataset. This competition ended with 792 participants on 616 teams. We had 10507 submissions from 66 countries. For 153 users (including 47 in the top 100), this was their first competition. We learned many interesting new techniques employed by the participating teams, such as: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Graph pruning / compression&lt;/em>;: Instead of using the GST method, many teams experimented with different ways to compress large graphs (eg, keeping only subgraphs that include the configurable nodes and their immediate neighbors). &lt;/li>;&lt;li>;&lt;em>;Feature padding value&lt;/em>;: Some teams observed that the default padding value of 0 is problematic because 0 clashes with a valid feature value, so using a padding value of -1 can improve the model accuracy significantly. &lt;/li>;&lt;li>;&lt;em>;Node features&lt;/em>;: Some teams observed that additional node features (such as &lt;a href=&quot;https://www.tensorflow.org/xla/operation_semantics#dot&quot;>;dot general&#39;s contracting dimensions&lt;/a>;) are important. A few teams found that different encodings of node features also matter. &lt;/li>;&lt;li>;&lt;em>;Cross-configuration attention&lt;/em>;: A winning team designed a simple layer that allows the model to explicitly &quot;compare&quot; configs against each other. This technique is shown to be much better than letting the model infer for each config individually. &lt;/li>; &lt;/ul>; &lt;p>; We will debrief the competition and preview the winning solutions at the competition session at the &lt;a href=&quot;https://mlforsystems.org/&quot;>;ML for Systems workshop&lt;/a>; at NeurIPS on December 16, 2023. Finally, congratulations to all the winners and thank you for your contributions to advancing research in ML for systems! &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeurIPS expo&lt;/h2>; &lt;p>; If you are interested in more research about structured data and artificial intelligence, we hosted the NeurIPS Expo panel &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;Graph Learning Meets Artificial Intelligence&lt;/a>; on December 9, which covered advancing learned cost models and more! &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Sami Abu-el-Haija (Google Research) contributed significantly to this work and write-up. The research in this post describes joint work with many additional collaborators including Mike Burrows, Kaidi Cao, Bahare Fatemi, Jure Leskovec, Charith Mendis, Dustin Zelle, and Yanqi Zhou.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6088118107306075362/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/advancements-in-machine-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6088118107306075362&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6088118107306075362&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/advancements-in-machine-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Advancements in machine learning for machine learning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s72-c/Screenshot%202023-12-15%20at%202.33.10%E2%80%AFPM.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6479608460479432217&lt;/id>;&lt;published>;2023-12-15T11:40:00.000-08:00&lt;/published>;&lt;updated>;2023-12-15T12:18:41.742-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Style Transfer&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;StyleDrop: Text-to-image generation in any style&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kihyuk Sohn and Dilip Krishnan, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC-uKMbsFvxRmRWkokKR4VinfUZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s1600/StyleDrop%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image models trained on large volumes of image-text pairs have enabled the creation of rich and diverse images encompassing many genres and themes. Moreover, popular styles such as “anime” or “steampunk”, when added to the input text prompt, may translate to specific visual outputs. While many efforts have been put into &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;prompt engineering&lt;/a>;, a wide range of styles are simply hard to describe in text form due to the nuances of color schemes, illumination, and other characteristics. As an example, “watercolor painting” may refer to various styles, and using a text prompt that simply says “watercolor painting style” may either result in one specific style or an unpredictable mix of several. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNKqXN -7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s959/image5.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= &quot;403&quot; data-original-width=&quot;959&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNKqXN-7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s16000/image5.gif&quot; />;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;When we refer to &quot;watercolor painting style,&quot; which do we mean? Instead of specifying the style in natural language, StyleDrop allows the generation of images that are consistent in style by referring to a style reference image&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In this blog we introduce “&lt;a href=&quot;https://arxiv.org/abs/2306.00983&quot;>;StyleDrop: Text-to-Image Generation in Any Style&lt;/a>;”, a tool that allows a significantly higher level of stylized text-to-image synthesis. Instead of seeking text prompts to describe the style, StyleDrop uses one or more style&lt;em>; reference images&lt;/em>; that describe the style for text-to-image generation. By doing so, StyleDrop enables the generation of images in a style consistent with the reference, while effectively circumventing the burden of text prompt engineering. This is done by efficiently fine-tuning the pre-trained text-to-image generation models via &lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;adapter tuning&lt;/a>; on a few style参考图像。 Moreover, by iteratively fine-tuning the StyleDrop on a set of images it generated, it achieves the style-consistent image generation from text prompts. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Method overview&lt;/h2>; &lt;p>; StyleDrop is a text-to-image generation model that allows generation of images whose visual styles are consistent with the user-provided style reference images. This is achieved by a couple of iterations of parameter-efficient fine-tuning of pre-trained text-to-image generation models. Specifically, we build StyleDrop on &lt;a href=&quot;https://muse-model.github.io/&quot;>;Muse&lt;/a>;, a text-to-image generative vision transformer. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Muse: text-to-image generative vision transformer&lt;/h3>; &lt;p>; &lt;a href=&quot;https://muse-model.github.io/&quot;>;Muse&lt;/a>; is a state-of-the-art text-to-image generation model based on the masked generative image transformer (&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf&quot;>;MaskGIT&lt;/a>;). Unlike diffusion models, such as &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; or &lt;a href=&quot;https://github.com/CompVis/stable-diffusion&quot;>;Stable Diffusion&lt;/a>;, Muse represents an image as a sequence of discrete tokens and models their distribution using a transformer architecture. Compared to diffusion models, Muse is known to be faster while achieving competitive generation quality. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Parameter-efficient adapter tuning&lt;/h3>; &lt;p>; StyleDrop is built by fine-tuning the pre-trained Muse model on a few style reference images and their corresponding text prompts. There have been many works on parameter-efficient fine-tuning of transformers, including &lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;>;prompt tuning&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;Low-Rank Adaptation&lt;/a>; (LoRA) of large language models. Among those, we opt for adapter tuning, which is shown to be effective at fine-tuning a large transformer network for &lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;language&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf&quot;>;image generation&lt;/a>; tasks in a parameter-efficient manner. For example, it introduces less than one million trainable parameters to fine-tune a Muse model of 3B parameters, and it requires only 1000 training steps to converge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s1632/image2.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1632&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Parameter-efficient adapter tuning of Muse.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Iterative training with feedback&lt;/h3>; &lt;p>; While StyleDrop is effective at learning styles from a few style reference images, it is still challenging to learn from a single style reference image. This is because the model may not effectively disentangle the &lt;em>;content &lt;/em>;(ie, what is in the image) and the &lt;em>;style&lt;/em>; (ie, how it is being presented), leading to reduced &lt;em>;text controllability&lt;/em>; in generation. For example, as shown below in Step 1 and 2, a generated image of a chihuahua from StyleDrop trained from a single style reference image shows a leakage of content (ie, the house) from the style reference image. Furthermore, a generated image of a temple looks too similar to the house in the reference image (concept collapse). &lt;/p>; &lt;p>; We address this issue by training a new StyleDrop model on a subset of synthetic images, chosen by the user or by image-text alignment models (eg, &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;), whose images are generated by the first round of the StyleDrop model trained on a single image. By training on multiple synthetic image-text aligned images, the model can easily disentangle the style from the content, thus achieving improved image-text alignment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s1295/image3 .gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;825&quot; data-original-width=&quot;1295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Iterative training with feedback&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;. The first round of StyleDrop may result in reduced text controllability, such as a content leakage or concept collapse, due to the difficulty of content-style disentanglement. Iterative training using synthetic images, generated by the previous rounds of StyleDrop models and chosen by human or image-text alignment models, improves the text adherence of stylized text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;StyleDrop gallery&lt;/h3>; &lt;p>; We show the effectiveness of StyleDrop by running experiments on 24 distinct style reference images. As shown below, the images generated by StyleDrop are highly consistent in style with each other and with the style reference image, while depicting various contexts, such as a baby penguin, banana, piano, etc. Moreover, the model can render alphabet images with a consistent style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/s1632/image6.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1632&quot; data-original-width=&quot;1536&quot; height=&quot;640&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/w602-h640/image6.gif&quot; width=&quot;602&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stylized text-to-image generation. Style reference images&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>; are on the left inside the yellow box. Text prompts used are:&lt;br>; First row: a baby penguin, a banana, a bench.&lt;br>; Second row: a butterfly, an F1 race car, a Christmas tree.&lt;br>; Third row: a coffee maker, a hat, a moose.&lt;br>; Fourth row: a robot, a towel, a wood cabin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s1526/image1.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;767&quot; data-original-width=&quot;1526&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stylized visual character generation. Style reference images&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>; are on the left inside the yellow box. Text prompts used are: (first row) letter &#39;A&#39;, letter &#39;B&#39;, letter &#39;C&#39;, (second row) letter &#39;E&#39;, letter &#39;F&#39;, letter &#39;G&#39;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Generating images of my object in my style&lt;/h3>; &lt;p>; Below we show generated images by sampling from two personalized generation distributions, one for an object and another for the style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lrXSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s1006/image8.gif &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;412&quot; data-original-width=&quot;1006&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lrXSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s16000/image8.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images at the top in the blue border are object reference images from the DreamBooth dataset (teapot, vase, dog and cat), and the image on the left at the bottom in the red border is the style reference image*. Images in the purple border (ie the four lower right images) are generated from the style image of the specific object.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Quantitative results&lt;/h3>; &lt;p>; For the quantitative evaluation, we synthesize images from a subset of &lt;a href=&quot;https://github.com/google-research/parti/blob/main/PartiPrompts.tsv&quot;>;Parti prompts&lt;/a>; and measure the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-to-image CLIP score&lt;/a>; for style consistency and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-to-text CLIP score&lt;/a>; for text consistency. We study non–fine-tuned models of Muse and Imagen. Among fine-tuned models, we make a comparison to &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; on &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, state-of-the-art personalized text-to-image method for subjects. We show two versions of StyleDrop, one trained from a single style reference image, and another, “StyleDrop (HF)”, that is trained iteratively using synthetic images with human feedback as described above. As shown below, StyleDrop (HF) shows significantly improved style consistency score over its non–fine-tuned counterpart (0.694 vs. 0.556), as well as DreamBooth on Imagen (0.694 vs. 0.644). We observe an improved text consistency score with StyleDrop (HF) over StyleDrop (0.322 vs. 0.313). In addition, in a human preference study between DreamBooth on Imagen and StyleDrop on Muse, we found that 86% of the human raters preferred StyleDrop on Muse over DreamBooth on Imagen in terms of consistency to the style reference image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s2980 /image16.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot; 2980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; StyleDrop achieves style consistency at text-to-image generation using a few style reference images. Google&#39;s AI Principles guided our development of Style Drop, and we urge the responsible use of the technology. StyleDrop was adapted to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-style&quot;>;create a custom style model in Vertex AI&lt;/a>;, and we believe it could be a helpful tool for art directors and graphic designers — who might want to brainstorm or prototype visual assets in their own styles, to improve their productivity and boost their creativity — or businesses that want to generate new media assets that reflect a particular brand. As with other generative AI capabilities, we recommend that practitioners ensure they align with copyrights of any media assets they use. More results are found on our &lt;a href=&quot;https://styledrop.github.io/&quot;>;project website&lt;/a>; and &lt;a href=&quot;https://youtu.be/gNHD0_hkJ9A&quot;>;YouTube video&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan. &lt;/em>;We thank owners of images used in our experiments (&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/main/images/assets/data.md&quot;>;links&lt;/a>; for attribution) for sharing their valuable assets. &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt; sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;*&lt;/b>;&lt;/a>;&lt;/sup>;See &lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/ main/images/assets/data.md&quot;>;image sources&lt;/a>;&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt; /p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6479608460479432217/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/12/styledrop-text-to-image-generation-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// blog.research.google/2023/12/styledrop-text-to-image-generation-in.html&quot; rel=&quot;alternate&quot; title=&quot;StyleDrop: Text-to-image generation in any style&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC-uKMbsFvxRmRWkokKR4VinfUZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s72- c/StyleDrop%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8223613466421639113&lt;/id>;&lt;published>;2023-12-10T06:11:00.000-08:00&lt;/published>;&lt; updated>;2024-01-22T09:18:36.334-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at NeurIPS 2023&lt;/stitle>;&lt;content type=&quot; html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq -kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s1100/google_research_sticker-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week the 37th annual &lt;a href=&quot;https://neurips.cc/Conferences/2023&quot;>;Conference on Neural Information Processing Systems&lt;/a>; (NeurIPS 2023), the biggest machine learning conference of the year, kicks off in New Orleans, LA. Google is proud to be a &lt;a href=&quot;https://neurips.cc/Conferences/2023/Sponsors&quot;>;Diamond Level sponsor&lt;/a>; of NeurIPS this year and will have a strong presence with &amp;gt;170 accepted papers, two keynote talks, and additional contributions to the broader research community through organizational support and involvement in &amp;gt;20 workshops and tutorials. Google is also proud to be a Platinum Sponsor for both the &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;Women in Machine Learning&lt;/a>; and &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;LatinX in AI&lt;/a>; workshops. We look forward to sharing some of our extensive ML research and expanding our partnership with the broader ML research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Attending for NeurIPS 2023 in person? Come visit the Google Research booth to learn more about the exciting work we&#39;re doing to solve some of the field&#39;s most interesting challenges. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). &lt;/p>; &lt;p>; You can learn more about our latest cutting edge work being presented at the conference in the list below (Google affiliations highlighted in &lt;strong>;bold&lt;/strong>;). And see &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023&quot;>;Google DeepMind&#39;s blog&lt;/a>; to learn more about their participation at NeurIPS 2023. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; NeurIPS Board: &lt;strong>;Corinna Cortes&lt;/strong>;&lt;br />; Advisory Board: &lt;strong>;John C. Platt&lt;/ strong>;&lt;br />; Senior Area Chair: &lt;strong>;Inderjit S. Dhillon&lt;/strong>;&lt;br />; Creative AI Chair: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; Program Chair: &lt;strong>;Amir Globerson &lt;/strong>;&lt;br />; Datasets and Benchmarks Chair:&lt;b>; Remi Denton&lt;/b>;&lt;br />;Expo Chair: &lt;b>;Wenming Ye&lt;/b>;&lt;/p>; &lt;/div>; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research Booth Demo/Q&amp;amp;A Schedule&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;h4>;该时间表可能会发生变化。 Please visit the Google booth (#215) for more information.&lt;/h4>; &lt;p>; What You See is What You Read? Improving Text-Image Alignment Evaluation&lt;br />; Presenter: &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;12:15PM - 1:45PM&lt;/em>; &lt;/p>; &lt;p>; Talk like a Graph: Encoding Graphs for Large Language Models&lt;br />; Presenters: &lt;strong>;Bahar Fatemi&lt;/strong>;,&lt;strong>; Jonathan Halcrow&lt;/strong>;,&lt;strong>; Bryan Perozzi&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use&lt;br />; Presenter: &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; MLCommons Croissant&lt;br />; Presenters: &lt;strong>;Omar Benjelloun&lt;/strong>;,&lt;strong>; Meg Risdal&lt;/strong>;, &lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model&lt;br />; Presenter: &lt;strong>;Xiuye Gu&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Embedding Large Graphs&lt;br />; Presenters: &lt;strong>;Bryan Perozzi&lt;/strong>;, &lt;strong>;Anton Tsitsulin&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Correlated Noise Provably Beats Independent Noise for Differentially Private Learning&lt;br />; Presenter: &lt;strong>;Krishna Pillutla&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Med-PaLM&lt;br />; Presenter: &lt;strong>;Tao Tu&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; StyleDrop: Text-to-Image Generation in Any Style&lt;br />; Presenters: &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; DICES Dataset: Diversity in Conversational AI Evaluation for Safety&lt;br />; Presenters: &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Alicia Parrish&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; Resonator: Scalable Game-Based Evaluation of Large Models&lt;br />; Presenters: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Michal Todorovic&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Adversarial Nibbler&lt;br />; Presenter: &lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Towards Generalist Biomedical AI&lt;br />; Presenter: &lt;strong>;Tao Tu&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; Conditional Adaptors&lt;br />; Presenter: &lt;strong>;Junwen Bai&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; Patient Assistance with Multimodal RAG&lt;br />; Presenters: &lt;strong>;Ryan Knuffman&lt;/strong>;, &lt;strong>;Milica Cvetkovic&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;p>; How Hessian Structure Explains Mysteries in Sharpness Regularization&lt;br />; Presenter: &lt;strong>;Hossein Mobahi&lt;/strong>;&lt;br />; Wednesday, 12 月 13 日 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Keynote Speakers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73993&quot;>;The Many Faces of Responsible AI&lt;/a>;&lt;br />; Speaker: &lt;strong>;Lora Aroyo&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73987&quot;>;Sketching: Core Tools, Learning-Augmentation, and Adaptive Robustness&lt;/a>;&lt;br />; Speaker: &lt;strong>;Jelani Nelson&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Affinity Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;Women in ML&lt;/a>;&lt;br />; &lt;strong>;Google Sponsored - Platinum&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;LatinX in AI&lt;/a>;&lt;br />; &lt;strong>;Google Sponsored - Platinum&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66605&quot;>;New in ML&lt;/a>;&lt;br />; Organizer: &lt;strong>;Sangnie Bhardwaj&lt;/strong>;&lt;br>; Speaker: &lt;strong>;Milind Tambe&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66541&quot;>;AI for Accelerated Materials Design&lt;/a>; (AI4Mat-2023)&lt;br />; Fireside Chat: &lt;strong>;Gowoon Cheon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66524&quot;>;Associative Memory &amp;amp; Hopfield Networks in 2023&lt;/a>;&lt;br />; Panelist: &lt;strong>;Blaise Agüera y Arcas&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66535&quot;>;Information-Theoretic Principles in Cognitive Systems&lt;/a>; (InfoCog)&lt;br />; Speaker: &lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66518&quot;>;Machine Learning and the Physical Sciences&lt;/a>;&lt;br />; Speaker: &lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66494&quot;>;UniReps: Unifying Representations in Neural Models&lt;/a>;&lt;br />; Organizer: &lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66517&quot;>;Robustness of Zero/Few-shot Learning in Foundation Models&lt;/a>; (R0-FoMo)&lt;br />; Speaker: &lt;strong>;Partha Talukdar&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Ananth Balashankar&lt;/strong>;, &lt;strong>;Yao Qin&lt;/strong>;, &lt;strong>;Ahmad Beirami&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66539&quot;>;Workshop on Diffusion Models&lt;/a>;&lt;br />; Speaker: &lt;strong>;Tali Dekel&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66502&quot;>;Algorithmic Fairness through the Lens of Time&lt;/a>;&lt;br />; Roundtable Lead: &lt;strong>;Stephen Pfohl&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Golnoosh Farnadi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66550&quot;>;Backdoors in Deep Learning: The Good, the Bad, and the Ugly&lt;/a>;&lt;br />; Organizer: &lt;strong>;Eugene Bagdasaryan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66549&quot;>;OPT 2023: Optimization for Machine Learning&lt;/a>;&lt;br />; Organizer: &lt;strong>;Cristóbal Guzmán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66545&quot;>;Machine Learning for Creativity and Design&lt;/a>;&lt;br />; Speaker: &lt;strong>;Aleksander Holynski&lt;/strong>;, &lt;strong>;Alexander Mordvintsev&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66511&quot;>;Robot Learning Workshop: Pretraining, Fine-Tuning, and Generalization with Large Scale Models&lt;/a>;&lt;br />; Speaker: &lt;strong>;Matt Barnes&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66516&quot;>;Machine Learning for Audio&lt;/a>;&lt;br />; Organizer: &lt;strong>;Shrikanth Narayanan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66531&quot;>;Federated Learning in the Age of Foundation Models&lt;/a>; (FL@FM-NeurIPS&#39;23)&lt;br />; Speaker: &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, &lt;strong>;Zheng Xu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66526&quot;>;Socially Responsible Language Modelling Research&lt;/a>; (SoLaR)&lt;br />; Panelist: &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66506&quot;>;I Can&#39;t Believe It&#39;s Not Better (ICBINB): Failure Modes in the Age of Foundation Models&lt;/a>;&lt;br />; Advisory Board: &lt;strong>;Javier Antorán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66501&quot;>;Machine Learning for Systems&lt;/a>;&lt;br />; Organizer: &lt;strong>;Yawen Wang&lt;/strong>;&lt;br />; Competition Committee: &lt;strong>;Bryan Perozzi&lt;/strong>;, &lt;strong>;Sami Abu-el-haija&lt;/strong>;&lt;br />; Steering Committee: &lt;strong>;Milad Hashemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66514&quot;>;Self-Supervised Learning: Theory and Practice&lt;/a>;&lt;br />; Organizer: &lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66496&quot;>;Attributing Model Behavior at Scale&lt;/a>; (ATTRIB 2023)&lt;br />; Organizers: Kevin Guu*, Tolga Bolukbasi* &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Competitions&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66581&quot;>;NeurIPS 2023 Machine Unlearning Competition&lt;/a>;&lt;br />; Organizer: &lt;b>;Isabelle Guyon&lt;/b>;, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66593&quot;>;Lux AI Challenge Season 2 NeurIPS Edition&lt;/a>;&lt;br />; Organizer: &lt;strong>;Bovard Doerschuk-Tiberi&lt;/strong>;, &lt;strong>;Addison Howard&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/tutorial/73949&quot;>;Data-Centric AI for Reliable and Responsible AI: From Theory to Practice&lt;/a>;&lt;br />; &lt;strong>;Isabelle Guyon&lt;/strong>;, Nabeel Seedat, Mihaela va der Schaar &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Creative AI Track&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Creative AI Performances 1 &amp;amp; 2&lt;br />; Speaker: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74093&quot;>;Performance 1&lt;/a>;: Mon, Dec 11 | 6:30PM - 8:30PM, Lobby Stage&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74094&quot;>;Performance 2&lt;/a>;: 12 月 14 日，星期四 | 7:00PM - 9:00PM, Lobby Stage&lt;/em>; &lt;/p>; &lt;p>; Creative AI Sessions 1 – 3&lt;br />; Speaker: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74090&quot;>;Session 1&lt;/a>;: Tue, Dec 12 | 3:05PM - 3:40PM, Hall D2&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74091&quot;>;Session 2&lt;/a>;: Wed, Dec 13 | 10:45AM - 2:15PM, Hall D2&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74092&quot;>;Session 3&lt;/a>;: 12 月 14 日，星期四 | 10:45 AM - 2:15PM, Hall D2&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/session/75889&quot;>;Creative AI Videos&lt;/a>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Expo Talks&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;Graph Learning Meets Artificial Intelligence&lt;/a>;&lt;br />; Speaker: &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78243&quot;>;Resonator: Music Space&lt;/a>;&lt;br />; Speakers: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Michal Todorovic&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78237&quot;>;Empirical Rigor in ML as a Massively Parallelizable Challenge&lt;/a>;&lt;br />; Speaker: &lt;strong>;Megan Risdal (Kaggle)&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Oral Talks&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sW8yGZ4uVJ&quot;>;Ordering-based Conditions for Global Convergence of Policy Gradient Methods&lt;/a>;&lt;br />; Jincheng Mei, Bo Dai, &lt;strong>;Alekh Agarwal&lt;/strong>;,&lt;strong>; &lt;/strong>;Mohammad Ghavamzadeh*, Csaba Szepesvari, Dale Schuurmans &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=y8UAQQHVTX&quot;>;Private Everlasting Prediction&lt;/a>;&lt;br />; Moni Naor, Kobbi Nissim, &lt;strong>;Uri Stemmer&lt;/strong>;, Chao Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PITeSdYQkv&quot;>;User-Level Differential Privacy With Few Examples Per User&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, Raghu Meka, &lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dVaWCDMBof&quot;>;DataComp: In Search of the Next Generation of Multimodal Datasets&lt;/a>;&lt;br />; Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, &lt;strong>;Pang Wei Koh&lt;/strong>;, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=w116w62fxH&quot;>;Optimal Learners for Realizable Regression: PAC Learning and Online Learning&lt;/a>;&lt;br />; Idan Attias, Steve Hanneke, Alkis Kalavasis, &lt;strong>;Amin Karbasi&lt;/strong>;, Grigoris Velegkas &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jDIlzSU8wJ&quot;>;The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation&lt;/a>;&lt;br />; Saurabh Saxena, &lt;strong>;Charles Herrmann&lt;/strong>;,&lt;strong>; Junhwa Hur&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;,&lt;strong>; &lt;/strong>;Mohammad Norouzi*, &lt;strong>;Deqing Sun&lt;/strong>;,&lt;strong>; &lt;/strong>;David J. Fleet &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Journal Track&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://jmlr.org/papers/v24/20-998.html&quot;>;Graph Clustering with Graph Neural Networks&lt;/a>;&lt;br />; &lt;strong>;Anton Tsitsulin&lt;/strong>;, &lt;strong>;John Palowitch&lt;/strong>;,&lt;strong>; Bryan Perozzi&lt;/strong>;, Emmanuel Müller &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Spotlight Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1p6teT6F73&quot;>;Alternating Updates for Efficient Transformers&lt;/a>; (see blog post)&lt;br />; &lt;strong>;Cenk Baykal&lt;/strong>;, &lt;strong>;Dylan Cutler&lt;/strong>;, &lt;strong>;Nishanth Dikkala&lt;/strong>;, Nikhil Ghosh*, &lt;strong>;Rina Panigrahy&lt;/strong>;, &lt;strong>;Xin Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EldbUlZtbd&quot;>;Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models&lt;/a>;&lt;br />; &lt;strong>;Peter Hase&lt;/strong>;, Mohit Bansal, &lt;strong>;Been Kim&lt;/strong>;,&lt;strong>; Asma Ghandeharioun&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jR2FkqW6GB&quot;>;Is Learning in Games Good for the Learners?&lt;/a>;&lt;br />; William Brown,&lt;strong>; Jon Schneider&lt;/strong>;,&lt;strong>; Kiran Vodrahalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Bj1QSgiBPP&quot;>;Participatory Personalization in Classification&lt;/a>;&lt;br />; Hailey Joren, &lt;strong>;Chirag Nagpal&lt;/strong>;,&lt;strong>; Katherine Heller&lt;/strong>;,&lt;strong>; &lt;/strong>;Berk Ustun &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=c2eedxSlPJ&quot;>;Tight Risk Bounds for Gradient Descent on Separable Data&lt;/a>;&lt;br />; Matan Schliserman, &lt;strong>;Tomer Koren&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=67o9UQgTD0&quot;>;Counterfactual Memorization in Neural Language Models&lt;/a>;&lt;br />; &lt;strong>;Chiyuan Zhang&lt;/strong>;,&lt;strong>; &lt;/strong>;Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5NxJuc0T1P&quot;>;Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models&lt;/a>;&lt;br />; &lt;strong>;Zhong Yi Wan&lt;/strong>;, Ricardo Baptista,&lt;strong>; Anudhyan Boral&lt;/strong>;, &lt;strong>;Yi-Fan Chen&lt;/strong>;,&lt;strong>; John Anderson&lt;/strong>;,&lt;strong>; Fei Sha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vTug54Uunq&quot;>;Faster Margin Maximization Rates for Generic Optimization Methods&lt;/a>;&lt;br />; Guanghui Wang, Zihao Hu, Vidya Muthukumar, &lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3PjCt4kmRx&quot;>;From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces&lt;/a>;&lt;br />; Peter Shaw, Mandar Joshi, &lt;strong>;James Cohan&lt;/strong>;,&lt;strong>; &lt;/strong>;Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina N Toutanova &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5Gw9YkJkFF&quot;>;PAC Learning Linear Thresholds from Label Proportions&lt;/a>;&lt;br />; &lt;strong>;Anand Brahmbhatt&lt;/strong>;,&lt;strong>; Rishi Saket&lt;/strong>;,&lt;strong>; Aravindan Raghuveer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CXPUg86A1D&quot;>;SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs&lt;/a>;&lt;br />; Lijun Yu*, &lt;strong>;Yong Cheng&lt;/strong>;,&lt;strong>; &lt;/strong>;Zhiruo Wang, &lt;strong>;Vivek Kumar&lt;/strong>;,&lt;strong>; Wolfgang Macherey&lt;/strong>;, &lt;strong>;Yanping Huang&lt;/strong>;,&lt;strong>; David Ross&lt;/strong>;,&lt;strong>; Irfan Essa&lt;/strong>;,&lt;strong>; &lt;/strong>;Yonatan Bisk,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Kevin Murphy&lt;/strong>;,&lt;strong>; &lt;/strong>;Alexander Hauptmann,&lt;strong>; Lu Jiang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QatZNssk7T&quot;>;Adaptive Data Analysis in a Balanced Adversarial Model&lt;/a>;&lt;br />; Kobbi Nissim,&lt;strong>; Uri Stemmer&lt;/strong>;,&lt;strong>; &lt;/strong>;Eliad Tsfadia &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NiQTy0NW1L&quot;>;Lexinvariant Language Models&lt;/a>;&lt;br />; Qian Huang, Eric Zelikman, Sarah Chen,&lt;strong>; Yuhuai Wu&lt;/strong>;, Gregory Valiant, Percy Liang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HF6bnhfSqH&quot;>;On Quantum Backpropagation, Information Reuse, and Cheating Measurement Collapse&lt;/a>;&lt;br />; &lt;strong>;Amira Abbas&lt;/strong>;,&lt;strong>; &lt;/strong>;Robbie King, Hsin-Yuan Huang, &lt;strong>;William J. Huggins&lt;/strong>;, &lt;strong>;Ramis Movassagh&lt;/strong>;,&lt;strong>; Dar Gilboa&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Jarrod McClean&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Pya0kCEpDk&quot;>;Private Estimation Algorithms for Stochastic Block Models and Mixture Models&lt;/a>;&lt;br />; Hongjie Chen,&lt;strong>; Vincent Cohen-Addad&lt;/strong>;,&lt;strong>; &lt;/strong>;Tommaso d&#39;Orsi,&lt;strong>; Alessandro Epasto&lt;/strong>;, Jacob Imola, David Steurer, Stefan Tiegel &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DBz9E5aZey&quot;>;Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation&lt;/a>;&lt;br />; &lt;strong>;Aniket Das&lt;/strong>;, &lt;strong>;Dheeraj Nagaraj&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=e0pRF9tOtm&quot;>;Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>;,&lt;strong>; &lt;/strong>;Daogao Liu*, Sewoong Oh, Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bKqrWLCMrX&quot;>;Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts&lt;/a>;&lt;br />; Pritam Sarkar, &lt;strong>;Ahmad Beirami&lt;/strong>;,&lt;strong>; Ali Etemad&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ikkdTD3hQJ&quot;>;AIMS: All-Inclusive Multi-Level Segmentation for Anything&lt;/a>;&lt;br />; Lu Qi, Jason Kuen, Weidong Guo, Jiuxiang Gu, Zhe Lin, Bo Du, Yu Xu, &lt;strong>;Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rheCTpRrxI&quot;>;DreamHuman: Animatable 3D Avatars from Text&lt;/a>;&lt;br />; &lt;strong>;Nikos Kolotouros&lt;/strong>;,&lt;strong>; Thiemo Alldieck&lt;/strong>;,&lt;strong>; Andrei Zanfir&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Mihai Fieraru&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=oaCDiKoJ2w&quot;>;Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts&lt;/a>;&lt;br />; Chaoqi Wang, Ziyu Ye, &lt;strong>;Zhe Feng&lt;/strong>;, &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;, Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=m21rQusNgb&quot;>;Learning List-Level Domain-Invariant Representations for Ranking&lt;/a>;&lt;br />; Ruicheng Xian*, &lt;strong>;Honglei Zhuang&lt;/strong>;, &lt;strong>;Zhen Qin&lt;/strong>;, Hamed Zamani*, &lt;strong>;Jing Lu&lt;/strong>;,&lt;strong>; Ji Ma&lt;/strong>;, &lt;strong>;Kai Hui&lt;/strong>;, Han Zhao, &lt;strong>;Xuanhui Wang&lt;/strong>;, &lt;strong>;Michael Bendersky&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hCdqDkA25J&quot;>;Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization&lt;/a>;&lt;br />; Liang Zhang, Junchi Yang, &lt;strong>;Amin Karbasi&lt;/strong>;, Niao He &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hJzEoQHfCe&quot;>;Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems&lt;/a>;&lt;br />; Benjamin Coleman, Wang-Cheng Kang, &lt;strong>;Matthew Fahrbach&lt;/strong>;,&lt;strong>; &lt;/strong>;Ruoxi Wang, Lichan Hong, Ed Chi, Derek Cheng &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xOJUmwwlJc&quot;>;Proximity-Informed Calibration for Deep Neural Networks&lt;/a>;&lt;br />; Miao Xiong, Ailin Deng, &lt;strong>;Pang Wei Koh&lt;/strong>;, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WfsWy59bX2&quot;>;Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization&lt;/a>;&lt;br />; &lt;strong>;Adel Javanmard&lt;/strong>;,&lt;strong>; Vahab Mirrokni&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EUiIbwV379&quot;>;Better Private Linear Regression Through Better Private Feature Selection&lt;/a>;&lt;br />; &lt;strong>;Travis Dick&lt;/strong>;, Jennifer Gillenwater*, &lt;strong>;Matthew Joseph&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=XAyPlfmWpu&quot;>;Binarized Neural Machine Translation&lt;/a>;&lt;br />; Yichi Zhang, Ankush Garg, Yuan Cao, &lt;strong>;Łukasz Lew&lt;/strong>;, Behrooz Ghorbani*, Zhiru Zhang, Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/poster/73665&quot;>;BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information&lt;/a>;&lt;br />; &lt;strong>;Mehran Kazemi&lt;/strong>;,&lt;strong>; Quan Yuan&lt;/strong>;, &lt;strong>;Deepti Bhatia&lt;/strong>;, &lt;strong>;Najoung Kim&lt;/strong>;, &lt;strong>;Xin Xu&lt;/strong>;, &lt;strong>;Vaiva Imbrasaite&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1vvsIJtnnr&quot;>;Boosting with Tempered Exponential Measures&lt;/a>;&lt;br />; &lt;strong>;Richard Nock&lt;/strong>;,&lt;strong>; &lt;/strong>;Ehsan Amid,&lt;strong>; Manfred Warmuth&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SGlrCuwdsB&quot;>;Concept Algebra for (Score-Based) Text-Controlled Generative Models&lt;/a>;&lt;br />; Zihao Wang, Lin Gui, Jeffrey Negrea, &lt;strong>;Victor Veitch&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q8mH2d6uw2&quot;>;Deep Contract Design via Discontinuous Networks&lt;/a>;&lt;br />; Tonghan Wang, &lt;strong>;Paul Dütting&lt;/strong>;, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YoghyvSG0H&quot;>;Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection&lt;/a>;&lt;br />; Cheng-Ju Ho, Chen-Hsuan Tai, Yen-Yu Lin, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PYASzxr2OP&quot;>;Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback&lt;/a>;&lt;br />; Han Shao, Lee Cohen, Avrim Blum, &lt;strong>;Yishay Mansour&lt;/strong>;, Aadirupa Saha, Matthew Walter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qCglMj6A4z&quot;>;Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy&lt;/a>;&lt;br />; Anastasia Koloskova*, &lt;strong>;Ryan McKenna&lt;/strong>;, &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;J Keith Rush&lt;/strong>;, &lt;strong>;Hugh Brendan McMahan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCwToX315b&quot;>;Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products&lt;/a>;&lt;br />; &lt;strong>;Tamas Sarlos&lt;/strong>;,&lt;strong>; &lt;/strong>;Xingyou Song, David P. Woodruff, Qiuyi (Richard) Zhang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JhQP33aMx2&quot;>;Module-wise Adaptive Distillation for Multimodality Foundation Models&lt;/a>;&lt;br />;&lt;br />; Chen Liang, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Matthew Brown&lt;/strong>;, Yin Cui, Tuo Zhao, &lt;strong>;Boqing Gong&lt;/strong>;, Tianyi Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zGRWp7yRqd&quot;>;Multi-Swap k-Means++&lt;/a>;&lt;br />; Lorenzo Beretta, &lt;strong>;Vincent Cohen-Addad&lt;/strong>;, &lt;strong>;Silvio Lattanzi&lt;/strong>;,&lt;strong>; Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8vuDHCxrmy&quot;>;OpenMask3D: Open-Vocabulary 3D Instance Segmentation&lt;/a>;&lt;br />; Ayça Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys,&lt;strong>; Federico Tombari&lt;/strong>;, &lt;strong>;Francis Engelmann&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7RMGI4slcb&quot;>;Order Matters in the Presence of Dataset Imbalance for Multilingual Learning&lt;/a>;&lt;br />; Dami Choi*, &lt;strong>;Derrick Xin&lt;/strong>;, &lt;strong>;Hamid Dadkhahi&lt;/strong>;, Justin Gilmer, Ankush Garg, Orhan Firat, Chih-Kuan Yeh, Andrew M. Dai, Behrooz Ghorbani &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=yEf8NSqTPu&quot;>;PopSign ASL v1.0: An Isolated American Sign Language Dataset Collected via Smartphones&lt;/a>;&lt;br />; Thad Starner, Sean Forbes, Matthew So, David Martin, Rohit Sridhar, Gururaj Deshpande, &lt;strong>;Sam Sepah&lt;/strong>;, Sahir Shahryar, Khushi Bhardwaj, Tyler Kwok, Daksh Sehgal, Saad Hassan, Bill Neubauer, Sofia Vempala, Alec Tan, Jocelyn Heath, Unnathi Kumar, Priyanka Mosur, Tavenner Hall, Rajandeep Singh, Christopher Cui, &lt;strong>;Glenn Cameron&lt;/strong>;, &lt;strong>;Sohier Dane&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gaktiSjatl&quot;>;Semi-Implicit Denoising Diffusion Models (SIDDMs)&lt;/a>;&lt;br />; Yanwu Xu*, Mingming Gong, Shaoan Xie, &lt;strong>;Wei Wei&lt;/strong>;,&lt;strong>; Matthias Grundmann&lt;/strong>;,&lt;strong>; &lt;/strong>;Kayhan Batmanghelich, &lt;strong>;Tingbo Hou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xGz0wAIJrS&quot;>;State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding&lt;/a>;&lt;br />; Devleena Das, Sonia Chernova, &lt;strong>;Been Kim&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AwhpBEqmyo&quot;>;StoryBench: A Multifaceted Benchmark for Continuous Story Visualization&lt;/a>;&lt;br />; Emanuele Bugliarello*, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan,&lt;strong>; Vittorio Ferrari&lt;/strong>;, Pieter-Jan Kindermans, &lt;strong>;Paul Voigtlaender&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wv3bHyQbX7&quot;>;Subject-driven Text-to-Image Generation via Apprenticeship Learning&lt;/a>;&lt;br />; Wenhu Chen, Hexiang Hu, &lt;strong>;Yandong Li&lt;/strong>;, &lt;strong>;Nataniel Ruiz&lt;/strong>;, &lt;strong>;Xuhui Jia&lt;/strong>;, Ming-Wei Chang, William W. Cohen &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=plAix1NxhU&quot;>;TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs&lt;/a>;&lt;br />; &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;, &lt;strong>;Sami Abu-El-Haija&lt;/strong>;, Kaidi Cao*, &lt;strong>;Bahare Fatemi&lt;/strong>;, &lt;strong>;Mike Burrows&lt;/strong>;, Charith Mendis*, &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=a147pIS2Co&quot;>;Training Chain-of-Thought via Latent-Variable Inference&lt;/a>;&lt;br />; &lt;strong>;Du Phan&lt;/strong>;, &lt;strong>;Matthew D. Hoffman&lt;/strong>;, David Dohan*, Sholto Douglas,&lt;strong>; Tuan Anh Le&lt;/strong>;, Aaron Parisi, &lt;strong>;Pavel Sountsov&lt;/strong>;, Charles Sutton, Sharad Vikram,&lt;strong>; Rif A. Saurous&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1ZzG6td0el&quot;>;Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints&lt;/a>;&lt;br />; Jayadev Acharya, Clement L. Canonne, &lt;strong>;Ziteng Sun&lt;/strong>;, Himanshu Tyagi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=j5AoleAIru&quot;>;What You See is What You Read? Improving Text-Image Alignment Evaluation&lt;/a>;&lt;br />; &lt;strong>;Michal Yarom&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Roee Aharoni&lt;/strong>;, &lt;strong>;Jonathan Herzig&lt;/strong>;,&lt;strong>; Oran Lang&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Eran Ofek&lt;/strong>;, &lt;strong>;Idan Szpektor&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=4KZhZJSPYU&quot;>;When Does Confidence-Based Cascade Deferral Suffice?&lt;/a>;&lt;br />; &lt;strong>;Wittawat Jitkrittum&lt;/strong>;, &lt;strong>;Neha Gupta&lt;/strong>;, &lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Harikrishna Narasimhan&lt;/strong>;, &lt;strong>;Ankit Singh Rawat&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=A18PgVSUgf&quot;>;Accelerating Molecular Graph Neural Networks via Knowledge Distillation&lt;/a>;&lt;br />; Filip Ekström Kelvinius, Dimitar Georgiev, Artur Petrov Toshev, &lt;strong>;Johannes Gasteiger&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7EMphtUgCI&quot;>;AVIS: Autonomous Visual Information Seeking with Large Language Model Agent&lt;/a>;&lt;br />; Ziniu Hu*, &lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, Kai-Wei Chang, Yizhou Sun, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=9mJXDcr17V&quot;>;Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing &quot;Spurious&quot; Correlations&lt;/a>;&lt;br />; Qingyao Sun, Kevin Patrick Murphy, &lt;strong>;Sayna Ebrahimi&lt;/strong>;, Alexander D&#39;Amour &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0tEjORCGFD&quot;>;Collaborative Score Distillation for Consistent Visual Editing&lt;/a>;&lt;br />; Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, &lt;strong>;Kihyuk Sohn&lt;/strong>;, Jinwoo Shin &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1SF2tiopYJ&quot;>;CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs&lt;/a>;&lt;br />; Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, &lt;strong>;Federico Tombari&lt;/strong>;, Nassir Navab, Benjamin Busam &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=65aDEXIhih&quot;>;Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;, Nathan Srebro, Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BopG5dhH7L&quot;>;A Computationally Efficient Sparsified Online Newton Method&lt;/a>;&lt;br />; Fnu Devvrit*, Sai Surya Duvvuri,&lt;strong>; &lt;/strong>;Rohan Anil, &lt;strong>;Vineet Gupta&lt;/strong>;, &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, &lt;strong>;Inderjit S Dhillon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6EDHfVHicP&quot;>;DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field&lt;/a>;&lt;br />; Chenyangguang Zhang, Yan Di, Ruida Zhang, Guangyao Zhai, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, Xiangyang Ji &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2nTpPxJ5Bs&quot;>;Double Auctions with Two-sided Bandit Feedback&lt;/a>;&lt;br />; &lt;strong>;Soumya Basu&lt;/strong>;, Abishek Sankararaman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2305.19234&quot;>;Grammar Prompting for Domain-Specific Language Generation with Large Language Models&lt;/a>;&lt;br />; Bailin Wang, Zi Wang, Xuezhi Wang, &lt;strong>;Yuan Cao&lt;/strong>;, Rif A. Saurous, Yoon Kim &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5SIz31OGFV&quot;>;Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training&lt;/a>;&lt;br />; Rie Johnson, Tong Zhang* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3YDukx2cpr&quot;>;Large Graph Property Prediction via Graph Segment Training&lt;/a>;&lt;br />; Kaidi Cao*, &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;, &lt;strong>;Sami Abu-El-Haija&lt;/strong>;, &lt;strong>;Dustin Zelle&lt;/strong>;, &lt;strong>;Yanqi Zhou&lt;/strong>;,&lt;strong>; &lt;/strong>;Charith Mendis*, Jure Leskovec, &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1GxKVprbwM&quot;>;On Computing Pairwise Statistics with Local Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Adam Sealfon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7UdVPRmpif&quot;>;On Student-teacher Deviations in Distillation: Does it Pay to Disobey?&lt;/a>;&lt;br />; &lt;strong>;Vaishnavh Nagarajan&lt;/strong>;, &lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Srinadh Bhojanapalli&lt;/strong>;, &lt;strong>;Hossein Mobahi&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CzkOzKWpMa&quot;>;Optimal Cross-learning for Contextual Bandits with Unknown Context Distributions&lt;/a>;&lt;br />; &lt;strong>;Jon Schneider&lt;/strong>;, &lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8XRMbNAP6Z&quot;>;Near-Optimal k-Clustering in the Sliding Window Model&lt;/a>;&lt;br />; David Woodruff, &lt;strong>;Peilin Zhong&lt;/strong>;, Samson Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3H37XciUEv&quot;>;Post Hoc Explanations of Language Models Can Improve Language Models&lt;/a>;&lt;br />; Satyapriya Krishna, Jiaqi Ma, Dylan Z Slack, &lt;strong>;Asma Ghandeharioun&lt;/strong>;, Sameer Singh, Himabindu Lakkaraju &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BJ0fQUU32w&quot;>;Recommender Systems with Generative Retrieval&lt;/a>;&lt;br />; Shashank Rajput*&lt;strong>;, &lt;/strong>;Nikhil Mehta, Anima Singh, &lt;strong>;Raghunandan Hulikal Keshavan&lt;/strong>;, &lt;strong>;Trung Vu, Lukasz Heldt&lt;/strong>;,&lt;strong>; &lt;/strong>;Lichan Hong, Yi Tay&lt;strong>;, Vinh Q. Tran&lt;/strong>;, &lt;strong>;Jonah Samost&lt;/strong>;, Maciej Kula, Ed H. Chi, Maheswaran Sathiamoorthy &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8OTPepXzeh&quot;>;Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models&lt;/a>;&lt;br />; &lt;strong>;Ying Fan&lt;/strong>;, Olivia Watkins, Yuqing Du, Hao Liu, &lt;strong>;Moonkyung Ryu&lt;/strong>;,&lt;strong>; Craig Boutilier&lt;/strong>;, Pieter Abbeel, Mohammad Ghavamzadeh*, Kangwook Lee, Kimin Lee* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5VQFAvUHcd&quot;>;Replicable Clustering&lt;/a>;&lt;br />; &lt;strong>;Hossein Esfandiari&lt;/strong>;, &lt;strong>;Amin Karbasi&lt;/strong>;, &lt;strong>;Vahab Mirrokni&lt;/strong>;, Grigoris Velegkas, Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5cPz5hrjy6&quot;>;Replicability in Reinforcement Learning&lt;/a>;&lt;br />; &lt;strong>;Amin Karbasi&lt;/strong>;, Grigoris Velegkas, Lin Yang, Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=szFqlNRxeS&quot;>;Riemannian Projection-free Online Learning&lt;/a>;&lt;br />; Zihao Hu, Guanghui Wang, &lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=29WbraPk8U&quot;>;Sharpness-Aware Minimization Leads to Low-Rank Features&lt;/a>;&lt;br />; Maksym Andriushchenko, &lt;strong>;Dara Bahri&lt;/strong>;, &lt;strong>;Hossein Mobahi&lt;/strong>;, Nicolas Flammarion &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2hQ7MBQApp&quot;>;What is the Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models&lt;/a>;&lt;br />; Khashayar Gatmiry, Zhiyuan Li, Ching-Yao Chuang, &lt;strong>;Sashank Reddi&lt;/strong>;, Tengyu Ma, Stefanie Jegelka &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JzQlGqBm8d&quot;>;Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization&lt;/a>;&lt;br />; Jui-Nan Yen, Sai Surya Duvvuri, &lt;strong>;Inderjit S Dhillon&lt;/strong>;, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Ntd6X7uWYF&quot;>;Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints&lt;/a>;&lt;br />; &lt;strong>;Soumyabrata Pal&lt;/strong>;, &lt;strong>;Arun Sai Suggala&lt;/strong>;, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, &lt;strong>;Prateek Jain&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NIrTSCiIZ7&quot;>;Boundary Guided Learning-Free Semantic Control with Diffusion Models&lt;/a>;&lt;br />; Ye Zhu, Yu Wu, &lt;strong>;Zhiwei Deng&lt;/strong>;, Olga Russakovsky, Yan Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=IyYyKov0Aj&quot;>;Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference&lt;/a>;&lt;br />; Tao Lei, &lt;strong>;Junwen Bai&lt;/strong>;, Siddhartha Brahma,&lt;strong>; Joshua Ainslie&lt;/strong>;, Kenton Lee, Yanqi Zhou, Nan Du*, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Yuexin Wu&lt;/strong>;, Bo Li,&lt;strong>; Yu Zhang&lt;/strong>;, Ming-Wei Chang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KTRwpWCMsC&quot;>;Conformal Prediction for Time Series with Modern Hopfield Networks&lt;/a>;&lt;br />; Andreas Auer, &lt;strong>;Martin Gauch&lt;/strong>;, Daniel Klotz, Sepp Hochreiter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PzYAMXmIT3&quot;>;Does Visual Pretraining Help End-to-End Reasoning?&lt;/a>;&lt;br />; &lt;strong>;Chen Sun&lt;/strong>;, Calvin Luo, &lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PAYXfIUKWY&quot;>;Effective Robustness Against Natural Distribution Shifts for Models with Different Training Data&lt;/a>;&lt;br />; Zhouxing Shi*, &lt;strong>;Nicholas Carlini&lt;/strong>;, &lt;strong>;Ananth Balashankar&lt;/strong>;, Ludwig Schmidt, &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, Alex Beutel*, &lt;strong>;Yao Qin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Nh5dp6Uuvx&quot;>;Improving Neural Network Representations Using Human Similarity Judgments&lt;/a>;&lt;br />; Lukas Muttenthaler*, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew K. Lampinen, Simon Kornblith &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=N6FhEMnxCU&quot;>;Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency&lt;/a>;&lt;br />; Xiyang Liu, &lt;strong>;Prateek Jain&lt;/strong>;, &lt;strong>;Weihao Kong&lt;/strong>;, &lt;strong>;Sewoong Oh&lt;/strong>;, &lt;strong>;Arun Sai Suggala&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Fdfyga5i0A&quot;>;Mnemosyne: Learning to Train Transformers with Transformers&lt;/a>;&lt;br />; Deepali Jain, Krzysztof Choromanski, &lt;strong>;Avinava Dubey&lt;/strong>;, Sumeet Singh, Vikas Sindhwani, Tingnan Zhang, Jie Tan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=MCkUS1P3Sh&quot;>;Nash Regret Guarantees for Linear Bandits&lt;/a>;&lt;br />; Ayush Sawarni, &lt;strong>;Soumyabrata Pal&lt;/strong>;, Siddharth Barman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot;>;A Near-Linear Time Algorithm for the Chamfer Distance&lt;/a>;&lt;br />; Ainesh Bakshi, Piotr Indyk, &lt;strong>;Rajesh Jayaram&lt;/strong>;, Sandeep Silwal, Erik Waingarten. &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FviF8vuz5B&quot;>;On Differentially Private Sampling from Gaussian and Product Distributions&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Xiao Hu*, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LelK6Mfoey&quot;>;On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes&lt;/a>;&lt;br />; Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh*, Marek Petrik &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HFQFAyNucq&quot;>;ResMem: Learn What You Can and Memorize the Rest&lt;/a>;&lt;br />; Zitong Yang, &lt;strong>;Michal Lukasik&lt;/strong>;,&lt;strong>; Vaishnavh Nagarajan&lt;/strong>;, &lt;strong>;Zonglin Li&lt;/strong>;, &lt;strong>;Ankit Singh Rawat&lt;/strong>;, &lt;strong>;Manzil Zaheer, &lt;/strong>;&lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PcNpL9Q39p&quot;>;Responsible AI (RAI) Games and Ensembles&lt;/a>;&lt;br />; Yash Gupta, Runtian Zhai, &lt;strong>;Arun Suggala&lt;/strong>;, Pradeep Ravikumar &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DVlawv2rSI&quot;>;RoboCLIP: One Demonstration Is Enough to Learn Robot Policies&lt;/a>;&lt;br />; Sumedh A Sontakke, Jesse Zhang, &lt;strong>;Sébastien MR Arnold&lt;/strong>;, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, Laurent Itti &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=I6aOjhpcNQ&quot;>;Robust Concept Erasure via Kernelized Rate-Distortion Maximization&lt;/a>;&lt;br />; Somnath Basu Roy Chowdhury, Nicholas Monath, &lt;strong>;Kumar Avinava Dubey&lt;/strong>;, &lt;strong>;Amr Ahmed&lt;/strong>;, Snigdha Chaturvedi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FmZVRe0gn8&quot;>;Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms&lt;/a>;&lt;br />; Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, &lt;strong>;Zhehui Chen&lt;/strong>;, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PTvxck0QDE&quot;>;Simplicity Bias in 1-Hidden Layer Neural Networks&lt;/a>;&lt;br />; Depen Morwani*, Jatin Batra, &lt;strong>;Prateek Jain&lt;/strong>;,&lt;strong>; Praneeth Netrapalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2302.03806&quot;>;SLaM: Student-Label Mixing for Distillation with Unlabeled Examples&lt;/a>;&lt;br />; Vasilis Kontonis,&lt;strong>; Fotis Iliopoulos&lt;/strong>;, &lt;strong>;Khoa Trinh&lt;/strong>;, &lt;strong>;Cenk Baykal&lt;/strong>;, &lt;strong>;Gaurav Menghani&lt;/strong>;, &lt;strong>;Erik Vee&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCHmP68Gtj&quot;>;SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding&lt;/a>;&lt;br />; Paul-Edouard Sarlin*, &lt;strong>;Eduard Trulls&lt;/strong>;, Marc Pollefeys, &lt;strong>;Jan Hosang&lt;/strong>;, &lt;strong>;Simon Lynen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QvIvWMaQdX&quot;>;SOAR: Improved Indexing for Approximate Nearest Neighbor Search&lt;/a>;&lt;br />; &lt;strong>;Philip Sun&lt;/strong>;, &lt;strong>;David Simcha&lt;/strong>;, &lt;strong>;Dave Dopson&lt;/strong>;, &lt;strong>;Ruiqi Guo&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KoaFh16uOc&quot;>;StyleDrop: Text-to-Image Synthesis of Any Style&lt;/a>;&lt;br />; &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Jarred Barber&lt;/strong>;, Kimin Lee*, &lt;strong>;Nataniel Ruiz&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>;, Huiwen Chang*,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Glenn Entis&lt;/strong>;, &lt;strong>;Irina Blok&lt;/strong>;, &lt;strong>;Daniel Castro Chin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LSYQB4CwD3&quot;>;Three Towers: Flexible Contrastive Learning with Pretrained Image Models&lt;/a>;&lt;br />; Jannik Kossen*,&lt;strong>; Mark Collier&lt;/strong>;, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, &lt;strong>;Jesse Berent&lt;/strong>;,&lt;strong>; &lt;/strong>;Rodolphe Jenatton,&lt;strong>; Efi Kokiopoulou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GIlsH0T4b2&quot;>;Two-Stage Learning to Defer with Multiple Experts&lt;/a>;&lt;br />; Anqi Mao, Christopher Mohri, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ZBzYWP2Gpl&quot;>;AdANNS: A Framework for Adaptive Semantic Search&lt;/a>;&lt;br />; Aniket Rege, &lt;strong>;Aditya Kusupati&lt;/strong>;, Sharan Ranjit S, Alan Fan, Qingqing Cao, Sham Kakade, &lt;strong>;Prateek Jain&lt;/strong>;, Ali Farhadi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Srt1hhQgqa&quot;>;Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer&lt;/a>;&lt;br />; Bowen Tan*, &lt;strong>;Yun Zhu&lt;/strong>;, &lt;strong>;Lijuan Liu&lt;/strong>;, Eric Xing, Zhiting Hu, &lt;strong>;Jindong Chen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dJZ3MvDw86&quot;>;Causal-structure Driven Augmentations for Text OOD Generalization&lt;/a>;&lt;br />; &lt;strong>;Amir Feder&lt;/strong>;, Yoav Wald, Claudia Shi, Suchi Saria, David Blei &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=S0xrBMFihS&quot;>;Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel&lt;/a>;&lt;br />; Valerii Likhosherstov, Krzysztof Choromanski, &lt;strong>;Avinava Dubey&lt;/strong>;, &lt;strong>;Frederick Liu&lt;/strong>;, &lt;strong>;Tamas Sarlos&lt;/strong>;, Adrian Weller &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vm1zeYqwdc&quot;>;Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence&lt;/a>;&lt;br />; Grace Luo, Lisa Dunlap, Dong Huk Park, &lt;strong>;Aleksander Holynski&lt;/strong>;, Trevor Darrell &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgv56R2YJ7&quot;>;Diffusion Self-Guidance for Controllable Image Generation&lt;/a>;&lt;br />; &lt;strong>;Dave Epstein&lt;/strong>;, Allan Jabri, &lt;strong>;Ben Poole&lt;/strong>;, Alexei A Efros,&lt;strong>; Aleksander Holynski&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dnGEPkmnzO&quot;>;Fully Dynamic k-Clustering in Õ(k) Update Time&lt;/a>;&lt;br />; Sayan Bhattacharya, Martin Nicolas Costa, &lt;strong>;Silvio Lattanzi&lt;/strong>;, &lt;strong>;Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SVjDiiVySh&quot;>;Improving CLIP Training with Language Rewrites&lt;/a>;&lt;br />; &lt;strong>;Lijie Fan&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>;, Phillip Isola, Dina Katabi, &lt;strong>;Yonglong Tian&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=UzUhiKACmS&quot;>;k-Means Clustering with Distance-Based Privacy&lt;/a>;&lt;br />; &lt;strong>;Alessandro Epasto&lt;/strong>;, &lt;strong>;Vahab Mirrokni&lt;/strong>;, Shyam Narayanan, &lt;strong>;Peilin Zhong&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Xu8aG5Q8M3&quot;>;LayoutGPT: Compositional Visual Planning and Generation with Large Language Models&lt;/a>;&lt;br />; Weixi Feng, Wanrong Zhu, Tsu-Jui Fu,&lt;strong>; Varun Jampani&lt;/strong>;, &lt;strong>;Arjun Reddy Akula&lt;/strong>;, Xuehai He, &lt;strong>;Sugato Basu&lt;/strong>;, Xin Eric Wang, William Yang Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SxXN3kNTsV&quot;>;Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management&lt;/a>;&lt;br />; Dhawal Gupta*, &lt;strong>;Yinlam Chow&lt;/strong>;,&lt;strong>; Azamat Tulepbergenov&lt;/strong>;, Mohammad Ghavamzadeh*, &lt;strong>;Craig Boutilier&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=aG6xOP9QY7&quot;>;Optimal Unbiased Randomizers for Regression with Label Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;, &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;,&lt;strong>; Ravi Kumar&lt;/strong>;, &lt;strong>;Ethan Jacob Leeman&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Avinash V Varadarajan&lt;/strong>;, &lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WbFhFvjjKj&quot;>;Paraphrasing Evades Detectors of AI-generated Text, but Retrieval Is an Effective Defense&lt;/a>;&lt;br />; &lt;strong>;Kalpesh Krishna&lt;/strong>;, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SaMrN9tnxE&quot;>;ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation&lt;/a>;&lt;br />; Shuyang Sun*, &lt;strong>;Weijun Wang&lt;/strong>;, Qihang Yu*, &lt;strong>;Andrew Howard&lt;/strong>;, Philip Torr, Liang-Chieh Chen* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SouroWC5Un&quot;>;Robust and Actively Secure Serverless Collaborative Learning&lt;/a>;&lt;br />; Nicholas Franzese, Adam Dziedzic, &lt;strong>;Christopher A. Choquette-Choo&lt;/strong>;, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, &lt;strong>;Somesh Jha&lt;/strong>;, Nicolas Papernot, Xiao Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SdYHLTCC5J&quot;>;SpecTr: Fast Speculative Decoding via Optimal Transport&lt;/a>;&lt;br />; &lt;strong>;Ziteng Sun&lt;/strong>;, &lt;strong>;Ananda Theertha Suresh&lt;/strong>;, &lt;strong>;Jae Hun Ro&lt;/strong>;,&lt;strong>; Ahmad Beirami&lt;/strong>;, &lt;strong>;Himanshu Jain&lt;/strong>;,&lt;strong>; Felix Yu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YZ7ip645Ra&quot;>;Structured Prediction with Stronger Consistency Guarantees&lt;/a>;&lt;br />; Anqi Mao, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgiG7WZohZ&quot;>;Affinity-Aware Graph Networks&lt;/a>;&lt;br />; &lt;strong>;Ameya Velingker&lt;/strong>;,&lt;strong>; Ali Kemal Sinop&lt;/strong>;, Ira Ktena, Petar Veličković, &lt;strong>;Sreenivas Gollapudi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rJc5Lsn5QU&quot;>;ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections&lt;/a>;&lt;br />; Chun-Han Yao*, &lt;strong>;Amit Raj&lt;/strong>;, Wei-Chih Hung,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Varun Jampani&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=eoDNaH3pfB&quot;>;Black-Box Differential Privacy for Interactive ML&lt;/a>;&lt;br />; &lt;strong>;Haim Kaplan&lt;/strong>;, &lt;strong>;Yishay Mansour&lt;/strong>;, &lt;strong>;Shay Moran&lt;/strong>;, Kobbi Nissim, &lt;strong>;Uri Stemmer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=orh4e0AO9R&quot;>;Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits&lt;/a>;&lt;br />; Haolin Liu, Chen-Yu Wei, &lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kXOXrVnwbb&quot;>;DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model&lt;/a>;&lt;br />; &lt;strong>;Xiuye Gu&lt;/strong>;, Yin Cui*, &lt;strong>;Jonathan Huang&lt;/strong>;, &lt;strong>;Abdullah Rashwan&lt;/strong>;, &lt;strong>;Xuan Yang&lt;/strong>;, &lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Huizhong Chen&lt;/strong>;, Liang-Chieh Chen*, &lt;strong>;David Ross&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kqBUgrkm1c&quot;>;Easy Learning from Label Proportions&lt;/a>;&lt;br />; &lt;strong>;Robert Busa-Fekete&lt;/strong>;, Heejin Choi*, &lt;strong>;Travis Dick&lt;/strong>;, &lt;strong>;Claudio Gentile&lt;/strong>;, &lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q3fCWoC9l0&quot;>;Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks&lt;/a>;&lt;br />; Eeshaan Jain, Tushar Nandy, &lt;strong>;Gaurav Aggarwal&lt;/strong>;, &lt;strong>;Ashish Tendulkar&lt;/strong>;, Rishabh Iyer, Abir De &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h2lkx9SQCD&quot;>;Faster Differentially Private Convex Optimization via Second-Order Methods&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>;, Mahdi Haghifam*, Thomas Steinke, Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gdVcFOvxT3&quot;>;Finding Safe Zones of Markov Decision Processes Policies&lt;/a>;&lt;br />; Lee Cohen, &lt;strong>;Yishay Mansour&lt;/strong>;, Michal Moshkovitz &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=s1FjXzJ0jy&quot;>;Focused Transformer: Contrastive Training for Context Scaling&lt;/a>;&lt;br />; Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu*, Henryk Michalewski, Piotr Miłoś &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h3kuB4z2G9&quot;>;Front-door Adjustment Beyond Markov Equivalence with Limited Graph Knowledge&lt;/a>;&lt;br />; Abhin Shah, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, Murat Kocaoglu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nI7EmXq2PL&quot;>;H-Consistency Bounds: Characterization and Extensions&lt;/a>;&lt;br />; Anqi Mao, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kjMGHTo8Cs&quot;>;Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation&lt;/a>;&lt;br />; David Brandfonbrener, &lt;strong>;Ofir Nachum&lt;/strong>;, Joan Bruna &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvPujuvjQd&quot;>;Most Neural Networks Are Almost Learnable&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;, Nathan Srebro, Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nQ84YY9Iut&quot;>;Multiclass Boosting: Simple and Intuitive Weak Learning Criteria&lt;/a>;&lt;br />; Nataly Brukhim, &lt;strong>;Amit Daniely&lt;/strong>;, &lt;strong>;Yishay Mansour&lt;/strong>;, &lt;strong>;Shay Moran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gJHAT79cZU&quot;>;NeRF Revisited: Fixing Quadrature Instability in Volume Rendering&lt;/a>;&lt;br />; Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas,&lt;strong>; Ke Li&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=izNfcaHJk0&quot;>;Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation&lt;/a>;&lt;br />; Wei-Ning Chen, Dan Song, Ayfer Ozgur, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rzDBoh1tBh&quot;>;Private Federated Frequency Estimation: Adapting to the Hardness of the Instance&lt;/a>;&lt;br />; Jingfeng Wu*, &lt;strong>;Wennan Zhu&lt;/strong>;, &lt;strong>;Peter Kairouz&lt;/strong>;, Vladimir Braverman &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pVlC0reMKq&quot;>;RETVec: Resilient and Efficient Text Vectorizer&lt;/a>;&lt;br />; &lt;strong>;Elie Bursztein&lt;/strong>;, &lt;strong>;Marina Zhang&lt;/strong>;, &lt;strong>;Owen Skipper Vallis&lt;/strong>;, &lt;strong>;Xinyu Jia&lt;/strong>;, &lt;strong>;Alexey Kurakin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ne6zeqLFCZ&quot;>;Symbolic Discovery of Optimization Algorithms&lt;/a>;&lt;br />; Xiangning Chen*, &lt;strong>;Chen Liang&lt;/strong>;, &lt;strong>;Da Huang&lt;/strong>;, &lt;strong>;Esteban Real&lt;/strong>;, &lt;strong>;Kaiyuan Wang&lt;/strong>;, &lt;strong>;Hieu Pham&lt;/strong>;, &lt;strong>;Xuanyi Dong&lt;/strong>;, &lt;strong>;Thang Luong&lt;/strong>;, Cho-Jui Hsieh, &lt;strong>;Yifeng Lu&lt;/strong>;, &lt;strong>;Quoc V. Le&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lds9D17HRd&quot;>;A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence&lt;/a>;&lt;br />; Junyi Zhang, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Luisa F. Polania&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;,&lt;strong>; Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iSd8g75QvP&quot;>;A Trichotomy for Transductive Online Learning&lt;/a>;&lt;br />; Steve Hanneke, &lt;strong>;Shay Moran&lt;/strong>;, Jonathan Shafer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=i2H2sEiq2T&quot;>;A Unified Fast Gradient Clipping Framework for DP-SGD&lt;/a>;&lt;br />; &lt;strong>;William Kong&lt;/strong>;, &lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=mlbes5TAAg&quot;>;Unleashing the Power of Randomization in Auditing Differentially Private ML&lt;/a>;&lt;br />; &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>;Galen Andrew&lt;/strong>;, &lt;strong>;Peter Kairouz&lt;/strong>;, &lt;strong>;H. Brendan McMahan&lt;/strong>;, &lt;strong>;Alina Oprea&lt;/strong>;, &lt;strong>;Sewoong Oh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zEm6hF97Pz&quot;>;(Amplified) Banded Matrix Factorization: A unified approach to private training&lt;/a>;&lt;br />; Christopher A Choquette-Choo,&lt;strong>; Arun Ganesh&lt;/strong>;, &lt;strong>;Ryan McKenna&lt;/strong>;, &lt;strong>;H Brendan McMahan&lt;/strong>;, &lt;strong>;Keith Rush&lt;/strong>;,&lt;strong>; &lt;/strong>;Abhradeep Guha Thakurta, &lt;strong>;Zheng Xu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xcGhx9FdxM&quot;>;Adversarial Resilience in Sequential Prediction via Abstention&lt;/a>;&lt;br />; Surbhi Goel, Steve Hanneke, &lt;strong>;Shay Moran&lt;/strong>;, Abhishek Shetty &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uTlKUAm68H&quot;>;Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception&lt;/a>;&lt;br />; &lt;strong>;Hassan Akbari&lt;/strong>;, &lt;strong>;Dan Kondratyuk&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Rachel Hornung&lt;/strong>;, &lt;strong>;Huisheng Wang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf/0083ab45a583fe1992b3c4f9222081c50b9d30c3.pdf&quot;>;Android in the Wild: A Large-Scale Dataset for Android Device Control&lt;/a>;&lt;br />; &lt;strong>;Christopher Rawles&lt;/strong>;, &lt;strong>;Alice Li&lt;/strong>;, &lt;strong>;Daniel Rodriguez&lt;/strong>;, &lt;strong>;Oriana Riva&lt;/strong>;, Timothy Lillicrap &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CiRHWaRbp0&quot;>;Benchmarking Robustness to Adversarial Image Obfuscations&lt;/a>;&lt;br />; Florian Stimberg, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Chun-Ta Lu&lt;/strong>;, &lt;strong>;Hussein Hazimeh&lt;/strong>;, &lt;strong>;Otilia Stretcu&lt;/strong>;, &lt;strong>;Wei Qiao&lt;/strong>;,&lt;strong>; Yintao Liu&lt;/strong>;, &lt;strong>;Merve Kaya&lt;/strong>;, &lt;strong>;Cyrus Rashtchian&lt;/strong>;, &lt;strong>;Ariel Fuxman&lt;/strong>;, &lt;strong>;Mehmet Tek&lt;/strong>;, Sven Gowal &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uIj1jDc8k6&quot;>;Building Socio-culturally Inclusive Stereotype Resources with Community Engagement&lt;/a>;&lt;br />; &lt;strong>;Sunipa Dev&lt;/strong>;, Jaya Goyal, &lt;strong>;Dinesh Tewari&lt;/strong>;, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=L9I9FhHfS3&quot;>;Consensus and Subjectivity of Skin Tone Annotation for ML Fairness&lt;/a>;&lt;br />; &lt;strong>;Candice Schumann&lt;/strong>;, &lt;strong>;Gbolahan O Olanubi&lt;/strong>;, &lt;strong>;Auriel Wright&lt;/strong>;, Ellis Monk Jr*, &lt;strong>;Courtney Heldreth&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Susanna Ricco&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zdli6OxpWd&quot;>;Counting Distinct Elements Under Person-Level Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Alexander Knop&lt;/strong>;, Thomas Steinke &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GjNvvswoUL&quot;>;DICES Dataset: Diversity in Conversational AI Evaluation for Safety&lt;/a>;&lt;br />; &lt;strong>;Lora Aroyo&lt;/strong>;, Alex S. Taylor, &lt;strong>;Mark Diaz&lt;/strong>;, &lt;strong>;Christopher M. Homan&lt;/strong>;, &lt;strong>;Alicia Parrish&lt;/strong>;, Greg Serapio-García, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Ding Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SS3CK3yx5Z&quot;>;Does Progress on ImageNet Transfer to Real-world Datasets?&lt;/a>;&lt;br />; Alex Fang, &lt;strong>;Simon Kornblith&lt;/strong>;, Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AA2uO0HHmr&quot;>;Estimating Generic 3D Room Structures from 2D Annotations&lt;/a>;&lt;br />; Denys Rozumnyi*, &lt;strong>;Stefan Popov&lt;/strong>;, &lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;, Matthias Nießner, &lt;strong>;Vittorio Ferrari&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6hZIfAY9GD&quot;>;Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias&lt;/a>;&lt;br />; Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, &lt;strong>;Jiaming Shen&lt;/strong>;,&lt;strong>; &lt;/strong>;Chao Zhang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Y45ZCxslFx&quot;>;MADLAD-400: A Multilingual And Document-Level Large Audited Dataset&lt;/a>;&lt;br />; Sneha Kudugunta, &lt;strong>;Isaac Caswell&lt;/strong>;, Biao Zhang, Xavier Garcia, Derrick Xin, &lt;strong>;Aditya Kusupati&lt;/strong>;, Romi Stella, Ankur Bapna, Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uhKtQMn21D&quot;>;Mechanic: A Learning Rate Tuner&lt;/a>;&lt;br />; Ashok Cutkosky, Aaron Defazio, &lt;strong>;Harsh Mehta&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8TMhs2pIfG&quot;>;NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations&lt;/a>;&lt;br />; &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;, &lt;strong>;Andreas Engelhardt&lt;/strong>;, &lt;strong>;Arjun Karpur&lt;/strong>;, &lt;strong>;Karen Truong&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Stefan Popov&lt;/strong>;, &lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Kaushal Patel&lt;/strong>;, &lt;strong>;Daniel Vlasic&lt;/strong>;,&lt;strong>; Vittorio Ferrari&lt;/strong>;, &lt;strong>;Ameesh Makadia&lt;/strong>;, Ce Liu*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Howard Zhou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=x6cOcxRnxG&quot;>;Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations&lt;/a>;&lt;br />; &lt;strong>;Anudhyan Boral&lt;/strong>;, &lt;strong>;Zhong Yi Wan&lt;/strong>;, &lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>;, &lt;strong>;James Lottes&lt;/strong>;, &lt;strong>;Qing Wang&lt;/strong>;, &lt;strong>;Yi-Fan Chen&lt;/strong>;,&lt;strong>; John Roberts Anderson&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wFuemocyHZ&quot;>;Restart Sampling for Improving Generative Processes&lt;/a>;&lt;br />; Yilun Xu, Mingyang Deng, Xiang Cheng, &lt;strong>;Yonglong Tian&lt;/strong>;, Ziming Liu, Tommi Jaakkola &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xUyBP16Q5J&quot;>;Rethinking Incentives in Recommender Systems: Are Monotone Rewards Always Beneficial?&lt;/a>;&lt;br />; Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao, &lt;strong>;Yan Zhu&lt;/strong>;, Qifan Wang, Hongning Wang, Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jGyMUum1Lq&quot;>;Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union&lt;/a>;&lt;br />; Zifu Wang, &lt;strong>;Maxim Berman&lt;/strong>;, Amal Rannen-Triki, Philip Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0H5fRQcpQ7&quot;>;RoboHive: A Unified Framework for Robot Learning&lt;/a>;&lt;br />; &lt;strong>;Vikash Kumar&lt;/strong>;, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Abhishek Gupta, &lt;strong>;Aravind Rajeswaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vn5qZGxGj3&quot;>;SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data&lt;/a>;&lt;br />; Mélisande Teng, Amna Elmustafa, Benjamin Akera, Yoshua Bengio, Hager Radi, &lt;strong>;Hugo Larochelle&lt;/strong>;, David Rolnick &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sqTcCXkG4P&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Yangsibo Huang*, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Amer Sinha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xpjsOQtKqx&quot;>;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/a>;&lt;br />; &lt;strong>;Yonglong Tian&lt;/strong>;, &lt;strong>;Lijie Fan&lt;/strong>;, Phillip Isola, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EPz1DcdPVE&quot;>;Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning&lt;/a>;&lt;br />; &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;Nicole Mitchell&lt;/strong>;, &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>;Michael Reneer&lt;/strong>;, &lt;strong>;Zachary Garrett&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zWxKYyW9ik&quot;>;Universality and Limitations of Prompt Tuning&lt;/a>;&lt;br />; Yihan Wang, Jatin Chauhan, Wei Wang, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sovxUzPzLN&quot;>;Unsupervised Semantic Correspondence Using Stable Diffusion&lt;/a>;&lt;br />; Eric Hedlin, Gopal Sharma, Shweta Mahajan, &lt;strong>;Hossam Isack&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;,&lt;strong>; &lt;/strong>;Kwang Moo Yi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QEDjXv9OyY&quot;>;YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus&lt;/a>;&lt;br />; &lt;strong>;Dave Uthus&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>;, &lt;strong>;Manfred Georg&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=swNtr6vGqg&quot;>;The Noise Level in Linear Regression with Dependent Data&lt;/a>;&lt;br />; Ingvar Ziemann, &lt;strong>;Stephen Tu&lt;/strong>;, George J. Pappas, Nikolai Matni &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8223613466421639113/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at NeurIPS 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s72-c/google_research_sticker-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6514019106482066697&lt;/id>;&lt;published>;2023-12-08T10:34:00.000-08:00&lt;/published>;&lt;updated>;2024-01-08T07:50:55.303-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparsity-preserving differentially private training&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yangsibo Huang, Research Intern, and Chiyuan Zhang, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s1600/Sparse%20DP-SGD.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;em>;Large embedding models&lt;/em>; have emerged as a fundamental tool for various applications in recommendation systems [&lt;a href=&quot;https://research.google/pubs/pub45530/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.05158&quot;>;2&lt;/a>;] and natural language processing [&lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;>;3&lt;/a>;, &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;5&lt;/a>;]. Such models enable the integration of non-numerical data into deep learning models by mapping categorical or &lt;a href=&quot;https://en.wikipedia.org/wiki/String_(computer_science)&quot;>;string&lt;/a>;-valued input attributes with large vocabularies to fixed-length representation vectors using embedding layers. These models are widely deployed in personalized recommendation systems and achieve state-of-the-art performance in language tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language modeling&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>;. In many such scenarios, privacy is an equally important feature when deploying those models. As a result, various techniques have been proposed to enable private data analysis. Among those, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) is a widely adopted definition that limits exposure of individual user information while still allowing for the analysis of population-level patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; For training deep neural networks with DP guarantees, the most widely used algorithm is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; (DP &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>;). One key component of DP-SGD is adding &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to every coordinate of the gradient vectors during training. However, this creates scalability challenges when applied to &lt;em>;large embedding models&lt;/em>;, because they rely on gradient sparsity for efficient training, but adding noise to all the coordinates destroys sparsity. &lt;/p>; &lt;p>; To mitigate this gradient sparsity problem, in “&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;” (to be presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), we propose a new algorithm called &lt;em>;adaptive filtering-enabled sparse training&lt;/em>; (DP-AdaFEST). At a high level, the algorithm maintains the sparsity of the gradient by selecting only a subset of feature rows to which noise is added at each iteration. The key is to make such selections differentially private so that a three-way balance is achieved among the privacy cost, the training efficiency, and the model utility. Our empirical evaluation shows that DP-AdaFEST achieves a substantially sparser gradient, with a reduction in gradient size of over 10&lt;sup>;5&lt;/sup>;X compared to the dense gradient produced by standard DP-SGD, while maintaining comparable levels of accuracy 。 This gradient size reduction could translate into 20X wall-clock time improvement. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview&lt;/h2>; &lt;p>; To better understand the challenges and our solutions to the gradient sparsity problem, let us start with an overview of how DP-SGD works during training. As illustrated by the figure below, DP-SGD operates by clipping the gradient contribution from each example in the current random subset of samples (called a mini-batch), and adding coordinate-wise &lt;a href=&quot;https://en. wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to the average gradient during each iteration of &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; （新加坡元）。 DP-SGD has demonstrated its effectiveness in protecting user privacy while maintaining model utility in a variety of applications [&lt;a href=&quot;https://arxiv.org/pdf/2204.13650.pdf&quot;>;6&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2110.06500.pdf&quot;>;7&lt;/a>;]. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s1999/image5.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;An illustration of how DP-SGD works. During each training step, a mini-batch of examples is sampled, and used to compute the per-example gradients. Those gradients are processed through clipping, aggregation and summation of Gaussian noise to produce the final privatized gradients.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The challenges of applying DP-SGD to large embedding models mainly come from 1) the non-numerical feature fields like user/product IDs and categories, and 2) words and tokens that are transformed into dense vectors through an embedding layer. Due to the vocabulary sizes of those features, the process requires large embedding tables with a substantial number of parameters. In contrast to the number of parameters, the gradient updates are usually extremely sparse because each mini-batch of examples only activates a tiny fraction of embedding rows (the figure below visualizes the ratio of zero-valued coordinates, ie, the sparsity, of the gradients under various batch sizes). This sparsity is heavily leveraged for industrial applications that efficiently handle the training of large-scale embeddings. For example, &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Google Cloud TPUs&lt;/a>;, custom-designed AI accelerators that are optimized for training and inference of large AI models, have &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/building-large-scale-recommenders-using-cloud-tpus&quot;>;dedicated APIs&lt;/a>; to handle large embeddings with sparse updates. This leads to &lt;a href=&quot;https://eng.snap.com/training-models-with-tpus&quot;>;significantly improved training throughput&lt;/a>; compared to training on GPUs, which at this time did not have specialized optimization for sparse embedding lookups. On the other hand, DP-SGD completely destroys the gradient sparsity because it requires adding independent Gaussian noise to &lt;em>;all&lt;/em>; the coordinates. This creates a road block for private training of large embedding models as the training efficiency would be significantly reduced compared to non-private training. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s1814/image3.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1058&quot; data-original-width=&quot;1814&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Embedding gradient sparsity (the fraction of zero-value gradient coordinates) in the &lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge -dataset&quot;>;Criteo pCTR&lt;/a>; model (see below). The figure reports the gradient sparsity, averaged over 50 update steps, of the top five categorical features (out of a total of 26) with the highest number of buckets, as well as the sparsity of all categorical features. The sprasity decreases with the batch size as more examples hit more rows in the embedding table, creating non-zero gradients. However, the sparsity is above 0.97 even for very large batch sizes. This pattern is consistently observed for all the five features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Algorithm&lt;/h2>; &lt;p>; Our algorithm is built by extending standard DP-SGD with an extra mechanism at each iteration to privately select the “hot features”, which are the features that are activated by multiple training examples in the current mini-batch. As illustrated below, the mechanism works in a few steps: &lt;/p>; &lt;ol>; &lt;li>;Compute how many examples contributed to each feature bucket (we call each of the possible values of a categorical feature a “bucket”). &lt;/li>;&lt;li>;Restrict the total contribution from each example by clipping their counts. &lt;/li>;&lt;li>;Add Gaussian noise to the contribution count of each feature bucket. &lt;/li>;&lt;li>;Select only the features to be included in the gradient update that have a count above a given threshold (a sparsity-controlling parameter), thus maintaining sparsity. This mechanism is differentially private, and the privacy cost can be easily computed by composing it with the standard DP-SGD iterations. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of the algorithm on a synthetic categorical feature that has 20 buckets. We compute the number of examples contributing to each bucket, adjust the value based on per-example total contributions (including those to other features), add Gaussian noise, and retain only those buckets with a noisy contribution exceeding the threshold for (noisy) gradient update.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Theoretical motivation&lt;/h2>; &lt;p>; We provide the theoretical motivation that underlies DP-AdaFEST by viewing it as optimization using stochastic &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_complexity_(optimization)&quot;>;gradient oracles&lt;/a>;. Standard analysis of stochastic gradient descent in a theoretical setting decomposes the test error of the model into &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot;>;“bias” and “variance” terms&lt;/a>;. The advantage of DP-AdaFEST can be viewed as reducing variance at the cost of slightly increasing the bias. This is because DP-AdaFEST adds noise to a smaller set of coordinates compared to DP-SGD, which adds noise to all the coordinates. On the other hand, DP-AdaFEST introduces some bias to the gradients since the gradient on the embedding features are dropped with some probability. We refer the interested reader to Section 3.4 of the &lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We evaluate the effectiveness of our algorithm with large embedding model applications, on public datasets, including one ad prediction dataset (&lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo-Kaggle&lt;/a>;) and one language understanding dataset (&lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;SST-2&lt;/a>;). We use &lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>; as a baseline comparison. &lt;/p>; &lt;p>; The effectiveness of DP-AdaFEST is evident in the figure below, where it achieves significantly higher gradient size reduction (ie, gradient sparsity) than the baseline while maintaining the same level of utility (ie, only minimal performance降解）。 &lt;/p>; &lt;p>; Specifically, on the Criteo-Kaggle dataset, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 5x10&lt;sup>;5&lt;/sup>; times while maintaining a comparable &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;>;AUC&lt;/a>; (which we define as a loss of less than 0.005). This reduction translates into a more efficient and cost-effective training process. In comparison, as shown by the green line below, the baseline method is not able to achieve reasonable cost reduction within such a small utility loss threshold. &lt;/p>; &lt;p>; In language tasks, there isn&#39;t as much potential for reducing the size of gradients, because the vocabulary used is often smaller and already quite compact (shown on the right below). However, the adoption of sparsity-preserving DP-SGD effectively obviates the dense gradient computation. Furthermore, in line with the bias-variance trade-off presented in the theoretical analysis, we note that DP-AdaFEST occasionally exhibits superior utility compared to DP-SGD when the reduction in gradient size is minimal. Conversely, when incorporating sparsity, the baseline algorithm faces challenges in maintaining utility. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s1860/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1860&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction (the ratio of the non-zero gradient value counts between regular DP-SGD and sparsity-preserving algorithms) achieved under ε =1.0 by DP -AdaFEST (our algorithm) and the baseline algorithm (&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>;) compared to DP-SGD at different thresholds for utility不同之处。 A higher curve indicates a better utility/efficiency trade-off.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, most ad prediction models are being continuously trained and evaluated. To simulate this online learning setup, we also evaluate with time-series data, which are notoriously challenging due to being non-stationary. Our evaluation uses the &lt;a href=&quot;https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/&quot;>;Criteo-1TB&lt;/a>; dataset, which comprises real-world user-click data collected over 24 days. Consistently, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 10&lt;sup>;4&lt;/sup>; times while maintaining a comparable AUC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s2500/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;891&quot; data-original-width=&quot;2500&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction achieved under ε =1.0 by DP-AdaFEST (our algorithm) and DP-SGD with exponential selection (a previous algorithm) compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms the previous method.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a new algorithm, DP-AdaFEST, for preserving gradient sparsity in differentially private training — particularly in applications involving large embedding models, a fundamental tool for various applications in recommendation systems and natural language processing. Our algorithm achieves significant reductions in gradient size while maintaining accuracy on real-world benchmark datasets. Moreover, it offers flexible options for balancing utility and efficiency via sparsity-controlling parameters, while our proposals offer much better privacy-utility loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was a collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi and Amer Sinha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6514019106482066697/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot; rel=&quot;alternate&quot; title=&quot;Sparsity-preserving differentially private training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s72-c/Sparse%20DP-SGD.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5912569868305742102&lt;/id>;&lt;published>;2023-12-07T09:51:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T09:53:23.920-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Virtual Reality&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VALID: A perceptually validated virtual avatar library for inclusion and diversity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mar Gonzalez-Franco, Research Scientist, Google AR &amp;amp; VR&lt;/span>;&lt;p>; As virtual reality (VR) and augmented reality (AR) technologies continue to grow in popularity, virtual avatars are becoming an increasingly important part of our digital interactions. In particular, virtual avatars are at the center of many social VR and AR interactions, as they are key to representing remote participants and facilitating collaboration. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the last decade, interdisciplinary scientists have dedicated a significant amount of effort to better understand the use of avatars, and have made many interesting observations, including the capacity of the users to &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frvir.2020.575943/full&quot;>;embody their avatar&lt;/a>; (ie, the illusion that the avatar body is their own) and the &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9089510&quot;>;self-avatar follower effect&lt;/a>;, which creates a binding between the actions of the avatar and the user strong enough that the avatar can actually affect user behavior. &lt;/p>; &lt;p>; The use of avatars in experiments isn&#39;t just about how users will interact and behave in VR spaces, but also about discovering the limits of human perception and neuroscience. In fact, some VR social experiments often rely on recreating scenarios that can&#39;t be reproduced easily in the real world, such as bar crawls to &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0052766&quot; >;explore ingroup vs. outgroup effects&lt;/a>;, or deception experiments, such as the &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0209704&quot;>;Milgram obedience to authority inside virtual reality&lt;/一个>;。 Other studies try to explore deep neuroscientific phenomena, like the &lt;a href=&quot;https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0453&quot;>;human mechanisms for motor control&lt;/a>;. This perhaps follows the trail of the &lt;a href=&quot;https://www.nature.com/articles/35784&quot;>;rubber hand illusion&lt;/a>; on brain plasticity, where a person can start feeling as if they own a rubber hand while their real hand is hidden behind a curtain. There is also an increased number of possible therapies for psychiatric treatment using personalized &lt;a href=&quot;https://doi.org/10.1186/s13063-022-06683-1&quot;>;avatars&lt;/a>;. In these cases, VR becomes an &lt;a href=&quot;https://en.wikipedia.org/wiki/Ecological_validity&quot;>;ecologically valid&lt;/a>; tool that allows scientists to explore or treat human behavior and perception. &lt;/p>; &lt;p>; None of these experiments and therapies could exist without good access to research tools and libraries that can enable easy experimentation. As such, multiple systems and open source tools have been released around avatar creation and animation over recent years. However, existing avatar libraries have not been validated systematically on the diversity spectrum. Societal &lt;a href=&quot;https://doi.org/10.1016/j.concog.2013.04.016&quot;>;bias and dynamics&lt;/a>; also transfer to VR/AR when interacting with avatars, which could lead to incomplete conclusions for studies on human behavior inside VR/AR. &lt;/p>; &lt;p>; To partially overcome this problem, we partnered with the University of Central Florida to create and release the open-source &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID). Described in &lt;a href=&quot;https://doi.org/10.3389/frvir.2023.1248915&quot;>;our recent paper&lt;/a>;, published in &lt;a href=&quot;https://www.frontiersin.org/journals/virtual -reality&quot;>;&lt;i>;Frontiers in Virtual Reality&lt;/i>;&lt;/a>;, this library of avatars is readily available for usage in VR/AR experiments and includes 210 avatars of seven different races and ethnicities recognized by the US Census Bureau 。 The avatars have been perceptually validated and designed to advance diversity and inclusion in virtual avatar research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s1717/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1413&quot; data-original-width=&quot;1717&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Headshots of all 42 base avatars available on the VALID library were created in extensive interaction with members of the 7 ethnic and racial groups from the &lt;a href=&quot;https://www .federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs-race-and-ethnicity-statistical-standards&quot;>;Federal Register&lt;/a>;, which include (AIAN, Asian, Black, Hispanic, MENA, NHPI and White).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Creation and validation of the library&lt;/h2>; &lt;p>; Our initial selection of races and ethnicities for the diverse avatar library follows the most recent guidelines of the &lt;a href=&quot;https://www.npr.org/2023/01/26/1151608403/mena-race-categories-us-census -middle-eastern-latino-hispanic&quot;>;US Census Bureau&lt;/a>; that as of 2023 recommended the use of 7 ethnic and racial groups representing a large demographic of the US society, which can also be extrapolated to the global population. These groups include &lt;em>;Hispanic or Latino&lt;/em>;, &lt;em>;American Indian or Alaska Native (AIAN), Asian,&lt;/em>; &lt;em>;Black or African American,&lt;/em>; &lt;em>;Native Hawaiian or Other Pacific Islander (NHPI),&lt;/em>; &lt;em>;White, Middle East or North Africa&lt;/em>; (MENA). We envision the library will continue to evolve to bring even more diversity and representation with future additions of avatars. &lt;/p>; &lt;p>; The avatars were hand modeled and created using a process that combined average facial features with extensive collaboration with representative stakeholders from each racial group, where their feedback was used to artistically modify the facial mesh of the avatars. Then we conducted an online study with participants from 33 countries to determine whether the race and gender of each avatar in the library are recognizable. In addition to the avatars, we also provide labels statistically validated through observation of users for the race and gender of all 42 base avatars (see below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V /s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;736&quot; data-original-width=&quot;1999&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the headshots of a Black/African American avatar presented to participants during the validation of the library.&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We found that all Asian, Black, and White avatars were universally identified as their modeled race by all participants, while our American Indian or Native Alaskan (AIAN ), Hispanic, and Middle Eastern or North African (MENA) avatars were typically only identified by participants of the same race. This also indicates that participant race can improve identification of a virtual avatar of the same race. The paper accompanying the library release highlights how this ingroup familiarity should also be taken into account when studying avatar behavior in VR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s1999/image1.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1509&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Confusion matrix heatmap of agreement rates for the 42 base avatars separated by other-race participants and same-race participants. One interesting aspect visible in this matrix, is that participants were significantly better at identifying the avatars of their own race than other races.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset details&lt;/h2>; &lt;p>; Our models are available in &lt;a href=&quot;https://www.autodesk.com/products/fbx/overview&quot;>;FBX format&lt;/a>;, are compatible with previous avatar libraries like the commonly used &lt;a href=&quot;https://github.com/microsoft/Microsoft-Rocketbox&quot;>;Rocketbox&lt;/a>;, and can be easily integrated into most game engines such as &lt;a href=&quot;https://unity.com/&quot;>;Unity&lt;/a>; and &lt;a href=&quot;https://www.unrealengine.com/&quot;>;Unreal&lt;/a>;. Additionally, the avatars come with 69 bones and 65 facial blendshapes to enable researchers and developers to easily create and apply dynamic facial expressions and animations. The avatars were intentionally made to be partially cartoonish to avoid extreme look-a-like scenarios in which a person could be impersonated, but still representative enough to be able to run reliable user studies and social experiments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s1999/image4.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Images of the skeleton rigging (bones that allow for animation) and some facial blend shapes included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;p>; The avatars can be further combined with variations of casual attires and five professional attires, including medical, military, worker and business. This is an intentional improvement from prior libraries that in some cases reproduced stereotypical gender and racial bias into the avatar attires, and provided very limited diversity to certain professional avatars. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s1537/image5.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1023&quot; data-original-width=&quot;1537&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Images of some sample attire included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Get started with VALID&lt;/h2 >; &lt;p>; We believe that the &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID) will be a valuable resource for researchers and developers working on VR/AR applications. We hope it will help to create more inclusive and equitable virtual experiences. To this end, we invite you to explore the avatar library, which we have released under the open source &lt;a href=&quot;https://en.wikipedia.org/wiki/MIT_License&quot;>;MIT license&lt;/a>;. You can download the avatars and use them in a variety of settings at no charge. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This library of avatars was born out of a collaboration with Tiffany D. Do, Steve Zelenty and Prof. Ryan P McMahan from the University of Central Florida.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5912569868305742102/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type =&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html#comment-form&quot; rel=&quot;replies&quot; title= &quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;edit&quot; type=&quot;application/atom+ xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html&quot; rel=&quot;alternate&quot; title=&quot;VALID: A perceptually validated virtual avatar library for inclusion and diversity&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s72 -c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;published>;2023-12-05T17:32:00.000-08:00&lt;/published>;&lt;updated >;2024-01-03T14:16:52.565-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;EMNLP&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Google is proud to be a &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;Diamond Sponsor&lt;/a>; of &lt;a href=&quot;https://2023.emnlp.org/&quot;>;Empirical Methods in Natural Language Processing&lt;/a>; (EMNLP 2023), a premier annual conference, which is being held this week in Sentosa, Singapore. Google has a strong presence at this year&#39;s conference with over 65 accepted papers and active involvement in 11 workshops and tutorials. Google is also happy to be a &lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;Major Sponsor&lt;/a>; for the &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;Widening NLP&lt;/a>; workshop (WiNLP), which aims to highlight global representations of people, perspectives, and cultures in AI and ML. We look forward to sharing some of our extensive NLP research and expanding our partnership with the broader research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We hope you&#39;ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in NLP, and check out some of the scheduled booth activities (eg, demos and Q&amp;amp;A sessions listed below). Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) and &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember=true&quot;>;LinkedIn&lt;/a>; accounts to find out more about the Google booth activities at EMNLP 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at EMNLP 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Sponsorship Chair: &lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; Industry Track Chair: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; Senior Program Committee: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Annie Louis&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brian Roark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax: Structured Probability Distributions for JAX&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Miloš Stanojević&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Laurent Sartran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.11077.pdf&quot;>;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;, &lt;em>;Hannah Sterz&lt;/em>;, &lt;em>;Indraneil Paul&lt;/em>;, &lt;em>;Sukannya Purkayastha&lt;/em>;, &lt;em>;Leon Engländer&lt;/em>;,&lt;em>; Timo Imhof&lt;/em>;, &lt;em>;Ivan Vulić&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Iryna Gurevych&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet: Bridging the Data Gap in Document Pre-training&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;, &lt;strong>;&lt;em>;Jin Miao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xiaoyu Sun&lt;/em>;&lt;/strong>;, &lt;em>;Jiayi Chen&lt;/em>;, &lt;em>;Alexander Hauptmann&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wei Wei&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.08592.pdf&quot;>;AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-Powered Applications&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Robinson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks&lt;/a>; &lt;br />; &lt;em>;Mete Ismayilzada&lt;/em>;, &lt;em>;Debjit Paul&lt;/em>;, &lt;em>;Syrielle Montariol&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;, &lt;em>;Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;Large Language Models Can Self-Improve&lt;/a>; &lt;br />; &lt;em>;Jiaxin Huang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Shixiang Shane Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuexin Wu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hongkun Yu&lt;/em>;&lt;/strong>;, &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;Dissecting Recall of Factual Associations in Auto-Regressive Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot;>;Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;em>;Yoav Goldberg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yichao Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Bradley Wendt&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jing Xie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sandeep Tata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;Measuring Attribution in Natural Language Generation Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Hannah Rashkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Lamm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Collins&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gaurav Singh Tomar&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Iulia Turc&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011.pdf&quot;>;Inverse Scaling Can Become U-Shaped&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;, &lt;em>;Danqing Wang&lt;/em>;, &lt;em>;Liangming Pan&lt;/em>;, &lt;em>;Zhenqiao Song&lt;/em>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;William Yang Wang&lt;/em>;, &lt;em>;Lei Li&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-Based Method&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;, &lt;em>;Nadav Oved&lt;/em>;,&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roi Reichart&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.04347.pdf&quot;>;Investigating Efficiently Extending Transformers for Long-Input Summarization&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09744.pdf&quot;>;DSI++: Updating Transformer Memory with New Documents&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Emma Strubell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12029.pdf&quot;>;MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup&lt;/a>; &lt;br />; &lt;em>;Hua Shen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vicky Zayats&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Johann C Rocholl&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel David Walker&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dirk Padfield&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d: Turning Questions into Dialogs to Teach Models How to Search&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ido Hakimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;Emergence of Abstract State Representations in Embodied Sequence Modeling&lt;/a>; &lt;br />; &lt;em>;Tian Yun&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Zilai Zeng&lt;/em>;, &lt;em>;Kunal Handa&lt;/em>;, &lt;strong>;&lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bo Pang&lt;/em>;&lt;/strong>;, &lt;em>;Ellie Pavlick&lt;/em>;, &lt;em>;Chen Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;Evaluating and Modeling Attribution for Cross-Lingual Question Answering&lt;/a>; &lt;br />; &lt;em>;Benjamin Muller&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;Weakly-Supervised Learning of Visual Relations in Multimodal Pre-training&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lisa Anne Hendricks&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13286.pdf&quot;>;How Do Languages Influence Each Other? Studying Cross-Lingual Data Sharing During LM Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;, &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;, &lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models&lt;/a>; &lt;br>; &lt;em>;Benjamin Minixhofer&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;, &lt;em>;Ivan Vulić&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.01328.pdf&quot;>;IC3: Image Captioning by Committee Consensus&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;, &lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; John Canny&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Ido Dagan&lt;/em>;, &lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;Evaluating Large Language Models on Controlled Generation Tasks&lt;/a>; &lt;br />; &lt;em>;Jiao Sun&lt;/em>;, &lt;em>;Yufei Tian&lt;/em>;, &lt;em>;Wangchunshu Zhou&lt;/em>;, &lt;em>;Nan Xu&lt;/em>;, &lt;em>;Qian Hu&lt;/em>;, &lt;em>;Rahul Gupta&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;Nanyun Peng&lt;/em>;, &lt;em>;Xuezhe Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Daniel Deutsch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;Transcending Scaling Laws with 0.1% Extra Compute&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; Hyung Won Chung&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;, &lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Siamak Shakeri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Huaixiu Steven Zheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Donald&lt;/em>; &lt;em>;Metzler&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;Data Similarity is Not Enough to Explain Language Model Performance&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Emily Reif&lt;/em>;&lt;/strong>;, &lt;em>;David Mimno&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00913.pdf&quot;>;Self-Influence Guided Data Reweighting for Language Model Pre-training&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Tolga Bolukbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;, &lt;em>; Sarath Chandar&lt;/em>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;ReTAG: Reasoning Aware Table to Analytic Text Generation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS: Using a New Multilingual Lexicon for Low-Resource Machine Translation&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;Video-Helpful Multimodal Machine Translation&lt;/a>; &lt;br />; &lt;em>;Yihang Li, Shuichiro Shimizu&lt;/em>;, &lt;em>;Chenhui Chu&lt;/em>;, &lt;em>;Sadao Kurohashi&lt;/em>;, &lt;strong>;&lt;em>;Wei Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;Symbol Tuning Improves In-Context Learning in Language Models&lt;/a>; &lt;br />; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt;/em>;, &lt;em>;Xiangning Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Da Huang&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Xinyun&lt;/em>; &lt;em>;Chen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yifeng Lu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Tengyu Ma&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;&quot;Don&#39;t Take This Out of Context!&quot; On the Need for Contextual Models and Evaluations for Stylistic Rewriting&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;, &lt;em>;Xuhui Zhou&lt;/em>;, &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon: Multilingual QA with Only 5 Examples&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chris Alberti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Joshua Maynez&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ji Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kuzman Ganchev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Eugene Kharitonov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Damien Vincent&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zalán Borsos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Matt Sharifi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD: A Programmable Task-Oriented Dialog System&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeffrey Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuan Cao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghav Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hagen Soltau&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Izhak Shafran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yonghui Wu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;Selectively Answering Ambiguous Questions&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael JQ Zhang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Eisenstein&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;strong>;&lt;em>;Rahul Goel&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Waleed Ammar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aditya Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Motoki Sano&lt;/em>;&lt;/strong>;,&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Max Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;David Greene&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chuan He&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhou Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM: Detecting Factual Errors via Cross Examination&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;, &lt;em>;May Hamri&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Geoff Brown&lt;/em>;&lt;/strong>;, &lt;em>;Bryan A. Plummer&lt;/em>;, &lt;em>;Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;, &lt;strong>;&lt;em>;Jianmo Ni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mandy Guo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg=AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Idris Abdulmumin&lt;/em>;, &lt;em>;Abinew Ali Ayele&lt;/em>;, &lt;em>;Nedjma Ousidhoum&lt;/em>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Seid Muhie Yimam&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Oumaima Hourrane&lt;/em>;, &lt;em>;Alipio Jorge&lt;/em>;, &lt;em>;Pavel Brazdil&lt;/em>;, &lt;em>;Felermino D. M&lt;/em>;. &lt;em>;A. Ali&lt;/em>;, &lt;em>;Davis David&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Bello Shehu-Bello&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;, &lt;em>;Samuel Rutunda&lt;/em>;, &lt;em>;Tadesse Destaw Belay&lt;/em>;, &lt;em>;Wendimu Baye Messelle&lt;/em>;,&lt;em>; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;, &lt;em>;Sisay Adugna Chala&lt;/em>;, &lt;em>;Hagos Tesfahun Gebremichael&lt;/em>;,&lt;em>; Bernard Opoku&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;Optimizing Retrieval-Augmented Reader Models via Token Elimination&lt;/a>; &lt;br />; &lt;em>;Moshe Berchansky&lt;/em>;, &lt;em>;Peter Izsak&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;em>; Ido Dagan&lt;/em>;, &lt;em>;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aditya Siddhant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;, &lt;em>;Michiel de Jong&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Federico Lebron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752.pdf&quot;>;CoLT5: Faster Long-Range Transformers with Conditional Computation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tao Lei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michiel de Jong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Santiago Ontanon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddhartha Brahma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandy Guo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yun-Hsuan Sung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16523.pdf&quot;>;Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas Blumm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;, &lt;em>;Alex Beutel&lt;/em>;, &lt;strong>;&lt;em>;Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;Universal Self-Adaptive Prompting&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Ruoxi Sun, Hootan Nakhost&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zorik Gekhman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.07871.pdf&quot;>;Hierarchical Pre-training on Multimodal Electronic Health Records&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;, &lt;em>;Junyu Luo&lt;/em>;, &lt;em>;Jiaqi Wang&lt;/em>;, &lt;em>;Ziyi Yin&lt;/em>;, &lt;em>;Suhan Cui&lt;/em>;, &lt;em>;Yuan Zhong&lt;/em>;, &lt;strong>;&lt;em>;Yaqing Wang&lt;/em>;&lt;/strong>;, &lt;em>;Fenglong Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841.pdf&quot;>;How Does Generative Retrieval Scale to Millions of Passages?&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Kai Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;, &lt;em>;Jimmy Lin&lt;/em>;, &lt;strong>;&lt;em>;Donald Metzler&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Findings of EMNLP&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs&lt;/a>; &lt;br />; &lt;em>;Jiefeng Chen&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Jinsung Yoon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sayna Ebrahimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Somesh Jha&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;A Comprehensive Evaluation of Tool-Assisted Generation Strategies&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER: One Pass Answer Generation and Evidence Retrieval&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM: Towards Multilingual Visual Question Answering&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Linting Xue, Michal Yarom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Julien&lt;/em>; &lt;em>;Amelot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xi Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431v1.pdf&quot;>;SDOH-NLI: A Dataset for Inferring Social Determinants of Health from Clinical Notes&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;, &lt;em>;Eric Loreaux&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Tal Schuster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Jun Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;Machine Reading Comprehension Using Case-based Reasoning&lt;/a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;, &lt;em>;Dhruv Agarwal&lt;/em>;, &lt;em>;Mudit Chaudhary&lt;/em>;, &lt;em>;Wenlong Zhao&lt;/em>;, &lt;em>;Rajarshi Das&lt;/em>;,&lt;em>; Jay-Yoon Lee&lt;/em>;, &lt;em>;Hannaneh Hajishirzi&lt;/em>;, &lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;, &lt;em>;Andrew McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;Cross-lingual Open-Retrieval Question Answering for African Languages&lt;/a>; &lt;br />; &lt;em>;Odunayo Ogundepo&lt;/em>;, &lt;em>;Tajuddeen Gwadabe&lt;/em>;, &lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Bonaventure FP Dossou&lt;/em>;, &lt;em>;Abdou Aziz DIOP&lt;/em>;, &lt;em>;Claytone Sikasote&lt;/em>;, &lt;em>;Gilles HACHEME&lt;/em>;, &lt;em>;Happy Buzaaba&lt;/em>;,&lt;em>; Ignatius Ezeani&lt;/em>;, &lt;em>;Rooweither Mabuya&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Chris&lt;/em>; &lt;em>;Chinenye Emezue&lt;/em>;, &lt;em>;Albert Kahira&lt;/em>;, &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Akintunde Oladipo&lt;/em>;, &lt;em>;Abraham Toluwase Owodunni&lt;/em>;, &lt;em>;Atnafu Lambebo Tonja&lt;/em>;, &lt;em>;Iyanuoluwa Shode&lt;/em>;, &lt;em>;Akari Asai&lt;/em>;, &lt;em>;Anuoluwapo Aremu&lt;/em>;, &lt;em>;Ayodele Awokoya&lt;/em>;, &lt;em>;Bernard Opoku&lt;/em>;, &lt;em>;Chiamaka Ijeoma Chukwuneke&lt;/em>;, &lt;em>;Christine Mwase&lt;/em>;, &lt;em>;Clemencia Siro&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>;, &lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;, &lt;em>;Verrah Akinyi Otiende&lt;/em>;, &lt;em>;Andre Niyongabo Rubungo&lt;/em>;, &lt;em>;Boyd Sinkala&lt;/em>;, &lt;em>;Daniel Ajisafe&lt;/em>;, &lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;, &lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;, &lt;em>;Mofetoluwa Adeyemi&lt;/em>;, &lt;em>;Mofya Phiri&lt;/em>;, &lt;em>;Orevaoghene Ahia&lt;/em>;, &lt;em>;Ruqayya Nasir Iro&lt;/em>;, &lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Polina Zablotskaia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Du Phan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shashi Narayan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;Sources of Hallucination by Large Language Models on Inference Tasks&lt;/a>; &lt;br />; &lt;em>;Nick McKenna&lt;/em>;, &lt;em>;Tianyi Li&lt;/em>;, &lt;em>;Liang Cheng&lt;/em>;, &lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;, &lt;em>;Mark Johnson&lt;/em>;, &lt;em>;Mark Steedman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;Don&#39;t Add, Don&#39;t Miss: Effective Content Preserving Generation from Pre-selected Text Spans&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Eran Hirsch&lt;/em>;, &lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Katherine Hermann&lt;/strong>;&lt;/em>;, &lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;Understanding HTML with Large Language Models&lt;/a>; &lt; br />; &lt;strong>;&lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ofir Nachum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yingjie Miao&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Mustafa Safdari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Austin Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha&lt;/em>; &lt; em>;Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Noah Fiedel&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Aleksandra Faust&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;Improving the Robustness of Summarization Models by Detecting and Removing Input Noise&lt;/a>; &lt;br />; &lt;em>;Kundan Krishna&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jiaming Luo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mohammad&lt;/em>; &lt;em>;Saleh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter J. Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2310.15916.pdf&quot;>;In-Context Learning Creates Task Vectors&lt;/a>; &lt;br />; &lt;em>;Roee Hendel&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10544.pdf&quot;>;Pre -training Without Attention&lt;/a>; &lt;br />; &lt;em>;Junxiong Wang&lt;/em>;, &lt;em>;Jing Nathan Yan&lt;/em>;, &lt;strong>;&lt;em>;Albert Gu&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Alexander M Rush&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>;MUX-PLMs: Data Multiplexing for High-Throughput Language Models&lt;/a>; &lt;br />; &lt;em>;Vishvak Murahari&lt;/em>;, &lt;em>;Ameet Deshpande&lt;/em>;, &lt;em>;Carlos E Jimenez&lt;/em>;,&lt;em>; &lt;strong >;Izhak Shafran&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yuan&lt;/em>; &lt;em>;Cao&lt;/em>;&lt;/ strong>;, &lt;em>;Karthik R Narasimhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14408.pdf&quot;>;PaRaDe: Passage Ranking Using Demonstrations with LLMs&lt;/ a>; &lt;br />; &lt;em>;Andrew Drozdov&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Zhuyun Dai&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhen Qin&lt;/em>;&lt;/strong>;, &lt;em>;Razieh Rahimi&lt;/em>;, &lt;strong>;&lt;em>;Xuanhui Wang&lt;/em>;&lt;/strong >;,&lt;strong>; &lt;em>;Dana Alon&lt;/em>;&lt;/strong>;, &lt;em>;Mohit Iyyer&lt;/em>;, &lt;em>;Andrew McCallum&lt;/em>;, &lt;em>;Donald Metzler&lt;sup>;*&lt;/ sup>;&lt;/em>;,&lt;strong>; &lt;em>;Kai Hui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13678.pdf&quot;>; Long-Form Speech Translation Through Segmentation with Finite-State Decoding Constraints on Large Language Models&lt;/a>; &lt;br />; &lt;em>;Arya D. McCarthy&lt;/em>;, &lt;strong>;&lt;em>;Hao Zhang&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Felix Stahlberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ke Wu&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;Unsupervised Opinion Summarization Using Approximate Geodesics&lt;/a>; &lt;br />; &lt;em>;Somnath Basu Roy Chowdhury&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Nicholas Monath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kumar Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Amr Ahmed&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02883. pdf&quot;>;SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sercan O . Arik&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rajarishi Sinha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hootan Nakhost&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em >;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/EMNLP_2023_retrieval.pdf&quot;>;Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty&lt;/a>; &lt;br />; &lt;em>;Zi Lin&lt; /em>;, &lt;strong>;&lt;em>;Quan Yuan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Panupong Pasupat&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Jingbo Shang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.08740.pdf&quot;>;A Zero-Shot Language Agent for Computer Control with Structured Reflection&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Tao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Zhiwei Deng&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wang&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches&lt;/a>; &lt;br / >; &lt;em>;Daniel Fried&lt;/em>;, &lt;em>;Nicholas Tomlin&lt;/em>;, &lt;em>;Jennifer Hu&lt;/em>;,&lt;strong>; &lt;em>;Roma Patel&lt;/em>;&lt;/strong>;, &lt;strong >;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;Improving Classifier Robustness Through Active Generation of Pairwise Counterfactuals &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ananth Balashankar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Qin &lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nithum Thain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jilin Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ed H.&lt;/em>; &lt;em>;Chi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Beutel&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224.pdf&quot;>;mmT5: Modular Multilingual Pre-training Solves Source Language Hallucinations&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Francesco Piccinno&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt; strong>;&lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Machel Reid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian&lt;/em>; &lt;em>;Ruder&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;Scaling Laws vs Model Architectures: How Does Inductive Bias Influence Scaling?&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Samira Abnar&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; William Fedus&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jinfeng Rao&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dani Yogatama&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00142. pdf&quot;>;TaTA: A Multilingual Table-to-Text Dataset for African Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vitaly Nikolaev,&lt;/em>; &lt;em>;Jan A. Botha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael Chavinda&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.11938.pdf&quot;>;XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Ruder ,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Gutkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mihir Kale&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Min Ma&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Parker Riley&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jean Michel Amath Sarr&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;John Frederick&lt;/em>; &lt;em>;Wieting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nitish Gupta&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna Katanova&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christo Kirov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dana L Dickinson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Roark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bidisha Samanta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Connie Tao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Ifeoluwa Adelani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vera Axelrod&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Isaac Rayburn&lt;/em>; &lt;em>;Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Garrette&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Reeve Ingle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dmitry Panteleev&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00693.pdf&quot;>;On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval&lt;/a>; &lt;br />; &lt;em>;Jiayi Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;em>;Aidong Zhang&lt;/em>;, &lt;em>;Wei Wei&lt;sup>;*&lt;/sup>;&lt;/ em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px ;&quot;>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701752203060961&amp;amp;usg =AOvVaw3sRozMNVYuwvyXeLluOgKI&quot;>;The Seventh Widening NLP Workshop&lt;/a>; (WiNLP) &lt;br />; Major Sponsor &lt;br />; Organizers: &lt;strong>;&lt;em>;Sunipa Dev&lt;/em>;&lt;/strong>; &lt;br />; Panelist: &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/crac2023/?pli=1&quot;>; The Sixth Workshop on Computational Models of Reference, Anaphora and Coreference&lt;/a>; (CRAC) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;The 3rd Workshop for Natural Language Processing Open Source Software&lt;/a>; (NLP-OSS) &lt;br />; Organizer: &lt;strong>;&lt; em>;Geeticka Chauhan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://splu-robonlp-2023.github.io/&quot;>;Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics&lt;/a>; (SpLU-RoboNLP) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem -benchmark.com/workshop&quot;>;Natural Language Generation, Evaluation, and Metric&lt;/a>; (GEM) &lt;br />; Organizer: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://arabicnlp2023.sigarab.org/&quot;>;The First Arabic Natural Language Processing Conference&lt;/a>; (ArabicNLP) &lt;br />; Organizer: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.bigpictureworkshop.com/&quot;>;The Big Picture: Crafting a Research Narrative&lt;/a>; (BigPicture) &lt;br />; Organizer: &lt;strong>;&lt;em>;Nora Kassner&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp .github.io/&quot;>;BlackboxNLP 2023: The 6th Workshop on Analysing and Interpreting Neural Networks for NLP&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br / >; Panelist: &lt;strong>;&lt;em>;Neel Nanda&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.conll.org/2023&quot;>;The SIGNLL Conference on Computational Natural Language Learning&lt;/a>; (CoNLL) &lt;br />; Co-Chair: &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;br />; Areas and ACs: &lt;strong>;&lt;em>;Kyle Gorman&lt; /em>;&lt;/strong>; (Speech and Phonology), &lt;strong>;&lt;em>;Fei Liu &lt;/em>;&lt;/strong>;(Natural Language Generation) &lt;/p>; &lt;p>; &lt;a href=&quot;https:// sigtyp.github.io/ws2023-mrl.html&quot;>;The Third Workshop on Multi-lingual Representation Learning&lt;/a>; (MRL) &lt;br />; Organizer: &lt;strong>;&lt;em>;Omer Goldman&lt;/em>;&lt;/strong >;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot; https://emnlp2023-creative-nlg.github.io/&quot;>;Creative Natural Language Generation&lt;/a>; &lt;br />; Organizer: &lt;em>;Tuhin Chakrabarty&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p >; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;p>; &lt;i>;This schedule is subject to change 。 Please visit the Google booth for more information.&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Developing and Utilizing Evaluation Metrics for Machine Translation &amp;amp; Improving Multilingual NLP &lt;br />; Presenter: &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jan-Thorsten Peter&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; Fri, Dec 8 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Differentiable Search Indexes &amp;amp; Generative Retrieval &lt;br />; Presenter: &lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;, &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; Fri, Dec 8 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; Retrieval and Generation in a single pass &lt;br />; Presenter: &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Amplifying Adversarial Attacks &lt;br />; Presenter:&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; Automate prompt design: Universal Self-Adaptive Prompting (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; Presenter: &lt;strong>;&lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8414954450937764241/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at EMNLP 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3665865722098768988&lt;/id>;&lt;published>;2023-12-04T14:41:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T14:46:06.688-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A new quantum algorithm for classical mechanics with an exponential speedup&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Robin Kothari and Rolando Somma, Research Scientists, Google Research, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Quantum computers promise to solve some problems exponentially faster than classical computers, but there are only a handful of examples with such a dramatic speedup, such as &lt;a href=&quot;https://en.wikipedia.org/wiki /Shor%27s_algorithm&quot;>;Shor&#39;s factoring algorithm&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot;>;quantum simulation&lt;/一个>;。 Of those few examples, the majority of them involve simulating physical systems that are inherently quantum mechanical — a natural application for quantum computers. But what about simulating systems that are not inherently quantum? Can quantum computers offer an exponential advantage for this? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://link.aps.org/doi/10.1103/PhysRevX.13.041041&quot;>;Exponential quantum speedup in simulating coupled classical oscillators&lt;/a>;”, published in &lt;a href=&quot;https://journals.aps.org/prx/&quot;>;Physical Review X&lt;/a>; (PRX) and presented at the &lt;a href=&quot;https://focs.computer.org/2023/&quot;>;Symposium on Foundations of Computer Science&lt;/a>; (FOCS 2023), we report on the discovery of a new quantum algorithm that offers an exponential advantage for simulating coupled &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_oscillator&quot;>;classical harmonic oscillators&lt;/a>;. These are some of the most fundamental, ubiquitous systems in nature and can describe the physics of countless natural systems, from electrical circuits to molecular vibrations to the mechanics of bridges. In collaboration with Dominic Berry of Macquarie University and Nathan Wiebe of the University of Toronto, we found a mapping that can transform any system involving coupled oscillators into a problem describing the time evolution of a quantum system. Given certain constraints, this problem can be solved with a quantum computer exponentially faster than it can with a classical computer. Further, we use this mapping to prove that any problem efficiently solvable by a quantum algorithm can be recast as a problem involving a network of coupled oscillators, albeit exponentially many of them. In addition to unlocking previously unknown applications of quantum computers, this result provides a new method of designing new quantum algorithms by reasoning purely about classical systems. &lt;/p>; &lt;br />; &lt;h2>;Simulating coupled oscillators&lt;/h2>; &lt;p>; The systems we consider consist of classical harmonic oscillators. An example of a single harmonic oscillator is a mass (such as a ball) attached to a spring. If you displace the mass from its rest position, then the spring will induce a restoring force, pushing or pulling the mass in the opposite direction. This restoring force causes the mass to oscillate back and forth. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;116&quot; data-original-width=&quot;359&quot; height=&quot;129&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A simple example of a harmonic oscillator is a mass connected to a wall by a spring. [Image Source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif&quot;>;Wikimedia&lt;/a>;]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now consider &lt;em>;coupled &lt;/em>;harmonic oscillators, where &lt;em>;multiple&lt;/em>; masses are attached to one another through springs. Displace one mass, and it will induce a wave of oscillations to pulse through the system. As one might expect, simulating the oscillations of a large number of masses on a classical computer gets increasingly difficult. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1 /s1129/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1129&quot; height =&quot;295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example system of masses connected by springs that can be simulated with the quantum algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To enable the simulation of a large number of coupled harmonic oscillators, we came up with a mapping that encodes the positions and velocities of all masses and springs into the quantum wavefunction of a system of qubits. Since the number of parameters describing the wavefunction of a system of qubits grows exponentially with the number of qubits, we can encode the information of &lt;em>;N&lt;/em>; balls into a quantum mechanical system of only about log(&lt;em>;N&lt;/em>;) qubits. As long as there is a compact description of the system (ie, the properties of the masses and the springs), we can evolve the wavefunction to learn coordinates of the balls and springs at a later time with far fewer resources than if we had used a naïve classical approach to simulate the balls and springs. &lt;/p>; &lt;p>; We showed that a certain class of coupled-classical oscillator systems can be efficiently simulated on a quantum computer. But this alone does not rule out the possibility that there exists some as-yet-unknown clever classical algorithm that is similarly efficient in its use of resources. To show that our quantum algorithm achieves an exponential speedup over &lt;em>;any&lt;/em>; possible classical algorithm, we provide two additional pieces of evidence. &lt;/p>; &lt;br />; &lt;h2>;The glued-trees problem and the quantum oracle&lt;/h2>; &lt;p>; For the first piece of evidence, we use our mapping to show that the quantum algorithm can efficiently solve a famous problem about graphs known to be difficult to solve classically, called the &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>;glued-trees problem&lt;/a>;. The problem takes two branching trees — a graph whose nodes each branch to two more nodes, resembling the branching paths of a tree — and glues their branches together through a random set of edges, as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;930&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual representation of the glued trees problem. Here we start at the node labeled ENTRANCE and are allowed to locally explore the graph, which is obtained by randomly gluing together two binary trees. The goal is to find the node labeled EXIT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The goal of the glued-trees problem is to find the exit node — the “root” of the second tree — as efficiently as possible. But the exact configuration of the nodes and edges of the glued trees are initially hidden from us. To learn about the system, we must query an &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_machine&quot;>;oracle&lt;/a>;, which can answer specific questions about the setup. This oracle allows us to explore the trees, but only locally. Decades ago, &lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;it was shown&lt;/a>; that the number of queries required to find the exit node on a classical computer is proportional to a polynomial factor of &lt;em>;N&lt;/em>;, the total number of nodes. &lt;/p>; &lt;p>; But recasting this as a problem with balls and springs, we can imagine each node as a ball and each connection between two nodes as a spring. Pluck the entrance node (the root of the first tree), and the oscillations will pulse through the trees. It only takes a time that scales with the &lt;em>;depth&lt;/em>; of the tree — which is exponentially smaller than &lt;em>;N&lt;/em>; — to reach the exit node. So, by mapping the glued-trees ball-and-spring system to a quantum system and evolving it for that time, we can detect the vibrations of the exit node and determine it exponentially faster than we could using a classical computer. &lt;/p>; &lt;br />; &lt;h2>;BQP-completeness&lt;/h2>; &lt;p>; The second and strongest piece of evidence that our algorithm is exponentially more efficient than any possible classical algorithm is revealed by examination of the set of problems a quantum computer can solve efficiently (ie, solvable in &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time&quot;>;polynomial time&lt;/a>;), referred to as &lt;a href=&quot;https://en.wikipedia.org/wiki/BQP&quot;>;bounded-error quantum polynomial time&lt;/a>; or BQP. The hardest problems in BQP are called “BQP-complete”. &lt;/p>; &lt;p>; While it is generally accepted that there exist some problems that a quantum algorithm can solve efficiently and a classical algorithm cannot, this has not yet been proven. So, the best evidence we can provide is that our problem is BQP-complete, that is, it is among the hardest problems in BQP. If someone were to find an efficient classical algorithm for solving our problem, then every problem solved by a quantum computer efficiently would be classically solvable! Not even the &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_factorization&quot;>;factoring problem&lt;/a>; (finding the prime factors of a given large number), which forms the basis of &lt;a href=&quot;https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;>;modern encryption&lt;/a>; and was famously solved by Shor&#39;s algorithm, is expected to be BQP-complete. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;563&quot; data-original-width=&quot;574&quot; height=&quot;314&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing the believed relationships of the classes BPP and BQP, which are the set of problems that can be efficiently solved on a classical computer and quantum computer, respectively. BQP-complete problems are the hardest problems in BQP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To show that our problem of simulating balls and springs is indeed BQP-complete, we start with a standard BQP-complete problem of simulating universal quantum circuits, and show that every quantum circuit can be expressed as a system of many balls coupled with springs. Therefore, our problem is also BQP-complete. &lt;/p>; &lt;br />; &lt;h2>;Implications and future work&lt;/h2>; &lt;p>; This effort also sheds light on work from 2002, when theoretical computer scientist Lov K. Grover and his colleague, Anirvan M. Sengupta, used an &lt;a href=&quot;https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319&quot;>;analogy to coupled&lt;/a>; pendulums to illustrate how Grover&#39;s famous quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Grover%27s_algorithm&quot;>;search algorithm&lt;/a>; could find the correct element in an unsorted database quadratically faster than could be done classically. With the proper setup and initial conditions, it would be possible to tell whether one of &lt;em>;N&lt;/em>; pendulums was different from the others — the analogue of finding the correct element in a database — after the system had evolved for time that was only ~√(&lt;em>;N)&lt;/em>;. While this hints at a connection between certain classical oscillating systems and quantum algorithms, it falls short of explaining why Grover&#39;s quantum algorithm achieves a quantum advantage. &lt;/p>; &lt;p>; Our results make that connection precise. We showed that the dynamics of any classical system of harmonic oscillators can indeed be equivalently understood as the dynamics of a corresponding quantum system of exponentially smaller size. In this way we can simulate Grover and Sengupta&#39;s system of pendulums on a quantum computer of log(&lt;em>;N&lt;/em>;) qubits, and find a different quantum algorithm that can find the correct element in time ~√(&lt;em>;N&lt;/em>;). The analogy we discovered between classical and quantum systems can be used to construct other quantum algorithms offering exponential speedups, where the reason for the speedups is now more evident from the way that classical waves propagate. &lt;/p>; &lt;p>; Our work also reveals that every quantum algorithm can be equivalently understood as the propagation of a classical wave in a system of coupled oscillators. This would imply that, for example, we can in principle build a classical system that solves the factoring problem after it has evolved for time that is exponentially smaller than the runtime of any known classical algorithm that solves factoring. This may look like an efficient classical algorithm for factoring, but the catch is that the number of oscillators is exponentially large, making it an impractical way to solve factoring. &lt;/p>; &lt;p>; Coupled harmonic oscillators are ubiquitous in nature, describing a broad range of systems from electrical circuits to chains of molecules to structures such as bridges. While our work here focuses on the fundamental complexity of this broad class of problems, we expect that it will guide us in searching for real-world examples of harmonic oscillator problems in which a quantum computer could offer an exponential advantage. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our Quantum Computing Science Communicator, Katie McCormick, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3665865722098768988/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html&quot; rel=&quot;alternate&quot; title=&quot;A new quantum algorithm for classical mechanics with an exponential speedup&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72-c/glued-trees.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;