<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-01-12T09:05:14.703-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="AI for Social Good"></category><category term="On-device Learning"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Quantum AI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="accessibility"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Audio"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="video"></category><category term="Awards"></category><category term="ML Fairness"></category><category term="Responsible AI"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Google Maps"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="RAI-HCT Highlights"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Cloud Platform"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Differential Privacy"></category><category term="Google Photos"></category><category term="Google+"></category><category term="Large Language Models"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Generative AI"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="Graphs"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1326&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1800430129205268706&lt;/id>;&lt;发布>;2024-01-12T09:04:00.000-08:00&lt;/发布>;&lt;更新>;2024-01- 12T09:04:41.711-08:00&lt;/updated>;&lt;title type=&quot;text&quot;>;AMIE：用于诊断医学推理和对话的研究人工智能系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;发布者：Google 研究主管 Alan Karthikesalingam 和 Vivek Natarajan&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO 5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkodC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s16000/AMIE .gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 医患对话是医学的基石，熟练且有意的沟通可以推动诊断、管理、同理心和信任。能够进行此类诊断对话的人工智能系统可以通过成为临床医生和患者等有用的对话伙伴来提高护理的可用性、可及性、质量和一致性。但接近临床医生丰富的专业知识是一项重大挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 医学领域之外的大型语言模型 (LLM) 的最新进展表明，它们可以计划、推理并使用相关上下文来进行丰富的对话。然而，良好的诊断对话有许多方面是医学领域独有的。一位高效的临床医生会获取完整的“临床病史”，并提出有助于得出鉴别诊断的明智问题。他们运用相当多的技能来建立有效的关系，清楚地提供信息，与患者共同做出明智的决定，对他们的情绪做出同理心的反应，并在下一步的护理中支持他们。虽然法学硕士可以准确地执行医学总结或回答医学问题等任务，但很少有专门针对开发此类对话诊断能力的工作。 &lt;/p>; &lt;p>; 受这一挑战的启发，我们开发了&lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot;>;Articulate Medical Intelligence Explorer (AMIE)&lt;/a>;，这是一个基于法学硕士，并针对诊断推理和对话进行了优化。我们从临床医生和患者的角度从反映现实世界临床咨询质量的多个维度对 AMIE 进行了培训和评估。为了将 AMIE 扩展到多种疾病状况、专业和场景，我们开发了一种新颖的基于自我游戏的模拟诊断对话环境，具有自动反馈机制，以丰富和加速其学习过程。我们还引入了推理时间链推理策略，以提高 AMIE 的诊断准确性和对话质量。最后，我们通过模拟与训练有素的演员的协商，在多轮对话的真实例子中前瞻性地测试了 AMIE。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcY Pb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s1200/AMIE%20GIF%201%20v2.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkd fshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s16000/AMIE%20GIF%201%20v2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;AMIE 针对诊断对话进行了优化，提出有助于减少不确定性并提高诊断准确性的问题，同时还平衡了有效临床沟通的其他要求，例如同理心、培养&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;对话式诊断AI的评估&lt;/h2>; &lt;p>;除了开发和优化AI系统本身对于诊断对话，如何评估此类系统也是一个悬而未决的问题。受到用于衡量现实环境中咨询质量和临床沟通技巧的公认工具的启发，我们构建了一个试点评估标准，以评估与病史采集、诊断准确性、临床管理、临床沟通技巧、关系培养和共情。 &lt;/p>; &lt;p>; 然后，我们设计了一项基于文本的咨询的随机、双盲交叉研究，其中经过验证的患者参与者与经过委员会认证的初级保健医生 (PCP) 或针对诊断对话优化的人工智能系统进行交互。我们以&lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structed_clinical_examination&quot;>;客观结构化临床检查&lt;/a>; (OSCE) 的方式进行咨询，这是一种实际评估中常用的方法世界以标准化和客观的方式检查临床医生的技能和能力。在典型的 OSCE 中，临床医生可能会在多个工作站之间轮换，每个工作站都模拟现实生活中的临床场景，在这些场景中，他们执行诸如与标准化患者演员（经过仔细培训以模拟患有特定病症的患者）进行咨询等任务。咨询是使用同步文本聊天工具进行的，模仿了当今大多数使用法学硕士的消费者所熟悉的界面。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETCl VRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/s1350/image4.gif&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1350&quot; height=&quot;568 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WG YwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/w640-h568/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE 是一个基于 LLM 的研究型人工智能系统，用于诊断推理和对话。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;h2>;AMIE：基于法学硕士的对话式诊断研究人工智能系统&lt;/h2>; &lt;p>; 我们在包括医学推理、医学总结和现实世界的真实数据集上训练 AMIE临床对话。 &lt;/p>; &lt;p>; 使用通过被动收集和转录现场临床就诊而开发的真实对话来培训法学硕士是可行的，但是，两个重大挑战限制了他们在培训法学硕士进行医学对话方面的有效性。首先，现有的现实世界数据往往无法捕捉广泛的医疗状况和场景，阻碍了可扩展性和全面性。其次，来自现实世界对话记录的数据往往很嘈杂，包含模棱两可的语言（包括俚语、行话、幽默和讽刺）、中断、不合语法的话语和隐含的引用。 &lt;/p>; &lt;p>; 为了解决这些限制，我们设计了一个基于自我游戏的模拟学习环境，具有自动反馈机制，可在虚拟护理环境中进行诊断医学对话，使我们能够在许多医疗条件和环境中扩展 AMIE 的知识和能力。除了所描述的真实世界数据的静态语料库之外，我们还使用此环境通过一组不断发展的模拟对话来迭代微调 AMIE。 &lt;/p>; &lt;p>; 这个过程由两个自我对弈循环组成：（1）一个“内部”自我对弈循环，其中 AMIE 利用上下文中的评论家反馈来改进其在与人工智能患者模拟器的模拟对话中的行为； （2）“外部”自我播放循环，其中一组经过改进的模拟对话被纳入后续的微调迭代中。由此产生的新版本的 AMIE 可以再次参与内部循环，从而创建一个良性的持续学习循环。 &lt;/p>; &lt;p>; 此外，我们还采用了推理时间链推理策略，使 AMIE 能够根据当前对话逐步完善其响应，以得出知情且有依据的答复。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE8 8KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/s1834/image4.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDX c9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;AMIE 使用一种新颖的基于自我游戏的模拟对话学习环境来提高多种疾病状况、专业和患者背景下的诊断对话质量。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 我们测试了模拟患者（由训练有素的演员扮演）的咨询表现，并与使用上述随机方法的 20 名真实 PCP 进行的咨询进行了比较。在一项随机、盲法交叉研究中，从专科主治医生和模拟患者的角度对 AMIE 和 PCP 进行了评估，该研究包括来自加拿大、英国和印度的 OSCE 提供者的 149 个案例场景，涉及不同的专业和疾病。值得注意的是，我们的研究并不是为了模仿传统的面对面 OSCE 评估或临床医生通常使用文本、电子邮件、聊天或远程医疗的方式。相反，我们的实验反映了当今消费者与法学硕士互动的最常见方式，这是人工智能系统参与远程诊断对话的一种潜在可扩展且熟悉的机制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_Jjw CM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s1999 /image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwh TQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过在线多轮同步文本聊天与模拟患者进行虚拟远程 OSCE 的随机研究设计概述。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;AMIE 的性能&lt;/h2>; &lt;p>; 在此设置中，我们观察到，在对两者进行评估时，AMIE 执行模拟诊断对话的效果至少与 PCP 一样好沿着多个具有临床意义的咨询质量轴。从专科医生的角度来看，AMIE 在 32 个轴中的 28 个轴上具有更高的诊断准确性和卓越的性能，从患者参与者的角度来看，在 26 个轴中的 24 个轴上，AMIE 具有更高的诊断准确性和卓越的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVR srwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/s1834/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_S vuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在我们的评估中，AMIE 在诊断对话的多个评估轴上均优于 PCP。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w- ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s1999/image9.png&quot;样式=“左边距：自动”； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLV HgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;专家评级的 top-k 诊断准确性。AMIE 和 PCP Top-k 鉴别诊断 (DDx) 准确性在 149 个场景中与地面真实诊断 (a) 和可接受的鉴别诊断 (b) 中列出的所有诊断进行比较。 Bootstrapping (n=10,000) 确认 AMIE 和 PCP DDx 准确度之间的所有前 k 个差异均显着，在&lt;a href=&quot;https://en.wikipedia.org/wiki/False_discovery_rate&quot;>;错误发现率之后 p &lt;0.05 &lt;/a>;&amp;nbsp;(FDR) 修正。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption -container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetE ptfRWgoWJ4- 4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s1892/image6.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1714&quot; data-original-width=&quot;1892&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpV gGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;诊断由专科医生评估的对话和推理质量。在 32 个轴中的 28 个轴上，AMIE 的表现优于 PCP，而在其他轴上则相当。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;局限性&lt;/h2>; &lt;p>; 我们的研究有几个局限性，应该谨慎解释。首先，我们的评估技术可能低估了人类对话的现实价值，因为我们研究中的临床医生仅限于不熟悉的文本——聊天界面，允许大规模的法学硕士与患者互动，但并不代表通常的临床实践。其次，任何此类研究都必须被视为漫长旅程中探索性的第一步。从我们在本研究中评估的法学硕士研究原型过渡到可供人们和为其提供护理的人使用的安全而强大的工具，将需要大量的额外研究。有许多重要的限制需要解决，包括现实世界约束下的实验性能，以及对健康公平、隐私、鲁棒性等重要主题的专门探索，以确保技术的安全性和可靠性。 &lt;/p>; &lt;br />; &lt;h2>;AMIE 对临床医生的帮助&lt;/h2>; &lt;p>; 在&lt;a href=&quot;https://arxiv.org/abs/2312.00164&quot;>;最近发布的预印本&lt;/a >;，我们评估了 AMIE 系统早期迭代单独生成 DDx 或作为临床医生辅助的能力。二十 (20) 名全科临床医生评估了来自&lt;em>;&lt;a href=&quot;https://www.nejm.org/&quot;>;新英格兰医学杂志&lt;/a>;&lt;/em>;的 303 个具有挑战性的真实医疗案例>; (NEJM) &lt;a href=&quot;https://www.nejm.org/case-challenges&quot;>;临床病理学会议&lt;/a>; (CPC)。每份病例报告均由两名随机接受两种辅助条件之一的临床医生阅读：搜索引擎和标准医疗资源的帮助，或除这些工具外的 AMIE 帮助。所有临床医生在使用相应的辅助工具之前都提供了基线、无协助的 DDx。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfv MTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s1999/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1022&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5 vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;协助随机读者研究设置，以调查 AMIE 对临床医生解决新英格兰医学杂志复杂诊断病例挑战的辅助效果。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AMIE 表现出的独立性能超过了无人协助的临床医生（前 10 名准确率分别为 59.1% 和 33.6%，p= 0.04）。比较两个辅助研究组，与没有 AMIE 协助的临床医生 (24.6%，p&lt;0.01) 和有搜索的临床医生 (5.45%，p=0.02) 相比，受 AMIE 协助的临床医生的前 10 名准确率更高。此外，与没有 AMIE 协助的临床医生相比，得到 AMIE 协助的临床医生得出了更全面的鉴别列表。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89Qzza HXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/s1999/image7.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1424&quot; data-original-width=&quot;1999&quot; height=&quot;456&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6 Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/w640-h456/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;除了强大的独立性能之外，使用AMIE系统还为临床医生解决这些复杂病例挑战带来了显着的辅助效果和诊断准确性的提高。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 值得注意的是，NEJM CPC 并不代表日常临床实践。它们是仅针对数百人的不寻常病例报告，因此为探讨公平或公平等重要问题提供了有限的范围。 &lt;/p>; &lt;br />; &lt;h2>;医疗保健领域大胆而负责任的研究——可能性的艺术&lt;/h2>; &lt;p>; 在世界各地，获得临床专业知识的机会仍然很少。虽然人工智能在特定的临床应用中显示出了巨大的前景，但参与临床实践的动态、对话式诊断过程需要人工智能系统尚未展示的许多功能。医生不仅拥有知识和技能，还致力于遵守无数原则，包括安全和质量、沟通、伙伴关系和团队合作、信任和专业精神。在人工智能系统中实现这些属性是一项鼓舞人心的挑战，应该负责任地、谨慎地对待。 AMIE 是我们对“可能性的艺术”的探索，这是一个仅供研究的系统，用于安全地探索未来的愿景，其中人工智能系统可能会更好地与委托我们护理的熟练临床医生的属性保持一致。它只是早期的实验工作，而不是产品，并且有一些局限性，我们认为值得进行严格和广泛的进一步科学研究，以设想一个对话式、同理心和诊断式人工智能系统可能变得安全、有用和易于使用的未来。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的研究是 Google Research 和 Google Deepmind 多个团队的共同工作。我们感谢所有的合著者——Tao Tu、Mike Schaekermann、Anil Palepu、Daniel McDuff、Jake Sunshine、Khaled Saab、Jan Freyberg、Ryutaro Tanno、Amy Wang、Brenna Li、Mohamed Amin、Sara Mahdavi、Karan Sighal、Shekoofeh阿齐兹、内纳德·托马塞夫、刘云、程勇、侯乐、阿尔伯特·韦伯森、杰克·加里森、亚什·夏尔马、阿努潘·帕塔克、苏珊特·普拉卡什、菲利普·曼斯菲尔德、施韦塔克·帕特尔、布拉德利·格林、埃娃·多米诺斯卡、蕾妮·王、尤拉吉·戈特维斯、戴尔·韦伯斯特、凯瑟琳·周、克里斯托弗·塞姆图斯、乔尔·巴拉尔、格雷格·科拉多和约西·马蒂亚斯。我们还要感谢萨米·拉赫加尔、劳伦·维纳和约翰·吉利亚德对叙事和视觉效果的支持。最后，我们感谢 Michael Howell、James Maynika、Jeff Dean、Karen DeSalvo、Zoubin Gharahmani 和 Demis Hassabis 在本项目过程中的支持&lt;/em>;。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1800430129205268706/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text/html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html&quot; rel=&quot;alternate&quot; title=&quot;AMIE：用于诊断医学推理和对话的研究人工智能系统&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suH yp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s72-c/ AMIE.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>; &lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7998118785777164574&lt;/id>;&lt;发布>;2024-01-11T14:42:00.000-08:00&lt;/发布>;&lt;更新>;2024- 01-11T14:42:51.944-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category schema =&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;大型语言模型能否识别并纠正错误？&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：Gladys Tyen，Google 研究部实习生&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQ例如xWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s1600/Backtracking.jpg&quot; style=&quot;显示：无；” />; &lt;p>; LLM 在推理任务中越来越受欢迎，例如&lt;a href=&quot;https://hotpotqa.github.io/&quot;>;多轮 QA&lt;/a>;、&lt;a href=&quot;https:// arxiv.org/abs/2207.01206&quot;>;任务完成&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2304.05128&quot;>;代码生成&lt;/a>;或&lt;a href=&quot;https:// /github.com/openai/grade-school-math&quot;>;数学&lt;/a>;。然而，就像人类一样，他们并不总是在第一次尝试时就正确解决问题，尤其是在他们没有接受过培训的任务上。因此，为了使此类系统最有用，它们应该能够 1) 确定其推理出错的地方，以及 2) 回溯以找到另一个解决方案。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 这导致了与自我纠正相关的方法激增，其中法学硕士用于识别其自身的问题。自己的输出，然后根据反馈产生改进的结果。自我纠正通常被认为是一个单一的过程，但我们决定将其分为两个部分：&lt;em>;错误查找&lt;strong>;&lt;/strong>;&lt;/em>;和&lt;em>;输出纠正&lt;/em>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2311.08516#:~:text=While%20self%2D Correction%20has%20shown,et%20al.%2C%202023) .&quot;>;LLM 无法发现推理错误，但可以纠正它们！&lt;/a>;”，我们分别测试最先进的 LLM 的错误发现和输出纠正。我们提出了 &lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake&quot;>;BIG-Bench Mistake&lt;/a>;，这是一个用于错误识别的评估基准数据集，我们用它来解决以下问题： &lt;/p>; &lt;ol>; &lt;li>;法学硕士能否发现&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;思想链&lt;/a>; (CoT) 风格推理中的逻辑错误？ &lt;/li>;&lt;li>;发现错误可以用来衡量正确性吗？ &lt;/li>;&lt;li>;知道错误在哪里，是否可以提示法学硕士回溯并得出正确答案？ &lt;/li>;&lt;li>;错误发现作为一种技能可以推广到法学硕士从未见过的任务吗？ &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;关于我们的数据集&lt;/h2>; &lt;p>; 错误查找是自然语言处理中一个尚未充分研究的问题，该领域特别缺乏评估任务。为了最好地评估法学硕士发现错误的能力，评估任务应该显示明确的错误。据我们所知，由于这个原因，当前大多数错误查找数据集并未超出&lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>;数学&lt;/a>;领域。 &lt;/p>; &lt;p>; 为了评估法学硕士推理数学领域之外的错误的能力，我们生成了一个供研究界使用的新数据集，称为&lt;strong>; &lt;/strong>;&lt;a href=&quot;https: //github.com/WHGTyen/BIG-Bench-Mistake&quot;>;大基准错误&lt;/a>;。该数据集由使用 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 针对 &lt;a href=&quot;https:// 中的五个任务生成的思想链轨迹组成github.com/suzgunmirac/BIG-Bench-Hard&quot;>;BIG-Bench&lt;/a>;。每个跟踪都标有第一个逻辑错误的位置。 &lt;/p>; &lt;p>; 为了最大限度地增加数据集中的错误数量，我们对答案不正确的 255 个迹线进行了采样（因此我们知道肯定存在错误），对答案正确的迹线进行了 45 个采样（因此可能存在或可能不是一个错误）。然后，我们要求人工贴标员检查每条痕迹并识别第一个错误步骤。每条迹线均由至少三位标注者进行注释，其答案的&lt;a href=&quot;https://en.wikipedia.org/wiki/Inter-rater_reliability&quot;>;评分者间可靠性&lt;/a>;水平>;0.98（使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Krippendorff%27s_alpha&quot;>;Krippendorff 的 α&lt;/a>;）。除 &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/dyck_languages&quot;>;Dyck Languages 任务&lt;/a>;（涉及预测）之外的所有任务均已完成标记给定输入序列的右括号序列。我们通过算法标记了该任务。 &lt;/p>; &lt;p>; 该数据集中出现的逻辑错误简单且明确，为测试法学硕士在执行更困难、更模糊的任务之前发现自己的错误的能力提供了良好的基准。&lt;/p>; &lt;div class =&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpy PmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/ s1354/BBMistakes2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;410&quot; data-original-width=&quot;1354&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwyt Nv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s16000/BBMistakes2.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;错误识别核心问题&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;1.法学硕士能否发现思想链式推理中的逻辑错误？&lt;/h3>; &lt;p>; 首先，我们想知道法学硕士是否能够独立于其纠正错误的能力来识别错误。我们尝试多种提示方式来测试&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>;系列模型定位错误的能力（提示&lt;a href=&quot;https ://github.com/WHGTyen/BIG-Bench-Mistake/tree/main/mistake_finding_prompts&quot;>;此处&lt;/a>;）假设它们通常代表现代法学硕士的表现。 &lt;/p>; &lt;p>; 一般来说，我们发现这些最先进的模型表现不佳，最好的模型总体准确率达到 52.9%。因此，有必要提高法学硕士在这方面的推理能力。 &lt;/p>; &lt;p>; 在我们的实验中，我们尝试了三种不同的提示方法：直接（trace）、直接（step）和CoT（step）。在直接（跟踪）中，我们向法学硕士提供跟踪并询问错误的位置步骤或&lt;em>;没有错误&lt;/em>;。在直接（步骤）中，我们提示法学硕士针对其采取的每一步问自己这个问题。在CoT（步骤）中，我们提示LLM给出每个步骤是否错误的推理。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdt imB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s1061/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;1061&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU 3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;直接（trace）、直接（step）和CoT（step）三种提示方式的示意图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们的发现是一致的，并且建立在&lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;之前的结果&lt;/a>;的基础上，但进一步表明，法学硕士即使是简单且明确的错误也难以避免（作为比较） ，我们的人类评估者在没有事先专业知识的情况下以高度一致的方式解决了问题）。我们假设这是法学硕士无法自我纠正推理错误的一个重要原因。有关完整结果，请参阅&lt;a href=&quot;https://arxiv.org/abs/2311.08516&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;2.发现错误可以用来衡量答案的正确性吗？&lt;/h3>; &lt;p>;当人们遇到我们不确定答案的问题时，我们可以逐步解决我们的解决方案。如果没有发现错误，我们可以假设我们做了正确的事情。 &lt;/p>; &lt;p>; 虽然我们假设这对于法学硕士也有类似的作用，但我们发现这是一个糟糕的策略。在我们包含 85% 错误轨迹和 15% 正确轨迹的数据集中，使用此方法并不比始终将轨迹标记为不正确的幼稚策略好多少，后者给出加权平均值 &lt;a href=&quot;https://en.wikipedia. org/wiki/F-score&quot;>;F1&lt;/a>; 为 78。&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBf qVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s698/Self- Correcting-LLMs-Tasks.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-原始高度=“427”数据原始宽度=“698”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzablySdTx5vpHv_5T0NwK DB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s16000/自校正-LLM -Tasks.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;显示错误查找效果的图表法学硕士可以用作每个数据集答案正确性的代理。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h3>;3. LLM 可以回溯知道错误在哪里吗？&lt;/h3>; &lt;p>; 由于我们已经证明 LLM 在查找 CoT 跟踪中的推理错误方面表现不佳，我们想知道 LLM 是否能够纠正错误&lt;em>;&lt; /em>;，即使他们知道错误在哪里。 &lt;/p>; &lt;p>; 请注意，了解&lt;em>;错误位置&lt;/em>;与了解&lt;em>;正确答案&lt;/em>;不同：即使最终答案是正确的，CoT 跟踪也可能包含逻辑错误，或者反之亦然。在大多数现实情况下，我们不知道正确的答案是什么，但我们也许能够识别中间步骤中的逻辑错误。 &lt;/p>; &lt;p>; 我们提出以下回溯方法：&lt;/p>; &lt;ol>; &lt;li>;在温度 = 0 时照常生成 CoT 迹线。（温度是控制生成响应的随机性的参数，温度越高，值产生更多样化和创造性的输出，通常以牺牲质量为代价。）&lt;/li>;&lt;li>;识别第一个逻辑错误的位置（例如使用分类器，或者在这里我们只使用数据集中的标签）。 &lt;/li>;&lt;li>;在温度 = 1 时重新生成错误步骤并产生一组八个输出。由于已知原始输出会导致不正确的结果，因此目标是在此步骤中找到与原始输出显着不同的替代生成。 &lt;/li>;&lt;li>;从这八个输出中，选择一个与原始错误步骤不同的输出。 （我们在这里只是使用精确匹配，但将来这可能会更加复杂。）&lt;/li>;&lt;li>;使用新步骤，在温度 = 0 时正常生成迹线的其余部分。&lt;/li>; &lt; /ol>; &lt;p>; 这是一种非常简单的方法，不需要任何额外的提示制作，并且避免了重新生成整个跟踪。我们使用 BIG-Bench Mistake 的错误位置数据对其进行测试，发现它可以纠正 CoT 错误。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;最近的工作&lt;/a>;表明自我纠正方法，例如&lt;a href=&quot;https://arxiv. org/abs/2303.11366&quot;>;反射&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2303.17491&quot;>;RCI&lt;/a>;，导致准确性分数下降，因为有更多的正确答案变得不正确反之亦然。另一方面，我们的方法产生的收益（通过纠正错误答案）多于损失（通过将正确答案更改为错误答案）。 &lt;/p>; &lt;p>; 我们还将我们的方法与随机基线进行比较，其中我们随机假设一个步骤是错误的。我们的结果表明，这个随机基线确实产生了一些收益，但不如使用正确的错误位置回溯那么多，并且损失更多。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy 95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s744/image4.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;744&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx 2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;该图表显示了我们的方法的准确性的增益和损失以及每个数据集的随机基线。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-高度：40%；&quot;>; &lt;br />; &lt;/div>; &lt;h3>;4.错误发现可以推广到法学硕士从未见过的任务吗？&lt;/h3>; &lt;p>;为了回答这个问题，我们在四个 BIG-Bench 任务上微调了一个小模型，并在第五个保留任务上对其进行了测试。我们对每项任务都这样做，总共生成了五个经过微调的模型。然后我们将结果与零样本提示进行比较 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-L-Unicorn &lt;/a>;，一个更大的模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQ dQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s759/image5.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;759&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4 h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;条形图显示了与 PaLM 2-L-Unicorn 的零样本提示相比，经过微调的小模型的准确性改进。&lt; /span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们的结果表明，较小的微调奖励模型通常比零样本提示大模型表现更好，即使奖励模型模型从未见过测试集中任务的数据。唯一的例外是逻辑演绎，它的表现与零样本提示相当。 &lt;/p>; &lt;p>; 这是一个非常有希望的结果，因为我们可以使用一个小的微调奖励模型来执行回溯并提高任何任务的准确性，即使我们没有相关数据。这个较小的奖励模型完全独立于生成器 LLM，并且可以针对个别用例进行更新和进一步微调。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1G kvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s867/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;867&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY 8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;展示我们的回溯方法如何工作的插图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 在这项工作中，我们创建了一个评估基准数据集，更广泛的学术界可以使用它来评估未来的法学硕士。我们进一步表明，法学硕士目前很难发现逻辑错误。然而，如果可以的话，我们将展示回溯作为一种可以为任务带来收益的策略的有效性。最后，可以在一般错误查找任务上训练较小的奖励模型，并用于改进域外错误查找，这表明错误查找可以泛化。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 Peter Chen、Tony Mak、Hassan Mansoor 和 Victor Cărbune 提供的想法以及帮助进行实验和数据收集。我们还要感谢 Sian Gooding 和 Vicky Zayats 对本文的评论和建议。&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google /feeds/7998118785777164574/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/ can-large-language-models-identify-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger .com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/7998118785777164574&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and .html&quot; rel=&quot;alternate&quot; title=&quot;大型语言模型可以识别并纠正错误吗？&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt; /电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded。 gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsks Z_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U- UqW0hglZ-cXXCkznmufJIfyPAR/s72-c/Backtracking.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/ thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7197275876457161088&lt;/id>;&lt;发布>;2024-01-08T14:07:00.000-08:00&lt; /published>;&lt;更新>;2024-01-08T14:07:07.614-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;Google Research 的负责任的 AI：用户体验团队&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：用户体验主管 Ayça Çakmakli , Google 研究部，负责任的人工智能和以人为本的技术团队&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-x yVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s16000/英雄。 jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; Google 的负责任的 AI 用户体验（Responsible AI UX）团队是嵌入 Google Research 内的一个以产品为导向的团队。这种独特的定位要求我们将负责任的人工智能开发实践应用于以用户为中心的用户体验 (UX) 设计流程。在这篇文章中，我们描述了用户体验设计和负责任的人工智能在产品开发中的重要性，并分享了一些示例，说明我们团队的能力和跨职能协作如何在整个 Google 中实现负责任的开发。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 首先是用户体验部分。我们是一个由产品设计专家组成的多学科团队：设计师、工程师、研究人员和策略师，他们管理以用户为中心的用户体验设计流程，从早期构思和问题框架到后期用户界面 (UI) 设计、原型设计和细化。我们相信，当未满足的重要用户需求与产品的主要价值主张之间存在明确的一致性时，就会出现有效的产品开发，并且这种一致性可以通过彻底的以用户为中心的用户体验设计流程可靠地实现。 &lt;/p>; &lt;p>; 其次，认识到生成式人工智能 (GenAI) 对社会产生重大影响的潜力，我们接受自己作为主要用户倡导者的角色，不断发展我们的用户体验设计流程，以应对人工智能带来的独特挑战，最大限度地提高用户体验。利益并最大限度地降低风险。当我们经历人工智能驱动的产品设计过程的每个阶段时，我们高度重视我们的决策的道德、社会和长期影响。我们致力于持续开发全面的&lt;a href=&quot;https://ai.google/responsibility/ai-governance-operations&quot;>;安全性和包容性协议&lt;/a>;，围绕内容管理等关键问题定义设计和部署护栏、安全性、隐私、模型功能、模型访问、公平性和公平性，有助于减轻 GenAI 风险。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_ GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s1920/image4。 png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBN h5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;Responsible AI UX 不断发展其以用户为中心的产品设计流程，以满足 GenAI 驱动的产品格局的需求，对用户和社会的需求更加敏感，强调道德、社会和长期影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 产品设计中的责任也体现在我们选择解决和解决的用户和社会问题上。我们提供资源的计划。因此，我们鼓励&lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;优先考虑规模和严重程度较高的用户问题&lt;/a>;帮助最大限度地发挥 GenAI 技术的积极影响。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK 8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s1920 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY7 8gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; 跨团队和学科的沟通对于负责任的产品设计至关重要。从用户研究团队到产品设计和工程团队（反之亦然）的信息和见解的无缝流动对于良好的产品开发至关重要。我们团队的核科学家和以用户为中心的设计研究专家。我们建立了一支在这些领域拥有专业知识的多学科团队，加深了我们对受众沟通需求的同理心，并使我们能够更好地在用户和用户之间建立联系。社会专家和我们的技术专家。我们创建框架、指南、原型、备忘单和多媒体工具，帮助在正确的时间为正确的人带来生活中的见解。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class =&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5 nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s1920/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border= “0”数据原始高度=“1080”数据原始宽度=“1920”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0w vd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s16000/ image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;促进负责任的 GenAI 原型设计和开发 &lt;/h2>; &lt;p>; 期间Responsible AI UX、&lt;a href=&quot;https://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot;>;人员 + AI 研究&lt;/a>;之间的合作（ PAIR）计划和&lt;a href=&quot;https://labs.google/&quot;>;实验室&lt;/a>;，我们发现&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564 &quot;>;原型设计&lt;/a>;可以提供参与大型语言模型 (LLM) 的创造性机会，并且通常是 GenAI 产品开发的第一步。为了满足将法学硕士引入原型制作过程的需求，我们探索了一系列不同的提示设计。然后，我们深入该领域，采用各种外部的第一人称用户体验设计研究方法来获取洞察力并获得对用户观点的同理心。通过用户/设计师共同创建会议、迭代和原型设计，我们能够让内部利益相关者、产品经理、工程师、作家、销售和营销团队一起参与，以确保用户的观点得到充分理解并加强一致性跨团队。 &lt;/p>; &lt;p>; 这项工作的成果是 &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>; ，一个在 &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;Google I/O 2023&lt;/a>; 上推出的生成式 AI 平台，可实现人们，即使是那些没有任何机器学习经验的人，也可以使用法学硕士创造性地进行原型设计。该团队对用户的第一手经验以及对他们所面临挑战的了解使我们能够将&lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI 原则&lt;/a>;融入到 MakerSuite 产品设计中。例如，&lt;a href=&quot;https://developers.generativeai.google/guide/safety_setting&quot;>;安全过滤器&lt;/a>;等产品功能使用户能够管理结果，从而利用 MakerSuite 实现更轻松、更负责任的产品开发。 &lt;/p>; &lt;p>; 由于我们与产品团队密切合作，我们能够调整纯文本原型以支持与 &lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform 的多模式交互&quot;>;Google AI Studio&lt;/a>;，MakerSuite 的演变。现在，Google AI Studio 使开发者和非开发者能够无缝利用 Google 最新的 &lt;a href=&quot;https://ai.google.dev/&quot;>;Gemini&lt;/a>; 模型来合并多种模态输入，例如文本和图像，在产品探索中。以这种方式促进产品开发使我们有机会更好地利用人工智能来识别&lt;a href=&quot;https://arxiv.org/pdf/2310.15428.pdf&quot;>;结果的适当性&lt;/a>;，并为开发人员和开发人员释放机会非开发人员可以玩 AI 沙箱。我们与合作伙伴一起，继续在我们支持的产品中积极推动这一努力。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style= “左边距：自动；右边距：自动；&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4Abl FB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; 数据-original-height=&quot;1406&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQ g-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/ s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https: //makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI studio&lt;/a>; 使开发者和非开发者能够利用 Google Cloud 基础架构并在产品探索中合并多种模式输入。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;公平语音识别&lt;/h2>; &lt;p>; 多个&lt; a href=&quot;https://www.pnas.org/doi/10.1073/pnas.1915768117&quot;>;外部研究&lt;/a>;，以及 Google 的&lt;a href=&quot;https://www.frontiersin.org/articles /10.3389/frai.2021.725911/full&quot;>;自己的研究&lt;/a>;发现，相对于白人说话者，当前语音识别技术平均理解黑人说话者的能力存在缺陷。随着多模式人工智能工具开始更加依赖语音提示，这个问题将会加剧并继续疏远用户。为了解决这个问题，Responsible AI UX 团队&lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;与世界知名的语言学家和科学家合作在霍华德大学&lt;/a>;，著名的&lt;a href=&quot;https://en.wikipedia.org/wiki/Historically_black_colleges_and_universities&quot; target=&quot;_blank&quot;>;HBCU&lt;/a>;，致力于打造高品质的非裔美式英语数据集，用于改进我们的语音技术产品的设计，使其更易于使用。这项名为“提升黑人之声项目”的工作将使霍华德大学能够与那些希望改进语音技术的人共享数据集，同时建立负责任的数据收集框架，确保数据造福黑人社区。霍华德大学将保留该数据集的所有权和许可，并作为其负责任使用的管理者。在 Google，我们正在提供资金支持，并与霍华德大学的合作伙伴密切合作，以确保该计划取得成功。 &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>; &lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot;frameborder=&quot;0&quot; height=&quot;360 &quot; src=&quot;https://www.youtube.com/embed/t_pdlrU8qhs?si=5xY1AoGc_d2HTzQf&quot; width=&quot;640&quot; youtube-src-id=&quot;5xY1AoGc_d2HTzQf&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;公平计算机视觉&lt;/h2>; &lt;p>; &lt;a href=&quot;http://gendershades.org/&quot;>;性别阴影&lt;/a>;项目强调，计算机视觉系统很难检测肤色较深的人，对于肤色较深的女性表现尤其差。这主要是因为用于训练这些模型的数据集不包含广泛的肤色。为了解决这一限制，Responsible AI UX 团队一直与社会学家 &lt;a href=&quot;https://www.ellismonk.com/&quot;>;Dr. Ellis Monk&lt;/a>; 发布&lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk 肤色等级&lt;/a>; (MST)，这是一款皮肤色调等级旨在更包容世界各地的肤色范围。它提供了一个工具来评估数据集的包容性和模型在各种肤色范围内的性能，从而产生更适合每个人的功能和产品。 &lt;/p>; &lt;p>; 我们已将 MST 集成到一系列 &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Google 产品&lt;/a>;中，例如如搜索、Google 相册等。我们还开源了 MST，&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3632120&quot;>;发表了我们的研究&lt;/a>;，&lt;a href=&quot;https://blog.research .google/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;描述了我们的注释实践&lt;/a>;，以及&lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;分享了一个示例数据集&lt;/a>;，以鼓励其他人轻松地将其集成到他们的产品中。 Responsible AI UX 团队继续与 Monk 博士合作，在多个产品应用程序中利用 MST，并继续进行国际研究，以确保其具有全球包容性。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;咨询与咨询指导&lt;/h2>; &lt;p>; 随着 Google 团队不断开发利用 GenAI 模型功能的产品，我们的团队认识到他们面临的挑战各不相同，而且市场竞争也很激烈。为了支持团队，我们开发了可操作的资产，以促进考虑可用资源的更加简化和负责任的产品设计流程。我们作为一家以产品为中心的设计咨询公司，寻找扩展服务、分享专业知识并更广泛地应用我们的设计原则的方法。我们的目标是通过负责任的产品设计，帮助 Google 的所有产品团队将未满足的重要用户需求与技术优势联系起来。 &lt;/p>; &lt;p>; 我们实现这一目标的方法之一是创建&lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>;，这是一本不断发展的我们学到的许多负责任的设计经验以及我们为内部和外部利益相关者提出的建议的总结性资源。即将推出的滚动&lt;a href=&quot;https://medium.com/people-ai-research/updating-the-people-ai-guidebook-in-the-age-of-generative-ai-cace6c846db4&quot;>;更新&lt;/a>; 特别关注如何利用 GenAI 进行最佳设计和考虑用户需求，我们希望我们的内部团队、外部利益相关者和更大的社区能够在产品开发过程中最关键的里程碑上获得有用且可操作的指导。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8 oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s1100/image1.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;622&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjOMslkRaMAkywAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSI bQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;《人+人工智能指南》共有六章，旨在涵盖产品生命周期的不同方面。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;如果您有兴趣阅读有关 Responsible AI 的更多信息用户体验以及我们如何具体考虑使用生成式 AI 进行负责任的设计，请查看此 &lt;a href=&quot;https://medium.com/people-ai-research/meet-ay%C3%A7a-%C3%A7akmakli- googles-new-head-of-responsible-ai-ux-d8f2700df95b&quot;>;问答&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;向我们负责任的 AI UX 团队成员致谢： Aaron Donsbach、Alejandra Molina、Courtney Heldreth、Diana Akrong、Ellis Monk、Femi Olanubi、Hope Neveux、Kafayat Abdul、Key Lee、Mahima Pushkarna、Sally Limb、Sarah Post、Sures Kumar Thoddu Srinivasan、Tesh Goyal、Ursula Lauriston 和 Zion门格沙。特别感谢 Michelle Cohn 对这项工作的贡献。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7197275876457161088/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html#comment-form&quot; rel=&quot;replies&quot; 标题=&quot;0条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;edit&quot; type=&quot;application/atom +xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href= &quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research 的负责任的人工智能：用户体验团队&quot; type=&quot; text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>; &lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked -9qdedksoczmbaiyrl/s72-c/hero.jpg“ width =” 72“ xmlns：媒体=” http：//search.yahoo.com/mrss/>; &lt;/media：thumbnail>; &lt;/thrumbnail>; &lt;thr>; &lt;thr>; &lt;thr>; &lt;thr：thr>; 0 &lt;/thr>; 0 &lt;/thr ：Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-8816183473385638131 &lt;/id>;已发布>; &lt;更新>; 2024-01-11T16：01：33.313-08：00 &lt;/updated>; &lt;title type =“ text”>; 2023：AI和Computing>; compution>; &lt;/stitle>; &lt;content>; &lt;content>; &lt;content>; &lt;content>; &lt;content>; &lt;content Type =“ html“>; &lt;span class =” Byline-author”>;由Google DeepMind＆amp＆amp; Jeff Dean发布Google Research，Demis Hassabis，首席执行官，Google DeepMind和James Monyika，SVP，Google Research，Technology＆amp;社会&lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxseiu12g_p2dzo4gq _p2dzo4gq-p95ap2kp95ap2krhrg5oik9vzj4wtt_4wtgznggzngngnggtzgngtzgnekgngtzgnegtzgnekgttegttzqguttzgtugttzqguttzqguttzqguttzqguttzqgtutzqgtutzqgtutzqqugtusttt Ofyhwnu-1S4SSBR2JQ5T36TQRBZUCTAP7GMXKGW77XN6S39IKXPAXBO5TW_CQ52ZFPWOCBKKWL-2XTUZSIH6VIRIQGGGGGGCGGCGGCGGCGGCGGCGGCGGCGGCGGCGGCDUDNQ-PHJZL/s1100 sight year play play style play plays plays plays plays play = ： 没有任何;” />; &lt;p>;这是人工智能（AI）研究及其实用应用领域中令人难以置信的一年。 &lt;/p>; &lt;p>;随着正在进行的研究将AI推得更远，我们回顾我们的&lt;a href =“ https://ai.google/static/documents/google-why-why-we-we-we-focus-on-ai.pdf “>;透视&lt;/a>;今年1月出版，标题为“为什么要专注于AI（以及到底）”，我们指出：&lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;/a>; &lt;/a>; &lt; Div style =“保证金左：40px;”>; &lt;p>;我们致力于领导和制定标准开发和运输有用和有益的应用程序，应用以人类价值为基础的道德原则，并在我们从研究中学习时发展我们的方法，经验，用户和更广泛的社区。 &lt;/p>; &lt;p>;我们还认为，使AI正确 - 对我们而言，这涉及创新和为人们和社会带来广泛访问的利益，同时减轻风险 - 必须是我们和其他人的集体努力，包括研究人员，开发人员，开发人员，开发人员，开发人员，开发人员，用户（个人，企业和其他组织），政府，监管机构和公民。 &lt;/p>; &lt;p>;我们坚信，我们专注于开发和交付的AI支持创新是有用的，令人信服的，并且有潜力帮助和改善各地的人们的生活 - 这就是迫使我们的原因。 &lt;/p>; &lt;/div>; &lt;p>;在本年度审查帖子中，我们将介绍一些Google Research的研究和Google DeepMind的努力，将这些段落置于2023年。&lt;/p>; &lt;br />; &lt;/p>; &lt;br />; &lt; H2>;产品＆amp; Technologies &lt;/h2>; &lt;p>;这是一年生成的AI引起了世界的关注，创造了图像，音乐，故事和引人入胜的对话，以了解所有可想象的一切，以创造力和速度几乎令人难以置信。 &lt;/p>; &lt;p>;在2月，我们&lt;a href=&quot;https://blog.google/technology/technology/ai/bard-google-google-ai-search-updates/&quot;>;首次启动&lt;/a>; &lt;/a>; &lt;a href = “ https://bard.google.com”>; bard &lt;/a>;，您可以用来探索创意并简单地解释事物的工具。它可以生成文本、翻译语言、编写不同类型的创意内容等等。 &lt;/p>; &lt;p>; 5月，我们观看了在阶段&lt;a href =“ https://blog.research.google/2023/05/google-research-- at-io-2023.html“>;在Google I/O &lt;/a>;上。主要是，这包括&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>; Palm 2 &lt;/a>;，一种大型语言模型（LLM），它将Compute-Optimal缩放缩放在一起，改进的数据集混合物，一种改进的数据集混合物，和模型体系结构可在高级推理任务中表现出色。 &lt;/p>; &lt;div class =“ siperator” style =“ clear：clear; text-align：center;”>; &lt;a href =“ https：//blogger.googger.googleusercontent.com/img/r29vz2xl/r29vz2xl/avvz2xl/avvxssegxsegxsege5r7e2f0bqisilzm25kbvm42 8EQC22BOG6SV_H8NBC-PKD2PPV7FHC -UBR43ZWPBRGVAGJ769J-UUCTPBFBO9BF-U81GKFU1OP4OGZHS6KKKKKKKKBB2ZNNSVCELERREN5KWKH2NPPJXFJXFJXFJDSVJDSVZZUIPZUIPZUIPZUIPZUIPZUIPZUIPZUIPZUIPPHEN3B_JCQ4/s1920/lockupp._clrmlock _clmnp._clmmplmmp g“ style =”边距 - 左：1EM;边缘右：1EM;“>; &lt;img border =“ 0” data-froliginal-height =“ 555” data-original-width=&quot;1920&quot; height=&quot;116&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5R7E2f0BQIsiLZm25kxfR_Ix_Bvm6HlQQCXI12sB42siKUAf8eZvVEDU5bi8EQc22BoG6SV_H8NbC-PKd2pPv7FhC-uBR43ZWpbrgvaGJ7699j-uUctPbFBO9Bf-u81gkfU1OP4oGZhs6KKkub2znNsvcElREn5kwKh2npPJxFJdSVZZUIPZhyphenhyphen3B_jCq4/w400-h116/lockup_ic_PaLM-2_H_4297x745px_clr_ @1x.jpg“ width =” 400“/>; &lt;/a>; &lt;/div>; &lt;p>;通过微调和指令敲打棕榈2用于不同的目的，我们能够将其集成到许多Google产品和功能中，包括：&lt;/p>; &lt;ul>; &lt;li>;对吟游诗人的更新，该更新启用了多语言功能。自首次推出以来，Bard现在可提供超过&lt;a href=&quot;https://support.google.com/bard/answer/13575153?hl = en&quot;>; 40种语言和230多种国家和领土&lt;/a>; ，and &lt;a href=&quot;https://blog.google/products/bard/google-bard-new-features-update-update-sept-2023/&quot;>;带有扩展名&lt;/a>;，Bard可以从每天都使用的Google工具 - 例如Gmail，Google Maps，YouTube等。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;搜索生成的体验&lt;/a>;（sge），使用LLMS来重新想象组织信息以及如何帮助人们浏览信息，为我们的核心搜索产品创建更流畅的对话互动模型。这项工作将搜索引擎体验从主要关注信息检索扩展到更多功能——能够检索、综合、创造性生成和延续先前的搜索——同时继续充当用户和他们所寻求的网络内容之间的连接点。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/musiclm/examples/&quot;>; musiclm &lt;/a>;，一种由&lt;a href提供的文本tok-music模型=“ https://ai.googleblog.com/2022/10/audiolm-language-modeling-apphack-to..html”>; audiolm &lt;/a>;和&lt;a href =“ https://arxiv.org/abs/ 2208.12415“>; Mulan &lt;/a>;，可以从文字，嗡嗡声，图像或视频或音乐伴奏到唱歌中制作音乐。 &lt;/li>; &lt;li>; Duet AI，我们的AI驱动的合作者，在使用Google Workspace和Google Cloud时为用户提供帮助。 &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>; Google Workspace中的二重奏AI &lt;/a>;并总结电子邮件和聊天消息，并总结会议。 &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-duet-duet-for-google-cloud-cloud&quot;>; Google Cloud中的Duet AI &lt;/a>; ，扩展和监控应用程序，并识别和加速对网络安全威胁的分辨率。 &lt;/li>; &lt;li>;和许多&lt;a href=&quot;https://blog.google/technology/developers/google-io-2023-100-anncements/&quot;>;其他发展&lt;/a>;。 &lt;/li>; &lt;/ul>; &lt;p>;在去年发布我们的文本形象生成模型之后，6月&lt;a href=&quot;https://imagen.research.google/&quot;>; imagen &lt;/a>;，我们发布了&lt;a href=&quot;https://blog.research.google/2023/06/imagen-editor-and-editor-and-editbench-advancench-advancing.html&quot;>; Imagen Editor &lt;/a>;，该编辑器提供了使用区域面具和面具和区域面具的能力自然语言提示交互编辑生成图像，以提供对模型输出的更精确的控制。 &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU-6oYBC5383noYqno-cYzUboAAOgXvDlWtqwu-zl94M2r02Fsid0jjgLBJl3JCVR4lc8ZVw7-c7q9OlHjzmrRXz5i5H/s1261/ image4.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g 4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU -6oybc5383noyqno-cyzuboxvdlwtqwu-Zl94M2R02FSID0JJGLBJLBJLBJLBJLBJCVR4LC8ZVW7-C7-C7C7Q9OLHJZMRRXZMRRXZMRRRXZ5I5I5I5H/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>; &lt;/>;基于人类的专业图像美学模型对良好照明，框架，曝光和清晰度等品质的偏好。 &lt;/p>; &lt;p>; 10月，我们启动了一个功能，该功能&lt;a href =“ https://blog.research.google/2023/10/google-search-can-can-now-now-now-help-with-with-english-speaking--练习.html“>;帮助人们练习说话和提高语言技能&lt;/a>;。实现此功能的关键技术是与 Google 翻译团队合作开发的一种新颖的深度学习模型，称为 Deep Aligner。与基于基于&lt;a href =“ https://aclanthology.org/aclanthology.org/aclanthology.org/aclanthology.org/c96-”的对齐方式相比，这个单一的新模型导致所有测试语言对的对齐质量的急剧提高，将平均对准错误率从25％降低到5％。 2141/“>;隐藏的马尔可夫模型&lt;/a>;（HMMS）。 &lt;/p>; &lt;p>; 11月，与&lt;a href=&quot;https://blog.youtube/inside-youtube/ai-and-music-experiment/&quot;>; youtube &lt;/a>;，我们宣布&lt;a href =“ https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/”>; lylia &lt;/a>;，我们迄今为止最先进的AI音乐生成模型。我们发布了两个实验，旨在为创造力，DreamTrack和Music AI工具打开一个新的操场，并与&lt;a href =“ https://blog.youtube/inside-youtube/inside-youtube/partnering-with-the-music-industry-instry-on” -ai/&quot;>;YouTube 在人工智能技术方面与音乐行业合作的原则&lt;/a>;。 &lt;/p>; &lt;p>;然后在12月，我们启动了&lt;a href=&quot;https://blog.google/technology/technology/google-google-gemini-ai/&quot;>; gemini &lt;/a>;，我们最有能力，最有能力，最一般的AI模型。双子座的建造是从文本，音频，图像和视频的地面上构成的。我们最初的双子座模型家族有三种不同尺寸的Nano，Pro和Ultra。纳米模型是我们最小，最有效的模型，用于为像素等产品中的设备体验供电。 Pro模型具有高度的能力，最适合在各种任务中扩展。 Ultra模型是我们用于高度复杂任务的最大，功能最强大的模型。 &lt;/p>; &lt;br />; &lt;div class =“ saparator” style =“ clear：clear; text-align：center;“>; &lt;a href =” https：//www.youtube.com/watch?v=jv1vkhv4zq8 “>; &lt;iframelo面路allyfullscreen =” class =“ blog_video_class” height =“ 360” src =“ https://www.youtube.com/embed/embed/jv1v1vkhv4zq8” width width width =“ 640 &lt;/iframe>; &lt;/a>; &lt;/div>; &lt;br />; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini/gemini_1_report.pdf&quot;>;技术报告&lt; /a>;关于&lt;a href=&quot;https://deepmind.google/technologies/gemini&quot;>; gemini型号&lt;/a>;，我们表明Gemini Ultra的性能超过了3232的30次最新最新的结果。 LLM研发中使用的广泛使用的学术基准。 Gemini Ultra的得分为90.04％，是第一个在&lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>; mmlu &lt;/a>;上超过人类专家的模型，并获得了一个州立大学的现实。 - 新的&lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>; mmmu &lt;/a>;基准的ART分数为59.4％。 &lt;/p>; &lt;p>;在&lt;a href=&quot;https://deepmind.google/discover/discover/blog/competitival-programming-with-alphacode/&quot;>; alphaCode上建造&lt;/a>;我们&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/alphacode2/alphacode2/alphacode2_tech_report.pdf&quot;>;引入了Alphacode 2 &lt;/a>;由专用版本的Geminii版本启用， 。当在与原始 AlphaCode 相同的平台上进行评估时，我们发现 AlphaCode 2 解决的问题多了 1.7 倍，并且表现优于 85% 的参赛者&lt;/p>; &lt;p>; 同时，&lt;a href=&quot;https: //blog.google/products/bard/google-bard-try-gemini-ai/&quot;>; bard使用Gemini Pro模型进行了最大的升级&lt;/a>;总结，推理，编码和计划。在八个基准测试中，Gemini Pro的六个基准优于GPT-3.5，包括MMLU，这是测量大型AI模型的关键标准之一，&lt;a href=&quot;https://huggingface.co/datasetsetset.co/datasetsets/gsm8k&quot;>; gsm8k &lt;>; gsm8k &lt;>; gsm8k &lt; /a>;，可以测量小学数学推理。 Gemini Ultra 将于明年初通过巴德高级 (Bard Advanced) 登陆巴德，这是一种全新的尖端人工智能体验。 &lt;/p>; &lt;p>; gemini pro也可以在&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/gemini-support-on-vertex-ai&quot;>; vertex AI &lt;/a>;，Google Cloud的端到端AI平台，该平台使开发人员能够构建可以在文本，代码，图像和视频中处理信息的应用程序。 &lt;a href=&quot;https://blog.google/technology/ai/gemini-api-developers-cloud/&quot;>; gemini pro在12月的AI工作室也提供了&lt;/a>;。 &lt;/p>; &lt;p>;为了最好地说明双子座的某些功能，我们制作了一个&lt;a href=&quot;https://deepmind.google/technologies/gemini/gemini/#hands-on&quot;>;一系列简短视频&lt;/a>;双子座如何可以：&lt;/p>; &lt;ul>; &lt;li>; &lt;a href=&quot;https://www.youtube.com/watch?v=spiop_cb54a&quot;>;解锁科学文学的见解&lt;/a>; &lt;/li >; &lt;li>; &lt;a href=&quot;https://www.youtube.com/watch?v=lvggmvmhv69s＆amp;t = 1s&quot;>;在竞争性编程中出色&lt;/a>; &lt;/li>; &lt;li>; &lt;a>; &lt;li>; &lt;a a href = https://www.youtube.com/watch?v=d64qd7swr3s&quot;>; process and prococess and Checeals and probocess &lt;/a>; &lt;/a>; &lt;/li>; &lt;li>; &lt;a href =“ https://www.youtube.com/watch？ v = k4px1vaxaai”>;在数学和物理学中解释推理&lt;/a>; &lt;/li>; &lt;li>; &lt;a href=&quot;https://www.youtube.com/watch?v=v5trc_5trc_5trc_5-8g4&quot;>;有关用户意图的原因要产生定制体验&lt;/a>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>; ml/ai研究&lt;/h2>; &lt;p>;除了我们在产品和技术方面的进步外，我们还做了一个机器学习和AI研究更广泛领域的重要进步。 &lt;/p>; &lt;p>; 最先进的 ML 模型的核心是 Transformer 模型架构，&lt;a href=&quot;https://blog.research.google/2017/08/transformer-novel-neural-network.html “>;由Google研究人员在2017年开发&lt;/a>;。它最初是为语言开发的，它已被证明在范围内有用，如&lt;a href=&quot;https://blog.research.google/2020/12/transformers-for-image-image-recognition-at.html&quot;>;计算机视觉&lt;/ a>;，&lt;a href=&quot;https://deepmind.google/discover/discover/blog/transforming-the-future-of-music-creation/&quot;>;音频&lt;/a>;，&lt;a href =“ https：// deepmind 。 Google/Technologies/alphafold/“>;蛋白质折叠&lt;/a>;等等。今年，我们在&lt;a href=&quot;https://blog.research.google/2023/03/scaling-vision-vision-vision-transformers-to-22.html&quot;>;缩放视觉变压器&lt;/a>;在各种视觉任务中取得了最先进的成果，并且在构建 &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot; 方面也很有用>;更有能力的机器人&lt;/a>;。 &lt;/p>; &lt;p>; &lt;/p>; &lt;p>;扩展模型的多功能性需要执行高级和多步推理的能力。今年，我们根据几个研究轨道实现了这个目标。例如，&lt;a href=&quot;https://blog.search.google/2023/08/teaching-language-models-models-models-to-reason.html&quot;>;算法提示&lt;/a>;是一种新方法，是一种教授语言模型的新方法通过演示一系列算法步骤，然后可以在新的上下文中应用该步骤。这种方法将一种中学数学基准的准确性从25.9％提高到61.1％。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s1200/image5.gif&quot; style=&quot;边距左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-Original-height =“ 166” data-Original-width =“ 1200” src =“ https：//blogger.googleusercercontent。 com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：center;“>; &lt;em style =” text-align：left;“>;通过提供算法提示，我们可以通过context学习来教授算法的规则。&lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;在视觉问题的域中，在与加州大学伯克利分校的研究人员的合作中，我们展示了如何&lt;a href =&#39;https://blog.research.google/2023/07 /模式 -  visual-Question-andwering-via.html&quot;>; better回答复杂的视觉问题&lt;/a>;（“马匹的马车？通过合成程序来执行多步推理的问题。 &lt;/p>; &lt;p>;我们现在正在使用&lt;a href=&quot;https://blog.research.google/2023/05/large-sequence-models-models-models-for-software.html&quot;>;一般模型软件开发生命周期&lt;/a>;要自动生成代码审核评论，响应代码审核评论，对代码部分提出绩效改进建议（通过在其他情况下从过去的更改中学习），请修复代码以响应编译错误等等。 &lt;/p>; &lt;p>;在与Google Maps团队进行的多年研究合作中，我们能够扩展逆增强学习，并将其应用于&lt;a href =“ https://blog.research.google/2023/2023/ 09/世界规模inverse-Reinforcression.html“>;改善路线建议的世界规模问题&lt;/a>; &lt;/a>;对于超过10亿用户。我们的工作最终达到了16-24％的全球路线匹配率相对改善，有助于确保路线与用户偏好更好地保持一致。 &lt;/p>; &lt;p>;我们还继续致力于提高机器学习模型的推理性能。在&lt;a href=&quot;https://blog.research.google/2023/08/neural-network-pruning-with.html&quot;>; computationally-fromely友好的方法中，用于修剪神经网络中的连接&lt;/a>;能够将一种近似算法设计到计算上棘手的最佳选项选择问题，该问题能够从图像分类模型中修复70％的边缘，并且仍然保留了原始原始的几乎所有精度。 &lt;/p>; &lt;p>;在&lt;a href=&quot;https://blog.reachearch.google/2023/06/speed-is-is-is-ry-you-need-on-device.html&quot;>;加速eve eviceing empererating on-deviceing上扩散模型&lt;/a>;，我们还能够将各种优化应用于注意机制，卷积内核和操作融合，以使运行高质量图像生成模型在设备上运行高质量的模型；例如，在智能手机上只能在12秒内生成“可爱的小狗，可爱的小狗的感性和高分辨率图像”。 &lt;/p>; &lt;br />; &lt;div class =“ saparator” style =“ clear：clear; clex-align：center;“>; &lt;a href =” https：//blogger.googleusercortent.com/img/img/r29vz2xxl /AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/s522/image7.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original- width=&quot;270&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/w208-h400/image7.gif&quot; width=&quot;208&quot; />; &lt;/a>; &lt;/div>; &lt;br />; &lt;p>;能够强大的语言和多模型模型的进步也使我们的机器人研究工作受益。我们将单独训练的语言，视觉和机器人控制模型组合到&lt;a href=&quot;https://blog.research.google/2023/03/palm-e-e-embodied-multimodal-language.html&quot;>; palm-e &lt;/ A>;，一种用于机器人技术的体现多模式模型，&lt;a href =“ https://deepmind.google/discover/discover/blog/rt-new-new-model-model-translates-vision-vision-vision-vision-vision-vision-language-into-into-into-action/ “>;机器人变压器2 &lt;/a>;（RT-2），一种新颖的视觉 - 语言 - 动作（VLA）模型，该模型&lt;a href =“ https://deepmind.google/discover/discover/blog/robobocat-a-a-a-self--self--self------从Web和Robotics数据中提高Robotic-Agent/“>;“学习&lt;/a>;”，并将这些知识转化为机器人控制的通用指令。&lt;/p>; &lt;p>; &lt;/p>; &lt;/p>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-left：auto; margin-right：auto;”>; &lt;tbody>; &lt;trbody>; &lt;tr>; &lt;tr>; &lt;td style =“ text-align：text-align： center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s616/image8.jpg&quot; style=&quot;margin-left: auto;边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 559” data-eriginal-width =“ 616” src =“ https://blogger.googleusercontent.com/img/img/b/b/ r29vz2xl/avvxseijjl6huj6swdyzladrsptthh9emde7tpglk92u9hxhu29anihmqlcb1px8hfwwwwwwwwatij6v-rje667 rjeimq67orugtdxb7mr4mr4movs-nbnvnbnbnbnbnbnbnbnbnbnbnbnbnbnbnbnbnbnbn t.nbn t.nbn t.nbntm1y1 y1 y1 y1 y1 y1 y1 y y1 y y1 y1 y y1 y1 JHLSYNM3DUUEVOYHM0NF8WBVRIGHFEDPRSKTHN_GG2DWVXZZSXTK6ROTB/s16000/image8.jpg“/>; &lt;/a>; &lt;/a>; &lt;/>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;tr>; &lt;tr>; &lt;ttd class- ：center;“>; &lt;em style =” text-align：left;“>; rt-2体系结构和培训：我们在机器人和网络数据上共同访问了预先训练的视觉语言模型。结果模型介入机器人摄像头图像并直接预测机器人要执行的动作。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;此外，我们还展示了如何&lt;a href=&quot;https:// blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot;>; language也可以用于控制Quadrupedal机器人的步态&lt;/a>;并探索&lt;a href =“ 。然后，在&lt;a href=&quot;https://blog.research.google/2023/05/barkour-benchmarking-animal-level.html&quot;>; barkour &lt;/a>;我们对Quadrupedal机器人的敏捷性限制进行了基准测试>; &lt;br />; &lt;h2>;算法＆amp;优化&lt;/h2>; &lt;p>; 设计高效、稳健且可扩展的算法仍然是重中之重。今年，我们的工作包括：应用和可扩展算法、市场算法、系统效率和优化以及隐私。 &lt;/p>; &lt;p>;我们介绍了&lt;a href=&quot;https://deepmind.google/discover/discover/blog/alphadev-discovers-faster-sorting-algorithms/&quot;>; alphadev &lt;/a>;学习发现增强的计算机科学算法。 Alphadev发现了一种用于排序的更快的算法，这是一种订购数据的方法，这导致LLVM LIBC ++排序库的改进，对于较短序列而言，该库的速度快70％，而超过250,000个元素的序列快约1.7％。 &lt;/p>; &lt;p>;我们开发了一个新型模型，以&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;预测大图的属性&lt;/a>;，可以估算大程序的性能。我们发布了一个新的数据集，&lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>; tpugraphs &lt;/a>;，以加速&lt;a a href =“ https:///www.kaggle.com/competitions/competitions/predicts/predict -ai-model-runtime&quot;>;该领域的开放研究&lt;/a>;，并展示了我们如何使用&lt;a href=&quot;https://blog.research.google/2023/12/advancements-in-machine-learning -for.html“>;现代ML提高ML效率&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/s1223/image11.png “ style =”边距 - 左：自动;边缘右：自动;“>; &lt;img border =“ 0” data-Foriginal-height =“ 1042” data-Original-width =“ 1223”高度=“ 341” SRC = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/w400-h341/image11.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>; tpugraphs数据集具有4400万个用于ML程序优化的图形。 &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;我们开发了一个新的&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/load_balancing_(computing_（computing）用于将查询分发到服务器的负载平衡&lt;/a>;算法，称为&lt;a href=&quot;https://arxiv.org/abs/2312.10172&quot;>;Prequal&lt;/a>;，它最大限度地减少了请求中的请求和请求的组合估计潜伏期。多个系统的部署已大大保存了CPU，延迟和RAM。我们还设计了一个新的&lt;a href=&quot;https://arxiv.org/abs/2305.02508&quot;>;分析框架&lt;/a>;用于具有容量预留的经典缓存问题。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s1896/image12.png “ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 422” data-Original-width =“ 1896” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“ style =” text-align：中心;“>; &lt;em style =” text-align：left;“>;归一化CPU使用的热图过渡到＆nbsp; &lt;a href=&quot;prequal&quot;>; presqual &lt;/a>; 08:00。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;我们在聚类中改进了最新的ART和&lt;a href =“ https：// en。 wikipedia.org/wiki/graph_neural_network&quot;>; Graph算法&lt;/a>;通过开发&lt;a href=&quot;https://arxiv.org/abs/2106.05513&quot;>;计算最小cut-cut-cut &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/abs/arxiv.org/abs/arxiv.org &lt;/a>; =“ https://arxiv.org/abs/2309.17243”>;近似相关聚类&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2308.00503&quot;>;大量平行图集群&lt;/a>; 。此外，我们介绍了&lt;a href=&quot;https://arxiv.org/abs/2308.03578&quot;>; terahac &lt;/a>;，一种新颖的层次结构聚类算法，用于数万亿个gragral，设计了a &lt;a href =“ https：// https：// https：// blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot;>; text clustering算法&lt;/a>;，可以在保持质量的同时进行更好的可扩展性，并设计了最有效的&lt;a href =“ https：/https：/https：/ /arxiv.org/abs/2307.03043&quot;>;近似倒角距离&lt;/a>;，多件模型的标准相似性函数，可在高度优化的精确算法上提供50倍; GT; GT; 50倍速度，并缩放到数十亿个点。 &lt;/p>; &lt;p>; &lt;/p>; &lt;p>;我们继续优化Google的大型嵌入式模型（LEM），这为我们的许多核心产品和推荐系统提供动力。一些新技术包括&lt;a href=&quot;https://arxiv.org/abs/2305.12102&quot;>; unified Embedding &lt;/a>;用于Web-Scale-Scale ML Systems的战斗特征表示形式，&lt;a href =“ https：// arxiv.org/abs/2209.14881&quot;>;序列注意&lt;/a>;，它使用注意机制在训练过程中发现高质量的稀疏模型体系结构。 &lt;/p>; &lt;！ -  &lt;p>;今年，我们还继续在市场算法上进行研究，以设计计算有效的市场和因果推断。首先，我们仍然致力于提高对广告自动化的迅速兴趣，我们最近的工作&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3543507.3583416&quot;>;爆发了自动化机制的采用&lt; /a>; and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597725&quot;>;探讨了不同拍卖格式对广告商激励措施的影响&lt;/a>;。在多频道设置中，我们的发现阐明了本地和全球优化之间的选择如何影响多频道的设计&lt;a href =“ https://dl.acm.org/doi/bog/doi/abs/10.1145/3580505050507.359777707” >;拍卖系统&lt;/a>;和&lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618709&quot;>; bidding Systems &lt;/a>;。 &lt;/p>;  - >; &lt;p>;除了自动竞标系统之外，我们还在其他复杂设置中研究了拍卖设计，例如&lt;a href=&quot;https://arxiv.org/abs/2204.01962&quot;>; buy-many机制&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2207.09429&quot;>;非均匀竞标者的拍卖合约设计&lt;/a>;，并创新了&lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618478&quot;>;强大的在线竞价算法&lt;/a>;。通过在协作创建中应用生成型AI（例如广告商的联合广告）的激励，我们提出了&lt;a href=&quot;https://arxiv.org/abs/2310.10826&quot;>;一种新颖的图表拍卖模型&lt;/a>; llms &lt;/a>;在协作AI创建中竞标影响力。最后，我们展示如何&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3580507.35977702&quot;>;减轻实验设计中的个性化效果&lt;/a>;随着时间的流逝。 &lt;/p>; &lt;p>; Google Research与Chrome之间的多年合作Chrome隐私沙箱已公开启动了多个API，包括for &lt;a href =“ https://privacysandbox.com/intl/en_us/learlearning-集线器/＃受保护的a>;受保护的受众&lt;/a>;，&lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#topics&quot;>; topics &lt;/a>;和&lt;a href =“ https://privacysandbox.com/intl/en_us/learning-hub/#attribution-reporting”>;属性报告&lt;/a>;。这是保护用户隐私的同时支持开放和免费的Web生态系统的主要步骤。基础研究在&lt;a href=&quot;https://arxiv.org/abs/2304.07210&quot;>;重新识别风险&lt;/a>;上，&lt;a href =“ https://arxiv.org/arxiv.org/abs /2301.05605&quot;>; private流计算&lt;/a>;，&lt;a href=&quot;https://blog.research.google/2023/2023/summary-report-eport-eptimization-in-privacy.html，html&quot;>;优化&lt;/a>;隐私上限和预算，&lt;a href=&quot;https://arxiv.org/pdf/2308.13510.pdf&quot;>;等级聚合&lt;/a>;和&lt;a href =“ /2312.05659.pdf&quot;>; label隐私&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;科学与社会&lt;/h2>; &lt;p>;在不太遥远的未来，AI适用于科学问题的可能性很可能会加速某些领域的发现率10×或100×或更多，并在包括生物工程在内的各个领域取得重大进展-LEALNING“>;材料科学&lt;/a>;，&lt;a href =” https://deepmind.google/discover/discover/blog/graphcast-ai-model-model-ford-faster-faster-and-more-more-accurate-global-weather-weather-forecasting/- “>;天气预测&lt;/a>;，&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;气候预测&lt;/a>; =“ https://blog.research.google/2023/09/google-research-embarks-on-an-formt-to.html”>; neuroscience &lt;/a>;，&lt;a href =“ https：//blog.creach。 Google/2023/04/基于AN-ML的 - 诉求 -  better.html“>;遗传医学&lt;/a>;和&lt;a href=&quot;https://health.google/health.google/health-sealth-research/publications/publications/&quot;>;医疗保健&lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;可持续性和气候变化&lt;/h3>; &lt;p>; blog.google/outreach-initiatives/sustainability/google-ai-reduce-greenhouse-emissions-missions-project-greenlight/&quot;>; Project绿灯&lt;/a>;，我们与世界各地的13个城市合作，以帮助改善交叉点的交通流量减少停止排放。这些合作伙伴关系的早期数字表明，停止的可能性最高30％，排放量最高可减少10％。 &lt;/p>; &lt;p>;在我们的&lt;a href=&quot;https://sites.research.google/contrails/&quot;>; contrails起作用&lt;/a>;，我们分析了大型天气数据，历史卫星图像和过去的航班。我们&lt;a href=&quot;https://blog.google/technology/ai/ai-ai-ai-airlines-contrails-contrails-contrails-climate-change/&quot;>;训练了AI模型&lt;/a>;，以预测在哪里形成和重新布鲁特飞机。与美国航空公司和突破性能源合作，我们使用该系统证明缩小缩小量减少了54％。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s957/image14.gif&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 400” data-Original-width =“ 957” src =“ https：//bloggerger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ text-align：center;”>; &lt;em style =“ text-align：left;”>;使用AI和GONE-16卫星图像在美国检测到的最符合尾声。&lt;/em>; &lt;/em>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt; /tbody>; &lt;/table>; &lt;p>;我们还正在开发新颖的技术驱动方法&lt;a href =“ https://blog.google/outreach-initiatives/sustainability/google-google-ai-climate-change-solutions/” >;帮助社区应对气候变化的影响&lt;/a>;。例如，我们拥有&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;将我们的洪水预报范围扩大到80个国家/地区&lt;/ A>;直接影响超过4.6亿人。我们已经启动了&lt;a href=&quot;https://blog.research.google/2023/10/looking-back-at-wildfire-research-research-in.html&quot;>;研究工作数量&lt;/a>;野火的危险增加，包括&lt;a href=&quot;https://blog.research.google/2023/02/realtim-time-tracking-of-wildfire.html&quot;>;实时跟踪野火边界&lt;/a>;卫星图像和工作，&lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-traffic-evacuations-case-study.html&quot;>;改善紧急疏散计划&lt;/a>;快速扩张的野火。我们的&lt;a href=&quot;https://www.americanforests.org/article/american-forests-unveils-unveils-undates-for-tree-equility-equity-score-score-tool-tool-tool-tool-tool-too--------------------- >;在美国森林中，我们的数据从我们的&lt;a href=&quot;https://insights.sustainability.google/places/chijvtppokywqkfqkrmtveauzljra/trees？ =“ https://treeequityscore.org/”>; Tree Equity评分&lt;/a>;平台，帮助社区识别并解决对树木的不平等访问。&lt;/p>; &lt;p>;最后，我们继续为天气预测开发更好的模型更长的时间范围。改进&lt;a href=&quot;https://blog.research.google/2020/03/a-neural-neural-neur-weather-model-model-for-eight-hour.html&quot;>; metnet &lt;/a>; ：//blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html“>; metnet-2 &lt;/a>;，在今年的&lt;a href =” https：https：https：https： //blog.research.google/2023/11/metnet-3-state-of-art-neur-weather.html&quot;>; metnet-3 &lt;/a>;，我们现在优于传统的传统数字天气模拟， 。在中期，全球天气预报领域，我们在&lt;a href =“与&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/integrated_forecast_systemqu.hres &lt;/a>; Hres &lt;/a>; HRES &lt;/a>;，Weather-forecasting/“>; Graphcast &lt;/a>;相比&lt;a href=&quot;https://en.wikipedia.org/a>; hres &lt;/a>;最准确的操作确定性预测，由&lt;a href=&quot;https://www.ecmwf.int/&quot;>;欧洲中期天气预测中心&lt;/a>;（ECMWF）产生。在与ECMWF合作的情况下，我们发布了&lt;a href=&quot;https://blog.research.google/2023/08/weatherbench-2-benchmark-2-benchmark-for-next.html&quot;>; weatherbench-2 &lt;/a>;在通用框架中评估天气预报的准确性。 &lt; /p>; &lt;br />; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-Left：auto; auto; margin-right; margin-right：auto;&#39;&#39; >; &lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;iframe lassefullscreen =“” class =“ blog_video_class” height =“ 360” src =“ https：//www.youtube.com/embed /q6folw-y_ss“ width =“ 640” youtube-src-id =“ q6folw-y_ss”>; &lt;/iframe>; &lt;/iframe>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;td class =“ trapcaption =” traption =“ style =” -Align：中心;“>; Graphcast预测的选择在10天内滚动10天，显示700座海Hectopascals（表面上方约3 km），表面温度和表面风速。&lt;/td>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt;/try>; &lt;/table>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>;医疗保健很重要。我们最初的&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06291-2&quot;>; Med-palm &lt;/a>;模型是第一个能够在美国医疗上获得通过得分的模型许可考试。我们最近的&lt;a href=&quot;https://blog.google/technology/health/health/ai-llm-medpalm-research-thecheckup/&quot;>; Med-palm 2模型&lt;/a>;进一步改善了19％，实现了一个专家级的准确性为86.5％。这些&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>; Med-palm模型&lt;/a>;是基于语言的，使临床医生能够提出问题并就复杂的医疗条件和对话进行对话是&lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/introducing-medlm-for-for-the-healthcare-industry&quot;>;可用于医疗保健组织的一部分&lt;/a>; &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview&quot;>; medlm &lt;/a>;通过Google Cloud。 &lt;/p>; &lt;p>;以我们的一般语言模型的发展方式以处理多种方式，我们最近显示了对A &lt;a href =“ https://blog.research.google/2023/08/multimodal--08/multimodal-- MED-PALM的Medical-ai.html“>;多模式版本&lt;/a>;能够解释医学图像，文本数据和其他方式，&lt;a href=&quot;https://arxiv.org/abs/2307.1434&quot;>;描述路径&lt;/a>;我们如何实现AI模型的令人兴奋的潜力，以帮助推进现实世界中的临床护理。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOASoon-BPt__Fh9jN2NiXY1W6z5em CW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s1600/image2.gif&quot;样式=“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 900” data-Original-width =“ 1600” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ Text-Align：Center;”>; &lt;em style =“ text-align：left;”>; med-palm m是一种大型的多模式生成模型，可以灵活地编码和解释生物医学数据，包括临床语言，成像和基因组学，模型权重。&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;我们也一直在研究&lt;a href =“ https://deepmind.google/discover/discover/blog/codoc - 开发可靠的ai-ai-tools-for-Healthcare/“>;如何最好地利用AI模型在临床工作流程中&lt;/a>;。我们已经表明，&lt;a href=&quot;https://blog.research.google/2023/03/learning-from-deep-learning-case-case-study.html&quot;>;与可解释性方法的深度学习结合&lt;/a>;临床医生的新见解。我们还表明，在仔细考虑隐私，安全，公平和道德的情况下，&lt;a href =“ https://blog.research.google/2023/04/ robust-and-medical-medical-imaging） .html“>;可以减少所需的取消识别数据&lt;/a>; &lt;/a>;将临床相关的医学成像模型训练3×–100倍，从而减少了在实际临床环境中采用模型的障碍。我们还发布了&lt;a href=&quot;https://blog.research.google/2023/11/enabling-large-scale-scale-scale-health-studies-for.html&quot;>;开放源代码移动数据收集平台&lt;/a>;使用慢性疾病为社区提供自己的研究工具。&lt;/p>; &lt;p>; AI系统还可以发现现有的医学数据形式的全新信号和生物标志物。在&lt;a href=&quot;https://blog.research.google/2023/03/detecting-novel-novel-systemic-biomarkers-in.html&quot;>;小说生物标志物中发现的小说生物标志物&lt;/a>;，我们证明了这一点，可以从外部眼睛的照片中预测跨越多个器官系统（例如肾脏，血液，肝脏）的全身生物标志物数量。在其他工作中，我们表明，结合&lt;a href=&quot;https://blog.research.google/2023/04/developing-gaging-gaging-gaging-clock-usis-deep.html&quot;>;视网膜图像和基因组信息&lt;/a>;确定一些衰老的根本因素。 &lt;/p>; &lt;p>;在基因组学领域，我们与60个机构的119位科学家合作，创建&lt;a href =“ https://blog.research.google/2023/05/building-better-better-better-bengenomes-pangenomes-pangenomes-pangenomes-pangenomes Revion.html“>;人类基因组的新图&lt;/a>;或pangenome。这种更公平的pangenome更好地代表了全球人群的基因组多样性。建立在我们开创性的&lt;a href=&quot;https://www.nature.com/articles/s41586-021-03819-2&quot;>; alphafold &lt;/a>;上/deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-thecause-thecause-of-of-diseases/&quot;>; alphamissense &lt;/a>;今年提供了预测目录为89％的目录在所有7100万&lt;a href=&quot;https://en.wikipedia.org/wiki/missense_munt&quot;>;错过变体中&lt;/a>;可能是病原或可能的良性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s1070/image1 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;em style =“ text-align：left;”>;字母秒预测的示例覆盖在alphafold预测的结构（红色 - 预测为致病性上；蓝色 - 被预测为良性；灰色 - 不确定）。红点代表已知的致病错义变体，蓝点代表已知的良性变体。该蛋白质中的变体会引起镰状细胞贫血。这种蛋白质的变体可导致囊性纤维化。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们还分享了&lt;a href=&quot;https://deepmind.google/discover/ blog/a-glimpse of-next-gener-of-alphafold/“>;更新&lt;/a>;有关下一代Alphafold的进展。现在，我们的最新模型可以对&lt;a href=&quot;https://www.wwpdb.org/&quot;>;蛋白质数据库中的几乎所有分子产生预测，&lt;/a>;（pdb）经常达到原子精度。这释放了新的理解并显着提高了多种关键生物分子类别的准确性，包括配体（小分子），蛋白质，核酸（DNA和RNA）以及包含翻译后修饰（PTMS）的配体。 /p>; &lt;p>;在神经科学方面，我们&lt;a href=&quot;https://blog.research.google/2023/09/google-research-emakearch-embarks-on-embarks-on-formort-to.html&quot;>;宣布了一次新的合作&lt;/a>;与哈佛，普林斯顿，NIH和其他人一起以突触分辨率绘制整个鼠标大脑，从第一阶段开始，该阶段将重点放在&lt;a href =“ https://en.wikipedia.org/wiki.org/wiki /Hippocampal_formation“>;海马形成&lt;/a>;  - 负责记忆形成，空间导航和其他重要功能的大脑区域。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h3>;量子计算&lt;/h3>; &lt;p>;量子计算机有可能解决大型现实世界科学和工业的问题。但是要意识到潜力，它们必须比今天大得多，并且必须可靠地执行无法在古典计算机上执行的任务。 &lt;/p>; &lt;p>;今年，我们朝着开发一台大规模，有用的量子计算机迈出了重要一步。我们的突破是&lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-erors-erors-erors-scaling.html&quot;>;量子错误校正&lt;/a>;表明它是的，可能减少错误的同时增加量子数的数量。 To enable real-world applications, these qubit building blocks must perform more reliably, lowering the error rate from ~1 in 10&lt;sup>;3&lt;/sup>; typically seen today, to ~1 in 10&lt;sup>;8&lt;/sup>; 。 &lt;/p>; &lt;br />; &lt;h2>;Responsible AI research&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Design for Responsibility &lt;/h3>; &lt;p>; Generative AI is having a transformative impact in a wide range of fields including healthcare, education, security, energy, transportation, manufacturing, and entertainment. Given these advances, the importance of designing technologies consistent with our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; remains a top priority. We also recently published case studies of &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;emerging practices in society-centered AI&lt;/a>; 。在我们的年度&lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_google_ai_ai_principles_progress_progress_update.pdf&quot; tarts=&quot;_blank&quot;>; details on how our Responsible AI research is integrated into products and risk management processes. &lt;/p>; &lt;p>; Proactive design for Responsible AI begins with identifying and documenting potential harms. For example, we recently &lt;a href=&quot;https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/&quot;>;introduced&lt;/a>; a &lt;a href=&quot;https://arxiv.org/abs/2310.11986&quot;>;three-layered&lt;/a>; context-based framework for comprehensively evaluating the social and ethical risks of AI systems.在模型设计过程中，可以通过使用&lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;负责任的数据集&lt;/a>;来减轻危害。 &lt;/p>; &lt;p>; We are &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with Howard University&lt;/a>; to build high quality African-American English (AAE) datasets to improve our products and make them work well for more people. Our research on &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3593013.3594016&quot;>;globally inclusive cultural representation&lt;/a>; and our publication of the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone scale&lt;/a>; furthers our commitments to equitable representation of all people. The insights we gain and techniques we develop not only help us improve our own models, they also power &lt;a href=&quot;https://blog.google/intl/en-in/company-news/using-ai-to-study-demographic-representation-in-indian-tv/&quot;>;large-scale studies of representation in popular media&lt;/a>; to inform and inspire more inclusive content creation around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s1196/image9.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Monk Skin Tone (MST) Scale.如需了解更多信息，请访问&lt;a href=&quot;http://skintone.google/&quot;>;skintone.google&lt;/a>;。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With advances in generative image models, &lt;a href=&quot;https://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot;>;fair and inclusive representation of people&lt;/a>; remains是重中之重。 In the development pipeline, we are working to &lt;a href=&quot;https://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot;>;amplify underrepresented voices and to better integrate social context knowledge&lt;/a>;. We proactively address potential harms and bias using &lt;a href=&quot;https://arxiv.org/pdf/2306.06135.pdf&quot;>;classifiers and filters&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf /2311.17259.pdf&quot;>;careful dataset analysis&lt;/a>;, and in-model mitigations such as fine-tuning, &lt;a href=&quot;https://arxiv.org/abs/2310.16523&quot;>;reasoning&lt;/a>;, &lt; a href=&quot;https://arxiv.org/abs/2306.14308&quot;>;few-shot prompting&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2310.16959&quot;>;data augmentation&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2310.17022&quot;>;受控解码&lt;/a>;，我们的研究表明生成式人工智能能够&lt;a href=&quot;https://arxiv.org/abs/2302.06541 &quot;>;higher quality safety classifiers&lt;/a>; to be developed with far less data. We also released &lt;a href=&quot;https://developers.googleblog.com/2023/10/make-with-makersuite-part-2-tuning-llms.html&quot;>;a powerful way to better tune models with less data&lt;/a>; giving developers more control of responsibility challenges in generative AI.&lt;/p>; &lt;p>; We have developed new&lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>; state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we found that we can identify mislabelled training examples.这使您可以减少训练数据中的噪音，从而导致模型准确性的显着提高。 &lt;/p>; &lt;p>; We initiated several efforts to improve safety and transparency about online content. For example, we introduced &lt;a href=&quot;https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/&quot;>;SynthID&lt;/a>;, a tool for watermarking and identifying AI-生成的图像。 SynthID对人眼是不可察觉的，不会损害图像质量，并且即使在添加过滤器，更改颜色和通过各种有损压缩方案进行修改之后，也允许水印保持可检测。 &lt;/p>; &lt;p>; We also launched &lt;a href=&quot;https://blog.google/products/search/google-search-new-fact-checking-features/&quot;>;About This Image&lt;/a>; to help people assess the credibility of images, showing information like an image&#39;s history, how it&#39;s used on other pages, and available metadata about an image. And we &lt;a href=&quot;https://arxiv.org/abs/2210.03535&quot;>;explored safety methods&lt;/a>; that have been developed in other fields, learning from established situations where there is low-risk tolerance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s616/image6 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;346&quot; data-original-width=&quot;616&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;SynthID generates an imperceptible digital watermark for AI-generated images.&lt;/em>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Privacy remains an essential aspect of our commitment to Responsible AI. We continued improving our state-of-the-art privacy preserving learning algorithm &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, developed the DP-Alternating Minimization algorithm (&lt;a href=&quot;https://arxiv.org/pdf/2310.15454.pdf&quot;>;DP-AM&lt;/a>;) to enable personalized recommendations with rigorous privacy protection, and defined a new &lt;a href=&quot;https://blog.research.google/2023/09/differentially-private-median-and-more.html&quot;>;general paradigm&lt;/a>; to reduce the privacy costs for many aggregation and learning tasks. We also proposed a scheme for &lt;a href=&quot;https://openreview.net/pdf?id=q15zG9CHi8&quot;>;auditing differentially private machine learning systems&lt;/a>;.&lt;/p>; &lt;p>; On the applications front we demonstrated that &lt;a href=&quot;https://arxiv.org/pdf/2308.10888.pdf&quot;>;DP-SGD offers a practical solution&lt;/a>; in the large model fine-tuning regime and showed that images generated by DP diffusion models are &lt;a href=&quot;https://arxiv.org/pdf/2302.13861.pdf&quot;>;useful for a range of downstream tasks&lt;/a>;. We &lt;a href=&quot;https://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot;>;proposed&lt;/a>; a new algorithm for DP training of large embedding models that provides efficient training on TPUs没有损害准确性。 &lt;/p>; &lt;p>; We also teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>; to address the scenario in which training images are forgotten to protect the privacy or rights of individuals. We shared a mechanism for &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;extractable memorization&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot; >;参与式系统&lt;/a>;让用户能够更好地控制他们的敏感数据。 &lt;/p>; &lt;p>; We continued to expand the world&#39;s largest corpus of atypical speech recordings to &amp;gt;1M utterances in &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize atypical speech by 37%&lt;/a>; on real-world benchmarks. &lt;/p>; &lt;p>; We also built an &lt;a href=&quot;https://blog.research.google/2023/08/study-socially-aware-temporally-causal.html&quot;>;audiobook recommendation system&lt;/a>; for students with reading disabilities such as dyslexia. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Adversarial testing&lt;/h3>; &lt;p>; Our work in adversarial testing &lt;a href=&quot;https://blog.research.google/2023/03/responsible-ai-at-google-research.html&quot;>;engaged community voices&lt;/a>; from historically marginalized communities. We partnered with groups such as the &lt;a href=&quot;https://arxiv.org/abs/2303.08177&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR) to ensure we represent the diverse communities who use our models and &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;engage with external users&lt;/a>; to identify potential harms in generative model outputs. &lt;/p>; &lt;p>; We &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;established a dedicated Google AI Red Team&lt;/a>; focused on testing AI models and products for security, privacy, and abuse risks. We showed that attacks such as “&lt;a href=&quot;https://arxiv.org/pdf/2302.10149.pdf?isApp=1&quot;>;poisoning&lt;/a>;” or &lt;a href=&quot;https://arxiv.org/pdf/2306.15447.pdf&quot;>;adversarial examples&lt;/a>; can be applied to production models and surface additional risks such as memorization in both &lt;a href=&quot;https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf&quot;>;image&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;text generative models&lt;/a>;. We also demonstrated that defending against such attacks can be challenging, as merely applying defenses can cause other &lt;a href=&quot;https://arxiv.org/pdf/2309.05610.pdf&quot;>;security and privacy leakages&lt;/a>;. We also introduced model evaluation for &lt;a href=&quot;https://arxiv.org/abs/2305.15324&quot;>;extreme risks&lt;/a>;, such as offensive cyber capabilities or strong manipulation skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Democratizing AI though tools and education&lt;/h3>; &lt;p>; As we advance the state-of-the-art in ML and AI, we also want to ensure people can understand and apply AI to specific problems. We released &lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; (now &lt;a href=&quot;https://makersuite.google.com&quot;>;Google AI Studio&lt;/a>;), a web-based tool that enables AI developers to quickly iterate and build lightweight AI-powered apps. To help AI engineers better understand and debug AI, we released &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;LIT 1.0&lt;/a>;, a state-of-the-art, open-source debugger for machine learning models. &lt;/p>; &lt;p>; &lt;a href=&quot;https://colab.google/&quot;>;Colab&lt;/a>;, our tool that helps developers and students access powerful computing resources right in their web browser, reached over 10 million users 。 We&#39;ve just added &lt;a href=&quot;https://blog.google/technology/ai/democratizing-access-to-ai-enabled-coding-with-colab/&quot;>;AI-powered code assistance&lt;/a>; to all users at no cost — making Colab an even more helpful and integrated experience in data and ML workflows. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s844 /image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;844&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9 hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s16000/image10.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;One of the most used features is “Explain error” — whenever the user encounters an execution error in Colab, the code assistance model provides an explanation along with a potential fix.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To ensure AI produces accurate knowledge when put to use, we also recently introduced &lt;a href=&quot;https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/&quot;>;FunSearch&lt; /a>;, a new approach that generates verifiably true knowledge in mathematical sciences using evolutionary methods and large language models.&lt;/p>; &lt;p>; For AI engineers and product designers, we&#39;re updating the &lt;a href=&quot;https:/ /pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>; with generative AI best practices, and we continue to design &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, which includes &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Community engagement&lt;/h3>; &lt;p>; We continue to advance the fields of AI and computer science by publishing much of our work and participating in and organizing conferences. We have published more than 500 papers so far this year, and have strong presences at conferences like ICML (see the &lt;a href=&quot;https://blog.research.google/2023/07/google-at-icml-2023.html&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/&quot;>;Google DeepMind&lt;/a>; posts), ICLR (&lt;a href=&quot;https://blog.research.google/2023/04/google-at-iclr-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/&quot;>;Google DeepMind&lt;/a>;), NeurIPS (&lt;a href=&quot;https://blog.research.google/2023/12/google-at-neurips-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/&quot;>;Google DeepMind&lt;/a>;), &lt;a href=&quot;https://blog.research.google/2023/10/google-at-iccv-2023.html&quot;>;ICCV&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/06/google-at-cvpr-2023.html&quot;>;CVPR&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/07/google-at-acl-2023.html&quot;>;ACL&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/google-at-chi-2023.html&quot;>;CHI&lt;/a>;, and &lt;a href=&quot;https://blog.research.google/2023/08/google-at-interspeech-2023.html&quot;>;Interspeech&lt;/a>;. We are also working to support researchers around the world, participating in events like the &lt;a href=&quot;https://deeplearningindaba.com/2023/google-outreach-mentorship-programme/&quot;>;Deep Learning Indaba&lt;/a>;, &lt; a href=&quot;https://khipu.ai/khipu2023/khipu-2023-speakers2023/&quot;>;Khipu&lt;/a>;, supporting &lt;a href=&quot;https://blog.google/around-the-globe/google- latin-america/phd-fellowship-research-latin-america/&quot;>;拉丁美洲博士奖学金&lt;/a>;等。 We also worked with partners from 33 academic labs to pool data from 22 different robot types and create the &lt;a href=&quot;https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/&quot;>;Open X-Embodiment dataset and RT-X model&lt;/a>; to better advance responsible AI development. &lt;/p>; &lt;p>; Google has spearheaded an industry-wide effort to develop &lt;a href=&quot;https://mlcommons.org/working-groups/ai-safety/ai-safety/&quot;>;AI safety benchmarks&lt;/a >; under the &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; standards organization with participation from several major players in the generative AI space including OpenAI, Anthropic, Microsoft, Meta, Hugging Face, and more 。 Along with others in the industry we also &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;co-founded&lt;/a>; the &lt;a href=&quot;https://www.frontiermodelforum.org/&quot;>;Frontier Model Forum&lt;/a>; (FMF), which is focused on ensuring safe and responsible development of frontier AI models. With our FMF partners and other philanthropic organizations, we launched a $10 million &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum-executive-director/&quot;>;AI Safety Fund&lt;/a>; to advance research into the ongoing development of the tools for society to effectively test and evaluate the most capable AI models. &lt;/p>; &lt;p>; 通过与 &lt;a href=&quot;http://Google.org&quot;>;Google.org&lt;/a>; 密切合作，我们&lt;a href=&quot;https://blog.google/technology/ai /google-ai-data-un-global-goals/&quot;>;worked with the United Nations&lt;/a>; to build the &lt;a href=&quot;https://unstats.un.org/UNSDWebsite/undatacommons/sdgs&quot;>;UN Data Commons for the Sustainable Development Goals&lt;/a>;, a tool that tracks metrics across the 17 &lt;a href=&quot;https://sdgs.un.org/goals&quot;>;Sustainable Development Goals&lt;/a>;, and &lt;a href =&quot;https://globalgoals.withgoogle.com/globalgoals/supported-organizations&quot;>;supported projects&lt;/a>; from NGOs, academic institutions, and social enterprises on &lt;a href=&quot;https://blog.google/outreach- initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;using AI to accelerate progress on the SDGs&lt;/a>;. &lt;/p>; &lt;p>; The items highlighted in this post are a small fraction of the research work we have done throughout the last year. Find out more at the &lt;a href=&quot;https://blog.research.google/&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/&quot;>;Google DeepMind&lt;/a>; blogs, and our &lt;a href=&quot;https://research.google/pubs/&quot;>;list of publications&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Future vision&lt;/h2>; &lt;p>; As multimodal models become even more capable, they will empower people to make incredible progress in areas from science to education to entirely new areas of knowledge. &lt;/p>; &lt;p>; Progress continues apace, and as the year advances, and our products and research advance as well, people will find more and interesting creative uses for AI. &lt;/p>; &lt;p>; Ending this Year-in-Review where we began, as we say in &lt;em>;&lt;a href=&quot;https://ai.google/static/documents/google-why-we-focus-on-ai.pdf&quot;>;Why We Focus on AI (and to what end)&lt;/a>;&lt;/em>;: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; If pursued boldly and responsibly, we believe that AI can be a foundational technology that transforms the lives of people everywhere — this is what excites us! &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;This Year-in-Review is cross-posted on both the &lt;a href=&quot;https://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html&quot;>;Google Research Blo&lt;/a>;g and the &lt;a href=&quot;https://deepmind.google/discover/blog/2023-a-year-of-groundbreaking-advances-in-ai-and-computing/&quot;>;Google DeepMind Blog&lt;/a>;.&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8816183473385638131/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8816183473385638131&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8816183473385638131&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html&quot; rel=&quot;alternate&quot; title=&quot;2023: A year of groundbreaking advances in AI and computing&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU12G_p2DZO4gQ-P95aP2IFvtKRHRG5Oik9VzJ4WtT_VznggjUrL4tTzqto-C8yx6ULgvugKgH9usiZxnKGk97pOFyHWnu-1S4sSbR2Jjq5T36tQRbZucTAP7gmXkGw77xN6s39IKxPaxbo5tw_Cq52ZfPWOCBkWL-2XTuzsIh6viRIqgGcdUdNq-pHjzl/s72-c/year_in_review-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5516200703494636207&lt;/id>;&lt;published>;2023-12-19T13:08:00.000-08:00&lt;/published>;&lt;updated>;2023-12-19T14:28:48.518-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Vision Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPoet: A large language model for zero-shot video generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dan Kondratyuk and David Ross, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s1600/videopoetpreview.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; 最近一波视频生成模型已经出现，在许多情况下展示了令人惊叹的如画质量。当前视频生成的瓶颈之一是产生连贯大运动的能力。在许多情况下，即使是当前领先的模型也会产生较小的运动，或者当产生较大的运动时，会表现出明显的伪影。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To explore the application of language models in video generation, we introduce &lt;a href=&quot;https://sites.research.google/videopoet/&quot; >;VideoPoet&lt;/a>;, a large language model (LLM) that is capable of a wide variety of video generation tasks, including text-to-video, image-to-video, video stylization, video &lt;a href=&quot;https: //en.wikipedia.org/wiki/Inpainting&quot;>;inpainting&lt;/a>; and &lt;a href=&quot;https://paperswithcode.com/task/image-outpainting&quot;>;outpainting&lt;/a>;, and video-to-声音的。 One notable observation is that the leading video generation models are almost exclusively diffusion-based (for one example, see &lt;a href=&quot;https://imagen.research.google/video/&quot;>;Imagen Video&lt;/a>;). On the other hand, LLMs are widely recognized as the &lt;em>;de facto&lt;/em>; standard due to their exceptional learning capabilities across various modalities, including language, code, and audio (eg, &lt;a href=&quot;https://google-research.github.io/seanet/audiopalm/examples/&quot;>;AudioPaLM&lt;/a>;).与该领域的替代模型相比，我们的方法将许多视频生成功能无缝集成在单个法学硕士中，而不是依赖于专门针对每个任务的单独训练的组件。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;概述&lt;/h2>; &lt;p>; 下图说明了VideoPoet 的功能。输入图像可以被动画化以产生运动，并且（可以选择裁剪或遮罩）视频可以被编辑以进行修复或修复。对于风格化，该模型接收代表深度和光流（代表运动）的视频，并在顶部绘制内容以产生文本引导的风格。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s1999 /image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;682&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of VideoPoet, capable of multitasking on a variety of video-centric inputs and outputs.法学硕士可以选择将文本作为输入来指导文本到视频、图像到视频、视频到音频、风格化和绘画任务的生成。 Resources used: &lt;a href=&quot;https://commons.wikimedia.org/wiki/Main_Page&quot;>;Wikimedia Commons&lt;/a>; and &lt;a href=&quot;https://davischallenge.org/&quot;>;DAVIS&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Language models as video generators&lt;/h2>; &lt;p>; One key advantage of using LLMs for training is that one can reuse many of the scalable efficiency improvements that have been introduced in existing LLM training infrastructure.然而，法学硕士在离散令牌上运行，这可能使视频生成具有挑战性。 Fortunately, there exist &lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;video&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>;audio&lt;/a>; tokenizers, which serve to encode video and audio clips as sequences of discrete tokens (ie, integer indices), and which can also be converted back into the original representation. &lt;/p>; &lt;p>; VideoPoet trains an &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive language model&lt;/a>; to learn across video, image, audio, and text modalities through the use of multiple tokenizers (&lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;MAGVIT V2&lt;/a>; for video and image and &lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>;SoundStream&lt;/a>; for audio).一旦模型生成以某些上下文为条件的标记，就可以使用标记器解码器将它们转换回可查看的表示形式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s2680 /image21.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;2680&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s16000/image21.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A detailed look at the VideoPoet task design, showing the training and inference inputs and outputs of various tasks.使用标记器编码器和解码器将模态与标记进行转换。 Each modality is surrounded by boundary tokens, and a task token indicates the type of task to perform.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples generated by VideoPoet&lt;/h2>; &lt;p>; Some examples generated by our model are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s1100/image18.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;963&quot; data-original-width=&quot;1100&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s16000/image18.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Videos generated by VideoPoet from various text prompts. For specific text prompts refer to &lt;a href=&quot;http://sites.research.google/videopoet&quot;>;the website&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br / >; &lt;p>; 对于文本到视频，视频输出的长度可变，并且可以根据文本内容应用一系列动作和样式。 To ensure responsible practices, we reference artworks and styles in the public domain eg, Van Gogh&#39;s “Starry Night”.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Text Input&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A Raccoon dancing in Times Square”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A horse galloping through Van-Gogh&#39;s &#39;Starry Night&#39;”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“Two pandas playing cards”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A large blob of exploding splashing rainbow paint, with an apple emerging, 8k”&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Video Output&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFNvzU_hzVGT7XCvA4AdBprfzwpOV_b8xs6Q54No72B88fgtHFK7eHFr3UJ9Ac0tXIemkOUR6VxNC2I8HQWznWs2x4IsB8biOEe2DXBrMxW6HveNhuiae40mF8asd3jYh2WqZMTTObSKiHb0RJIFsK_C7VzBq685OPUY_uqPC5NJGXqKs3AWJOG-Gqgk0V/s448/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhFNvzU_hzVGT7XCvA4AdBprfzwpOV_b8xs6Q54No72B88fgtHFK7eHFr3UJ9Ac0tXIemkOUR6VxNC2I8HQWznWs2x4IsB8biOEe2DXBrMxW6HveNhuiae40mF8asd3jYh2WqZ MTTObSKiHb0RJIFsK_C7VzBq685OPUY_uqPC5NJGXqKs3AWJOG-Gqgk0V/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s448/image11.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRumvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI9 5oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s16000/image11.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453hyphenhyphenXE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxd wiWsMiQ59NDH3axwd0UOIFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s448/image12.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453连字符XE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxdwiWsMiQ59NDH3axwd0UO IFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s16000/image12.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4 Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s448/image17.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TexRdg3WZANuc c-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s16000/image17.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;对于图像到视频， VideoPoet 可以获取输入图像并通过提示为其设置动画。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq- Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl 6vVVtqNNg4nf0mw/s1536/image13.gif&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq-Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsb vnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s16000/image13.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;带有文本提示来引导动作的图像到视频的示例。每个视频都与其左侧的图像配对。&lt;strong>;左侧&lt;/strong>;：“一艘船航行在波涛汹涌的大海上，雷雨闪电，布面动画油画”。&lt;strong>;中&lt;/strong>;：“飞过星云闪烁的星云”。&lt;strong>;右&lt;/strong>;：“悬崖上的流浪者在大风天拄着拐杖俯视下面旋转的海雾”。参考：&lt;a href=&quot;https://commons.wikimedia.org/wiki/Main_Page&quot;>;维基共享资源&lt;/a>;，公共领域** .&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 对于视频风格化，我们在将一些额外的输入文本输入 VideoPoet 之前预测光流和深度信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHG xJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s1536/image16 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-ner lt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s16000/image16.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPoet 文本到视频生成的视频之上的视频风格化示例，其中文本提示、深度和光流用作调节。每对中的左侧视频是输入视频，右侧是风格化输出。 &lt;b>;左&lt;/b>;：“戴着墨镜的袋熊在阳光明媚的海滩上拿着沙滩球。” &lt;b>;中&lt;/b>;：“泰迪熊在清澈的冰冻湖面上滑冰。” &lt;b>;右&lt;/b>;：“一只金属狮子在熔炉的光芒中咆哮。”&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; VideoPoet 还能够生成音频。在这里，我们首先从模型生成 2 秒的剪辑，然后尝试在没有任何文本指导的情况下预测音频。这使得可以从单个模型生成视频和音频。&lt;/p>; &lt;br />; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot; >; &lt;tbody>; &lt;tr>; &lt;td>;&lt;videocontrols=&quot;controls&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/105_drums_with_audio.mp4 &quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;videocontrols=&quot;controls&quot;playsinline=&quot;&quot;width=&quot;100 %&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/107_cat_piano_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td>;&lt;videocontrols=&quot;controls&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/108_train_with_audio.mp4 &quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;videocontrols=&quot;controls&quot;playsinline=&quot;&quot;width=&quot;100 %&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/104_dog_popcorn_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;视频转音频的示例，从视频示例生成音频，无需任何文本输入。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 默认情况下，VideoPoet 模型会生成纵向视频，以针对短格式内容定制其输出。为了展示其功能，我们制作了一部由 VideoPoet 生成的许多短片组成的短片。对于剧本，我们请 &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; 写一个关于旅行的短篇故事浣熊，具有逐个场景的细分和随附的提示列表。然后，我们为每个提示生成视频剪辑，并将所有生成的剪辑拼接在一起以生成下面的最终视频。&lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: 两者； text-align: center;&quot;>; &lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/70wZKfx6Ylk&quot; width=&quot;640 &quot; youtube-src-id=&quot;70wZKfx6Ylk&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt;p>; 当我们开发 VideoPoet 时，我们注意到模型功能的一些不错的属性，我们在下面重点介绍。&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;长视频&lt;/h3>; &lt;p>; 我们可以通过调节最后 1 秒来生成更长的视频视频并预测接下来的 1 秒。通过重复链接，我们表明该模型不仅可以很好地扩展视频，而且即使在多次迭代中也能忠实地保留所有对象的外观。&lt;/p>; &lt;p>; 这里有两个例子VideoPoet 从文本输入生成长视频： &lt;br />; &lt;br />; &lt;/p>;&lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot;>;&lt;tbody >; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;文本输入&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;tdalign=&quot;center&quot; width= &quot;35%&quot;>;&lt;em>;“一名宇航员开始在火星上跳舞。然后五彩缤纷的烟花在背景中爆炸。”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;tdalign=&quot;center&quot; width=&quot;35%&quot;>;&lt;em>;“丛林中非常锋利的精灵石城，有明亮的蓝色河流、瀑布和大而陡峭的垂直悬崖面。”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;视频输出&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAI V6ZbjeXK5JPtQr8Mb_ZcPKiIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/ s448/image14.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAIV6ZbjeXK5JPtQr8Mb_ZcPK iIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>; &amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqW tNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/ s448/image9.gif&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqWtNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9 ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/s16000/image9.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp ;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 还可以交互地编辑 VideoPoet 生成的现有视频剪辑。如果我们提供输入视频，我们可以更改运动对象的操作可以集中在第一帧或中间帧，这样可以实现高度的编辑控制。&lt;/p>; &lt;p>; 例如，我们可以从输入视频并选择所需的下一个剪辑。 &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2NFzPadmS- 8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqL tms_7QNMbSYSspRefkisKLzeWXAIW9/s1280/image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg2NFzPadmS-8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9 CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;左侧的输入视频用作条件，根据初始提示生成四个选择：“一个可爱的、生锈的、损坏的蒸汽朋克机器人的特写，上面覆盖着潮湿的苔藓和发芽的植被，周围环绕着高高的草丛”。对于前三个输出我们展示了无提示动作会发生什么。对于下面列表中的最后一个视频，我们添加了提示“在背景中冒烟通电”来指导操作。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;图像到视频控件&lt;/h3>; &lt;p>; 同样，我们可以将运动应用于输入图像根据文本提示将其内容编辑为所需状态。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpm FjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q -Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s512/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;512 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu- rlgTmes9_4-GONo7NFHYAw1q-Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s16000/image5.gif&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用不同的提示对绘画进行动画处理。 &lt;b>;左&lt;/b>;：“一个女人转身看着镜头。” &lt;b>;右&lt;/b>;：“一个女人打哈欠。” **&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;相机运动&lt;/h3>; &lt;我们还可以通过将所需的相机运动类型附加到文本提示来精确控制相机运动。举个例子，我们通过模型生成了一张带有提示的图像，&lt;em>;“雪山上日出的冒险游戏概念艺术，清澈见底的河流”&lt;/em>;。下面的示例附加给定的文本后缀以应用所需的动作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVR szJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s1536 /image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMj AENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从左到右提示：“缩小”、“移动变焦”、“向左平移”、“弧线拍摄”、“吊车拍摄” ”, “FPV无人机拍摄”。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;评价结果&lt;/h2>; &lt;p>; 我们使用各种基准来评估 VideoPoet 在文本到视频生成方面的性能，以将结果与其他方法进行比较。为了确保中立的评估，我们在各种不同的提示下运行了所有模型，没有挑选示例，并要求人们对他们的偏好进行评分。下图以绿色突出显示了 VideoPoet 被选为以下问题的首选选项的时间百分比。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;文本保真度&lt;/h3>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IFIFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6Umz FqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img 边框=“0”数据原始高度=“800”数据原始宽度=“1999”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-Ifi Fh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s16000/image1 .png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用户对文本保真度的偏好评分，即什么就准确跟随提示而言，视频的百分比是首选。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div >; &lt;h3>;运动兴趣&lt;/h3>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellpacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left:auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB- OFr2gqCUC33D3J6aLCS9sE4LKuoXha89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2Ll N6jExQysy68Ovgmgk1/s1999/image15.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;797&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojuUuwpNmLv7LhYv4a Gs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用户动作兴趣度的偏好评分，即在产生有趣的动作方面首选视频的百分比。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 基于上述情况，平均人们选择了 24 –VideoPoet 中的 35% 示例的提示效果优于竞争模型，而竞争模型的这一比例为 8–11%。评分者还更喜欢 VideoPoet 中 41–54% 的示例，因为其动作比其他模型的 11–21% 更有趣。&lt; /p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;通过VideoPoet，我们展示了法学硕士极具竞争力的视频生成质量跨越各种任务，特别是在视频中制作有趣且高质量的动作。我们的结果表明法学硕士在视频生成领域的巨大潜力。对于未来的方向，我们的框架应该能够支持“任意到任意”的生成，例如，扩展到文本到音频、音频到视频和视频字幕等。 &lt;/p>; &lt;p>; 要查看更多原始质量的示例，请参阅&lt;a href=&quot;http://sites.research.google/videopoet&quot;>;网站演示&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究得到了大量的支持贡献者，包括 Dan Kondratyuk、Lijun Yu、Xiuye​​ Gu、José Lezama、Jonathan Huang、Rachel Hornung、Hartwig Adam、Hassan Akbari、Yair Alon、Vighnesh Birodkar、Yong Cheng、Ming-Chang Chiu、Josh Dillon、Irfan Essa、Agrim Gupta、 Meera Hahn、Anja Hauth、David Hendon、Alonso Martinez、David Minnen、David Ross、Grant Schindler、Mikhail Sirotenko、Kihyuk Sohn、Krishna Somandepalli、Huishing Wang、Jimmy Yan、Ming-Hsuan Yang、Xuan Yang、Bryan Seybold 和 Lu Jiang .&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;我们特别感谢 Alex Siegman 和 Victor Gomes 管理计算资源。我们还要感谢 Aren Jansen、Marco Tagliasacchi、Neil Zeghidour、John Hershey 的音频标记化和处理、Angad Singh 的“Rookie the Raccoon”故事板、Cordelia Schmid 的研究讨论、Alonso Martinez 的平面设计、David Salesin、Tomas Izo和 Rahul Sukthankar 的支持，以及 Jay Yagnik 作为初始概念的架构师。&lt;/em>; &lt;/p>; &lt;br />; &lt;p>; &lt;em>;**&lt;/em>; &lt;br />; &lt;em>;( a) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Rembrandt_Christ_in_the_Storm_on_the_Lake_of_Galilee.jpg&quot;>;加利利海上的风暴&lt;/a>;，伦勃朗 1633 年创作，公共领域。&lt;/em>; &lt; br />; &lt;em>;(b) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Pillars_of_creation_2014_HST_WFC3-UVIS_full-res.jpg&quot;>;创造之柱&lt;/a>;，NASA 2014 年，公开&lt;/em>; &lt;br />; &lt;em>;(c) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Caspar_David_Friedrich_-_Wanderer_above_the_Sea_of_Fog.jpeg&quot;>;雾海上空的漫游者&lt;/ a>;，卡斯帕·大卫·弗里德里希 (Caspar David Friedrich)，1818 年，公共领域&lt;/em>; &lt;br />; &lt;em>;(d) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched .jpg&quot;>;《蒙娜丽莎》&lt;/a>;，达芬奇创作，1503 年，公共领域。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/ 5516200703494636207/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/videopoet-large -language-model-for-zero.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/ feeds/8474926331452026626/posts/default/5516200703494636207&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5516200703494636207 “ rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPoet：用于零镜头视频生成的大型语言模型&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www .blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#缩略图&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_ 5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s72-c/videopoetpreview.gif“宽度=”72“xmlns：媒体=“http://search.yahoo .com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-1587965303727576226 &lt;/id>;&lt;发布>;2023-12-19T08:01:00.000-08:00&lt;/发布>;&lt;更新>;2023-12-19T08:15:57.101-08:00&lt;/更新>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;人工智能促进社会公益&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;算法&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google 地图&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;模拟照亮路径活动后流量&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Yechen Li 和 Neha Arora&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKy KrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s600/TrafficFlow.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>;十五分钟。这是&lt;a href=&quot;https://colosseum.tours/interesting-facts#:~:text=THE%20COLOSSEUM%20COULD%20BE%20FILLED,in%20a%20matter%20of%20mins.&quot;>;需要多长时间清空罗马斗兽场&lt;/a>;，这是一个工程奇迹，至今仍是世界上最大的圆形剧场。两千年后，这种设计继续运转良好，可以将巨大的人群从体育和娱乐场所移出。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 当然，退出竞技场只是第一步。接下来，人们必须在周围街道上拥挤的交通中行驶。这是一个古老的问题，至今仍未解决。在罗马，他们通过禁止直接经过罗马斗兽场的街道上的私人交通来解决这个问题。这项政策在那里有效，但如果你不在罗马怎么办？如果你在超级碗怎么办？或者在泰勒·斯威夫特的演唱会上？解决这个问题的一种方法是使用仿真模型，有时称为“数字孪生”，它们是现实世界交通网络的虚拟复制品，试图捕捉从街道和交叉口的布局到交通网络的每一个细节。车辆流量。这些模型使交通专家能够缓解拥堵，减少事故，并改善驾驶员、乘客和步行者的体验。此前，我们的团队使用这些模型&lt;a href=&quot;https://arxiv.org/abs/2111.03426&quot;>;量化路线的可持续性影响&lt;/a>;，&lt;a href=&quot;https://blog.research.google /2023/10/improving-traffic-evacuations-case-study.html&quot;>;测试疏散计划&lt;/a>;并在 &lt;a href=&quot;https://blog.google/products/maps/google-maps 中显示模拟交通情况-immersive-view-routes/&quot;>;地图沉浸式视图&lt;/a>;。 &lt;/p>; &lt;p>; 校准高分辨率交通模拟以匹配特定环境的特定动态是该领域长期存在的挑战。聚合交通数据的可用性、详细的 Google 地图道路网络数据、交通科学的进步（例如了解&lt;a href=&quot;https://tristan2022.org/Papers/TRISAN_2022_paper_3704.pdf&quot;>;路段需求与速度之间的关系&lt; /a>; 对于有交通信号的路段），以及在基于物理的交通模型中利用速度数据的&lt;a href=&quot;https://research.google/pubs/pub52679/&quot;>;校准技术&lt;/a>;为全球范围内的计算效率优化铺平道路。 &lt;/p>; &lt;p>; 为了在现实世界中测试这项技术，Google Research 与西雅图交通部 (SDOT) 合作开发了基于模拟的交通引导计划。我们的目标是帮助数千名大型体育和娱乐活动的参加者快速安全地离开体育场区域。拟议的计划将大型活动期间离开体育场区域的车辆平均出行时间减少了 7 分钟。我们使用动态消息标志 (DMS) 与 SDOT 合作部署它，并验证了 2023 年 8 月至 11 月期间多个事件的影响。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class= &quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV 48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s1200/PreImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot; 0“数据原始高度=“518”数据原始宽度=“1200”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8Wj wwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHAUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s16000/预实现.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIc​​VEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe6 6vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s1200/PostImplementation.gif&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;519&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhfOuJeVqGFMNIc​​VEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66v GRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s16000/PostImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;我们提出的一项政策建议是从 S Spokane St 分流交通，S Spokane St 是连接该地区与 I-5 和 SR 99 号高速公路的主干道，在活动结束后经常出现拥堵。建议的改变改善了高速公路和主干道的交通流量体育场附近，并减少了交通信号灯后面车辆队列的长度。（请注意，在演示视频中，车辆比实际车辆要大。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2 >;模拟模型&lt;/h2>; &lt;p>; 对于这个项目，我们为西雅图体育场周围区域创建了一个新的模拟模型。该模型的目的是尽可能地重播指定日期的每种交通状况。我们使用开源模拟软件&lt;a href=&quot;https://www.eclipse.org/sumo/&quot;>;城市交通模拟&lt;/a>; (SUMO)。 SUMO 的行为模型帮助我们描述交通动态，例如驾驶员如何做出决策，例如跟车、变道和遵守限速。我们还使用 Google 地图的见解来定义网络的结构和各种静态路段属性（例如车道数量、速度限制、交通灯的存在）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGc B9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s1601/SUMO.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPC esgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s16000/SUMO.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;模拟框架概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 出行需求是模拟器的重要输入。为了计算它，我们首先将给定大都市区的道路网络分解为区域，特别是 13 级 &lt;a href=&quot;http://s2geometry.io/devguide/s2cell_hierarchy.html&quot;>;S2 单元&lt;/a>;，面积为 1.27 公里每个单元&lt;sup>;2&lt;/sup>;区域。由此，我们将出行需求定义为在给定时间段内从出发地区域前往目的地区域的预期出行次数。需求表示为聚合的出发地-目的地 (OD) 矩阵。 &lt;/p>; &lt;p>; 为了获得出发地区域和目的地区域之间的初始预期出行次数，我们使用汇总和匿名的出行统计数据。然后，我们通过将初始需求与观察到的交通统计数据（如路段速度、行程时间和车辆数量）相结合来解决 OD 校准问题，以重现事件场景。 &lt;/p>; &lt;p>; 我们对西雅图 T-Mobile 公园和流明球场过去多次活动的流量进行建模，并通过计算汇总和匿名流量统计数据来评估准确性。分析这些事件场景有助于我们了解不同路由策略对区域拥塞的影响。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ第877章&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;655&quot; data-original-width=&quot;1601&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_C qMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s16000/TrafficHeatMap.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;热图显示，与非比赛日同一时间相比，比赛结束后该地区的出行次数大幅增加。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge- mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s1999/TrafficFlow.png&quot; style=&quot;margin&quot; -左：自动； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;931&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04Jko Z0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s16000/TrafficFlow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;该图在 x 轴上显示了观测到的路段速度，在 y 轴上显示了模拟事件的模拟速度。沿红色 x=y 线的数据点集中表明了模拟重现真实交通状况的能力。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;路线政策&lt;/h2>; &lt;p>; SDOT 和西雅图警察局 (SPD) 的本地知识帮助我们确定了需要改进的最拥堵路线：&lt; /p>; &lt;ul>; &lt;li>;从 T-Mobile Park 体育场停车场的 Edgar Martinez Dr. S 出口到东行 I-5 高速公路/西行 SR 99 高速公路的交通&lt;/li>; &lt;li>;从 Lumen Field 体育场停车场到东行的交通Cherry St. I-5 入口匝道北行 &lt;/li>; &lt;li>;通过西雅图 SODO 社区南行至 S Spokane St. 的交通&lt;/li>; &lt;/ul>; &lt;p>; 我们制定了路线策略并使用模拟对其进行了评估模型。为了更快地疏散交通，我们尝试了将北行/南行交通从最近的匝道路由到更远的高速公路匝道的政策，以缩短等待时间。我们还尝试向活动交通开放 HOV 车道、推荐替代路线（例如 SR 99）或在不同车道之间共享负载以到达最近的体育场坡道。 &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215X z -GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s1600/NorthboundCherry.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot; 0&quot; 数据原始高度=&quot;840&quot; 数据原始宽度= “1600”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5ramD jeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s16000/NorthboundCherry.gif&quot; />;&lt;/a>; &lt;br />; &lt;a href= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpMI1PDffBOZt4rxagHm wra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s1600/SouthboundSpokane.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;837&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VS w75NsygwD2ijowkTSUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s16000/SouthboundSpokane.gif&quot; />;&lt;/a>;&lt;/div>; &lt;h2>;评估结果&lt;/h2>; &lt;p>; 我们对具有不同交通条件、活动时间和参与者的多个活动进行建模对于每项政策，模拟都会重现赛后交通并报告车辆从离开体育场到到达目的地或离开西雅图 SODO 区域的行驶时间。节省的时间计算为赛前/赛后的行驶时间差政策，并显示​​在下表中，针对小型和大型活动，每个政策。我们将每个政策应用于一定比例的流量，并重新估计行程时间。结果显示为 10%、30% 或 50 % 的车辆受政策影响。 &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x -K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s1252/TrafficImprovement.png&quot; style=&quot;左边距：1em；右边距：1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;916&quot; data-original-宽度=“1252”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuO iVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s16000/TrafficImprovement.png&quot; />;&lt;/a>;&lt;/div>; &lt;p>;基于这些模拟结果、实施的可行性和其他考虑，SDOT 决定在大型活动期间使用 DMS 实施“北行 Cherry St 坡道”和“南行 S Spokane St 坡道”政策。这些标志建议司机采取替代路线到达目的地。根据大型活动期间重新规划 30% 的交通路线，这两项政策的结合使每辆车平均节省 7 分钟的出行时间。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 这项工作展示了模拟在建模、识别和量化拟议交通引导政策效果方面的力量。模拟使网络规划人员能够识别未充分利用的网段并评估不同路由策略的效果，从而实现更好的流量空间分布。离线建模和在线测试表明我们的方法可以减少总旅行时间。可以通过添加更多交通管理策略来进一步改进，例如优化交通信号灯。模拟模型历来非常耗时，因此只有最大的城市和高风险项目才能负担得起。通过投资更具可扩展性的技术，我们希望将这些模型带到世界各地更多的城市和用例。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;与 Alex Shashko、Andrew Tomkins、Ashley Carrick、Carolina Osorio、Chao Zhang、Damien Pierce、Iveel Tsogsuren、Sheila de Guia 合作，还有陈一凡。约翰·吉利亚德的视觉设计。我们要感谢我们的 SDOT 合作伙伴 Carter Danne、Chun Kwan、Ethan Bancroft、Jason Cambridge、Laura Wojcicki、Michael Minor、Mohammed Said、Trevor Partap 以及 SPD 合作伙伴 Bryan Clenna 中尉和 Sgt. 中士。 Brian Kokesh。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1587965303727576226/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/simulations-Illuminate-path-to-post.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;edit&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2023/12/simulations-Illuminate-path-to-post.html&quot; rel=&quot;alternate&quot; title=&quot;模拟照亮事件后交通流的路径&quot; type=&quot; text/html“/>; &lt;aunder>; &lt;名称>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/1209862651477775266161 &lt;/uri>; &lt;Email>; &lt;Email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =” https://img1.blog1.blogblog.com/img/b16-round.gif =&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEto ufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPla7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s72 -c/TrafficFlow.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6088118107306075362&lt;/id>;&lt;已发布>;2023-12-15T14:34:00.000-08:00&lt;/已发布>;&lt;已更新>;2023-12-15T14:34:10.381-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“Kaggle”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= &quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;机器的进步机器学习学习&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：Google DeepMind 研究员 Phitchaya Mangpo Phothilimthana 和 Google Research 高级研究员 Bryan Perozzi &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQ JnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s1600/屏幕截图%202023-12-15%20at%202.33.10%E2 %80%AFPM.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 随着机器学习 (ML) 领域近期的快速发展，机器可以&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&quot;>;理解自然语言&lt; /a>;、&lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;>;参与对话&lt;/a>;、&lt;a href=&quot;https://cloud.google.com/vertex- ai/docs/generative-ai/image/overview&quot;>;绘制图像&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;创建视频&lt;/a>;等。现代 ML 模型使用 ML 编程框架进行编程和训练，例如 &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;、&lt;a href=&quot;https://github.com/ google/jax&quot;>;JAX&lt;/a>;、&lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>; 等等。这些库为机器学习从业者提供高级指令，例如线性代数运算（例如矩阵乘法、卷积等）和神经网络层（例如&lt;a href=&quot;https://keras.io/api/layers /卷积层/卷积2d/&quot;>;2D 卷积层&lt;/a>;、&lt;a href=&quot;https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/&quot;>;变压器层&lt;/a>;）。重要的是，从业者不必担心如何使他们的模型在硬件上高效运行，因为机器学习框架将通过底层编译器自动优化用户的模型。因此，机器学习工作负载的效率取决于编译器的性能。编译器通常依靠启发式方法来解决复杂的优化问题，这通常会导致性能不佳。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在这篇博文中，我们展示了 ML for ML 方面令人兴奋的进展。我们特别展示了如何使用 ML 来提高 ML 工作负载的效率！先前的内部和外部研究表明，我们可以通过选择更好的 ML 编译器决策来使用 ML 来提高 ML 程序的性能。尽管存在一些用于程序性能预测的数据集，但它们针对的是小型子程序，例如基本块或内核。我们介绍“&lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TpuGraphs：大型张量计算图上的性能预测数据集&lt;/a>;”（介绍于&lt;a href=&quot;https://nips .cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;），我们最近发布了它，以推动更多关于程序优化的 ML 研究。我们针对该数据集举办了一场 &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime/overview&quot;>;Kaggle 竞赛&lt;/a>;，该竞赛最近已完成，共有 616 个团队的 792 名参与者参加来自 66 个国家。此外，在“&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;通过图段训练学习大图属性预测&lt;/a>;”中，我们介绍了一种新的扩展方法&lt;a href=&quot;https ://arxiv.org/abs/2005.03675&quot;>;图神经网络&lt;/a>;（GNN）训练来处理以图表示的大型程序。该技术既可以在内存容量有限的设备上训练任意大的图，又可以提高模型的泛化能力。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ML 编译器&lt;/h2>; &lt;p>; ML 编译器是转换用户编写的程序（这里，由 TensorFlow 等库提供的数学指令）到可执行文件（在实际硬件上执行的指令）。 ML程序可以表示为计算图，其中节点代表张量运算（例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;矩阵乘法&lt;/a>;），并且边表示从一个节点流向另一个节点的张量。机器学习编译器必须解决许多复杂的优化问题，包括&lt;em>;图级&lt;/em>;和&lt;em>;内核级&lt;/em>;优化。图级优化需要整个图的上下文来做出最佳决策并相应地转换整个图。内核级优化一次转换一个内核（融合子图），独立于其他内核。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCs SyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869u wnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;机器学习编译器中的重要优化包括图形级和内核级优化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;具体示例，想象一个 &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_(mathematics)&quot;>;矩阵&lt;/a>;（2D 张量）：&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7v JMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/ s1999/image4.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;290&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8curCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjir TOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 它可以在计算机内存中存储为 [ABC abc] 或 [A a B b C c]，分别称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Row-_and_column-major_order&quot;>;行主内存布局和列主内存布局&lt;/a>;。一个重要的ML编译器优化是为程序中的所有中间张量分配内存布局。下图显示了同一程序的两种不同的布局配置。我们假设在左侧，分配的布局（红色）是最有效的每个单独运算符的选项。但是，此布局配置要求编译器插入&lt;em>;复制&lt;/em>;操作，以在&lt;em>;添加&lt;/em>;和&lt;em>;卷积&lt;/em>;之间转换内存布局运营。另一方面，右侧配置对于每个单独的运算符来说可能效率较低，但它不需要额外的内存转换。布局分配优化必须在本地计算效率和布局转换开销之间进行权衡。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo 6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;343&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAP_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2 mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;一个节点代表一个张量算子，用其输出张量形状进行注释 [&lt;em>;n&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;n&lt;sub>;1&lt;/sub>;&lt;/ em>;, ...]，其中 &lt;em>;n&lt;sub>;i &lt;/sub>;&lt;/em>; 是维度 &lt;em>;i&lt;/em>; 的大小。布局 {&lt;em>;d&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;d&lt;sub>;1&lt;/sub>;&lt;/em>;, ...} 表示内存中从小到大的排序。应用的配置以红色突出显示，其他有效配置以蓝色突出显示。布局配置指定有影响力的算子的输入和输出的布局（即，卷积和重塑）。当布局不匹配时，将插入复制运算符。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;如果编译器做出最佳选择，则可以显着提高速度。例如，在&lt;a href=&quot;https://ieeexplore.ieee.org/document/9563030&quot;>;选择最佳布局配置而不是默认编译器配置时，我们发现速度提升了 32%&lt;/a>;。 href=&quot;https://www.tensorflow.org/xla&quot;>;XLA&lt;/a>; 基准测试套件。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TpuGraphs 数据集&lt;/h2>; &lt;p>; 鉴于上述情况，我们的目标是通过以下方式提高 ML 模型效率：改进机器学习编译器。具体来说，为编译器配备&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;学习成本模型&lt;/a>;&lt;strong>;&lt;/strong>;会非常有效接受输入程序和编译器配置，然后输出程序的预测运行时间。 &lt;/p>; &lt;p>; 出于这种动机，我们&lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;发布了 TpuGraphs&lt;/a>;，这是一个用于在 Google 自定义&lt; a href=&quot;https://cloud.google.com/tpu/docs/intro-to-tpu&quot;>;张量处理单元&lt;/a>; (TPU)。该数据集针对两种 XLA 编译器配置：&lt;em>;布局&lt;/em>;（从矩阵到更高维度张量的行和列主要排序的概括）和&lt;em>;平铺&lt;/em>;（平铺大小的配置） 。我们在 &lt;a href=&quot;https://github.com/google-research-datasets/tpu_graphs&quot;>;TpuGraphs GitHub&lt;/a>; 上提供下载说明和入门代码。数据集中的每个示例都包含 ML 工作负载的计算图、编译配置以及使用该配置编译时图的执行时间。数据集中的图表是从开源 ML 程序收集的，具有流行的模型架构，例如 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;、&lt;a href=&quot; https://arxiv.org/abs/1905.11946&quot;>;EfficientNet&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;Mask R-CNN&lt;/a>; 和 &lt;a href =&quot;https://arxiv.org/abs/1706.03762&quot;>;变压器&lt;/a>;。该数据集提供的图形比最大（早期）图形属性预测数据集（图形大小相当）多 25 倍，与 ML 程序上现有的性能预测数据集相比，图形大小平均大 770 倍。随着规模的大幅扩大，我们第一次可以探索大图上的图级预测任务，这面临着可扩展性、训练效率和模型质量等挑战。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4 Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1546&quot; data-original-width=&quot;2868&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXb QN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;TpuGraph 与其他图属性预测数据集相比的规模。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们提供基线学习成本模型我们的数据集（架构如下所示）。我们的基线模型基于 GNN，因为输入程序表示为图形。节点特征（如下图蓝色所示）由两部分组成。第一部分是操作码id，它是节点最重要的信息，指示张量操作的类型。因此，我们的基线模型通过嵌入查找表将操作码 ID 映射到操作码嵌入。然后，操作码嵌入与第二部分（节点特征的其余部分）连接，作为 GNN 的输入。我们结合 GNN 生成的节点嵌入，使用简单的图池化缩减（即总和和均值）来创建图的固定大小嵌入。然后，前馈层将所得的图嵌入线性变换为最终标量输出。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUl UWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1106&quot; data-original-width=&quot;2284&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0 m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;我们的基线学习成本模型采用了 GNN，因为程序可以自然地表示为图形。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;此外，我们还提出了&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;图分段训练&lt;/a>;（GST），这是一种扩展 GNN 训练的方法，可以在内存容量有限的设备上处理大型图。预测任务在整个图上的情况（即图级预测）。与节点或边缘级预测的缩放训练不同，图级预测的缩放尚未得到充分研究，但对我们的领域至关重要，因为计算图可以包含数十万个节点。在典型的 GNN 训练（“全图训练”，左下）中，GNN 模型是使用整个图进行训练的，这意味着图的所有节点和边都用于计算梯度。对于大图，这在计算上可能是不可行的。在GST中，每个大图被划分为更小的段，并随机选择段的子集来更新模型；剩余片段的嵌入是在不保存中间激活的情况下生成的（以避免消耗内存）。然后，将所有片段的嵌入组合起来，生成原始大图的嵌入，然后将其用于预测。此外，我们引入历史嵌入表来有效地获取图段的嵌入和段丢失，以减轻历史嵌入的陈旧性。总之，我们的完整方法将端到端训练时间加快了 3 倍。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7AL euVLaUBgKUveWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;790&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9 NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;比较全图训练（典型方法）与图分段训练（我们提出的方法）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Kaggle 竞赛&lt;/h2>; &lt;p>; 最后，我们运行了“&lt;a href=&quot;https://kaggle.com/competitions/predict-ai-model-运行时“快还是慢？预测 AI 模型运行时间&lt;/a>;”在 TpuGraph 数据集上的竞赛。本次比赛共有616支队伍、792名参赛者参加。我们收到了来自 66 个国家/地区的 10507 份意见书。对于 153 名用户（其中​​前 100 名中的 47 名）来说，这是他们的第一次比赛。我们学到了参赛团队采用的许多有趣的新技术，例如：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;图剪枝/压缩&lt;/em>;：许多团队没有使用 GST 方法，而是尝试了不同的方法压缩大图（例如，仅保留包括可配置节点及其直接邻居的子图）。 &lt;/li>;&lt;li>;&lt;em>;特征填充值&lt;/em>;：一些团队观察到默认填充值 0 是有问题的，因为 0 与有效特征值冲突，因此使用填充值 -1 可以改善特征值。模型精度显着提高。 &lt;/li>;&lt;li>;&lt;em>;节点功能&lt;/em>;：一些团队观察到额外的节点功能（例如&lt;a href=&quot;https://www.tensorflow.org/xla/operation_semantics#dot&quot;>;点将军的收缩尺寸&lt;/a>;）很重要。一些团队发现节点特征的不同编码也很重要。 &lt;/li>;&lt;li>;&lt;em>;跨配置注意力&lt;/em>;：获胜团队设计了一个简单的层，允许模型显式地相互“比较”配置。事实证明，这种技术比让模型单独推断每个配置要好得多。 &lt;/li>; &lt;/ul>; &lt;p>; 我们将在&lt;a href=&quot;https://mlforsystems.org/&quot;>;系统机器学习研讨会&lt;/a>;的比赛环节汇报比赛情况并预览获胜解决方案于 2023 年 12 月 16 日在 NeurIPS 举行。最后，祝贺所有获奖者，并感谢你们为推进系统机器学习研究做出的贡献！ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeurIPS expo&lt;/h2>; &lt;p>; 如果您对结构化数据和人工智能的更多研究感兴趣智能方面，我们于 12 月 9 日举办了 NeurIPS Expo 小组&lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;图学习遇见人工智能&lt;/a>;，其中涵盖了先进技术学习成本模型等等！ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Sami Abu-el-Haija（Google 研究）对这项工作和写作做出了重大贡献。本文中的研究描述了与许多其他合作者的联合工作，包括 Mike Burrows、Kaidi Cao、Bahar Fatemi、Jure Leskovec、Charith Mendis、Dustin Zelle 和 Yanqi Zhou。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href =&quot;http://blog.research.google/feeds/6088118107306075362/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/12/advancements-in-machine-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href =&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6088118107306075362&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger .com/feeds/8474926331452026626/posts/default/6088118107306075362&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/advancements- in-machine-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;机器学习的机器学习进展&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>; http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com /g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height= “72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgn Mo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s72-c/截图%202023-12-15%20at%202.33.10%E2% 80%AFPM.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6479608460479432217&lt;/id>;&lt;已发布>;2023-12-15T11:40:00.000-08:00&lt;/已发布>;&lt;已更新>; 2023-12-15T12:18:41.742-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“NeurIPS”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= &quot;样式转移&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;StyleDrop：任意样式的文本到图像生成&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部研究科学家 Kihyuk Sohn 和 Dilip Krishnan&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bq g9UcqVUFC-uKMbsFvxRmRWkokKR4VinfuZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s1600/StyleDrop% 20hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在大量图像-文本对上训练的文本到图像模型使得能够创建涵盖许多流派和主题的丰富多样的图像。此外，当添加到输入文本提示符中时，诸如“动漫”或“ Steampunk”之类的流行样式可能会转化为特定的视觉输出。虽然我们在&lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;即时工程&lt;/a>;方面付出了很多努力，但由于配色方案、照明和其他特征的细微差别。例如，“水彩画”可能指多种风格，并且使用简单地说“水彩画风格”的文本提示可能会导致一种特定风格或几种不可预测的混合。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto” ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf 6qm2M0LgRtNKqXN -7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s959/image5.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img 边框=&quot;0&quot; 数据原始高度= “403”数据原始宽度=“959”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNK qXN-7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s16000/image5.gif&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;当我们提到“水彩绘画风格”时，我们指的是什么？ StyleDrop 不是用自然语言指定样式，而是通过引用样式参考图像来生成样式一致的图像&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt; span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在此博客中我们引入了“&lt;a href=&quot;https://arxiv.org/abs/2306.00983&quot;>;StyleDrop：任何样式的文本到图像生成&lt;/a>;”，这是一个允许显着更高水平的风格化文本的工具 -到图像合成。 StyleDrop 不寻找文本提示来描述样式，而是使用一个或多个样式参考图像来描述文本到图像生成的样式。通过这样做，StyleDrop 能够以与参考一致的风格生成图像，同时有效地规避文本提示工程的负担。这是通过对一些样式的&lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;适配器调整&lt;/a>;有效地微调预训练的文本到图像生成模型来完成的参考图像。此外，通过对其生成的一组图像迭代地微调 StyleDrop，它实现了从文本提示生成风格一致的图像。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;方法概述&lt;/h2>; &lt;p>; StyleDrop是一种文本到图像的生成模型，允许生成视觉风格与用户提供的风格参考图像一致的图像。这是通过对预训练的文本到图像生成模型进行几次参数高效微调来实现的。具体来说，我们在 &lt;a href=&quot;https://muse-model.github.io/&quot;>;Muse&lt;/a>; 上构建 StyleDrop，这是一个文本到图像生成视觉转换器。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Muse：文本到图像生成视觉转换器&lt;/h3>; &lt;p>; &lt;a href=&quot; https://muse-model.github.io/&quot;>;Muse&lt;/a>; 是一种基于蒙版生成图像转换器的最先进的文本到图像生成模型（&lt;a href=&quot;https: //openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf&quot;>;MaskGIT&lt;/a>;）。与扩散模型不同，例如 &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; 或 &lt;a href=&quot;https://github.com/CompVis/stable-diffusion&quot;>;Stable Diffusion&lt;/a>;，Muse 将图像表示为一系列离散标记，并使用变压器架构对其分布进行建模。与扩散模型相比，Muse 速度更快，同时实现了具有竞争力的发电质量。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;参数高效的适配器调整&lt;/h3>; &lt;p>; StyleDrop 是通过微调预调整来构建的在一些风格参考图像及其相应的文本提示上训练 Muse 模型。关于变压器参数高效微调的工作有很多，包括&lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;>;提示调整&lt;/a>;和&lt;a href=&quot;https:// arxiv.org/abs/2106.09685&quot;>;大型语言模型的低阶适应&lt;/a>;（LoRA）。其中，我们选择适配器调整，它被证明可以有效地针对&lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;语言&lt;/a>;微调大型变压器网络，并且以参数高效的方式执行&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf&quot;>;图像生成&lt;/a>;任务。例如，它引入了不到一百万个可训练参数来微调 3B 参数的 Muse 模型，并且只需要 1000 个训练步骤即可收敛。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6 so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s1632/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1632&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6Yv RncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Muse 的参数高效适配器调整。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;带反馈的迭代训练&lt;/h3>; &lt;p>; 虽然 StyleDrop 在从一些风格参考图像中学习风格方面很有效，但从单个风格参考图像中学习仍然具有挑战性风格参考图像。这是因为模型可能无法有效地分解&lt;em>;内容&lt;/em>;（即图像中的内容）和&lt;em>;风格&lt;/em>;（即图像的呈现方式），从而导致减少生成中的&lt;em>;文本可控性&lt;/em>;。例如，如下文步骤 1 和 2 所示，根据单个风格参考图像训练的 StyleDrop 生成的吉娃娃图像显示了风格参考图像中内容（即房屋）的泄漏。此外，生成的寺庙图像看起来与参考图像中的房屋太相似（概念崩溃）。 &lt;/p>; &lt;p>; 我们通过在由用户或图像文本对齐模型选择的合成图像子集上训练新的 StyleDrop 模型来解决这个问题（例如，&lt;a href=&quot;https://arxiv.org /abs/2103.00020&quot;>;CLIP&lt;/a>;)，其图像是由在单个图像上训练的第一轮 StyleDrop 模型生成的。通过对多个合成的图文对齐图像进行训练，该模型可以轻松地将风格与内容分开，从而实现改进的图文对齐。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw4 16CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s1295/image3 .gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;825&quot; data-original-width=&quot;1295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CV ubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;带反馈的迭代训练&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;。第一轮StyleDrop可能会由于内容风格解缠的困难而导致文本可控性降低，例如内容泄漏或概念崩溃。使用由前几轮 StyleDrop 模型生成并由人类或图像文本对齐模型选择的合成图像进行迭代训练，提高了风格化文本到图像生成的文本依从性。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;StyleDrop gallery&lt;/h3>; &lt;p>; 我们通过在 24 个不同风格的参考图像上运行实验来展示 StyleDrop 的有效性。如下所示，StyleDrop 生成的图像在风格上彼此高度一致，并且与风格参考图像高度一致，同时描绘了各种上下文，例如小企鹅、香蕉、钢琴等。此外，该模型可以使用以下方式渲染字母图像一致的风格。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2 ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/s1632/image6.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1632&quot; data-original-width=&quot;1536&quot; height=&quot;640&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8 g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/w602-h640/image6.gif&quot; width=&quot;602&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;风格化的文本到图像生成。样式参考图片&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt; /sup>; 位于黄色框内的左侧。使用的文本提示为：&lt;br>;第一行：小企鹅、香蕉、长凳。&lt;br>;第二行：蝴蝶、F1 赛车、圣诞树。&lt;br>;第三行：咖啡机、咖啡机帽子，驼鹿。&lt;br>;第四行：机器人，毛巾，木屋。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing =&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt; a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUp DrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s1526/image1.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;767&quot; data-original-width=&quot;1526&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpayPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07 IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;程式化的视觉角色生成。样式参考图片&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt; /sup>; 位于黄色框内的左侧。使用的文本提示为：（第一行）字母“A”、字母“B”、字母“C”，（第二行）字母“E”、字母“F”、字母“G”。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;以我的风格生成对象的图像&lt;/h3>; &lt;p>; 下面我们通过从两个个性化生成分布中采样来显示生成的图像，一个用于对象，另一个用于样式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lr​​XSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurR p8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s1006/image8.gif “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;412&quot; data-original-width=&quot;1006&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lr​​XSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe 4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s16000/image8.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;蓝色边框顶部的图像是来自 DreamBooth 数据集的对象参考图像（茶壶、花瓶、狗和猫），左侧的图像红色边框底部是样式参考图像*。紫色边框中的图像（即右下四张图像）是根据特定对象的样式图像生成的。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40 %;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;定量结果&lt;/h3>; &lt;p>; 为了进行定量评估，我们从 &lt;a href=&quot;https://github.com/google- Research/parti/blob/main/PartiPrompts.tsv&quot;>;Parti 提示&lt;/a>;并测量&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;图像到图像 CLIP 分数&lt;/a >; 用于风格一致性，&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;图像到文本 CLIP 分数&lt;/a>;用于文本一致性。我们研究 Muse 和 Imagen 的非微调模型。在微调模型中，我们与 &lt;a href=&quot;https://imagen.research.google/&quot; 上的 &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; 进行了比较>;Imagen&lt;/a>;，最先进的个性化主题文本到图像方法。我们展示了 StyleDrop 的两个版本，一个是根据单个风格参考图像进行训练的，另一个是“StyleDrop (HF)”，它是使用如上所述的人类反馈的合成图像进行迭代训练的。如下所示，与未微调的对应项（0.694 vs. 0.556）以及 Imagen 上的 DreamBooth（0.694 vs. 0.644）相比，StyleDrop (HF) 的风格一致性得分显着提高。我们观察到，与 StyleDrop 相比，StyleDrop (HF) 的文本一致性得分有所提高（0.322 vs. 0.313）。此外，在 Imagen 上的 DreamBooth 和 Muse 上的 StyleDrop 之间的人类偏好研究中，我们发现，就风格参考图像的一致性而言，86% 的人类评分者更喜欢 Muse 上的 StyleDrop，而不是 Imagen 上的 DreamBooth。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl 1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCICKNpmhiJNvYDvI4ux5/s2980 /image16.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;第2980章phenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s16000/image16.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height :40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; StyleDrop 使用一些样式参考图像在文本到图像生成中实现了样式一致性。 Google 的人工智能原则指导了我们 Style Drop 的开发，我们敦促负责任地使用该技术。 StyleDrop 经过改编，可&lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-style&quot;>;在 Vertex AI 中创建自定义样式模型&lt;/a>; ，我们相信它对于艺术总监和平面设计师（他们可能想要以自己的风格集思广益或制作视觉资产原型，以提高他们的生产力和创造力）或想要生成新媒体资产的企业来说可能是一个有用的工具体现一个特定的品牌。与其他生成式人工智能功能一样，我们建议从业者确保其与所使用的任何媒体资产的版权保持一致。更多结果请访问我们的&lt;a href=&quot;https://styledrop.github.io/&quot;>;项目网站&lt;/a>;和&lt;a href=&quot;https://youtu.be/gNHD0_hkJ9A&quot;>;YouTube 视频&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Kihyuk Sohn、Nataniel Ruiz 进行， Kimin Lee、Daniel Castro Chin、Irina Blok、Huiwen Chang、Jarred Barber、Lu Jiang、Glenn Entis、李元珍、袁浩、Irfan Essa、Michael Rubinstein 和 Dilip Krishnan。 &lt;/em>;我们感谢实验中使用的图像的所有者（&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/main/images/assets/data.md&quot;>;链接&lt; /a>; 归属）分享他们的宝贵资产。 &lt;/p>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt; super>;&lt;a name=&quot;fn1&quot;>;&lt;b>;*&lt;/b>;&lt;/a>;&lt;/sup>;请参阅&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/ main/images/assets/data.md&quot;>;图片来源&lt;/a>;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt; /p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6479608460479432217/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/12/styledrop-text-to-image- Generation-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// blog.research.google/2023/12/styledrop-text-to-image- Generation-in.html&quot; rel=&quot;alternate&quot; title=&quot;StyleDrop：任何样式的文本到图像生成&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqV UFC-UKMbsFvxRmRWkokKR4VinfuZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s72- c/StyleDrop%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8223613466421639113&lt;/id>;&lt;已发布>;2023-12-10T06:11:00.000-08:00&lt;/已发布>;&lt;更新>;2023-12-13T14:11:22.885-08:00&lt;/更新>;&lt;title type=&quot;text&quot;>;Google 在 NeurIPS 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;发布者：Google 项目经理 Catherine Armato&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7 vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s1100/google_research_sticker-hero.jpg “样式=“显示：无；” />; &lt;p>; 本周，第 37 届年度&lt;a href=&quot;https://neurips.cc/Conferences/2023&quot;>;神经信息处理系统会议&lt;/a>; (NeurIPS 2023) 召开，这是全球最大的机器学习会议今年，在洛杉矶新奥尔良拉开帷幕。 Google 很荣幸成为今年 NeurIPS 的&lt;a href=&quot;https://neurips.cc/Conferences/2023/Sponsors&quot;>;钻石级赞助商&lt;/a>;，并且将在超过 170 篇被接受的论文中占据一席之地，两次主题演讲，并通过组织支持和参与超过 20 个研讨会和教程为更广泛的研究界做出额外贡献。 Google 还很荣幸成为&lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;机器学习女性&lt;/a>;和&lt;a href=&quot;的白金赞助商https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;人工智能中的拉丁语&lt;/a>;研讨会。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 亲自参加 NeurIPS 2023？欢迎参观 Google 研究展位，详细了解我们为解决该领域一些最有趣的挑战而所做的令人兴奋的工作。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) 帐户，了解 Google 展位活动（例如演示和问答环节）。 &lt;/p>; &lt;p>; 您可以在下面的列表中详细了解我们在会议上展示的最新前沿工作（Google 附属机构以&lt;strong>;粗体&lt;/strong>;突出显示）。请参阅 &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023&quot;>;Google DeepMind 博客&lt;/a>;，详细了解他们参加 NeurIPS 2023 的信息。&lt;/p >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;板和线组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; NeurIPS 董事会：&lt;strong>;Corinna Cortes&lt;/strong>;&lt;br />; 顾问委员会：&lt;strong>;John C. Platt&lt;/strong>; strong>;&lt;br />; 高级领域主席：&lt;strong>;Inderjit S. Dhillon&lt;/strong>;&lt;br />; 创意人工智能主席：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; 项目主席：&lt;strong>;Amir Globerson &lt;/strong>;&lt;br />; 数据集和基准主席：&lt;b>; Remi Denton&lt;/b>;&lt;br />;博览会主席：&lt;b>;叶文明&lt;/b>;&lt;/p>; &lt;/div>; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展台演示/问答时间表&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;h4>;该时间表可能会发生变化。请访问 Google 展位 (#215) 了解更多信息。&lt;/h4>; &lt;p>; 所见即所读？改进文本-图像对齐评估&lt;br />; 演讲者：&lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;12:15PM - 1:45PM&lt;/em>; &lt;/p>; &lt;p>; 像图一样说话：大型语言模型的图编码&lt;br />; 演讲者：&lt;strong>;Bahar Fatemi&lt;/strong>;、&lt;strong >; Jonathan Halcrow&lt;/strong>;、&lt;strong>; Bryan Perozzi&lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; VisIT-Bench：受实际使用启发的视觉语言教学基准&lt;br />; 演讲者：&lt;strong>;Yonatan Bitton &lt;/strong>;&lt;br />; 12 月 11 日星期一 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; MLCommons Croissant&lt;br />; 演讲者：&lt;strong>;Omar Benjelloun&lt;/strong>;、&lt;strong>; Meg Risdal&lt;/strong>;、&lt; strong>;Lora Aroyo&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; DaTaSeg：驯服通用多数据集多任务分割模型&lt;br />; 演讲者：&lt;strong>;顾秀野&lt;/strong>;&lt; br />; 12 月 12 日星期二 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; 嵌入大图&lt;br />; 演讲者：&lt;strong>;Bryan Perozzi&lt;/strong>;、&lt;strong>;Anton Tsitsulin&lt;/strong>;&lt; br />; 12 月 12 日星期二 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; 相关噪声被证明可以击败独立噪声，实现差异化的私人学习&lt;br />; 演讲者：&lt;strong>;Krishna Pillutla&lt;/strong>;&lt;br />; 12 月 12 日星期二 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Med-PaLM&lt;br />; 演讲者：&lt;strong>;涂涛&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; StyleDrop：任意样式的文本到图像生成&lt;br />; 演讲者：&lt;strong>;Kihyuk Sohn&lt;/strong>;、&lt;strong >;Lu Jiang&lt;/strong>;、&lt;strong>;Irfan Essa&lt;/strong>;&lt;br />; 12 月 12 日，星期二 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; DICES 数据集：对话式 AI 安全评估的多样性&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>;、&lt;strong>; Alicia Parrish&lt;/strong>;、&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; 谐振器：基于游戏的可扩展大型模型评估&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong >;Michal Todorovic&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Adversarial Nibbler&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; 走向通才生物医学人工智能&lt;br />; 演讲者：&lt;strong>;涂涛&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; 条件适配器&lt;br />; 演讲者：&lt;strong>;Junwen Bai&lt;/strong>;&lt;br />; 12 月 13 日星期三 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; 通过多模式 RAG 进行患者援助&lt;br />; 演讲者：&lt;strong>;Ryan Knuffman&lt;/strong>;、&lt;strong>;Milica Cvetkovic&lt;/strong >;&lt;br />; 12 月 13 日星期三 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;p>; Hessian 结构如何解释锐度正则化的奥秘&lt;br />; 演讲者：&lt;strong>;Hossein Mobahi&lt;/strong>;&lt;br />; 星期三， 12 月 13 日 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;主题演讲嘉宾&lt; /h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73993&quot;>;负责任的人工智能的多面性&lt; /a>;&lt;br />; 演讲者：&lt;strong>;Lora Aroyo&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73987&quot;>;草图：核心工具、学习增强和自适应鲁棒性&lt;/a>;&lt;br />; 演讲者：&lt;strong>;Jelani Nelson&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%; &quot;>; &lt;br />; &lt;/div>; &lt;h2>;亲和力研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/ 2023/affinity-workshop/66602&quot;>;机器学习领域的女性&lt;/a>;&lt;br />; &lt;strong>;Google 赞助 - 白金级&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc /virtual/2023/affinity-workshop/66607&quot;>;AI 中的拉丁语&lt;/a>;&lt;br />; &lt;strong>;Google 赞助 - 白金级&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// nips.cc/virtual/2023/affinity-workshop/66605&quot;>;机器学习新功能&lt;/a>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https:// nips.cc/virtual/2023/workshop/66541&quot;>;人工智能加速材料设计&lt;/a>; (AI4Mat-2023)&lt;br />; 炉边聊天：&lt;strong>;Gowoon Cheon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66524&quot;>;联想记忆和联想记忆Hopfield Networks 2023 年&lt;/a>;&lt;br />; 小组成员：&lt;strong>;Blaise Agüera y Arcas&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop /66535&quot;>;认知系统中的信息理论原理&lt;/a>; (InfoCog)&lt;br />;演讲者：&lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips .cc/virtual/2023/workshop/66518&quot;>;机器学习和物理科学&lt;/a>;&lt;br />;演讲者：&lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://nips.cc/virtual/2023/workshop/66494&quot;>;UniReps：统一神经模型中的表示&lt;/a>;&lt;br />;组织者：&lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nips.cc/virtual/2023/workshop/66517&quot;>;基础模型中零/少样本学习的鲁棒性&lt;/a>; (R0-FoMo)&lt;br />; 演讲者：&lt;strong>; Partha Talukdar&lt;/strong>;&lt;br />; 组织者：&lt;strong>;Ananth Balashankar&lt;/strong>;、&lt;strong>;姚勤&lt;/strong>;、&lt;strong>;Ahmad Beirami&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66539&quot;>;扩散模型研讨会&lt;/a>;&lt;br />;演讲者：&lt;strong>;Tali Dekel&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66502&quot;>;时间视角下的算法公平性&lt;/a>;&lt;br />;圆桌会议负责人：&lt;strong>;Stephen Pfohl&lt;/strong>;&lt; br />; 组织者：&lt;strong>;Golnoosh Farnadi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66550&quot;>;深度学习中的后门：好处、坏人和丑人&lt;/a>;&lt;br />;组织者：&lt;strong>;Eugene Bagdasaryan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/ Workshop/66549&quot;>;OPT 2023：机器学习优化&lt;/a>;&lt;br />; 组织者：&lt;strong>;Cristóbal Guzmán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc /virtual/2023/workshop/66545&quot;>;机器学习促进创意和设计&lt;/a>;&lt;br />;演讲者：&lt;strong>;Aleksander Holynski&lt;/strong>;、&lt;strong>;Alexander Mordvintsev&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66511&quot;>;机器人学习研讨会：大规模模型的预训练、微调和泛化&lt;/a>;&lt;br />;演讲者： &lt;strong>;Matt Barnes&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66516&quot;>;音频机器学习&lt;/a>;&lt;br />;组织者：&lt;strong>;Shrikanth Narayanan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66531&quot;>;基础模型时代的联邦学习&lt;/ a>; (FL@FM-NeurIPS&#39;23)&lt;br />; 演讲嘉宾：&lt;strong>;谢卓瑞&lt;/strong>;、&lt;strong>;徐峥&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://nips.cc/virtual/2023/workshop/66526&quot;>;社会责任语言建模研究&lt;/a>; (SoLaR)&lt;br />;小组成员：&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66506&quot;>;我不敢相信这不是更好（ICBINB）：基础模型时代的失败模式&lt;/a>;&lt;br / >; 顾问委员会：&lt;strong>;Javier Antorán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66501&quot;>;系统机器学习&lt;/a>; &lt;br />; 主办方：&lt;strong>;王亚文&lt;/strong>;&lt;br />; 竞赛组委会：&lt;strong>;Bryan Perozzi&lt;/strong>;、&lt;strong>;Sami Abu-el-haija&lt;/strong>;&lt;br />; 指导委员会：&lt;strong>;Milad Hashemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66514&quot;>;自我监督学习：理论与实践&lt;/ a>;&lt;br />; 组织者：&lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66496&quot;>;将模型行为归因于比例&lt;/a>; (ATTRIB 2023)&lt;br />; 组织者：Kevin Guu*、Tolga Bolukbasi* &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;竞赛&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66581&quot;>;NeurIPS 2023机器遗忘竞赛&lt;/a>;&lt;br />; 组织者：&lt;b>;Isabelle Guyon&lt;/b>;、&lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips. cc/virtual/2023/competition/66593&quot;>;Lux AI 挑战赛第二季 NeurIPS 版&lt;/a>;&lt;br />; 组织者：&lt;strong>;Bovard Doerschuk-Tiberi&lt;/strong>;、&lt;strong>;Addison Howard&lt;/strong>; &lt; /p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/tutorial/73949&quot;>;以数据为中心的人工智能，打造可靠且负责任的人工智能：从理论到实践&lt;/a>;&lt;br />; &lt;strong>; Isabelle Guyon&lt;/strong>;、Nabeel Seedat、Mihaela va der Schaar &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;创意 AI 赛道&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 创意人工智能表演 1 和 20px 2&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; &lt;em>;&lt; a href=&quot;https://nips.cc/virtual/2023/session/74093&quot;>;性能 1&lt;/a>;：12 月 11 日星期一 |下午 6:30 - 晚上 8:30，大堂舞台&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74094&quot;>;表演 2&lt;/a>;： 12 月 14 日，星期四 | 7:00PM - 9:00PM，大堂舞台&lt;/em>; &lt;/p>; &lt;p>; 创意人工智能会议 1 – 3&lt;br />; 演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt; /strong>;&lt;br />; 组织者：&lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74090&quot;>;会议 1&lt; /a>;：12 月 12 日，星期二 |下午 3:05 - 3:40，D2 厅&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74091&quot;>;会议 2&lt;/a>;： 12 月 13 日，星期三 |上午 10:45 - 下午 2:15，D2 厅&lt;/em>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74092&quot;>;第 3 场会议&lt;/a>;： 12 月 14 日，星期四 |上午 10:45 - 下午 2:15，D2 厅&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/session/75889&quot;>;创意 AI 视频&lt;/a >;&lt;br />; 主办方：&lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;博览会讲座&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;图学习会面人工智能&lt;/a>;&lt;br />;演讲者：&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel /78243&quot;>;共鸣器：音乐空间&lt;/a>;&lt;br />;演讲者：&lt;strong>;Erin Drake Kajioka&lt;/strong>;、&lt;strong>;Michal Todorovic&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://nips.cc/Expo/Conferences/2023/talk%20panel/78237&quot;>;ML 中的经验严谨性是一项大规模可并行化的挑战&lt;/a>;&lt;br />; 演讲者：&lt;strong>;Megan Risdal (Kaggle)&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;口头演讲&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sW8yGZ4uVJ&quot;>;基于有序条件的策略梯度方法全局收敛&lt;/a>;&lt;br />; 梅金成，博Dai，&lt;strong>;Alekh Agarwal&lt;/strong>;，&lt;strong>; &lt;/strong>;Mohammad Ghavamzadeh*，Csaba Szepesvari，Dale Schuurmans &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum? id=y8UAQQHVTX&quot;>;私人永恒预测&lt;/a>;&lt;br />; Moni Naor、Kobbi Nissim、&lt;strong>;Uri Stemmer&lt;/strong>;、Chao Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=PITeSdYQkv&quot;>;用户级差异隐私，每个用户的示例很少&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt; strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Pasin Manurangsi&lt;/strong>;、Raghu Meka、&lt;strong>;张驰远&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=dVaWCDMBof&quot;>;DataComp：寻找下一代多模态数据集&lt;/a>;&lt;br />; Samir Yitzhak Gadre、Gabriel Ilharco、Alex Fang、Jonathan Hayase、Georgios Smyrnis、Thao Nguyen、Ryan Marten、Mitchell Wortsman、Dhruba Ghosh、张洁宇、Eyal Orgad、Rahim Entezari、Giannis Daras、Sarah Pratt、Vivek Ramanujan、Yonatan Bitton、Kalyani Marathe、Stephen Mussmann、Richard Vencu、Mehdi Cherti、Ranjay Krishna、&lt;strong>;Pang Wei Koh&lt;/strong >;、Olga Saukh、Alexander Ratner、Shuran Song、Hannaneh Hajishirzi、Ali Farhadi、Romain Beaumont、Sewoong Oh、Alex Dimakis、Jenia Jitsev、Yair Carmon、Vaishaal Shankar、Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=w116w62fxH&quot;>;可实现回归的最佳学习者：PAC 学习和在线学习&lt;/a>;&lt;br />; Idan Attias、Steve Hanneke、Alkis Kalavasis、&lt;strong>;Amin Karbasi&lt;/strong >;, Grigoris Velegkas &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jDIlzSU8wJ&quot;>;扩散模型对于光流和单目深度估计的惊人有效性&lt;/a>;&lt;br />; Saurabh Saxena、&lt;strong>;Charles Herrmann&lt;/strong>;、&lt;strong>;Junhwa Hur&lt;/strong>;、&lt;strong>;Abhishek Kar&lt;/strong>;、&lt;strong>; &lt;/strong>;Mohammad Norouzi*、&lt;strong>;德清太阳&lt;/strong>;，&lt;strong>; &lt;/strong>;大卫·J·弗利特 &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;期刊轨道&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://jmlr.org/papers/v24/20-998.html&quot;>;使用图进行图聚类神经网络&lt;/a>;&lt;br />; &lt;strong>;Anton Tsitsulin&lt;/strong>;、&lt;strong>;John Palowitch&lt;/strong>;、&lt;strong>;Bryan Perozzi&lt;/strong>;、Emmanuel Müller &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;聚焦论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;https://openreview.net/forum?id=1p6teT6F73&quot;>;高效变压器的交替更新&lt;/a>;（请参阅博客文章）&lt;br />; &lt;strong>;Cenk Baykal&lt;/strong>;、&lt;strong>;Dylan Cutler &lt;/strong>;、&lt;strong>;Nishanth Dikkala&lt;/strong>;、Nikhil Ghosh*、&lt;strong>;Rina Panigrahy&lt;/strong>;、&lt;strong>;王鑫&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=EldbUlZtbd&quot;>;本地化是否有助于编辑？语言模型中基于因果关系的本地化与知识编辑的惊人差异&lt;/a>;&lt;br />; &lt;strong>;Peter Hase&lt;/strong>;、Mohit Bansal、&lt;strong>;Been Kim&lt;/strong>;、&lt;strong>; Asma Ghandeharioun &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jR2FkqW6GB&quot;>;在游戏中学习对学习者有好处吗？&lt;/a>;&lt;br />; William Brown ,&lt;strong>; Jon Schneider&lt;/strong>;,&lt;strong>; Kiran Vodrahalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Bj1QSgiBPP&quot;>;参与式个性化分类&lt;/a>;&lt;br />; Hailey Joren、&lt;strong>;Chirag Nagpal&lt;/strong>;、&lt;strong>;Katherine Heller&lt;/strong>;、&lt;strong>; &lt;/strong>;Berk Ustun &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=c2eedxSlPJ&quot;>;可分离数据上梯度下降的严格风险界限&lt;/a>;&lt;br />; Matan Schliserman，&lt;strong>;Tomer Koren&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=67o9UQgTD0&quot;>;神经语言模型中的反事实记忆&lt;/a>;&lt;br />; &lt;strong>;张驰远&lt;/strong>;,&lt;strong >; &lt;/strong>;Daphne Ippolito、Katherine Lee、Matthew Jagielski、Florian Tramèr、Nicholas Carlini &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5NxJuc0T1P&quot;>;Debias Coarsely，示例有条件地：通过最优输运和概率扩散模型进行统计降尺度&lt;/a>;&lt;br />; &lt;strong>;万忠一&lt;/strong>;、Ricardo Baptista、&lt;strong>; Anudhyan Boral&lt;/strong>;、&lt;strong>;陈一凡&lt;/strong>;,&lt;strong>;约翰·安德森&lt;/strong>;,&lt;strong>;沙飞&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;莱昂纳多·泽佩达-努涅斯&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vTug54Uunq&quot;>;通用优化方法的更快利润最大化&lt;/a>;&lt;br />;Guanghui Wang、Zihao Hu、Vidya Muthukumar、&lt;strong>;Jacob Abernethy &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3PjCt4kmRx&quot;>;从像素到 UI 操作：学习通过图形用户界面遵循说明&lt;/a>;&lt; br />; Peter Shaw、Mandar Joshi、&lt;strong>;James Cohan&lt;/strong>;、&lt;strong>; &lt;/strong>;Jonathan Berant、Panupong Pasupat、胡鹤翔、Urvashi Khandelwal、Kenton Lee、Kristina N Toutanova &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/forum?id=5Gw9YkJkFF&quot;>;PAC 根据标签比例学习线性阈值&lt;/a>;&lt;br />; &lt;strong>;Anand Brahmbhatt&lt;/strong>;、&lt;strong>; Rishi Saket&lt;/strong>;,&lt;strong>; Aravindan Raghuveer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CXPUg86A1D&quot;>;SPAE：用于多模式生成的语义金字塔自动编码器与冻结法学硕士&lt;/a>;&lt;br />;于丽君*、&lt;strong>;程勇&lt;/strong>;、&lt;strong>; &lt;/strong>;王志若、&lt;strong>;Vivek Kumar&lt;/strong>;、&lt;strong>; Wolfgang Macherey &lt;/strong>;、&lt;strong>;黄艳萍&lt;/strong>;、&lt;strong>;大卫·罗斯&lt;/strong>;、&lt;strong>;伊尔凡·埃萨&lt;/strong>;、&lt;strong>; &lt;/strong>;Yonatan Bisk、&lt;strong>;明-宣扬&lt;/strong>;、&lt;strong>;凯文·墨菲&lt;/strong>;、&lt;strong>; &lt;/strong>;亚历山大·豪普特曼、&lt;strong>;陆江&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=QatZNssk7T&quot;>;平衡对抗模型中的自适应数据分析&lt;/a>;&lt;br />; Kobbi Nissim，&lt;strong>; Uri Stemmer&lt;/strong>;，&lt;strong>; &lt;/strong>;Eliad Tsfadia &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NiQTy0NW1L&quot;>;Lexinvariant 语言模型&lt;/a>;&lt;br/>;Qian Huang、Eric Zelikman、Sarah Chen、&lt;strong >; Yuhuai Wu&lt;/strong>;、Gregory Valiant、Percy Liang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HF6bnhfSqH&quot;>;论量子反向传播、信息重用和作弊测量折叠&lt;/a>;&lt;br />; &lt;strong>;阿米拉·阿巴斯&lt;/strong>;、&lt;strong>; &lt;/strong>;罗比·金、黄心源、&lt;strong>;威廉·J·哈金斯&lt;/strong>;、&lt;strong>;雷米斯莫瓦萨格、&lt;strong>;达尔吉尔博亚&lt;/strong>;、&lt;strong>;&lt;/strong>;&lt;strong>;贾罗德·麦克林&lt;/strong>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/forum?id=Pya0kCEpDk&quot;>;随机块模型和混合模型的私有估计算法&lt;/a>;&lt;br />;陈宏杰，&lt;strong>; Vincent Cohen-Addad&lt;/strong>;，&lt;strong>; &lt;/strong>;Tommaso d&#39;Orsi、&lt;strong>; Alessandro Epasto&lt;/strong>;、Jacob Imola、David Steurer、Stefan Tiegel &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DBz9E5aZey&quot;>;可证明通过虚拟粒子随机逼近的 SVGD 快速有限粒子变体&lt;/a>;&lt;br />; &lt;strong>;Aniket Das&lt;/strong>;、&lt;strong>;Dheeraj Nagaraj&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=e0pRF9tOtm&quot;>;重新审视私人（随机）非凸优化：二阶平稳点和过度风险&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>; ,&lt;strong>; &lt;/strong>;刘道高*、Sewoong Oh、Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bKqrWLCMrX&quot;>;揭示视频分布变化下的自监督学习&lt;/a>;&lt;br />; Pritam Sarkar，&lt;strong>;Ahmad Beirami&lt;/strong>;，&lt;strong>;Ali Etemad&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/forum?id=ikkdTD3hQJ&quot;>;AIMS：无所不包的多层次细分&lt;/a>;&lt;br />; Lu Qi, Jason Kuen, Weidong Gui, Jiuxiang Gu, Zhe Lin, Bo杜宇旭、&lt;strong>;杨明轩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rheCTpRrxI&quot;>;DreamHuman：来自文本的可动画 3D 头像&lt;/a>;&lt;br />; &lt;strong>;尼科斯·科洛图罗斯&lt;/strong>;、&lt;strong>;蒂莫·阿尔迪克&lt;/strong>;、&lt;strong>;安德烈·赞菲尔&lt;/strong>;、&lt;strong>;爱德华·加布里埃尔·巴扎万&lt;/strong>;、&lt; strong>;Mihai Fieraru&lt;/strong>;、&lt;strong>;Cristian Sminchisescu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=oaCDiKoJ2w&quot;>;后续行动也很重要: 通过任职后情境改善情境强盗&lt;/a>;&lt;br />;王超奇、叶子宇、&lt;strong>;冯哲&lt;/strong>;、&lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;、徐海峰&lt;/p>; &lt; p>; &lt;a href=&quot;https://openreview.net/forum?id=m21rQusNgb&quot;>;学习用于排名的列表级域不变表示&lt;/a>;&lt;br />; 西安睿成*，&lt;strong>;庄红雷&lt; /strong>;、&lt;strong>;秦珍&lt;/strong>;、Hamed Zamani*、&lt;strong>;路静&lt;/strong>;、&lt;strong>;吉马&lt;/strong>;、&lt;strong>;凯辉&lt;/strong>;、韩昭, &lt;strong>;Xuanhui Wang&lt;/strong>;, &lt;strong>;Michael Bendersky&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hCdqDkA25J&quot;>;最优保证凸优化中的算法再现性和梯度复杂性&lt;/a>;&lt;br />; 张亮，杨俊驰，&lt;strong>;Amin Karbasi&lt;/strong>;，鸟何&lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=hJzEoQHfCe&quot;>;统一嵌入：网络规模机器学习系统经过实战检验的特征表示&lt;/a>;&lt;br />; Benjamin Coleman、Wang-Cheng Kang、&lt;strong>;Matthew Fahrbach&lt;/strong>; ,&lt;strong>; &lt;/strong>;Ruoxi Wang、Lichan Hong、Ed Chi、Derek Cheng &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xOJUmwwlJc&quot;>;接近通知校准深度神经网络&lt;/a>;&lt;br />; 熊苗、邓爱琳、&lt;strong>;Pang Wei Koh&lt;/strong>;、Jiaying Wu、Shen Li、Jianqing Xu、Bryan Hooi &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https ://openreview.net/forum?id=WfsWy59bX2&quot;>;通过相似聚类进行匿名学习：模型泛化的精确分析&lt;/a>;&lt;br />; &lt;strong>;Adel Javanmard&lt;/strong>;,&lt;strong>; Vahab Mirrokni&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EUiIbwV379&quot;>;通过更好的私有特征选择实现更好的私有线性回归&lt;/a>;&lt;br />; &lt;特拉维斯·迪克，詹妮弗·吉伦沃特*，马修·约瑟夫&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=XAyPlfmWpu&quot;>;二值化神经机器翻译&lt;/a>;&lt;br />; 张一驰、Ankush Garg、曹元、&lt;strong>;Łukasz Lew&lt;/strong>;、Behrooz Ghorbani*、张志如、Orhan Firat &lt;/p>; &lt;p>; &lt;a href= &quot;https://nips.cc/virtual/2023/poster/73665&quot;>;BoardgameQA：具有矛盾信息的自然语言推理数据集&lt;/a>;&lt;br />; &lt;strong>;Mehran Kazemi&lt;/strong>;，&lt;strong>;袁泉&lt;/strong>;、&lt;strong>;Deepti Bhatia&lt;/strong>;、&lt;strong>;Najoung Kim&lt;/strong>;、&lt;strong>;徐鑫&lt;/strong>;、&lt;strong>;Vaiva Imbrasaite&lt;/strong>;、&lt;strong>; Deepak Ramachandran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1vvsIJtnnr&quot;>;通过缓和的指数测量进行提升&lt;/a>;&lt;br />; &lt;strong>;Richard诺克，&lt;strong>; &lt;/strong>;埃桑·阿米德，&lt;strong>;曼弗雷德·沃穆斯&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SGlrCuwdsB &quot;>;（基于分数的）文本控制生成模型的概念代数&lt;/a>;&lt;br />; Zihao Wang，Lin Gui，Jeffrey Negrea，&lt;strong>;Victor Veitch&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q8mH2d6uw2&quot;>;通过不连续网络进行深度合约设计&lt;/a>;&lt;br />; Tonghan Wang、&lt;strong>;Paul Dütting&lt;/strong>;、Dmitry Ivanov、Inbal Talgam -Cohen，David C. Parkes &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YoghyvSG0H&quot;>;扩散-SS3D：半监督 3D 物体检测的扩散模型&lt;/a >;&lt;br />; 何正如、戴振轩、林彦雨、&lt;strong>;杨明轩&lt;/strong>;、&lt;strong>;蔡艺轩&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PYASzxr2OP&quot;>;通过比较反馈获取用户偏好以进行个性化多目标决策&lt;/a>;&lt;br />; Han Shao、Lee Cohen、Avrim Blum、 &lt;strong>;Yshay Mansour&lt;/strong>;、Aadirupa Saha、Matthew Walter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qCglMj6A4z&quot;>;线性相关噪声的梯度下降：理论差异隐私及其应用&lt;/a>;&lt;br />; Anastasia Koloskova*、&lt;strong>;Ryan McKenna&lt;/strong>;、&lt;strong>;Zachary Charles&lt;/strong>;、&lt;strong>;J Keith Rush&lt;/strong>;、&lt;strong >;Hugh Brendan McMahan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCwToX315b&quot;>;Entrywise 变换矩阵乘积的低秩近似的难度&lt;/a>;&lt; br />; &lt;strong>;Tamas Sarlos&lt;/strong>;、&lt;strong>; &lt;/strong>;宋星友、David P. Woodruff、张秋仪 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/forum?id=JhQP33aMx2&quot;>;多模态基础模型的逐模块自适应蒸馏&lt;/a>;&lt;br />;&lt;br />; 陈亮，&lt;strong>;于家辉&lt;/strong>;，&lt;strong>;明轩杨&lt;/strong>;、&lt;strong>;Matthew Brown&lt;/strong>;、崔寅、赵拓、&lt;strong>;宫伯清&lt;/strong>;、周天一&lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/forum?id=zGRWp7yRqd&quot;>;多重交换 k-Means++&lt;/a>;&lt;br />; Lorenzo Beretta、&lt;strong>;Vincent Cohen-Addad&lt;/strong>;、&lt;strong>;Silvio Lattanzi&lt;/strong>;、 &lt;strong>; Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8vuDHCxrmy&quot;>;OpenMask3D：开放词汇 3D 实例分割&lt;/a>;&lt;br />; Ayça Takmaz、Elisabetta Fedele、Robert Sumner、Marc Pollefeys、&lt;strong>;Federico Tombari&lt;/strong>;、&lt;strong>;Francis Engelmann&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/forum?id=7RMGI4slcb&quot;>;多语言学习中数据集不平衡的情况下的顺序问题&lt;/a>;&lt;br />; Dami Choi*, &lt;strong>;Derrick Xin&lt;/strong>;, &lt;strong>;Hamid Dadkhahi&lt;/a>;&lt;br />;强>;、Justin Gilmer、Ankush Garg、Orhan Firat、Chih-Kuan Yeh、Andrew M. Dai、Behrooz Ghorbani &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=yEf8NSqTPu&quot; >;PopSign ASL v1.0：通过智能手机收集的独立美国手语数据集&lt;/a>;&lt;br />; Thad Starner、Sean Forbes、Matthew So、David Martin、Rohit Sridhar、Gururaj Deshpande、&lt;strong>;Sam Sepah&lt;/strong >;、Sahir Shahryar、Khushi Bhardwaj、Tyler Kwok、Daksh Sehgal、Saad Hassan、Bill Neubauer、Sofia Vempala、Alec Tan、Jocelyn Heath、Unnathi Kumar、Priyanka Mosur、Tavenner Hall、Rajandeep Singh、Christopher Cui、&lt;strong>;Glenn Cameron&lt; /strong>;、&lt;strong>;索希尔·戴恩&lt;/strong>;、&lt;strong>;加勒特·坦泽&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gaktiSjatl&quot;>;半隐式去噪扩散模型（SIDDM）&lt;/a>;&lt;br />; 徐彦武*、龚明明、谢少安、&lt;strong>;魏伟&lt;/strong>;、&lt;strong>; Matthias Grundmann&lt;/strong>;、&lt;strong>; &lt;/strong>;Kayhan Batmanghelich，&lt;strong>;侯廷波&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xGz0wAIJrS&quot;>;State2Explanation：基于概念的解释有益于代理学习和用户理解&lt;/a>;&lt;br />; Devleena Das、Sonia Chernova、&lt;strong>;Been Kim&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum ?id=AwhpBEqmyo&quot;>;StoryBench：连续故事可视化的多方面基准&lt;/a>;&lt;br />; Emanuele Bugliarello*、Hernan Moraldo、Ruben Villegas、Mohammad Babaeizadeh、Mohammad Taghi Saffar、Han Zhang、Dumitru Erhan、&lt;strong>; Vittorio Ferrari&lt;/strong>;、Pieter-Jan Kindermans、&lt;strong>;Paul Voigtlaender&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wv3bHyQbX7&quot;>;主题驱动通过学徒学习实现文本到图像生成&lt;/a>;&lt;br />;陈文虎、胡鹤翔、&lt;strong>;李彦东&lt;/strong>;、&lt;strong>;Nataniel Ruiz&lt;/strong>;、&lt;strong>;贾旭辉&lt;/strong>; strong>;、Ming-Wei Chang、William W. Cohen &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=plAix1NxhU&quot;>;TpuGraphs：大张量计算图上的性能预测数据集&lt;/a>;&lt;br />; &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;、&lt;strong>;Sami Abu-El-Haija&lt;/strong>;、Kaidi Cao*、&lt;strong>;Bahare Fatemi&lt;/strong>;、&lt;strong>; Mike Burrows&lt;/strong>;、Charith Mendis*、&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=a147pIS2Co&quot;>;培训链-通过潜在变量推理进行思想&lt;/a>;&lt;br />; &lt;strong>;Du Phan&lt;/strong>;、&lt;strong>;Matthew D. Hoffman&lt;/strong>;、David Dohan*、Sholto Douglas、&lt;strong>; Tuan Anh Le&lt;/strong>;、Aaron Parisi、&lt;strong>;Pavel Sountsov&lt;/strong>;、Charles Sutton、Sharad Vikram、&lt;strong>; Rif A. Saurous&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=1ZzG6td0el&quot;>;信息约束下交互式高维估计的统一下界&lt;/a>;&lt;br />; Jayadev Acharya, Clement L. Canonne, &lt;strong>;孙子腾&lt;/strong>; , Himanshu Tyagi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=j5AoleAIru&quot;>;所见即所读？改进文本-图像对齐评估&lt;/a>;&lt;br />; &lt;strong>;Michal Yarom&lt;/strong>;、&lt;strong>;Yonatan Bitton&lt;/strong>;、&lt;strong>;Soravit Changpinyo&lt;/strong>;、&lt;strong>;Roee Aharoni&lt; /strong>;、&lt;strong>;乔纳森·赫齐格&lt;/strong>;、&lt;strong>;奥兰·朗&lt;/strong>;、&lt;strong>;&lt;/strong>;&lt;strong>;埃兰·奥菲克&lt;/strong>;、&lt;strong>;伊丹·斯佩克托&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=4KZhZJSPYU&quot;>;什么时候基于置信度的级联延迟就足够了？&lt;/a>;&lt;br />; &lt;strong>;Wittawat Jitkrittum&lt; /strong>;、&lt;strong>;内哈·古普塔&lt;/strong>;、&lt;strong>;阿迪亚·克里希纳·梅农&lt;/strong>;、&lt;strong>;Harikrishna Narasimhan&lt;/strong>;、&lt;strong>;安基特·辛格·拉瓦特&lt;/strong>;、&lt;strong>;桑吉夫Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=A18PgVSUgf&quot;>;通过知识蒸馏加速分子图神经网络&lt;/a>;&lt;br />; Filip Ekström Kelvinius、Dimitar Georgiev、Artur Petrov Toshev、&lt;strong>;Johannes Gasteiger&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7EMphtUgCI&quot;>;AVIS：自主视觉信息使用大语言模型智能体进行搜索&lt;/a>;&lt;br />;胡紫牛*、&lt;strong>;Ahmet Iscen&lt;/strong>;、&lt;strong>;孙晨&lt;/strong>;、张凯伟、孙一舟、&lt;strong>;David罗斯&lt;/strong>;、&lt;strong>;科迪莉亚·施密德&lt;/strong>;、&lt;strong>;阿里雷扎·法蒂&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=9mJXDcr17V &quot;>;超越不变性：解决“虚假”相关性的测试时标签转移适应&lt;/a>;&lt;br />; Qingyao Sun、Kevin Patrick Murphy、&lt;strong>;Sayna Ebrahimi&lt;/strong>;、Alexander D&#39;Amour &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0tEjORCGFD&quot;>;协作分数蒸馏以实现一致的视觉编辑&lt;/a>;&lt;br />; Subin Kim、Kyungmin Lee、June Suk Choi、Jongheon Jeong、&lt;strong>;Kihyuk Sohn&lt;/strong>;、Jinwoo Shin&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1SF2tiopYJ&quot;>;CommonScenes：使用 Scene 生成常识性 3D 室内场景图表&lt;/a>;&lt;br />;Guangyao Zhai、Evin Pınar Örnek、Shun-Cheng Wu、Yan Di、&lt;strong>;Federico Tombari&lt;/strong>;、Nassir Navab、Benjamin Busam &lt;/p>; &lt;p>; &lt;a href= &quot;https://openreview.net/forum?id=65aDEXIhih&quot;>;学习神经网络的计算复杂性：平滑性和简并性&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;、Nathan Srebro、Gal Vardi &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BopG5dhH7L&quot;>;一种计算高效的稀疏在线牛顿法&lt;/a>;&lt;br />; Fnu Devvrit*, Sai Surya Duvvuri,&lt; &lt;/strong>;Rohan Anil、&lt;strong>;Vineet Gupta&lt;/strong>;、&lt;strong>;谢卓瑞&lt;/strong>;、&lt;strong>;Inderjit S Dhillon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6EDHfVHicP&quot;>;DDF-HO：基于条件定向距离场的手持物体重建&lt;/a>;&lt;br />; 张晨阳光，严迪，张瑞达，光耀翟，&lt;strong>;Fabian Manhardt&lt;/strong>;，&lt;strong>;Federico Tombari&lt;/strong>;，纪向阳&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2nTpPxJ5Bs&quot; >;双向强盗反馈的双重拍卖&lt;/a>;&lt;br />; &lt;strong>;Soumya Basu&lt;/strong>;，Abiishek Sankararaman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs /2305.19234&quot;>;使用大型语言模型生成特定领域语言的语法提示&lt;/a>;&lt;br />;Bailin Wang, Zi Wang, Xuzhi Wang, &lt;strong>;Yuan Cao&lt;/strong>;, Rif A. Saurous, Yoon Kim &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5SIz31OGFV&quot;>;深度神经网络训练的不一致、不稳定和泛化差距&lt;/a>;&lt;br />; Rie Johnson，张桐* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3YDukx2cpr&quot;>;通过图段训练进行大图属性预测&lt;/a>;&lt;br />;曹凯迪*, &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;、&lt;strong>;Sami Abu-El-Haija&lt;/strong>;、&lt;strong>;Dustin Zelle&lt;/strong>;、&lt;strong>;周艳琪&lt;/strong>;、&lt;strong>;&lt;/ strong>;Charith Mendis*、Jure Leskovec、&lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1GxKVprbwM&quot;>;关于计算成对统计量本地差异隐私&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;、&lt;strong>;Pritish Kamath&lt;/strong>;、&lt;strong>;Ravi Kumar&lt;/strong>;、&lt;strong>;Pasin Manurangsi&lt;/strong>; , &lt;strong>;Adam Sealfon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7UdVPRmpif&quot;>;关于蒸馏中的师生偏差：不服从是否值得？ &lt;/a>;&lt;br />; &lt;strong>;Vaishnavh Nagarajan&lt;/strong>;、&lt;strong>;Aditya Krishna Menon&lt;/strong>;、&lt;strong>;Srinadh Bhojanapalli&lt;/strong>;、&lt;strong>;Hossein Mobahi&lt;/strong>;、 &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CzkOzKWpMa&quot;>;具有未知上下文分布的上下文强盗的最佳交叉学习&lt;/a >;&lt;br />; &lt;strong>;乔恩·施奈德&lt;/strong>;、&lt;strong>;朱利安·齐默特&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8XRMbNAP6Z&quot; >;滑动窗口模型中的近最优 k 聚类&lt;/a>;&lt;br />; David Woodruff、&lt;strong>;钟培林&lt;/strong>;、Samson Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/forum?id=3H37XciUEv&quot;>;语言模型的事后解释可以改进语言模型&lt;/a>;&lt;br />; Satyapriya Krishna、Jiaqi Ma、Dylan Z Slack、&lt;strong>;Asma Ghandeharioun&lt;/strong>;、 Sameer Singh，Himabindu Lakkaraju &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BJ0fQUU32w&quot;>;具有生成检索的推荐系统&lt;/a>;&lt;br />; Shashank Rajput*&lt;strong >;、&lt;/strong>;Nikhil Mehta、Anima Singh、&lt;strong>;Raghunandan Hulikal Keshavan&lt;/strong>;、&lt;strong>;Trung Vu、Lukasz Heldt&lt;/strong>;、&lt;strong>; &lt;/strong>;Lichan Hong、Yi Tay&lt;strong>; >;、Vinh Q. Tran&lt;/strong>;、&lt;strong>;Jonah Samost&lt;/strong>;、Maciej Kula、Ed H. Chi、Maheswaran Sathiamoorthy &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=8OTPepXzeh&quot;>;用于微调文本到图像扩散模型的强化学习&lt;/a>;&lt;br />; &lt;strong>;范颖&lt;/strong>;、Olivia Watkins、杜雨晴、刘浩、&lt;strong >;Moonkyung Ryu&lt;/strong>;、&lt;strong>; Craig Boutilier&lt;/strong>;、Pieter Abbeel、Mohammad Ghavamzadeh*、Kangwook Lee、Kimin Lee* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /forum?id=5VQFAvUHcd&quot;>;可复制集群&lt;/a>;&lt;br />; &lt;strong>;Hossein Esfandiari&lt;/strong>;、&lt;strong>;Amin Karbasi&lt;/strong>;、&lt;strong>;Vahab Mirrokni&lt;/strong>;、Grigoris Velegkas , Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5cPz5hrjy6&quot;>;强化学习的可复制性&lt;/a>;&lt;br />; &lt;strong>;Amin Karbasi&lt;/strong >;、Grigoris Velegkas、林杨、Felix Zhou&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=szFqlNRxeS&quot;>;黎曼投影免费在线学习&lt;/a>;&lt;br / >; Zihao Hu、Guanghui Wang、&lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=29WbraPk8U&quot;>;锐度感知最小化导致低-排名特征&lt;/a>;&lt;br />; Maksym Andriushchenko、&lt;strong>;Dara Bahri&lt;/strong>;、&lt;strong>;Hossein Mobahi&lt;/strong>;、Nicolas Flammarion &lt;/p>; &lt;p>; &lt;a href=&quot;https: //openreview.net/forum?id=2hQ7MBQApp&quot;>;什么是平坦度正则化的归纳偏差？深度矩阵分解模型研究&lt;/a>;&lt;br />;Khashayar Gatmiry、李志远、庄庆耀、&lt;strong>;Sashank Reddi&lt;/strong>;、马腾宇、Stefanie Jegelka&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JzQlGqBm8d&quot;>;具有随机优化共享基础的块低阶预处理器&lt;/a>;&lt;br />; Jui-Nan Yen、Sai Surya Duvvuri、&lt;strong>;Inderjit S Dhillon&lt;/strong>;、&lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Ntd6X7uWYF&quot;>;阻止合作匪徒：在线具有每项预算约束的协作过滤&lt;/a>;&lt;br />; &lt;strong>;Soumyabrata Pal&lt;/strong>;、&lt;strong>;Arun Sai Suggala&lt;/strong>;、&lt;strong>;Karthikeyan Shanmugam&lt;/strong>;、&lt;strong>; Prateek Jain&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NIrTSCiIZ7&quot;>;具有扩散模型的边界引导无学习语义控制&lt;/a>;&lt;br / >; 叶朱、吴宇、&lt;strong>;邓志伟&lt;/strong>;、Olga Russakovsky、严岩&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=IyYyKov0Aj&quot;>;有条件适配器：具有快速推理的参数高效迁移学习&lt;/a>;&lt;br />; 雷涛、&lt;strong>;Junwen Bai&lt;/strong>;、Siddhartha Brahma、&lt;strong>; Joshua Ainslie&lt;/strong>;、Kenton Lee、Yanqi Zhou、杜楠*、&lt;strong>;赵云&lt;/strong>;、&lt;strong>;吴跃新&lt;/strong>;、李波、&lt;strong>;张宇&lt;/strong>;、张明伟&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ktrwpwcmsc&quot;>;与现代Hopfield Networks的时间序列&lt;/a>; &lt;br />; Andreas Auer，&lt;strong>; Martin Gauch &lt;/strong>;，丹尼尔·克洛兹（Daniel Klotz），sepp hochreiter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum？ >; &lt;strong>; Chen Sun &lt;/strong>;，Calvin Luo，&lt;strong>; Xingyi Zhou &lt;/strong>;，&lt;strong>; Anurag Arnab &lt;/strong>;，&lt;strong>; Cordelia Schmid &lt;/strong>; &lt;a href=&quot;https://openreview.net/forum?id=payxfiukwy&quot;>;具有不同训练数据的模型的自然分配变化有效鲁棒性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; /strong>;，&lt;strong>; ananth balashankar &lt;/strong>;，路德维格·施密特（Ludwig Schmidt），&lt;strong>; cho-jui hsieh &lt;/strong>;，alex beutel*，&lt;strong>; yao Qin a href =“ https://openreview.net/forum?id=nh5dp6uuvx”>;使用人类相似性判断改善神经网络表示&lt;/a>; &lt;br />; &lt;br />; &lt;br />; lukas uttenthaler*，Lorenz Linhardt，Linhardt，Jonas Dippel，Jonas Dippel，Robert A. Vandermeeulen，，Robert A. Vandermeulen，，，Robert A. Vandermeulen，，，，lorenz linhardt Katherine Hermann，Andrew K. Lampinen，Simon Kornblith &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum？ /a>; &lt;br />; xiyang liu，&lt;strong>; prateek jain &lt;/strong>;，&lt;strong>; weihao kong &lt;/strong>;，&lt;strong>; sewoong oh oh &lt;/strong>;，&lt;strong>; arun sai suggala &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=fdfyga5i0a&quot;>; mnemosyne：学习用变形金刚培训变形金刚&lt;/a>; &lt;br />; &lt;br />; Deepali Jain，deepala Strong>; Avinava Dubey &lt;/strong>;，Sumeet Singh，Vikas Sindhwani，Tingnan Zhang，Jie Tan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum？对于线性土匪&lt;/a>; &lt;br />; Ayush Sawarni，&lt;strong>; soumyabrata pal &lt;/strong>;，siddharth Barman &lt;/p>; &lt;p>; &lt;a href =“ >;倒角距离的接近线性时间算法&lt;/a>; &lt;br />; Ainesh Bakshi，Piotr Indyk，&lt;strong>; Rajesh Jayaram &lt;/strong>;，Sandeep Silwal，Erik Waingarten。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=fvif8vuz5b&quot;>;从高斯和产品分发的差异私人抽样&lt;/a>; &lt;br />; &lt;br />; &lt;br />; /strong>;，xiao hu*，&lt;strong>; ravi kumar &lt;/strong>;，&lt;strong>; pasin manurangsi &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum？id =马尔可夫决策过程中静态风险度量的动态编程分解&lt;/a>; &lt;br />; &lt;br />; ：//openreview.net/forum？id = hfqfaynucq“>; resmem：了解您可以记住什么并记住其余的&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; Michal Lukasik &lt;/strong>; &lt;/strong>;，&lt;strong>; Zonglin li &lt;/strong>;，&lt;strong>; ankit Singh Rawat &lt;/strong>;，&lt;strong>; Manzil Zaheer，&lt;/strong>; &lt;strong>; Aditya Krishna Menon &lt;/strong>;，&lt;strong>; sanjiv kumar &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pcnpl9q39p&quot;>;负责AI（RAI）Games and Ensembles &lt;/a>; Gupta，Runtian Zhai，&lt;strong>; Arun Suggala &lt;/strong>;，Pradeep Ravikumar &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=dvlawv2rsi&quot;>; roboclip &quot;>; roboclip：一个示范足以学习机器人政策&lt;/a>; &lt;br />; >; &lt;a href=&quot;https://openreview.net/forum?id=i6aojhpcnq&quot;>;通过kernelized rate-distortht通过kernelized rate-distortion的最大化&lt;/a>; &lt;br />; &lt;br />; Avinava Dubey &lt;/strong>;，&lt;strong>; AMR AHMED &lt;/strong &lt;/strong>;，Snigdha Chaturvedi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=fmzvre0ge通过对抗正规化学习的强化学习：理论基础和稳定算法&lt;/a>; &lt;br />; Alexander Bukharin，Yan Li，Yue Yu，Yue Yu，Qingru Zhang，&lt;strong>; Zhehui Chen &lt;/strong>; tuo zhao &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ptvxckk0qde&quot;>; 1- hidden层神经网络中的简单偏见&lt;/a>; &lt;br />; &lt;br />; &lt;br />; Jatin Batra，&lt;strong>; Prateek Jain &lt;/strong>;，&lt;strong>; praneeth netrapalli &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2302.03806&quot;>; SLAM：学生 - 与未标记的示例蒸馏的标签混合&lt;/a>; &lt;br />; &lt;br />; vasilis kontonis，&lt;strong>; fotis iliopoulos &lt;/strong>;，&lt;strong>; khoa trinh &lt;/strong>;，&lt;strong>; cenk baykal &lt;/strong>; strong>; gaurav menghani &lt;/strong>;，&lt;strong>; erik vee &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lchmp68gtj&quot;>;视觉定位和语义理解的神经图&lt;/a>; &lt;br />; Paul-Edouard Sarlin*，&lt;strong>; Eduard Trulls &lt;/strong>;，Marc Pollefeys，&lt;strong>; Jan Hosang &lt;/strong>;，&lt;strong>; Simon Lynen &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qvivwmaqdx&quot;>; soar soar：改进索引索引，以获取大约最近的邻居搜索&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong &lt;strong >; Philip Sun &lt;/strong>;，&lt;strong>; David Simcha &lt;/strong>;，&lt;strong>; Dave Dopson &lt;/strong>;，&lt;strong>; ruiqi guo &lt;/strong>;，&lt;strong>; sanjiv kumar &lt;/strong>; &lt;/strong>; &lt;/p &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=koafh16uoc&quot;>; StysledRop：任何样式的文本对图像合成&lt;/a>; &lt;br />; &lt;br />; &lt;br />; strong>;，&lt;strong>; lu jiang &lt;/strong>;，&lt;strong>; Jarred Barber &lt;/strong>;，Kimin Lee*，&lt;strong>; nataniel ruiz &lt;/strong>;，&lt;strong>; dialip krishnan &lt;/strong>;，huiwen chang* ，&lt;strong>; yuanzhen li &lt;/strong>;，&lt;strong>; irfan essa &lt;/strong>;，&lt;strong>;迈克尔·鲁宾斯坦&lt;/strong>;，&lt;strong>; yuan hao &lt;/strong>; ，&lt;strong>; Irina Blok &lt;/strong>;，&lt;strong>; Daniel Castro Chin &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lsyqb4cwd3&quot;>;三座：通过预审前的图像模型&lt;/a>; &lt;br />; Jannik Kossen*，&lt;strong>; Mark Collier &lt;/strong>;，Basil Mustafa，Xiao Wang，Xiaohua Zhai，Lucas Beyer，Andreas Steiner，&lt;strong>; Jesse Berent，&lt;strong>; Jesse Berent，在>;两个阶段学习与多位专家推迟&lt;/a>; &lt;br />; Anqi Mao，Christopher Mohri，&lt;strong>; Mehryar Mohri &lt;/strong>;，Yutao Zhong Zhong &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ href =” https：https：https：https： ///openreview.net/forum?id=zbzywp2gpl&quot;>; adanns：自适应语义搜索的框架&lt;/a>; &lt;br />; Aniket Rege，&lt;strong>; aditya kusupati &lt;/strong>;，Sharan Ranjit S，Alan Fan，Alan Fan，Qingqing cao，sham kakade，&lt;strong>; prateek jain &lt;/strong>;，ali farhadi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=srt1hhqgqa&quot;>;带有小得分手的多任务LM &lt;/a>; &lt;br />; bowen tan*，&lt;strong>; yun Zhu &lt;/strong>;，&lt;strong>; lijuan liuu &lt;/strong &lt;/strong &lt;/strong>;，eric xing，zh​​iting hu，&lt;strong>; jindong Chen &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=djz3mvdw86&quot;>;因果关系驱动的文本驱动的文本增强&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Strong>; Amir Feder &lt;/strong>;，Yoav Wald，Claudia Shi，Susti Saria，David Blei &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=s0xrbmfihs&quot;>;随机特征：高斯内核的正面估计量&lt;/a>; &lt;br />; &lt;br />; valerii likosherstov，krzysztof choromanski，&lt;strong>; avinava dubey &lt;/strong>;，&lt;strong>; frederick liu &lt;/strong>; /strong>;，Adrian Weller &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vm1zeyqwdc&quot;>; dizfusion hyperfeatures：在时间和空间中寻找语义通信&lt;/a>; &lt;br &lt;/a>; &lt;br />; Grace Luo，Lisa Dunlap，Dong Huk Park，&lt;strong>; Aleksander Holynski &lt;/strong>;，Trevor Darrell &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum？ >;可控图像生成的扩散自我诱导&lt;/a>; &lt;br />; &lt;strong>; dave Epstein &lt;/strong>;，Allan Jabri，&lt;strong>; Ben Poole &lt;/strong>;，Alexei a efros，&lt;strong>; Aleksander holynski &lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dngepkmnzo&quot;>;完全动态k-c群集在（k）更新时间&lt;/a>; &lt;br />; &lt;br />; Sayan Bhattacharya，Martin Nicolas Costa，&lt;strong>; Silvio Lattanzi &lt;/strong>;，&lt;strong>; Nikos parostidis &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ svjdiivysh“>;通过语言重写改进剪辑培训&lt;/a>; &lt;br />; &lt;strong>; lijie fan &lt;/strong>;，&lt;strong>; dilip krishnan &lt;/strong>;，Phillip Isela，Dina Katabi，&lt;strong>; strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uzuhikacms&quot;>; k-means聚集与远距离的隐私&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Epasto &lt;/strong>;，&lt;strong>; vahab mirrokni &lt;/strong>;，shyam narayanan，&lt;strong>; peilin zhong &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum？ id = xu8ag5q8m3“>; layoutgpt：大型语言模型的组成视觉计划和发电&lt;/a>; &lt;br />; &lt;br />; weixi feng，wanrong zhu，tsu-jui fu，&lt;strong>; varun jampani &lt;/strong>;，&lt;strong>; varun jampani &lt;/strong>; Akula &lt;/strong>;，Xuehai He，&lt;strong>; Sugato Basu &lt;/strong>;，Xin Eric Wang，William Yang Wang &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net.net/forum?id sxxn3kntsv“>;脱机加固学习，用于混合专家对话管理&lt;/a>; &lt;br />; &lt;br />; dhawal gupta*，&lt;strong>; yinlam chow &lt;/strong>;，&lt;strong>; azamat tulepbergenov &lt;/strong>; &lt;strong>; craig boutilier &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ag6xop9qy7&quot;>;最佳无偏随机化因素用于带有标签差异隐私的回归&lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; br/>; &lt;strong>; ashwinkumar badanidiyuru &lt;/strong>;，&lt;strong>; badih ghazi &lt;/strong>;，&lt;strong>; pritish kamath &lt;/strong>;，&lt;strong>; ravi kumar &lt;/strong>;，&lt;strong>; Ethan Jacob Leeman &lt; /strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; pasin manurangsi &lt;/strong>;，&lt;strong>; avinash v varadarajan &lt;/strong>;，&lt;strong>; chiyuan zhang zhang &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=wbfhfvjjkj”>; paraphrasing Exaphrasing逃避了AI生成的文本的检测器，但检索是有效的防御&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; ，Yixiao Song，Marzena Karpinska，John Wieting，Mohit Iyyer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=samrn9tnxe&quot;>; remax：remax：为有效的全次综合汇总培训&lt;remax： /a>; &lt;br />; Shuyang Sun*，&lt;strong>; Weijun Wang &lt;/strong>;，Qihang Yu*，&lt;strong>; Andrew Howard &lt;/strong>;，Philip Torr，Liang-Chieh Chen*&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id= sourowc5un&quot;>;稳健而积极的无服务器协作学习&lt;/a>; &lt;br />; &lt;br />; Nicholas Franzese，Adam dziedzic，&lt;strong>; Christopher A. Christopher A. Christopher A. choquette-Chooth A.Choquette-Choo &lt;/strong>;，马克·R·托马斯（Mark R. /openreview.net/forum?id= sdyhltcc5j&quot;>; prectr：通过最佳传输快速投机解码&lt;/a>; &lt;br />; &lt;br />; &lt;brong>; &lt;strong>; Ziteng Sun &lt;/strong>;，&lt;strong>; ananda theertha theertha suresh &lt;/stranda suresh &lt;/strong &lt;/strong>;，&lt;/strong>;，&lt;/>; strong>; jae hun ro &lt;/strong>;，&lt;strong>; ahmad beirami &lt;/strong>;，&lt;strong>; Himanshu Jain &lt;/strong>;，&lt;strong>; felix yu &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href = “ https://openreview.net/forum?id=yz7ip645ra&quot;>;结构性预测具有更强的一致性保证&lt;/a>; &lt;br />; &lt;br />; &lt;br/>; Anqi Mao，&lt;strong>; Mehryar Mohri &lt;/strong>; p>; &lt;a href=&quot;https://openreview.net/forum?id=qgig7wzohz&quot;>; affinity-ware图形网络&lt;/a>; &lt;br />; &lt;br />; Sinop &lt;/strong>;，IraKtena，PetarVeličković，&lt;strong>; sreenivas gollapudi &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum？从嘈杂的Web图像集合中学习强大的表达3D形状&lt;/a>; &lt;br />; Chun-Han Yao*，&lt;strong>; Amit Raj &lt;/strong>;，Wei-Chih Hung，&lt;strong>; Yuanzhen li &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>; Strong>; Michael Rubinstein &lt;/strong>;，&lt;strong>; ming-hsuan Yang &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; ：//openreview.net/forum？id = eodnah3pfb“>;交互式ml &lt;/a>; &lt;br />; &lt;br />; &lt;strong>; haim kaplan &lt;/strong>;，&lt;strong>; yishay mansour &lt;/ashay mansour &lt;/strong>;， &lt;strong>; Shay Moran &lt;/strong>;，Kobbi Nissim，&lt;strong>; uri stemmer &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=orh4e0ao9r模拟器：近乎最佳的对抗线性上下文匪徒&lt;/a>; &lt;br />; haolin liu，chen-yu wei，&lt;strong>; julian julian Zimmert &lt;/strong>; &lt;/pr>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href = a href =“ /openreview.net/forum？id = kxoxrvnwbb&quot;>; dataseg：驯服通用的多数据固定多任务分段模型&lt;/a>; &lt;br />; &lt;br />; Jonathan Huang &lt;/strong>;，&lt;strong>; Abdullah Rashwan &lt;/strong>;，&lt;strong>; Xuan Yang &lt;/strong>;，&lt;strong>; Xingyi Zhou &lt;/strong>;，&lt;strong>; golnaz ghiasi &lt;/strong>;，&lt;strong>; Weicheng Kuo &lt;/strong>;，&lt;strong>; Huizhong Chen &lt;/strong>;，Liang-Chieh Chen*，&lt;strong>; David Ross &lt;/strong>; &lt;/pross &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“网络/论坛？ >; claudio gentile &lt;/strong &lt;/strong>;，&lt;strong>;安德烈斯·穆纳兹·麦迪纳（Andres Munoz Medina）&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q3fcwoc9l0&quot;>;有效的数据子集选择跨模型的概括：跨性和电感网络&lt;/a>; &lt;br />; &lt;br />; eeshaan Jain，Tushar Nandy，&lt;strong>; Gaurav aggarwal &lt;/strong>;，&lt;strong>; Ashish Tendulkar &lt;/strong>;，Rishabh Iyer，Abir de &lt;/abir de &lt;/abir de &lt;/abir de &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h2lkx9sqcd&quot;>;通过二阶方法更快地差异私人凸优化&lt;/a>; &lt;br />; &lt;br />; &lt;br />; Strong>;，Mahdi Haghifam*，Thomas Steinke，Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=gdvcfovxt3&quot;>; A>; &lt;br />; Lee Cohen，&lt;strong>; Yishay Mansour &lt;/strong>;，Michal Moshkovitz &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=s1fjxzj0jy>; ：上下文扩展的对比培训&lt;/a>; &lt;br />; szymon tworkowski，konrad staniszewski，mikołajpacek，yuhuai wu*，henryk michalewski，piotrmiłośmimiłoś&lt;/p>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a a href =网络/论坛？ &lt;a href=&quot;https://openreview.net/forum?id=ni77emxq2pl&quot;>; h-consististency bounds：特征和扩展Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kjmghto8cs&quot;>;逆动力学预处理学习多任务模仿&lt;/a>; &lt;br />; &lt;br />; &lt;br />; >; ofir nachum &lt;/strong>;，琼·布鲁纳（Joan Bruna）&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvpujuvjqd&quot;>;大多数神经网络几乎是可学习的&lt;/a>; &lt;/a>; &lt;/a>; &lt;br/ >; &lt;strong>; Amit Daniely &lt;/strong>;，Nathan Srebro，gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nq84yyy9iut&quot;>;多类的增强学习标准&lt;/a>; &lt;br />; a href =“ https://openreview.net/forum?id=gjhat79czu”>; nerf重新访问：固定数量渲染的正交不稳定&lt;/a>; &lt;br/>; mikaela angelina uy，nakayama，kiyohiro nakayama，guandao yankayama，guandao yangul yang yangul yangul krishna thomas， leonidas guibas，&lt;strong>; ke li &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iznfcahjk0&quot;>;通过压缩的隐私放大 - 分布式平均值估计中的交流权衡取舍&lt;/a>; &lt;br />; &lt;br />; we-ning Chen，Dan Song，Ayfer Ozgur，&lt;strong>; Peter Kairouz &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https ：//openreview.net/forum？id = rzdboh1tbh“>;私人联合频率估计：适应实例的硬度&lt;/a>; &lt;/a>; &lt;br />; jingfeng wu*，&lt;strong>; wennan zhu &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>; >; Peter Kairouz &lt;/strong>;，vladimir braverman &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvlc0remkq&quot;>; retvec：retvec：弹性和有效的文本矢量/>; &lt;strong>; Elie Bursztein &lt;/strong>;，&lt;strong>; Marina Zhang &lt;/strong>;，&lt;strong>; Owen Skipper Vallis &lt;/strong>;，&lt;strong>; Xinyu Jia &lt;/strong>;，&lt;strong>; Alexey Kurakin &lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ne6zeqlfcz&quot;>;象征性发现优化算法的符号发现&lt;/a>; &lt;br />; &lt;br />; xiangning chen chen*，&lt;strong>; chen chen liang &lt;/strong>;，&lt;strong>; da huang &lt;/strong>;，&lt;strong>; esteban Real &lt;/strong>;，&lt;strong>; kaiyuan wang &lt;/strong>;，&lt;strong>; hieu pham &lt;/strong &lt;/strong>;，&lt;strong>; xuanyi dong &lt;/strong>;，&lt;strong>; thang luong &lt;/strong>;，cho-jui hsieh，&lt;strong>; yifeng lu &lt;/strong>;，&lt;strong>; quoc V. le &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;p>; &lt; a href =“ https://openreview.net/forum?id=lds9d17hrd”>;一个两个特征的故事：稳定的扩散补充DINO为零拍的语义通信&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; Herrmann &lt;/strong>;，&lt;strong>; Junhwa hur &lt;/strong>;，&lt;strong>; Luisa F. Polania &lt;/strong>;，&lt;strong>; Varun Jampani &lt;/strong>;，&lt;strong>; deqing Sun &lt;/strong>;，&lt;strong >; ming-hsuan yang &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iSd8g75qvp&quot;>;用于跨传播在线学习的三分法&lt;/a>; &lt;br />; Steve Hanneke，&lt;strong>; Shay Moran &lt;/strong>;，Jonathan Shafer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=i2h2seiq2t&quot;>;一个统一的统一快速梯度剪切框架DP -sgd &lt;/a>; &lt;br />; &lt;strong>; William Kong &lt;/strong>;，&lt;strong>; Andres Munoz Medina &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/论坛？ &lt;/strong>;，&lt;strong>; h。 Brendan McMahan &lt;/strong>;，&lt;strong>; alina oprea &lt;/strong>;，&lt;strong>; sewoong oh oh &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =“ https:///openreview.net/forum?id= ZEM6HF97PZ“>;（放大）带矩阵分解：一种统一的私人训练方法&lt;/a>; &lt;br/>; Christopher A Choquette-Choo，&lt;strong>; Arun Ganesh &lt;/strong>;，&lt;strong>; Ryan McKenna &lt;/strong>;， &lt;strong>; H Brendan McMahan &lt;/strong>;，&lt;strong>; Keith Rush &lt;/strong>;，&lt;strong>; &lt;/strong>; abhradeep guha thakurta，&lt;strong>; Zheng Xu Xu &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=xcghx9fdxm”>;通过弃权的顺序预测中的对抗性弹性&lt;/a>; &lt;br />; &lt;br />; shetty &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=utlkuam68h&quot;>;交替的梯度下降和混合物的综合多模式感知&lt;/a>; &lt;br />; &lt;br />; &lt;br />; strong>; hassan akbari &lt;/strong>;，&lt;strong>; dan kondratyuk &lt;/strong>;，&lt;strong>; yin cui &lt;/strong>;，&lt;strong>; rachel hornung &lt;/strong>;，&lt;strong>; huisheng wang &lt;/strong>;，&lt; Strong>; Hartwig Adam &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf/0083ab45a583fe1992b3c4f92222081c50b922081c50b9d30c30d30c3.pdf&quot;>; android in the Field：a glod-scale dateforce &lt; /a>; &lt;br />; &lt;strong>; Christopher Rawles &lt;/strong>;，&lt;strong>; Alice Li &lt;/strong>;，&lt;strong>; Daniel Rodriguez &lt;/strong>;，&lt;strong>; Oriana Riva &lt;/strong>;，Timothy Lillicrap &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=cirhwarbp0&quot;>;基于对抗性图像obfusccations的鲁棒性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; strong>;，&lt;strong>; chun-ta lu &lt;/strong>;，&lt;strong>; hussein hazimeh &lt;/strong>;，&lt;strong>; otilia stretcu &lt;/strong>;，&lt;strong>; wei qiao &lt;/strong &lt;/strong>;，&lt;strong>; yintao liu &lt;/strong>;，&lt;strong>; merve kaya &lt;/strong>;，&lt;strong>; Cyrus rashtchian &lt;/strong>;，&lt;strong>; ariel fuxman &lt;/strong>;，&lt;strong>; mehmet tek &lt;/strong>;，sven gowal &lt;/p &lt;/p >; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uijjdc8k6&quot;>;通过社区参与建立社会culturally-cultury-culturally-Culturomentimentiment-Cultife Inderipements Imcorials &lt;/a>; &lt;br />; &lt;br />; &lt;br />; >;，Jaya Goyal，&lt;strong>; Dinesh Tewari &lt;/strong>;，&lt;strong>; Shachi Dave &lt;/strong>;，&lt;strong>; vinodkumar prabhakaran &lt;/strong>; &lt;/prabhakaran &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href = a href = a href =“ https：// https：// https：// OpenReview.net/forum?id=l9i9fhhfs3&quot;>; consensus和Ml Fairness的肤色注释的主观性&lt;/a>; &lt;br />; &lt;br />; &lt;brand>; Candice Schumann &lt;/strong>; &lt;strong>; Auriel Wright &lt;/strong>;，Ellis Monk Jr*，&lt;strong>; Courtney Heldreth &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; susanna ricco &lt;/strong>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=zdli6oxpwd”>;计算人级差异隐私下的不同元素>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gjnvvswsweld &quot;>; dices数据集：安全性AI安全性的多样性&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; &lt;strong>; >;，Alex S. Taylor，&lt;strong>; Mark Diaz &lt;/strong>;，&lt;strong>; Christopher M.Homan &lt;/strong>;，&lt;strong>; Alicia Parrish &lt;/strong>;，Greg Serapio-García，&lt;strong>; vinodkumar prabhakaran &lt; /strong>;，&lt;strong>; ding wang &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ss3ck3yx5z&quot;>;在Imagenet转移到现实世界数据集上确实会进展？&lt;/a>; &lt;br />; Alex Fang，&lt;strong>; Simon Kornblith &lt;/strong>;，Ludwig Schmidt &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id=Aaa2uoo0hhmr” >;从2D注释中估算通用3D房间结构&lt;/a>; &lt;br />; &lt;br />; Denys Rozumnyi*，&lt;strong>; Stefan Popov &lt;/strong>;，&lt;strong>; kevis-kokitsi Maninis &lt;/strong>;，MatthiasNießnieous，&lt;strong>; vittorio法拉利&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6hzifay9gd&quot;>;大语言模型作为属性培训数据生成器：多样性和偏见的故事&lt;/a >; &lt;br />; Yue Yu，Yuchen Zhuang，Jieyu Zhang，Yu Meng，Alexander Ratner，Ranjay Krishna，&lt;strong>; Jiaming Shen &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;/strong>; &lt;/strong>; a href =“ https://openreview.net/forum?id=y45zcxslfx”>; madlad-400：多语言和文档级的大型审核数据集&lt;/a>; &lt;br />; Strong>;，Biao Zhang，Xavier Garcia，Derrick Xin，&lt;strong>; Aditya Kusupati &lt;/strong>;，Romi Stella，Ankur Bapna，Orhan Firat &lt;/p>; &lt;p>; &lt;p>; &lt;a href = &lt;a href =“ https://openreview.net/论坛？ //openreview.net/forum?id=8tmHs2pifg&quot;>; navi：具有高质量3D形状和姿势注释的类别-Agnostic图像集&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;strong>; Kevis-Kokitsi Maninis &lt;/strong>;，&lt;strong>; Andreas Engelhardt &lt;/strong>;，&lt;strong>; arjun karpur &lt;/strong>;，&lt;strong>; karen truong &lt;/strong>;，&lt;strong>; kyle sargent &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt; Strong>; Stefan Popov &lt;/strong>;，&lt;strong>; Andre Araujo &lt;/strong>;，&lt;strong>; Ricardo Martin Brualla &lt;/strong>;，&lt;strong>; kaushal Patel &lt;/strong>;，&lt;strong>; daniel vlasic &lt;/strong>;， &lt;strong>; Vittorio Ferrari &lt;/strong>;，&lt;strong>; Ameesh Makadia &lt;/strong>;，Ce Liu*，&lt;strong>; Yuanzhen li &lt;/strong>;，&lt;strong>; &lt;a href=&quot;https://openreview.net/forum?id=x6cocxrnxg&quot;>;神经理想大型涡流模拟：用神经随机微分方程建模湍流&lt;/a>; &lt;br />; &lt;br />; &lt;br />; ，&lt;strong>; Zhong yi Wan &lt;/strong>;，&lt;strong>; leonardo Zepeda-Nunez &lt;/strong>;，&lt;strong>; James Lottes &lt;/strong>;，&lt;strong>; Qing Wang wang &lt;/strong>;，&lt;strong>; yi-fan陈&lt;/strong>;，&lt;strong>;约翰·罗伯茨·安德森（John Roberts Anderson）&lt;/strong>;，&lt;strong>; fei sha &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&#39;https://openreview.net/forum?id= wfuemocyhz”>;重新开始采样以改善生成过程&lt;/a>; &lt;br />; yilun Xu，Mingyang Deng，Xiang Cheng，&lt;strong>; Yonglong Tian &lt;/strong>;，Ziming Liu，Tommi Jaakkola href =“ https://openreview.net/forum?id=xuybp16q5j”>;推荐系统中的重新思考激励措施：单调奖励总是有益的&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; fan yao，chuanhao li，chuanhao li，karthik abinav sankararararaman，yiming liaiao ，&lt;strong>; Yan Zhu &lt;/strong>;，Qifan Wang，Hongning Wang，Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jgymuum？语义分割：通过联合上的细粒交叉点的优化和评估&lt;/a>; &lt;br />; &lt;br />; Zifu Wang，&lt;strong>; Maxim Berman &lt;/strong>;，Amal Rannen-Triki，Philip Torr，Philip Torr，Devis Tuia，Tinne Tuia，Tinne Tuytelaars，Luc Van Gool，Jiqian Yu，Matthew B. Blaschko &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum？ />; &lt;strong>; Vikash Kumar &lt;/strong>;，Rutav Shah，Gaoyue Zhou，Vincent Moens，Vittorio Caggiano，Abhishek Gupta，&lt;strong>; Aravind Rajeswaran &lt;/pravind rajeswaran &lt;/prong>; &lt;/p>; &lt;/p>; &lt;/p>; &lt;p>; &lt;p>; &lt;p>; &lt;p>; &lt;p>; &lt;a href =“ //openreview.net/forum?id=vn5qzgxgj3&quot;>; satbird：鸟类物种分布建模与遥感和公民科学数据&lt;/a>; &lt;br />;MélisandeTeng，Amna Elmustafa，Benjamin Akera，Benjamin akera，Yoshua Bengio，Hager Radi，Hager Radi，Hager Radi，Hager Radi，&lt; Strong>; Hugo Larochelle &lt;/strong>;，David Rolnick &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sqtccxkg4p&quot;>; Sparsity-Parsity-Parsity-Parsity-Parsity-Parsity-Parsity-Parsity-Parsity-Parsity-Parsity在大型嵌入模型的差异化私有培训&lt;/ a>; &lt;br />; &lt;strong>; badih ghazi &lt;/strong>;，yangsibo huang*，&lt;strong>; pritish kamath &lt;/strong>;，&lt;strong>; ravi kumar &lt;/strong>;，&lt;strong>; pasin manurangsi &lt;/strong>;， &lt;strong>; Amer Sinha &lt;/strong>;，&lt;strong>; &lt;/strong>; &lt;strong>; chiyuan Zhang &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?id= XPJSOQTKQX“>; stableRep：文本到图像模型的合成图像使强大的视觉表示学习者&lt;/a>; &lt;br />; &lt;br />; &lt;strong>; yonglong tian &lt;/strong>;，&lt;strong>; lijie fan &lt;/strong>;，phillip Isela，Phillip Isela，Phillip Isela， &lt;strong>; huiwen chang &lt;/strong>;，&lt;strong>; dialip krishnan &lt;/strong>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=epz1dcdpve&quot;>; to to fied froed froed froed froed fiederated folderate ：用于群体结构学习的可扩展数据集管道&lt;/a>; &lt;br />; &lt;strong>; Zachary Charles &lt;/strong>;，&lt;strong>; Nicole Mitchell &lt;/strong>;，&lt;strong>; Krishna Pillutla &lt;/strong>;，&lt;strong>; Michael Reneer &lt;/strong>;，&lt;strong>; Zachary Garrett &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zwxkyyw9ik&quot;>;普遍性和及时调整&lt;&lt;&lt;&lt;&lt;&lt; /a>; &lt;br />; Yihan Wang，Jatin Chauhan，Wei Wang，&lt;strong>; cho-jui hsieh &lt;/strong>; &lt;/p>; &lt;/p>; &lt;p>; &lt;a href =“ https://openreview.net/forum?idiid = sovxuzpzln“>;使用稳定扩散&lt;/a>; &lt;br />; Eric Hedlin，Gopal Sharma，Shweta Mahajan，&lt;strong>; Hossam Isack &lt;/strong>;，&lt;strong>; abhishek kar kar &lt;/strong>;，&lt;strong>; Andrea tagliasacchi &lt;/strong>;，&lt;strong>; &lt;/strong>; kwang moo yi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net.net/forum?id=qedjxv9oyy“>; youtube-asl - 规模，开放域美国手语 - 英语平行语料库&lt;/a>; &lt;br />; &lt;strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=swntr6vgqg&quot;>;线性回归中的噪声水平与相关数据&lt;/a>; &lt;br/>; Ingvar Ziemann，&lt;brong>; Stephen Tu &lt;/strong>;，George J. Pappas，Nikolai Matni &lt;/p>; &lt;/div>; &lt;！ - 脚注 - >; &lt;hr width =“ 80％”/>; &lt;p>; &lt;span class =“苹果 - style-span“ style =”字体大小：small;“>; &lt;b>;*&lt;/b>;＆nbsp;在Google上完成的工作/blog.research.google/feeds/82223613466421639113/comments/default“ rel =” rel =“ requeies” title =“ post comment” type =“ application =” application/atom+xml“/>; &lt;link href href href =” http：//blog..research。 Google/2023/12/google-at-neurips-2023.html＃评论式“ rel =” reply =“ revers” title =“ 0注释” type =“ text/html”/>; &lt;link href =“ http：// http：// .blogger.com/feeds/8474926331452026626/posts/posts/default/82223613466421639113“ rel =“ edit”帖子/默认/8223613466421639113“ rel =“ self” type =“ application/application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2023/2023/2023/12/google-at-neurips-AT-Neurips-AT-Neurips-20233.html “ rel =“备用” title =“ google at neurips 2023” type =“ text/html”/>; &lt;aunder>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/ 120986265147775266161 &lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/email>; &lt;gd：image Height =“ 16” //img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/wurs>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blognger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s72-c/google_research_sticker-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;在2023-12-08T10：34：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-01-08T07：50：55.303-08：00 &lt;/updateated &lt;/divate &lt;categorated>; &lt;category schemit =“ http：//www.blogger。 com/atom/ns＃“ term =“算法”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” scheme =“ http://www.blogger.com/atom/ns#” term =“ dindialial私密”>; &lt;/category>; &lt;title type =“ text”>; sparsity-sparsity-pressity-pressity pressity privation private Training &lt;/stitle>; &lt;content>; &lt;content类型=“ html”>; &lt;span class =“ byline-author”>;由研究实习生扬西波黄（Yangsibo Huang）和研究科学家Chiyuan Zhang发表&lt;/span>; &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s1600/Sparse%20DP-SGD.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;em>;大型嵌入模型&lt;/em>;已成为推荐系统各种应用的基本工具[&lt;a href=&quot;https://research.google/pubs/pubs/pub45530/&quot;>; 1 &lt;/>; a>;，&lt;a href=&quot;https://arxiv.org/abs/2104.05158&quot;>; 2 &lt;/a>;] 3 &lt;/a>;，&lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;>; 4 &lt;/a>;，&lt;a href =“ https://arxiv.org/abs/1810.048055 “>; 5 &lt;/a>;]。这样的模型可以通过映射分类或&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/string_(computer_science)&quot;>; string &lt;/a>;  -  valued Input属性来使非数字数据将非数字数据集成到深度学习模型中使用嵌入层的大词汇到固定长度表示向量。这些模型被广泛部署在个性化的推荐系统中，并在语言任务中实现最先进的性能，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/language_model&quot;>;语言建模&lt;/a >;，&lt;a href=&quot;https://en.wikipedia.org/wiki/sentiment_analysis&quot;>;情感分析&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wikipedia.org/wiki/wiki/wiki/question_answering&quot;>;问题回答&lt;/a>;。在许多这样的情况下，在部署这些模型时，隐私是同样重要的功能。结果，已经提出了各种技术来实现私人数据分析。其中，&lt;a href=&quot;https://en.wikipedia.org/wiki/differential_privacy&quot;>;差异隐私&lt;/a>;（dp）是一个广泛采用的定义，它限制了个人用户信息的暴露，同时仍允许进行分析，人口级别的模式。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;用于培训DP保证的深神经网络，最广泛使用的算法是&lt;a href =“ https://arxiv.org/abs/1607.00133 “>; dp-sgd &lt;/a>;（dp &lt;a href=&quot;https://en.wikipedia.org/wiki/stochastic_gradient_descent&quot;>;随机梯度下降&lt;/a>;）。 DP-SGD的一个关键组成部分是将&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/gaussian_noise&quot;>;高斯噪声&lt;/a>;添加到训练期间梯度向量的每个坐标。但是，当应用于&lt;em>;大型嵌入模型&lt;/em>;时，这会产生可伸缩性的挑战，因为它们依靠梯度稀疏性来进行有效的训练，但是为所有坐标增加了噪声。 &lt;/p>; &lt;p>;减轻了这个梯度稀疏问题，在“ &lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>; sparsity-preseral对大型嵌入模型的私人私有培训&lt;/a>;”（要在&lt;a href=&quot;https://nips.cc/conferences/2023&quot;>; Neurips 2023 &lt;/a>;上介绍，我们提出了一种称为&lt;em>;自适应过滤的稀疏培训&lt;/em>;的新算法（（ DP-Adafest）。在高水平上，该算法仅选择在每次迭代时添加噪声的特征行来维持梯度的稀疏性。关键是要在私密的情况下进行差异化的选择，以便在隐私成本，培训效率和模型实用程序中实现三向余额。 Our empirical evaluation shows that DP-AdaFEST achieves a substantially sparser gradient, with a reduction in gradient size of over 10&lt;sup>;5&lt;/sup>;X compared to the dense gradient produced by standard DP-SGD, while maintaining comparable levels of accuracy 。这种梯度尺寸减小可以转化为20倍壁锁定时间的改进。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;概述&lt;/h2>; &lt;p>;，以更好地理解我们对梯度稀疏问题的挑战和解决方案，让我们概述DP-SGD在培训期间的工作方式。如下图所示，DP-SGD通过将样品当前随机子集中的每个示例的梯度贡献（称为迷你批次）中的每个示例夹住，并添加坐标&lt;a href =“ https：// en。 wikipedia.org/wiki/gaussian_noise&quot;>; gaussian噪声&lt;/a>;在&lt;a href=&quot;https://en.wikipedia.org/wiki/stochastical/stochastic_gradient_descent &quot;>; stochastic渐变的渐变梯度&quot;>; stochacrastic Descent &quot;>; sentochastic promentient quentient doustenter &quot;>; sentochastion &lt;/a>; （SGD）。 DP-SGD已证明了其在保护用户隐私的有效性的同时，同时在各种应用程序中保持模型实用性[&lt;a href=&quot;https://arxiv.org/pdf/2204.13650.pdf&quot;>; 6 &lt;/a>;，&lt;a href =“ https://arxiv.org/pdf/2110.06500.pdf”>; 7 &lt;/a>;]。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s1999/image5.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 768” data-Original-width =“ 1999” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “文本合格：中心;”>; DP-SGD的工作方式的例证。在每个训练步骤中，对一个小批量的示例进行采样，并用于计算每个示例梯度。这些梯度通过剪接，聚合和高斯噪声的汇总来处理最终的私有化梯度。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;将DP-SGD应用于大型嵌入模型的挑战主要来自1）用户/产品ID和类别等非数字特征字段，以及2）单词和令牌通过嵌入层转换为密集的向量。由于这些特征的词汇大小，该过程需要大量参数的大嵌入表。与参数的数量相反，梯度更新通常非常稀疏，因为每个小批量的示例仅激活一小部分嵌入行（以下图可视化零值坐标的比率，即刺激性，稀疏性，的比例各个批量的梯度）。这种稀疏性很大程度上利用了有效处理大规模嵌入培训的工业应用。例如，&lt;a href=&quot;https://cloud.google.com/tpu&quot;>; Google cloud tpus &lt;/a>;，为培训和推断大型AI模型的custic-designed AI加速器具有&lt;href &lt;a href =“ https://cloud.google.com/blog/topics/developers-practitioners/building-large-scale-scale-recommenders-using-cloud-tpus”>; detticated apis &lt;/a>; &lt;/a>;可以处理具有稀疏更新的大型嵌入。与GPU的培训相比，这导致&lt;a href=&quot;https://eng.snap.com/training-models-with-tpus&quot;>;显着改善了培训吞吐量&lt;/a>;用于稀疏的嵌入查找。另一方面，DP-SGD完全破坏了梯度的稀疏性，因为它需要将独立的高斯噪声添加到&lt;em>; ahl &lt;/em>;的坐标中。这为大型嵌入模型的私人培训创造了一个路障，因为与非私人培训相比，训练效率将大大降低。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s1814/image3.png&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 1058” data-Original-width =“ 1814” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &lt;a href =&#39;http://labs.criteo.com/2014/02/kaggle-display-display-display-dervertis--challenge--clate-display-challenge--clite-criteo.com/kaggle-display-challenge-display-challenge-display-challenge-display-challenge- -dataset“>; criteo pctr &lt;/a>;模型（见下文）。该图报告了梯度稀疏性，平均超过50个更新步骤，具有最多的桶数量的前五名分类特征（总计26个），以及所有类别功能的稀疏性。随着越来越多的示例在嵌入式表中击中更多的行，散布的梯度随批量的大小而降低，从而产生了非零梯度。但是，即使对于非常大的批量尺寸，稀疏度也高于0.97。对于所有五个功能，始终观察到这种模式。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;/div>; &lt; H2>;算法&lt;/h2>; &lt;p>;我们的算法是通过扩展标准DP-SGD构建的，每次迭代时都有额外的机制来私下选择“热功能”，这是当前当前多个训练示例激活的功能小批量。如下所示，该机制在几个步骤中起作用：&lt;/p>; &lt;ol>; &lt;li>;计算为每个特征存储桶贡献了多少个示例（我们将分类特征的每个可能值称为“桶”）。 &lt;/li>; &lt;li>;通过剪辑计数来限制每个示例的总贡献。 &lt;/li>; &lt;li>;将高斯噪声添加到每个特征存储桶的贡献计数中。 &lt;/li>; &lt;li>;仅选择要包含在梯度更新中的功能，该功能的计数高于给定的阈值（稀疏控制参数），从而保持稀疏性。这种机制是私人的，可以通过与标准的DP-SGD迭代组合来轻松计算隐私成本。 &lt;/li>; &lt;/ol>; &lt;table align =“中心” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container”样式=“ margin-Left：auto; auto; margin-right：auto; auto;&#39;&#39; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s1600 /image4.png“ style =”边距 - 左左右：自动; margin-right：auto;“>; &lt;img border =“ 0” data-Original-height =“ 743” data-Original-width =“ 1600” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption”样式=“ text-align：center;”>;在具有20个存储桶的合成分类特征上算法的过程的说明。我们计算为每个存储桶造成的示例数量，根据每个示例总贡献（包括其他功能）调整值更新。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;理论动机&lt;/h2>; &lt; p>;我们提供了基于DP-Adafest的理论动机，它通过使用随机&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/oracle_complexity_（optimization_（optimization）将其视为优化。理论环境中随机梯度下降的标准分析将模型的测试误差分解为&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/bias%E2%80%93variance_tradeoff“rias术语&lt;/a>;。 DP-Adafest的优势可以看作是减少偏差的成本减少差异。这是因为与DP-SGD相比，DP-Adafest为较小的坐标集增加了噪声，这为所有坐标增加了噪声。另一方面，DP-Adafest对梯度引入了一些偏见，因为嵌入特征上的梯度以某种概率删除。我们将有兴趣的读者推荐给&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>; paper &lt;/a>;的第3.4节。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>;我们通过大型嵌入模型应用程序评估算法的有效性，在公共数据集上，包括一个广告预测数据集（&lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-andvertis--challenge-challenge-dataset&quot;>; criteo-kaggle &lt;/a>;）和一个语言理解数据集（&lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>; sst-2 &lt;/a>;）。我们使用&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;带指数选择的DP-SGD &lt;/a>;作为基线比较。 &lt;/p>; &lt;p>; DP-Adafest的有效性在下图中很明显，在下面的图中，它的梯度尺寸降低（即，梯度稀疏性）明显高于基线，同时保持相同水平降解）。 &lt;/p>; &lt;p>;具体来说，在Criteo-Kaggle数据集上，DP-Adafest将常规DP-SGD的梯度计算成本降低了超过5x10 &lt;sup>; 5 &lt;/sup>; 5 &lt;/sup>;，同时保持可比的&lt;a href = a href = “ https://en.wikipedia.org/wiki/receiver_operating_characteristic#area_under_the_curve&quot;>; auc &lt;/a>;（我们定义为损失小于0.005）。这种减少转化为更有效，更具成本效益的培训过程。相比之下，如下绿线所示，基线方法无法在如此小的公用事业损失阈值中实现合理的成本降低。 &lt;/p>; &lt;p>;在语言任务中，减少梯度大小的潜力不大，因为使用的词汇通常较小且已经非常紧凑（如下所示）。但是，采用稀疏性DP-SGD有效地避免了密集的梯度计算。此外，根据理论分析中给出的偏差变化权衡权衡，我们注意到，当梯度大小的减小最小时，DP-Adafest偶尔与DP-SGD相比表现出优越的效用。相反，当纳入稀疏性时，基线算法在维持实用程序方面面临挑战。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s1860/image6.png “ style =”保证金左：自动;边缘权利：自动;”>; &lt;img border =“ 0” data-eriginal-height =“ 560” data-eriginal-width =“ 1860” src =“ https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-字幕“样式=”文本align：中心;“>;最佳梯度尺寸减小的比较（在ε= 1.0下，普通dp-sgd和普通dp-sgd和sparsity-parparsity-preserve算法之间的非零梯度值计数的比率是DP -adafest（我们的算法）和基线算法（&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;具有指数性选择的DP-SGD与DP-SGD相比不同之处。 A higher curve indicates a better utility/efficiency trade-off.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, most ad prediction models are being continuously trained and evaluated. To simulate this online learning setup, we also evaluate with time-series data, which are notoriously challenging due to being non-stationary. Our evaluation uses the &lt;a href=&quot;https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/&quot;>;Criteo-1TB&lt;/a>; dataset, which comprises real-world user-click data collected over 24 days. Consistently, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 10&lt;sup>;4&lt;/sup>; times while maintaining a comparable AUC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s2500/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;891&quot; data-original-width=&quot;2500&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction achieved under ε =1.0 by DP-AdaFEST (our algorithm) and DP-SGD with exponential selection (a previous algorithm) compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms the previous method.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a new algorithm, DP-AdaFEST, for preserving gradient sparsity in differentially private training — particularly in applications involving large embedding models, a fundamental tool for various applications in recommendation systems and natural language processing. Our algorithm achieves significant reductions in gradient size while maintaining accuracy on real-world benchmark datasets. Moreover, it offers flexible options for balancing utility and efficiency via sparsity-controlling parameters, while our proposals offer much better privacy-utility loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was a collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi and Amer Sinha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6514019106482066697/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot; rel=&quot;alternate&quot; title=&quot;Sparsity-preserving differentially private training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s72-c/Sparse%20DP-SGD.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5912569868305742102&lt;/id>;&lt;published>;2023-12-07T09:51:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T09:53:23.920-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Virtual Reality&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VALID: A perceptually validated virtual avatar library for inclusion and diversity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mar Gonzalez-Franco, Research Scientist, Google AR &amp;amp; VR&lt;/span>;&lt;p>; As virtual reality (VR) and augmented reality (AR) technologies continue to grow in popularity, virtual avatars are becoming an increasingly important part of our digital interactions. In particular, virtual avatars are at the center of many social VR and AR interactions, as they are key to representing remote participants and facilitating collaboration. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the last decade, interdisciplinary scientists have dedicated a significant amount of effort to better understand the use of avatars, and have made many interesting observations, including the capacity of the users to &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frvir.2020.575943/full&quot;>;embody their avatar&lt;/a>; (ie, the illusion that the avatar body is their own) and the &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9089510&quot;>;self-avatar follower effect&lt;/a>;, which creates a binding between the actions of the avatar and the user strong enough that the avatar can actually affect user behavior. &lt;/p>; &lt;p>; The use of avatars in experiments isn&#39;t just about how users will interact and behave in VR spaces, but also about discovering the limits of human perception and neuroscience. In fact, some VR social experiments often rely on recreating scenarios that can&#39;t be reproduced easily in the real world, such as bar crawls to &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0052766&quot; >;explore ingroup vs. outgroup effects&lt;/a>;, or deception experiments, such as the &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0209704&quot;>;Milgram obedience to authority inside virtual reality&lt;/一个>;。 Other studies try to explore deep neuroscientific phenomena, like the &lt;a href=&quot;https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0453&quot;>;human mechanisms for motor control&lt;/a>;. This perhaps follows the trail of the &lt;a href=&quot;https://www.nature.com/articles/35784&quot;>;rubber hand illusion&lt;/a>; on brain plasticity, where a person can start feeling as if they own a rubber hand while their real hand is hidden behind a curtain. There is also an increased number of possible therapies for psychiatric treatment using personalized &lt;a href=&quot;https://doi.org/10.1186/s13063-022-06683-1&quot;>;avatars&lt;/a>;. In these cases, VR becomes an &lt;a href=&quot;https://en.wikipedia.org/wiki/Ecological_validity&quot;>;ecologically valid&lt;/a>; tool that allows scientists to explore or treat human behavior and perception. &lt;/p>; &lt;p>; None of these experiments and therapies could exist without good access to research tools and libraries that can enable easy experimentation. As such, multiple systems and open source tools have been released around avatar creation and animation over recent years. However, existing avatar libraries have not been validated systematically on the diversity spectrum. Societal &lt;a href=&quot;https://doi.org/10.1016/j.concog.2013.04.016&quot;>;bias and dynamics&lt;/a>; also transfer to VR/AR when interacting with avatars, which could lead to incomplete conclusions for studies on human behavior inside VR/AR. &lt;/p>; &lt;p>; To partially overcome this problem, we partnered with the University of Central Florida to create and release the open-source &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID). Described in &lt;a href=&quot;https://doi.org/10.3389/frvir.2023.1248915&quot;>;our recent paper&lt;/a>;, published in &lt;a href=&quot;https://www.frontiersin.org/journals/virtual -reality&quot;>;&lt;i>;Frontiers in Virtual Reality&lt;/i>;&lt;/a>;, this library of avatars is readily available for usage in VR/AR experiments and includes 210 avatars of seven different races and ethnicities recognized by the US Census Bureau 。 The avatars have been perceptually validated and designed to advance diversity and inclusion in virtual avatar research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s1717/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1413&quot; data-original-width=&quot;1717&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Headshots of all 42 base avatars available on the VALID library were created in extensive interaction with members of the 7 ethnic and racial groups from the &lt;a href=&quot;https://www .federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs-race-and-ethnicity-statistical-standards&quot;>;Federal Register&lt;/a>;, which include (AIAN, Asian, Black, Hispanic, MENA, NHPI and White).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Creation and validation of the library&lt;/h2>; &lt;p>; Our initial selection of races and ethnicities for the diverse avatar library follows the most recent guidelines of the &lt;a href=&quot;https://www.npr.org/2023/01/26/1151608403/mena-race-categories-us-census -middle-eastern-latino-hispanic&quot;>;US Census Bureau&lt;/a>; that as of 2023 recommended the use of 7 ethnic and racial groups representing a large demographic of the US society, which can also be extrapolated to the global population. These groups include &lt;em>;Hispanic or Latino&lt;/em>;, &lt;em>;American Indian or Alaska Native (AIAN), Asian,&lt;/em>; &lt;em>;Black or African American,&lt;/em>; &lt;em>;Native Hawaiian or Other Pacific Islander (NHPI),&lt;/em>; &lt;em>;White, Middle East or North Africa&lt;/em>; (MENA). We envision the library will continue to evolve to bring even more diversity and representation with future additions of avatars. &lt;/p>; &lt;p>; The avatars were hand modeled and created using a process that combined average facial features with extensive collaboration with representative stakeholders from each racial group, where their feedback was used to artistically modify the facial mesh of the avatars. Then we conducted an online study with participants from 33 countries to determine whether the race and gender of each avatar in the library are recognizable. In addition to the avatars, we also provide labels statistically validated through observation of users for the race and gender of all 42 base avatars (see below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V /s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;736&quot; data-original-width=&quot;1999&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the headshots of a Black/African American avatar presented to participants during the validation of the library.&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We found that all Asian, Black, and White avatars were universally identified as their modeled race by all participants, while our American Indian or Native Alaskan (AIAN ), Hispanic, and Middle Eastern or North African (MENA) avatars were typically only identified by participants of the same race. This also indicates that participant race can improve identification of a virtual avatar of the same race. The paper accompanying the library release highlights how this ingroup familiarity should also be taken into account when studying avatar behavior in VR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s1999/image1.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1509&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Confusion matrix heatmap of agreement rates for the 42 base avatars separated by other-race participants and same-race participants. One interesting aspect visible in this matrix, is that participants were significantly better at identifying the avatars of their own race than other races.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset details&lt;/h2>; &lt;p>; Our models are available in &lt;a href=&quot;https://www.autodesk.com/products/fbx/overview&quot;>;FBX format&lt;/a>;, are compatible with previous avatar libraries like the commonly used &lt;a href=&quot;https://github.com/microsoft/Microsoft-Rocketbox&quot;>;Rocketbox&lt;/a>;, and can be easily integrated into most game engines such as &lt;a href=&quot;https://unity.com/&quot;>;Unity&lt;/a>; and &lt;a href=&quot;https://www.unrealengine.com/&quot;>;Unreal&lt;/a>;. Additionally, the avatars come with 69 bones and 65 facial blendshapes to enable researchers and developers to easily create and apply dynamic facial expressions and animations. The avatars were intentionally made to be partially cartoonish to avoid extreme look-a-like scenarios in which a person could be impersonated, but still representative enough to be able to run reliable user studies and social experiments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s1999/image4.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Images of the skeleton rigging (bones that allow for animation) and some facial blend shapes included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;p>; The avatars can be further combined with variations of casual attires and five professional attires, including medical, military, worker and business. This is an intentional improvement from prior libraries that in some cases reproduced stereotypical gender and racial bias into the avatar attires, and provided very limited diversity to certain professional avatars. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s1537/image5.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1023&quot; data-original-width=&quot;1537&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Images of some sample attire included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Get started with VALID&lt;/h2 >; &lt;p>; We believe that the &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID) will be a valuable resource for researchers and developers working on VR/AR applications. We hope it will help to create more inclusive and equitable virtual experiences. To this end, we invite you to explore the avatar library, which we have released under the open source &lt;a href=&quot;https://en.wikipedia.org/wiki/MIT_License&quot;>;MIT license&lt;/a>;. You can download the avatars and use them in a variety of settings at no charge. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This library of avatars was born out of a collaboration with Tiffany D. Do, Steve Zelenty and Prof. Ryan P McMahan from the University of Central Florida.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5912569868305742102/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html&quot; rel=&quot;alternate&quot; title=&quot;VALID: A perceptually validated virtual avatar library for inclusion and diversity&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;published>;2023-12-05T17:32:00.000-08:00&lt;/published>;&lt;updated>;2024-01-03T14:16:52.565-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Google is proud to be a &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;Diamond Sponsor&lt;/a>; of &lt;a href=&quot;https://2023.emnlp.org/&quot;>;Empirical Methods in Natural Language Processing&lt;/a>; (EMNLP 2023), a premier annual conference, which is being held this week in Sentosa, Singapore. Google has a strong presence at this year&#39;s conference with over 65 accepted papers and active involvement in 11 workshops and tutorials. Google is also happy to be a &lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;Major Sponsor&lt;/a>; for the &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;Widening NLP&lt;/a>; workshop (WiNLP), which aims to highlight global representations of people, perspectives, and cultures in AI and ML. We look forward to sharing some of our extensive NLP research and expanding our partnership with the broader research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We hope you&#39;ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in NLP, and check out some of the scheduled booth activities (eg, demos and Q&amp;amp;A sessions listed below). Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) and &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember=true&quot;>;LinkedIn&lt;/a>; accounts to find out more about the Google booth activities at EMNLP 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at EMNLP 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Sponsorship Chair: &lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; Industry Track Chair: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; Senior Program Committee: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Annie Louis&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brian Roark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax: Structured Probability Distributions for JAX&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Miloš Stanojević&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Laurent Sartran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.11077.pdf&quot;>;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;, &lt;em>;Hannah Sterz&lt;/em>;, &lt;em>;Indraneil Paul&lt;/em>;, &lt;em>;Sukannya Purkayastha&lt;/em>;, &lt;em>;Leon Engländer&lt;/em>;,&lt;em>; Timo Imhof&lt;/em>;, &lt;em>;Ivan Vulić&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Iryna Gurevych&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet: Bridging the Data Gap in Document Pre-training&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;, &lt;strong>;&lt;em>;Jin Miao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xiaoyu Sun&lt;/em>;&lt;/strong>;, &lt;em>;Jiayi Chen&lt;/em>;, &lt;em>;Alexander Hauptmann&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wei Wei&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.08592.pdf&quot;>;AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-Powered Applications&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Robinson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks&lt;/a>; &lt;br />; &lt;em>;Mete Ismayilzada&lt;/em>;, &lt;em>;Debjit Paul&lt;/em>;, &lt;em>;Syrielle Montariol&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;, &lt;em>;Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;Large Language Models Can Self-Improve&lt;/a>; &lt;br />; &lt;em>;Jiaxin Huang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Shixiang Shane Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuexin Wu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hongkun Yu&lt;/em>;&lt;/strong>;, &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;Dissecting Recall of Factual Associations in Auto-Regressive Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot;>;Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;em>;Yoav Goldberg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yichao Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Bradley Wendt&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jing Xie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sandeep Tata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;Measuring Attribution in Natural Language Generation Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Hannah Rashkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Lamm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Collins&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gaurav Singh Tomar&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Iulia Turc&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011.pdf&quot;>;Inverse Scaling Can Become U-Shaped&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;, &lt;em>;Danqing Wang&lt;/em>;, &lt;em>;Liangming Pan&lt;/em>;, &lt;em>;Zhenqiao Song&lt;/em>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;William Yang Wang&lt;/em>;, &lt;em>;Lei Li&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-Based Method&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;, &lt;em>;Nadav Oved&lt;/em>;,&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roi Reichart&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.04347.pdf&quot;>;Investigating Efficiently Extending Transformers for Long-Input Summarization&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09744.pdf&quot;>;DSI++: Updating Transformer Memory with New Documents&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Emma Strubell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12029.pdf&quot;>;MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup&lt;/a>; &lt;br />; &lt;em>;Hua Shen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vicky Zayats&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Johann C Rocholl&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel David Walker&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dirk Padfield&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d: Turning Questions into Dialogs to Teach Models How to Search&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ido Hakimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;Emergence of Abstract State Representations in Embodied Sequence Modeling&lt;/a>; &lt;br />; &lt;em>;Tian Yun&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Zilai Zeng&lt;/em>;, &lt;em>;Kunal Handa&lt;/em>;, &lt;strong>;&lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bo Pang&lt;/em>;&lt;/strong>;, &lt;em>;Ellie Pavlick&lt;/em>;, &lt;em>;Chen Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;Evaluating and Modeling Attribution for Cross-Lingual Question Answering&lt;/a>; &lt;br />; &lt;em>;Benjamin Muller&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;Weakly-Supervised Learning of Visual Relations in Multimodal Pre-training&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lisa Anne Hendricks&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13286.pdf&quot;>;How Do Languages Influence Each Other? Studying Cross-Lingual Data Sharing During LM Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;, &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;, &lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models&lt;/a>; &lt;br>; &lt;em>;Benjamin Minixhofer&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;, &lt;em>;Ivan Vulić&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.01328.pdf&quot;>;IC3: Image Captioning by Committee Consensus&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;, &lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; John Canny&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Ido Dagan&lt;/em>;, &lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;Evaluating Large Language Models on Controlled Generation Tasks&lt;/a>; &lt;br />; &lt;em>;Jiao Sun&lt;/em>;, &lt;em>;Yufei Tian&lt;/em>;, &lt;em>;Wangchunshu Zhou&lt;/em>;, &lt;em>;Nan Xu&lt;/em>;, &lt;em>;Qian Hu&lt;/em>;, &lt;em>;Rahul Gupta&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;Nanyun Peng&lt;/em>;, &lt;em>;Xuezhe Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Daniel Deutsch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;Transcending Scaling Laws with 0.1% Extra Compute&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; Hyung Won Chung&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;, &lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Siamak Shakeri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Huaixiu Steven Zheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Donald&lt;/em>; &lt;em>;Metzler&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;Data Similarity is Not Enough to Explain Language Model Performance&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Emily Reif&lt;/em>;&lt;/strong>;, &lt;em>;David Mimno&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00913.pdf&quot;>;Self-Influence Guided Data Reweighting for Language Model Pre-training&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Tolga Bolukbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;, &lt;em>; Sarath Chandar&lt;/em>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;ReTAG: Reasoning Aware Table to Analytic Text Generation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS: Using a New Multilingual Lexicon for Low-Resource Machine Translation&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;Video-Helpful Multimodal Machine Translation&lt;/a>; &lt;br />; &lt;em>;Yihang Li, Shuichiro Shimizu&lt;/em>;, &lt;em>;Chenhui Chu&lt;/em>;, &lt;em>;Sadao Kurohashi&lt;/em>;, &lt;strong>;&lt;em>;Wei Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;Symbol Tuning Improves In-Context Learning in Language Models&lt;/a>; &lt;br />; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt;/em>;, &lt;em>;Xiangning Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Da Huang&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Xinyun&lt;/em>; &lt;em>;Chen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yifeng Lu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Tengyu Ma&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;&quot;Don&#39;t Take This Out of Context!&quot; On the Need for Contextual Models and Evaluations for Stylistic Rewriting&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;, &lt;em>;Xuhui Zhou&lt;/em>;, &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon: Multilingual QA with Only 5 Examples&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chris Alberti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Joshua Maynez&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ji Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kuzman Ganchev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Eugene Kharitonov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Damien Vincent&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zalán Borsos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Matt Sharifi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD: A Programmable Task-Oriented Dialog System&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeffrey Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuan Cao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghav Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hagen Soltau&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Izhak Shafran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yonghui Wu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;Selectively Answering Ambiguous Questions&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael JQ Zhang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Eisenstein&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;strong>;&lt;em>;Rahul Goel&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Waleed Ammar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aditya Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Motoki Sano&lt;/em>;&lt;/strong>;,&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Max Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;David Greene&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chuan He&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhou Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM: Detecting Factual Errors via Cross Examination&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;, &lt;em>;May Hamri&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Geoff Brown&lt;/em>;&lt;/strong>;, &lt;em>;Bryan A. Plummer&lt;/em>;, &lt;em>;Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;, &lt;strong>;&lt;em>;Jianmo Ni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mandy Guo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg=AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Idris Abdulmumin&lt;/em>;, &lt;em>;Abinew Ali Ayele&lt;/em>;, &lt;em>;Nedjma Ousidhoum&lt;/em>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Seid Muhie Yimam&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Oumaima Hourrane&lt;/em>;, &lt;em>;Alipio Jorge&lt;/em>;, &lt;em>;Pavel Brazdil&lt;/em>;, &lt;em>;Felermino D. M&lt;/em>;. &lt;em>;A. Ali&lt;/em>;, &lt;em>;Davis David&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Bello Shehu-Bello&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;, &lt;em>;Samuel Rutunda&lt;/em>;, &lt;em>;Tadesse Destaw Belay&lt;/em>;, &lt;em>;Wendimu Baye Messelle&lt;/em>;,&lt;em>; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;, &lt;em>;Sisay Adugna Chala&lt;/em>;, &lt;em>;Hagos Tesfahun Gebremichael&lt;/em>;,&lt;em>; Bernard Opoku&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;Optimizing Retrieval-Augmented Reader Models via Token Elimination&lt;/a>; &lt;br />; &lt;em>;Moshe Berchansky&lt;/em>;, &lt;em>;Peter Izsak&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;em>; Ido Dagan&lt;/em>;, &lt;em>;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aditya Siddhant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;, &lt;em>;Michiel de Jong&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Federico Lebron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752.pdf&quot;>;CoLT5: Faster Long-Range Transformers with Conditional Computation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tao Lei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michiel de Jong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Santiago Ontanon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddhartha Brahma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandy Guo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yun-Hsuan Sung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16523.pdf&quot;>;Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas Blumm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;, &lt;em>;Alex Beutel&lt;/em>;, &lt;strong>;&lt;em>;Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;Universal Self-Adaptive Prompting&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Ruoxi Sun, Hootan Nakhost&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zorik Gekhman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.07871.pdf&quot;>;Hierarchical Pre-training on Multimodal Electronic Health Records&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;, &lt;em>;Junyu Luo&lt;/em>;, &lt;em>;Jiaqi Wang&lt;/em>;, &lt;em>;Ziyi Yin&lt;/em>;, &lt;em>;Suhan Cui&lt;/em>;, &lt;em>;Yuan Zhong&lt;/em>;, &lt;strong>;&lt;em>;Yaqing Wang&lt;/em>;&lt;/strong>;, &lt;em>;Fenglong Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841.pdf&quot;>;How Does Generative Retrieval Scale to Millions of Passages?&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Kai Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;, &lt;em>;Jimmy Lin&lt;/em>;, &lt;strong>;&lt;em>;Donald Metzler&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Findings of EMNLP&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs&lt;/a>; &lt;br />; &lt;em>;Jiefeng Chen&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Jinsung Yoon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sayna Ebrahimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Somesh Jha&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;A Comprehensive Evaluation of Tool-Assisted Generation Strategies&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER: One Pass Answer Generation and Evidence Retrieval&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM: Towards Multilingual Visual Question Answering&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Linting Xue, Michal Yarom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Julien&lt;/em>; &lt;em>;Amelot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xi Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431v1.pdf&quot;>;SDOH-NLI: A Dataset for Inferring Social Determinants of Health from Clinical Notes&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;, &lt;em>;Eric Loreaux&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Tal Schuster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Jun Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;Machine Reading Comprehension Using Case-based Reasoning&lt;/a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;, &lt;em>;Dhruv Agarwal&lt;/em>;, &lt;em>;Mudit Chaudhary&lt;/em>;, &lt;em>;Wenlong Zhao&lt;/em>;, &lt;em>;Rajarshi Das&lt;/em>;,&lt;em>; Jay-Yoon Lee&lt;/em>;, &lt;em>;Hannaneh Hajishirzi&lt;/em>;, &lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;, &lt;em>;Andrew McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;Cross-lingual Open-Retrieval Question Answering for African Languages&lt;/a>; &lt;br />; &lt;em>;Odunayo Ogundepo&lt;/em>;, &lt;em>;Tajuddeen Gwadabe&lt;/em>;, &lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Bonaventure FP Dossou&lt;/em>;, &lt;em>;Abdou Aziz DIOP&lt;/em>;, &lt;em>;Claytone Sikasote&lt;/em>;, &lt;em>;Gilles HACHEME&lt;/em>;, &lt;em>;Happy Buzaaba&lt;/em>;,&lt;em>; Ignatius Ezeani&lt;/em>;, &lt;em>;Rooweither Mabuya&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Chris&lt;/em>; &lt;em>;Chinenye Emezue&lt;/em>;, &lt;em>;Albert Kahira&lt;/em>;, &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Akintunde Oladipo&lt;/em>;, &lt;em>;Abraham Toluwase Owodunni&lt;/em>;, &lt;em>;Atnafu Lambebo Tonja&lt;/em>;, &lt;em>;Iyanuoluwa Shode&lt;/em>;, &lt;em>;Akari Asai&lt;/em>;, &lt;em>;Anuoluwapo Aremu&lt;/em>;, &lt;em>;Ayodele Awokoya&lt;/em>;, &lt;em>;Bernard Opoku&lt;/em>;, &lt;em>;Chiamaka Ijeoma Chukwuneke&lt;/em>;, &lt;em>;Christine Mwase&lt;/em>;, &lt;em>;Clemencia Siro&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>;, &lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;, &lt;em>;Verrah Akinyi Otiende&lt;/em>;, &lt;em>;Andre Niyongabo Rubungo&lt;/em>;, &lt;em>;Boyd Sinkala&lt;/em>;, &lt;em>;Daniel Ajisafe&lt;/em>;, &lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;, &lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;, &lt;em>;Mofetoluwa Adeyemi&lt;/em>;, &lt;em>;Mofya Phiri&lt;/em>;, &lt;em>;Orevaoghene Ahia&lt;/em>;, &lt;em>;Ruqayya Nasir Iro&lt;/em>;, &lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Polina Zablotskaia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Du Phan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shashi Narayan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;Sources of Hallucination by Large Language Models on Inference Tasks&lt;/a>; &lt;br />; &lt;em>;Nick McKenna&lt;/em>;, &lt;em>;Tianyi Li&lt;/em>;, &lt;em>;Liang Cheng&lt;/em>;, &lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;, &lt;em>;Mark Johnson&lt;/em>;, &lt;em>;Mark Steedman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;Don&#39;t Add, Don&#39;t Miss: Effective Content Preserving Generation from Pre-selected Text Spans&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Eran Hirsch&lt;/em>;, &lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Katherine Hermann&lt;/strong>;&lt;/em>;, &lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;Understanding HTML with Large Language Models&lt;/a>; &lt; br />; &lt;strong>;&lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ofir Nachum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yingjie Miao&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Mustafa Safdari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Austin Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha&lt;/em>; &lt; em>;Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Noah Fiedel&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Aleksandra Faust&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;Improving the Robustness of Summarization Models by Detecting and Removing Input Noise&lt;/a>; &lt;br />; &lt;em>;Kundan Krishna&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jiaming Luo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mohammad&lt;/em>; &lt;em>;Saleh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter J. Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2310.15916.pdf&quot;>;In-Context Learning Creates Task Vectors&lt;/a>; &lt;br />; &lt;em>;Roee Hendel&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10544.pdf&quot;>;Pre -training Without Attention&lt;/a>; &lt;br />; &lt;em>;Junxiong Wang&lt;/em>;, &lt;em>;Jing Nathan Yan&lt;/em>;, &lt;strong>;&lt;em>;Albert Gu&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Alexander M Rush&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>;MUX-PLMs: Data Multiplexing for High-Throughput Language Models&lt;/a>; &lt;br />; &lt;em>;Vishvak Murahari&lt;/em>;, &lt;em>;Ameet Deshpande&lt;/em>;, &lt;em>;Carlos E Jimenez&lt;/em>;,&lt;em>; &lt;strong >;Izhak Shafran&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yuan&lt;/em>; &lt;em>;Cao&lt;/em>;&lt;/ strong>;, &lt;em>;Karthik R Narasimhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14408.pdf&quot;>;PaRaDe: Passage Ranking Using Demonstrations with LLMs&lt;/ a>; &lt;br />; &lt;em>;Andrew Drozdov&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Zhuyun Dai&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhen Qin&lt;/em>;&lt;/strong>;, &lt;em>;Razieh Rahimi&lt;/em>;, &lt;strong>;&lt;em>;Xuanhui Wang&lt;/em>;&lt;/strong >;,&lt;strong>; &lt;em>;Dana Alon&lt;/em>;&lt;/strong>;, &lt;em>;Mohit Iyyer&lt;/em>;, &lt;em>;Andrew McCallum&lt;/em>;, &lt;em>;Donald Metzler&lt;sup>;*&lt;/ sup>;&lt;/em>;,&lt;strong>; &lt;em>;Kai Hui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13678.pdf&quot;>; Long-Form Speech Translation Through Segmentation with Finite-State Decoding Constraints on Large Language Models&lt;/a>; &lt;br />; &lt;em>;Arya D. McCarthy&lt;/em>;, &lt;strong>;&lt;em>;Hao Zhang&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Felix Stahlberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ke Wu&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;Unsupervised Opinion Summarization Using Approximate Geodesics&lt;/a>; &lt;br />; &lt;em>;Somnath Basu Roy Chowdhury&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Nicholas Monath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kumar Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Amr Ahmed&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02883. pdf&quot;>;SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sercan O . Arik&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rajarishi Sinha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hootan Nakhost&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em >;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/EMNLP_2023_retrieval.pdf&quot;>;Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty&lt;/a>; &lt;br />; &lt;em>;Zi Lin&lt; /em>;, &lt;strong>;&lt;em>;Quan Yuan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Panupong Pasupat&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Jingbo Shang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.08740.pdf&quot;>;A Zero-Shot Language Agent for Computer Control with Structured Reflection&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Tao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Zhiwei Deng&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wang&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches&lt;/a>; &lt;br / >; &lt;em>;Daniel Fried&lt;/em>;, &lt;em>;Nicholas Tomlin&lt;/em>;, &lt;em>;Jennifer Hu&lt;/em>;,&lt;strong>; &lt;em>;Roma Patel&lt;/em>;&lt;/strong>;, &lt;strong >;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;Improving Classifier Robustness Through Active Generation of Pairwise Counterfactuals &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ananth Balashankar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Qin &lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nithum Thain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jilin Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ed H.&lt;/em>; &lt;em>;Chi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Beutel&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224.pdf&quot;>;mmT5: Modular Multilingual Pre-training Solves Source Language Hallucinations&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Francesco Piccinno&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt; strong>;&lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Machel Reid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian&lt;/em>; &lt;em>;Ruder&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;Scaling Laws vs Model Architectures: How Does Inductive Bias Influence Scaling?&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Samira Abnar&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; William Fedus&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jinfeng Rao&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dani Yogatama&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00142. pdf&quot;>;TaTA: A Multilingual Table-to-Text Dataset for African Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vitaly Nikolaev,&lt;/em>; &lt;em>;Jan A. Botha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael Chavinda&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.11938.pdf&quot;>;XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Ruder ,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Gutkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mihir Kale&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Min Ma&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Parker Riley&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jean Michel Amath Sarr&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;John Frederick&lt;/em>; &lt;em>;Wieting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nitish Gupta&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna Katanova&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christo Kirov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dana L Dickinson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Roark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bidisha Samanta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Connie Tao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Ifeoluwa Adelani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vera Axelrod&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Isaac Rayburn&lt;/em>; &lt;em>;Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Garrette&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Reeve Ingle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dmitry Panteleev&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00693.pdf&quot;>;On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval&lt;/a>; &lt;br />; &lt;em>;Jiayi Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;em>;Aidong Zhang&lt;/em>;, &lt;em>;Wei Wei&lt;sup>;*&lt;/sup>;&lt;/ em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px ;&quot;>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701752203060961&amp;amp;usg =AOvVaw3sRozMNVYuwvyXeLluOgKI&quot;>;The Seventh Widening NLP Workshop&lt;/a>; (WiNLP) &lt;br />; Major Sponsor &lt;br />; Organizers: &lt;strong>;&lt;em>;Sunipa Dev&lt;/em>;&lt;/strong>; &lt;br />; Panelist: &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/crac2023/?pli=1&quot;>; The Sixth Workshop on Computational Models of Reference, Anaphora and Coreference&lt;/a>; (CRAC) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;The 3rd Workshop for Natural Language Processing Open Source Software&lt;/a>; (NLP-OSS) &lt;br />; Organizer: &lt;strong>;&lt; em>;Geeticka Chauhan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://splu-robonlp-2023.github.io/&quot;>;Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics&lt;/a>; (SpLU-RoboNLP) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem -benchmark.com/workshop&quot;>;Natural Language Generation, Evaluation, and Metric&lt;/a>; (GEM) &lt;br />; Organizer: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://arabicnlp2023.sigarab.org/&quot;>;The First Arabic Natural Language Processing Conference&lt;/a>; (ArabicNLP) &lt;br />; Organizer: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.bigpictureworkshop.com/&quot;>;The Big Picture: Crafting a Research Narrative&lt;/a>; (BigPicture) &lt;br />; Organizer: &lt;strong>;&lt;em>;Nora Kassner&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp .github.io/&quot;>;BlackboxNLP 2023: The 6th Workshop on Analysing and Interpreting Neural Networks for NLP&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br / >; Panelist: &lt;strong>;&lt;em>;Neel Nanda&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.conll.org/2023&quot;>;The SIGNLL Conference on Computational Natural Language Learning&lt;/a>; (CoNLL) &lt;br />; Co-Chair: &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;br />; Areas and ACs: &lt;strong>;&lt;em>;Kyle Gorman&lt; /em>;&lt;/strong>; (Speech and Phonology), &lt;strong>;&lt;em>;Fei Liu &lt;/em>;&lt;/strong>;(Natural Language Generation) &lt;/p>; &lt;p>; &lt;a href=&quot;https:// sigtyp.github.io/ws2023-mrl.html&quot;>;The Third Workshop on Multi-lingual Representation Learning&lt;/a>; (MRL) &lt;br />; Organizer: &lt;strong>;&lt;em>;Omer Goldman&lt;/em>;&lt;/strong >;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot; https://emnlp2023-creative-nlg.github.io/&quot;>;Creative Natural Language Generation&lt;/a>; &lt;br />; Organizer: &lt;em>;Tuhin Chakrabarty&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p >; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;p>; &lt;i>;This schedule is subject to change 。 Please visit the Google booth for more information.&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Developing and Utilizing Evaluation Metrics for Machine Translation &amp;amp; Improving Multilingual NLP &lt;br />; Presenter: &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jan-Thorsten Peter&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; Fri, Dec 8 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Differentiable Search Indexes &amp;amp; Generative Retrieval &lt;br />; Presenter: &lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;, &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; Fri, Dec 8 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; Retrieval and Generation in a single pass &lt;br />; Presenter: &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Amplifying Adversarial Attacks &lt;br />; Presenter:&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; Automate prompt design: Universal Self-Adaptive Prompting (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; Presenter: &lt;strong>;&lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8414954450937764241/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at EMNLP 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3665865722098768988&lt;/id>;&lt;published>;2023-12-04T14:41:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T14:46:06.688-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A new quantum algorithm for classical mechanics with an exponential speedup&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Robin Kothari and Rolando Somma, Research Scientists, Google Research, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Quantum computers promise to solve some problems exponentially faster than classical computers, but there are only a handful of examples with such a dramatic speedup, such as &lt;a href=&quot;https://en.wikipedia.org/wiki /Shor%27s_algorithm&quot;>;Shor&#39;s factoring algorithm&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot;>;quantum simulation&lt;/一个>;。 Of those few examples, the majority of them involve simulating physical systems that are inherently quantum mechanical — a natural application for quantum computers. But what about simulating systems that are not inherently quantum? Can quantum computers offer an exponential advantage for this? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://link.aps.org/doi/10.1103/PhysRevX.13.041041&quot;>;Exponential quantum speedup in simulating coupled classical oscillators&lt;/a>;”, published in &lt;a href=&quot;https://journals.aps.org/prx/&quot;>;Physical Review X&lt;/a>; (PRX) and presented at the &lt;a href=&quot;https://focs.computer.org/2023/&quot;>;Symposium on Foundations of Computer Science&lt;/a>; (FOCS 2023), we report on the discovery of a new quantum algorithm that offers an exponential advantage for simulating coupled &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_oscillator&quot;>;classical harmonic oscillators&lt;/a>;. These are some of the most fundamental, ubiquitous systems in nature and can describe the physics of countless natural systems, from electrical circuits to molecular vibrations to the mechanics of bridges. In collaboration with Dominic Berry of Macquarie University and Nathan Wiebe of the University of Toronto, we found a mapping that can transform any system involving coupled oscillators into a problem describing the time evolution of a quantum system. Given certain constraints, this problem can be solved with a quantum computer exponentially faster than it can with a classical computer. Further, we use this mapping to prove that any problem efficiently solvable by a quantum algorithm can be recast as a problem involving a network of coupled oscillators, albeit exponentially many of them. In addition to unlocking previously unknown applications of quantum computers, this result provides a new method of designing new quantum algorithms by reasoning purely about classical systems. &lt;/p>; &lt;br />; &lt;h2>;Simulating coupled oscillators&lt;/h2>; &lt;p>; The systems we consider consist of classical harmonic oscillators. An example of a single harmonic oscillator is a mass (such as a ball) attached to a spring. If you displace the mass from its rest position, then the spring will induce a restoring force, pushing or pulling the mass in the opposite direction. This restoring force causes the mass to oscillate back and forth. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;116&quot; data-original-width=&quot;359&quot; height=&quot;129&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A simple example of a harmonic oscillator is a mass connected to a wall by a spring. [Image Source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif&quot;>;Wikimedia&lt;/a>;]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now consider &lt;em>;coupled &lt;/em>;harmonic oscillators, where &lt;em>;multiple&lt;/em>; masses are attached to one another through springs. Displace one mass, and it will induce a wave of oscillations to pulse through the system. As one might expect, simulating the oscillations of a large number of masses on a classical computer gets increasingly difficult. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1 /s1129/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1129&quot; height =&quot;295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example system of masses connected by springs that can be simulated with the quantum algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To enable the simulation of a large number of coupled harmonic oscillators, we came up with a mapping that encodes the positions and velocities of all masses and springs into the quantum wavefunction of a system of qubits. Since the number of parameters describing the wavefunction of a system of qubits grows exponentially with the number of qubits, we can encode the information of &lt;em>;N&lt;/em>; balls into a quantum mechanical system of only about log(&lt;em>;N&lt;/em>;) qubits. As long as there is a compact description of the system (ie, the properties of the masses and the springs), we can evolve the wavefunction to learn coordinates of the balls and springs at a later time with far fewer resources than if we had used a naïve classical approach to simulate the balls and springs. &lt;/p>; &lt;p>; We showed that a certain class of coupled-classical oscillator systems can be efficiently simulated on a quantum computer. But this alone does not rule out the possibility that there exists some as-yet-unknown clever classical algorithm that is similarly efficient in its use of resources. To show that our quantum algorithm achieves an exponential speedup over &lt;em>;any&lt;/em>; possible classical algorithm, we provide two additional pieces of evidence. &lt;/p>; &lt;br />; &lt;h2>;The glued-trees problem and the quantum oracle&lt;/h2>; &lt;p>; For the first piece of evidence, we use our mapping to show that the quantum algorithm can efficiently solve a famous problem about graphs known to be difficult to solve classically, called the &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>;glued-trees problem&lt;/a>;. The problem takes two branching trees — a graph whose nodes each branch to two more nodes, resembling the branching paths of a tree — and glues their branches together through a random set of edges, as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;930&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual representation of the glued trees problem. Here we start at the node labeled ENTRANCE and are allowed to locally explore the graph, which is obtained by randomly gluing together two binary trees. The goal is to find the node labeled EXIT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The goal of the glued-trees problem is to find the exit node — the “root” of the second tree — as efficiently as possible. But the exact configuration of the nodes and edges of the glued trees are initially hidden from us. To learn about the system, we must query an &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_machine&quot;>;oracle&lt;/a>;, which can answer specific questions about the setup. This oracle allows us to explore the trees, but only locally. Decades ago, &lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;it was shown&lt;/a>; that the number of queries required to find the exit node on a classical computer is proportional to a polynomial factor of &lt;em>;N&lt;/em>;, the total number of nodes. &lt;/p>; &lt;p>; But recasting this as a problem with balls and springs, we can imagine each node as a ball and each connection between two nodes as a spring. Pluck the entrance node (the root of the first tree), and the oscillations will pulse through the trees. It only takes a time that scales with the &lt;em>;depth&lt;/em>; of the tree — which is exponentially smaller than &lt;em>;N&lt;/em>; — to reach the exit node. So, by mapping the glued-trees ball-and-spring system to a quantum system and evolving it for that time, we can detect the vibrations of the exit node and determine it exponentially faster than we could using a classical computer. &lt;/p>; &lt;br />; &lt;h2>;BQP-completeness&lt;/h2>; &lt;p>; The second and strongest piece of evidence that our algorithm is exponentially more efficient than any possible classical algorithm is revealed by examination of the set of problems a quantum computer can solve efficiently (ie, solvable in &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time&quot;>;polynomial time&lt;/a>;), referred to as &lt;a href=&quot;https://en.wikipedia.org/wiki/BQP&quot;>;bounded-error quantum polynomial time&lt;/a>; or BQP. The hardest problems in BQP are called “BQP-complete”. &lt;/p>; &lt;p>; While it is generally accepted that there exist some problems that a quantum algorithm can solve efficiently and a classical algorithm cannot, this has not yet been proven. So, the best evidence we can provide is that our problem is BQP-complete, that is, it is among the hardest problems in BQP. If someone were to find an efficient classical algorithm for solving our problem, then every problem solved by a quantum computer efficiently would be classically solvable! Not even the &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_factorization&quot;>;factoring problem&lt;/a>; (finding the prime factors of a given large number), which forms the basis of &lt;a href=&quot;https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;>;modern encryption&lt;/a>; and was famously solved by Shor&#39;s algorithm, is expected to be BQP-complete. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;563&quot; data-original-width=&quot;574&quot; height=&quot;314&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing the believed relationships of the classes BPP and BQP, which are the set of problems that can be efficiently solved on a classical computer and quantum computer, respectively. BQP-complete problems are the hardest problems in BQP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To show that our problem of simulating balls and springs is indeed BQP-complete, we start with a standard BQP-complete problem of simulating universal quantum circuits, and show that every quantum circuit can be expressed as a system of many balls coupled with springs. Therefore, our problem is also BQP-complete. &lt;/p>; &lt;br />; &lt;h2>;Implications and future work&lt;/h2>; &lt;p>; This effort also sheds light on work from 2002, when theoretical computer scientist Lov K. Grover and his colleague, Anirvan M. Sengupta, used an &lt;a href=&quot;https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319&quot;>;analogy to coupled&lt;/a>; pendulums to illustrate how Grover&#39;s famous quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Grover%27s_algorithm&quot;>;search algorithm&lt;/a>; could find the correct element in an unsorted database quadratically faster than could be done classically. With the proper setup and initial conditions, it would be possible to tell whether one of &lt;em>;N&lt;/em>; pendulums was different from the others — the analogue of finding the correct element in a database — after the system had evolved for time that was only ~√(&lt;em>;N)&lt;/em>;. While this hints at a connection between certain classical oscillating systems and quantum algorithms, it falls short of explaining why Grover&#39;s quantum algorithm achieves a quantum advantage. &lt;/p>; &lt;p>; Our results make that connection precise. We showed that the dynamics of any classical system of harmonic oscillators can indeed be equivalently understood as the dynamics of a corresponding quantum system of exponentially smaller size. In this way we can simulate Grover and Sengupta&#39;s system of pendulums on a quantum computer of log(&lt;em>;N&lt;/em>;) qubits, and find a different quantum algorithm that can find the correct element in time ~√(&lt;em>;N&lt;/em>;). The analogy we discovered between classical and quantum systems can be used to construct other quantum algorithms offering exponential speedups, where the reason for the speedups is now more evident from the way that classical waves propagate. &lt;/p>; &lt;p>; Our work also reveals that every quantum algorithm can be equivalently understood as the propagation of a classical wave in a system of coupled oscillators. This would imply that, for example, we can in principle build a classical system that solves the factoring problem after it has evolved for time that is exponentially smaller than the runtime of any known classical algorithm that solves factoring. This may look like an efficient classical algorithm for factoring, but the catch is that the number of oscillators is exponentially large, making it an impractical way to solve factoring. &lt;/p>; &lt;p>; Coupled harmonic oscillators are ubiquitous in nature, describing a broad range of systems from electrical circuits to chains of molecules to structures such as bridges. While our work here focuses on the fundamental complexity of this broad class of problems, we expect that it will guide us in searching for real-world examples of harmonic oscillator problems in which a quantum computer could offer an exponential advantage. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our Quantum Computing Science Communicator, Katie McCormick, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3665865722098768988/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html&quot; rel=&quot;alternate&quot; title=&quot;A new quantum algorithm for classical mechanics with an exponential speedup&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72-c/glued-trees.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-68038970217457115&lt;/id>;&lt;published>;2023-12-04T10:00:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T11:29:40.224-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ads&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Summary report optimization in the Privacy Sandbox Attribution Reporting API&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hidayet Aksu, Software Engineer, and Adam Sealfon, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, the &lt;a href=&quot;https://privacysandbox.com/&quot;>;Privacy Sandbox&lt;/a>; initiative was launched to explore responsible ways for advertisers to measure the effectiveness of their campaigns, by aiming to &lt;a href=&quot;https://blog.chromium.org/2020/01/building-more-private-web-path-towards.html&quot;>;deprecate third-party cookies&lt;/a>; (subject to &lt;a href=&quot;https://www.gov.uk/cma-cases/investigation-into-googles-privacy-sandbox-browser-changes&quot;>;resolving any competition concerns with the UK&#39;s Competition and Markets Authority&lt;/a>;). &lt;a href=&quot;https://en.wikipedia.org/wiki/HTTP_cookie#Third-party_cookie&quot;>;Cookies&lt;/a>; are small pieces of data containing user preferences that websites store on a user&#39;s device; they can be used to provide a better browsing experience (eg, allowing users to automatically sign in) and to serve relevant content or ads. The Privacy Sandbox attempts to address concerns around the use of cookies for tracking browsing data across the web by providing a privacy-preserving alternative. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Many browsers use &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) to provide privacy-preserving APIs, such as the &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting&quot;>;Attribution Reporting API&lt;/a>; (ARA), that don&#39;t rely on cookies for ad conversion measurement. ARA encrypts individual user actions and collects them in an aggregated &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/&quot;>;summary report&lt;/a>;, which estimates measurement goals like the number and value of conversions (useful actions on a website, such as making a purchase or signing up for a mailing list) attributed to ad campaigns. &lt;/p>; &lt;p>; The task of configuring API parameters, eg, allocating a contribution budget across different conversions, is important for maximizing the utility of the summary reports. In “&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;Summary Report Optimization in the Privacy Sandbox Attribution Reporting API&lt;/a>;”, we introduce a formal mathematical framework for modeling summary reports. Then, we formulate the problem of maximizing the utility of summary reports as an optimization problem to obtain the optimal ARA parameters. Finally, we evaluate the method using real and synthetic datasets, and demonstrate significantly improved utility compared to baseline non-optimized summary reports. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ARA summary reports&lt;/h2>; &lt;p>; We use the following example to illustrate our notation. Imagine a fictional gift shop called &lt;em>;Du &amp;amp; Penc&lt;/em>; that uses digital advertising to reach its customers. The table below captures their holiday sales, where each record contains impression features with (i) an impression ID, (ii) the campaign, and (iii) the city in which the ad was shown, as well as conversion features with (i) the number of items purchased and (ii) the total dollar value of those items. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/s1035/image4.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;397&quot; data-original-width=&quot;1035&quot; height=&quot;245&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/w640-h245/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Impression and conversion feature logs for Du &amp;amp; Penc.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Mathematical model&lt;/h3>; &lt;p>; ARA summary reports can be modeled by four algorithms: (1) Contribution Vector, (2) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATE.md#contribution-bounding-and-budgeting&quot;>;Contribution Bounding&lt;/a>;, (3) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATION_SERVICE_TEE.md?&quot;>;Summary Reports&lt;/a>;, and (4) Reconstruct Values. Contribution Bounding and Summary Reports are performed by the ARA, while Contribution Vector and Reconstruct Values are performed by an AdTech provider — tools and systems that enable businesses to buy and sell digital advertising. The objective of this work is to assist AdTechs in optimizing summary report algorithms. &lt;/p>; &lt;p>; The Contribution Vector algorithm converts measurements into an ARA format that is discretized and scaled. Scaling needs to account for the overall contribution limit per impression. Here we propose a method that clips and performs randomized rounding. The outcome of the algorithm is a histogram of aggregatable keys and values. &lt;/p>; &lt;p>; Next, the Contribution Bounding algorithm runs on client devices and enforces the contribution bound on attributed reports where any further contributions exceeding the limit are dropped. The output is a histogram of attributed conversions. &lt;/p>; &lt;p>; The Summary Reports algorithm runs on the server side inside a &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;trusted execution environment&lt;/a>; and returns noisy aggregate results that satisfy DP. Noise is sampled from the discrete &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_distribution&quot;>;Laplace distribution&lt;/a>;, and to enforce privacy budgeting, a report may be queried only once. &lt;/p>; &lt;p>; Finally, the Reconstruct Values algorithm converts measurements back to the original scale. Reconstruct Values and Contribution Vector Algorithms are designed by the AdTech, and both impact the utility received from the summary report. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s1999/image1.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Illustrative usage of ARA summary reports, which include Contribution Vector (Algorithm A), Contribution Bounding (Algorithm C), Summary Reports (Algorithm S), and Reconstruct Values (Algorithm R). Algorithms C and S are fixed in the API. The AdTech designs A and R.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Error metrics&lt;/h2>; &lt;p>; There are several factors to consider when selecting an error metric for evaluating the quality of an approximation. To choose a particular metric, we considered the desirable properties of an error metric that further can be used as an objective function. Considering desired properties, we have chosen &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/design-decisions/#rmsre&quot;>;𝜏-truncated root mean square relative error&lt;/a>; (RMSRE&lt;sub>;𝜏&lt;/sub>;) as our error metric for its properties. See the &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;paper&lt;/a>; for a detailed discussion and comparison to other possible metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimization&lt;/h2>; &lt;p>; To optimize utility as measured by RMSRE&lt;sub>;𝜏&lt;/sub>;, we choose a capping parameter, &lt;em>;C&lt;/em>;, and privacy budget, 𝛼, for each slice. The combination of both determines how an actual measurement (such as two conversions with a total value of $3) is encoded on the AdTech side and then passed to the ARA for Contribution Bounding algorithm processing. RMSRE&lt;sub>;𝜏&lt;/sub>; can be computed exactly, since it can be expressed in terms of the bias from clipping and the variance of the noise distribution. Following those steps we find out that RMSRE&lt;sub>;𝜏&lt;/sub>; for a fixed privacy budget, 𝛼,&lt;sub>; &lt;/sub>;or a capping parameter, C, is &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_optimization&quot;>;convex&lt;/a>; (so the error-minimizing value for the other parameter can be obtained efficiently), while for joint variables (C, 𝛼) it becomes non-convex (so we may not always be able to select the best possible parameters). In any case, any off-the-shelf optimizer can be used to select privacy budgets and capping parameters. In our experiments, we use the &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html&quot;>;SLSQP&lt;/a>; minimizer from the &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html&quot;>;scipy.optimize&lt;/a>; library. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Synthetic data&lt;/h2>; &lt;p>; Different ARA configurations can be evaluated empirically by testing them on a conversion dataset. However, access to such data can be restricted or slow due to privacy concerns, or simply unavailable. One way to address these limitations is to use synthetic data that replicates the characteristics of real data. &lt;/p>; &lt;p>; We present a method for generating synthetic data responsibly through statistical modeling of real-world conversion datasets. We first perform an empirical analysis of real conversion datasets to uncover relevant characteristics for ARA. We then design a pipeline that uses this distribution knowledge to create a realistic synthetic dataset that can be customized via input parameters. &lt;/p>; &lt;p>; The pipeline first generates impressions drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_law&quot;>;power-law distribution&lt;/a>; (step 1), then for each impression it generates conversions drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_distribution&quot;>;Poisson distribution&lt;/a>; (step 2) and finally, for each conversion, it generates conversion values drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;>;log-normal distribution&lt;/a>; (step 3). With dataset-dependent parameters, we find that these distributions closely match ad-dataset characteristics. Thus, one can learn parameters from historical or public datasets and generate synthetic datasets for experimentation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s841 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;841&quot; data-original-width=&quot;840&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall dataset generation steps with features for illustration.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>; Experimental evaluation &lt;/h2>; &lt;p>; We evaluate our algorithms on three real-world datasets (&lt;a href=&quot; https://ailab.criteo.com/criteo-sponsored-search-conversion-log-dataset/&quot;>;Criteo&lt;/a>;, AdTech Real Estate, and AdTech Travel) and three synthetic datasets. Criteo consists of 15M clicks, Real Estate consists of 100K conversions, and Travel consists of 30K conversions. Each dataset is partitioned into a training set and a test set. The training set is used to choose contribution budgets, clipping threshold parameters, and the conversion count limit (the real-world datasets have only one conversion per click), and the error is evaluated on the test set. Each dataset is partitioned into slices using impression features. For real-world datasets, we consider three queries for each slice; for synthetic datasets, we consider two queries for each slice. &lt;/p>; &lt;p>; For each query we choose the RMSRE&lt;sub>;𝝉&lt;/sub>; 𝜏 value to be five times the median value of the query on the training dataset. This ensures invariance of the error metric to data rescaling, and allows us to combine the errors from features of different scales by using 𝝉 per each feature. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s1971/image5.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1971&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Scatter plots of real-world datasets illustrating the probability of observing a conversion value. The fitted curves represent best log-normal distribution models that effectively capture the underlying patterns in the data.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Results&lt;/h3>; &lt;p>; We compare our optimization-based algorithm to a simple baseline approach. For each query, the baseline uses an equal contribution budget and a fixed quantile of the training data to choose the clipping threshold. Our algorithms produce substantially lower error than baselines on both real-world and synthetic datasets. Our optimization-based approach adapts to the privacy budget and data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s1999/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1730&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RMSRE&lt;sub>;τ&lt;/sub>; for privacy budgets {1, 2, 4, 8, 16, 32, 64} for our algorithms and baselines on three real-world and three synthetic datasets. Our optimization-based approach consistently achieves lower error than baselines that use a fixed quantile for the clipping threshold and split the contribution budget equally among the queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We study the optimization of summary reports in the ARA, which is currently deployed on hundreds of millions of Chrome browsers. We present a rigorous formulation of the contribution budgeting optimization problem for ARA with the goal of equipping researchers with a robust abstraction that facilitates practical improvements. &lt;/p>; &lt;p>; Our recipe, which leverages historical data to bound and scale the contributions of future data under differential privacy, is quite general and applicable to settings beyond advertising. One approach based on this work is to use past data to learn the parameters of the data distribution, and then to apply synthetic data derived from this distribution for privacy budgeting for queries on future data. Please see the &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;paper&lt;/a>; and &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/ara_optimization&quot;>;accompanying code&lt;/a>; for detailed algorithms and proofs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was done in collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, and Avinash Varadarajan. We thank Akash Nadan for his help.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/68038970217457115/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot; rel=&quot;alternate&quot; title=&quot;Summary report optimization in the Privacy Sandbox Attribution Reporting API&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8288223977991952319&lt;/id>;&lt;published>;2023-12-01T09:19:00.000-08:00&lt;/published>;&lt;updated>;2023-12-05T09:38:21.187-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Translation&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Translate&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unsupervised speech-to-speech translation from monolingual data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Michelle Tadmor Ramanovich, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s1600/T3.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Speech-to-speech translation (S2ST) is a type of &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;machine translation&lt;/a>; that converts spoken language from one language to another. This technology has the potential to break down language barriers and facilitate communication between people from different cultures and backgrounds. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Previously, we introduced &lt;a href=&quot;https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html&quot;>;Translatotron 1&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html&quot;>;Translatotron 2&lt;/a>;, the first ever models that were able to directly translate speech between two languages. However they were trained in supervised settings with parallel speech data. The scarcity of parallel speech data is a major challenge in this field, so much that most public datasets are semi- or fully-synthesized from text. This adds additional hurdles to learning translation and reconstruction of speech attributes that are not represented in the text and are thus not reflected in the synthesized training data. &lt;/p>; &lt;p>; Here we present &lt;a href=&quot;https://arxiv.org/abs/2305.17547&quot;>;Translatotron 3&lt;/a>;, a novel unsupervised speech-to-speech translation architecture. In Translatotron 3, we show that it is possible to learn a speech-to-speech translation task from monolingual data alone. This method opens the door not only to translation between more language pairs but also towards translation of the non-textual speech attributes such as pauses, speaking rates, and speaker identity. Our method does not include any direct supervision to target languages and therefore we believe it is the right direction for paralinguistic characteristics (eg, such as tone, emotion) of the source speech to be preserved across translation. To enable speech-to-speech translation, we use &lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;>;back-translation&lt;/a>;, which is a technique from unsupervised machine translation (UMT) where a synthetic translation of the source language is used to &lt;a href=&quot;https://arxiv.org/abs/1710.11041&quot;>;translate texts without bilingual text datasets&lt;/a>;. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Translatotron 3 &lt;/h2>; &lt;p>; Translatotron 3 addresses the problem of unsupervised S2ST, which can eliminate the requirement for bilingual speech datasets. To do this, Translatotron 3&#39;s design incorporates three key aspects: &lt;/p>; &lt;ol>; &lt;li>;Pre-training the entire model as a &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/html /He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html&quot;>;masked autoencoder&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;>;SpecAugment&lt;/a>;, a simple data augmentation method for speech recognition that operates on the logarithmic &lt;a href=&quot;https://en.wikipedia.org/wiki/Mel-frequency_cepstrum&quot;>;mel spectogram&lt;/a>; of the input audio (instead of the raw audio itself) and is shown to effectively improve the generalization capabilities of编码器。 &lt;/li>;&lt;li>;Unsupervised embedding mapping based on &lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;multilingual unsupervised embeddings&lt;/a>; (MUSE), which is trained on unpaired languages but allows the model to learn an embedding space that is shared between the source and target languages. &lt;/li>;&lt;li>;A reconstruction loss based on back-translation, to train an encoder-decoder direct S2ST model in a fully unsupervised manner. &lt;/li>; &lt;/ol>; &lt;p>; The model is trained using a combination of the unsupervised MUSE embedding loss, reconstruction loss, and S2S back-translation loss. During inference, the shared encoder is utilized to encode the input into a multilingual embedding space, which is subsequently decoded by the target language decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Architecture&lt;/h3>; &lt;p>; Translatotron 3 employs a shared encoder to encode both the source and target语言。 The decoder is composed of a linguistic decoder, an acoustic synthesizer (responsible for acoustic generation of the translation speech), and a singular attention module, like Translatotron 2. However, for Translatotron 3 there are two decoders, one for the source language and another for the target language. During training, we use monolingual speech-text datasets (ie, these data are made up of speech-text pairs; they are &lt;em>;not&lt;/em>; translations). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Encoder&lt;/h3>; &lt;p>; The encoder has the same architecture as the speech encoder in the Translatotron 2. The output of the encoder is split into two parts: the first part incorporates semantic information whereas the second part incorporates acoustic information. By using the MUSE loss, the first half of the output is trained to be the MUSE embeddings of the text of the input speech spectrogram. The latter half is updated without the MUSE loss. It is important to note that the same encoder is shared between source and target languages. Furthermore, the MUSE embedding is multilingual in nature. As a result, the encoder is able to learn a multilingual embedding space across source and target languages. This allows a more efficient and effective encoding of the input, as the encoder is able to encode speech from both languages into a common embedding space, rather than maintaining a separate embedding space for each language. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Decoder&lt;/h3>; &lt;p>; Like Translatotron 2, the decoder is composed of three distinct components, namely the linguistic decoder, the acoustic synthesizer, and the attention module. To effectively handle the different properties of the source and target languages, however, Translatotron 3 has two separate decoders, for the source and target languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Two part training&lt;/h3>; &lt;p>; The training methodology consists of two parts: (1) auto-encoding with reconstruction and (2) a back-translation term. In the first part, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and the reconstruction loss. This phase aims to ensure that the network generates meaningful multilingual representations. In the second part, the network is further trained to translate the input spectrogram by utilizing the back-translation loss. To mitigate the issue of &lt;a href=&quot;https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=Catastrophic%20interference%2C%20also%20known%20as,information%20upon%20learning%20new%20information.&quot;>;catastrophic forgetting&lt;/a>; and enforcing the latent space to be multilingual, the MUSE loss and the reconstruction loss are also applied in this second part of training. To ensure that the encoder learns meaningful properties of the input, rather than simply reconstructing the input, we apply SpecAugment to encoder input at both phases. It has been shown to effectively improve the generalization capabilities of the encoder by augmenting the input data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; During the back-translation training phase (illustrated in the section below), the network is trained to translate the input spectrogram to the target language and then back to the source language. The goal of back-translation is to enforce the latent space to be multilingual. To achieve this, the following losses are applied: &lt;/p>; &lt;ul>; &lt;li>;MUSE loss: The MUSE loss measures the similarity between the multilingual embedding of the input spectrogram and the multilingual embedding of the back-translated spectrogram. &lt;/li>;&lt;li>;Reconstruction loss: The reconstruction loss measures the similarity between the input spectrogram and the back-translated spectrogram. &lt;/li>; &lt;/ul>; &lt;p>; In addition to these losses, SpecAugment is applied to the encoder input at both phases. Before the back-translation training phase, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and reconstruction loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MUSE loss&lt;/h3>; &lt;p>; To ensure that the encoder generates multilingual representations that are meaningful for both decoders, we employ a MUSE loss during training. The MUSE loss forces the encoder to generate such a representation by using pre-trained MUSE embeddings. During the training process, given an input text transcript, we extract the corresponding MUSE embeddings from the embeddings of the input language. The error between MUSE embeddings and the output vectors of the encoder is then minimized. Note that the encoder is indifferent to the language of the input during inference due to the multilingual nature of the embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;999&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The training and inference in Translatotron 3. Training includes the reconstruction loss via the auto-encoding path and employs the reconstruction loss via back-translation. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of direct speech-to-speech translation from Translatotron 3: &lt;/p>; &lt;h3>; Spanish-to-English (on Conversational dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left ： 汽车; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 Synthesized dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS reference (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance &lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on English and Spanish using various datasets, including the &lt;a href=&quot;https://aclanthology.org/2020.lrec-1.520/&quot;>;Common Voice 11&lt;/a>; dataset, as well as two synthesized datasets derived from the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;Conversational&lt;/a>; and Common Voice 11 datasets. &lt;/p>; &lt;p>; The translation quality was measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; (higher is better) on ASR (automatic speech recognition) transcriptions from the translated speech, compared to the corresponding reference translation text. Whereas, the speech quality is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; score (higher is better). Furthermore, the speaker similarity is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;average cosine similarity&lt;/a>; (higher is better). &lt;/p>; &lt;p>; Because Translatotron 3 is an &lt;em>;unsupervised&lt;/em>; method, as a baseline we used a cascaded S2ST system that is combined from ASR, unsupervised machine translation (UMT), and TTS (text-to -演讲）。 Specifically, we employ UMT that uses the nearest neighbor in the embedding space in order to create the translation. &lt;/p>; &lt;p>; Translatotron 3 outperforms the baseline by large margins in every aspect we measured: translation quality, speaker similarity, and speech quality. It particularly excelled on the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;conversational corpus&lt;/a>;. Moreover, Translatotron 3 achieves speech naturalness similar to that of the ground truth audio samples (measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>;, higher is better). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s1200/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Translation quality (measured by BLEU, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Speech similarity (measured by average cosine similarity between input speaker and output speaker, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Mean-opinion-score (measured by average MOS metric, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; As future work, we would like to extend the work to more languages and investigate whether zero-shot S2ST can be applied with the back-translation technique. We would also like to examine the use of back-translation with different types of speech data, such as noisy speech and low-resource languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The direct contributors to this work include Eliya Nachmani, Alon Levkovitch, Yifan Ding, Chulayutsh Asawaroengchai, Heiga Zhen, and Michelle Tadmor Ramanovich. We also thank Yu Zhang, Yuma Koizumi, Soroosh Mariooryad, RJ Skerry-Ryan, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Yonghui Wu.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8288223977991952319/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Unsupervised speech-to-speech translation from monolingual data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s72-c/T3.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1055595481112584523&lt;/id>;&lt;published>;2023-11-22T08:03:00.000-08:00&lt;/published>;&lt;updated>;2023-11-30T13:46:35.804-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;High-Performance Computing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Weather&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving simulations of clouds and their effects on climate&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Tapio Schneider, Visiting Researcher, and Yi-fan Chen, Engineering Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Today&#39;s climate models successfully capture broad global warming trends. However, because of uncertainties about processes that are &lt;a href=&quot;https://physicstoday.scitation.org/doi/abs/10.1063/PT.3.4772&quot;>;small in scale yet globally important&lt;/a>;, such as &lt;a href=&quot;http://rdcu.be/ohot&quot;>;clouds&lt;/a>; and &lt;a href=&quot;https://doi.org/10.3389/fmars.2019.00065&quot;>;ocean turbulence&lt;/a>;, these models&#39; predictions of upcoming climate changes are not very accurate in detail. For example, predictions of the time by which the global mean surface temperature of Earth will have warmed 2℃, relative to preindustrial times, &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_TS.pdf&quot;>;vary by 40–50 years&lt;/a>; (a full human generation) among today&#39;s models. As a result, we do not have the &lt;a href=&quot;https://www.nature.com/articles/s41558-020-00984-6&quot;>;accurate and geographically granular predictions&lt;/a>; we need to plan resilient infrastructure, adapt supply chains to climate disruption, and assess the risks of climate-related hazards to vulnerable communities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In large part this is because clouds dominate errors and uncertainties in climate predictions for the coming decades [&lt;a href=&quot;https://doi.org/10.1029/2005GL023851&quot;>;1&lt;/a>;, &lt;a href=&quot;https://link.springer.com/article/10.1007/s00382-013-1725-9&quot;>;2&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1038/nclimate3402&quot;>;3&lt;/a>;]. Clouds reflect sunlight and exert a &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_effect&quot;>;greenhouse effect&lt;/a>;, making them crucial for regulating Earth&#39;s energy balance and mediating the response of the climate system to changes in greenhouse gas concentrations. However, they are too small in scale to be directly resolvable in today&#39;s climate models. Current climate models resolve motions at scales of tens to a hundred kilometers, with a &lt;a href=&quot;https://link.springer.com/article/10.1186/s40645-019-0304-z&quot;>;few pushing toward&lt;/a>; the &lt;a href=&quot;https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml&quot;>;kilometer-scale&lt;/a>;. However, the turbulent air motions that sustain, for example, the low clouds that cover large swaths of tropical oceans have scales of meters to tens of meters. Because of this wide difference in scale, climate models use empirical parameterizations of clouds, rather than simulating them directly, which result in large errors and uncertainties. &lt;/p>; &lt;p>; While clouds cannot be directly resolved in global climate models, their turbulent dynamics can be simulated in limited areas by using high-resolution &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_eddy_simulation&quot;>;large eddy simulations&lt;/a>; (LES). However, the high computational cost of simulating clouds with LES has inhibited broad and systematic numerical experimentation, and it has held back the generation of large datasets for training parameterization schemes to represent clouds in coarser-resolution global climate models. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;Accelerating Large-Eddy Simulations of Clouds with Tensor Processing Units&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/journal/19422466&quot;>;Journal of Advances in Modeling Earth Systems&lt;/a>;&lt;/em>; (JAMES), and in collaboration with a &lt;a href=&quot;https://clima.caltech.edu/&quot;>;Climate Modeling Alliance&lt;/a>; (CliMA) lead who is a visiting researcher at Google, we demonstrate that &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs) — application-specific integrated circuits that were originally developed for machine learning (ML) applications — can be effectively used to perform LES of clouds. We show that TPUs, in conjunction with tailored software implementations, can be used to simulate particularly computationally challenging &lt;a href=&quot;https://en.wikipedia.org/wiki/Marine_stratocumulus&quot;>;marine stratocumulus clouds&lt;/a>; in the conditions observed during the &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;Dynamics and Chemistry of Marine Stratocumulus&lt;/a>; (DYCOMS) field study. This successful TPU-based LES code reveals the utility of TPUs, with their large computational resources and tight interconnects, for cloud simulations. &lt;/p>; &lt;p>; Climate model accuracy for critical metrics, like precipitation or the energy balance at the top of the atmosphere, has improved roughly 10% per decade in the last 20 years. Our goal is for this research to enable a 50% reduction in climate model errors by improving their representation of clouds. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-eddy simulations on TPUs&lt;/h2>; &lt;p>; In this work, we focus on stratocumulus clouds, which cover ~20% of the tropical oceans and are the most prevalent cloud type on earth. Current climate models are not yet able to reproduce stratocumulus cloud behavior correctly, which has been one of the largest sources of errors in these models. Our work will provide a much more accurate ground truth for large-scale climate models. &lt;/p>; &lt;p>; Our simulations of clouds on TPUs exhibit unprecedented computational throughput and scaling, making it possible, for example, to simulate stratocumulus clouds with 10× speedup over real-time evolution across areas up to about 35 × 54 km&lt;sup>;2&lt;/sup>;. Such domain sizes are close to the cross-sectional area of typical global climate model grid boxes. Our results open up new avenues for computational experiments, and for substantially enlarging the sample of LES available to train parameterizations of clouds for global climate models.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;8&quot; cellspacing=&quot;4&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s540/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s540/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of the cloud evolution from a simulation of a 285 x 285 x 2 km&lt;sup>;3&lt;/sup>; stratocumulus cloud sheet. This is the largest cloud sheet of its kind ever simulated. &lt;strong>;Left&lt;/strong>;: An oblique view of the cloud field with the camera cruising. &lt;strong>;Right&lt;/strong>;: Top view of the cloud field with the camera gradually pulled away.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The LES code is written in TensorFlow, an open-source software platform developed by Google for ML applications. The code takes advantage of TensorFlow&#39;s graph computation and &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;Accelerated Linear Algebra&lt;/a>; (XLA) optimizations, which enable the full exploitation of TPU hardware, including the high-speed, low-latency &lt;a href=&quot;https://patents.google.com/patent/US9372800&quot;>;inter-chip interconnects&lt;/a>; (ICI) that helped us achieve this unprecedented performance. At the same time, the TensorFlow code makes it easy to incorporate ML components directly within the physics-based fluid solver. &lt;/p>; &lt;p>; We validated the code by simulating canonical test cases for atmospheric flow solvers, such as a buoyant bubble that rises in neutral stratification, and a negatively buoyant bubble that sinks and impinges on the surface. These test cases show that the TPU-based code faithfully simulates the flows, with increasingly fine turbulent details emerging as the resolution increases. The validation tests culminate in simulations of the conditions during the DYCOMS field campaign. The TPU-based code reliably reproduces the cloud fields and turbulence characteristics observed by aircraft during a field campaign — a feat that is &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;notoriously difficult to achieve for LES&lt;/a>; because of the &lt;a href=&quot;https://doi.org/10.1029/2018MS001312&quot;>;rapid changes in temperature and other thermodynamic properties&lt;/a>; at the top of the stratocumulus decks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/s1023/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;815&quot; data-original-width=&quot;1023&quot; height=&quot;510&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/w640-h510/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One of the test cases used to validate our TPU Cloud simulator. The fine structures from the density current generated by the negatively buoyant bubble impinging on the surface are much better resolved with a high resolution grid (10m, bottom row) compared to a low resolution grid (200 m, top row).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outlook&lt;/h2>; &lt;p>; With this foundation established, our next goal is to substantially enlarge existing &lt;a href=&quot;https://doi.org/10.1029/2021MS002631&quot;>;databases&lt;/a>; of high-resolution cloud simulations that researchers building climate models can use to develop better cloud parameterizations — whether these are for physics-based models, ML models, or hybrids of the two. This requires additional physical processes beyond that described in the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;paper&lt;/a>;; for example, the need to integrate radiative transfer processes into the code. Our goal is to generate data across a variety of cloud types, eg, thunderstorm clouds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s540/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;303&quot; data-original-width=&quot;540&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of a thunderstorm simulation using the same simulator as the stratocumulus simulation work. Rainfall can also be observed near the ground.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; This work illustrates how advances in hardware for ML can be surprisingly effective when repurposed in other research areas — in this case, climate modeling. These simulations provide detailed training data for processes such as in-cloud turbulence, which are not directly observable, yet are crucially important for climate modeling and prediction. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the co-authors of the paper: Sheide Chammas, Qing Wang, Matthias Ihme, and John Anderson. We&#39;d also like to thank Carla Bromberg, Rob Carver, Fei Sha, and Tyler Russell for their insights and contributions to the work.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog .research.google/feeds/1055595481112584523/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/11/improving-simulations-of-clouds-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/1055595481112584523&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and .html&quot; rel=&quot;alternate&quot; title=&quot;Improving simulations of clouds and their effects on climate&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www .blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#缩略图&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search .yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post -3970135078050750650&lt;/id>;&lt;published>;2023-11-21T10:09:00.000-08:00&lt;/published>;&lt;updated>;2023-11-21T10:09:33.541-08:00&lt;/updated>;&lt;category scheme= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;Open sourcing Project Guideline: A platform for computer vision accessibility technology&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Hawkey, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Two years ago we &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;announced Project Guideline&lt;/a>;, a collaboration between Google Research and &lt;a href=&quot;https://www.guidingeyes.org/&quot;>;Guiding Eyes for the Blind&lt;/a>; that enabled people with visual impairments (eg, blindness and low-vision) to walk, jog, and run independently. Using only a Google Pixel phone and headphones, Project Guideline leverages on-device machine learning (ML) to navigate users along outdoor paths marked with a painted line. The technology has been &lt;a href=&quot;https://projectguidelinejp.withgoogle.com/intl/en/&quot;>;tested all over the world&lt;/a>; and even demonstrated during the &lt;a href=&quot;https://www.youtube.com/live/2cW1-plwqeQ?si=MTIX2uJkyWuLluht&amp;amp;t=7334&quot;>;opening ceremony at the Tokyo 2020 Paralympic Games&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Since the original announcement, we set out to improve Project Guideline by embedding new features, such as obstacle detection and advanced path planning, to safely and reliably navigate users through more complex scenarios (such as sharp turns and nearby pedestrians). The early version featured a simple frame-by-frame image segmentation that detected the position of the path line relative to the image frame. This was sufficient for orienting the user to the line, but provided limited information about the surrounding environment. Improving the navigation signals, such as alerts for obstacles and upcoming turns, required a much better understanding and mapping of the users&#39; environment. To solve these challenges, we built a platform that can be utilized for a variety of spatially-aware applications in the accessibility space and beyond. &lt;/p>; &lt;p>; Today, we announce the &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;open source release of Project Guideline&lt;/a>;, making it available for anyone to use to improve upon and build new accessibility experiences. The release includes &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;source code&lt;/a>; for the core platform, an &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/android&quot;>;Android application&lt;/a>;, pre-trained &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/vision/models&quot;>;ML models&lt;/a>;, and a &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/unreal&quot;>;3D simulation framework&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;System design&lt;/h2>; &lt;p>; The primary use-case is an Android application, however we wanted to be able to run, test, and debug the core logic in a variety of environments in a reproducible way. This led us to design and build the system using C++ for close integration with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; and other core libraries, while still being able to integrate with Android using the &lt;a href=&quot;https://developer.android.com/ndk&quot;>;Android NDK&lt;/a>;. &lt;/p>; &lt;p>; Under the hood, Project Guideline uses &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; to estimate the position and orientation of the user as they navigate the课程。 A segmentation model, built on the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLabV3+&lt;/a>; framework, processes each camera frame to generate a binary mask of the guideline (see the &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;previous blog post&lt;/a>; for more details). Points on the segmented guideline are then projected from image-space coordinates onto a world-space ground plane using the camera pose and lens parameters (intrinsics) provided by ARCore. Since each frame contributes a different view of the line, the world-space points are aggregated over multiple frames to build a virtual mapping of the real-world guideline. The system performs piecewise curve approximation of the guideline world-space coordinates to build a spatio-temporally consistent trajectory. This allows refinement of the estimated line as the user progresses along the path. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s800/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;355&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Project Guideline builds a 2D map of the guideline, aggregating detected points in each frame (&lt;strong>;red&lt;/strong>;) to build a stateful representation (&lt;strong>;blue&lt;/strong>;) as the runner progresses along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A control system dynamically selects a target point on the line some distance ahead based on the user&#39;s current position, velocity,和方向。 An audio feedback signal is then given to the user to adjust their heading to coincide with the upcoming line segment. By using the runner&#39;s velocity vector instead of camera orientation to compute the navigation signal, we eliminate noise caused by irregular camera movements common during running. We can even navigate the user back to the line while it&#39;s out of camera view, for example if the user overshot a turn. This is possible because ARCore continues to track the pose of the camera, which can be compared to the stateful line map inferred from previous camera images. &lt;/p>; &lt;p>; Project Guideline also includes obstacle detection and avoidance features. An ML model is used to estimate depth from single images. To train this monocular depth model, we used &lt;a href=&quot;https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot;>;SANPO&lt;/a>;, a large dataset of outdoor imagery from urban, park, and suburban environments that was curated in-house. The model is capable of detecting the depth of various obstacles, including people, vehicles, posts, and more. The depth maps are converted into 3D point clouds, similar to the line segmentation process, and used to detect the presence of obstacles along the user&#39;s path and then alert the user through an audio signal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/w640-h480/image2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using a monocular depth ML model, Project Guideline constructs a 3D point cloud of the environment to detect and alert the user of potential obstacles along the path .&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A low-latency audio system based on the &lt;a href=&quot;https://developer.android.com/ndk/guides/audio/ aaudio/aaudio&quot;>;AAudio API&lt;/a>; was implemented to provide the navigational sounds and cues to the user. Several &lt;em>;sound packs&lt;/em>; are available in Project Guideline, including a spatial sound implementation using the &lt;a href=&quot;https://resonance-audio.github.io/resonance-audio/&quot;>;Resonance Audio API&lt; /a>;. The sound packs were developed by a team of sound researchers and engineers at Google who designed and tested many different sound models. The sounds use a combination of panning, pitch, and spatialization to guide the user along the line. For example, a user veering to the right may hear a beeping sound in the left ear to indicate the line is to the left, with increasing frequency for a larger course correction. If the user veers further, a high-pitched warning sound may be heard to indicate the edge of the path is approaching. In addition, a clear “stop” audio cue is always available in the event the user veers too far from the line, an anomaly is detected, or the system fails to provide a navigational signal. &lt;/p>; &lt;p>; Project Guideline has been built specifically for Google Pixel phones with the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Google Tensor&lt;/a>; chip. The Google Tensor chip enables the optimized ML models to run on-device with higher performance and lower power consumption. This is critical for providing real-time navigation instructions to the user with minimal delay. On a Pixel 8 there is a 28x latency improvement when running the depth model on the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Tensor Processing Unit&lt;/a>; (TPU) instead of CPU, and 9x improvement compared to GPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s1124/image3 .jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;748&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Testing and simulation&lt;/h2>; &lt;p>; Project Guideline includes a simulator that enables rapid testing and prototyping of the system in a virtual environment. Everything from the ML models to the audio feedback system runs natively within the simulator, giving the full Project Guideline experience without needing all the hardware and physical environment set up. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/s1200/image4.jpg&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1200&quot; height=&quot;480&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/w640-h480/image4.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of Project Guideline simulator.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; To launch the technology forward, &lt;a href=&quot;https://www.wear.works&quot;>;WearWorks &lt;/a>; has become an early adopter and teamed up with Project Guideline to integrate their patented haptic navigation experience, utilizing haptic feedback in addition to sound to guide runners. WearWorks has been developing haptics for over 8 years, and previously empowered the first blind marathon runner to complete the NYC Marathon without sighted assistance. We hope that integrations like these will lead to new innovations and make the world a more accessible place.&lt;br />; &lt;/p>; &lt;p>; The Project Guideline team is also working towards removing the painted line completely, using the latest advancements in mobile ML technology, such as the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;ARCore Scene Semantics API&lt;/a>;, which can identify sidewalks, buildings, and other objects in outdoor scenes. We invite the accessibility community to build upon and improve this technology while exploring new use cases in other fields. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of Project Guideline and the technologies behind it. We&#39;d like to thank Project Guideline team members: Dror Avalon, Phil Bayer, Ryan Burke, Lori Dooley, Song Chun Fan, Matt Hall, Amélie Jean-aimée, Dave Hawkey, Amit Pitaru, Alvin Shi, Mikhail Sirotenko, Sagar Waghmare, John Watkinson, Kimberly Wilber, Matthew Willson, Xuan Yang, Mark Zarich, Steven Clark, Jim Coursey, Josh Ellis, Tom Hoddes, Dick Lyon, Chris Mitchell, Satoru Arao, Yoojin Chung, Joe Fry, Kazuto Furuichi, Ikumi Kobayashi, Kathy Maruyama, Minh Nguyen, Alto Okamura, Yosuke Suzuki, and Bryan Tanaka. Thanks to ARCore contributors: Ryan DuToit, Abhishek Kar, and Eric Turner. Thanks to Alec Go, Jing Li, Liviu Panait, Stefano Pellegrini, Abdullah Rashwan, Lu Wang, Qifei Wang, and Fan Yang for providing ML platform support. We&#39;d also like to thank Hartwig Adam, Tomas Izo, Rahul Sukthankar, Blaise Aguera y Arcas, and Huisheng Wang for their leadership support. Special thanks to our partners Guiding Eyes for the Blind and Achilles International.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3970135078050750650/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html&quot; rel=&quot;alternate&quot; title=&quot;Open sourcing Project Guideline: A platform for computer vision accessibility technology&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6563276834599281497&lt;/id>;&lt;published>;2023-11-17T11:36:00.000-08:00&lt;/published>;&lt;updated>;2023-11-17T11:36:32.036-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research Awards&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;University Relations&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Emerging practices for Society-Centered AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Research Director, Technology &amp;amp; Society, and Yossi Matias, Vice President, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The first of &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>; is to “Be socially beneficial.” As AI practitioners, we&#39;re inspired by the transformative potential of AI technologies to benefit society and our shared environment at a scale and swiftness that wasn&#39;t possible before. From &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;helping address the climate crisis&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/how-were-using-ai-to-help-transform-healthcare/&quot;>;helping transform healthcare&lt;/a>;, to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/global-accessibility-awareness-day-google-product-update/&quot;>;making the digital world more accessible&lt;/a>;, our goal is to apply AI responsibly to be helpful to more people around the globe. Achieving global scale requires researchers and communities to think ahead — and act — collectively across the AI ecosystem. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We call this approach Society-Centered AI. It is both an extension and an expansion of &lt;a href=&quot;https://hcil.umd.edu/human-centered-ai/&quot;>;Human-Centered AI, &lt;/a>;focusing on the aggregate needs of society that are still informed by the needs of individual users, specifically within the context of the larger, shared human experience. Recent AI advances offer unprecedented, societal-level capabilities, and we can now methodically address those needs — if we apply collective, multi-disciplinary AI research to society-level, shared challenges, from forecasting hunger to predicting diseases to improving productivity. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/AI_Opportunity_Agenda.pdf&quot;>;opportunity for AI&lt;/a>; to benefit society increases each天。 We took a look at our work in these areas and at the research projects we have supported. Recently, Google &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;announced that 70 professors were selected&lt;/a>; for the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;2023 Award for Inclusion Research Program&lt;/a>;, which supports academic research that addresses the needs of historically marginalized groups globally. Through evaluation of this work, we identified a few emerging practices for Society-Centered AI: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Understand society&#39;s needs&lt;/strong>; &lt;br />;Listening to communities and partners is crucial to understanding major issues deeply and identifying priority challenges to address. As an emerging general purpose technology, AI has the potential to address major global societal issues that can significantly impact people&#39;s lives (eg, educating workers, improving healthcare, and improving productivity). We have found the key to impact is to be centered on society&#39;s needs. For this, we focus our efforts on goals society has agreed should be prioritized, such as the United Nations&#39; &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/globalgoals&quot;>;17 Sustainable Development Goals&lt;/a>;, a set of interconnected goals jointly developed by more than 190 countries to address global challenges. &lt;/li>;&lt;li>;&lt;strong>;Collective efforts to address those needs &lt;br />;&lt;/strong>;Collective efforts bring stakeholders (eg, local and academic communities, NGOs, private-public collaborations) into a joint process of design, development, implementation, and evaluation of AI technologies as they are being developed and deployed to address societal needs. &lt;/li>;&lt;li>;&lt;strong>;Measuring success by how well the effort addresses society&#39;s needs &lt;br />;&lt;/strong>;It is important and challenging to measure how well AI solutions address society&#39;s needs. In each of our cases, we identified primary and secondary indicators of impact that we optimized through our collaborations with stakeholders. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why is Society-Centered AI important?&lt;/h2>; &lt;p>; The case examples described below show how the Society-Centered AI approach has led to impact across topics, such as accessibility, health, and climate. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Understanding the needs of individuals with non-standard speech&lt;/h3>; &lt;p>; There are &lt;a href=&quot;https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language&quot;>;millions of people&lt;/a>; with non-standard speech (eg, impaired articulation, &lt;a href=&quot;https://en.wikipedia.org/wiki/Dysarthria&quot;>;dysarthria&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Spasmodic_dysphonia&quot;>;dysphonia&lt;/a>;)仅在美国。 In &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/&quot;>;2019&lt;/a>;, Google Research launched &lt;a href=&quot;https://blog.research.google/2019/08/project-euphonias-personalized-speech.html&quot;>;Project Euphonia&lt;/a>;, a methodology that allows individual users with non-standard speech to train personalized speech recognition models. Our success began with the impact we had on each individual who is now able to use voice dictation on their mobile device. &lt;/p>; &lt;p>; Euphonia started with a Society-Centered AI approach, including collective efforts with the non-profit organizations &lt;a href=&quot;http://als.net/&quot;>;ALS Therapy Development Institute&lt;/a>; and &lt;a href=&quot;http://www.alsri.org/&quot;>;ALS Residence Initiative&lt;/a>; to understand the needs of individuals with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;amyotrophic lateral sclerosis&lt;/a>; (ALS) and their ability to use automatic speech recognition systems. Later, we developed &lt;a href=&quot;https://blog.research.google/2021/09/personalized-asr-models-from-large-and.html&quot;>;the world&#39;s largest corpus&lt;/a>; of non-standard speech recordings, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize disordered speech by 37%&lt;/a>; on real conversation &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;word error rate&lt;/a>; (WER) measurement. This also led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/&quot;>;2022&lt;/a>; collaboration between the University of Illinois Urbana-Champaign, Alphabet, Apple, Meta, Microsoft, and Amazon to begin the &lt;a href=&quot;https://speechaccessibilityproject.beckman.illinois.edu/&quot;>;Speech Accessibility Project&lt;/a>;, an ongoing initiative to create a publicly available dataset of disordered speech samples to improve products and make speech recognition more inclusive of diverse speech patterns. Other technologies that use AI to help remove barriers of modality and languages, include &lt;a href=&quot;https://about.google/stories/making-conversation-more-accessible-with-live-transcribe/&quot;>;live transcribe&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/first-time-i-was-able-call-my-23-year-old-son/&quot;>;live caption&lt;/a>; and &lt;a href=&quot;https://blog.google/intl/en-in/products/explore-communicate/easier-access-to-web-pages-let/&quot;>;read aloud&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Focusing on society&#39;s health needs&lt;/h3>; &lt;p>; Access to timely maternal health information can save lives globally: &lt;a href=&quot;https://www.who.int/news/item/23-02-2023-a-woman-dies-every-two-minutes-due-to-pregnancy-or-childbirth--un-agencies&quot;>;every two minutes a woman dies during pregnancy or childbirth&lt;/a>; and &lt;a href=&quot;https://data.unicef.org/topic/child-survival/under-five-mortality/&quot;>;1 in 26 children die before reaching age five&lt;/a>;. In rural India, the education of expectant and new mothers around key health issues pertaining to pregnancy and infancy required scalable, low-cost technology solutions. Together with &lt;a href=&quot;https://armman.org/&quot;>;ARMMAN&lt;/a>;, Google Research supported &lt;a href=&quot;https://blog.research.google/2022/08/using-ml-to-boost-engagement-with.html&quot;>;a program&lt;/a>; that uses mobile messaging and machine learning (ML) algorithms to predict when women might benefit from receiving interventions (ie, targeted preventative care information) and encourages them to engage with the &lt;a href=&quot;https://armman.org/mmitra/&quot;>;mMitra&lt;/a>; free voice call program. Within a year, the mMitra program has shown a 17% increase in infants with tripled birth weight and a 36% increase in women understanding the importance of taking iron tablets during pregnancy. Over 175K mothers and growing have been reached through this automated solution, which public health workers use to improve the quality of information delivery. &lt;/p>; &lt;p>; These efforts have been successful in improving health due to the close collective partnership among the community and those building the AI technology. We have adopted this same approach via collaborations with caregivers to address a variety of medical needs. Some examples include: the use of the &lt;a href=&quot;https://health.google/caregivers/arda/&quot;>;Automated Retinal Disease Assessment&lt;/a>; (ARDA) to &lt;a href=&quot;https://blog.google/technology/health/5-myths-about-medical-ai-debunked/&quot;>;help screen for diabetic retinopathy&lt;/a>; in 250,000 patients in clinics around the world; our partnership with &lt;a href=&quot;https://www.icadmed.com/&quot;>;iCAD&lt;/a>; to bring our &lt;a href=&quot;https://blog.google/technology/ai/icad-partnership-breast-cancer-screening/&quot;>;mammography&lt;/a>; AI models to clinical settings to aid in breast cancer detection; and the development of &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>;, a medical large language model that is now being &lt;a href=&quot;https: //cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;tested with Cloud partners&lt;/a>; to help doctors provide better病人护理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Compounding impact from sustained efforts for crisis response&lt;/h3>; &lt;p>; Google Research&#39;s flood prediction efforts began in 2018 with &lt;a href=&quot;https://www.blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;flood forecasting&lt;/a>; in India and &lt;a href=&quot;https://www.undp.org/bangladesh/blog/climate-change-google-and-bangladesh-floods&quot;>;expanded to Bangladesh&lt;/a>; to help combat the catastrophic damage from yearly floods. The initial efforts began with partnerships with &lt;a href=&quot;https://cwc.gov.in/&quot;>;India&#39;s Central Water Commission&lt;/a>;, local governments and communities. The implementation of these efforts used &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>; on Search and Maps, and, more recently, broadly expanded access via &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>;. Continued &lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;collaborations&lt;/a>; and advancing an AI-based global flood forecasting model allowed us to &lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;expand this capability&lt;/a>; to &lt;a href=&quot;https://blog .google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;over 80 countries&lt;/a>; across Africa, the Asia-Pacific region, Europe, and South, Central, and North美国。 We also partnered with networks of community volunteers to further amplify flood alerts. By working with governments and communities to measure the impact of these efforts on society, we refined our approach and algorithms each year. &lt;/p>; &lt;p>; We were able to leverage those methodologies and some of the underlying technology, such as &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>;, from &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;flood forecasting&lt;/a>; to similar societal needs, such as &lt;a href=&quot;https://blog.google/products/search/mapping-wildfires-with-satellite-data/&quot;>;wildfire forecasting&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;heat alerts&lt;/a>;. Our continued engagements with organizations led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/&quot;>;support of additional efforts&lt;/a>;, such as the World Meteorological Organization&#39;s (WMO) &lt;a href=&quot;https://public.wmo.int/en/earlywarningsforall&quot;>;Early Warnings For All Initiative&lt;/a>;. The continued engagement with communities has allowed us to learn about our users&#39; needs on a societal level over time, expand our efforts, and compound the societal reach and impact of our efforts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Further supporting Society-Centered AI research&lt;/h2>; &lt;p>; &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;We recently funded&lt;/a>; 18 university research proposals exemplifying a Society-Centered AI approach, a new track within the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;Google Award for Inclusion Research Program&lt;/a>;. These researchers are taking the Society-Centered AI methodology and helping create beneficial applications across the world. Examples of some of the projects funded include: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;AI-Driven Monitoring of Attitude Polarization in Conflict-Affected Countries for Inclusive Peace Process and Women&#39;s Empowerment:&lt;/strong>; This project&#39;s goal is to create LLM-powered tools that can be used to monitor peace in online conversations in developing nations. The initial target communities are where peace is in flux and the effort will put a particular emphasis on mitigating polarization that impacts women and promoting harmony.&lt;/li>; &lt;li>;&lt;strong>;AI-Assisted Distributed Collaborative Indoor Pollution Meters: A Case Study , Requirement Analysis, and Low-Cost Healthy Home Solution for Indian Communities: &lt;/strong>;This project is looking at the usage of low-cost pollution monitors combined with AI-assisted methodology for identifying recommendations for communities to improve air quality and at home健康。 The initial target communities are highly impacted by pollution, and the joint work with them includes the goal of developing how to measure improvement in outcomes in the local community. &lt;/li>; &lt;li>;&lt;strong>;Collaborative Development of AI Solutions for Scaling Up Adolescent Access to Sexual and Reproductive Health Education and Services in Uganda: &lt;/strong>;This project&#39;s goal is to create LLM-powered tools to provide personalized coaching and learning for users&#39; needs on topics of sexual and reproductive health education in low-income settings in Sub-Saharan Africa. The local societal need is significant, with an estimated &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9652643/&quot;>;25% rate of teenage pregnancy&lt;/a>;, and the project aims to address the needs with a collective development process for the AI solution.&lt;strong>; &lt;/strong>; &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; Focusing on society&#39;s needs, working via multidisciplinary collective research, and measuring the impact on society helps lead to AI solutions that are relevant, long-lasting, empowering, and beneficial. See the &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/&quot;>;AI for the Global Goals&lt;/a>; to learn more about potential Society-Centered AI research problems. &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;Our efforts&lt;/a>; with non-profits in these areas is complementary to the research that we are doing and encouraging. We believe that further initiatives using Society-Centered AI will help the collective research community solve problems and positively impact society at large. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the many individuals who have worked on these projects at Google including Shruti Sheth, Reena Jana, Amy Chung-Yu Chou, Elizabeth Adkison, Sophie Allweis, Dan Altman, Eve Andersson, Ayelet Benjamini&lt;/em>;, &lt;em>;Julie Cattiau, Yuval Carny, Richard Cave, Katherine Chou, Greg Corrado, Carlos De Segovia, Remi Denton, Dotan Emanuel, Ashley Gardner, Oren Gilon, Taylor Goddu, Brigitte Hoyer Gosselink, Jordan Green, Alon Harris&lt;/em>;, &lt;em>;Avinatan Hassidim, Rus Heywood, Sunny Jansen, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Ronit Levavi Morad, Bob MacDonald, Alicia Martin, Shakir Mohamed, Philip Nelson, Moriah Royz, Katie Seaver, Joel Shor, Milind Tambe, Aparna Taneja, Divy Thakkar, Jimmy Tobin, Katrin Tomanek, Blake Walsh, Gal Weiss, Kasumi Widner, Lihong Xi, and teams.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6563276834599281497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot; rel=&quot;alternate&quot; title=&quot;Emerging practices for Society-Centered AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6928081948246813203&lt;/id>;&lt;published>;2023-11-16T13:11:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T08:16:11.190-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Adversarial testing for generative AI safety&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kathy Meier-Hellstern, Building Responsible AI &amp;amp; Data Systems, Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The&lt;em>; &lt;/em>;&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research is committed to advancing the theory and practice of responsible human-centered AI through a lens of culturally-aware research, to meet the needs of billions of users today, and blaze the path forward for a better AI future. The BRAIDS (Building Responsible AI Data and Solutions) team within RAI-HCT aims to simplify the adoption of RAI practices through the utilization of scalable tools, high-quality data, streamlined processes, and novel research with a current emphasis on addressing the unique challenges posed by &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; (GenAI). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; GenAI models have enabled unprecedented capabilities leading to a rapid surge of innovative applications. Google actively leverages &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;GenAI&lt;/a>; to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview&quot;>;enhance&lt;/a>; its &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai-in-workspace-now-available&quot;>;products&#39; utility&lt;/a>; and to improve lives. While enormously beneficial, GenAI also presents risks for disinformation, bias, and security. In 2018, Google pioneered the &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>;, emphasizing beneficial use and prevention of harm. Since then, Google has focused on effectively implementing our principles in &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; through 1) a comprehensive risk assessment framework, 2) internal governance structures, 3) education, empowering Googlers to integrate AI Principles into their work, and 4) the development of processes and tools that identify, measure, and analyze ethical risks throughout the lifecycle of AI-powered products. The BRAIDS team focuses on the last area, creating tools and techniques for identification of ethical and safety risks in GenAI products that enable teams within Google to apply appropriate mitigations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;What makes GenAI challenging to build responsibly?&lt;/h2>; &lt;p>; The unprecedented capabilities of GenAI models have been accompanied by a new spectrum of potential failures, underscoring the urgency for a comprehensive and systematic RAI approach to understanding and mitigating potential safety concerns before the model is made broadly available. One key technique used to understand potential risks is &lt;em>;adversarial testing&lt;/em>;, which is testing performed to systematically evaluate the models to learn how they behave when provided with malicious or inadvertently harmful inputs across a range of scenarios. To that end, our research has focused on three directions: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Scaled adversarial data generation&lt;/em>;&lt;br />; Given the diverse user communities, use cases, and behaviors, it is difficult to comprehensively identify critical safety issues prior to launching a product or service. Scaled adversarial data generation with humans-in-the-loop addresses this need by creating test sets that contain a wide range of diverse and potentially unsafe model inputs that stress the model capabilities under adverse circumstances. Our unique focus in BRAIDS lies in identifying societal harms to the diverse user communities impacted by our models. &lt;/li>; &lt;li>;&lt;em>;Automated test set evaluation and community engagement&lt;/em>;&lt;br />; Scaling the testing process so that many thousands of model responses can be quickly evaluated to learn how the model responds across a wide range of potentially harmful scenarios is aided with automated test set evaluation. Beyond testing with adversarial test sets, community engagement is a key component of our approach to identify “unknown unknowns” and to seed the data generation process.&lt;/li>; &lt;li>;&lt;em>;Rater diversity&lt;/em>;&lt;br />; Safety evaluations rely on human judgment, which is shaped by community and culture and is not easily automated. To address this, we prioritize research on rater diversity.&lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaled adversarial data generation&lt;/h2>; &lt;p>; High-quality, comprehensive data underpins many key programs across Google. Initially reliant on manual data generation, we&#39;ve made significant strides to automate the adversarial data generation process. A centralized data repository with use-case and policy-aligned prompts is available to jump-start the generation of new adversarial tests. We have also developed multiple synthetic data generation tools based on large language models (LLMs) that prioritize the generation of data sets that reflect diverse societal contexts and that integrate data quality metrics for improved dataset quality and diversity. &lt;/p>; &lt;p>; Our data quality metrics include: &lt;/p>; &lt;ul>; &lt;li>;Analysis of language styles, including query length, query similarity, and diversity of language styles.&lt;/li>; &lt;li>;Measurement across a wide range of societal and multicultural dimensions, leveraging datasets such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>;, &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;, the &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;Societal Context Repository&lt;/a>;.&lt;/li>; &lt;li>;Measurement of alignment with Google&#39;s &lt;a href=&quot;https://policies.google.com/terms/generative-ai/use-policy&quot;>;generative AI policies&lt;/a>; and intended use cases.&lt;/li>; &lt;li>;Analysis of adversariality to ensure that we examine both explicit (the input is clearly designed to produce an unsafe output) and implicit (where the input is innocuous but the output is harmful) queries. &lt;/li>; &lt;/ul>; &lt;p>; One of our approaches to scaled data generation is exemplified in our paper on &lt;a href=&quot;https://arxiv.org/abs/2311.08592&quot;>;AI-Assisted Red Teaming&lt;/a>; (AART). AART generates evaluation datasets with high diversity (eg, sensitive and harmful concepts specific to a wide range of cultural and geographic regions), steered by AI-assisted recipes to define, scope and prioritize diversity within an application context. Compared to some state-of-the-art tools, AART shows promising results in terms of concept coverage and data quality. Separately, we are also working with MLCommons to contribute to &lt;a href=&quot;https://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot;>;public benchmarks for AI Safety&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adversarial testing and community insights &lt;/h2>; &lt;p>; Evaluating model output with adversarial test sets allows us to identify critical safety issues prior to deployment. Our initial evaluations relied exclusively on human ratings, which resulted in slow turnaround times and inconsistencies due to a lack of standardized safety definitions and policies. We have improved the quality of evaluations by introducing policy-aligned rater guidelines to improve human rater accuracy, and are researching additional improvements to better reflect the perspectives of diverse communities. Additionally, automated test set evaluation using LLM-based auto-raters enables efficiency and scaling, while allowing us to direct complex or ambiguous cases to humans for expert rating. &lt;/p>; &lt;p>; Beyond testing with adversarial test sets, gathering community insights is vital for continuously discovering “unknown unknowns”. To provide high quality human input that is required to seed the scaled processes, we partner with groups such as the &lt;a href=&quot;https://sites.google.com/corp/google.com/earr-external-research-group/home&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR), and with our internal ethics and analysis teams to ensure that we are representing the diverse communities who use our models. The &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;Adversarial Nibbler Challenge&lt;/a>; engages external users to understand potential harms of &lt;a href=&quot;https://arxiv.org/abs/2305.14384&quot;>;unsafe, biased or violent outputs&lt;/a>; to end users at scale. Our continuous commitment to community engagement includes gathering feedback from diverse communities and collaborating with the research community, for example during &lt;a href=&quot;https://sites.google.com/view/art-of-safety&quot;>;The ART of Safety workshop&lt;/a>; at the &lt;a href=&quot;http://www.ijcnlp-aacl2023.org/&quot;>;Asia-Pacific Chapter of the Association for Computational Linguistics Conference&lt;/a>; (IJCNLP-AACL 2023) to address adversarial testing challenges for GenAI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Rater diversity in safety evaluation&lt;/h2>; &lt;p>; Understanding and mitigating GenAI safety risks is both a technical and social challenge. Safety perceptions are intrinsically subjective and influenced by a wide range of intersecting factors. Our in-depth study on demographic influences on safety perceptions explored the &lt;a href=&quot;https://arxiv.org/abs/2306.11530&quot;>;intersectional effects of rater demographics&lt;/a>; (eg, race/ethnicity, gender, age) and content characteristics (eg, degree of harm) on safety assessments of GenAI outputs. Traditional approaches largely ignore inherent subjectivity and the systematic disagreements among raters, which can mask important cultural differences. Our &lt;a href=&quot;https://arxiv.org/abs/2311.05074&quot;>;disagreement analysis framework&lt;/a>; surfaced a variety of disagreement patterns between raters from diverse backgrounds including also with “ground truth” expert ratings. This paves the way to new approaches for assessing quality of human annotation and model evaluations beyond the simplistic use of gold labels. Our &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;NeurIPS 2023 publication&lt;/a>; introduces the &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot;>;DICES&lt;/a>; (Diversity In Conversational AI Evaluation for Safety) dataset that facilitates nuanced safety evaluation of LLMs and accounts for variance, ambiguity, and diversity in various cultural contexts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Summary&lt;/h2>; &lt;p>; GenAI has resulted in a technology transformation, opening possibilities for rapid development and customization even without coding. However, it also comes with a risk of generating harmful outputs. Our proactive adversarial testing program identifies and mitigates GenAI risks to ensure inclusive model behavior. Adversarial testing and red teaming are essential components of a Safety strategy, and conducting them in a comprehensive manner is essential. The rapid pace of innovation demands that we constantly challenge ourselves to find “unknown unknowns” in cooperation with our internal partners, diverse user communities, and other industry experts. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6928081948246813203/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Adversarial testing for generative AI safety&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1338549955376716163&lt;/id>;&lt;published>;2023-11-14T12:28:00.000-08:00&lt;/published>;&lt;updated>;2023-11-14T14:09:51.250-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Scaling multimodal understanding to long videos&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Isaac Noble, Software Engineer, Google Research, and Anelia Angelova, Research Scientist, Google DeepMind &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s1600/mirasol.png&quot; style=&quot;display: none;&quot; />; &lt;p>; When building machine learning models for real-life applications, we need to consider inputs from multiple modalities in order to capture various aspects of the world around us. For example, audio, video, and text all provide varied and complementary information about a visual input. However, building multimodal models is challenging due to the heterogeneity of the modalities. Some of the modalities might be well synchronized in time (eg, audio, video) but not aligned with text. Furthermore, the large volume of data in video and audio signals is much larger than that in text, so when combining them in multimodal models, video and audio often cannot be fully consumed and need to be disproportionately compressed. This problem is exacerbated for longer video inputs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.05698&quot;>;Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities&lt;/a>;”, we introduce a multimodal &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; model (Mirasol3B) for learning across audio, video, and text方式。 The main idea is to decouple the multimodal modeling into separate focused autoregressive models, processing the inputs according to the characteristics of the modalities. Our model consists of an autoregressive component for the time-synchronized modalities (audio and video) and a separate autoregressive component for modalities that are not necessarily time-aligned but are still sequential, eg, text inputs, such as a title or description. Additionally, the time-aligned modalities are partitioned in time where local features can be jointly learned. In this way, audio-video inputs are modeled in time and are allocated comparatively more parameters than prior works. With this approach, we can effortlessly handle much longer videos (eg, 128-512 frames) compared to other multimodal models. At 3B parameters, Mirasol3B is compact compared to prior &lt;a href=&quot;https://arxiv.org/abs/2204.14198&quot;>;Flamingo&lt;/a>; (80B) and &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI-X&lt;/a>; (55B) models. Finally, Mirasol3B outperforms the state-of-the-art approaches on &lt;a href=&quot;https://blog.research.google/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (video QA), long video QA, and audio-video-text benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s1240/image5.gif &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1240&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Mirasol3B architecture consists of an autoregressive model for the time-aligned modalities (audio and video), which are partitioned in chunks, and a separate autoregressive model for the unaligned context modalities (eg, text). Joint feature learning is conducted by the Combiner, which learns compact but sufficiently informative features, allowing the processing of long video/audio inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Coordinating time-aligned and contextual modalities&lt;/h2>; &lt;p>; Video, audio and text are diverse modalities with distinct characteristics. For example, video is a spatio-temporal visual signal with 30–100 frames per second, but due to the large volume of data, typically only 32–64 frames &lt;em>;per video&lt;/em>; are consumed by current models. Audio is a one-dimensional temporal signal obtained at much higher frequency than video (eg, at 16 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hertz&quot;>;Hz&lt;/a>;), whereas text inputs that apply to the whole video, are typically 200–300 word-sequence and serve as a context to the audio-video inputs. To that end, we propose a model consisting of an autoregressive component that fuses and jointly learns the time-aligned signals, which occur at high frequencies and are roughly synchronized, and another autoregressive component for processing non-aligned signals. Learning between the components for the time-aligned and contextual modalities is coordinated via cross-attention mechanisms that allow the two to exchange information while learning in a sequence without having to synchronize them in time. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-aligned autoregressive modeling of video and audio&lt;/h2>; &lt;p>; Long videos can convey rich information and activities happening in a sequence. However, present models approach video modeling by extracting all the information at once, without sufficient temporal information. To address this, we apply an autoregressive modeling strategy where we condition jointly learned video and audio representations for one time interval on feature representations from previous time intervals. This preserves temporal information. &lt;/p>; &lt;p>; The video is first partitioned into smaller video chunks. Each chunk itself can be 4–64 frames. The features corresponding to each chunk are then processed by a learning module, called the Combiner (described below), which generates a joint audio and video feature representation at the current step — this step extracts and compacts the most important information per chunk. Next, we process this joint feature representation with an autoregressive Transformer, which applies attention to the previous feature representation and generates the joint feature representation for the next step. Consequently, the model learns how to represent not only each individual chunk, but also how the chunks relate temporally. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s1259/image1.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1259&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use an autoregressive modeling of the audio and video inputs, partitioning them in time and learning joint feature representations, which are then autoregressively learned in sequence.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Modeling long videos with a modality combiner&lt;/h2>; &lt; p>; To combine the signals from the video and audio information in each video chunk, we propose a learning module called the Combiner. Video and audio signals are aligned by taking the audio inputs that correspond to a specific video timeframe. We then process video and audio inputs spatio-temporally, extracting information particularly relevant to &lt;em>;changes in the inputs&lt;/em>; (for videos we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>;, and for audio we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;>;spectrogram&lt;/a>; representation, both of which are processed by a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer)&lt;/a>;. We concatenate and input these features to the Combiner, which is designed to learn a new feature representation capturing both these inputs. To address the challenge of the large volume of data in video and audio signals, another goal of the Combiner is to reduce the dimensionality of the joint video/audio inputs, which is done by selecting a smaller number of output features to be produced. The Combiner can be implemented simply as a causal Transformer, which processes the inputs in the direction of time, ie, using only inputs of the prior steps or the current one. Alternatively, the Combiner can have a learnable memory, described below. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Combiner styles&lt;/h2>; &lt;p>; A simple version of the Combiner adapts a Transformer architecture. More specifically, all audio and video features from the current chunk (and optionally prior chunks) are input to a Transformer and projected to a lower dimensionality, ie, a smaller number of features are selected as the output “combined” features. While Transformers are not typically used in this context, we find it effective for reducing the dimensionality of the input features, by selecting the last &lt;em>;m&lt;/em>; outputs of the Transformer, if &lt;em>;m&lt;/em>; is the desired output dimension (shown below). Alternatively, the Combiner can have a memory component. For example, we use the &lt;a href=&quot;https://arxiv.org/abs/2211.09119&quot;>;Token Turing Machine&lt;/a>; (TTM), which supports a differentiable memory unit, accumulating and compressing features from all previous timesteps 。 Using a fixed memory allows the model to work with a more compact set of features at every step, rather than process all the features from previous steps, which reduces computation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s1798/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;1798&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a simple Transformer-based Combiner (&lt;b>;left&lt;/b>;) and a Memory Combiner (&lt;b>;right&lt;/b>;) , based on the Token Turing Machine (TTM), which uses memory to compress previous history of features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our approach on several benchmarks, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp- content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1906.02467&quot;>;ActivityNet-QA &lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>;, for the video QA task, where a text-based question about a video is issued and the model needs to answer. This evaluates the ability of the model to understand both the text-based question and video content, and to form an answer, focusing on only relevant information. Of these benchmarks, the latter two target long video inputs and feature more complex questions. &lt;/p>; &lt;p>; We also evaluate our approach in the more challenging open-ended text generation setting, wherein the model generates the answers in an unconstrained fashion as free form text, requiring an exact match to the ground truth answer. While this stricter evaluation counts synonyms as incorrect, it may better reflect a model&#39;s ability to generalize. &lt;/p>; &lt;p>; Our results indicate improved performance over state-of-the-art approaches for most benchmarks, including all with open-ended generation evaluation — notable considering our model is only 3B parameters, considerably smaller than prior approaches, eg, Flamingo 80B. We used only video and text inputs to be comparable to other work. Importantly, our model can process 512 frames without needing to increase the model parameters, which is crucial for handling longer videos. Finally with the TTM Combiner, we see both better or comparable performance while reducing compute by 18%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s1200/image3.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/ 06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>; (video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center &quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s1200/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;860&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>; benchmark, which features long videos for the video QA task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results on audio-video benchmarks&lt;/h2>; &lt;p>; Results on the popular audio-video datasets &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>; and &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; are shown below. Since these benchmarks are classification-only, we treat them as an open-ended text generative setting where our model produces the text of the desired class; eg, for the class ID corresponding to the “playing drums” activity, we expect the model to generate the text “playing drums”. In some cases our approach outperforms the prior state of the art by large margins, even though our model outputs the results in the generative open-ended setting. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s1200/image6.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG -Sound&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s1200/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Benefits of autoregressive modeling&lt;/h2>; &lt;p>; We conduct an ablation study comparing our approach to a set of baselines that use the same input information but with standard methods (ie, without autoregression and the Combiner). We also compare the effects of pre-training. Because standard methods are ill-suited for processing longer video, this experiment is conducted for 32 frames and four chunks only, across all settings for fair comparison. We see that Mirasol3B&#39;s improvements are still valid for relatively short videos. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s1200/image4.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Ablation experiments comparing the main components of our model. Using the Combiner, the autoregressive modeling, and pre-training all improve performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion &lt;/h2>; &lt;p>; We present a multimodal autoregressive model that addresses the challenges associated with the heterogeneity of multimodal data by coordinating the learning between time-aligned and time-unaligned modalities. Time-aligned modalities are further processed autoregressively in time with a Combiner, controlling the sequence length and producing powerful representations. We demonstrate that a relatively small model can successfully represent long video and effectively combine with other modalities. We outperform the state-of-the-art approaches (including some much bigger models) on video- and audio-video question answering. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research is co-authored by AJ Piergiovanni, Isaac Noble, Dahun Kim, Michael Ryoo, Victor Gomes, and Anelia Angelova. We thank Claire Cui, Tania Bedrax-Weiss, Abhijit Ogale, Yunhsuan Sung, Ching-Chung Chang, Marvin Ritter, Kristina Toutanova, Ming-Wei Chang, Ashish Thapliyal, Xiyang Luo, Weicheng Kuo, Aren Jansen, Bryan Seybold, Ibrahim Alabdulmohsin, Jialin Wu, Luke Friedman, Trevor Walker, Keerthana Gopalakrishnan, Jason Baldridge, Radu Soricut, Mojtaba Seyedhosseini, Alexander D&#39;Amour, Oliver Wang, Paul Natsev, Tom Duerig, Younghui Wu, Slav Petrov, Zoubin Ghahramani for their help and support. We also thank Tom Small for preparing the animation. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1338549955376716163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html&quot; rel=&quot;alternate&quot; title=&quot;Scaling multimodal understanding to long videos&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s72-c/mirasol.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5546775652504697591&lt;/id>;&lt;published>;2023-11-10T10:05:00.000-08:00&lt;/published>;&lt;updated>;2023-11-10T10:05:13.301-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling large-scale health studies for the research community&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Chintan Ghate, Software Engineer, and Diana Mincu, Research Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s1100/MSSignals-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; As consumer technologies like &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/google-cloud-fitbit-haga-collaborate-on-pilot-heart-study&quot;>;fitness trackers&lt;/a>; and &lt;a href=&quot;https://blog.google/products/pixel/health-ai-better-sleep/&quot;>;mobile phones&lt;/a>; become more widely used for health-related data collection, so does the opportunity to leverage these data pathways to study and advance our understanding of medical conditions. We have &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot;>;previously&lt;/a>; touched upon how our work explores the use of this technology within the context of chronic diseases, in particular multiple sclerosis (MS). This effort leverages the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, an open-source platform used to create clinical study apps, that makes it easier for anyone to run their own studies and collect good quality healthcare data, in a trusted and safe way. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe the setup that we developed by expanding the FDA MyStudies platform and demonstrate how it can be used to set up a digital health study. We also present our exploratory research study created through this platform, called MS Signals, which consists of a symptom tracking app for MS patients. The goal for this app is twofold: 1) to ensure that the enhancements to the FDA MyStudies platform made for a more streamlined study creation experience; and 2) to understand how new data collection mechanisms can be used to revolutionize patients&#39; chronic disease management and tracking. We have &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; our extension to the FDA MyStudies platform under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;>;Apache 2.0 license&lt;/a>; to provide a resource for the community to build their own studies. &lt;/p>; &lt;br />; &lt;h2>;Extending the FDA MyStudies platform&lt;/h2>; &lt;p>; The original FDA MyStudies platform allowed people to configure their own study apps, manage participants, and create separate iOS and Android apps. To simplify the study creation process and ensure increased study engagement, we made a number of accessibility changes. Some of the main improvements include: cross-platform (iOS and Android) app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>;, an open source framework by Google for building multi-platform applications from a single codebase; a simplified setup, so that users can prototype their study quickly (under a day in most cases); and, most importantly, an emphasis on accessibility so that diverse patient&#39;s voices are heard. The accessibility enhancements include changes to the underlying features of the platform and to the particular study design of the MS Signals study app. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Multi-platform support with rapid prototyping&lt;/h3>; &lt;p>; We decided on the use of Flutter as it would be a single point that would generate both iOS and Android apps in one go, reducing the work required to support multiple platforms. Flutter also provides &lt;a href=&quot;https://docs.flutter.dev/tools/hot-reload&quot;>;hot-reloading&lt;/a>;, which allows developers to build &amp;amp; preview features quickly. The design-system in the app takes advantage of this feature to provide a central point from which the branding &amp;amp; theme of the app can be changed to match the tone of a new study and previewed instantly. The demo environment in the app also utilizes this feature to allow developers to mock and preview questionnaires locally on their machines. In our experience this has been a huge time-saver in A/B testing the UX and the format and wording of questions live with clinicians. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;System accessibility enhancements&lt;/h3>; &lt;p>; To improve the accessibility of the platform for more users, we made several usability enhancements: &lt;/p>; &lt;ol>; &lt;li>;Light &amp;amp; dark theme support&lt;/li>; &lt;li>;Bold text &amp;amp; variable font-sizes&lt;/li>; &lt;li>;High-contrast mode&lt;/li>; &lt;li>;Improving user awareness of accessibility settings&lt;/li>; &lt;/ol>; &lt;p>; Extended exposure to bright light themes can strain the eyes, so supporting dark theme features was necessary to make it easier to use the study app frequently. Some small or light text-elements are illegible to users with vision impairments, so we added 1) bold-text and support for larger font-sizes and 2) high-contrast color-schemes. To ensure that accessibility settings are easy to find, we placed an introductory one-time screen that was presented during the app&#39;s first launch, which would directly take users to their system accessibility settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study accessibility enhancements&lt;/h3>; &lt;p>; To make the study itself easier to interact with and reduce cognitive overload, we made the following changes: &lt;/p>; &lt;ol>; &lt;li>;Clarified the onboarding process&lt;/li>; &lt;li>;Improved design for questionnaires&lt;/li>; &lt;/ol>; &lt;p>; First, we clarified the on-boarding process by presenting users with a list of required steps when they first open the app in order to reduce confusion and participant drop-off. &lt;/p>; &lt;p>; The original questionnaire design in the app presented each question in a card format, which utilizes part of the screen for shadows and depth effects of the card. In many situations, this is a pleasant aesthetic, but in apps where accessibility is priority, these visual elements restrict the space available on the screen. Thus, when more accessible, larger font-sizes are used there are more frequent word breaks, which reduces readability. We fixed this simply by removing the card design elements and instead using the entire screen, allowing for better visuals with larger font-sizes. &lt;/p>; &lt;br />; &lt;h2>;The MS Signals prototype study&lt;/h2>; &lt;p>; To test the usability of these changes, we used our redesigned platform to create a prototype study app called MS Signals, which uses surveys to gather information about a participant&#39;s MS-related symptoms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/s1760/MSSignals.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1728&quot; data-original-width=&quot;1760&quot; height=&quot;628&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/w640-h628/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1971&quot; data-original-width=&quot;1999&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/w640-h632/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MS Studies app design&lt;/h3>; &lt;p>; As a first step, before entering any study information, participants are asked to complete an eligibility and study comprehension questionnaire to ensure that they have read through the potentially lengthy terms of study participation. This might include, for example, questions like &quot;In what country is the study available?&quot; or “Can you withdraw from the study?&quot; A section like this is common in most health studies, and it tends to be the first drop-off point for participants. &lt;/p>; &lt;p>; To minimize study drop-off at this early stage, we kept the eligibility test brief and reflected correct answers for the comprehension test back to the participants. This helps minimize the number of times a user may need to go through the initial eligibility questionnaire and ensures that the important aspects of the study protocol are made clear to them. &lt;/p>; &lt;p>; After successful enrollment, participants are taken to the main app view, which consists of three pages: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Activities:&lt;/strong>; &lt;br />; This page lists the questionnaires available to the participant and is where the majority of their time is spent. The questionnaires vary in frequency — some are one-time surveys created to gather medical history, while others are repeated daily, weekly or monthly, depending on the symptom or area they are exploring. For the one-time survey we provide a counter above each question to signal to users how far they have come and how many questions are left, similar to the questionnaire during the eligibility and comprehension step. &lt;/li>; &lt;li>;&lt;strong>;Dashboard:&lt;/strong>; &lt;br />; To ensure that participants get something back in return for the information they enter during a study, the Dashboard area presents a summary of their responses in graph or pie chart form. Participants could potentially show this data to their care provider as a summary of their condition over the last 6 months, an improvement over the traditional pen and paper methods that many employ today. &lt;/li>; &lt;li>;&lt;strong>;Resources:&lt;/strong>; &lt;br />; A set of useful links, help articles and common questions related to MS. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Questionnaire design&lt;/h3>; &lt;p>; Since needing to frequently input data can lead to cognitive overload, participant drop off, and bad data quality, we reduced the burden in two ways: &lt;/p>; &lt;ol>; &lt;li>;We break down large questionnaires into smaller ones, resulting in 6 daily surveys, containing 3–5 questions each, where each question is multiple choice and related to a single symptom. This way we cover a total of 20 major symptoms, and present them in a similar way to how a clinician would ask these questions in an in-clinic setting.&lt;/li>; &lt;li>;We ensure previously entered information is readily available in the app, along with the time of the entry.&lt;/li>; &lt;/ol>; &lt;p>; In designing the survey content, we collaborated closely with experienced clinicians and researchers to finalize the wording and layout. While studies in this field typically use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;Likert scale&lt;/a>; to gather symptom information, we defined a more intuitive verbose scale to provide better experience for participants tracking their disease and the clinicians or researchers viewing the disease history. For example, in the case of vision issues, rather than asking participants to rate their symptoms on a scale from 1 to 10, we instead present a multiple choice question where we detail common vision problems that they may be experiencing. &lt;/p>; &lt;p>; This verbose scale helps patients track their symptoms more accurately by including context that helps them more clearly define their symptoms. This approach also allows researchers to answer questions that go beyond symptom correlation. For example, for vision issues, data collected using the verbose scale would reveal to researchers whether &lt;a href=&quot;https://www.webmd.com/eye-health/nystagmus&quot;>;nystagmus&lt;/a>; is more prominent in patients with MS compared to double vision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/s1780/MSSignals-2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;1780&quot; height=&quot;630&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/w640-h630/MSSignals-2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side-by-side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1901&quot; data-original-width=&quot;1999&quot; height=&quot;608&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/w640-h608/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side by side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;h2>;Focusing on accessibility&lt;/h2>; &lt;p>; Mobile-based studies can often present additional challenges for participants with chronic conditions: the text can be hard to read, the color contrast could make it difficult to see certain bits of information, or it may be challenging to scroll through pages. This may result in participant drop off, which, in turn, could yield a biased dataset if the people who are experiencing more advanced forms of a disease are unable to provide data. &lt;/p>; &lt;p>; In order to prevent such issues, we include the following accessibility features: &lt;/p>; &lt;ul>; &lt;li>;Throughout, we employ color blind accessible color schemes. This includes improving the contrast between crucial text and important additional information, which might otherwise be presented in a smaller font and a faded text color.&lt;/li>; &lt;li>;We reduced the amount of movement required to access crucial controls by placing all buttons close to the bottom of the page and ensuring that pop-ups are controllable from the bottom part of the screen.&lt;/li>; &lt;/ul>; &lt;p>; To test the accessibility of MS Signals, we collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study. For this, a call for participation was sent out by the Society to their members, and 9 respondents were asked to test out the various app flows. The majority indicated that they would like a better way than their current method to track their symptom data, that they considered MS Signals to be a unique and valuable tool that would enhance the accuracy of their symptom tracking, and that they would want to share the dashboard view with their healthcare providers. &lt;/p>; &lt;br />; &lt;h2>;Next steps&lt;/h2>; &lt;p>; We want to encourage everyone to make use of the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open source&lt;/a>; platform to start setting up and running their own studies. We are working on creating a set of standard study templates, which would incorporate what we learned from above, and we hope to release those soon. For any issues, comments or questions please check out our &lt;a href=&quot;https://goo.gle/ms-signals&quot;>;resource page&lt;/a>;. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5546775652504697591/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments &quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:/ /blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling large-scale health studies for the research community&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s72- c/MSSignals-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7987071997778787277&lt;/id>;&lt;published>;2023-11-09T14:23:00.000-08:00&lt;/published>;&lt; updated>;2023-11-09T14:23:38.431-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Africa&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Context in AI Research (CAIR)&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;Posted by Katherine Heller, Research Scientist, Google Research, on behalf of the CAIR Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s1100 /CAIR-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Artificial intelligence (AI) and related machine learning (ML) technologies are increasingly influential in the world around us, making it imperative that we consider the potential impacts on society and individuals in all aspects of the technology that we create. To these ends, the Context in AI Research (CAIR) team develops novel AI methods in the context of the entire AI pipeline:&lt;strong>; &lt;/strong>;from data to end-user feedback. The pipeline for building an AI system typically starts with &lt;em>;data&lt;/em>; collection, followed by designing a &lt;em>;model&lt;/em>; to run on that data, &lt;em>;deployment&lt;/em>; of the model in the real world, and lastly, compiling and incorporation of &lt;em>;human feedback&lt;/em>;. Originating in the health space, and now expanded to additional areas, the work of the CAIR team impacts every aspect of this pipeline. While specializing in model building, we have a particular focus on building systems with responsibility in mind, including fairness, robustness, transparency, and inclusion. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto” ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC -Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/s1050/CAIR.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;383&quot; data-original-width=&quot;1050 &quot; height=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/w640-h234/CAIR.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Data &lt;/h2>; &lt;p>; The CAIR team focuses on understanding the data on which ML systems are built. Improving the standards for the transparency of ML datasets is instrumental in our work. First, we employ documentation frameworks to elucidate dataset and model characteristics as guidance in the development of data and model documentation techniques — &lt;a href=&quot;https://arxiv.org/abs/1803.09010&quot;>;Datasheets for Datasets&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1810.03993&quot;>;Model Cards for Model Reporting&lt;/a>;. &lt;/p>; &lt;p>; For example, health datasets are highly sensitive and yet can have high impact. For this reason, we developed &lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533239&quot;>;Healthsheets&lt;/a>;, a health-contextualized adaptation of a Datasheet. Our motivation for developing a health-specific sheet lies in the limitations of existing regulatory frameworks for AI and health. &lt;a href=&quot;https://www.nejm.org/doi/10.1056/NEJMp1816373&quot;>;Recent research&lt;/a>; suggests that data privacy regulation and standards (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;>;HIPAA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;>;GDPR&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act&quot;>;California Consumer Privacy Act&lt;/a>;) do not ensure ethical collection, documentation, and use of data. Healthsheets aim to fill this gap in ethical dataset analysis. The development of Healthsheets was done in collaboration with many stakeholders in relevant job roles, including clinical, legal and regulatory, bioethics, privacy, and product. &lt;/p>; &lt;p>; Further, we studied how Datasheets and Healthsheets could serve as diagnostic tools that surface the limitations and strengths of datasets. Our aim was to start a conversation in the community and tailor Healthsheets to dynamic healthcare scenarios over time. &lt;/p>; &lt;p>; To facilitate this effort, we joined the &lt;a href=&quot;http://www.datadiversity.org/&quot;>;STANDING Together&lt;/a>; initiative, a consortium that aims to develop &lt;a href=&quot;https://www.nature.com/articles/s41591-022-01987-w&quot;>;international, consensus-based standards for documentation of diversity and representation&lt;/a>; within health datasets and to provide guidance on how to mitigate risk of bias translating to harm and health inequalities. Being part of this international, interdisciplinary partnership that spans academic, clinical, regulatory, policy, industry, patient, and charitable organizations worldwide enables us to engage in the conversation about responsibility in AI for healthcare internationally. Over 250 stakeholders from across 32 countries have contributed to refining the standards&lt;strong>;.&lt;/strong>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s1050/Healthsheets.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;731&quot; data-original-width=&quot;1050&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s16000/Healthsheets.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Healthsheets and STANDING Together: towards health data documentation and standards.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Model&lt;/h2>; &lt;p>; When ML systems are deployed in the real world, they may fail to behave in expected ways, making poor predictions in new contexts. Such failures can occur for a myriad of reasons and can carry negative consequences, especially within the context of healthcare. Our work aims to identify situations where unexpected model behavior may be discovered, before it becomes a substantial problem, and to mitigate the unexpected and undesired consequences. &lt;/p>; &lt;p>; Much of the CAIR team&#39;s modeling work focuses on identifying and mitigating when models are &lt;a href=&quot;https://blog.research.google/2021/10/how-underspecification-presents.html&quot;>;underspecified&lt;/a>;. We show that models that perform well on held-out data drawn from a training domain are not equally robust or fair under distribution shift because the models vary in the extent to which they rely on spurious correlations. This poses a risk to users and practitioners because it can be difficult to anticipate model instability using standard model evaluation practices. &lt;a href=&quot;https://www.jmlr.org/papers/v23/20-1335.html&quot;>;We have demonstrated&lt;/a>; that this concern arises in several domains, including computer vision, natural language processing, medical imaging, and prediction from electronic health records. &lt;/p>; &lt;p>; We have also shown how to use knowledge of causal mechanisms to diagnose and mitigate fairness and robustness issues in new contexts. Knowledge of causal structure allows practitioners to anticipate &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html&quot;>;the generalizability of fairness properties under distribution shift in real-world medical settings&lt;/a>;. Further, investigating the capability for specific causal pathways, or “shortcuts”, to introduce bias in ML systems, we demonstrate how to identify cases where &lt;a href=&quot;https://www.nature.com/articles/s41467-023-39902-7&quot;>;shortcut learning&lt;/a>; leads to predictions in ML systems that are unintentionally dependent on sensitive attributes (eg, age, sex, race). We have shown how to use causal &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;>;directed acyclic graphs&lt;/a>; to &lt;a href=&quot;https://proceedings.mlr.press/v206/alabdulmohsin23a&quot;>;adapt ML systems to changing environments&lt;/a>; under complex forms of distribution shift. Our team is currently investigating how a causal interpretation of different forms of bias, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Selection_bias&quot;>;selection bias&lt;/a>;, label bias, and measurement error, &lt;a href=&quot;https://nips.cc/virtual/2022/58452&quot;>;motivates the design of techniques to mitigate bias during model development and evaluation&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s727/image6.png&quot; style=&quot; margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;442&quot; data-original-width=&quot;727&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s1078/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;382&quot; data-original-width=&quot;1078&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Shortcut Learning: For some models, age may act as a shortcut in classification when using medical images.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team focuses on developing methodology to build more inclusive models broadly. For example, we also have &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;work on the design of participatory systems&lt;/a>;, which allows individuals to choose whether to disclose sensitive attributes, such as race, when an ML system makes predictions. We hope that our methodological research positively impacts the societal understanding of inclusivity in AI method development. &lt;/p>; &lt;br />; &lt;h2>;Deployment &lt;/h2>; &lt;p>; The CAIR team aims to build technology that improves the lives of all people through the use of mobile device technology. We aim to reduce suffering from health conditions, address systemic inequality, and enable transparent device-based data collection. As consumer technology, such as fitness trackers and mobile phones, become central in data collection for health, we explored the use of these technologies within the context of chronic disease, in particular, for &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_sclerosis&quot;>;multiple sclerosis&lt;/a>; (MS). We developed new data collection mechanisms and predictions that we hope will eventually revolutionize patient&#39;s chronic disease management, clinical trials, medical reversals and drug development. &lt;/p>; &lt;p>; First, we extended the open-source &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, which is used to create clinical study apps, to make it easier for anyone to run their own studies and collect good quality data, in a trusted and safe way. Our improvements include zero-config setups, so that researchers can prototype their study in a day, cross-platform app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>; and, most importantly, an emphasis on accessibility so that all patient&#39;s voices are heard. We are excited to announce this work has now been &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; as an extension to the original FDA-Mystudies platform. You can start setting up your own studies today! &lt;/p>; &lt;p>; To test this platform, we built a prototype app, which we call MS Signals, that uses surveys to interface with patients in a novel consumer setting. We collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study for the app, with the goal of reducing dropout rates and improving the platform further. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/s915/MSSignals.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;915&quot; height=&quot;632&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/w640-h632/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;Study welcome screen.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;Questionnaire.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Once data is collected, researchers could potentially use it to drive the frontier of ML research in MS. In a separate study, we established a research collaboration with the &lt;a href=&quot;https://neurology.duke.edu/&quot;>;Duke Department of Neurology&lt;/a>; and demonstrated that ML models can &lt;a href=&quot;https://www.researchsquare.com/article/rs-2547289/v1&quot;>;accurately predict the incidence of high-severity symptoms&lt;/a>; within three months using continuously collected data from mobile apps. Results suggest that the trained models can be used by clinicians to evaluate the symptom trajectory of MS participants, which may inform decision making for administering interventions. &lt;/p>; &lt;p>; The CAIR team has been involved in the deployment of many other systems, for both internal and external use. For example, we have also partnered with &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot;>;build a book recommendation system&lt;/a>; for children with learning disabilities, such as dyslexia. We hope that our work positively impacts future product development. &lt;/p>; &lt;br />; &lt;h2>;Human feedback &lt;/h2>; &lt;p>; As ML models become ubiquitous throughout the developed world, it can be far too easy to leave voices in less developed countries behind. A priority of the CAIR team is to bridge this gap, develop deep relationships with communities, and work together to address ML-related concerns through community-driven approaches. &lt;/p>; &lt;p>; One of the ways we are doing this is through working with grassroots organizations for ML, such as &lt;a href=&quot;https://www.sisonkebiotik.africa/home&quot;>;Sisonkebiotik&lt;/a>;, an open and inclusive community of researchers, practitioners and enthusiasts at the intersection of ML and healthcare working together to build capacity and drive forward research initiatives in Africa. We worked in collaboration with the Sisonkebiotik community to detail limitations of historical top-down approaches for global health, and suggested complementary health-based methods, specifically those of grassroots participatory communities (GPCs). We jointly created a &lt;a href=&quot;https://openreview.net/forum?id=jHY_G91R880&quot;>;framework for ML and global health&lt;/a>;, laying out a practical roadmap towards setting up, growing and maintaining GPCs, based on common values across various GPCs such as &lt;a href=&quot;https://www.masakhane.io/&quot;>;Masakhane&lt;/a>;, Sisonkebiotik and &lt;a href=&quot;https://ro-ya-cv4africa.github.io/homepage/&quot;>;Ro&#39;ya&lt;/a>;. &lt;/p>; &lt;p>; We are engaging with open initiatives to better understand the role, perceptions and use cases of AI for health in non-western countries through human feedback, with an initial focus in Africa. Together with &lt;a href=&quot;https://ghananlp.org/&quot;>;Ghana NLP&lt;/a>;, we have worked to detail the need to &lt;a href=&quot;https://arxiv.org/abs/2304.02190&quot;>;better understand algorithmic fairness and bias in health in non-western contexts&lt;/a>;. We recently launched a study to expand on this work using human feedback. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy /s927/MLPipelineBiases.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;927&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s16000/MLPipelineBiases.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Biases along the ML pipeline and their associations with African-contextualized axes of disparities.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team is committed to creating opportunities to hear more perspectives in AI development. We partnered with Sisonkebiotik to co-organize the &lt;a href=&quot;https://www.sisonkebiotik.africa/events/workshops/dl-indaba-2023&quot;>;Data Science for Health Workshop&lt;/a>; at &lt;a href=&quot;https://deeplearningindaba.com/2023/&quot;>;Deep Learning Indaba 2023&lt;/a>; in Ghana. Everyone&#39;s voice is crucial to developing a better future using AI technology. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Negar Rostamzadeh, Stephen Pfohl, Subhrajit Roy, Diana Mincu, Chintan Ghate, Mercy Asiedu, Emily Salkey, Alexander D&#39;Amour, Jessica Schrouff, Chirag Nagpal, Eltayeb Ahmed, Lev Proleev, Natalie Harris, Mohammad Havaei, Ben Hutchinson, Andrew Smart, Awa Dieng, Mahima Pushkarna, Sanmi Koyejo, Kerrie Kauer, Do Hee Park, Lee Hartsell, Jennifer Graves, Berk Ustun, Hailey Joren, Timnit Gebru and Margaret Mitchell for their contributions and influence, as well as our many friends and collaborators at Learning Ally, National MS Society, Duke University Hospital, STANDING Together, Sisonkebiotik, and Masakhane.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7987071997778787277/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Context in AI Research (CAIR)&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s72-c/CAIR-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1303378857635363504&lt;/id>;&lt;published>;2023-11-09T11:20:00.002-08:00&lt;/published>;&lt;updated>;2023-11-09T11:59:02.393-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Overcoming leakage on error-corrected quantum processors&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kevin Miao and Matt McEwen, Research Scientists, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/s320/Quantum%20leakage.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Qubit&quot;>;qubits&lt;/a>; that make up &lt;a href=&quot;https://quantumai.google/hardware&quot;>;Google quantum devices&lt;/a>; are delicate and noisy, so it&#39;s necessary to incorporate error correction procedures that identify and account for qubit errors on the way to building a useful quantum computer. Two of the most prevalent error mechanisms are &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction#Bit_flip_code&quot;>;bit-flip errors&lt;/a>; (where the energy state of the qubit changes) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_error_correction#Sign_flip_code&quot;>;phase-flip errors&lt;/a>; (where the phase of the encoded quantum information changes). &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;Quantum error correction&lt;/a>; (QEC) promises to address and mitigate these two prominent errors. However, there is an assortment of other error mechanisms that challenges the effectiveness of QEC. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While we want qubits to behave as ideal &lt;a href=&quot;https://en.wikipedia.org/wiki/Two-state_quantum_system&quot;>;two-level systems&lt;/a>; with no loss mechanisms, this is not the case in reality. We use the lowest two energy levels of our qubit (which form the &lt;em>;computational basis&lt;/em>;) to carry out computations. These two levels correspond to the absence (computational ground state) or presence (computational excited state) of an excitation in the qubit, and are labeled |0⟩ (“&lt;a href=&quot;https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation&quot;>;ket&lt;/a>; zero”) and |1⟩ (“ket one”), respectively. However, our qubits also host many higher levels called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1509.05470&quot;>;leakage states&lt;/a>;&lt;/em>;, which can become occupied. Following the convention of labeling the level by indicating how many excitations are in the qubit, we specify them as |2⟩, |3⟩, |4⟩, and so on. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;Overcoming leakage in quantum error correction&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/nphys/&quot;>;Nature Physics&lt;/a>;&lt;/em>;, we identify when and how our qubits leak energy to higher states, and show that the leaked states can corrupt nearby qubits through our two-qubit gates. We then identify and implement a strategy that can remove leakage and convert it to an error that QEC can efficiently fix. Finally, we show that these operations lead to notably improved performance and stability of the QEC process. This last result is particularly critical, since additional operations take time, usually leading to more errors. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Working with imperfect qubits&lt;/h2>; &lt;p>; Our quantum processors are built from superconducting qubits called &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transmon&quot;>;transmons&lt;/a>;&lt;/em>;. Unlike an ideal qubit, which only has two computational levels — a computational ground state and a computational excited state — transmon qubits have many additional states with higher energy than the computational excited state. These higher leakage states are useful for particular operations that generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;entanglement&lt;/a>;, a necessary resource in quantum algorithms, and also keep transmons from becoming too non-linear and difficult to operate. However, the transmon can also be inadvertently excited into these leakage states through a variety of processes, including imperfections in the control pulses we apply to perform operations or from the small amount of stray heat leftover in our cryogenic refrigerator. These processes are collectively referred to as &lt;em>;leakage&lt;/em>;, which describes the transition of the qubit from computational states to leakage states. &lt;/p>; &lt;p>; Consider a particular two-qubit operation that is used extensively in our QEC experiments: the &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=%5B1%5D-,Controlled%2DZ,-%2C%0Acontrolled%20sign&quot;>;CZ gate&lt;/a>;. This gate operates on two qubits, and when both qubits are in their |1⟩ level, an interaction causes the two individual excitations to briefly “bunch” together in one of the qubits to form |2⟩, while the other qubit becomes |0⟩, before returning to the original configuration where each qubit is in |1⟩. This bunching underlies the entangling power of the CZ gate. However, with a small probability, the gate can encounter an error and the excitations do not return to their original configuration, causing the operation to leave a qubit in |2⟩, a leakage state. When we execute hundreds or more of these CZ gates, this small leakage error probability accumulates. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/s559/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;559&quot; height=&quot;400&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnq7iPbsqcqffIGk3VgmUsIycfrYFQdyArF3RlBEdRKjJigk0mLym_E0OK_GZwix040pxZSPdZYr8loGNaX4An1pwtMGSiPIWruSj-S6Nleklj7YF9Y61fOtmsgl5pbeKIZCOLcWr4_4bQjZoBHz4zPNkBelyF-ZW0aUJDBpIQdamwg7VYWZxeCNk0q57L/w400-h400/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transmon qubits support many leakage states (|2⟩, |3⟩, |4⟩, …) beyond the computational basis (|0⟩ and |1⟩). While we typically only use the computational basis to represent quantum information, sometimes the qubit enters these leakage states, and disrupts the normal operation of our qubits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; A single leakage event is especially damaging to normal qubit operation because it induces many individual errors. When one qubit starts in a leaked state, the CZ gate no longer correctly entangles the qubits, preventing the algorithm from executing correctly. Not only that, but CZ gates applied to one qubit in leaked states can cause the other qubit to leak as well, spreading leakage through the device. Our work includes extensive characterization of how leakage is caused and how it interacts with the various operations we use in our quantum processor. &lt;/p>; &lt;p>; Once the qubit enters a leakage state, it can remain in that state for many operations before relaxing back to the computational states. This means that a single leakage event interferes with many operations on that qubit, creating operational errors that are bunched together in time (&lt;em>;time-correlated &lt;/em>;errors). The ability for leakage to spread between the different qubits in our device through the CZ gates means we also concurrently see bunches of errors on neighboring qubits (&lt;em>;space-correlated&lt;/em>; errors). The fact that leakage induces patterns of space- and time-correlated errors makes it especially hard to diagnose and correct from the perspective of QEC algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The effect of leakage in QEC&lt;/h2>; &lt;p>; We aim to mitigate qubit errors by implementing &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;surface code QEC&lt;/a>;, a set of operations applied to a collection of imperfect physical qubits to form a &lt;em>;logical qubit&lt;/em>;, which has properties much closer to an ideal qubit. In a nutshell, we use a set of qubits called &lt;em>;data qubits&lt;/em>; to hold the quantum information, while another set of &lt;em>;measure qubits&lt;/em>; check up on the data qubits, reporting on whether they have suffered any errors, without destroying the delicate quantum state of the data qubits. One of the key underlying assumptions of QEC is that errors occur independently for each operation, but leakage can persist over many operations and cause a correlated pattern of multiple errors. The performance of our QEC strategies is significantly limited when leakage causes this assumption to be violated. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g -WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s1588/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original- width=&quot;1588&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUgCL-XuUZldvjck8dvEO5-9ei_Oq0m4kihQ5rHgGr66ggLVjJ3XZDuNl7Tw3s6EePJ-5NTL42-0UCpNuUoC2-15jJ6uX2aPCULu-KISQJEiCYKfe3HR55vWCr3oDxmKu5g-WzBDS3jM-tuGSnxOHFb8kM4shNqKGPqGtKzs8FFRCPs-hH8mjsvkKLrNza/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Once leakage manifests in our surface code transmon grid, it persists for a long time relative to a single surface code QEC cycle. To make matters worse, leakage on one qubit can cause its neighbors to leak as well.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our &lt;a href=&quot;https://blog.research.google/2021/08/demonstrating-fundamentals-of-quantum.html#:~:text=the%20Sycamore%20device.-,Leaky%20Qubits,-The%20goal%20of&quot;>;previous work&lt;/a>; has shown that we can remove leakage from measure qubits using an operation called &lt;em>;multi-level reset&lt;/em>; (MLR). This is possible because once we perform a measurement on measure qubits, they no longer hold any important quantum information. At this point, we can interact the qubit with a very lossy frequency band, causing whichever state the qubit was in (including leakage states) to decay to the computational ground state |0⟩. If we picture a &lt;em>;Jenga&lt;/em>; tower representing the excitations in the qubit, we tumble the entire stack over. Removing just one brick, however, is much more challenging. Likewise, MLR doesn&#39;t work with data qubits because they &lt;em>;always&lt;/em>; hold important quantum information, so we need a new leakage removal approach that minimally disturbs the computational basis states. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Gently removing leakage&lt;/h2>; &lt;p>; We introduce a new quantum operation called&lt;em>; data qubit leakage removal &lt;/em>;(DQLR), which targets leakage states in a data qubit and converts them into computational states in the data qubit and a neighboring measure qubit. DQLR consists of a two-qubit gate (dubbed &lt;em>;Leakage iSWAP&lt;/em>; — an &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_quantum_logic_gates#Clifford_qubit_gates:~:text=1%5D%5B6%5D-,Imaginary%20swap,-2&quot;>;iSWAP&lt;/a>; operation with leakage states) inspired by and similar to our CZ gate, followed by a rapid reset of the measure qubit to further remove errors. The Leakage iSWAP gate is very efficient and greatly benefits from our extensive characterization and calibration of CZ gates within the surface code experiment. &lt;/p>; &lt;p>; Recall that a CZ gate takes two single excitations on two different qubits and briefly brings them to one qubit, before returning them to their respective qubits. A Leakage iSWAP gate operates similarly, but almost in reverse, so that it takes a single qubit with two excitations (otherwise known as |2⟩) and splits them into |1⟩ on two qubits. The Leakage iSWAP gate (and for that matter, the CZ gate) is particularly effective because it does not operate on the qubits if there are fewer than two excitations present. We are precisely removing the |2⟩ &lt;em>;Jenga&lt;/em>; brick without toppling the entire tower. &lt;/p>; &lt;p>; By carefully measuring the population of leakage states on our transmon grid, we find that DQLR can reduce average leakage state populations over all qubits to about 0.1%, compared to nearly 1% without it. Importantly, we no longer observe a gradual rise in the amount of leakage on the data qubits, which was always present to some extent prior to using DQLR. &lt;/p>; &lt;p>; This outcome, however, is only half of the puzzle. As mentioned earlier, an operation such as MLR could be used to effectively remove leakage on the data qubits, but it would also completely erase the stored quantum state. We also need to demonstrate that DQLR is compatible with the preservation of a logical quantum state. &lt;/p>; &lt;p>; The second half of the puzzle comes from executing the QEC experiment with this operation interleaved at the end of each QEC cycle, and observing the logical performance. Here, we use a metric called &lt;em>;detection probability&lt;/em>; to gauge how well we are executing QEC. In the presence of leakage, time- and space-correlated errors will cause a gradual rise in detection probabilities as more and more qubits enter and stay in leakage states. This is most evident when we perform no reset at all, which rapidly leads to a transmon grid plagued by leakage, and it becomes inoperable for the purposes of QEC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s1532/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1532&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgC7T3bILXM6tqNCbHKb04ZwJhdanfunDEFSSbXDTa5J2MyjOWLyCGfW-dEsSrmjIZJY3e0KfvdzVYQZidJniivyaCPGJ_UouNnqxvkmWzOYUm8Zp9DY4dWS3CZmYVddS9hNVMvZgrXA1djXw9LgzvE3hZjLyzSrvzLy1qR3_d76Tp9zfdUK_pEFaWBmCYO/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The prior state-of-the-art in our QEC experiments was to use MLR on the measure qubits to remove leakage. While this kept leakage population on the measure qubits (green circles) sufficiently low, data qubit leakage population (green squares) would grow and saturate to a few percent. With DQLR, leakage population on both the measure (blue circles) and data qubits (blue squares) remain acceptably low and stable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; With MLR, the large reduction in leakage population on the measure qubits drastically decreases detection probabilities and mitigates a considerable degree of the gradual rise. This reduction in detection probability happens even though we spend more time dedicated to the MLR gate, when other errors can potentially occur. Put another way, the correlated errors that leakage causes on the grid can be much more damaging than the uncorrelated errors from the qubits waiting idle, and it is well worth it for us to trade the former for the latter. &lt;/p>; &lt;p>; When only using MLR, we observed a small but persistent residual rise in detection probabilities. We ascribed this residual increase in detection probability to leakage accumulating on the data qubits, and found that it disappeared when we implemented DQLR. And again, the observation that the detection probabilities end up lower compared to only using MLR indicates that our added operation has removed a damaging error mechanism while minimally introducing uncorrelated errors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s1205/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1205&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpZk_lDGNXr0w8H-mPyPMSGpojxd2ecI48zxj5p9ElpofUVhPX0mqkvKCPjcxMf3q5VVEhoed_PKIRHta73hwW8Jt153XGpeqasioHLlO0OapK8Amv1C3A_njsjZHuU330rOyNiL3kWqJLNA8YAAZhBha7Jn_eH8nbKd5-fbKiojSa5Yk_o0w8NQGgN_6B/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Leakage manifests during surface code operation as increased errors (shown as error detection probabilities) over the number of cycles. With DQLR, we no longer see a notable rise in detection probability over more surface code cycles.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prospects for QEC scale-up&lt;/h2>; &lt;p>; Given these promising results, we are eager to implement DQLR in future QEC experiments, where we expect error mechanisms outside of leakage to be greatly improved, and sensitivity to leakage to be enhanced as we work with larger and larger transmon grids. In particular, our simulations indicate that scale-up of our surface code will almost certainly require a large reduction in leakage generation rates, or an active leakage removal technique over all qubits, such as DQLR. &lt;/p>; &lt;p>; Having laid the groundwork by understanding where leakage is generated, capturing the dynamics of leakage after it presents itself in a transmon grid, and showing that we have an effective mitigation strategy in DQLR, we believe that leakage and its associated errors no longer pose an existential threat to the prospects of executing a surface code QEC protocol on a large grid of transmon qubits. With one fewer challenge standing in the way of demonstrating working QEC, the pathway to a useful quantum computer has never been more promising. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work would not have been possible without the contributions of the entire Google Quantum AI Team.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1303378857635363504/comments/default&quot; rel=&quot;replies&quot; title= &quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1303378857635363504&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://blog.research.google/2023/11/overcoming-leakage-on-error-corrected.html&quot; rel=&quot;alternate&quot; title=&quot;Overcoming leakage on error-corrected quantum processors&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt; /电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded。 gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidYny_9v0IyOBy7QbYVgwS5FAHzgHhVUt5FQvjXhi6WWJHyIIal3awBfaXNu9l3g47HElKNkFNf6XfpU7a_9ExGLlKFgOWvUOaT0PLhEoCfofJrazqT2IL1fBMtZMwfGJSnrpBrQZwQkYJOV6iMOmhWBWIH5OCPcsEPkgw-LuiPDBoLYToGGIN3Hjfpmwr/ s72-c/Quantum%20leakage.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr: total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4661241136828203614&lt;/id>;&lt;published>;2023-11-07T12:34:00.003-08:00&lt;/published >;&lt;updated>;2023-11-07T12:34:21.947-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/ category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Alternating updates for efficient transformers&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Xin Wang, Software Engineer, and Nishanth Dikkala, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s1600/altup.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Contemporary deep learning models have been remarkably successful in many domains, ranging from natural language to computer vision. &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer neural networks&lt;/a>; (transformers) are a popular deep learning architecture that today comprise the foundation for most tasks in natural language processing and also are starting to extend to applications in other domains, such as &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;robotics&lt;/a>;, and &lt;a href=&quot;https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/&quot;>;autonomous driving&lt;/a>;. Moreover, they form the backbone of all the current state-of-the-art &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language models&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Increasing scale in Transformer networks has led to improved performance and the &lt;a href=&quot;https://arxiv.org/abs/2206.07682&quot;>;emergence of behavior&lt;/a>; not present in smaller networks. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. A natural question is whether we can reap the benefits of larger models without incurring the computational burden. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2301.13310&quot;>;Alternating Updates for Efficient Transformers&lt;/a>;”, accepted as a Spotlight at &lt;a href=&quot;https://neurips.cc/virtual/2023/poster/72994&quot;>;NeurIPS 2023&lt;/a>;, we introduce AltUp, a method to take advantage of increased token representation without increasing the computation cost. AltUp is easy to implement, widely applicable to any transformer architecture, and requires minimal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter tuning&lt;/a>;. For instance, using a variant of AltUp on a 770M parameter T5-Large model, the addition of ~100 parameters yields a model with a significantly better quality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; To understand how we can achieve this, we dig into how transformers工作。 First, they partition the input into a sequence of tokens. Each token is then mapped to an embedding vector (via the means of an embedding table) called the token embedding. We call the dimension of this vector the token representation dimension. The Transformer then operates on this sequence of token embeddings by applying a series of computation modules (called layers&lt;em>;)&lt;/em>; using its network parameters. The number of parameters in each transformer layer is a function of the layer&#39;s &lt;em>;width&lt;/em>;, which is determined by the token representation dimension. &lt;/p>; &lt;p>; To achieve benefits of scale without incurring the compute burden, prior works such as sparse mixture-of-experts (Sparse MoE) models (eg, &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot;>;Switch Transformer&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1&quot;>;Expert Choice&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/01/scaling-vision-with-sparse-mixture-of.html?m=1&quot;>;V-MoE&lt;/a>;) have predominantly focused on efficiently scaling up the network parameters (in the self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward layers&lt;/a>;) by conditionally activating a subset based on the input. This allows us to scale up network size without significantly increasing compute per input. However, there is a research gap on scaling up the token representation dimension itself by conditionally activating parts of the token representation vector. &lt;/p>; &lt;p>; Recent works (for example, &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;>;scaling laws&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2011.14522&quot;>;infinite-width networks)&lt;/a>; have empirically and theoretically established that a wider token representation helps in learning more complicated functions. This phenomenon is also evident in modern architectures of increasing capability. For instance, the representation dimension grows from 512 (small) to 768 (base) and 1024 (corresponding to models with 770M, 3B, and 11B parameters respectively) in &lt;a href=&quot;https://arxiv.org/abs/1910.10683 &quot;>;T5 models&lt;/a>;, and from 4096 (8B) to 8192 (64B) and 18432 (540B) in &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM models&lt;/a>; 。 A widened representation dimension also significantly improves performance for dual encoder retrieval models. However, naïvely widening the representation vector requires one to increase the model dimension accordingly, which quadratically&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;1&lt;/a>;&lt;/sup>; increases the amount of computation in the feedforward computation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Method&lt;/h2>; &lt;p>; AltUp works by partitioning a widened representation vector into equal sized blocks, processing only a single block at each layer, and using an efficient prediction-correction mechanism to infer the outputs of the other blocks (shown below on the right). This allows AltUp to simultaneously keep the model dimension, hence the computation cost, roughly constant and take advantage of using an increased token dimension. The increased token dimension allows the model to pack more information into each token&#39;s embedding. By keeping the width of each transformer layer constant, AltUp avoids incurring the quadratic increase in computation cost that would otherwise be present with a naïve expansion of the representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s1367/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;823&quot; data-original-width=&quot;1367&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiA3Jus4rPOjc_P58AjvIw6pUlQn8O4H828q_VFXYW2-dHxVQbfrMtplwmTy-s8gPCQTg4_Cd5mOrxZHGKhFswEC3gjGs_ffzRsBLkbyzDUDiRDUe1OncBGayjX4JVpfNNTtG8RVu3r9MYaG0nx1TxzeLGvZhxKC5ySlN8GTR1LN5rSvwXIdkqGEaCvuegv/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;An illustration of widening the token representation without (&lt;b>;left&lt;/b>;) and with AltUp (&lt;b>;right&lt;/b>;). This widening causes a near-quadratic increase in computation in a vanilla transformer due to the increased layer width. In contrast, Alternating Updates keeps the layer width constant and efficiently computes the output by operating on a sub-block of the representation at each layer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More specifically, the input to each layer is two or more blocks, one of which is passed into the 1x width transformer layer (see figure below). We refer to this block as the “activated” block. This computation results in the exact output for the activated block. In parallel, we invoke a lightweight predictor that computes a weighted combination of all the input blocks. The predicted values, along with the computed value of the activated block, are passed on to a lightweight corrector that updates the predictions based on the observed values. This correction mechanism enables the inactivated blocks to be updated as a function of the activated one. Both the prediction and correction steps only involve a limited number of vector additions and multiplications and hence are much faster than a regular transformer layer. We note that this procedure can be generalized to an arbitrary number of blocks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s957/image3.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;504&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgGWWOVTTIeC_BqXAmfpdePJlLKoDIQdvT38SNyv6bepjrKJG8TjCqWVW1nwJMxNdE0JPaRaCvic5tE4xVYLX6Yg0wRkeucBD1u0PYKNBQ1BEmZ7YbO10IzWuOU-y8idTPYzDg69HVPqWaH3WIvMqgC4u0nucCp_0O5VsSKP1DqwzeVZAkQj3nA7wgsw4vN/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;The predictor and corrector computations: The predictor mixes sub-blocks with trainable scalar coefficients; the corrector returns a weighted average of the predictor output and the transformer output. The predictor and corrector perform scalar-vector multiplications and incur negligible computation cost compared to the transformer. The predictor outputs a linear mixing of blocks with scalar mixing coefficients p&lt;sub>;i, j&lt;/sub>; , and the corrector combines predictor output and transformer output with weights g&lt;sub>;i&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; At a higher level, AltUp is similar to sparse MoE in that it is a method to add capacity to a model in the form of conditionally accessed (external) parameters. In sparse MoE, the additional parameters take the form of feed forward network (FFN) experts and the conditionality is with respect to the input. In AltUp, the external parameters come from the widened embedding table and the conditionality takes the form of alternating block-wise activation of the representation vector, as in the figure above. Hence, AltUp has the same underpinning as sparse MoE models. &lt;/p>; &lt;p>; An advantage of AltUp over sparse MoE is that it does not necessitate sharding since the number of additional parameters introduced is a factor&lt;sup id=&quot;fnref2&quot;>;&lt;a href=&quot;#fn2&quot; rel=&quot;footnote&quot;>;2&lt;/a>;&lt;/sup>; of the embedding table size, which typically makes up a small fraction of the overall model size. Moreover, since AltUp focuses on conditionally activating parts of a wider token representation, it can be applied synergistically with orthogonal techniques like MoE to obtain complementary performance gains. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; AltUp was evaluated on T5 models on various benchmark language tasks. Models augmented with AltUp are uniformly faster than the extrapolated dense models at the same accuracy. For example, we observe that a T5 Large model augmented with AltUp leads to a 27%, 39%, 87%, and 29% speedup on &lt;a href=&quot;https://gluebenchmark.com/&quot;>;GLUE&lt;/a>;, &lt;a href=&quot;https://super.gluebenchmark.com/&quot;>;SuperGLUE&lt;/a>;, &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;>;SQuAD&lt;/a>;, and &lt;a href=&quot;https://nlp.cs.washington.edu/triviaqa/&quot;>;Trivia-QA&lt;/a>; benchmarks, respectively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s1386 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;982&quot; data-original-width=&quot;1386&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOoxdT_-82XpER17MuIVo3MNk4EcD-gFd-WDK1PVoKzfwxh8z86-b_JHf5HNvlJAmaATyIsAxJJ8Fm96KJxBXwIf290qZcTzBEEa21lObPv4tGoinpHbGCFlzoVJsZnGkb9g7sb268t3Ut6z1DhzWz790GD1wulkLB2-vs4qIvL08chhMiLj8RZhyphenhyphenwWX7b/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Evaluations of AltUp on T5 models of various sizes and popular benchmarks. AltUp consistently leads to sizable speedups relative to baselines at the same accuracy. Latency is measured on &lt;a href=&quot;https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&quot;>;TPUv3&lt;/a>; with 8 cores. Speedup is defined as the change in latency divided by the AltUp latency (B = T5 Base, L = T5 Large, XL = T5 XL models).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AltUp&#39;s relative performance improves as we apply it to larger models — compare the relative speedup of T5 Base + AltUp to that of T5 Large + AltUp. This demonstrates the scalability of AltUp and its improved performance on even larger models. Overall, AltUp consistently leads to models with better predictive performance than the corresponding baseline models with the same speed on all evaluated model sizes and benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Extensions: Recycled AltUp&lt;/h2>; &lt;p>; The AltUp formulation adds an insignificant amount of per-layer computation, however, it does require using a wider embedding table. In certain scenarios where the vocabulary size (ie, the number of distinct tokens the tokenizer can produce) is very large, this may lead to a non-trivial amount of added computation for the initial embedding lookup and the final &lt;a href=&quot;https://www.google.com/url?q=https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1699300629405641&amp;amp;usg=AOvVaw3tnZ18-LvY0tPP_NClQ_P-&quot;>;linear + softmax operation&lt;/a>;. A very large vocabulary may also lead to an undesirable amount of added embedding parameters. To address this, Recycled-AltUp is an extension of AltUp that avoids these computational and parameter costs by keeping the embedding table&#39;s width the same. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s989/image4.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;515&quot; data-original-width=&quot;989&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEha4G9LHEhhkFzDTHspvL4DKoYq3BDO7KJfkEY3oy4AAOOHJnNmM0pYp_JjmUiuVTjAAddCowxzCLlcAA7CNKIwId8pujnAXoUMH2kM7ecQLHLfRyfqSqC5RKI-7yHwo8SaXN_CCQzVSHxldF7phMVY7CiaHUYIFIpMdWxpwBAZSpQem1RmBkyvEdjUYg2K/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Illustration of the Architecture for Recycled-AltUp with &lt;em>;K&lt;/em>; = 2.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In Recycled-AltUp, instead of widening the initial token embeddings, we replicate the embeddings &lt;em>;K&lt;/em>; times to form a wider token representation. Hence, Recycled-AltUp adds virtually no additional parameters relative to the baseline transformer, while benefiting from a wider token representation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s1999/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRoANGe48-HncQDuNpxAVSUGf2GPp1qp6KpnvqMbQ5Mejlq4gWpq96vu9FxkDub8aO3RqYyDmTki2Xqkj9drOprwqrHDyoQvHI4oziYFguhDnkkYZO4GXdhAKgIlfffHJ5Po7mEVGCaAkOH7oobnt-8qKjWlerZp4EeoAjZg6IG_CEVO3hhsQvlPaV2GHU/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Recycled-AltUp on T5-B/L/XL compared to baselines. Recycled-AltUp leads to strict improvements in pre-training performance without incurring any perceptible slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also evaluate the lightweight extension of AltUp, Recycled-AltUp, with &lt;em>;K&lt;/em>; = 2 on T5 base, large, and XL models and compare its pre-trained accuracy and speed to those of baselines. Since Recycled-AltUp does not require an expansion in the embedding table dimension, the models augmented with it have virtually the same number of trainable parameters as the baseline models. We again observe consistent improvements compared to the dense baselines. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why does AltUp work?&lt;/h2>; &lt;p>; AltUp increases a model&#39;s capacity by adding and &lt;em>;efficiently&lt;/em>; leveraging auxiliary parameters to the embedding table, and maintaining the higher dimensional representation across the layers. We believe that a key ingredient in this computation lies in AltUp&#39;s prediction mechanism that performs an ensemble of the different blocks. This weighted combination enables continuous message passing to the entire vector despite activating only sub-blocks of it in each layer. Recycled-AltUp, on the other hand, does not add any additional parameters to the token embeddings. However, it still confers the benefit of simulating computation in a higher dimensional representation space since a higher dimensional representation vector is maintained when moving from one transformer layer to another. We conjecture that this aids the training by augmenting the flow of information through the network. An interesting research direction is to explore whether the benefits of Recycled-AltUp can be explained entirely by more favorable training dynamics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;We thank our collaborators Cenk Baykal, Dylan Cutler, and Rina Panigrahy at Google Research, and Nikhil Ghosh at University of California, Berkeley (work done during research internship at Google).&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;This is because the feedforward layers of a Transformer are typically scaled quadratically with the model dimension.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;br />; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn2&quot;>;&lt;b>;2&lt;/b>;&lt;/a>;&lt;/sup>;This factor depends on the user-specified expansion factor, but is typically 1, ie, we double the embedding table dimension.&amp;nbsp;&lt;a href=&quot;#fnref2&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4661241136828203614/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4661241136828203614&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/alternating-updates-for-efficient.html&quot; rel=&quot;alternate&quot; title=&quot;Alternating updates for efficient transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg68VOGvAuGk6w6db-De6SNc2OstzYrfUaCY4c03VVn_hAVb-wVsqk5zy07izav41UP3ZpurAn5kUs5QAJSzMU7Y_4en5TInN_0DA6iC8rUqAO0qmnwZXWll7yZyWvL_1HndCAtk7USVEyNxrXezwlmWDfrtQsEsQhnfjNkYvkoK5QiSIXESnXcxGvTOeKk/s72-c/altup.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8959024707515398632&lt;/id>;&lt;published>;2023-11-03T11:23:00.001-07:00&lt;/published>;&lt;updated>;2023-11-03T11:27:07.156-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Best of both worlds: Achieving scalability and quality in text clustering&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sara Ahmadian and Mehran Kazemi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a fundamental, ubiquitous problem in data mining and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning, where the goal is to group together similar items. The standard forms of clustering are metric clustering and graph clustering. In metric clustering, a given metric space defines distances between data points, which are grouped together based on their &lt;em>;separation&lt;/em>;. In graph clustering, a given graph connects similar data points through edges, and the clustering process groups data points together based on the &lt;em>;connections&lt;/em>; between them. Both clustering forms are particularly useful for large corpora where class labels can&#39;t be defined. Examples of such corpora are the ever-growing digital text collections of various internet platforms, with applications including organizing and searching documents, identifying patterns in text, and recommending relevant documents to users (see more examples in the following posts: &lt;a href=&quot;https://blog.research.google/2010/10/clustering-related-queries-based-on.html?m=1&quot;>;clustering related queries based on user intent&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2021/10/practical-differentially-private.html&quot;>;practical differentially private clustering&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The choice of text clustering method often presents a dilemma. One approach is to use embedding models, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BERT_(language_model)&quot;>;BERT&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>;, to define a metric clustering problem. Another is to utilize &lt;a href=&quot;https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-cross-attention-3b396266d82e&quot;>;cross-attention&lt;/a>; (CA) models, such as &lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>;, to define a graph clustering problem. CA models can provide highly accurate similarity scores, but constructing the input graph may require a prohibitive quadratic number of inference calls to the model. On the other hand, a metric space can efficiently be defined by distances of embeddings produced by embedding models. However, these similarity distances are typically of substantial lower-quality compared to the similarity signals of CA models, and hence the produced clustering can be of much lower-quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s835/figure%20top.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRHbZ9egdnFlc6ZCmLeODTdPZoOXcjjEGCLHP8Ve3aihMExIjyLiX4n0Zy9l4nmkx-3k6rxYN5696irWyubaYAqecpxFTmP1wzjBo0MKnZMnnTjQrdSxKIWGzsySs8XB-OVHyyxKJBWI0dlrYvb_S204S6aSbkPTx8_FQTJXk8f_WDIoGgqNJTACuXNewu/s16000/figure%20top.png&quot; />;&lt;/a>;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt; a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s835/figure%20bottom.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;641&quot; data-original-width=&quot;835&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoY4juklU42gca4fdtnUN4t5j8Z-4rEEhyphenhyphenDi4kRfkV1wfqBs_wyldX9sU2pwtMHdVZPZf2vWnDweEPzwiLseYZenC3afIuaDRf_7arpoF-xnMAwgGXs48a_3wNG2trhN0CTkHgMVa6ZQ7ToP7IE6fpIFmbTF7YYGPFgYU3cMtyyBYIfSWVLVvRXY6Dueiy/s16000/figure%20bottom.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the embedding-based and cross-attention–based similarity scoring functions and their scalability vs. quality dilemma.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>; Motivated by this, in “&lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals”, presented at ICLR 2023&lt;/a>;, we describe a novel clustering algorithm that effectively combines the scalability benefits from embedding models and the quality from CA models. This graph clustering algorithm has query access to both the CA model and the embedding model, however, we apply a budget on the number of queries made to the CA model. This algorithm uses the CA model to answer edge queries, and benefits from unlimited access to similarity scores from the embedding model. We describe how this proposed setting bridges algorithm design and practical considerations, and can be applied to other clustering problems with similar available scoring functions, such as clustering problems on images and media. We demonstrate how this algorithm yields high-quality clusters with almost a linear number of query calls to the CA model. We have also &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;open-sourced&lt;/a>; the data used in our experiments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The clustering algorithm&lt;/h2>; &lt;p>; The KwikBucks algorithm is an extension of the well-known &lt;a href=&quot;https://cesa-bianchi.di.unimi.it/Algo2/Note/kwik.pdf&quot;>;KwikCluster algorithm (Pivot algorithm)&lt;/a>;. The high-level idea is to first select a set of documents (ie, centers) with no similarity edge between them, and then form clusters around these centers. To obtain the quality from CA models and the runtime efficiency from embedding models, we introduce the novel &lt;em>;combo similarity oracle&lt;/em>; mechanism. In this approach, we utilize the embedding model to guide the selection of queries to be sent to the CA model. When given a set of center documents and a target document, the combo similarity oracle mechanism outputs a center from the set that is similar to the target document, if present. The combo similarity oracle enables us to save on budget by limiting the number of query calls to the CA model when selecting centers and forming clusters. It does this by first ranking centers based on their embedding similarity to the target document, and then querying the CA model for the pair (ie, target document and ranked center), as shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s1601/process.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjRaIWRUaPHb_4QFG2wusDoqrVUJkePKRYgLhU992eEUjnI8gpZ2JzIDBdOEb56zHEBammMVEDj_hgejIhCLcmZK5r8ADTVn54ttrOPnOnR2tuk2J8UVIRQoRc_LOHA56_WnQSWPZbB31dZMkj4VI6btb7NkdUOtM1iM1VAKP5t3F-KiBxXySFb4BrYdPmy/s16000/process.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;A combo similarity oracle that for a set of documents and a target document, returns a similar document from the set, if present.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We then perform a post processing step to merge clusters if there is a strong connection between two of them, ie, when the number of connecting edges is higher than the number of missing edges between two clusters. Additionally, we apply the following steps for further computational savings on queries made to the CA model, and to improve performance at runtime: &lt;/p>; &lt;ol>; &lt;li>;We leverage &lt;a href=&quot;https://arxiv.org /pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; to form a set of centers from a set of randomly selected documents instead of selecting these centers from all the documents (in the illustration below, the center nodes are red ）。 &lt;/li>;&lt;li>;We apply the combo similarity oracle mechanism to perform the cluster assignment step in parallel for all non-center documents and leave documents with no similar center as singletons. In the illustration below, the assignments are depicted by blue arrows and initially two (non-center) nodes are left as singletons due to no assignment. &lt;/li>;&lt;li>;In the post-processing step, to ensure scalability, we use the embedding similarity scores to filter down the potential mergers (in the illustration below, the green dashed boundaries show these merged clusters). &lt;/li>; &lt;/ol>;&lt;div>;&lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s1322/Kwikbux%20gif.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;704&quot; data-original-width=&quot;1322&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1bJ65zsG-PYQwvRncviG5befEFPFhA95xoPUB_QV4hQjvz9vNEi-k4_WfZnMG9-25_t6-lpyNCE2MfD_ZSa6q30CjgBF5iRcmH3n6ThprinvD_Qx6rNCIOYuI4ml2U_rI3qI9q6E5qKW9qL1PKAzkOO6wyr8_kS2iDb_FIx2W7Hgjewh5ms2oe0QClfrj/s16000/Kwikbux%20gif.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of progress of the clustering algorithm on a given graph instance. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate the novel clustering algorithm on various datasets with different properties using different embedding-based and cross-attention–based models. We compare the clustering algorithm&#39;s performance with the two best performing baselines (see the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details): &lt;/p>; &lt;ul>; &lt;li>;The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only. &lt;/li>;&lt;li>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;Spectral clustering&lt;/a>; on the &lt;em>;&lt;a href=&quot;https://en.wikipedia .org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor graph&lt;/a>;&lt;/em>; (kNN) formed by querying the CA model for the &lt;em>;k&lt;/em>;-nearest neighbors of each vertex from embedding-based相似。 &lt;/li>; &lt;/ul>; &lt;p>; To evaluate the quality of clustering, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision and recall&lt;/a>;. Precision is used to calculate the percentage of similar pairs out of all co-clustered pairs and recall is the percentage of co-clustered similar pairs out of all similar pairs. To measure the quality of the obtained solutions from our experiments, we use the &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>;, which is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_mean&quot;>;harmonic mean&lt;/a>; of the precision and recall, where 1.0 is the highest possible value that indicates perfect precision and recall, and 0 is the lowest possible value that indicates if either precision or recall are zero. The table below reports the F1-score for Kwikbucks and various baselines in the case that we allow only a linear number of queries to the CA model. We show that Kwikbucks offers a substantial boost in performance with a 45% relative improvement compared to the best baseline when averaging across all datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s1574/results.jpg&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;582&quot; data-original-width=&quot;1574&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQbrQVGenUUG0sSZ_Ofe9xIiJ068UIBcUCEvH2dUEdji1XMElQJM13RQkuZp2XExktKlpY6VGr6QfujDWyvu9kUlYtIFSV7hKYEgz6D-CUF0Q7UPN4p58EJji5HeVgXfLoAAhatmG74b2zUnewtGKkoyPz9NQIbc43cqKZ35vcyMlJ99PaCdzk1X-WFAqr/s16000/results.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Comparing the clustering algorithm to two baseline algorithms using various public datasets: (1) The &lt;a href=&quot;https://arxiv.org/pdf/2002.11557.pdf&quot;>;query-efficient correlation clustering&lt;/a>; algorithm for budgeted clustering with access to CA only, and (2) &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_clustering&quot;>;spectral clustering&lt;/a>; on the &lt;em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Nearest_neighbor_graph&quot;>;k-nearest neighbor (kNN) graph&lt;/a>;&lt;/em>; formed by querying the CA model for the &lt;em>;k&lt;/ em>;-nearest neighbors of each vertex from embedding-based similarity. Pre-processed datasets can be downloaded &lt;a href=&quot;https://storage.googleapis.com/gresearch/kwikbucks/kwikbucks.zip&quot;>;here&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The figure below compares the clustering algorithm&#39;s performance with baselines using different query budgets. We observe that KwikBucks consistently outperforms other baselines at various budgets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s1236/stackoverflow.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1236&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3CboOkZ4lbp9dmH-5EkPGe1zo6JPlOQbXT1n67ke4qN0kXeZXMQKb4SUM-E6TXuyWF9UF7vvKTtl2lecg-Z5tV0mA6Hobh9NKbmy__dqGRIYcTEIzdfGjCCyih2eZW4xF33n7Y1pJGpwyDvQ75rwQsp0kZICGSPoyR8ahiBQwcZvT7ZLLeAA1BQkJtLwU/s16000/stackoverflow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of KwikBucks with top-2 baselines when allowed different budgets for querying the cross-attention model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Text clustering often presents a dilemma in the choice of similarity function: embedding models are scalable but lack quality, while cross-attention models offer quality but substantially hurt scalability. We present a clustering algorithm that offers the best of both worlds: the scalability of embedding models and the quality of cross-attention models. KwikBucks can also be applied to other clustering problems with multiple similarity oracles of varying accuracy levels. This is validated with an exhaustive set of experiments on various datasets with diverse properties. See the &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project was initiated during Sandeep Silwal&#39;s summer internship at Google in 2022. We would like to express our gratitude to our co-authors, Andrew McCallum, Andrew Nystrom, Deepak Ramachandran, and Sandeep Silwal, for their valuable contributions to this work. We also thank Ravi Kumar and John Guilyard for assistance with this blog post.&lt;/em>; &lt;/p>;&lt;/div>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8959024707515398632/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both-worlds-achieving.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8959024707515398632&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8959024707515398632&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot; rel=&quot;alternate&quot; title=&quot;Best of both worlds: Achieving scalability and quality in text clustering&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjR2mD5afdT_Qg9Ex4Lj94FaxB9KHiq20iLoDlOY8S8Rk5XsV6n_4dU9CFbCvSeBSONGQYqy-yMGY6-KU_y_RGICOpz76GNdzv7ecxor_rVnF31lZOd3STQF4MIE4F_EadrSI1DXY67MXnsCswY3w3X8vX8KgU_rRPs6eTZndYAbVcEezgwaUsdZEAHD59Z/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;