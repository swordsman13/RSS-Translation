<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-08-16T04:25:46.788-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="optimization"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="ICML"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="Responsible AI"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="RAI-HCT Highlights"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1264&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;发布>;2023-08-15T12:59:00.001-07:00&lt;/发布>;&lt;更新>;2023-08- 15T13:00:54.887-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;推荐系统&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;社交网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;研究：社会意识暂时因果解码器推荐系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Eltayeb Ahmed 发布， Google 研究部研究工程师兼高级研究科学家 Subhrajit Roy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqm TvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s837/学习英雄。 png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 阅读对于年轻学生有很多好处，例如&lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure。 pdf&quot;>;更好的语言和生活技能&lt;/a>;，并且快乐阅读已被证明与&lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;学业成功相关&lt; /a>;.此外，学生们还表示，阅读&lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;情绪健康得到改善&lt;/a>;，并且&lt;a href=&quot;https:// eric.ed.gov/?id=ED496343&quot;>;更好的常识和对其他文化的更好的理解&lt;/a>;。由于线上和线下有大量的阅读材料，找到适合年龄的、相关的和引人入胜的内容可能是一项具有挑战性的任务，但帮助学生做到这一点是让他们参与阅读的必要步骤。向学生提供相关阅读材料的有效推荐有助于学生持续阅读，而这正是机器学习 (ML) 可以提供帮助的地方。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 机器学习已广泛应用于构建&lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592 -5&quot;>;推荐系统&lt;/a>;，适用于各种类型的数字内容，从视频到书籍再到电子商务项目。推荐系统在一系列数字平台上使用，以帮助向用户展示相关且有吸引力的内容。在这些系统中，机器学习模型经过训练，可以根据用户偏好、用户参与度和推荐项目单独向每个用户推荐项目。这些数据为模型提供了强大的学习信号，使其能够推荐可能感兴趣的项目，从而改善用户体验。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;研究：社交意识暂时因果解码器推荐系统&lt;/a>;”中，我们提出了一个有声读物的内容推荐系统在教育环境中考虑阅读的社会性质。我们与 &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 合作开发了 STUDY 算法，这是一个教育非营利组织，旨在促进阅读障碍学生的阅读，通过学校向学生提供有声读物广泛的订阅计划。利用 Learning Ally 图书馆中的各种有声读物，我们的目标是帮助学生找到合适的内容，以帮助提高他们的阅读体验和参与度。由于一个人的同龄人当前正在阅读的内容会对他们感兴趣的阅读内容产生重大影响，因此我们共同处理同一教室中学生的阅读参与历史。这使得我们的模型能够从有关学生本地社交群体（在本例中为他们的教室）当前趋势的实时信息中受益。 &lt;/p>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 拥有一个大型数字图书馆，其中包含针对以下人群的精选有声读物：学生，使其非常适合构建社会推荐模型，以帮助提高学生的学习成果。我们收到了两年的匿名有声读物消费数据。数据中的所有学生、学校和分组都是匿名的，仅通过随机生成的 ID 进行识别，无法通过 Google 追溯到真实实体。此外，所有潜在可识别元数据仅以聚合形式共享，以保护学生和机构不被重新识别。这些数据包括学生与有声读物互动的带时间戳的记录。对于每次交互，我们都有一个匿名的学生 ID（包括学生的年级和匿名的学校 ID）、有声读物标识符和日期。虽然许多学校将单一年级的学生分布在多个教室中，但我们利用此元数据做出简化的假设，即同一所学校和同一年级的所有学生都在同一教室。虽然这为建立更好的社交推荐模型提供了基础，但值得注意的是，这并不能让我们重新识别个人、班级群体或学校。 &lt;/p>; &lt;br />; &lt;h2>;STUDY 算法&lt;/h2>; &lt;p>; 我们将推荐问题描述为&lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;点击率&lt;/a>; 预测问题，我们对用户与每个特定项目交互的条件概率进行建模，条件是 1）用户和项目特征以及 2）当前用户的项目交互历史序列。 &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;之前的工作&lt;/a>;建议&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;基于 /a>; 的模型是 Google Research 开发的一种广泛使用的模型类，非常适合对这个问题进行建模。当单独处理每个用户时，这将成为一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;自回归序列建模问题&lt;/a>;。我们使用这个概念框架来建模我们的数据，然后扩展这个框架来创建研究方法。 &lt;/p>; &lt;p>; 虽然这种点击率预测方法可以对单个用户过去和未来的项目偏好之间的依赖关系进行建模，并且可以在训练时学习用户之间的相似性模式，但它无法在推理时对不同用户之间的依赖关系进行建模时间。为了认识阅读的社会性并弥补这一缺陷，我们开发了 STUDY 模型，该模型将每个学生阅读的多个书籍序列连接成一个序列，该序列收集单个教室中多个学生的数据。 &lt;/p>; &lt;p>; 然而，如果要通过 Transformer 建模，这种数据表示需要仔细研究。在 Transformer 中，注意力掩码是控制哪些输入可用于通知哪些输出的预测的矩阵。使用序列中的所有先验标记来通知输出预测的模式导致传统上在因果解码器中发现的上三角注意矩阵。然而，由于输入 STUDY 模型的序列不是按时间排序的，即使它的每个组成子序列都是标准的&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;因果解码器&lt;/a>;不再适合这个序列。当尝试预测每个标记时，模型不允许关注序列中位于其之前的每个标记；其中一些令牌可能具有较晚的时间戳，并且包含在部署时不可用的信息。 &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tq Rd-ZWDXOCRpHAy0FZE9sV0xB8reyJ-- Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width = =第1787章pHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot;宽度=&quot; 400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们显示了通常用于因果解码器。每列代表一个输出，每列代表一个输出。特定位置处的矩阵条目的值为 1（显示为蓝色）表示模型在预测相应列的输出时可以观察到该行的输入，而值为 0（显示为白色）表示相反的情况.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; STUDY 模型建立在因果变换器的基础上，通过将三角矩阵注意掩码替换为灵活的注意掩码，其值基于时间戳，以允许关注不同的子序列。与常规 Transformer 相比，常规 Transformer 不允许跨不同子序列进行关注，并且在序列内具有三角矩阵掩码，而 STUDY 在序列内维护因果三角关注矩阵，并且跨序列具有灵活的值，其值取决于时间戳。因此，序列中任何输出点的预测都是由过去相对于当前时间点发生的所有输入点通知的，无论它们是出现在序列中当前输入之前还是之后。这种因果约束很重要，因为如果不在训练时强制执行，模型可能会学习使用未来的信息进行预测，而这对于现实世界的部署来说是不可用的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6Zm UAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFHANO2eJdeFMIWt7K 58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;在（a）中，我们展示了一个具有因果注意力的顺序自回归变压器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQf vFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB 8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; !--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt; tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2 _wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9Fd caJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们使用 Learning Ally 数据集来训练 STUDY 模型以及多个基线以进行比较。我们实现了一个自回归点击率转换器解码器，我们将其称为“个体”，一个&lt;em>;k&lt;/em>;最近邻基线（KNN），以及一个可比较的社交基线，社交注意力记忆网络（SAMN） 。我们使用第一学年的数据进行训练，并使用第二学年的数据进行验证和测试。 &lt;/p>; &lt;p>; 我们通过测量用户实际交互的下一个项目在模型的前 &lt;em>;n&lt;/em>; 个推荐中的时间百分比来评估这些模型，即，hits@&lt;em>;n， &lt;/em>;对于&lt;em>;n&lt;/em>;的不同值。除了在整个测试集上评估模型之外，我们还报告了模型在测试集的两个子集上的得分，这两个子集比整个数据集更具挑战性。我们观察到，学生通常会在多个会话中与有声读物进行交互，因此简单地推荐用户最近读过的一本书将是一个强有力的简单推荐。因此，第一个测试子集（我们称之为“非延续”）是指当学生与与之前交互不同的书籍进行交互时，我们仅查看每个模型在推荐上的表现。我们还观察到学生会重温他们过去读过的书籍，因此通过将针对每个学生的推荐仅限于他们过去读过的书籍，可以在测试集上取得出色的表现。尽管向学生推荐旧的收藏夹可能很有价值，但推荐系统的大部分价值来自于向用户展示新的和未知的内容。为了衡量这一点，我们在学生第一次与标题交互的测试集子集上评估模型。我们将这个评估子集命名为“小说”。 &lt;/p>; &lt;p>; 我们发现，在我们评估的几乎每个切片中，STUDY 都优于所有其他测试模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr 7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx 0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot;宽度=“400”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们比较了 Study、Individual、KNN 和 SAMN 四种模型的性能。我们用 hist@5 来衡量性能，即模型在模型的前 5 个推荐中建议用户阅读的下一个标题的可能性有多大。我们在整个测试集（全部）以及新颖的和非连续的分割上评估模型。我们发现 STUDY 在所有分组中始终优于其他三个模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;适当分组的重要性&lt;/h2>; &lt;p>; STUDY 算法的核心是将用户组织成组，并在模型的单次前向传递中对同一组中的多个用户进行联合推理。我们进行了一项消融研究，研究了实际分组对模型性能的重要性。在我们提出的模型中，我们将同一年级和学校的所有学生分组在一起。然后，我们对由同一年级和学区的所有学生定义的组进行实验，并将所有学生放入一个组中，并在每次前向传递中使用随机子集。我们还将这些模型与个体模型进行比较以供参考。 &lt;/p>; &lt;p>; 我们发现使用更加本地化的分组更为有效，学校和年级分组的效果优于地区和年级分组。这支持了这样的假设：学习模型之所以成功，是因为阅读等活动的社会性质——人们的阅读选择可能与周围人的阅读选择相关。这两种模型都优于其他两种模型（单组和个人），其中不使用年级水平对学生进行分组。这表明来自具有相似阅读水平和兴趣的用户的数据有利于性能。 &lt;/p>; &lt;br />; &lt;h2>;未来的工作&lt;/h2>; &lt;p>; 这项工作仅限于为假设社交关系同质的用户群进行推荐建模。将来，对关系不均匀的用户群体进行建模将是有益的，即，存在明显不同类型的关系或已知不同关系的相对强度或影响。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作涉及由研究人员、软件工程师和教育主题专家组成的多学科团队的协作努力。我们感谢我们的合著者：来自 Google 的 Diana Mincu、Lauren Harrell 和 Katherine Heller。我们还要感谢 Learning Ally 的同事 Jeff Ho、Akshat Shah、Erin Walker 和 Tyler Bastian，以及 Google 的合作者 Marc Repnyek、Aki Estrella、Fernando Diaz、Scott Sanner、Emily Salkey 和 Lev Proleev。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; 类型=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;链接 href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai .googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;STUDY：社交意识时间因果解码器推荐系统&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 “rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqa JM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;发布>;2023-08-09T11:32:00.001-07:00&lt;/发布>;&lt;更新>;2023-08-09T12 :26:53.831-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;文档理解方面的进展&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队 Google 研究部软件工程师 Sandeep Tata&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85z UbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 过去几年，能够自动处理复杂业务文档并将其转换为结构化对象的系统取得了快速进展。一个可以从收据、保险报价等文档中&lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;自动提取数据&lt;/a>;的系统和财务报表，通过避免容易出错的手动工作，有可能显着提高业务工作流程的效率。基于 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; 架构的最新模型已经展示了&lt;a href=&quot; https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;准确度显着提升&lt;/a>;。更大的模型，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;，也被用来进一步简化这些业务工作流程。然而，学术文献中使用的数据集未能捕捉到现实世界用例中遇到的挑战。因此，学术基准报告了很强的模型准确性，但这些相同的模型在用于复杂的现实应用程序时表现不佳。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU：视觉丰富文档理解的基准&lt; /a>;”，在 &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>; 上发布，我们宣布发布新的&lt;a href=&quot;https://research.google /resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集旨在弥补这一差距，帮助研究人员更好地跟踪文档理解任务的进展。根据经常使用文档理解模型的现实文档类型，我们列出了良好的文档理解基准的五个要求。然后，我们描述了研究界当前使用的大多数数据集为何无法满足其中一项或多项要求，而 VRDU 却满足了所有这些要求。我们很高兴地宣布公开发布 VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;数据集&lt;/a>;和&lt;a href=&quot;https ://github.com/google-research-datasets/vrdu&quot;>;评估代码&lt;/a>;，采用&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;知识共享许可&lt;/a>;一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基准要求&lt;/h2>; &lt;p>; 首先，我们比较了最先进的模型准确性（例如，使用 &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/ a>;）关于现实世界用例与学术基准（例如，&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.a>;） net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;）。我们观察到，最先进的模型与学术基准结果不符，并且在现实世界中的准确性要低得多。接下来，我们将经常使用文档理解模型的典型数据集与学术基准进行比较，并确定了五个数据集要求，使数据集能够更好地捕获现实应用程序的复杂性：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;丰富模式：&lt;/strong>;在实践中，我们看到了各种用于结构化提取的丰富模式。实体具有不同的数据类型（数字、字符串、日期等），这些数据类型可能是必需的、可选的或在单个文档中重复，甚至可能是嵌套的。像（标题、问题、答案）这样的简单平面模式的提取任务并不能反映实践中遇到的典型问题。 &lt;/li>;&lt;li>;&lt;strong>;布局丰富的文档：&lt;/strong>;文档应该具有复杂的布局元素。实际设置中的挑战来自以下事实：文档可能包含表格、键值对、在单列和双列布局之间切换、不同部分具有不同的字体大小、包括带有标题甚至脚注的图片。将此与大多数文档以句子、段落和带有节标题的章节组织的数据集进行对比 - 这些文档通常是 &lt;a href=&quot;https://arxiv.org/abs 上经典自然语言处理文献的焦点/2007.14062&quot;>;长&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;输入&lt;/a>;。 &lt;/li>;&lt;li>;&lt;strong>;多样化模板：&lt;/strong>;基准测试应包括不同的结构布局或模板。对于大容量模型来说，通过记忆结构从特定模板中提取数据是微不足道的。然而，在实践中，人们需要能够推广到新的模板/布局，这是基准测试中的训练-测试分割应该衡量的一种能力。 &lt;/li>;&lt;li>;&lt;strong>;高质量 OCR&lt;/strong>;：文档应具有高质量&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;光学字符识别&lt;/a>; (OCR) 结果。我们此基准测试的目标是专注于 VRDU 任务本身，并排除 OCR 引擎选择带来的可变性。 &lt;/li>;&lt;li>;&lt;strong>;令牌级注释&lt;/strong>;：文档应包含可以映射回相应输入文本的真实注释，以便每个令牌都可以注释为相应实体的一部分。这与简单地提供要为实体提取的值的文本形成对比。这是生成干净的训练数据的关键，我们不必担心与给定值的偶然匹配。例如，在某些收据中，如果税额为零，“税前总计”字段可能具有与“总计”字段相同的值。具有令牌级别注释会阻止我们生成训练数据，其中匹配值的两个实例都被标记为“total”字段的真实值，从而产生嘈杂的示例。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn 5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600 /Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6Ai WX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU 数据集和任务&lt;/h2>; &lt;p>; VRDU 数据集是两个公开可用数据集的组合，&lt;a href=&quot;https://efile.40%;&quot;>; fara.gov/ords/fara/f?p=1235:10&quot;>;注册表&lt;/a>;和&lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;广告购买表单&lt;/a>;。这些数据集提供了代表现实世界用例的示例，并满足上述五个基准要求。 &lt;/p>; &lt;p>; Ad-buy Forms 数据集包含 641 个包含政治广告详细信息的文档。每份文件都是由电视台和竞选团队签署的发票或收据。文档使用表格、多列和键值对来记录广告信息，例如产品名称、播放日期、总价、发布日期和时间。 &lt;/p>; &lt;p>; 登记表数据集包含 1,915 个文档，其中包含有关外国代理人在美国政府登记的信息。每份文件都记录了涉及需要公开披露的活动的外国代理人的基本信息。内容包括注册人名称、相关部门地址、活动目的等详细信息。 &lt;/p>; &lt;p>; 我们从公众&lt;a href=&quot;https://www.fcc.gov/&quot;>;联邦通信委员会&lt;/a>; (FCC) 和&lt;a href=&quot; https://www.justice.gov/nsd-fara&quot;>;外国代理人登记法&lt;/a>; (FARA) 网站，并使用 &lt;a href=&quot;https://cloud.google.com/ 将图像转换为文本&quot;>;Google Cloud 的&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;。我们丢弃了少量几页长的文档，并且处理在两分钟内没有完成。这也使我们能够避免发送很长的文档进行手动注释——对于单个文档来说，这项任务可能需要一个多小时。然后，我们为具有文档标记任务经验的注释者团队定义了模式和相应的标记指令。 &lt;/p>; &lt;p>; 还向注释者提供了一些我们自己标记的示例标记文档。该任务要求注释者检查每个文档，围绕每个文档架构中实体的每次出现绘制边界框，并将该边界框与目标实体相关联。第一轮标注后，一组专家被指派对结果进行审核。修正后的结果包含在已发布的 VRDU 数据集中。有关标记协议和每个数据集架构的更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG 2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i7 2av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;现有学术基准 (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.net/ pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;、&lt;a href=&quot;https:// /arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;、&lt;a href=&quot;https ://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;）未达到我们为项目确定的五项要求中的一项或多项良好的文档理解基准。 VRDU 满足了所有这些要求。请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;，了解每个数据集的背景以及它们如何未能满足一项或多项要求的讨论。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们构建了四个不同的模型训练集，分别有 10、50、100 和 200 个样本。然后，我们使用三个任务（如下所述）评估 VRDU 数据集：(1) 单模板学习、(2) 混合模板学习和 (3) 看不见的模板学习。对于每项任务，我们在测试集中包含 300 个文档。我们使用测试集上的 &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 分数&lt;/a>; 来评估模型。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;单模板学习&lt;/em>; (STL)：这是最简单的场景，其中训练、测试和验证集仅包含单个模板。这个简单的任务旨在评估模型处理固定模板的能力。当然，我们期望此任务的 F1 分数非常高（0.90+）。 &lt;/li>;&lt;li>;&lt;em>;混合模板学习&lt;/em>;（MTL）：此任务与大多数相关论文使用的任务类似：训练集、测试集和验证集都包含属于同一集的文档模板。我们从数据集中随机抽取文档并构建分割，以确保每个模板的分布在采样过程中不会改变。 &lt;/li>;&lt;li>;&lt;em>;未见模板学习&lt;/em>;（UTL）：这是最具挑战性的设置，我们评估模型是否可以泛化到未见模板。例如，在注册表数据集中，我们使用三个模板中的两个来训练模型，并使用其余一个来测试模型。训练、测试和验证集中的文档来自不相交的模板集。据我们所知，以前的基准测试和数据集没有明确提供这样的任务，旨在评估模型泛化到训练期间未见过的模板的能力。 &lt;/li>; &lt;/ul>; &lt;p>; 目标是能够评估模型的数据效率。在我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;中，我们比较了使用 STL、MTL 和 UTL 任务的两个最新模型，并提出了三个观察结果。首先，与其他基准测试不同，VRDU 具有挑战性，并表明模型有很大的改进空间。其次，我们表明，即使是最先进的模型，few-shot 性能也低得惊人，即使是最好的模型，其 F1 分数也低于 0.60。第三，我们表明模型很难处理结构化的重复字段，并且在这些字段上的表现特别差。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们发布了新的&lt;a href=&quot;https:// Research.google/resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集，可帮助研究人员更好地跟踪文档理解任务的进度。我们描述了为什么 VRDU 更好地反映了该领域的实际挑战。我们还提出了实验，表明 VRDU 任务具有挑战性，并且与文献中通常使用的数据集（典型的 F1 分数为 0.90+）相比，最近的模型有很大的改进空间。我们希望 VRDU 数据集和评估代码的发布有助于研究团队推进文档理解的最新技术。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;非常感谢王子龙、周一超、魏Wei 和 Chen-Yu Lee 与 Sandeep Tata 共同撰写了这篇论文。感谢 Marc Najork、Riham Mansour 以及 Google Research 和 Cloud AI 团队的众多合作伙伴提供了宝贵的见解。感谢 John Guilyard 创建本文中的动画。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /ai.googleblog.com/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;文档理解方面的进展&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google人工智能&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http ://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/作者>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV 8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;发布>;2023-08-08T14:02:00.000-07:00&lt;/发布>;&lt;更新>;2023-08-08T14： 02:34.659-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;AdaTape：具有自适应计算和动态读写的基础模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：薛福兆、研究实习生和研究科学家 Mostafa Dehghani，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIq JhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;显示：无；” />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;自适应计算&lt;/a>;是指机器学习系统根据环境变化调整其行为的能力。虽然传统的神经网络具有固定的功能和计算能力，即它们花费相同数量的 FLOP 来处理不同的输入，但具有自适应和动态计算的模型会根据输入的复杂性来调整其专用于处理每个输入的计算预算。输入。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 神经网络中的自适应计算具有吸引力有两个关键原因。首先，引入适应性的机制提供了归纳偏差，可以在解决一些具有挑战性的任务中发挥关键作用。例如，为不同的输入启用不同数量的计算步骤对于解决需要不同深度的建模层次结构的算术问题至关重要。其次，它使从业者能够通过动态计算提供的更大灵活性来调整推理成本，因为可以调整这些模型以花费更多的 FLOP 来处理新输入。 &lt;/p>; &lt;p>; 神经网络可以通过对各种输入使用不同的函数或计算预算来实现自适应。深度神经网络可以被认为是一个基于输入及其参数输出结果的函数。为了实现自适应函数类型，根据输入选择性地激活参数子集，这一过程称为条件计算。在 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>; 的研究中探索了基于函数类型的自适应性，其中每个输入的稀疏激活参数样本是通过路由确定的。自适应计算的另一个研究领域涉及动态计算预算。与标准神经网络（例如 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;）不同，&lt;a href= &quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling -to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;其计算预算对于不同的样本是固定的，&lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;最近的研究&lt;/a>;表明，自适应计算预算可以提高 Transformer 不足的任务的性能。其中许多工作通过使用动态深度来分配计算预算来实现自适应性。例如，提出了自适应计算时间（ACT）算法来为递归神经网络提供自适应计算预算。 &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;通用转换器&lt;/a>;将 ACT 算法扩展到转换器，使计算预算取决于用于每个输入示例或标记的转换器层数。最近的研究，例如 &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;，在改进动态停止机制的同时也遵循类似的方法。 &lt;/p>; &lt;p>; 在论文“&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;具有弹性输入序列的自适应计算&lt;/a>;”中，我们介绍了一种利用自适应计算的新模型，称为&lt;em>;AdaTape&lt;/em>;。该模型是基于 Transformer 的架构，使用一组动态令牌来创建弹性输入序列，与以前的作品相比，提供了关于适应性的独特视角。 AdaTape 使用自适应磁带读取机制来根据输入的复杂性确定添加到每个输入的不同数量的磁带令牌。 AdaTape 实现起来非常简单，提供了一个有效的旋钮来在需要时提高准确性，而且与&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;其他自适应基线&lt;/a相比也更加高效>; 因为它直接将自适应性注入到输入序列中，而不是模型深度中。最后，Adatape 在标准任务（例如图像分类）以及算法任务上提供更好的性能，同时保持良好的质量和成本权衡。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;具有弹性输入序列的自适应计算转换器&lt;/h2>; &lt;p>; AdaTape 使用两种自适应函数类型和动态计算预算。具体来说，对于标记化后的一批输入序列（例如，来自视觉变换器中的图像的非重叠补丁的线性投影），AdaTape 使用表示每个输入的向量来动态选择可变大小的磁带标记序列。 &lt;/p>; &lt;p>; AdaTape 使用称为“磁带库”的令牌库来存储通过自适应磁带读取机制与模型交互的所有候选磁带令牌。我们探索了两种不同的创建磁带库的方法：输入驱动的磁带库和可学习的磁带库。 &lt;/p>; &lt;p>; 输入驱动库的总体思想是从输入中提取一组令牌，同时采用与原始模型令牌生成器不同的方法将原始输入映射到输入标记的序列。这使得能够动态地按需访问来自使用不同观点（例如，不同图像分辨率或不同抽象级别）获得的输入的信息。 &lt;/p>; &lt;p>; 在某些情况下，不同抽象级别的标记化是不可能的，因此输入驱动的磁带库是不可行的，例如当很难进一步拆分 &lt;a href=&quot; 中的每个节点时https://arxiv.org/abs/2012.09699&quot;>;图形转换器&lt;/a>;。为了解决这个问题，AdaTape 提供了一种更通用的方法，通过使用一组可训练向量作为磁带令牌来生成磁带库。这种方法被称为可学习库，可以被视为嵌入层，其中模型可以根据输入示例的复杂性动态检索标记。可学习的银行使 AdaTape 能够生成更灵活的磁带库，使其能够根据每个输入示例的复杂性动态调整其计算预算，例如，更复杂的示例从磁带库中检索更多的令牌，这使得模型不会只使用存储在银行中的知识，但由于输入现在更大，所以还要花费更多的 FLOP 来处理它。 &lt;/p>; &lt;p>; 最后，选定的磁带标记将附加到原始输入并馈送到以下转换器层。对于每个转换器层，在所有输入和磁带标记上使用相同的多头注意力。然而，使用了两种不同的前馈网络（FFN）：一种用于来自原始输入的所有令牌，另一种用于所有磁带令牌。通过对输入和磁带令牌使用单独的前馈网络，我们观察到质量稍好一些。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfda EU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxf ccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AdaTape 概述。对于不同的样本，我们从磁带库中挑选不同数量的不同标记。磁带库可以由输入驱动，例如通过提取一些额外的细粒度信息，或者它可以是一组可训练向量。自适应磁带读取用于针对不同的输入递归地选择不同长度的磁带令牌序列。然后将这些标记简单地附加到输入并馈送到变压器编码器。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;AdaTape 提供有用的归纳偏差&lt;/h2>; &lt;p>; 我们在奇偶校验上评估 AdaTape，这对于标准 Transformer 来说是一项非常具有挑战性的任务，以研究 AdaTape 中归纳偏差的影响。对于奇偶校验任务，给定序列 1、0 和 -1，模型必须预测序列中 1 数量的偶数或奇数。 Parity 是最简单的非计数器自由或&lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;周期性正则语言&lt;/a>;，但也许令人惊讶的是，标准 Transformer 无法解决该任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3 EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36 zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;对奇偶校验任务的评估。标准 Transformer 和 Universal Transformer 无法执行此任务，两者都显示出随机猜测基线水平的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;尽管进行了短期评估，对于简单的序列，标准 Transformer 和通用 Transformer 都无法执行奇偶校验任务，因为它们无法在模型中维护计数器。然而，AdaTape 的性能优于所有基线，因为它在其输入选择机制中融入了轻量级循环，提供了归纳偏差，可以实现计数器的隐式维护，这在标准 Transformer 中是不可能的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图像分类评估&lt;/h2>; &lt;p>; 我们还在图像分类任务上评估 AdaTape。为此，我们从头开始在 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; 上训练 AdaTape。下图显示了 AdaTape 和基线方法的准确性，包括 &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>; 和 Universal Transformer ViT（UViT 和 U2T）与它们的速度（以每秒由每个代码处理的图像数量来衡量）。在质量和成本权衡方面，AdaTape 的性能比替代自适应变压器基线要好得多。就效率而言，较大的 AdaTape 模型（就参数数量而言）比较小的基线更快。这些结果与&lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;之前的工作&lt;/a>;的发现一致，该发现表明自适应模型深度架构不太适合许多加速器，例如热塑性聚氨酯。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7Z Ih6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H -Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473 “高度=“285”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd- H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; 宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们通过从头开始在 ImageNet 上进行训练来评估 AdaTape 。对于&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;，我们不仅从论文中报告了他们的结果，还通过从头开始训练重新实​​现了A-ViT，即，A-ViT（我们的）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A AdaTape 行为的研究&lt;/h2>; &lt;p>; 除了奇偶校验任务和 ImageNet-1K 上的性能之外，我们还评估了 AdaTape 在 &lt;a href=&quot;https:// /arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>;验证集。为了更好地理解模型的行为，我们将输入驱动银行上的代币选择结果可视化为热图，其中较浅的颜色意味着该位置被更频繁地选择。热图显示 AdaTape 更频繁地选择中心补丁。这与我们的先验知识相一致，因为中心补丁通常包含更多信息，尤其是在具有自然图像的数据集的背景下，其中主要对象位于图像的中间。这一结果凸显了 AdaTape 的智能性，因为它可以有效地识别信息更丰富的补丁并确定优先级，以提高其性能。 &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgk R2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/ image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYq crCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr >;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们可视化 AdaTape-B/32（左）和 AdaTape-B/16（右）的磁带令牌选择热图。颜色越热/越浅，意味着该位置的补丁被更频繁地选择。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; AdaTape 的特点是自适应磁带读取机制生成的弹性序列长度。这还引入了一种新的感应偏置，使 AdaTape 能够解决对标准变压器和现有自适应变压器都具有挑战性的任务。通过对图像识别基准进行全面的实验，我们证明了当计算保持不变时，AdaTape 的性能优于标准转换器和自适应架构转换器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Mostafa Dehghani ，现供职于 Google DeepMind。 &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTape：具有自适应计算和动态读写的基础模型&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhd Qrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudiEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72- c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4436946873570093049&lt;/id>;&lt;已发布>;2023-08-03T11:24:00.001-07:00&lt;/已发布>;&lt;已更新>; 2023-08-03T11:24:40.465-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“AI”>;&lt;/类别>;&lt;类别方案=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;健康&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;多模式医疗人工智能&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：健康人工智能主管 Greg Corrado 、Google 研究部和 Google 研究部工程与研究副总裁 Yossi Matias&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjm G9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3 /s1600/medpalm.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 医学本质上是一门多模式学科。在提供护理时，临床医生通常会解读各种模式的数据，包括医学图像、临床记录、实验室测试、电子健康记录、基因组学等。在过去十年左右的时间里，人工智能系统在特定模式下的特定任务上取得了专家级的性能——一些人工智能系统&lt;a href=&quot;https://www.nature.com/articles/s41591 -019-0447-x&quot;>;处理 CT 扫描&lt;/a>;，而其他人&lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;分析高倍病理切片&lt;/a>;，还有一些&lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc2112090&quot;>;寻找罕见的遗传变异&lt;/a>;。这些系统的输入往往是复杂的数据，例如图像，它们通常提供结构化输出，无论是离散等级的形式还是&lt;a href=&quot;https://www.nature.com/articles/s41591-018- 0107-6&quot;>;密集图像分割掩模。&lt;/a>;同时，大型语言模型 (LLM) 的容量和功能具有&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;变得如此先进&lt;/a>;，以至于他们通过用简单的语言进行解释和回应，表现出了对医学知识的理解和专业知识。但是，我们如何将这些功能结合在一起来构建可以利用所有这些来源的信息的医疗人工智能系统？ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在今天的博文中，我们概述了为法学硕士带来多模式能力的一系列方法，并分享了关于构建多模式医学法学硕士的可处理性的一些令人兴奋的结果，正如最近的三篇研究论文所述。这些论文反过来概述了如何将&lt;em>;从头&lt;/em>;模式引入法学硕士，如何将最先进的医学成像基础模型移植到对话式法学硕士上，以及建立法学硕士的第一步真正的多模式医疗人工智能系统。如果成功成熟，多模式医学法学硕士可能会成为涵盖专业医学、医学研究和消费者应用的新辅助技术的基础。与我们之前的工作一样，我们强调需要与医学界和医疗保健生态系统合作仔细评估这些技术。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;一系列方法&lt;/h2>; &lt;p>; 最近提出了几种构建多模式法学硕士的方法月 [&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;1&lt;/a>;，&lt;a href=&quot;https:// /www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;2&lt;/a>;，&lt;a href=&quot;https://ai.googleblog.com/2023 /03/palm-e-embodied-multimodal-language.html&quot;>;3&lt;/a>;]，毫无疑问，新方法将在一段时间内继续出现。为了了解为医疗人工智能系统带来新模式的机会，我们将考虑三种广泛定义的方法：工具使用、模型移植和通才系统。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s1600/image1.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8 Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;工具使用&lt;/h2>; &lt;p>; 在 &lt;em >;工具使用方法，一个中心医学法学硕士将各种模式的数据分析外包给一组针对这些任务独立优化的软件子系统：工具。工具使用的常见助记示例是教法学硕士使用计算器而不是自己做算术。在医疗领域，面对胸部 X 光检查的医学法学硕士可以将该图像转发到放射学人工智能系统并整合该响应。这可以通过子系统提供的应用程序编程接口（API）来完成，或者更奇特的是，两个具有不同专业的医疗人工智能系统进行对话来完成。 &lt;/p>; &lt;p>; 这种方法有一些重要的好处。它允许子系统之间实现最大的灵活性和独立性，使卫生系统能够根据子系统经过验证的性能特征混合和匹配技术提供商之间的产品。此外，子系统之间的人类可读通信通道最大限度地提高了可审核性和可调试性。也就是说，在独立子系统之间实现正确的通信可能很棘手，这会缩小信息传输范围，或暴露通信错误和信息丢失的风险。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;模型嫁接&lt;/h2>; &lt;p>; 一种更综合的方法是采用专门用于每个相关领域，并对其进行调整以直接插入法学硕士 - 将视觉模型&lt;em>;嫁接&lt;/em>;到核心推理代理上。与由法学硕士确定所使用的具体工具的工具使用相反，在模型移植中，研究人员可以在开发过程中选择使用、完善或开发特定模型。在谷歌研究中心最近的两篇论文中，我们表明这实际上是可行的。神经法学硕士通常通过首先将单词映射到&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings&quot;>;向量嵌入空间&lt;/一个>;。这两篇论文都基于将数据从新模态映射到法学硕士已经熟悉的输入词嵌入空间的想法。第一篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.09018&quot;>;基于个人特定数据的健康多模式法学硕士&lt;/a>;”表明&lt;a href= “https://www.ukbiobank.ac.uk/&quot;>;英国生物银行&lt;/a>;如果我们首先训练神经网络分类器来解释&lt;a href=&quot;https://www.mayoclinic.org/tests -procedures/spirometry/about/pac-20385201&quot;>;呼吸图&lt;/a>;（一种用于评估呼吸能力的方式），然后调整该网络的输出作为 LLM 的输入。 &lt;/p>; &lt;p>; 第二篇论文，“&lt;a href=&quot;https://arxiv.org/abs/2308.01317&quot;>;ELIXR：通过大语言模型和放射学的结合实现通用 X 射线人工智能系统视觉编码器&lt;/a>;”，采用相同的策略，但将其应用于放射学中的全尺寸图像编码器模型。从&lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;了解胸部 X 光检查的基础模型&lt;/a>;开始，已展示为了成为在此模式中构建各种分类器的良好基础，本文描述了训练一个轻量级的医疗信息适配器，该适配器将基础模型的顶层输出重新表示为一系列标记LLM 的输入嵌入空间。尽管对视觉编码器和语言模型都没有进行微调，但生成的系统显示了未经训练的功能，包括&lt;a href=&quot;https://arxiv.org/abs/2210.10163&quot;>;语义搜索&lt;/a>;和&lt;a href=&quot;https://www.nature.com/articles/sdata2018251&quot;>;视觉问答&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFM aMMzTSUu4fif7t9SVsr9MduEcISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s1600/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFMaMMzTSUu4fif7t9SVsr9MduEc ISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;我们嫁接模型的方法是通过训练医疗信息适配器来将现有或改进的图像编码器的输出映射为法学硕士可理解的形式。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 模型嫁接有很多优点。它使用相对适度的计算资源来训练适配器层，但允许法学硕士在每个数据域中现有的高度优化和验证的模型上构建。将问题模块化为编码器、适配器和 LLM 组件还可以方便在开发和部署此类系统时对各个软件组件进行测试和调试。相应的缺点是，专业编码器和 LLM 之间的通信不再是人类可读的（作为一系列高维向量），并且嫁接过程不仅需要为每个特定领域的编码器构建一个新的适配器，还需要为每个每个编码器的&lt;em>;修订&lt;/em>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;通才系统&lt;/h2>; &lt;p>; 多模式医疗人工智能最根本的方法是构建一个集成的系统，完全通才的系统本身就能够吸收所有来源的信息。在该领域的第三篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;迈向通才生物医学人工智能&lt;/a>;”中，我们没有为每种数据模态使用单独的编码器和适配器，以 &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>; 为基础构建，PaLM-E 是最近发布的多模式模型单个 LLM (&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;) 和 &lt; a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;单视觉编码器 (ViT)&lt;/a>;。在此设置中，LLM 文本编码器涵盖文本和表格数据模式，但现在所有其他数据都被视为图像并馈送到视觉编码器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42 Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s1600/image2.gif&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ 2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Med-PaLM M 是一个大型多模态生成模型，可以使用相同的模型权重灵活地编码和解释生物医学数据，包括临床语言、成像和基因组学。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们通过在论文中描述的医学数据集上微调整套模型参数，将 PaLM-E 专门应用于医学领域。由此产生的通用医疗人工智能系统是 &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; 的多模式版本，我们称之为 Med-PaLM M。灵活的多模式序列到序列的架构使我们能够在一次交互中交织各种类型的多模态生物医学信息。据我们所知，这是单个统一模型的首次演示，该模型可以解释多模式生物医学数据并在所有任务中使用相同的模型权重集来处理各种任务（论文中有详细评估）。 &lt;/p>; &lt;p>; 这种多模态通才系统方法是我们描述的方法中最雄心勃勃、同时也是最优雅的方法。原则上，这种直接方法最大限度地提高了模式之间的灵活性和信息传输。由于没有 API 来维持兼容性，也没有适配器层的激增，通用方法可以说是最简单的设计。但同样的优雅也是其一些缺点的根源。计算成本通常更高，并且由于单一视觉编码器服务于多种模式，领域专业化或系统可调试性可能会受到影响。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;多模式医疗人工智能的现实&lt;/h2>; &lt;p>;为了在医学中充分利用人工智能，我们需要将经过预测人工智能训练的专家系统的优势与通过生成人工智能实现的灵活性结合起来。哪种方法（或方法组合）在该领域最有用取决于许多尚未评估的因素。通才模型的灵活性和简单性是否比模型嫁接或工具使用的模块化更有价值？哪种方法可以为特定的实际用例提供最高质量的结果？支持医学研究或医学教育与增强医疗实践的首选方法是否不同？回答这些问题需要持续进行严格的实证研究，并与医疗保健提供者、医疗机构、政府实体和医疗保健行业合作伙伴继续进行广泛的直接合作。我们期待着共同寻找答案。 &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4436946873570093049/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/multimodal-medical-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com /2023/08/multimodal-medical-ai.html&quot; rel=&quot;alternate&quot; title=&quot;多模式医疗 AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72“网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL _N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s72-c/medpalm.png“宽度=“72”xmlns：媒体=“http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-5980448298988153366&lt;/id>;&lt;发布>;2023-07-26T09:33:00.003-07:00&lt;/发布>;&lt;更新>;2023-08-01T08:31:25.216-07:00&lt;/更新>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;寻找无源域的通用方法适应&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Eleni Triantafillou 和学生研究员 Malik Boudiaf &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgy M4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s1500/notela.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 深度学习最近在广泛的问题和应用中取得了巨大进展，但模型在部署在未知的领域或分布中时常常会出现不可预测的失败。 &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;无源域适应&lt;/a>; (SFDA) 是一个研究领域，旨在设计适应预训练模型的方法（在一个“源域”）到一个新的“目标域”，仅使用后者的未标记数据。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 设计深度模型的适应方法是一个重要的研究领域。虽然模型和训练数据集规模的不断扩大是其成功的关键因素，但这种趋势的负面后果是训练此类模型的计算成本越来越高，在某些情况下使得大型模型训练&lt;a href=&quot;https:// spectrum.ieee.org/deep-learning-computational-cost&quot;>;不太容易访问&lt;/a>;并且不必要&lt;a href=&quot;https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy- and-resource-strain&quot;>;增加碳足迹&lt;/a>;。缓解这一问题的一个途径是通过设计技术来利用和重用已经训练好的模型来处理新任务或推广到新领域。事实上，在&lt;a href=&quot;https://arxiv.org/abs/1911.02685&quot;>;迁移学习&lt;/a>;的框架下，对模型适应新任务进行了广泛的研究。 &lt;/p>; &lt;p>; SFDA 是这项研究的一个特别实用的领域，因为一些需要适应的现实应用程序会遇到无法获得来自目标域的标记示例的问题。事实上，SFDA 正受到越来越多的关注 [&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot; >;2&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2110.04202&quot;>;3&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;4 &lt;/a>;]。然而，尽管出于雄心勃勃的目标，大多数 SFDA 研究都基于一个非常狭窄的框架，考虑图像分类任务中简单的&lt;a href=&quot;https://arxiv.org/abs/2108.13624&quot;>;分布变化&lt;/a>;。 &lt;/p>; &lt;p>; 与这一趋势有很大的不同，我们将注意力转向生物声学领域，其中自然发生的分布变化无处不在，通常以目标标记数据不足为特征，这对从业者来说是一个障碍。因此，在此应用中研究 SFDA 不仅可以让学术界了解现​​有方法的普遍性并确定开放的研究方向，而且还可以直接使该领域的从业者受益，并有助于解决本世纪最大的挑战之一：生物多样性保存。 &lt;/p>; &lt;p>; 在这篇文章中，我们宣布“&lt;a href=&quot;https://arxiv.org/abs/2302.06658&quot;>;寻找一种通用的无源域适应方法&lt;/a>;”，出现在 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023。&lt;/a>;我们表明，最先进的 SFDA 方法在面对现实的分布变化时可能表现不佳甚至崩溃在生物声学中。此外，现有方法相对于彼此的表现与视觉基准中观察到的不同，并且令人惊讶的是，有时表现比根本没有适应更差。我们还提出了NOTELA，这是一种新的简单方法，它在这些转变上优于现有方法，同时在一系列视觉数据集上表现出强大的性能。总的来说，我们得出的结论是，（仅）在常用数据集和分布变化上评估 SFDA 方法，让我们对其相对性能和普遍性产生了短视的看法。为了兑现他们的承诺，SFDA 方法需要在更广泛的分布变化上进行测试，我们主张考虑可以有利于高影响力应用的自然发生的方法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;生物声学中的分布变化&lt;/h2>; &lt;p>; 自然发生的分布变化在生物声学中普遍存在。最大的鸟类歌曲标记数据集是 &lt;a href=&quot;https://www.researchgate.net/publication/280978332_The_Xeno-canto_collection_and_its_relation_to_sound_recognition_and_classification&quot;>;Xeno-Canto&lt;/a>; (XC)，这是用户贡献的野生鸟类录音的集合来自世界各地的鸟类。 XC 中的录音是“焦点”的：它们针对的是在自然条件下捕获的个体，其中所识别的鸟的歌声位于前景。然而，出于持续监控和跟踪的目的，从业者通常更感兴趣的是在通过全向麦克风获得的被动录音（“声景”）中识别鸟类。这是一个&lt;a href=&quot;https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments&quot;>;最近&lt;/a>;&lt;a href=&quot;https://www.sciencedirect.com/science/ Article/pii/S1574954121000273&quot;>;工作&lt;/a>; 展示非常具有挑战性。受到这个现实应用的启发，我们使用在 XC 上预先训练的鸟类分类器作为源模型，以及来自不同地理位置的几个“声景”来研究 SFDA 的生物声学 — &lt;a href=&quot;https://doi. org/10.5281/zenodo.7050013&quot;>;内华达山脉&lt;/a>;（内华达州）； &lt;a href=&quot;https://doi.org/10.1002/ecy.3329&quot;>;Powdermill&lt;/a>; 自然保护区，美国宾夕法尼亚州； &lt;a href=&quot;https://doi.org/10.5281/zenodo.7078498&quot;>;夏威夷&lt;/a>;；美国加利福尼亚州卡普尔斯分水岭； &lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;Sapsucker Woods&lt;/a>;，美国纽约 (SSW)；和 &lt;a href=&quot;https://www.google.com/url?q=https://zenodo.org/record/7525349%23.ZB8z_-xudhE&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1688498539746392&amp;amp; usg=AOvVaw07CsIKIE-dcNyMKFT-n_JT&quot;>;哥伦比亚&lt;/a>; — 作为我们的目标域。 &lt;/p>; &lt;p>; 这种从聚焦域到无源域的转变是巨大的：后者的录音通常具有低得多的信噪比，几只鸟同时发声，以及明显的干扰因素和环境噪音，例如雨或风。此外，不同的音景源自不同的地理位置，从而导致极端的标签变化，因为 XC 中的物种的一小部分将出现在给定位置。此外，正如现实世界数据中常见的那样，源域和目标域都存在明显的类别不平衡，因为某些物种比其他物种更常见。此外，我们还考虑了多标签分类问题，因为每个记录中可能会识别出几只鸟类，这与通常研究 SFDA 的标准单标签图像分类场景有很大不同。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeG afX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s1378/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;1378&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQ OzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;“焦点→音景”转变的图示。在聚焦域中，录音通常由前景中的单个鸟类发声组成，以高信噪比 (SNR) 捕获，尽管背景中可能还有其他鸟类发声。&lt;strong>; &lt;/strong>;另一方面，音景包含全向麦克风的录音，可以由多只鸟同时发声以及昆虫、雨、汽车、飞机等环境噪音组成。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/表>; &lt;br />; &lt;表align =“中心”cellpadding =“0”cellspacing =“0”class =“tr-caption-container”>; &lt;tbody>; &lt;tr>; &lt;td style =“text-align：左; &quot;>;&lt;b>;音频文件&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;焦点域&lt;em>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https:// /storage.googleapis.com/chirp-public-bucket/notela-blog-post/XC417991%20-%20Yellow-throated%20Vireo%20-%20Vireo%20flavifrons.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/ em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;音景域名&lt;em>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>; &lt;/sup>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/ Yetvir-soundscape.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;频谱图图像&lt;/ b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR 4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/s936/left.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0 NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/w640-h546/left.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;td>; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iy jBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/s936/右.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTC mlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/w640-h546/right.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;table对齐=“中心”cellpadding=“0”cellspacing=“0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从焦点域（&lt;b>;向左&lt;/ b>;）到音景域（&lt;b>;右&lt;/b>;），根据代表性的音频文件（&lt;b>;顶部&lt;/b>;）和频谱图图像（&lt;b>;底部&lt;/b>;）每个数据集的录音。请注意，在第二个音频剪辑中，鸟鸣声非常微弱；这是音景录音中的一个常见属性，其中鸟叫声不在“前景”。学分：&lt;b>;左：&lt;/b>; XC Sue Riffe &lt;a href=&quot;https://xeno-canto.org/417991&quot;>;录音&lt;/a>; (&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/ &quot;>;CC-BY-NC 许可证&lt;/a>;）。&lt;b>;右：&lt;/b>;摘自 Kahl、Charif 和 Klinck 提供的录音。(2022)“完整注释的音景录音合集来自美国东北部的 SSW 音景数据集的 [&lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;链接&lt;/a>;] (&lt;a href=&quot;https://creativecommons.7018483&quot;>;链接&lt;/a>;) org/licenses/by/4.0/&quot;>;CC-BY 许可证&lt;/a>;)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;最先进的 SFDA 模型在生物声学变化方面表现不佳&lt;/h2>; &lt;p>; 作为起点，我们对六种最先进的 SFDA 方法进行了基准测试我们的生物声学基准，并将它们与&lt;em>;未适应&lt;/em>;基线（源模型）进行比较。我们的发现令人惊讶：毫无例外，现有方法无法在所有目标领域始终优于源模型。事实上，他们的表现常常明显不佳。 &lt;/p>; &lt;p>; 举个例子，&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>; 是一种最新的方法，旨在通过以下方式使模型对每个示例产生可信的预测：减少模型输出概率的不确定性。虽然 Tent 在各种任务中表现良好，但它对于我们的生物声学任务却效果不佳。在单标签场景中，最小化熵迫使模型自信地为每个示例选择一个类别。然而，在我们的多标签场景中，不存在任何类都应该被选择为存在的约束。再加上显着的分布变化，这可能会导致模型崩溃，导致所有类别的概率为零。其他基准测试方法，例如 &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;、&lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt; /a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;帐篷&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98 -Paper.pdf&quot;>;NRC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;灰尘&lt;/a>;和&lt;a href=&quot;https://www.researchgate .net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;伪标签&lt;/a>;是标准 SFDA 基准的强大基线，但也难以完成这项生物声学任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj 38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s1434/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;730&quot; data-original-width=&quot;1434&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1 aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试的演变&lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_ precision&quot;>;平均精度&lt;/a>; (mAP)，一种多标签分类的标准度量，贯穿六个音景数据集的适应过程。我们对我们提出的NOTELA和Dropout Student（见下文）以及&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;、&lt;a href=&quot;https://openreview .net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;帐篷&lt;/a>;、&lt;a href=&quot;https://proceedings .neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;灰尘&lt;/a>;和&lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;伪标签&lt;/a>;。除了NOTELA之外，所有其他方法都无法持续改进源模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;引入带有拉普拉斯调整的 NOisy 学生 TEacher (NOTELA)&lt;/h2>; &lt;p>; 尽管如此，一个令人惊讶的积极结果脱颖而出：不太著名的 &lt;a href=&quot;https://arxiv.org/abs/ 1911.04252&quot;>;吵闹的学生&lt;/a>;原则看起来很有前途。这种无监督方法鼓励模型在某些目标数据集上重建自己的预测，但在随机噪声的应用下。虽然噪声可能通过各种渠道引入，但我们力求简单并使用&lt;a href=&quot;https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9&quot;>;模型丢失&lt;/a>;作为唯一的噪声源：因此，我们将这种方法称为&lt;em>;辍学学生（DS）&lt;/em>;。简而言之，它鼓励模型在对特定目标数据集进行预测时限制单个神经元（或过滤器）的影响。 &lt;/p>; &lt;p>; DS 虽然有效，但在各种目标域上都面临模型崩溃问题。我们假设发生这种情况是因为源模型最初对这些目标域缺乏信心。我们建议通过直接使用特征空间作为辅助事实来源来提高 DS 稳定性。 NOTELA 的灵感来自于 &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC 的方法&lt; /a>; 和拉普拉斯&lt;a href=&quot;https://www.researchgate.net/publication/220319905_Manifold_Regularization_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples&quot;>;正则化&lt;/a>;。这种简单的方法如下图所示，在音频和视觉任务中始终显着优于源模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4 VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s1870/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1870&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb 5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; &quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-对齐：center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9Q nJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s1080/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92 969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;NOTELA 的实际应用。通过完整模型转发音频记录以获得第一组预测，然后通过拉普拉斯正则化（一种基于聚类附近点的后处理形式）对这些预测进行细化。最后，细化的预测为用作&lt;em>;噪声模型&lt;/em>;重建的目标。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 标准的人工图像分类基准无意中限制了我们对 SFDA 方法的真正通用性和鲁棒性的理解。我们主张扩大范围并采用新的评估框架，其中包含生物声学自然发生的分布变化。我们还希望NOTELA 能够成为促进该方向研究的坚实基础。 NOTELA 的强劲表现也许表明了两个因素可以导致开发更通用的模型：首先，开发着眼于更困难问题的方法，其次，支持简单的建模原则。然而，未来仍需开展工作来查明和理解现有方法在解决更困难问题时的失效模式。我们相信，我们的研究代表了朝这个方向迈出的重要一步，为设计具有更大普适性的 SFDA 方法奠定了基础。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Eleni Triantafillou ，现供职于 Google DeepMind。我们代表 NOTELA 论文的作者发布此博文：Malik Boudiaf、Tom Denton、Bart van Merriënboer、Vincent Dumoulin*、Eleni Triantafillou*（其中 * 表示同等贡献）。我们感谢合著者为本文所做的辛勤工作，以及 Perch 团队其他成员的支持和反馈。&lt;/em>; &lt;/p>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/ a>;&lt;/sup>;请注意，在此音频片段中，鸟鸣声非常微弱；这是音景录音中的一个常见属性，鸟叫声不在“前景”。&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5980448298988153366/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/in-search-of-generalized-method-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /ai.googleblog.com/2023/07/in-search-of-generalized-method-for.html&quot; rel=&quot;alternate&quot; title=&quot;寻找无源域适应的通用方法&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4ep C-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/ s72-c/notela.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;已发布>;2023-07-23T14:13:00.002-07:00&lt;/已发布>;&lt;更新>;2023-08-07T10:08:27.713-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 在 ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：项目经理 Cat Armato 、谷歌&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4n vXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s320/Google-ICML-hero.jpg&quot;样式=“显示：无；” />; &lt;p>; Google 各个团队积极开展机器学习 (ML) 领域的理论和应用研究。我们构建机器学习系统来解决语言、音乐、视觉处理、算法开发等领域的深层科学和工程挑战。我们的目标是通过开源工具和数据集、发布我们的工作并积极参加会议，与更广泛的机器学习研究社区建立一个更具协作性的生态系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Google 很荣幸成为&lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;钻石赞助商&lt;/a >; 第 40 届&lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;国际机器学习会议&lt;/a>; (ICML 2023)，这是一次重要的年度会议，将于今年举行夏威夷檀香山一周。作为 ML 研究领域的领导者，Google 在今年的会议上表现强劲，共发表了 120 多篇被接受的论文，并积极参与了许多研讨会和教程。 Google 还很荣幸成为 &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; 和 &lt;a href=&quot;https://sites .google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;机器学习中的女性&lt;/a>;研讨会。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;p>; 已注册 ICML 2023？我们希望您能够参观 Google 展位，详细了解解决该领域最有趣挑战的部分令人兴奋的工作、创造力和乐趣。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解 Google 展位活动（例如演示和问答环节）。请参阅 &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind 博客&lt;/a>;，了解他们在 ICML 2023 上的技术参与情况。&lt;/ p>; &lt;p>; 请查看下文，了解有关 ICML 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。&lt;/p>; &lt;br>; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 董事会成员包括：&lt;strong>;&lt;em >;Corinna Cortes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; 辅导主席包括：&lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong >; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin- left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75KA&quot;>;将视觉 Transformer 扩展到 220 亿个参数&lt;/a>;（请参阅 &lt;a href=&quot;https:// /ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;约西普·乔隆加&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;巴兹尔·穆斯塔法&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;皮奥特·帕德鲁斯基&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;乔纳森·希克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯汀·吉尔默&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;安德烈亚斯·斯坦纳&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伯特·盖尔霍斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;易卜拉欣·阿拉卜杜尔莫辛&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Michael Tschannen&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;阿努拉格·阿纳布&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;小王&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡洛斯·里克尔梅&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Joan Puigcerver、&lt;/em>;&lt;em>;Utku Evci&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;阿拉文德·马亨德兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿维塔尔·奥利弗&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;芳汀·胡特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯米恩·巴斯廷斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马克·帕特里克·科利尔&lt;/em>;&lt;/strong>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alexey Gritsenko&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Cristina Vasconcelos&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;托马斯·门辛克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;亚历山大·科列斯尼科夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Filip Pavetić&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;达斯汀·特兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;托马斯Kipf&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mario Lučić&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·凯泽斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;耶利米·哈姆森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔·霍尔斯比&lt;/em>;&lt;br />;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;通过推理解码从 Transformers 进行快速推理&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em >;&lt;/strong>;、&lt;strong>;&lt;em>;马坦·卡尔曼&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尤西·马蒂亚斯&lt;/em>;&lt;/strong>;&lt;/p>;&lt;p>;&lt;a href= “https://openreview.net/pdf?id=bUFUaawOTk&quot;>;两全其美的政策优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;，&lt;em>;陈-于伟&lt;/em>;，&lt;strong>;&lt;em>;朱利安·齐默特&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf? id=9PJ2V6qvQL&quot;>;机器学习中的流入、流出和互惠&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Walid Krichene&lt;/em>; &lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;Transformers 通过梯度下降在上下文中学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;约翰内斯·冯·奥斯瓦尔德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;艾文德·尼克拉森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;埃托雷·兰达佐&lt;/em>;&lt;/strong>; >;、&lt;em>;若昂·萨克拉门托&lt;/em>;、&lt;strong>; &lt;em>;亚历山大·莫德文采夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈·日莫吉诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Max Vladymyrov&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;算术采样：并行多样化解码大型语言模型&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;em>;Patrick Murray* &lt;/em>;、&lt;em>;亚历山大·帕索斯*&lt;/em>;、&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https: //openreview.net/pdf?id=ayBKRjGDEI&quot;>;具有可证明近似保证的差分私有层次聚类&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/05/ Differentially-private- clustering-for.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;em>;Jacob Imola&lt;/em>;*,&lt;strong>; &lt;em>;Alessandro Epasto&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; >;穆罕默德·马赫迪安&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;文森特·科恩-阿达德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong >; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2ToHR5&quot;>;用于私有机器学习的多历元矩阵分解机制&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;H.布伦丹·麦克马汉&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;基思·拉什&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;阿卜拉迪普·塔库塔&lt;/em>;&lt;br />;&lt;/strong>; &lt;/ p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;无论模型选择如何，随机分类噪声都不会击败所有凸潜在助推器&lt;/a>; &lt;br>; &lt;strong>;&lt; em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;理查德·诺克&lt;/em>;&lt;/strong>;、&lt;em>;罗伯特·威廉姆森&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;单纯形随机特征&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;，&lt;strong>; &lt;em>;Krzysztof Choromanski&lt;/ em>;&lt;/strong>;、&lt;em>;Valerii Likhosherstov&lt;/em>;、&lt;em>;阿德里安·韦勒&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/ pdf?id=bF1LVbP493&quot;>;Pix2Struct：屏幕截图解析作为视觉语言理解的预训练&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mandar Joshi&lt; /em>;&lt;/strong>;、&lt;em>;Iulia Turc&lt;/em>;、&lt;strong>; &lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;、&lt;em>;刘芳宇&lt;/em>;、&lt;strong>; &lt;em>; >;Julian Eisenschlos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;张明伟&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Kristina Tooutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net /pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM：多任务、多语言语音和语言模型&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yong Cheng&lt;/​​em>;&lt;/strong>;, &lt;strong>; &lt;em>;张宇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;梅尔文·约翰逊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;沃尔夫冈·马切里&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;稳健的预算使用单个样本调整节奏&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;、&lt;em>;Rachitesh Kumar&lt;/em>;*、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/ em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王迪&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p >;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;基于检索的模型的统计视角&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;、&lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot; https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;张量分解的近似最佳核心形状&lt;/a>; &lt;br>; &lt;em>;Mehrdad Ghadiri&lt;/em>;，&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;傅刚&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;使用批次的高效列表可解码回归&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>; &lt;/strong>;、&lt;em>;Ayush Jain&lt;/em>;*、&lt;strong>; &lt;em>;孔伟豪&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt; /strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;使用少样本学习高效训练语言模型&lt;/a>; &lt;br >; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;金承妍&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp ;name=pdf&quot;>;拟阵上的完全动态子模最大化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt; em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/ strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;用于学习组合潜变量模型的 GFlowNet-EM&lt;/a>; &lt;br>; &lt; em>;Edward J Hu&lt;/em>;、&lt;em>;Nikolay Malkin&lt;/em>;、&lt;em>;Moksh Jain&lt;/em>;、&lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;、&lt;em>; Alexandros Graikos&lt;/em>;、&lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;改进在线学习广告拍卖中点击率预测算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;冯哲&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;, &lt;em >;Zixin Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;大型语言模型难以学习长尾知识&lt;/ a>; &lt;br>; &lt;em>;尼基尔·坎德帕尔&lt;/em>;、&lt;em>;邓海康&lt;/em>;、&lt;strong>; &lt;em>;亚当·罗伯茨&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;埃里克·华莱士&lt;/em>;&lt;/strong>;，&lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;多渠道自动出价预算和投资回报率限制&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yuan Deng&lt;/​​em>;&lt;/strong>;、&lt;em>;Negin Golrezaei&lt;/em>;、&lt;em>;Patrick Jaillet&lt;/em>;、&lt;em>; >;杰森·卓南梁&lt;/em>;，&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= ZMvv6laV5b&amp;amp;name=pdf&quot;>;多层神经网络作为希尔伯特空间的可训练阶梯&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;陈正道&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;关于用户级私有凸优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;强>;&lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;拉维库马尔&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;， &lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张驰远&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;通过不变表示进行 PAC 泛化&lt;/a>; &lt;br>; &lt;em>;Advait U Parulekar&lt;/em>;，&lt;strong>; &lt;em>;Karthikeyan Shanmugam&lt;/em>;&lt; /strong>;, &lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;正则化和方差加权回归在线性 MDP 中实现极小极大最优：理论与实践&lt;/a>; &lt;br>; &lt;em>;Toshinori Kitamura&lt;/em>;、&lt;em>;Tadashi Kozuno&lt;/em>;、&lt;em>;Yunhao Tang&lt;/em>; 、&lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;、&lt;em>;Michal Valko&lt;/em>;、&lt;em>;杨文浩&lt;/em>;、&lt;strong>;&lt;em>;梅金城&lt;/em>; &lt;/strong>;、&lt;em>;皮埃尔·梅纳德&lt;/em>;、&lt;em>;Mohammad Gheshlaghi Azar&lt;/em>;、&lt;em>;雷米·穆诺斯&lt;/em>;、&lt;strong>; &lt;em>;奥利维耶·皮埃奎因&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;Csaba Szepesvari&lt;/em>;、&lt;em>;Wataru Kumagai&lt;/em>;、&lt;em>;松尾裕&lt;/em>;&lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt39VUw&amp;amp;name=pdf&quot;>;通过最小违规排列加速 Bellman Ford&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;西尔维奥·拉坦齐&lt;/em>;&lt;/strong>;、&lt;em>;奥拉·斯文森&lt;/em>;、&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/attachment?id=LxodbQa62n&amp;amp;name=pdf&quot;>;学习算法的统计不可区分性&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;，&lt;strong>; &lt;em>;Amin Karbasi&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢伊·莫兰&lt;/em>;&lt;/strong>;、&lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/pdf?id=G5vKSJVhJL&quot;>;以时隙为中心的模型进行测试时调整&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;、&lt;em>;Deepak Pathak&lt;/em>;、&lt;em>;Katerina Fragkiadaki>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;用户级隐私下直方图估计的边界贡献算法&lt;/a>; &lt;br>; &lt;em>;刘玉涵&lt;/em>;*、&lt;strong>;&lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;朱文楠&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Peter Kairouz&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment ?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;带有提示和查询的 Bandit 在线线性优化&lt;/a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;、&lt;em>;Ashok Cutkosky&lt;/em>;、&lt;strong>; &lt;em >;拉维·库马尔&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;曼尼什·普罗希特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR：通过无监督任务表示学习进行课程学习&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;，&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/ &lt;em>;贾斯帕·埃姆霍夫&lt;/em>;、&lt;em>;纳撒尼尔·亚历克西斯&lt;/em>;、&lt;strong>; &lt;em>;亚历山大·福斯特&lt;/em>;&lt;/strong>;、&lt;em>;彼得·阿贝尔&lt;/em>;、 &lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP：自我监督对比空间预训练地理空间视觉表示&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;, &lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;, &lt;em>;Yutong He&lt;/em>;, &lt; em>;宋嘉明&lt;/em>;、&lt;em>;斯特凡诺·埃尔蒙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;埃瓦尔德-基于分子图的远程消息传递&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;，&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;，&lt;em>;Nicholas Gau&lt; /em>;, &lt;em>;Stephan Günnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;快 (1+ε) -二元矩阵分解的近似算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;、&lt;em>;Maximilian Vötsch&lt;/em>;、&lt;strong>;&lt;em>;David Woodruff&lt; /em>;&lt;/strong>;，&lt;em>;Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;联合线性具有用户级差分隐私的上下文强盗&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;、&lt;em>;Huanyu Zhang&lt;/em>;、&lt;em>;Luca Melis&lt;/em>;、&lt;em>;Milan Shen &lt;/em>;，&lt;strong>; &lt;em>;Meisam Hejazinia&lt;/em>;&lt;/strong>;，&lt;em>;杨静&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;研究基于模型的学习在探索和迁移中的作用&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;、&lt;em>;Eszter Vértes&lt;/em>; >;、&lt;em>;李亚哲&lt;/em>;、&lt;strong>; &lt;em>;加布里埃尔·杜拉克-阿诺德&lt;/em>;&lt;/strong>;、&lt;em>;Ankesh Anand&lt;/em>;、&lt;em>;Theophane Weber&lt;/em>; ,&lt;em>; Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;标记差异隐私和私人训练数据发布&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;罗伯特·布萨-费科特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈斯·穆尼奥斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;奥马尔·赛义德&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name= pdf&quot;>;与分销专家一起进行终身语言预训练&lt;/a>; &lt;br>; &lt;em>;Wuyang Chen&lt;/em>;*, &lt;strong>;&lt;em>;Yanqi Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Nan Du&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄艳萍&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;James Laudon&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;陈志峰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;崔嘉儿&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;低排名奖励的多用户强化学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;，&lt;em>;Suhas S Kowshik&lt;/em>;、&lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;用于视觉机器人操作的多视图蒙版世界模型&lt;/a >; &lt;br>;&lt;em>;Younggyo Seo&lt;/em>;、&lt;em>;Junsu Kim&lt;/em>;、&lt;em>;Stephen James&lt;/em>;、&lt;strong>;&lt;em>;Kimin Lee&lt;/em>;&lt;/strong>; 、 &lt;em>;申振宇&lt;/em>;、&lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot; >;PaLM-E：体现的多模态语言模型&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;夏飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科里·林奇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aakanksha Chowdery&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Brian Ichter&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em >;Quan Vuong&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;余天河&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄文龙&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Daniel Duckworth&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;文森特·范霍克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡罗尔·豪斯曼&lt;/em>;&lt;/strong>;、&lt;em>; >;Marc Toussaint&lt;/em>;、&lt;strong>; &lt;em>;Klaus Greff&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曾安迪&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Igor Mordatch &lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;皮特·弗洛伦斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs &quot;>;采用自动调整压缩的私有联邦学习&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*，&lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;使用线性函数逼近的对抗性 MDP 的精致遗憾&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;、&lt;em>;罗海鹏&lt;/em>;、&lt;em>;魏震宇&lt;/em>;、&lt;strong>;&lt;em>;朱利安·齐默特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp ;name=pdf&quot;>;使用恒定内存将数据集蒸馏扩展到 ImageNet-1K&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;、&lt;em>; Ruoche Wan&lt;/em>;、&lt;strong>;&lt;em>;丝丝&lt;/em>;&lt;/strong>;，&lt;em>;谢祖瑞&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name= pdf&quot;>;采用 AdaGrad 步长的 SGD：对未知参数、无界梯度和仿射方差具有高概率的完全自适应性&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;分位数时间差异学习对价值估计的统计优势&lt;/a>; &lt;br >; &lt;em>;马克·罗兰&lt;/em>;、&lt;em>;唐云浩&lt;/em>;、&lt;em>;克莱尔·莱尔&lt;/em>;、&lt;em>;雷米·穆诺斯&lt;/em>;、&lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;、&lt;em>;威尔·达布尼&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;透过图像特征的迷雾揭开位置信息模式的面具&lt;/a>; &lt;br>; &lt;em>;林杰休伯特&lt;/em>;、&lt;em>;曾鸿宇&lt;/em>;、&lt;em>;新英Lee&lt;/em>;、&lt;em>;Maneesh Kumar Singh&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/pdf?id=4UStsbnfVT&quot;>;具有最佳速率的用户级私有随机凸优化&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;，&lt;strong>; &lt;em>;孙子腾&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;一种改进文本中提示集成的简单零样本提示加权技术-图像模型&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*、&lt;strong>;&lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>; em>;&lt;/strong>;,&lt;strong>;&lt;em>;顾秀野&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;崔银&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Dustin Tran&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘哲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/attachment?id=mXv2aVqUGG&amp;amp;name=pdf&quot;>;大型语言模型可以推理程序不变量吗？&lt;/a>; &lt;br>; &lt;strong>;裴可欣&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;大卫·比伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;石肯森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;查尔斯·萨顿&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;殷鹏程&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name =pdf&quot;>;持续观察下的并发随机差异隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong >;、&lt;strong>;&lt;em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;恒定问题：差分隐私持续观察中的细粒度误差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/强>;，&lt;em>;莫妮卡·亨辛格&lt;/em>;，&lt;em>;Jalaj Upadhyay&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name= pdf&quot;>;交叉熵损失函数：理论分析与应用&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;，&lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;，&lt;em>; Yutaozhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;使用在线函数逼近的对抗性上下文 MDP 的高效速率最优遗憾&lt; /a>; &lt;br>; &lt;em>;奥林·利维&lt;/em>;、&lt;strong>;&lt;em>;阿隆·科恩&lt;/em>;&lt;/strong>;、&lt;em>;阿萨夫·卡塞尔&lt;/em>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;拟阵约束下流子模最大化的公平性&lt; /a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Jakab Tardos&lt;/em>;&lt;/strong>;，&lt;em>;Jakub Tarnawski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name= pdf&quot;>;Flan Collection：设计有效指令调优的数据和方法&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open。 html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;em>;Shayne Longpre&lt;/em>;、&lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tu Vu&lt;/em>; em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿尔伯特·韦伯森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;郑亨元&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;巴雷特·佐夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾森·魏&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚当·罗伯茨&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;通过双层优化进行网络控制的图强化学习&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/ em>;、&lt;strong>;&lt;em>;詹姆斯·哈里森&lt;/em>;&lt;/strong>;、&lt;em>;杨凯迪&lt;/em>;、&lt;em>;Marco Pavone&lt;/em>;、&lt;em>;Filipe Rodrigues&lt;/em>;、 &lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;多分位数的学习增强私有算法发布&lt;/a>; &lt;br>; &lt;em>;米哈伊尔·霍达克&lt;/em>;*、&lt;strong>;&lt;em>;卡里姆·阿明&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;特拉维斯·迪克&lt;/em>;&lt;/强>;，&lt;强>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>; LegendreTron：起义正确的多类损失学习&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;、&lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;、&lt;em>;Spiridon Penev&lt;/em>; >;,&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;测量编程语言分布的影响&lt;/a>; &lt;br>; &lt;em>;Gabriel Orlanski&lt;/em>;*, &lt;strong>;&lt;em>;肖克凡&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xavier Garcia&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;许志锋&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·豪兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·马尔莫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·奥斯汀&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;、&lt;em>;米歇尔·卡塔斯塔&lt;/em>;&lt;/strong>; em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;分布倾斜下的多任务差分隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;Walid Krichene&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;宋爽&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张莉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse：通过 Masked Generative Transformers 生成文本到图像&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Huiwen Chang &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张翰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾里德·巴伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;José Lezama&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陆江&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;凯文·墨菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;威廉·弗里曼&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;李元珍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迪利普·克里希南&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;关于联合平均与循环客户端参与的融合&lt;/a>; &lt;br>; &lt;em>; Yae Jee Cho&lt;/em>;、&lt;em>;Pranay Sharma&lt;/em>;、&lt;em>;Gauri Joshi&lt;/em>;、&lt;strong>;&lt;em>;郑旭&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Satyen Kale&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;张童&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=GimajxXNc0&amp;amp;name=pdf&quot;>;通过在线到非凸转换实现最优随机非平滑非凸优化&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;，&lt;strong>;&lt;em>;严厉的梅塔&lt;/em>;&lt;/strong>;，&lt;em>;弗朗西斯科·奥拉博纳&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot; >;通过定向增强实现域外鲁棒性&lt;/a>; &lt;br>; &lt;em>;Irena Gau&lt;/em>;、&lt;em>;Shiori Sakawa&lt;/em>;、&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>; &lt;/strong>;、&lt;em>;桥本龙典&lt;/em>;、&lt;em>;梁培西&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=b6Hxt4Jw10&quot; >;无界高斯混合模型的多项式时间和私人学习&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;、&lt;em>;Hassan Ashtiani&lt;/em>;、&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;预计算内存还是即时编码？检索增强的混合方法可充分利用您的计算&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;、&lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;尼古拉斯·菲茨杰拉德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约书亚·安斯利&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;沙飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;威廉·科恩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;用于迭代生成的可扩展自适应计算&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*、&lt;strong>;&lt;em>;David J. Fleet&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*、&lt;strong>;&lt;em>;David J. Fleet&lt;/a>; em>;&lt;/strong>;,&lt;strong>; &lt;em>;陈婷&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;缩放球形 CNN&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;、&lt;em>;Jean-Jacques Slotine&lt;/em>;、&lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;步骤：在前提条件下从头开始学习 N:M 结构化稀疏掩模&lt; /a>; &lt;br>; &lt;em>;吕玉成&lt;/em>;、&lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;奥列格·雷巴科夫&lt;/em>;&lt;/strong>;、&lt;em>;克里斯托弗·德萨&lt;/em>;、&lt;em>;阿米尔·亚兹丹巴赫什&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;带有拒绝的分层对抗鲁棒性&lt;/a>; &lt;br>; &lt;em>;Jiefeng Chen&lt;/em>;，&lt;em>;Jayaram Raghuram&lt;/em>;， &lt;em>;Jihye Choi&lt;/em>;、&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;、&lt;em>;梁英宇&lt;/em>;、&lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;特权信息何时解释消除标签噪音？&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez &lt;/em>;*、&lt;strong>; &lt;em>;马克·科利尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;阿南特·纳瓦尔加利亚&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚历山大·达莫尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰西·贝伦特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;鲁道夫·杰纳顿&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;埃夫罗西尼Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;具有弹性输入序列的自适应计算&lt;/a>; &lt;br>; &lt;em>;薛福兆&lt;/em>;*、&lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;em>;杨游&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;神经网络记忆可以本地化吗？&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;,&lt;strong>; &lt; em>;Michael C. Mozer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>;、&lt;em>;Zachary C. Lipton&lt;/em>;、&lt;em>;J. Zico Kolter&lt;/em>;，&lt;strong>; &lt;em>;张驰远&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name =pdf&quot;>;可控意识无监督技能发现&lt;/a>; &lt;br>; &lt;em>;Seohong Park&lt;/em>;，&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;，&lt;em>;Youngwoon Lee&lt; /em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;基于网格的高效学习Bi-Stride多尺度图神经网络物理模拟&lt;/a>; &lt;br>; &lt;em>;曹亚迪&lt;/em>;，&lt;strong>; &lt;em>;柴梦雷&lt;/em>;&lt;/strong>;，&lt;em>;Minchen李&lt;/em>;，&lt;em>;蒋陈凡富&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;联邦重量级选手恢复线性草图下&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;、&lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;、&lt;strong>; &lt;em>;孙子腾&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf? id=SpA7YFu02k&quot;>;用于对图神经网络进行基准测试的图生成模型&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;、&lt;em>;Yue Wu&lt;/em>;、&lt;strong>;&lt;em>;John Palowitch&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;成对错误排名损失代理的 H 一致性界限&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;，&lt;strong>;&lt;em>;Mehryar Mohri&lt;/ em>;&lt;/strong>;, &lt;em>;钟宇涛&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;改进遗憾使用线性函数逼近的高效在线强化学习&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yishay Mansour &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;不变槽注意力：使用以槽为中心的参考进行对象发现框架&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*，&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Gamaleldin Fathy Elsayed&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;从 Bandit 反馈中进行多任务离策略学习&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;、&lt;em>;Branislav Kveton&lt;/em>;、&lt;em>;Manzil Zaheer&lt;/em>;、&lt;em>;Sumeet Katariya&lt;/em>;、&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;片面 Lipschitz 函数的最佳无悔学习&lt;/ a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jon Schneider&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;王睿智&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name =pdf&quot;>;平均场游戏中高效和独立学习的政策镜像上升&lt;/a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;、&lt;em>;Semih Cayci&lt;/em>;、&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;鸟何&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;广义和马尔可夫博弈中的遗憾最小化和均衡收敛&lt;/a>; &lt;br>; &lt;em>;Liad Erez&lt;/em>;, &lt;em>;Tal Lancewicki&lt;/em>;, &lt;em>;Uri Sherman&lt;/em>;, &lt;强>;&lt;em>;托默·科伦&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;伊莎伊·曼苏尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;通过多重奖励，强化学习可以更加高效&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf ?id=rdOuTlTUMX&quot;>;利用依赖于历史的动态上下文进行强化学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;、&lt;em>;Nadav Merlis&lt;/em>;、&lt;strong >;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马丁·姆拉德诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdhcjMzhHN&amp;amp;name=pdf&quot;>;物理动力系统扩散模型中的用户定义事件采样和不确定性量化&lt;/a>; &lt;br >; &lt;em>;马克·安东·芬齐&lt;/em>;*、&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;、&lt;em>;安德鲁·戈登·威尔逊&lt;/em>;、&lt;strong>; &lt;em>;飞沙&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;莱昂纳多·泽佩达-努涅斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= LDBIVZCnLl&amp;amp;name=pdf&quot;>;离散键值瓶颈&lt;/a>; &lt;br>; &lt;em>;Frederik Träuble&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;em>;Nasim Rahaman&lt;/em>;、&lt;强>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;、&lt;em>;Kenji Kawaguchi&lt;/em>;、&lt;em>;Yoshua Bengio&lt;/em>;、&lt;em>;Bernhard Schölkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA：采用通信最优精确共识算法的去中心化 SGD&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/ em>;, &lt;em>;金可欣&lt;/em>;,&lt;strong>; &lt;em>;碧城英&lt;/em>;&lt;/strong>;, &lt;em>;袁坤&lt;/em>;, &lt;em>;殷窝涛&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer：图的稀疏变换器&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/ em>;、&lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balaji Venkatachalam&lt;/em>;&lt;/strong>;、&lt;em>;Danica J. Sutherland&lt;/em>;、&lt; strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;快速、可微分和稀疏 Top-k：凸分析视角&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>;*、&lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Josip Djolonga&lt;/em>;&lt;/strong>;、&lt;em>;加布里埃尔·佩雷&lt;/em>;、&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= “https://openreview.net/pdf?id=priTMs7n6e&quot;>;改进算法资源分配随机试验的政策评估&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;， &lt;em>;布莱恩·怀尔德&lt;/em>;、&lt;strong>; &lt;em>;阿帕娜·塔内贾&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;寻找一种通用的无源域适应方法&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em >;*、&lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mSofpvUxCL&amp;amp;name=pdf &quot;>;存在分布偏移的学习率表&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;并非所有语义都是平等的：具有自动温度个性化的对比自我监督学习&lt;/a>; &lt;br>; &lt;em>;邱子浩&lt;/em>; , &lt;em>;胡全琪&lt;/em>;, &lt;em>;袁卓宁&lt;/em>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;张丽君&lt;/em>;, &lt;em>; >;杨天宝&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;论解释与预测之间的关系：因果观&lt; /a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*、&lt;em>;Krikamol Muandet&lt;/em>;、&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;、&lt;em>;Bernhard Schölkopf&lt;/em>;，&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name= pdf&quot;>;关于注意力在提示调整中的作用&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;，&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;，&lt;em>; Mahdi Soltanolkotabi&lt;/em>;、&lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;播放：参数化使用潜在扩散生成条件布局&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/​​em>;&lt;/strong>;, &lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;李刚&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;李杨&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;用于非线性策略优化的学习局部线性模型的力量&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;、&lt;em>;Max Simchowitz&lt;/em>; , &lt;em>;Tyler Westenbroek&lt;/em>;, &lt;em>;Nikolai Matni&lt;/em>;,&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;解释图神经网络的相关步行搜索&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;，&lt;em>;Thomas Schnake&lt;/ em>;、&lt;em>;Michael Gastegger&lt;/em>;、&lt;em>;Grégoire Montavon&lt;/em>;、&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;、&lt;em>;中岛新一&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;大型代码语言模型的存储库级提示生成&lt;/a>; &lt;br>; &lt; em>;Disha Shrivastava&lt;/em>;、&lt;strong>; &lt;em>;雨果·拉罗谢尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·塔洛&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;稳健且私密的随机线性强盗&lt;/a>; &lt;br>; &lt;em>;Vasileios Charisopoulos&lt;/em>;*, &lt;strong>; &lt;em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;简单扩散：高分辨率图像的端到端扩散&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;,&lt;strong >; &lt;em>;乔纳森·希克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;蒂姆·萨利曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;绑定增强：控制表示相似性改进数据增强&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;、&lt;strong>;&lt;em>;李子超&lt;/em>; >;&lt;/strong>;、&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;为什么私人模特训练需要公开预训练？&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt; /em>;&lt;/strong>;、&lt;em>;Mahdi Haghifam&lt;/em>;*、&lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/em>;&lt;/em>; &lt;strong>;、&lt;strong>; &lt;em>;托马斯·斯坦克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;王伦&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf &quot;>;强化学习中一步强化学习与批评正则化之间的联系&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;em>;鲁斯兰·萨拉胡迪诺夫&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;差分隐私优化中的超越统一 Lipschitz 条件&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*，&lt;strong>;&lt;em>;Satyen Kale&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;徐峥&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;张潼&lt;/em>;&lt;/strong>;, &lt;em>;Sujay Sanghavi&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;高效图场积分器满足点云&lt;/a>; &lt;br>; &lt;strong>;&lt;em>; Krzysztof Choromanski&lt;/em>;&lt;/strong>;、&lt;em>;Arijit Sehanobish&lt;/em>;、&lt;em>;林瀚&lt;/em>;、&lt;em>;赵云帆&lt;/em>;、&lt;em>;Eli Berger、&lt;/em>; >; &lt;em>;Tetiana Parshakova&lt;/em>;、&lt;em>;Alvin Pan&lt;/em>;、&lt;em>;David Watkins&lt;/em>;、&lt;em>;张天一&lt;/em>;、&lt;em>;Valerii Likhosherstov&lt;/em>; 、&lt;em>;Somnath Basu Roy Chowdhury&lt;/em>;、&lt;strong>;&lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塔马斯·萨洛斯&lt;/em>;&lt;/strong>;、&lt;em>;Snigdha Chaturvedi&lt;/em>;、&lt;em>;阿德里安·韦勒&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>;快如 CHITA：组合优化的神经网络剪枝&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;、&lt;em>;Wenyu Chen&lt;/em>; , &lt;em>;孟翔&lt;/em>;, &lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;赵哲&lt;/em>;&lt;/strong>;，&lt;em>;拉胡尔·马宗德&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf &quot;>;快速启动强化学习&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;博文&lt;/a>;）&lt;br >; &lt;em>;Ikechukwu Uchendu&lt;/em>;*、&lt;strong>;&lt;em>;Ted Shaw&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;姚璐&lt;/em>;&lt;/strong>;、&lt;em>;Banghua朱&lt;/em>;、&lt;em>;严孟源&lt;/em>;、&lt;em>;Joséphine Simon&lt;/em>;、&lt;em>;Matthew Bennice&lt;/em>;、&lt;em>;付楚源&lt;/em>;、&lt;em>;丛马&lt;/em>;、&lt;em>;焦建涛&lt;/em>;、&lt;strong>; &lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡罗尔·豪斯曼&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;POMDP 中的学习具有样本效率和事后可观察性&lt;/a>; &lt;br>; &lt;em>;乔纳森·李&lt;/em>;、&lt;strong>;&lt;em>;阿勒克·阿加瓦尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;克里斯托夫·丹恩&lt;/em>;&lt;/strong>;、&lt;em>;张潼&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;使用 ES-Single 展开计算图中的低方差梯度估计&lt;/a>; &lt;br >; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;掩蔽轨迹预测、表示和控制模型&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;、&lt;em>;Arjun Majumdar&lt;/em>;、&lt;em>;Kevin Stone&lt;/em>;、&lt;em>;Yixin Lin &lt;/em>;、&lt;strong>;&lt;em>;伊戈尔·莫达奇&lt;/em>;&lt;/strong>;、&lt;em>;彼得·阿贝尔&lt;/em>;、&lt;em>;Aravind Rajeswaran&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;使用特征筛克服深度网络中的简单性偏差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;普拉迪普·谢诺伊&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name= pdf&quot;>;广告拍卖中福利最大化点击率预测的成对排名损失&lt;/a>; &lt;br>; &lt;em>;吕博相&lt;/em>;,&lt;strong>; &lt;em>;冯哲&lt;/em>;&lt;/strong >;、&lt;em>;扎卡里·罗伯逊&lt;/em>;、&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Faster Ford-Fulkerson 的预测流程&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;、&lt;em>;Benjamin Moseley&lt;/em>;、&lt;strong>; &lt;em>;Sergei瓦西尔维茨基&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;王雨燕&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= SVCYSBgFIr&amp;amp;name=pdf&quot;>;多语言神经机器翻译的缩放定律&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Behrooz Ghorbani&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;泽维尔·加西亚&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马库斯·弗雷塔格&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;奥尔罕·菲拉特&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;用于时间序列结构发现的顺序蒙特卡罗学习&lt;/a>; &lt; br>; &lt;strong>;&lt;em>;费拉斯·萨德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;布莱恩·巴顿&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马修·道格拉斯·霍夫曼&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;随机梯度成功实现强盗&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;梅金城&lt;/em>;&lt;/strong>;, &lt; em>;钟子鑫&lt;/em>;、&lt;strong>;&lt;em>;戴波&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;、&lt;em>;Csaba Szepesvari&lt;/ em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;基于子集的实例私人估计中的最优性&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; >;孙子腾&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment ?id=zvCSNsoyKW&amp;amp;name=pdf&quot;>;机器翻译的少样本学习的不合理有效性&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;、&lt;em>;Yamini Bansal&lt;/em>;、&lt;strong >; &lt;em>;科林·切里&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔治·福斯特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马克西姆·克里昆&lt;/em>;&lt;/strong>;、&lt; em>;梅尔文·约翰逊&lt;/em>;、&lt;em>;奥尔罕·菲拉特&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot; >;视觉中的自我监督学习：从研究进展到最佳实践&lt;/a>; &lt;br>; &lt;em>;Xinlei Chen&lt;/em>;、&lt;em>;Ishan Misra&lt;/em>;、&lt;em>;Randall Balestriero&lt;/em>; 、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;em>;克里斯托夫·费希滕霍夫&lt;/em>;、&lt;em>;马克·易卜拉欣&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://icml.cc/virtual/2023/tutorial/21560&quot;>;如何 DP-fy ML：差异隐私机器学习实用教程&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog .com/2023/05/making-ml-models- Differentially-private.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;徐峥&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/ virtual/2023/tutorial/21558&quot;>;神经网络泛化理论的最新进展&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;马腾宇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alex Damian &lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;世博日研讨会&lt;/h2 >; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Tensorflow 中的图神经网络：A实用指南&lt;/a>; &lt;br>; 研讨会组织者包括：&lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;布兰登·梅尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·哈尔克劳&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 赞助的亲和力研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www .latinxinai.org/icml-2023&quot;>;人工智能中的拉丁语&lt;/a>; (LAXAI) &lt;br>;白金赞助商&lt;br>;主题演讲者：&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>;小组成员：&lt;strong>;&lt;em>;姚勤&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop- 2023/call-for-participation?authuser=0&quot;>;机器学习领域的女性&lt;/a>; (WiML) &lt;br>;白金赞助商&lt;br>;小组成员：&lt;strong>;&lt;em>;姚勤&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github.io/&quot;>;联邦学习和分析实践：算法、系统、应用程序和机会&lt;/a>; &lt;br>; 组织者： &lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;徐铮&lt;/em>;&lt;/strong>; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Brendan McMahan&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home?authuser=1&quot;>;医疗保健领域的可解释机器学习&lt;/a>; (IMLH) &lt; br>; 组织者：&lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;数据驱动学习时代的知识与逻辑推理&lt;/a>; &lt;br>; 主办方：&lt;strong>;&lt;em>;Beliz Günel&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;基于偏好的学习的方方面面&lt;/a>; (MFPL) &lt;br>;组织者：&lt;strong>;&lt;em>;罗伯特·布萨-费科特&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;穆罕默德·加瓦姆扎德&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://syns -ml.github.io/2023/&quot;>;科学和机器学习建模的协同作用&lt;/a>; (SynS &amp;amp; ML）&lt;br>;演讲者：&lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot; >;沟通主体的心智理论&lt;/a>; &lt;br>; 主办方：&lt;strong>;&lt;em>;周培&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //sites.google.com/corp/view/aihci/home&quot;>;人工智能与人工智能人机交互&lt;/a>; &lt;br>; 主办方：&lt;em>; &lt;strong>;杨丽&lt;/strong>;&lt;/em>;、&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt; /p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;以数据为中心的机器学习研究&lt;/a>; (DMLR) &lt;br>; 组织者：&lt;strong>;&lt;em>;Alicia Parrish&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Isabelle Guyon&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;神经压缩：从信息理论到应用&lt;/a >; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Johannes Ballé&lt;/em>;&lt;/strong>; &lt;br>; 小组成员：&lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p >; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;神经会话人工智能研讨会 - 还剩下什么可以教（值得信赖、增强、适应性强、有能力和以人为中心）聊天机器人？&lt;/a>; &lt;br>; 组织者：&lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /sites.google.com/corp/view/scis-workshop-23&quot;>;虚假相关性、不变性和稳定性&lt;/a>; (SCIS) &lt;br>;组织者：&lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展位活动&lt;/h2>; &lt;div style=&quot;margin-左：20px;&quot;>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; 标题：无监督图嵌入 @ Google（&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;论文&lt;/a>;，&lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO 研讨会&lt;/a>;) &lt;br>; 7 月 25 日星期二上午 10:30 HST &lt;/p>; &lt;p >; 演讲者：&lt;strong>;&lt;em>;徐峥&lt;/em>;&lt;/strong>; &lt;br>; 标题：具有差异隐私的 Gboard 语言模型的联邦学习 (&lt;a href=&quot;https://openreview.net/attachment?id =d8LTNXt97w&amp;amp;name=pdf&quot;>;论文 1&lt;/a>;、&lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;论文 2&lt;/a>;、&lt;a href= &quot;https://ai.googleblog.com/2023/05/making-ml-models- Differentially-private.html&quot;>;博客文章&lt;/a>;) &lt;br>; 7 月 25 日星期二下午 3:30 HST &lt;/ p>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; 标题：自监督场景理解 (&lt;a href=&quot;https://arxiv.org/abs/2302.04973 &quot;>;论文 1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2203.11194&quot;>;论文 2&lt;/a>;) &lt;br>; 7 月 26 日星期三上午 10:30 HST &lt;/p >; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; 标题：变形金刚在情境中学习通过梯度下降（&lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;论文&lt;/a>;）&lt;br>; 7 月 26 日星期三下午 3:30 HST &lt;/p>; &lt;/div >; &lt;br>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>; *&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2519516457542613363/comments/default&quot; rel=&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html#comment-表单&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;编辑&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+ xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google 在 ICML 2023&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJg Lj1nhzPr3JesD4nvXkj- FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>;&lt;thr:总计>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;发布>;2023-07-20T09:22:00.002- 07:00&lt;/已发布>;&lt;更新>;2023-07-20T15:31:26.737-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= RAI-HCT 亮点&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用社会情境知识，以促进人工智能的负责任应用&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：技术项目经理、社会情境理解工具负责人 Donald Martin, Jr.和解决方案 (SCOUTS)，Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4Y RykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;显示：无；” />; &lt;p>; 人工智能相关产品和技术是在&lt;em>;社会环境&lt;/em>;中构建和部署的：即社会、文化、历史、政治和经济环境的动态且复杂的集合。由于社会环境本质上是动态的、复杂的、非线性的、有争议的、主观的和高度定性的，因此将它们转化为主导标准机器学习 (ML) 方法和负责任的人工智能产品开发的定量表示、方法和实践具有挑战性做法。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;人工智能产品开发的第一个阶段是&lt;em>;问题理解&lt;/em>;，这个阶段对问题的解决方式有着巨大的影响（例如，越来越多的问题）癌症筛查的可用性和准确性）是为 ML 系统制定的，以解决许多其他下游决策，例如数据集和 ML 架构选择。当产品运行的社会环境没有得到充分阐明，无法形成强有力的问题理解时，最终的机器学习解决方案可能会很脆弱，甚至会传播不公平的偏见。 &lt;/p>; &lt;p>; 当人工智能产品开发人员缺乏在开发过程中有效理解和考虑社会背景所需的知识和工具时，他们往往会将其抽象化。这种抽象使他们对他们寻求解决的问题有一个浅层的、定量的理解，而产品用户和社会利益相关者——他们最接近这些问题并嵌入相关的社会背景——往往对这些相同的问题有深刻的定性理解。这种理解复杂问题的定性-定量差异将产品用户和社会与开发人员区分开来，这就是我们所说的“问题理解鸿沟”。 &lt;/p>; &lt;p>; 这种鸿沟在现实世界中产生了影响：例如，它是&lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;种族歧视的根本原因广泛使用的医疗保健算法发现了偏差，旨在解决为特殊计划选择具有最复杂医疗保健需求的患者的问题。对算法运行的社会背景的不完全理解导致系统设计者对关键问题因素形成了错误且过于简单化的因果理论。关键的社会结构因素，包括缺乏医疗保健机会、对医疗保健系统缺乏信任以及由于人为偏见导致的诊断不足，都被排除在外，而医疗保健支出则被强调为复杂的预测因素。健康需要。为了负责任地弥合问题理解鸿沟，人工智能产品开发人员需要工具，将经过社区验证的、关于复杂社会问题的社会背景的结构化知识放在他们的指尖——从问题理解开始，而且贯穿整个产品开发过程。生命周期。为此，&lt;a href=&quot;https://sites.research.google/scouts/&quot;>;社会情境理解工具和解决方案&lt;/a>; (SCOUTS) — &lt;a href=&quot;https://research .google/teams/responsible-ai/&quot;>;Google 研究院内的负责任的人工智能和以人为本的技术&lt;/a>; (RAI-HCT) 团队是一个专门的研究团队，致力于“为人们提供可扩展、值得信赖的技术”实现负责任、强大的人工智能并解决世界上最复杂的社会问题所需的社会背景知识。” SCOUTS 的动力来自于阐明社会背景的重大挑战，它进行创新的基础和应用研究，以产生结构化的社会背景知识，并将其整合到人工智能相关产品开发生命周期的所有阶段。去年，我们&lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;宣布&lt;/a>;&lt;a href=&quot;https:// jigsaw.google.com/&quot;>;Jigsaw&lt;/a>; 是 Google 的技术孵化器，旨在探索开放社会威胁的解决方案，在模型开发的数据准备和评估阶段，利用我们的结构化社会背景知识方法来扩大偏见缓解范围他们广泛使用的 &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; 毒性分类器。展望未来，SCOUTS 的研究议程重点关注人工智能相关产品开发的问题理解阶段，目标是弥合问题理解鸿沟。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;弥合人工智能问题理解鸿沟&lt;/h2>; &lt;p>;弥合人工智能问题理解鸿沟需要两个关键要素：1）用于组织结构化社会背景知识的参考框架；2）参与式非提取方法，以获取有关复杂问题的社区专业知识并将其表示为结构化知识。 SCOUTS 在这两个领域都发表了创新研究。&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot; muted=&quot;&quot;playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0% ;” width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt; div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;理解鸿沟问题的说明。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;社会背景参考框架&lt;/h3>; &lt;p >; 产生结构化知识的一个基本要素是创建组织结构的分类法。 SCOUTS 与其他 RAI-HCT 团队合作 (&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;、&lt;a href =&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;影响实验室&lt;/a>;)，&lt;a href=&quot;https://www.deepmind. com/&quot;>;Google DeepMind&lt;/a>; 和外部系统动力学专家&lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;针对社会背景开发分类参考框架&lt;/a>;。为了应对社会环境的复杂性、动态性和适应性，我们利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_adaptive_system&quot;>;复杂适应性系统&lt;/a>; (CAS) 理论提出用于组织社会背景知识的高级分类模型。该模型精确指出了社会环境的三个关键要素以及将它们结合在一起的动态反馈循环：代理、戒律和工件。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;代理人&lt;/em>;：可以是个人或机构。 &lt;/li>;&lt;li>;&lt;em>;戒律&lt;/em>;：约束和驱动主体行为的先入之见，包括信仰、价值观、刻板印象和偏见。基本规则的一个例子是“所有篮球运动员都超过 6 英尺高”。这种限制性假设可能会导致无法识别身材较小的篮球运动员。 &lt;/li>;&lt;li>;&lt;em>;人工制品&lt;/em>;：主体行为产生多种人工制品，包括语言、数据、技术、社会问题和产品。 &lt;/li>; &lt;/ul>; &lt;p>; 这些实体之间的关系是动态且复杂的。我们的工作假设戒律是社会环境中最关键的元素，并且我们强调&lt;em>;人们感知到的问题&lt;/em>;以及&lt;em>;他们所持有的关于这些问题为何存在的因果理论&lt;/em>;作为特别有影响力的戒律，是理解社会背景的核心。例如，在前面描述的医疗算法中存在种族偏见的情况下，设计者所持有的因果理论规则是，复杂的健康问题将导致所有人群的医疗支出增加。这一不正确的规则直接导致选择医疗保健支出作为模型预测复杂医疗保健需求的代理变量，这反过来又导致该模型对黑人患者产生偏见，这些黑人患者由于缺乏医疗保健和缺乏医疗保健等社会因素而存在偏见。由于平均偏差而导致诊断不足，当他们有复杂的医疗保健需求时，并不总是在医疗保健上花费更多。一个关键的开放问题是，我们如何以道德和公平的方式从最接近不平等问题的人和社区中引出因果理论，并将其转化为有用的结构化知识？ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0E uHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4Zay BDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;社会背景参考框架的说明性版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding= &quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB- Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNC KRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWilmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUISHdlors19GZ0WuikJGz 6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;社会背景参考框架的分类版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;与社区合作，促进人工智能在医疗保健领域的负责任应用&lt;/h3>; &lt;p>; 自成立以来，SCOUTS 一直致力于&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive -ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>;在历史上被边缘化的社区中进行能力建设&lt;/a>;，以阐明与重要的复杂问题有关的更广泛的社会背景他们使用一种称为基于社区的系统动力学（CBSD）的实践。 &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;系统动力学&lt;/a>;（SD）是一种阐明复杂问题因果理论的方法论，无论是&lt;em>;定性&lt;strong>;&lt;/强>;&lt;/em>;作为因果循环和&lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;库存和流量图表&lt;/a>;（分别为 CLD 和 SFD）和&lt;em>;定量&lt;/em>;作为仿真模型。视觉定性工具、定量方法和协作模型构建的固有支持使其成为弥合问题理解鸿沟的理想成分。 CBSD 是一个&lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;基于社区的，SD 的参与式变体&lt;/a>;特别注重社区内部的能力建设，以协作方式将他们面临的问题描述和建模为因果理论，而无需中介。通过 CBSD，我们见证了社区团体在 2 小时内学习基础知识并开始绘制 CLD。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM 13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyY rvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; 社区成员学习系统动态。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;p>;人工智能在&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms% 20可以%20分析%20医疗、疾病%20更多%20准确%20并且快速%20。&quot;>;改善医疗诊断&lt;/a>;。但人工智能相关健康诊断算法的安全性、公平性和可靠性取决于多样化且平衡的训练数据集。健康诊断领域的一个公开挑战是缺乏来自历史边缘群体的训练样本数据。 SCOUTS 与 &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; 社区和 CBSD 专家合作制作了&lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot; >;数据差距问题的定性和定量因果理论&lt;/a>;。这些理论包括构成健康诊断更广泛社会背景的关键因素，包括死亡的文化记忆和对医疗的信任。 &lt;/p>; &lt;p>; 下图描绘了上述协作期间生成的因果理论作为 CLD。它假设对医疗保健的信任会影响这个复杂系统的所有部分，并且是增加筛查的关键杠杆，而筛查反过来又会生成数据以克服数据多样性差距。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQ Df-c0Tv-JIUAI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI /s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jiUaI9FFsbeiza BGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动” ;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2Vq XJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCedzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y /s1072/image2.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcuaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzI iJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;健康诊断数据差距的因果循环图&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;这些社区来源的因果理论是弥合问题理解鸿沟与值得信赖的社会的第一步&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 正如本博客中所讨论的，理解鸿沟问题是负责任的人工智能领域的一项关键的开放挑战。SCOUTS 与 Google 研究院的其他团队、外部社区以及跨多个学科的学术合作伙伴合作进行探索性和应用性研究，以在解决该问题方面取得有意义的进展。展望未来，我们的工作将重点关注三个关键要素，以我们的&lt;a href=&quot;http://ai.google/principles&quot;>;人工智能原则&lt;/a>;为指导：&lt;/p>; &lt;ol>; &lt;li>;提高意识和通过讲座、出版物和培训了解问题、理解鸿沟及其影响。 &lt;/li>;&lt;li>;开展基础和应用研究，将社会背景知识表示并整合到人工智能产品开发工具和工作流程中，从概念到监测、评估和适应。 &lt;/li>;&lt;li>;将基于社区的因果建模方法应用于人工智能健康公平领域，以实现影响并增强社会和 Google 生成和利用全球范围社会背景知识的能力，以实现负责任的人工智能。 &lt;/li>; &lt;/ol>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto；margin-right：auto；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47Nw zxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73Day9XL-CEtVDRJfZIwhBpa99m/s960/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEG CcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;SCOUTS 飞轮用于弥合问题理解鸿沟。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 John Guilyard 的图形开发、SCOUTS 中的每个人以及我们所有的合作者和赞助商。&lt;/em>; &lt;/p>; &lt; /content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/ 2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;利用社会背景知识促进人工智能的负责任应用&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name >;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRy kesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot;宽度= “72” xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;发布>;2023-07-18T13:15:00.000-07:00&lt;/发布>;&lt;更新>;2023-07-18T13:15： 15.633-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>; blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;SimPer：周期性目标的简单自我监督学习&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Staff Research 的 Daniel McDuff科学家和学生研究员 Yuzhe Yang，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNg x0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer %20hero.jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 从周期性数据（重复的信号，例如心跳或地球表面的每日温度变化）中学习对于许多现实世界的应用程序至关重要，来自 &lt;a href=&quot;https://cloud.google .com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;监控天气系统&lt;/a>;到&lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and -wellness-your-phone/&quot;>;检测生命体征&lt;/a>;。例如，在环境遥感领域，通常需要定期学习来实现环境变化的即时预报，例如&lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to -nowcast.html&quot;>;降水模式或地表温度&lt;/a>;。在健康领域，从视频测量中学习已证明可以提取（准）周期性生命体征，例如心房颤动&lt;/a>; 和&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;睡眠呼吸暂停发作&lt;/a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 类似&lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos. html&quot;>;RepNet&lt;/a>; 强调了此类任务的重要性，并提出了一种识别单个视频中重复活动的解决方案。然而，这些是受监督的方法，需要大量数据来捕获重复活动，所有数据都标记为指示操作重复的次数。标记此类数据通常具有挑战性且需要大量资源，需要研究人员手动捕获与感兴趣的模态（例如视频或卫星图像）同步的黄金标准时间测量。 &lt;/p>; &lt;p>; 或者，自我监督学习（SSL）方法（例如，&lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html &quot;>;SimCLR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;)，它们利用大量未标记的数据来学习捕获周期性或准周期的表示-周期性时间动态，在&lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;解决分类任务&lt;/a>;方面取得了成功。然而，他们忽视了数据中固有的周期性（即识别帧是否是周期性过程的一部分的能力），并且无法学习捕获周期性或频率属性的鲁棒表示。这是因为定期学习表现出与普遍学习任务不同的特征。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5 ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/ s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ _o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= &quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;与静态特征（例如图像）相比，周期性表示上下文中的特征相似性是不同的。例如，因短时间延迟而偏移或反转的视频应与原始样本相似，而按因子 &lt;em>;x&lt;/em>; 上采样或下采样的视频应与原始样本相差&lt;em>;x&lt;/em>; 的因子。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 为了应对这些挑战，在“&lt;a href=&quot;https:// openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer：周期性目标的简单自我监督学习&lt;/a>;”，发表于第十一届&lt;a href=&quot;https://iclr.cc/&quot;>;国际学习会议表示&lt;/a>;（ICLR 2023），我们引入了一种用于学习数据中周期性信息的自监督对比框架。具体来说，SimPer 使用时间自对比学习来利用周期性目标的时间属性，其中正样本和负样本是通过来自相同目标的周期性不变和周期性变化增强来获得的输入实例。我们提出&lt;em>;周期性特征相似性&lt;/em>;，它明确定义了如何在周期性学习的背景下测量相似性。此外，我们设计了一种广义对比损失&lt;em>;&lt;/em>;，将经典的&lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE损失&lt;/a>;扩展到软回归变体可以对连续标签（频率）进行对比。接下来，我们证明与最先进的 SSL 方法相比，SimPer 可以有效地学习周期特征表示，并强调其有趣的特性，包括更好的数据效率、对虚假相关性的鲁棒性以及对分布变化的泛化。最后，我们很高兴与研究社区一起发布 &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer 代码存储库&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SimPer 框架&lt;/h2>; &lt;p>; SimPer 引入了一个时间自我对比学习框架。正样本和负样本是通过同一输入实例的周期性不变和周期性变化增强获得的。对于时间视频示例，周期性不变的变化是裁剪、旋转或翻转，而周期性变化的变化涉及增加或降低视频的速度。 &lt;/p>; &lt;p>; 为了明确定义如何在周期性学习的背景下测量相似性，SimPer 提出了周期性特征相似性。这种结构使我们能够将训练制定为对比学习任务。可以使用没有任何标签的数据来训练模型，然后根据需要进行微调，以将学习到的特征映射到特定的频率值。 &lt;/p>; &lt;p>; 给定一个输入序列&lt;em>;x&lt;/em>;，我们知道存在一个潜在的相关周期信号。然后，我们变换&lt;em>;x&lt;/em>;来创建一系列速度或频率改变的样本，这改变了潜在的周期性目标，从而创建了不同的负面视图。尽管原始频率未知，但我们有效地为未标记的输入 x 设计伪速度或频率标签。 &lt;/p>; &lt;p>; 传统的&lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;相似性度量&lt;/a>;，例如&lt;a href=&quot;https://en.wikipedia.org /wiki/Cosine_similarity&quot;>;余弦相似度&lt;/a>;强调两个特征向量之间的严格接近，并且对索引移位特征（代表不同时间戳）、反转特征和频率变化的特征敏感。相反，对于具有较小时间偏移和/或反向索引的样本，周期性特征相似性应该很高，同时在特征频率变化时捕获连续的相似性变化。这可以通过频域中的相似性度量来实现，例如两个&lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;傅里叶变换&lt;/a>;之间的距离。 &lt;/p>; &lt;p>; 为了利用频域中增强样本的内在连续性，SimPer 设计了一种广义对比损失，它扩展了经典的 &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>; InfoNCE&lt;/a>; 损失为软回归变体，可以对连续标签（频率）进行对比。这使得它适合回归任务，其目标是恢复连续信号，例如心跳。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUv HgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm60 7Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;SimPer 通过频域变换构建数据的负面视图。输入序列&lt;em>;x&lt;/em>;具有底层关联的周期信号。 SimPer 转换&lt;em>;x&lt;/em>;来创建一系列速度或频率改变的样本，这改变了底层的周期性目标，从而创建了不同的负面视图。尽管原始频率未知，但我们有效地为未标记的输入&lt;em>;x&lt;/em>;设计了伪速度或频率标签（周期变量增强&lt;em>;τ&lt;/em>;）。 SimPer 采用不改变输入身份的变换，并将其定义为周期性不变的增强&lt;em>;σ&lt;/em>;，从而创建样本的不同正面视图。然后，它将这些增强视图发送到编码器&lt;em>;f&lt;/em>;，&lt;em>; &lt;/em>;提取相应的特征。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 为了评估 SimPer 的性能，我们将其与最先进的 SSL 方案（例如，&lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;、 &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;，&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e- Paper.pdf&quot;>;BYOL&lt;/a>;、&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;）在一组六个不同的周期上用于人类行为分析、环境遥感和医疗保健中常见现实任务的学习数据集。具体来说，下面我们展示了视频中的心率测量和运动重复计数的结果。结果表明，SimPer 在所有六个数据集上都优于最先进的 SSL 方案，突显了其在数据效率、对虚假相关性的鲁棒性以及对未见目标的泛化方面的卓越性能。 &lt;/p>; &lt;p>; 在这里，我们使用 SimPer 显示了两个代表性数据集的定量结果，这些数据集使用各种 SSL 方法进行预训练，并对标记数据进行微调。首先，我们使用 &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ 预训练 SimPer。 Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) 数据集，人类&lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;光电体积描记术&lt;/a>;和心率预测数据集，并将其性能与最先进的 SSL 方法进行比较。我们观察到 SimPer 优于 SimCLR、MoCo v2、BYOL 和 CVRL 方法。人类行为计数数据集 &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>; 的结果进一步证实了 SimPer 相对于其他方法的优势，因为它的性能明显优于其他方法监督基线。有关特征评估结果以及在其他数据集上的表现，请参阅&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorce K9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1 YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Univ 上 SimCLR、MoCo v2、BYOL、CVRL 和 SimPer 的结果。勃艮第弗朗什孔泰远程光电体积描记法 (UBFC) 和 Countix 数据集。心率和重复次数表现报告为&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;平均绝对误差&lt;/a>; (MAE)。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和应用&lt;/h2>; &lt;p>; 我们提出 SimPer，一个用于学习数据中周期性信息的自监督对比框架。我们证明，通过结合时间自对比学习框架、周期性不变和周期性变化增强以及连续周期性特征相似性，SimPer 提供了一种直观且灵活的方法来学习周期性信号的强特征表示。此外，SimPer可以应用于从环境遥感到医疗保健的各个领域。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢杨宇哲、刘鑫、Ming-Zher Poh、Jiang Wu、Silviu Borac 和 Dina Katabi 对这项工作的贡献。 &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel =&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer：周期性目标的简单自监督学习&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN -dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR _u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;已发布>;2023-07-13T14:01 :00.001-07:00&lt;/已发布>;&lt;更新>;2023-07-13T14:01:18.428-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;机器智能&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;符号调整可改善语言模型中的上下文学习&lt;/stitle>;&lt;content type= &quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究中心学生研究员 Jerry Wei 和首席科学家 Denny Zhou&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRicUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiI j-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人类智能的一个关键特征是，人类可以仅使用几个例子进行推理来学习执行新任务。扩展语言模型解锁了机器学习中的一系列新应用和范例，包括通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;执行具有挑战性的推理任务的能力>;情境学习&lt;/a>;。然而，语言模型仍然对给出提示的方式敏感，这表明它们没有以稳健的方式进行推理。例如，语言模型通常需要繁重的提示工程或措辞任务作为指令，并且它们会表现出意想不到的行为，例如 &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do- in-context.html&quot;>;即使显示不正确的标签，任务性能也不受影响&lt;/a>;。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;符号调整改善了语言模型中的上下文学习&lt; /a>;”，我们提出了一个简单的微调过程，称为&lt;em>;符号调整&lt;/em>;，它可以通过强调输入标签映射来改进上下文学习。我们在 &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalized.html&quot;>;Flan-PaLM&lt;/a>; 模型中尝试符号调整，并观察各种设置的好处。 &lt;/p>; &lt;ul>; &lt;li>;符号调整可以提高未见过的上下文学习任务的性能，并且对于未指定的提示（例如没有说明或没有自然语言标签的提示）更加稳健。 &lt;/li>;&lt;li>;符号调整模型在算法推理任务上要强大得多。 &lt;/li>;&lt;li>;最后，符号调整模型在上下文中呈现的翻转标签方面显示出巨大的改进，这意味着它们更有能力使用上下文信息来覆盖先验知识。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q 33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C /s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width =“1035”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a 5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;符号调整概述，其中模型针对自然语言标签替换为任意符号的任务进行微调。符号调整依赖于这样的直觉：当指令和相关标签不可用时，模型必须使用上下文示例来学习任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;动机&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more -generalized.html&quot;>;指令调优&lt;/a>;是一种常见的微调方法，已被证明可以提高性能并允许模型更好地遵循上下文示例。然而，一个缺点是模型不会被迫学习使用示例，因为任务是通过指令和自然语言标签在评估示例中冗余定义的。例如，在上图左侧，虽然示例可以帮助模型理解任务（情感分析），但它们并不是绝对必要的，因为模型可以忽略示例，只阅读指示任务是什么的指令。 &lt;/p>; &lt;p>; 在符号调整中，模型对示例进行微调，其中删除了指令，并将自然语言标签替换为语义不相关的标签（例如“Foo”、“Bar”等）。在此设置中，如果不查看上下文示例，任务就不清楚。例如，在上图右侧，需要多个上下文示例才能弄清楚任务。由于符号调整教会模型对上下文中的示例进行推理，因此符号调整后的模型在需要在上下文中的示例与其标签之间进行推理的任务上应该具有更好的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpk HGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035 /image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;第1035章vaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于符号调整的数据集和任务类型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;符号调整过程&lt;/h2>; &lt;p>; 我们选择了 22 个公开可用的&lt;a href=&quot;https:// /en.wikipedia.org/wiki/Natural_language_processing&quot;>;我们用于符号调整过程的自然语言处理&lt;/a>; (NLP) 数据集。这些任务在过去已经被广泛使用，我们只选择分类类型的任务，因为我们的方法需要离散标签。然后，我们将标签重新映射到一组约 30K 任意标签中的随机标签，这些标签选自以下三个类别之一：整数、字符组合和单词。 &lt;/p>; &lt;p>; 对于我们的实验，我们对 &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>; 进行符号调优，这是 &lt;a href= 的指令调优变体“https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;。我们使用三种不同尺寸的 Flan-PaLM 型号：Flan-PaLM-8B、Flan-PaLM-62B 和 Flan-PaLM-540B。我们还测试了 &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>;（Flan-PaLM-62B 为 1.3T 代币而不是 780B 代币），我们缩写为62B-c。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx -KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861 /image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;第861章MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用来自三个类别（整数、字符组合和单词）的一组 ∼300K 任意符号。 ∼30K 符号用于调优，其余符号用于评估。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt; /div>; &lt;h2>;实验设置&lt;/h2>; &lt;p>; 我们想要评估模型执行看不见的任务的能力，因此我们无法评估符号调整中使用的任务（22 个数据集）或指令调整期间使用的任务（1.8K 任务） ）。因此，我们选择了 11 个微调期间未使用的 NLP 数据集。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;上下文学习&lt;/h2>; &lt;p>; 在符号调整过程中，模型必须学会与上下文中的示例进行推理，以便成功执行任务，因为修改了提示以确保任务不能简单地从相关标签或说明中学习。经过符号调整的模型在任务不明确且需要在上下文示例及其标签之间进行推理的环境中应该表现得更好。为了探索这些设置，我们定义了四种上下文学习设置，这些设置改变了输入和标签之间所需的推理量，以便学习任务（基于指令/相关标签的可用性）&lt;/p>; &lt;tablealign=&quot;中心&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text -align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9 -LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2。 png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ld A-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据指令和相关自然语言标签的可用性，模型可能需要进行不同数量的推理并附有上下文示例。当这些功能不可用时，模型必须根据给定的上下文示例进行推理才能成功执行任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;符号调整可提高所有设置的性能模型 62B 及更大，具有相关自然语言标签的设置略有改进（+0.8% 至 +4.2%），而无相关自然语言标签的设置则有重大改进（+5.5% 至 +15.5%）。引人注目的是，当相关标签不可用时，符号调整的 Flan-PaLM-8B 优于 FlanPaLM-62B，符号调整的 Flan-PaLM-62B 优于 Flan-PaLM-540B。这种性能差异表明，符号调整可以允许更小的模型在这些任务上与大型模型一样执行（有效地节省约 10 倍的推理计算）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7 iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziu OIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;足够大的符号调整模型比基线更适合上下文学习，特别是在相关标签不可用的设置中。性能显示为十一项任务的平均模型准确度 (%)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div >; &lt;h2>;算法推理&lt;/h2>; &lt;p>; 我们还对 &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>; 的算法推理任务进行实验。主要有两组任务：1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;列出函数&lt;/a>; - 识别转换包含非负整数的输入和输出列表之间的函数（例如，删除列表中的最后一个元素）； 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;简单的图灵概念&lt;/a>; - 使用二进制字符串进行推理，以了解以下概念：将输入映射到输出（例如，交换字符串中的 0 和 1）。 &lt;/p>; &lt;p>; 在列表函数和简单图灵概念任务上，符号调整的平均性能分别提高了 18.2% 和 15.3%。此外，具有符号调整功能的 Flan-cont-PaLM-62B 在列表函数任务上的平均性能优于 Flan-PaLM-540B，这相当于推理计算量减少了约 10 倍。这些改进表明符号调整增强了模型在上下文中学习未见过的任务类型的能力，因为符号调整不包含任何算法数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5 XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB -jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original -width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL _Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot;/>;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;符号调整模型在列表函数任务和简单图灵概念任务上实现更高的性能。 (A–E)：列表函数任务的类别。 (F)：简单的图灵概念任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;翻转标签&lt;/h2>; &lt;p>; 在翻转标签实验中，上下文和评估示例的标签被翻转，这意味着先验知识和输入标签映射不一致（例如，包含积极情绪的句子被标记为“消极情绪”） ，从而使我们能够研究模型是否可以覆盖先验知识。 &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;之前的工作&lt;/a>;表明，虽然预训练模型（没有指令调优）在某种程度上可以遵循上下文中呈现的翻转标签，指令调优降低了这种能力。 &lt;/p>; &lt;p>; 我们发现所有模型大小都存在类似的趋势 - 符号调整模型比指令调整模型更能遵循翻转标签。我们发现，经过符号调整后，Flan-PaLM-8B 在所有数据集上的平均改进为 26.5%，Flan-PaLM-62B 的改进为 33.7%，Flan-PaLM-540B 的改进为 34.0%。此外，经过符号调整的模型与仅预训练模型的平均性能相似或更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOS K4VS -3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt; img border =“0”数据原始高度=“434”数据原始-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3Tlq SJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt; /a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;符号调整模型比上下文中呈现的翻转标签更擅长跟踪指令调整模型是。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2 >; &lt;p>; 我们提出了符号调优，这是一种在将自然语言标签重新映射到任意符号的任务中调整模型的新方法。符号调整基于这样的直觉：当模型无法使用指令或相关标签来确定所呈现的任务时，它必须通过从上下文示例中学习来实现这一点。我们使用符号调优程序调优了四种语言模型，利用 22 个数据集和大约 30K 任意符号作为标签的调优混合物。 &lt;/p>; &lt;p>; 我们首先表明，符号调整可以提高未见过的上下文学习任务的性能，特别是当提示不包含说明或相关标签时。我们还发现，尽管符号调整过程中缺乏数值或算法数据，但符号调整模型在算法推理任务方面表现得更好。最后，在输入具有翻转标签的上下文学习环境中，符号调整（对于某些数据集）可以恢复在指令调整期间丢失的遵循翻转标签的能力。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;未来的工作&lt;/h2>; &lt;p>;通过符号调整，我们的目标是提高模型的程度可以在上下文学习期间检查输入-标签映射并从中学习。我们希望我们的结果能够鼓励进一步努力提高语言模型对上下文中呈现的符号进行推理的能力。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者现已加入 Google DeepMind 。这项工作由 Jerry Wei、Le Hou、Andrew Lampinen、Xiangning Chen、Da Huang、Yi Tay、Xinyun Chen、Yifeng Lu、Denny Zhou、Tengyu Ma 和 Quoc V. Le 进行。我们要感谢 Google Research 和 Google DeepMind 的同事提供的建议和有益的讨论。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2013323302512835157 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning- Improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel= “self” type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;符号调整改善了语言模型中的上下文学习&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yK WoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;发布>;2023-07- 11T10:00:00.010-07:00&lt;/发布>;&lt;更新>;2023-07-11T10:44:15.111-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>;&lt;/category>;&lt;category schema=&quot; http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;机器学习辅助计算机架构设计的开源体育馆&lt;/stitle>;&lt; content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Amir Yazdanbakhsh 和访问研究员 Vijay Janapa Reddi&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7St Ub4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;计算机体系结构&lt;/a>;研究在开发模拟器和工具来评估和塑造计算机系统的设计方面有着悠久的历史。例如，&lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNv NONNQ&quot;>;SimpleScalar&lt;/a>; 模拟器于 20 世纪 90 年代末推出并允许研究人员探索各种&lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;微架构&lt;/a>;想法。计算机架构模拟器和工具，例如 &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHg Med-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; 等在推进计算机体系结构研究方面发挥了重要作用。从那时起，这些共享的资源和基础设施使工业界和学术界受益，并使研究人员能够系统地借鉴彼此的工作，从而导致该领域取得重大进展。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 尽管如此，计算机体系结构研究正在不断发展，行业和学术界转向机器学习 (ML) 优化，以满足严格的特定领域要求，例如 &lt; a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;计算机架构机器学习&lt;/a>;、&lt;a href=&quot;https://arxiv.org /pdf/2201.01863.pdf&quot;>;用于 TinyML 加速的机器学习&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN &lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;加速器&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/ 10.1145/3503222.3507767&quot;>;数据路径优化&lt;/a>;、&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z4 6Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;内存控制器&lt;/a>; 、&lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;功耗&lt;/a>;、&lt;a href= “https://ieeexplore.ieee.org/document/7430287&quot;>;安全&lt;/a>;，以及&lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;隐私&lt; /a>;.尽管之前的工作已经证明了机器学习在设计优化方面的优势，但缺乏强大的、可重复的基线阻碍了不同方法之间的公平和客观比较，并给它们的部署带来了一些挑战。为了确保稳步进展，必须共同理解和应对这些挑战。 &lt;/p>; &lt;p>; 为了缓解这些挑战，请参阅“&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym：用于机器学习辅助架构设计的开源健身房&lt; /a>;”，在 &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>; 上接受，我们引入了 ArchGym，其中包括各种计算机架构模拟器和 ML 算法。在 ArchGym 的支持下，我们的结果表明，只要有足够多的样本，任何不同的 ML 算法集合都能够为每个目标问题找到最佳的架构设计参数集； &lt;i>;没有一种解决方案一定比另一种更好&lt;/i>;。这些结果进一步表明，为给定的 ML 算法选择最佳超参数对于寻找最佳架构设计至关重要，但选择它们并非易事。我们&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;发布&lt;/a>;跨多个计算机架构模拟和机器学习算法的代码和数据集。&lt;/p>; &lt;br />; &lt;h2>;机器学习中的挑战-辅助架构研究 &lt;/h2>; &lt;p>; 机器学习辅助架构研究提出了多项挑战，包括： &lt;/p>; &lt;ol>; &lt;li>;对于特定的机器学习辅助计算机架构问题（例如，为&lt; a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; 控制器）没有系统的方法来识别最佳机器学习算法或超参数（例如学习速率、预热步骤） ， ETC。）。机器学习和启发式方法的范围更广，从&lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;随机游走&lt;/a>;到&lt;a href=&quot;http://incompleteideas.net/ book/RLbook2020.pdf&quot;>;强化学习&lt;/a>; (RL)，可用于&lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;设计空间探索&lt;/a>;（经济发展局）。虽然这些方法在其选择的基线上显示出显着的性能改进，但尚不清楚这些改进是由于优化算法还是超参数的选择所致。&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;因此，为了确保可重复性并促进机器学习辅助架构 DSE 的广泛采用，有必要概述系统的基准测试方法。&lt;/em>; &lt;br />;&lt;br />; &lt; /li>; &lt;li>;虽然计算机架构模拟器一直是架构创新的支柱，但越来越需要解决架构探索中的准确性、速度和成本之间的权衡。性能估计的准确性和速度因模拟器而异，具体取决于底层建模细节（例如，&lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;周期&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f -zIkC1q6_OUX11TzJQ&quot;>;准确&lt;/a>;与&lt;a href =&quot;https://arxiv.org/abs/2210.03894&quot;>;机器学习&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;基于&lt;/a>; &lt;a href=&quot;https ://arxiv.org/abs/1808.07412&quot;>;代理&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;模型&lt;/a>;）。虽然分析或基于机器学习的代理模型由于丢弃低级细节而非常灵活，但它们通常会出现较高的预测误差。此外，由于商业许可，&lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;从模拟器收集的运行次数可能有严格的限制&lt;/a>;。总体而言，这些约束表现出明显的性能与样本效率权衡，影响架构探索的优化算法的选择。 &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;描述如何在这些约束下系统地比较各种机器学习算法的有效性具有挑战性。&lt;/ em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;最后，机器学习算法的前景正在迅速发展，一些机器学习算法需要数据才能发挥作用。此外，将 DSE 的结果呈现为有意义的工件（例如数据集）对于深入了解设计空间至关重要。 &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;在这个快速发展的生态系统中，确保如何分摊架构搜索算法的开销至关重要勘探。目前尚不清楚，也没有系统地研究如何在不了解底层搜索算法的情况下利用探索数据。&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym 设计&lt;/h2>; &lt;p>; ArchGym 通过提供一个统一的框架来公平地评估不同的基于 ML 的搜索算法来解决这些挑战。 It comprises two main components: 1) the ArchGym environment and 2) the ArchGym agent. The environment is an encapsulation of the architecture cost model — which includes latency, throughput, area, energy, etc., to determine the computational cost of running the workload, given a set of architectural parameters — paired with the target workload(s). The agent is an encapsulation of the ML algorithm used for the search and consists of hyperparameters and a guiding policy. The hyperparameters are intrinsic to the algorithm for which the model is to be optimized and can significantly influence performance. The policy, on the other hand, determines how the agent selects a parameter iteratively to optimize the target objective. &lt;/p>; &lt;p>; Notably, ArchGym also includes a standardized interface that connects these two components, while also saving the exploration data as the ArchGym Dataset. At its core, the interface entails three main signals: &lt;i>;hardware state&lt;/i>;, &lt;i>;hardware parameters&lt;/i>;, and &lt;i>;metrics&lt;/i>;. These signals are the bare minimum to establish a meaningful communication channel between the environment and the agent.&amp;nbsp;Using these signals, the agent observes the state of the hardware and suggests a set of hardware parameters to iteratively optimize a (user-defined) reward. The reward is a function of hardware performance metrics, such as performance, energy consumption, etc.&amp;nbsp;&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1226&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym-animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym comprises two main components: the ArchGym environment and the ArchGym agent. The ArchGym environment encapsulates the cost model and the agent is an abstraction of a policy and hyperparameters. With a standardized interface that connects these two components, ArchGym provides a unified framework for evaluating different ML-based search algorithms fairly while also saving the exploration data as the ArchGym Dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ML algorithms could be equally favorable to meet user-defined target specifications&lt;/h2>; &lt;p>; Using ArchGym, we empirically demonstrate that across different optimization objectives and DSE problems, &lt;em>;at least one set of hyperparameters exists that results in the same hardware performance as other ML algorithms&lt;/em>;. A poorly selected (random selection) hyperparameter for the ML algorithm or its baseline can lead to a misleading conclusion that a particular family of ML algorithms is better than another. We show that with sufficient hyperparameter tuning, different search algorithms, even &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;random walk&lt;/a>; (RW), are able to identify the best possible reward. However, note that finding the right set of hyperparameters may require exhaustive search or even luck to make it competitive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With a sufficient number of samples, there exists at least one set of hyperparameters that results in the same performance across a range of search algorithms. Here the dashed line represents the maximum normalized reward. &lt;i>;Cloud-1&lt;/i>;, &lt;i>;cloud-2&lt;/i>;, &lt;i>;stream&lt;/i>;, and &lt;i>;random&lt;/i>; indicate four different memory traces for &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; (DRAM subsystem design space exploration framework).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset construction and high-fidelity proxy model training&lt;/h2>; &lt;p>; Creating a unified interface using ArchGym also enables the creation of datasets that can be used to design better data-driven ML-based proxy architecture cost models to improve the speed of architecture simulation. To evaluate the benefits of datasets in building an ML model to approximate architecture cost, we leverage ArchGym&#39;s ability to log the data from each run from DRAMSys to create four dataset variants, each with a different number of data points. For each variant, we create two categories: (a) Diverse Dataset, which represents the data collected from different agents (&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/a>;), and (b) ACO only, which shows the data collected exclusively from the ACO agent, both of which are released along with ArchGym. We train a proxy model on each dataset using &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;random forest regression&lt;/a>; with the objective to predict the latency of designs for a &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM simulator&lt;/a>;. Our results show that: &lt;/p>; &lt;ol>; &lt;li>;As we increase the dataset size, the average normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;root mean squared error&lt;/a>; (RMSE) slightly decreases. &lt;/li>;&lt;li>;However, as we introduce diversity in the dataset (eg, collecting data from different agents), we observe 9× to 42× lower RMSE across different dataset sizes. &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1143&quot; data-original-width=&quot;1826&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diverse dataset collection across different agents using ArchGym interface.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/s1803/ArchGym1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The impact of a diverse dataset and dataset size on the normalized RMSE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;The need for a community-driven ecosystem for ML-assisted architecture research&lt;/h2>; &lt;p>;While, ArchGym is an initial effort towards creating an open-source ecosystem that (1) connects a broad range of search algorithms to computer architecture simulators in an unified and easy-to-extend manner, (2) facilitates research in ML-assisted computer architecture, and (3) forms the scaffold to develop reproducible baselines, there are a lot of open challenges that need community-wide support. Below we outline some of the open challenges in ML-assisted architecture design. Addressing these challenges requires a well coordinated effort and a community driven ecosystem.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot; data-original-width=&quot;1999&quot; height=&quot;310&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key challenges in ML-assisted architecture design.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We call this ecosystem &lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/&quot;>;Architecture 2.0&lt;/a>;.&amp;nbsp;We outline the key challenges and a vision for building an inclusive ecosystem of interdisciplinary researchers to tackle the long-standing open problems in applying ML for computer architecture research.&amp;nbsp;If you are interested in helping shape this ecosystem, please fill out the &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/viewform&quot;>;interest survey&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>; is an open source gymnasium for ML architecture DSE and enables an standardized interface that can be readily extended to suit different use cases. Additionally, ArchGym enables fair and reproducible comparison between different ML algorithms and helps to establish stronger baselines for computer architecture research problems. &lt;/p>; &lt;p>; We invite the computer architecture community as well as the ML community to actively participate in the development of ArchGym. We believe that the creation of a gymnasium-type environment for computer architecture research would be a significant step forward in the field and provide a platform for researchers to use ML to accelerate research and lead to new and innovative designs. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This blogpost is based on joint work with several co-authors at Google and Harvard University.&amp;nbsp;&lt;/i>;&lt;i>;We would like to acknowledge and highlight Srivatsan Krishnan (Harvard) who contributed several ideas to this project in collaboration with Shvetank Prakash (Harvard), Jason Jabbour (Harvard), Ikechukwu Uchendu (Harvard), Susobhan Ghosh (Harvard), Behzad Boroujerdian (Harvard), Daniel Richins (Harvard), Devashree Tripathy (Harvard), and Thierry Thambe (Harvard).&amp;nbsp; In addition, we would also like to thank James Laudon, Douglas Eck, Cliff Young, and Aleksandra Faust for their support, feedback, and motivation for this work. We would also like to thank John Guilyard for the animated figure used in this post. Amir Yazdanbakhsh is now a Research Scientist at Google DeepMind and Vijay Janapa Reddi is an Associate Professor at Harvard.&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;An open-source gymnasium for machine learning assisted computer architecture design&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;published>;2023-07-10T06:02:00.005-07:00&lt;/published>;&lt;updated>;2023-07-21T13:24:14.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://2023.aclweb.org/&quot;>;61st annual meeting&lt;/a>; of the &lt;a href=&quot;https://www.aclweb.org/&quot;>;Association for Computational Linguistics&lt;/a>; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place in Vancouver, BC. As a leader in natural language processing and understanding, and a&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;Diamond Level sponsor&lt;/a>;&amp;nbsp;of&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, Google will showcase the latest research in the field with over 50 publications, and active involvement in a variety of workshops and tutorials.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;If you&#39;re registered for ACL 2023, we hope that you&#39;ll visit the Google booth to learn more about the projects at Google that go into solving interesting problems for billions of people. You can also learn more about Google&#39;s participation below (Google affiliations in &lt;b>;bold&lt;/b>;).&lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Area chairs include: &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; Workshop chairs include: &lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/strong>; &lt;br />; Publication chairs include: &lt;strong>;&lt;em>;Lei Shu&lt;/em>;&lt;/strong>; &lt;br />; Program Committee includes: &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Spotlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd: Open Source Initiative for Indonesian NLP Resources&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, &lt;strong>;Sebastian Ruder&lt;/strong>;, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;Optimizing Test-Time Query Representations for Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, &lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, Dan Roth, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02301.pdf&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chih-Kuan Yeh&lt;/strong>;, &lt;strong>;Hootan Nakhost&lt;/strong>;, &lt;strong>;Yasuhisa Fujii&lt;/strong>;, Alex Ratner, Ranjay Krishna, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;Large Language Models with Controllable Working Memory&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Daliang Li&lt;/b>;, &lt;b>;Ankit Singh Rawat&lt;/b>;, &lt;b>;Manzil Zaheer&lt;/b>;, &lt;b>;Xin Wang&lt;/b>;, &lt;b>;Michal Lukasik&lt;/b>;, &lt;b>;Andreas Veit&lt;/b>;, &lt;b>;Felix Yu&lt;/b>;,&lt;b>; Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10791.pdf&quot;>;OpineSum: Entailment-Based Self-Training for Abstractive Opinion Summarization&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE: Leveraging Retrieval Techniques for Summarization Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Uthus&lt;/b>;, &lt;b>;Jianmo Ni&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;Follow the Leader(board) with Confidence: Estimating p-Values from a Single Test Set with Item and Response Variance&lt;/a>; &lt;br />; &lt;i>; Shira Wein*, Christopher Homan, &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Chris Welty&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.02516.pdf&quot;>;SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;, &lt;strong>;Gustavo Hernandez Abrego&lt;/strong>;, &lt;strong>;Peter Dornbach&lt;/strong>;, &lt;strong>;Imed Zitouni&lt;/strong>;, &lt;strong>;Enrique Alfonseca&lt;/strong>;, &lt;strong>;Zhe Dong&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou, &lt;strong>;Colin Cherry&lt;/strong>;, &lt;strong>;George Foster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;Prompting PaLM for Translation: Assessing Strategies and Performance&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Vilar&lt;/b>;, &lt;b>;Markus Freitag&lt;/b>;, &lt;b>;Colin Cherry&lt;/b>;, &lt;b>;Jiaming Luo&lt;/b>;, &lt;b>;Viresh Ratnakar&lt;/b>;, &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;Query Refinement Prompts for Closed-Book Long-Form QA&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kellie Webster&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*, &lt;strong>;Emma Strubell&lt;/strong>;, Sameer Singh, &lt;strong>;Pat Verga&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;b>;Parker Riley&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Jan A. Botha&lt;/b>;, &lt;b>;Xavier Garcia&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Jason Riesa&lt;/b>;, &lt;b>;Orhan Firat&lt;/b>;,&lt;b>; Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.00397.pdf&quot;>;Conditional Generation with a Question-Answering Blueprint&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kuzman Ganchev&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;, &lt;b>;Fantine Huot&lt;/b>;, &lt;b>;Anders Sandholm&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;Coreference Resolution Through a Seq2Seq Transition-Based System&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;, &lt;b>;Chris Alberti&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni, &lt;strong>;Dan Garrette&lt;/strong>;, Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot;>;DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue&lt;/a>; &lt;br />; &lt;em>;William Held*, &lt;strong>;Christopher Hidey&lt;/strong>;, &lt;strong>;Fei Liu&lt;/strong>;, &lt;strong>;Eric Zhu&lt;/strong>;, &lt;strong>;Rahul Goel&lt;/strong>;, Diyi Yang, &lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.08726.pdf&quot;>;RARR: Researching and Revising What Language Models Say, Using Language Models&lt;/a>; &lt;br />; Luyu Gao*, &lt;strong>;Zhuyun Dai&lt;/strong>;, &lt;strong>;Panupong Pasupat&lt;/strong>;, Anthony Chen*, &lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;, &lt;strong>;Yicheng Fan&lt;/strong>;, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Ni Lao&lt;/strong>;, &lt;strong>;Hongrae Lee&lt;/strong>;, &lt;strong>;Da-Cheng Juan&lt;/strong>;, &lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.16793.pdf&quot;>;Benchmarking Large Language Model Capabilities for Conditional Generation&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez, Priyanka Agrawal, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;Crosslingual Generalization Through Multitask Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff, Thomas Wang, Lintang Sutawika, &lt;strong>;Adam Roberts&lt;/strong>;, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering&lt;/a>; &lt;br />; &lt;em>;Ella Neeman, &lt;strong>;Roee Aharoni&lt;/strong>;, Or Honovich, Leshem Choshen, &lt;strong>;Idan Szpektor&lt;/strong>;, Omri Abend&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;Resolving Indirect Referring Expressions for Entity Selection&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini&lt;/b>;, &lt;b>;Filip Radlinski&lt;/b>;, &lt;b>;Silvia Pareti&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*, &lt;strong>;Aida Mostafazadeh Davani&lt;/strong>;, Chandan K Reddy, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam, &lt;strong>;Sunipa Dev&lt;/strong>;, Daniel Khashabi, Tushar Khot, Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;Character-Aware Models Improve Visual Text Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Chitwan Saharia&lt;/b>;, &lt;b>;William Chan&lt;/b>;, &lt;b>;Adam Roberts&lt;/b>;, &lt;b>;Sharan Narang&lt;/b>;, &lt;b>;Irina Blok&lt;/b>;, &lt;b>;RJ Mical&lt;/b>;, &lt;b>;Mohammad Norouzi&lt;/b>;, &lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;Cold-Start Data Selection for Better Few-Shot Language Model Fine-Tuning: A Prompt-Based Uncertainty Propagation Approach&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment &lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;, &lt;b>;Alexandre Djerbetian&lt;/b>;, &lt;b>;Roee Engelberg&lt;/b>;, &lt;b>;Lidan Hackmon&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;, &lt;b>;Amir Globerson&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Chen-Yu Lee&lt;/b>;, &lt;b>;Chun-Liang Li&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Vincent Perot&lt;/b>;, &lt;b>;Guolong Su&lt;/b>;, &lt;b>;Xiang Zhang&lt;/b>;,&lt;b>; Kihyuk Sohn&lt;/b>;, &lt;b>;Nikolay Glushinev&lt;/b>;, &lt;b>;Renshen Wang&lt;/b>;, &lt;b>;Joshua Ainslie&lt;/b>;, &lt;b>;Shangbang Long&lt;/b>;, &lt;b>;Siyang Qin&lt;/b>;,&lt;b>; Yasuhisa Fujii&lt;/b>;, &lt;b>;Nan Hua&lt;/b>;, &lt;b>;Tomas Pfister&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;Dialect-Robust Evaluation of Generated Text&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy Dozat&lt;/strong>;, &lt;strong>;Dan Garrette&lt;/strong>;, &lt;strong>;Aditya Siddhant&lt;/strong>;, &lt;strong>;Jacob Eisenstein&lt;/strong>;, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;MISGENDERED: Limits of Large Language Models in Understanding Pronouns&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain, &lt;strong>;Sunipa Dev&lt;/strong>;, Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;, &lt;b>;Najoung Kim&lt;/b>;, &lt;b>;Deepti Bhatia&lt;/b>;, &lt;b>;Xin Xu&lt;/b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*, &lt;strong>;Annie Louis&lt;/strong>;, &lt;strong>;Mohammad Javad Hosseini&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, &lt;strong>;Donald Metzler&lt;/strong>;, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392.pdf&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Andy Zeng&lt;/strong>;, Trevor Darrell, Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters&lt;/a>; &lt;br />; &lt;em>;Boshi Wang, Sewon Min, Xiang Deng, &lt;strong>;Jiaming Shen&lt;/strong>;, &lt;strong>;You Wu&lt;/strong>;, Luke Zettlemoyer and Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;Better Zero-Shot Reasoning with Self-Adaptive Prompting&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*, &lt;strong>;Ruoxi Sun&lt;/strong>;, Hanjun Dai, &lt;strong>;Sercan Ö. Arik&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;, &lt;b>;Johan Ferret&lt;/b>;, &lt;b>;Lior Shani&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Geoffrey Cideron&lt;/b>;, &lt;b>;Robert Dadashi&lt;/b>;, &lt;b>;Matthieu Geist&lt;/b>;,&lt;b>; Sertan Girgin&lt;/b>;, &lt;b>;Léonard Hussenot&lt;/b>;, &lt;b>;Orgad Keller&lt;/b>;, &lt;b>;Nikola Momchev&lt;/b>;, &lt;b>;Sabela Ramos&lt;/b>;, &lt;b>;Piotr Stanczyk&lt;/b>;, &lt;b>;Nino Vieillard&lt;/b>;, &lt;b>;Olivier Bachem&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Avinatan Hassidim&lt;/b>;, &lt;b>;Olivier Pietquin&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;Natural Language to Code Generation in Interactive Data Science Notebooks&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Pengcheng Yin&lt;/b>;, &lt;b>;Wen-Ding Li&lt;/b>;, &lt;b>;Kefan Xiao&lt;/b>;, &lt;b>;Abhishek Rao&lt;/b>;, &lt;b>;Yeming Wen&lt;/b>;, &lt;b>;Kensen Shi&lt;/b>;, &lt;b>;Joshua Howland&lt;/b>;,&lt;b>; Paige Bailey&lt;/b>;, &lt;b>;Michele Catasta&lt;/b>;, &lt;b>;Henryk Michalewski&lt;/b>;, &lt;b>;Oleksandr Polozov&lt;/b>;, &lt;b>;Charles Sutton&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08410.pdf&quot;>;Teaching Small Language Models to Reason&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*, &lt;strong>;Jonathan Mallinson&lt;/strong>;, &lt;strong>;Jakub Adamek&lt;/strong>;, &lt;strong>;Eric Malmi&lt;/strong>;, &lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf/papers/NeuPSL.pdf&quot;>;Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*, &lt;strong>;Quan Yuan&lt;/strong>;, &lt;strong>;Jeremiah Liu&lt;/strong>;, &lt;strong>;Mehran Kazemi&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>;, &lt;strong>;Tania Bedrax-Weiss&lt;/strong>;, Lise Getoor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization&lt;/a>; &lt;br />; &lt;em>;Lining Zhang, Simon Mille, Yufang Hou, &lt;strong>;Daniel Deutsch&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Yixin Liu, Saad Mahamood, &lt;strong>;Sebastian Gehrmann&lt;/strong>;, Miruna Clinciu, Khyathi Raghavi Chandu and João Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Industry Track papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18465.pdf&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Zheng Xu&lt;/b>;, &lt;b>;Yanxiang Zhang&lt;/b>;, &lt;b>;Galen Andrew&lt;/b>;, &lt;b>;Christopher Choquette&lt;/b>;, &lt;b>;Peter Kairouz&lt;/b>;, &lt;b>;Brendan McMahan&lt;/b>;, &lt;b>;Jesse Rosenstock&lt;/b>;, &lt;b>;Yuanbo Zhang&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, Hao Su, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL Findings papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;Multilingual Summarization with Factual Consistency Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Jonathan Herzig&lt;/b>;, &lt;b>;Elizabeth Clark&lt;/b>;, &lt;b>;Mirella Lapata &lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;Parameter-Efficient Fine-Tuning for Robust Continual Multilingual Learning&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;, &lt;b>;Shachi Dave&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*, &lt;strong>;Yury Zemlyanskiy&lt;/strong>;, &lt;strong>;Joshua Ainslie&lt;/strong>;, &lt;strong>;Nicholas FitzGerald&lt;/strong>;, &lt;strong>;Sumit Sanghai&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;William Cohen&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;A Simple, Yet Effective Approach to Finding Biases in Code Generation&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos, Mateusz Malinowski, &lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun&lt;/b>;, &lt;b>;Nathan Scales&lt;/b>;, &lt;b>;Nathanael Scharli&lt;/b>;, &lt;b>;Sebastian Gehrmann&lt;/b>;, &lt;b>;Yi Tay&lt;/b>;, &lt;b>;Hyung Won Chung&lt;/b>;,&lt;b>; Aakanksha Chowdhery&lt;/b>;, &lt;b>;Quoc Le&lt;/b>;, &lt;b>;Ed Chi&lt;/b>;, &lt;b>;Denny Zhou&lt;/b>;, &lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm: A Simple Zero-Shot Form Entity Query Framework&lt;/a>; &lt;br />; &lt;em>;Zifeng Wang*, &lt;strong>;Zizhao Zhang&lt;/strong>;, &lt;strong>;Jacob Devlin&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Guolong Su&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, Jennifer Dy, &lt;strong>;Vincent Perot&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;Multilingual Sequence-to-Sequence Models for Hebrew NLP&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;, &lt;b>;Hila Noga&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;Triggering Multi-Hop Reasoning for Question Answering in Language Models Using Soft Prompts and Random Walks&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*, &lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;, Siamak Shakeri&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Complex Reasoning in Natural Language&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, &lt;strong>;Aman Madaan&lt;/strong>;, Tao Yu&lt;/em>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Generating Text from Language Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini&lt;/b>;, &lt;b>;Ryan Cotterell&lt;/b>;, &lt;b>;John Hewitt&lt;/b>;, &lt;b>;Clara Meister&lt;/b>;, &lt;b>;Tiago Pimentel&lt;/b>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/sustainlp2023&quot;>;Simple and Efficient Natural Language Processing (SustaiNLP)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.workshopononlineabuse.com/&quot;>;Workshop on Online Abuse and Harms (WOAH)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;Document-Grounded Dialogue and Conversational Question Answering (DialDoc)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4convai/home?authuser=0&quot;>;NLP for Conversational AI&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl.wellformedness.com/&quot;>;Computation and Written Language (CAWL)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;b>;Kyle Gorman&lt;/b>;, &lt;b>;Brian Roark&lt;/b>;, &lt;b>;Richard Sproat&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;Computational Morphology and Phonology (SIGMORPHON)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023&quot;>;Workshop on Narrative Understanding (WNU)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;published>;2023-07-07T11:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-07T11:09:20.724-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modular visual question answering via code generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sanjay Subramanian, PhD student, UC Berkeley, and Arsha Nagrani, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;Visual question answering&lt;/a>; (VQA) is a machine learning task that requires a model to answer a question about an image or &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;a set of images&lt;/a>;. Conventional VQA approaches need a large amount of labeled training data consisting of thousands of human-annotated question-answer pairs associated with images. In recent years, advances in large-scale pre-training have led to the development of VQA methods that perform well with &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;fewer than fifty training examples&lt;/a>; (few-shot) and &lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions-for-visual.html&quot;>;without any human-annotated VQA training data&lt;/a>; (zero-shot). However, there is still a significant performance gap between these methods and state-of-the-art fully supervised VQA methods, such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;. In particular, few-shot methods struggle with spatial reasoning, counting, and multi-hop reasoning. Furthermore, few-shot methods have generally been limited to answering questions about single images. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve accuracy on VQA examples that involve complex reasoning, in “&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>;,” to appear at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we introduce CodeVQA, a framework that answers visual questions using program synthesis. Specifically, when given a question about an image or set of images, CodeVQA generates a Python program (code) with simple visual functions that allow it to process images, and executes this program to determine the answer. We demonstrate that in the few-shot setting, CodeVQA outperforms prior work by roughly 3% on the &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; dataset and 2% on the &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; The CodeVQA approach uses a code-writing large language model (LLM), such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;, to generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; programs (code). We guide the LLM to correctly use visual functions by crafting a prompt consisting of a description of these functions and fewer than fifteen “in-context” examples of visual questions paired with the associated Python code for them. To select these examples, we compute embeddings for the input question and of all of the questions for which we have annotated programs (a randomly chosen set of fifty). Then, we select questions that have the highest similarity to the input and use them as in-context examples. Given the prompt and question that we want to answer, the LLM generates a Python program representing that question. &lt;/p>; &lt;p>; We instantiate the CodeVQA framework using three visual functions: (1) &lt;code>;query&lt;/code>;, (2) &lt;code>;get_pos&lt;/code>;, and (3) &lt;code>;find_matching_image&lt;/code>;. &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;, which answers a question about a single image, is implemented using the few-shot &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;>;Plug-and-Play VQA&lt;/a>; (PnP-VQA) method. PnP-VQA generates captions using &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; — an image-captioning &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; pre-trained on millions of image-caption pairs — and feeds these into a LLM that outputs the answers to the question. &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>;, which is an object localizer that takes a description of an object as input and returns its position in the image, is implemented using &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;. Specifically, the description and the image are passed through the BLIP joint text-image encoder, which predicts an image-text matching score. GradCAM takes the gradient of this score with respect to the image features to find the region most relevant to the text. &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;, which is used in multi-image questions to find the image that best matches a given input phrase, is implemented by using BLIP text and image encoders to compute a text embedding for the phrase and an image embedding for each image. Then the dot products of the text embedding with each image embedding represent the relevance of each image to the phrase, and we pick the image that maximizes this relevance. &lt;/li>; &lt;/ul>; &lt;p>; The three functions can be implemented using models that require very little annotation (eg, text and image-text pairs collected from the web and a small number of VQA examples). Furthermore, the CodeVQA framework can be easily generalized beyond these functions to others that a user might implement (eg, object detection, image segmentation, or knowledge base retrieval). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the CodeVQA method. First, a large language model generates a Python program (code), which invokes visual functions that represent the question. In this example, a simple VQA method (&lt;code>;query&lt;/code>;) is used to answer one part of the question, and an object localizer (&lt;code>;get_pos&lt;/code>;) is used to find the positions of the objects mentioned. Then the program produces an answer to the original question by combining the outputs of these functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; The CodeVQA framework correctly generates and executes Python programs not only for single-image questions, but also for multi-image questions. For example, if given two images, each showing two pandas, a question one might ask is, “Is it true that there are four pandas?” In this case, the LLM converts the counting question about the pair of images into a program in which an object count is obtained for each image (using the &lt;em>;query&lt;/em>; function). Then the counts for both images are added to compute a total count, which is then compared to the number in the original question to yield a yes or no answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate CodeVQA on three visual reasoning datasets: &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; (single-image), &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; (multi-image), and &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>; (multi-image). For GQA, we provide 12 in-context examples to each method, and for COVR and NLVR2, we provide six in-context examples to each method. The table below shows that CodeVQA improves consistently over the baseline few-shot VQA method on all three datasets. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the GQA, COVR, and NLVR2 datasets, showing that CodeVQA consistently improves over few-shot PnP-VQA. The metric is exact-match accuracy, ie, the percentage of examples in which the predicted answer exactly matches the ground-truth answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that in GQA, CodeVQA&#39;s accuracy is roughly 30% higher than the baseline on spatial reasoning questions, 4% higher on “and” questions, and 3% higher on “or” questions. The third category includes multi-hop questions such as “Are there salt shakers or skateboards in the picture?”, for which the generated program is shown below. &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px;右边距：40px； white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;Are there any salt shakers?&quot;) skateboards_exist = query(img, &quot;Are there any skateboards?&quot;) if salt_shakers_exist == &quot;yes&quot; or skateboards_exist == &quot;yes&quot;: answer = &quot;yes&quot; else: answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>; In COVR, we find that CodeVQA&#39;s gain over the baseline is higher when the number of input images is larger, as shown in the table below. This trend indicates that breaking the problem down into single-image questions is beneficial. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Number of images&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>;&amp;nbsp; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present CodeVQA, a framework for few-shot visual question answering that relies on code generation to perform multi-step visual reasoning. Exciting directions for future work include expanding the set of modules used and creating a similar framework for visual tasks beyond VQA. We note that care should be taken when considering whether to deploy a system such as CodeVQA, since vision-language models like the ones used in our visual functions &lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;have been shown to exhibit social biases&lt;/a>;. At the same time, compared to monolithic models, CodeVQA offers additional interpretability (through the Python program) and controllability (by modifying the prompts or visual functions), which are useful in production systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was a collaboration between &lt;a href=&quot;https://bair.berkeley.edu/baircommons.html&quot;>;UC Berkeley&#39;s Artificial Intelligence Research lab&lt;/a>; (BAIR) and Google Research, and was conducted by Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;Modular visual question answering via code generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-561916049750681872&lt;/id>;&lt;published>;2023-07-06T12:50:00.000-07:00&lt;/published>;&lt;updated>;2023-07-06T12:50:04.339-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Google Research, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s1100/Pic2Word.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Image retrieval plays a crucial role in search engines. Typically, their users rely on either image or text as a query to retrieve a desired target image. However, text-based retrieval has its limitations, as describing the target image accurately using words can be challenging. For instance, when searching for a fashion item, users may want an item whose specific attribute, eg, the color of a logo or the logo itself, is different from what they find in a website. Yet searching for the item in an existing search engine is not trivial since precisely describing the fashion item by text can be challenging. To address this fact, &lt;a href=&quot;https://arxiv.org/pdf/1812.07119.pdf&quot;>;composed image retrieval&lt;/a>; (CIR) retrieves images based on a query that combines both an image and a text sample that provides instructions on how to modify the image to fit the intended retrieval target. Thus, CIR allows precise retrieval of the target image by combining image and text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, CIR methods require large amounts of labeled data, ie, triplets of a 1) query image, 2) description, and 3) target image. Collecting such labeled data is costly, and models trained on this data are often tailored to a specific use case, limiting their ability to generalize to different datasets. &lt;/p>; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://arxiv.org/abs/2302.03084&quot;>;Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval&lt;/a>;”, we propose a task called zero-shot CIR (ZS-CIR). In ZS-CIR, we aim to build a single CIR model that performs a variety of CIR tasks, such as &lt;a href=&quot;https://www.zheyuanliu.me/CIRR/&quot;>;object composition,&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;attribute editing&lt;/a>;, or domain conversion, without requiring labeled triplet data. Instead, we propose to train a retrieval model using large-scale image-caption pairs and unlabeled images, which are considerably easier to collect than supervised CIR datasets at scale. To encourage reproducibility and further advance this space, we also &lt;a href=&quot;https://github.com/google-research/composed_image_retrieval&quot;>;release the code&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s1062/image9.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;342&quot; data-original-width=&quot;1062&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Description of existing composed image retrieval model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s949/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;476&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCWi9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We train a composed image retrieval model using image-caption data only. Our model retrieves images aligned with the composition of the query image and text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Method overview&lt;/h2>; &lt;p>; We propose to leverage the language capabilities of the language encoder in the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;contrastive language-image pre-trained model&lt;/a>; (CLIP), which excels at generating semantically meaningful language embeddings for a wide range of textual concepts and attributes. To that end, we use a lightweight mapping sub-module in CLIP that is designed to map an input picture (eg, a photo of a cat) from the image embedding space to a word token (eg, “cat”) in the textual input space. The whole network is optimized with the vision-language contrastive loss to again ensure the visual and text embedding spaces are as close as possible given a pair of an image and its textual description. Then, the query image can be treated as if it is a word. This enables the flexible and seamless composition of query image features and text descriptions by the language encoder. We call our method Pic2Word and provide an overview of its training process in the figure below. We want the mapped token &lt;em>;s&lt;/em>; to represent the input image in the form of word token. Then, we train the mapping network to reconstruct the image embedding in the language embedding, &lt;em>;p&lt;/em>;. Specifically, we optimize the contrastive loss proposed in CLIP computed between the visual embedding &lt;em>;v&lt;/em>; and the textual embedding &lt;em>;p&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s611/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;611&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYHXfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training of the mapping network (&lt;em>;f&lt;sub>;M&lt;/sub>;&lt;/em>;) using unlabeled images only. We optimize only the mapping network with a frozen visual and text encoder.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given the trained mapping network, we can regard an image as a word token and pair it with the text description to flexibly compose the joint image-text query as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s827/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;827&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With the trained mapping network, we regard the image as a word token and pair it with the text description to flexibly compose the joint image-text query.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a variety of experiments to evaluate Pic2Word&#39;s performance on a variety of CIR tasks. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Domain conversion&lt;/h3>; &lt;p>; We first evaluate the capability of compositionality of the proposed method on domain conversion — given an image and the desired new image domain (eg, sculpture, origami, cartoon, toy), the output of the system should be an image with the same content but in the new desired image domain or style. As illustrated below, we evaluate the ability to compose the category information and domain description given as an image and text, respectively. We evaluate the conversion from real images to four domains using &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>;. &lt;/p>; &lt;p>; To compare with approaches that do not require supervised training data, we pick three approaches: (i) &lt;em>;image only&lt;/em>; performs retrieval only with visual embedding, (ii) &lt;em>;text only &lt;/em>;employs only text embedding, and (iii) &lt;em>;image + text &lt;/em>;averages the visual and text embedding to compose the query. The comparison with (iii) shows the importance of composing image and text using a language encoder. We also compare with &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;Combiner&lt;/a>;, which trains the CIR model on &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRR&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s965/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;513&quot; data-original-width=&quot;965&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We aim to convert the domain of the input query image into the one described with text, eg, origami.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in figure below, our proposed approach outperforms baselines by a large margin. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s754/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;754&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for domain conversion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Fashion attribute composition&lt;/h3>; &lt;p>; Next, we evaluate the composition of fashion attributes, such as the color of cloth, logo, and length of sleeve, using the &lt;a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; dataset. The figure below illustrates the desired output given the query. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s818/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;309&quot; data-original-width=&quot;818&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of CIR for fashion attributes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the figure below, we present a comparison with baselines, including supervised baselines that utilized triplets for training the CIR model: (i) &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;CB&lt;/a>; uses the same architecture as our approach, (ii) &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRPLANT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2203.08101.pdf&quot;>;ALTEMIS&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2007.00145.pdf&quot;>;MAAF&lt;/a>; use a smaller backbone, such as ResNet50. Comparison to these approaches will give us the understanding on how well our zero-shot approach performs on this task. &lt;/p>; &lt;p>; Although CB outperforms our approach, our method performs better than supervised baselines with smaller backbones. This result suggests that by utilizing a robust CLIP model, we can train a highly effective CIR model without requiring annotated triplets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s807/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;807&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results (&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;recall&lt;/a>;@10, ie, the percentage of relevant instances in the first 10 images retrieved.) on composed image retrieval for Fashion-IQ dataset (higher is better). Light blue bars train the model using triplets. Note that our approach performs on par with these supervised baselines with shallow (smaller) backbones.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Qualitative results&lt;/h3>; &lt;p>; We show several examples in the figure below. Compared to a baseline method that does not require supervised training data (text + image feature averaging), our approach does a better job of correctly retrieving the target image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s1999/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative results on diverse query images and text description.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this article, we introduce Pic2Word, a method for mapping pictures to words for ZS-CIR. We propose to convert the image into a word token to achieve a CIR model using only an image-caption dataset. Through a variety of experiments, we verify the effectiveness of the trained model on diverse CIR tasks, indicating that training on an image-caption dataset can build a powerful CIR model. One potential future research direction is utilizing caption data to train the mapping network, although we use only image data in the present work. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Also thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/561916049750681872/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for.html&quot; rel=&quot;alternate&quot; title=&quot;Pic2Word: Mapping pictures to words for zero-shot composed image retrieval&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s72-c/Pic2Word.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-667019077470952746&lt;/id>;&lt;published>;2023-06-29T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-06-29T14:08:24.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Announcing the first Machine Unlearning Challenge&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Fabian Pedregosa and Eleni Triantafillou, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s900/Unlearning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Deep learning has recently driven tremendous progress in a wide array of applications, ranging from &lt;a href=&quot;https://imagen.research.google/&quot;>;realistic image generation&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;impressive retrieval systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/ai/bard-google-ai-search-updates/&quot;>;language models that can hold human-like conversations&lt;/a>;. While this progress is very exciting, the widespread use of deep neural network models requires caution: as guided by Google&#39;s AI &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Principles&lt;/a>;, we seek to develop AI technologies responsibly by understanding and mitigating potential risks, such as the propagation and amplification of unfair biases and protecting user privacy. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Fully erasing the influence of the data requested to be deleted is challenging since, aside from simply deleting it from databases where it&#39;s stored, it also requires erasing the influence of that data on other artifacts such as trained machine learning models. Moreover, recent research [&lt;a href=&quot;https://arxiv.org/abs/1610.05820&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;2&lt;/a>;] has shown that in some cases it may be possible to infer with high accuracy whether an example was used to train a machine learning model using &lt;a href=&quot;https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction&quot;>;membership inference attacks&lt;/a>; (MIAs). This can raise privacy concerns, as it implies that even if an individual&#39;s data is deleted from a database, it may still be possible to infer whether that individual&#39;s data was used to train a model. &lt;/p>; &lt;p>; Given the above, &lt;em>;machine unlearning&lt;/em>; is an emergent subfield of machine learning that aims to remove the influence of a specific subset of training examples — the &quot;forget set&quot; — from a trained model. Furthermore, an ideal unlearning algorithm would remove the influence of certain examples &lt;em>;while maintaining&lt;/em>; other beneficial properties, such as the accuracy on the rest of the train set and generalization to held-out examples. A straightforward way to produce this unlearned model is to retrain the model on an adjusted training set that excludes the samples from the forget set. However, this is not always a viable option, as retraining deep models can be computationally expensive. An ideal unlearning algorithm would instead use the already-trained model as a starting point and efficiently make adjustments to remove the influence of the requested data. &lt;/p>; &lt;p>; Today we&#39;re thrilled to announce that we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;. The competition considers a realistic scenario in which after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>;, and submissions will be automatically scored in terms of both forgetting quality and model utility. We hope that this competition will help advance the state of the art in machine unlearning and encourage the development of efficient, effective and ethical unlearning algorithms. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Machine unlearning applications&lt;/h2>; &lt;p>; Machine unlearning has applications beyond protecting user privacy. For instance, one can use unlearning to erase inaccurate or outdated information from trained models (eg, due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. &lt;/p>; &lt;p>; The field of machine unlearning is related to other areas of machine learning such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1802.07569&quot;>;life-long learning&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&quot;>;fairness&lt;/a>;. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (eg, demographics, age groups, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Anatomy of unlearning.&lt;/b>; An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the &quot;forget set&quot;). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Challenges of machine unlearning&lt;/h2>; &lt;p>; The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model&#39;s utility (eg, accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while &lt;a href=&quot;https://arxiv.org/abs/2007.02923&quot;>;adding noise&lt;/a>; to the weights achieves forgetting at the expense of utility. &lt;/p>; &lt;p>; Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some &lt;a href=&quot;https://arxiv.org/abs/1911.04933&quot;>;works&lt;/a>; report the classification accuracy on the samples to unlearn, &lt;a href=&quot;https://proceedings.mlr.press/v119/wu20b.html&quot;>;others&lt;/a>; report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [&lt;a href=&quot;https://arxiv.org/abs/2302.09880&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.10981&quot;>;5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.02205&quot;>;6&lt;/a>;]. &lt;/p>; &lt;p>; We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Announcing the first Machine Unlearning Challenge&lt;/h2>; &lt;p>; We are pleased to announce the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;, which will be held as part of the &lt;a href=&quot;https://neurips.cc/Conferences/2023/CompetitionTrack&quot;>;NeurIPS 2023 Competition Track.&lt;/a>; The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities. &lt;/p>; &lt;p>; The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; and run between mid-July 2023 and mid-September 2023. As part of the competition, today we&#39;re announcing the availability of the &lt;a href=&quot;https://github.com/unlearning-challenge/starting-kit&quot;>;starting kit&lt;/a>;. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset. &lt;/p>; &lt;p>; The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we&#39;ll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;2000&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Excerpt images from the &lt;a href=&quot;https://github.com/microsoft/FaceSynthetics&quot;>;Face Synthetics&lt;/a>; dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For evaluating forgetting, we will use tools inspired by MIAs, such as &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;LiRA&lt;/a>;. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be &lt;em>;unable&lt;/em>; to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio CS Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/667019077470952746/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html&quot; rel=&quot;alternate&quot; title=&quot;Announcing the first Machine Unlearning Challenge&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s72-c/Unlearning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9161751123508054082&lt;/id>;&lt;published>;2023-06-29T12:10:00.004-07:00&lt;/published>;&lt;updated>;2023-07-18T14:55:41.583-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device diffusion plugins for conditioned text-to-image generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao and Tingbo Hou, Software Engineers, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s800/MediaPipeDiffusion-hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown great success in text-to-image generation, achieving high image quality, improved inference performance, and expanding our creative inspiration. Nevertheless, it is still challenging to efficiently control the generation, especially with conditions that are difficult to describe with text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we announce &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; diffusion plugins, which enable controllable text-to-image generation to be run on-device. Expanding upon our &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;prior work&lt;/a>; on GPU inference for on-device large generative models, we introduce new low-cost solutions for controllable text-to-image generation that can be plugged into existing diffusion models and their Low-Rank Adaptation (&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;LoRA&lt;/a>;) variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s1080/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Text-to-image generation with control plugins running on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Background&lt;/h2>; &lt;p>; With diffusion models, image generation is modeled as an iterative denoising process. Starting from a noise image, at each step, the diffusion model gradually denoises the image to reveal an image of the target concept. Research shows that leveraging language understanding via text prompts can greatly improve image generation. For text-to-image generation, the text embedding is connected to the model via cross-attention layers. Yet, some information is difficult to describe by text prompts, eg, the position and pose of an object. To address this problem, researchers add additional models into the diffusion to inject control information from a condition image. &lt;/p>; &lt;p>; Common approaches for controlled text-to-image generation include &lt;a href=&quot;https://arxiv.org/abs/2211.12572&quot;>;Plug-and-Play&lt;/a>;, &lt;a href=&quot;https://github.com/lllyasviel/ControlNet&quot;>;ControlNet&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.08453&quot;>;T2I Adapter&lt;/a>;. Plug-and-Play applies a widely used denoising diffusion implicit model (&lt;a href=&quot;https://arxiv.org/abs/2010.02502&quot;>;DDIM&lt;/a>;) inversion approach that reverses the generation process starting from an input image to derive an initial noise input, and then employs a copy of the diffusion model (860M parameters for Stable Diffusion 1.5) to encode the condition from an input image. Plug-and-Play extracts spatial features with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>; from the copied diffusion, and injects them into the text-to-image diffusion. ControlNet creates a trainable copy of the encoder of a diffusion model, which connects via a convolution layer with zero-initialized parameters to encode conditioning information that is conveyed to the decoder layers. However, as a result, the size is large, half that of the diffusion model (430M parameters for Stable Diffusion 1.5). T2I Adapter is a smaller network (77M parameters) and achieves similar effects in controllable generation. T2I Adapter only takes the condition image as input, and its output is shared across all diffusion iterations. Yet, the adapter model is not designed for portable devices. &lt;/p>; &lt;br />; &lt;h2>;The MediaPipe diffusion plugins&lt;/h2>; &lt;p>; To make conditioned generation efficient, customizable, and scalable, we design the MediaPipe diffusion plugin as a separate network that is: &lt;/p>; &lt;ul>; &lt;li>;&lt;i>;Plugable&lt;/i>;: It can be easily connected to a pre-trained base model. &lt;/li>;&lt;li>;&lt;i>;Trained from scratch&lt;/i>;: It does not use pre-trained weights from the base model. &lt;/li>;&lt;li>;&lt;i>;Portable&lt;/i>;: It runs outside the base model on mobile devices, with negligible cost compared to the base model inference. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Parameter Size&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Plugable&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;From Scratch&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Portable&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Plug-and-Play &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;860M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;430M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;T2I Adapter &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;77M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Plug-and-Play, ControlNet, T2I Adapter, and the MediaPipe diffusion plugin.&lt;br />;* The number varies depending on the particulars of the diffusion model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The MediaPipe diffusion plugin is a portable on-device model for text-to-image generation. It extracts multiscale features from a conditioning image, which are added to the encoder of a diffusion model at corresponding levels. When connecting to a text-to-image diffusion model, the plugin model can provide an extra conditioning signal to the image generation. We design the plugin network to be a lightweight model with only 6M parameters. It uses depth-wise convolutions and inverted bottlenecks from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetv2&lt;/a>; for fast inference on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s1724/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;628&quot; data-original-width=&quot;1724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the MediaPipe diffusion model plugin. The plugin is a separate network, whose output can be plugged into a pre-trained text-to-image generation model. Features extracted by the plugin are applied to the associated downsampling layer of the diffusion model (blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Unlike ControlNet, we inject the same control features in all diffusion iterations. That is, we only run the plugin once for one image generation, which saves computation. We illustrate some intermediate results of a diffusion process below. The control is effective at every diffusion step and enables controlled generation even at early steps. More iterations improve the alignment of the image with the text prompt and generate more detail. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s1280/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the generation process using the MediaPipe diffusion plugin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Examples&lt;/h2>; &lt;p>; In this work, we developed plugins for a diffusion-based text-to-image generation model with MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmark&lt;/a>;, MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmark&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_map&quot;>;depth maps&lt;/a>;, and &lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot;>;Canny edge&lt;/a>;. For each task, we select about 100K images from a &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;web-scale image-text dataset&lt;/a>;, and compute control signals using corresponding MediaPipe solutions. We use refined captions from &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; for training the plugins. &lt;/p>; &lt;br />; &lt;h3>;Face Landmark&lt;/h3>; &lt;p>; The MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmarker&lt;/a>; task computes 478 landmarks (with attention) of a human face. We use the &lt;a href=&quot;https://github.com/google/mediapipe/blob/26a7ca5c64cd885978677931a7218d33cd7d1dec/mediapipe/python/solutions/drawing_utils.py&quot;>;drawing utils&lt;/a>; in MediaPipe to render a face, including face contour, mouth, eyes, eyebrows, and irises, with different colors. The following table shows randomly generated samples by conditioning on face mesh and prompts. As a comparison, both ControlNet and Plugin can control text-to-image generation with given conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s1346/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1018&quot; data-original-width=&quot;1346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Face-landmark plugin for text-to-image generation, compared with ControlNet.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Holistic Landmark&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmarker&lt;/a>; task includes landmarks of body pose, hands, and face mesh. Below, we generate various stylized images by conditioning on the holistic features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s1351/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;970&quot; data-original-width=&quot;1351&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Holistic-landmark plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Depth&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s1344/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depth-plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Canny Edge&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s1345/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;955&quot; data-original-width=&quot;1345&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Canny-edge plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a quantitative study of the &lt;em>;face landmark&lt;/em>; plugin to demonstrate the model&#39;s performance. The evaluation dataset contains 5K human images. We compare the generation quality as measured by the widely used metrics, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;>;Fréchet Inception Distance&lt;/a>; (FID) and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; scores. The base model is a pre-trained text-to-image diffusion model. We use &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>; v1.5 here. &lt;/p>; &lt;p>; As shown in the following table, both ControlNet and the MediaPipe diffusion plugin produce much better sample quality than the base model, in terms of FID and CLIP scores. Unlike ControlNet, which needs to run at every diffusion step, the MediaPipe plugin only runs once for each image generated. We measured the performance of the three models on a server machine (with Nvidia V100 GPU) and a mobile phone (Galaxy S23). On the server, we run all three models with 50 diffusion steps, and on mobile, we run 20 diffusion steps using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/image_generator&quot;>;MediaPipe image generation app&lt;/a>;. Compared with ControlNet, the MediaPipe plugin shows a clear advantage in inference efficiency while preserving the sample quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Model&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;FID↓&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;CLIP↑&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Inference Time (s)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Nvidia V100&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;10.32 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.26 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.5 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.51 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.31 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.4 (+48%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;18.2 (+58.3%) &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.30 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 (+0.2%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.8 (+2.6%) &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quantitative comparison on FID, CLIP, and inference time.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We test the performance of the plugin on a wide range of mobile devices from mid-tier to high-end. We list the results on some representative devices in the following table, covering both Android and iOS. &lt;/p>; &lt;p>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Device&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;7&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Android&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;iOS&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 4&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 6&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 7&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 12 Pro&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 13 Pro&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;strong>;Time&lt;/strong>; (ms) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;128 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;68 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;48 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;73 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;63 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Inference time (ms) of the plugin on different mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we present MediaPipe, a portable plugin for conditioned text-to-image generation. It injects features extracted from a condition image to a diffusion model, and consequently controls the image generation. Portable plugins can be connected to pre-trained diffusion models running on servers or devices. By running text-to-image generation and plugins fully on-device, we enable more flexible applications of generative AI. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank all team members who contributed to this work: Raman Sarokin and Juhyun Lee for the GPU inference solution; Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann for leadership. Special thanks to Jiuqiang Tang&lt;/em>;&lt;i>;, Joe Zou and Lu wang,&lt;/i>;&lt;em>;&amp;nbsp;who made this technology and all the demos running on-device.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9161751123508054082/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html&quot; rel=&quot;alternate&quot; title=&quot;On-device diffusion plugins for conditioned text-to-image generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s72-c/MediaPipeDiffusion-hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6036162561497757163&lt;/id>;&lt;published>;2023-06-27T14:19:00.000-07:00&lt;/published>;&lt;updated>;2023-06-27T14:19:04.029-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unifying image-caption and image-classification datasets with prefix conditioning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s320/Prefix%20conditioning%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Pre-training visual language (VL) models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pre-training on image classification data. Image-caption datasets are considered to be more “open-domain” because they contain broader scene types and vocabulary words, which result in models with &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;strong performance in few- and zero-shot recognition tasks&lt;/a>;. However, images with fine-grained class descriptions can be rare, and the class distribution can be imbalanced since image-caption datasets do not go through manual curation. By contrast, large-scale classification datasets, such as &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>;, are often curated and can thus provide fine-grained categories with a balanced label distribution. While it may sound promising, directly combining caption and classification datasets for pre-training is often unsuccessful as it can result in biased representations that do not generalize well to various downstream tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2206.01125&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we demonstrate a pre-training strategy that uses both classification and caption datasets to provide complementary benefits. First, we show that naïvely unifying the datasets results in sub-optimal performance on downstream zero-shot recognition tasks as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem during training through &lt;em>;prefix conditioning&lt;/em>;, a novel simple and effective method that uses prefix tokens to disentangle dataset biases from visual concepts. This approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Prefix conditioning is a generic method that can be easily integrated into existing VL pre-training objectives, such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;Contrastive Language-Image Pre-training&lt;/a>; (CLIP) or &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;Unified Contrastive Learning&lt;/a>; (UniCL). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High-level idea&lt;/h2>; &lt;p>; We note that classification datasets tend to be biased in at least two ways: (1) the images mostly contain single objects from restricted domains, and (2) the vocabulary is limited and lacks the linguistic flexibility required for zero-shot learning. For example, the class embedding of “a photo of a dog” optimized for ImageNet usually results in a photo of one dog in the center of the image pulled from the ImageNet dataset, which does not generalize well to other datasets containing images of multiple dogs in different spatial locations or a dog with other subjects. &lt;/p>; &lt;p>; By contrast, caption datasets contain a wider variety of scene types and vocabularies. As shown below, if a model simply learns from two datasets, the language embedding can entangle the bias from the image classification and caption dataset, which can decrease the generalization in zero-shot classification. If we can disentangle the bias from two datasets, we can use language embeddings that are tailored for the caption dataset to improve generalization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/s1150/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1150&quot; data-original-width=&quot;856&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/w476-h640/image1.png&quot; width=&quot;476&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Top&lt;/strong>;: Language embedding entangling the bias from image classification and caption dataset. &lt;strong>;Bottom&lt;/strong>;: Language embeddings disentangles the bias from two datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prefix conditioning&lt;/h2>; &lt;p>; Prefix conditioning is partially inspired by &lt;a href=&quot;https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html&quot;>;prompt tuning&lt;/a>;, which prepends learnable tokens to the input token sequences to instruct a pre-trained model backbone to learn task-specific knowledge that can be used to solve downstream tasks. The prefix conditioning approach differs from prompt tuning in two ways: (1) it is designed to unify image-caption and classification datasets by disentangling the dataset bias, and (2) it is applied to VL pre-training while the standard prompt tuning is used to fine-tune models. Prefix conditioning is an explicit way to specifically steer the behavior of model backbones based on the type of datasets provided by users. This is especially helpful in production when the number of different types of datasets is known ahead of time. &lt;/p>; &lt;p>; During training, prefix conditioning learns a text token (prefix token) for each dataset type, which absorbs the bias of the dataset and allows the remaining text tokens to focus on learning visual concepts. Specifically, it prepends prefix tokens for each dataset type to the input tokens that inform the language and visual encoder of the input data type (eg, classification vs. caption). Prefix tokens are trained to learn the dataset-type-specific bias, which enables us to disentangle that bias in language representations and utilize the embedding learned on the image-caption dataset during test time, even without an input caption. &lt;/p>; &lt;p>; We utilize prefix conditioning for CLIP using a language and visual encoder. During test time, we employ the prefix used for the image-caption dataset since the dataset is supposed to cover broader scene types and vocabulary words, leading to better performance in zero-shot recognition. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s1038/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;1038&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the Prefix Conditioning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We apply prefix conditioning to two types of contrastive loss, &lt;a href=&quot;http://proceedings.mlr.press/v139/radford21a&quot;>;CLIP&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf&quot;>;UniCL,&lt;/a>; and evaluate their performance on zero-shot recognition tasks compared to models trained with &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet21K&lt;/a>; (IN21K) and &lt;a href=&quot;https://github.com/google-research-datasets/conceptual-12m&quot;>;Conceptual 12M&lt;/a>; (CC12M). CLIP and UniCL models trained with two datasets using prefix conditioning show large improvements in zero-shot classification accuracy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s440/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Zero-shot classification accuracy of models trained with only IN21K or CC12M compared to CLIP and UniCL models trained with both two datasets using prefix conditioning (“Ours”). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on test-time prefix&lt;/h3>; &lt;p>; The table below describes the performance change by the prefix used during test time. We demonstrate that by using the same prefix used for the classification dataset (“Prompt”), the performance on the classification dataset (&lt;a href=&quot;https://www.image-net.org/&quot;>;IN-1K&lt;/a>;) improves. When using the same prefix used for the image-caption dataset (“Caption”), the performance on other datasets (Zero-shot AVG) improves. This analysis illustrates that if the prefix is tailored for the image-caption dataset, it achieves better generalization of scene types and vocabulary words. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/w640-h396/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis of the prefix used for test-time. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on robustness to image distribution shift &lt;/h3>; &lt;p>; We study the shift in image distribution using ImageNet variants. We see that the “Caption” prefix performs better than “Prompt” in &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>; (IN-R) and &lt;a href=&quot;https://github.com/HaohanWang/ImageNet-Sketch&quot;>;ImageNet-Sketch&lt;/a>; (IN-S), but underperforms in &lt;a href=&quot;https://github.com/modestyachts/ImageNetV2&quot;>;ImageNet-V2&lt;/a>; (IN-V2). This indicates that the “Caption” prefix achieves generalization on domains far from the classification dataset. Therefore, the optimal prefix probably differs by how far the test domain is from the classification dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis on the robustness to image-level distribution shift. IN: ImageNet, IN-V2: ImageNet-V2, IN-R: Art, Cartoon style ImageNet, IN-S: ImageNet Sketch. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce prefix conditioning, a technique for unifying image caption and classification datasets for better zero-shot classification. We show that this approach leads to better zero-shot classification accuracy and that the prefix can control the bias in the language embedding. One limitation is that the prefix learned on the caption dataset is not necessarily optimal for the zero-shot classification. Identifying the optimal prefix for each test dataset is an interesting direction for future work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6036162561497757163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html&quot; rel=&quot;alternate&quot; title=&quot;Unifying image-caption and image-classification datasets with prefix conditioning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s72-c/Prefix%20conditioning%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8471212041038116532&lt;/id>;&lt;published>;2023-06-23T12:24:00.001-07:00&lt;/published>;&lt;updated>;2023-06-23T12:34:21.337-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Preference learning with automated feedback for cache eviction&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ramki Gummadi, Software Engineer, Google and Kevin Chen, Software Engineer, YouTube&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s320/Halp%20hero%20gif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Caching is a ubiquitous idea in computer science that significantly improves the performance of storage and retrieval systems by storing a subset of popular items closer to the client based on request patterns. An important algorithmic piece of cache management is the decision policy used for dynamically updating the set of items being stored, which has been extensively optimized over several decades, resulting in several &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;>;efficient and robust heuristics&lt;/a>;. While applying machine learning to cache policies has shown promising results in recent years (eg, &lt;a href=&quot;https://www.usenix.org/conference/nsdi20/presentation/song&quot;>;LRB&lt;/a>;, &lt;a href=&quot;https://www.usenix.org/system/files/conference/nsdi18/nsdi18-beckmann.pdf&quot;>;LHD&lt;/a>;, &lt;a href=&quot;https://www.pdl.cmu.edu/PDL-FTP/Storage/MLSys2021-zhou.pdf&quot;>;storage applications&lt;/a>;), it remains a challenge to outperform robust heuristics in a way that can generalize reliably beyond benchmarks to production settings, while maintaining competitive compute and memory overheads. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c9ebaf640152028a6cdedb684577a7c9e52b6f10.pdf&quot;>;HALP: Heuristic Aided Learned Preference Eviction Policy for YouTube Content Delivery Network&lt;/a>;”, presented at &lt;a href=&quot;https://www.usenix.org/conference/nsdi23&quot;>;NSDI 2023&lt;/a>;, we introduce a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. The Heuristic Aided Learned Preference (HALP) framework is a meta-algorithm that uses randomization to merge a lightweight heuristic baseline eviction rule with a learned reward model. The reward model is a lightweight neural network that is continuously trained with ongoing automated feedback on preference comparisons designed to mimic the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;offline oracle&lt;/a>;. We discuss how HALP has improved infrastructure efficiency and user video playback latency for YouTube&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Content_delivery_network&quot;>;content delivery network&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learned preferences for cache eviction decisions&lt;/h2>; &lt;p>; The HALP framework computes cache eviction decisions based on two components: (1) a neural reward model trained with automated feedback via preference learning, and (2) a meta-algorithm that combines a learned reward model with a fast heuristic. As the cache observes incoming requests, HALP continuously trains a small neural network that predicts a scalar reward for each item by formulating this as a preference learning method via &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_to_rank#Pairwise_approach&quot;>;pairwise&lt;/a>; preference feedback. This aspect of HALP is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&quot;>;reinforcement learning from human feedback&lt;/a>; (RLHF) systems, but with two important distinctions: &lt;/p>; &lt;ul>; &lt;li>;Feedback is &lt;em>;automated&lt;/em>; and leverages well-known results about the structure of offline &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;optimal&lt;/a>; cache eviction policies. &lt;/li>;&lt;li>;The model is &lt;em>;learned continuously&lt;/em>; using a transient buffer of training examples constructed from the automated feedback process. &lt;/li>; &lt;/ul>; &lt;p>; The eviction decisions rely on a filtering mechanism with two steps. First, a small subset of candidates is selected using a heuristic that is efficient, but suboptimal in terms of performance. Then, a re-ranking step optimizes from within the baseline candidates via the sparing use of a neural network scoring function to “boost” the quality of the final decision. &lt;/p>; &lt;p>; As a production ready cache policy implementation, HALP not only makes eviction decisions, but also subsumes the end-to-end process of sampling pairwise preference queries used to efficiently construct relevant feedback and update the model to power eviction decisions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A neural reward model&lt;/h2>; &lt;p>; HALP uses a light-weight two-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) as its reward model to selectively score individual items in the cache. The features are constructed and managed as a metadata-only “ghost cache” (similar to classical policies like &lt;a href=&quot;https://en.wikipedia.org/wiki/Adaptive_replacement_cache&quot;>;ARC&lt;/a>;). After any given lookup request, in addition to regular cache operations, HALP conducts the book-keeping (eg, tracking and updating feature metadata in a capacity-constrained key-value store) needed to update the dynamic internal representation. This includes: (1) externally tagged features provided by the user as input, along with a cache lookup request, and (2) internally constructed dynamic features (eg, time since last access, average time between accesses) constructed from lookup times observed on each item. &lt;/p>; &lt;p>; HALP learns its reward model fully online starting from a random weight initialization. This might seem like a bad idea, especially if the decisions are made exclusively for optimizing the reward model. However, the eviction decisions rely on both the learned reward model and a suboptimal but simple and robust heuristic like &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU&quot;>;LRU&lt;/a>;. This allows for optimal performance when the reward model has fully generalized, while remaining robust to a temporarily uninformative reward model that is yet to generalize, or in the process of catching up to a changing environment. &lt;/p>; &lt;p>; Another advantage of online training is specialization. Each cache server runs in a potentially different environment (eg, geographic location), which influences local network conditions and what content is locally popular, among other things. Online training automatically captures this information while reducing the burden of generalization, as opposed to a single offline training solution.&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scoring samples from a randomized priority queue &lt;/h2>; &lt;p>; It can be impractical to optimize for the quality of eviction decisions with an exclusively learned objective for two reasons. &lt;/p>; &lt;ol>; &lt;li>;Compute efficiency constraints: Inference with a learned network can be significantly more expensive than the computations performed in practical cache policies operating at scale. This limits not only the expressivity of the network and features, but also how often these are invoked during each eviction decision. &lt;/li>;&lt;li>;Robustness for generalizing out-of-distribution: HALP is deployed in a setup that involves continual learning, where a quickly changing workload might generate request patterns that might be temporarily out-of-distribution with respect to previously seen data. &lt;/li>; &lt;/ol>; &lt;p>; To address these issues, HALP first applies an inexpensive heuristic scoring rule that corresponds to an eviction priority to identify a small candidate sample. This process is based on efficient random sampling that &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;approximates&lt;/a>; exact &lt;a href=&quot;https://en.wikipedia.org/wiki/Priority_queue&quot;>;priority queues&lt;/a>;. The priority function for generating candidate samples is intended to be quick to compute using existing manually-tuned algorithms, eg, LRU. However, this is configurable to approximate other cache replacement heuristics by editing a simple cost function. Unlike &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;prior work&lt;/a>;, where the randomization was used to tradeoff approximation for efficiency, HALP also &lt;em>;relies&lt;/em>; on the inherent randomization in the sampled candidates across time steps for providing the necessary &lt;a href=&quot;https://lilianweng.github.io/posts/2020-06-07-exploration-drl/&quot;>;exploratory&lt;/a>; diversity in the sampled candidates for both training and inference. &lt;/p>; &lt;p>; The final evicted item is chosen from among the supplied candidates, equivalent to the &lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;>;best-of-n&lt;/a>; reranked sample, corresponding to maximizing the predicted&lt;em>; &lt;/em>;preference score according to the neural reward model. The same pool of candidates used for eviction decisions is also used to construct the pairwise preference queries for automated feedback, which helps minimize the training and inference skew between samples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s1470/Halp.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s16000/Halp.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the two-stage process invoked for each eviction decision.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Online preference learning with automated feedback&lt;/h2>; &lt;p>; The reward model is learned using online feedback, which is based on automatically assigned preference labels that indicate, wherever feasible, the ranked preference ordering for the time taken to receive future re-accesses, starting from a given snapshot in time among each queried sample of items. This is similar to the oracle optimal policy, which, at any given time, evicts an item with the farthest future access from all the items in the cache. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generation of the automated feedback for learning the reward model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To make this feedback process informative, HALP constructs pairwise preference queries that are most likely to be relevant for eviction decisions. In sync with the usual cache operations, HALP issues a small number of pairwise preference queries while making each eviction decision, and appends them to a set of &lt;em>;pending comparisons. &lt;/em>;The labels for these pending comparisons can only be resolved at a random future time. To operate online, HALP also performs some additional book-keeping after each lookup request to process any pending comparisons that can be labeled incrementally after the current request. HALP indexes the pending comparison buffer with each element involved in the comparison, and recycles the memory consumed by stale comparisons (neither of which may ever get a re-access) to ensure that the memory overhead associated with feedback generation remains bounded over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/s1468/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1032&quot; data-original-width=&quot;1468&quot; height=&quot;450&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/w640-h450/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of all main components in HALP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results: Impact on the YouTube CDN&lt;/h2>; &lt;p>; Through empirical analysis, we show that HALP compares favorably to state-of-the-art cache policies on public benchmark traces in terms of cache miss rates. However, while public benchmarks are a useful tool, they are rarely sufficient to capture all the usage patterns across the world over time, not to mention the diverse hardware configurations that we have already deployed. &lt;/p>; &lt;p>; Until recently, YouTube servers used an optimized LRU-variant for memory cache eviction. HALP increases YouTube&#39;s memory egress/ingress — the ratio of the total bandwidth egress served by the CDN to that consumed for retrieval (ingress) due to cache misses — by roughly 12% and memory hit rate by 6%. This reduces latency for users, since memory reads are faster than disk reads, and also improves egressing capacity for disk-bounded machines by shielding the disks from traffic. &lt;/p>; &lt;p>; The figure below shows a visually compelling reduction in the &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usits97/full_papers/cao/cao_html/node12.html&quot;>;byte miss ratio&lt;/a>; in the days following HALP&#39;s final rollout on the YouTube CDN, which is now serving significantly more content from within the cache with lower latency to the end user, and without having to resort to more expensive retrieval that increases the operating costs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s505/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;257&quot; data-original-width=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Aggregate worldwide YouTube byte miss ratio before and after rollout (vertical dashed line).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An aggregated performance improvement could still hide important regressions. In addition to measuring overall impact, we also conduct an analysis in the paper to understand its impact on different racks using a machine level analysis, and find it to be overwhelmingly positive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduced a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. Because of its design choices, HALP can be deployed in a manner similar to any other cache policy without the operational overhead of having to separately manage the labeled examples, training procedure and the model versions as additional offline pipelines common to most machine learning systems. Therefore, it incurs only a small extra overhead compared to other classical algorithms, but has the added benefit of being able to take advantage of additional features to make its eviction decisions and continuously adapt to changing access patterns. &lt;/p>; &lt;p>; This is the first large-scale deployment of a learned cache policy to a widely used and heavily trafficked CDN, and has significantly improved the CDN infrastructure efficiency while also delivering a better quality of experience to users. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Ramki Gummadi is now part of Google DeepMind. We would like to thank John Guilyard for help with the illustrations and Richard Schooler for feedback on this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8471212041038116532/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html&quot; rel=&quot;alternate&quot; title=&quot;Preference learning with automated feedback for cache eviction&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s72-c/Halp%20hero%20gif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3798161588273442525&lt;/id>;&lt;published>;2023-06-22T11:33:00.000-07:00&lt;/published>;&lt;updated>;2023-06-22T11:33:53.578-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Audio&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SoundStorm: Efficient parallel audio generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zalán Borsos, Research Software Engineer, and Marco Tagliasacchi, Senior Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s2400/SoundStorm.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The recent progress in generative AI unlocked the possibility of creating new content in several different domains, including text, vision and audio. These models often rely on the fact that raw data is first converted to a compressed format as a sequence of tokens. In the case of audio, neural audio codecs (eg, &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;SoundStream&lt;/a>; or &lt;a href=&quot;https://github.com/facebookresearch/encodec&quot;>;EnCodec&lt;/a>;) can efficiently compress waveforms to a compact representation, which can be inverted to reconstruct an approximation of the original audio signal. Such a representation consists of a sequence of discrete audio tokens, capturing the local properties of sounds (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Phoneme&quot;>;phonemes&lt;/a>;) and their temporal structure (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;>;prosody&lt;/a>;). By representing audio as a sequence of discrete tokens, audio generation can be performed with &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based sequence-to-sequence models — this has unlocked rapid progress in speech continuation (eg, with &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;), text-to-speech (eg, with &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;), and general audio and music generation (eg, &lt;a href=&quot;https://felixkreuk.github.io/audiogen/&quot;>;AudioGen&lt;/a>; and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;). Many generative audio models, including AudioLM, rely on auto-regressive decoding, which produces tokens one by one. While this method achieves high acoustic quality, inference (ie, calculating an output) can be slow, especially when decoding long sequences. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To address this issue, in “&lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm: Efficient Parallel Audio Generation&lt;/a>;”, we propose a new method for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on two novel elements: 1) an architecture adapted to the specific nature of audio tokens as produced by the SoundStream neural codec, and 2) a decoding scheme inspired by &lt;a href=&quot;https://arxiv.org/abs/2202.04200&quot;>;MaskGIT&lt;/a>;, a recently proposed method for image generation, which is tailored to operate on audio tokens. Compared to the autoregressive decoding approach of AudioLM, SoundStorm is able to generate tokens in parallel, thus decreasing the inference time by 100x for long sequences, and produces audio of the same quality and with higher consistency in voice and acoustic conditions. Moreover, we show that SoundStorm, coupled with the text-to-semantic modeling stage of &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;, can synthesize high-quality, natural dialogues, allowing one to control the spoken content (via transcripts), speaker voices (via short voice prompts) and speaker turns (via transcript annotations), as demonstrated by the examples below: &lt;/p>; &lt;br>; &lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/SoundStorm_promo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;br>; &lt;br>; &lt;table>; &lt;tr>; &lt;td>;&lt;b>;Input: Text&lt;em>; (transcript used to drive the audio generation in bold)&lt;/em>;&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;Something really funny happened to me this morning. |哦，哇，什么？ | &lt;b>;Well, uh I woke up as usual. |呃呃|下楼去吃早餐。 |是啊|开始吃饭了。然后呃 10 分钟后我意识到现在是半夜了。 | Oh no way, that&#39;s so funny!&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;I didn&#39;t sleep well last night. |不好了。发生了什么？ | &lt;b>;I don&#39;t know.我就是无法入睡，整个晚上我都翻来覆去。 |这太糟糕了。也许你今晚应该尝试早点睡觉，或者你可以尝试读书。 |是的，谢谢你的建议，我希望你是对的。 |没问题。 II hope you get a good night&#39;s sleep&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Input: Audio prompt&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Output: Audio prompt + generated audio&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;SoundStorm design&lt;/h2>; &lt;p>; In our previous work on AudioLM, we showed that audio generation can be decomposed into two steps: 1) semantic modeling, which generates semantic tokens from either previous semantic tokens or a conditioning signal (eg, a transcript as in SPEAR-TTS, or a text prompt as in MusicLM), and 2) acoustic modeling, which generates acoustic tokens from semantic tokens.通过 SoundStorm，我们专门解决了第二个声学建模步骤，用更快的并行解码取代了较慢的自回归解码。 &lt;/p>; &lt;p>; SoundStorm relies on a bidirectional attention-based &lt;a href=&quot;https://arxiv.org/abs/2005.08100&quot;>;Conformer&lt;/a>;, a model architecture that combines a Transformer with convolutions to capture both local and global structure of a sequence of tokens. Specifically, the model is trained to predict audio tokens produced by SoundStream given a sequence of semantic tokens generated by AudioLM as input. When doing this, it is important to take into account the fact that, at each time step &lt;em>;t&lt;/em>;, SoundStream uses up to &lt;em>;Q&lt;/em>; tokens to represent the audio using a method known as &lt;em>;residual vector quantization&lt;/em>; (RVQ), as illustrated below on the right. The key intuition is that the quality of the reconstructed audio progressively increases as the number of generated tokens at each step goes from 1 to &lt;em>;Q&lt;/em>;. &lt;/p>; &lt;p>; At inference time, given the semantic tokens as input conditioning signal, SoundStorm starts with all audio tokens masked out, and fills in the masked tokens over multiple iterations, starting from the coarse tokens at RVQ level &lt;em>;q = 1&lt;/em>; and proceeding level-by-level with finer tokens until reaching level &lt;em>;q = Q&lt;/em>;. &lt;/p>; &lt;p>; There are two crucial aspects of SoundStorm that enable fast generation: 1) tokens are predicted in parallel during a single iteration within a RVQ level and, 2) the model architecture is designed in such a way that the complexity is only mildly affected by the number of levels &lt;em>;Q&lt;/em>;. To support this inference scheme, during training a carefully designed masking scheme is used to mimic the iterative process used at inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;951&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SoundStorm model architecture. &lt;em>;T&lt;/em>; denotes the number of time steps and &lt;em>;Q&lt;/em>; the number of RVQ levels used by SoundStream. The semantic tokens used as conditioning are time-aligned with the SoundStream frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Measuring SoundStorm performance&lt;/h2>; &lt;p>; We demonstrate that SoundStorm matches the quality of AudioLM&#39;s acoustic generator, replacing both AudioLM&#39;s stage two (coarse acoustic model) and stage three (fine acoustic model).此外，SoundStorm 生成音频的速度比 AudioLM 的分层自回归声音发生器（下图上半部分）快 100 倍，并且在说话者身份和声学条件方面具有匹配的质量和更高的一致性（下图下半部分）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1122&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Runtimes of SoundStream decoding, SoundStorm and different stages of AudioLM on a TPU-v4.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s1312/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1312&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Acoustic consistency between the prompt and the generated audio. The shaded area represents the inter-quartile range.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Safety and risk mitigation&lt;/h2>; &lt;p>; We acknowledge that the audio samples produced by the model may be influenced by the unfair biases present in the training data, for instance in terms of represented accents and voice characteristics. In our generated samples, we demonstrate that we can reliably and responsibly control speaker characteristics via prompting, with the goal of avoiding unfair biases. A thorough analysis of any training data and its limitations is an area of future work in line with our responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. &lt;/p>; &lt;p>; In turn, the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification and using the model for the purpose of impersonation.因此，制定防止潜在滥用的保护措施至关重要：为此，我们已经验证了 SoundStorm 生成的音频仍然可以被专用分类器检测到，该专用分类器使用与我们原始 AudioLM 论文中描述的相同分类器。因此，作为一个更大系统的组件，我们相信 SoundStorm 不太可能给我们之前关于 AudioLM 和 SPEAR-TTS 的论文中讨论的风险带来额外的风险。与此同时，放宽 AudioLM 的内存和计算要求将使音频生成领域的研究更容易被更广泛的社区所接受。 In the future, we plan to explore other approaches for detecting synthesized speech, eg, with the help of audio watermarking, so that any potential product usage of this technology strictly follows our responsible AI Principles. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have introduced SoundStorm, a model that can efficiently synthesize high-quality audio from discrete conditioning tokens. When compared to the acoustic generator of &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;, SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>; with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker turns, controlling both the voices of the speakers and the generated content. SoundStorm is not limited to generating speech. For example, MusicLM uses SoundStorm to synthesize longer outputs efficiently (&lt;a href=&quot;https://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot;>;as seen at I/O&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The work described here was authored by Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour and Marco Tagliasacchi. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3798161588273442525/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html&quot; rel=&quot;alternate&quot; title=&quot;SoundStorm: Efficient parallel audio generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s72-c/SoundStorm.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3542229559799425219&lt;/id>;&lt;published>;2023-06-21T13:57:00.004-07:00&lt;/published>;&lt;updated>;2023-06-22T11:02:35.392-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Automatic Speech Recognition&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: AI for Social Good&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jimmy Tobin and Katrin Tomanek, Software Engineers, Google Research, AI for Social Good&lt;/span>; &lt;p>; Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>; team consists of researchers, engineers, volunteers, and others with a shared focus on positive social impact. Our mission is to demonstrate AI&#39;s societal benefit by enabling real-world value, with projects spanning work in &lt;a href=&quot;https://blog.google/technology/health/making-data-useful-public-health/&quot;>;public health&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/making-android-more-accessible/&quot;>;accessibility&lt;/a>;, &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;crisis response&lt;/a>;, &lt;a href=&quot;https://sustainability.google/&quot;>;climate and energy,&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;nature and society&lt;/a>;.我们相信，在服务不足的社区推动积极变革的最佳方式是与变革者及其服务的组织合作。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post we discuss work done by &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, a team within &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>;, that aims to &lt;a href=&quot;https://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html&quot;>;improve automatic speech recognition&lt;/a>; (ASR) for people with disordered speech. For people with typical speech, an ASR model&#39;s word error rate (WER) can be less than 10%. But for people with disordered speech patterns, such as stuttering, &lt;a href=&quot;https://www.mayoclinic.org/diseases-conditions/dysarthria/symptoms-causes/syc-20371994&quot;>;dysarthria&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Apraxia&quot;>;apraxia&lt;/a>;, the WER could reach 50% or even 90% depending on the etiology and severity. To help address this problem, we worked with more than 1,000 participants to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-euphonia-1000-hours-speech-recordings/&quot;>;collect over 1,000 hours of disordered speech samples&lt;/a>; and used the data to show that ASR &lt;a href=&quot;https://ai.googleblog.com/2021/09/personalized-asr-models-from-large-and.html&quot;>;personalization&lt;/a>; is a viable avenue for bridging the performance gap for users with disordered speech. We&#39;ve shown that personalization can be successful with as little as &lt;a href=&quot;https://arxiv.org/abs/2110.04612&quot;>;3-4 minutes of training speech&lt;/a>;&amp;nbsp;using &lt;a href=&quot;https://arxiv.org/abs/1907.13511&quot;>;layer freezing techniques&lt;/a>;. &lt;/p>; &lt;p>; This work led to the development of &lt;a href=&quot;https://sites.research.google/relate/&quot;>;Project Relate&lt;/a>;&amp;nbsp;for anyone with atypical speech who could benefit from a personalized speech model. Built in partnership with &lt;a href=&quot;https://research.google/research-areas/speech-processing/&quot;>;Google&#39;s Speech team&lt;/a>;, Project Relate enables people who find it hard to be understood by other people and technology to train their own models. People can use these personalized models to communicate more effectively and gain more independence. To make ASR more accessible and usable, we describe how we fine-tuned Google&#39;s &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; (USM) to better understand disordered speech out of the box, without personalization, for use with digital assistant technologies, dictation apps, and in conversations. &lt;/p>; &lt;br />; &lt;h2>;Addressing the challenges&lt;/h2>; &lt;p>; Working closely with Project Relate users, it became clear that personalized models can be very useful, but for many users, recording dozens or hundreds of examples can be challenging. In addition, the personalized models did not always perform well in freeform conversation. &lt;/p>; &lt;p>; To address these challenges, Euphonia&#39;s research efforts have been focusing on &lt;em>;speaker independent&lt;/em>; ASR (SI-ASR) to make models work better out of the box for people with disordered speech so that no additional training is necessary. &lt;/p>; &lt;br />; &lt;h2>;Prompted Speech dataset for SI-ASR&lt;/h2>; &lt;p>; The first step in building a robust SI-ASR model was to create representative dataset splits. We created the Prompted Speech dataset by splitting the &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2021/macdonald21_interspeech.pdf&quot;>;Euphonia corpus&lt;/a>; into train, validation and test portions, while ensuring that each split spanned a range of speech impairment severity and underlying etiology and that no speakers or phrases appeared in multiple splits. The training portion consists of over 950k speech utterances from over 1,000 speakers with disordered speech. The test set contains around 5,700 utterances from over 350 speakers. Speech-language pathologists manually reviewed all of the utterances in the test set for transcription accuracy and audio quality. &lt;/p>; &lt;br />; &lt;h2>;Real Conversation test set&lt;/h2>; &lt;p>; Unprompted or conversational speech differs from prompted speech in several ways. In conversation, people speak faster and enunciate less. They repeat words, repair misspoken words, and use a more expansive vocabulary that is specific and personal to themselves and their community. To improve a model for this use case, we created the Real Conversation test set to benchmark performance. &lt;/p>; &lt;p>; The Real Conversation test set was created with the help of trusted testers who recorded themselves speaking during conversations. The audio was reviewed, any personally identifiable information (PII) was removed, and then that data was transcribed by speech-language pathologists. The Real Conversation test set contains over 1,500 utterances from 29 speakers. &lt;/p>; &lt;br />; &lt;h2>;Adapting USM to disordered speech&lt;/h2>; &lt;p>; We then tuned USM on the training split of the Euphonia Prompted Speech set to improve its performance on disordered speech. Instead of fine-tuning the full model, our tuning was based on &lt;a href=&quot;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&quot;>;residual adapters&lt;/a>;, a parameter-efficient tuning approach that adds tunable bottleneck layers as residuals between the transformer layers. Only these layers are tuned, while the rest of the model weights are untouched. We have &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.541/&quot;>;previously shown&lt;/a>; that this approach works very well to adapt ASR models to disordered speech. Residual adapters were only added to the encoder layers, and the bottleneck dimension was set to 64. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate the adapted USM, we compared it to older ASR models using the two test sets described above. For each test, we compare adapted USM to the pre-USM model best suited to that task: (1) For short prompted speech, we compare to Google&#39;s production ASR model optimized for short form ASR; (2) for longer Real Conversation speech, we compare to a model &lt;a href=&quot;https://arxiv.org/abs/2005.03271&quot;>;trained for long form ASR&lt;/a>;. USM improvements over pre-USM models can be explained by USM&#39;s relative size increase, 120M to 2B parameters, and other improvements discussed in the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;USM blog post&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model word error rates (WER) for each test set (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see that the USM adapted with disordered speech significantly outperforms the other models. The adapted USM&#39;s WER on Real Conversation is 37% better than the pre-USM model, and on the Prompted Speech test set, the adapted USM performs 53% better. &lt;/p>; &lt;p>; These findings suggest that the adapted USM is significantly more usable for an end user with disordered speech. We can demonstrate this improvement by looking at transcripts of Real Conversation test set recordings from a trusted tester of Euphonia and Project Relate (see below). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; -->; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example audio and transcriptions of a trusted tester&#39;s speech from the Real Conversation test set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A comparison of the Pre-USM and adapted USM transcripts revealed some key advantages: &lt;/p>; &lt;ul>;&lt;li>; The first example shows that Adapted USM is better at recognizing disordered speech patterns. The baseline misses key words like “XBox” and “controller” that are important for a listener to understand what they are trying to say. &lt;/li>; &lt;li>; The second example is a good example of how deletions are a primary issue with ASR models that are not trained with disordered speech. Though the baseline model did transcribe a portion correctly, a large part of the utterance was not transcribed, losing the speaker&#39;s intended message. &lt;/li>;&lt;/ul>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that this work is an important step towards making speech recognition more accessible to people with disordered speech. We are continuing to work on improving the performance of our models. With the rapid advancements in ASR, we aim to ensure people with disordered speech benefit as well. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Key contributors to this project include Fadi Biadsy, Michael Brenner, Julie Cattiau, Richard Cave, Amy Chung-Yu Chou, Dotan Emanuel, Jordan Green, Rus Heywood, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Bob MacDonald, Philip Nelson, Katie Seaver, Joel Shor, Jimmy Tobin, Katrin Tomanek, and Subhashini Venugopalan. We gratefully acknowledge the support Project Euphonia received from members of the USM research team including Yu Zhang, Wei Han, Nanxin Chen, and many others. Most importantly, we wanted to say a huge thank you to the 2,200+ participants who recorded speech samples and the many &lt;a href=&quot;https://sites.research.google/euphonia/about/#thank-partners&quot;>;advocacy groups&lt;/a>; who helped us connect with these participants.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Audio volume has been adjusted for ease of listening, but the original files would be more consistent with those used in training and would have pauses, silences, variable volume, etc.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3542229559799425219/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: AI for Social Good&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s72-w640-h396-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4041992163804186827&lt;/id>;&lt;published>;2023-06-21T10:29:00.000-07:00&lt;/published>;&lt;updated>;2023-06-21T10:29:53.804-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;The world&#39;s first braiding of non-Abelian anyons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Trond Andersen and Yuri Lensky, Research Scientists, Google Quantum AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s320/Non-Abelian%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine you&#39;re shown two identical objects and then asked to close your eyes. When you open your eyes, you see the same two objects in the same position. How can you determine if they have been swapped back and forth? Intuition and the laws of &lt;a href=&quot;https://en.wikipedia.org/wiki/Identical_particles#Quantum_mechanical_description_of_identical_particles&quot;>;quantum mechanics agree&lt;/a>;: If the objects are truly identical, there is no way to tell. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While this sounds like common sense, it only applies to our familiar three-dimensional world. Researchers have predicted that for a special type of particle, called an &lt;a href=&quot;https://en.wikipedia.org/wiki/Anyon&quot;>;anyon&lt;/a>;, that is restricted to move only in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Plane_(mathematics)&quot;>;two-dimensional&lt;/a>; (2D) plane, quantum mechanics allows for something quite different. Anyons are indistinguishable from one another and some, non-Abelian anyons, have a special property that causes observable differences in the shared quantum state under exchange, making it possible to tell when they have been exchanged, despite being fully indistinguishable from one another. While researchers have managed to detect their relatives, Abelian anyons, whose change under exchange is more subtle and impossible to directly detect, realizing “non-Abelian exchange behavior” has proven more difficult due to challenges with both control and detection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05954-4&quot;>;Non-Abelian braiding of graph vertices in a superconducting processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we report the observation of this non-Abelian exchange behavior for the first time. Non-Abelian anyons could open a new avenue for quantum computation, in which quantum operations are achieved by swapping particles around one another like strings are swapped around one another to create braids. Realizing this new exchange behavior on our &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;superconducting quantum processor&lt;/a>; could be an alternate route to so-called &lt;a href=&quot;https://arxiv.org/abs/quant-ph/9707021&quot;>;topological quantum computation&lt;/a>;, which benefits from being robust against environmental noise. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Exchange statistics and non-Abelian anyons&lt;/h2>; &lt;p>; In order to understand how this strange non-Abelian behavior can occur, it&#39;s helpful to consider an analogy with the braiding of two strings. Take two identical strings and lay them parallel next to one another. Swap their ends to form a double-helix shape. The strings are identical, but because they wrap around one another when the ends are exchanged, it is very clear when the two ends are swapped. &lt;/p>; &lt;p>; The exchange of non-Abelian anyons can be visualized in a similar way, where the strings are made from extending the particles&#39; positions into the time dimension to form “world-lines.” Imagine plotting two particles&#39; locations vs. time. If the particles stay put, the plot would simply be two parallel lines, representing their constant locations. But if we exchange the locations of the particles, the world lines wrap around one another. Exchange them a second time, and you&#39;ve made a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knot_theory&quot;>;knot&lt;/a>;. &lt;/p>; &lt;p>; While a bit difficult to visualize, knots in four dimensions (three spatial plus one time dimension) can always easily be undone. They are trivial — like a shoelace, simply pull one end and it unravels. But when the particles are restricted to two spatial dimensions, the knots are in three total dimensions and — as we know from our everyday 3D lives — cannot always be easily untied. The braiding of the non-Abelian anyons&#39; world lines can be used as quantum computing operations to transform the state of the particles. &lt;/p>; &lt;p>; A key aspect of non-Abelian anyons is “degeneracy”: the full state of several separated anyons is not completely specified by local information, allowing the same anyon configuration to represent superpositions of several quantum states. Winding non-Abelian anyons about each other can change the encoded state. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;How to make a non-Abelian anyon&lt;/h2>; &lt;p>; So how do we realize non-Abelian braiding with one of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sycamore_processor&quot;>;Google&#39;s quantum processors&lt;/a>;? We start with the familiar surface code, which we recently used to achieve a &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestone in quantum error correction&lt;/a>;, where qubits are arranged on the vertices of a checkerboard pattern. Each color square of the checkerboard represents one of two possible joint measurements that can be made of the qubits on the four corners of the square. These so-called “stabilizer measurements” can return a value of either + or – 1. The latter is referred to as a plaquette violation, and can be created and moved diagonally — just like bishops in chess — by applying single-qubit &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_logic_gate&quot;>;X- and Z-gates&lt;/a>;. Recently, we showed that these bishop-like &lt;a href=&quot;https://arxiv.org/abs/2104.01180&quot;>;plaquette violations are Abelian anyons&lt;/a>;. In contrast to non-Abelian anyons, the state of Abelian anyons changes only subtly when they are swapped — so subtly that it is impossible to directly detect. While Abelian anyons are interesting, they do not hold the same promise for topological quantum computing that non-Abelian anyons do. &lt;/p>; &lt;p>; To produce non-Abelian anyons, we need to control the degeneracy (ie, the number of &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function&quot;>;wavefunctions&lt;/a>; that causes all &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;stabilizer&lt;/a>; measurements to be +1). Since a stabilizer measurement returns two possible values, each stabilizer cuts the degeneracy of the system in half, and with sufficiently many stabilizers, only one wave function satisfies the criterion. Hence, a simple way to increase the degeneracy is to merge two stabilizers together. In the process of doing so, we remove one edge in the stabilizer grid, giving rise to two points where only three edges intersect. These points, referred to as “&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;degree-3 vertices&lt;/a>;” (D3Vs), are predicted to be non-Abelian anyons. &lt;/p>; &lt;p>; In order to braid the D3Vs, we have to move them, meaning that we have to stretch and squash the stabilizers into new shapes. We accomplish this by implementing two-qubit gates between the anyons and their neighbors (middle and right panels shown below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/s661/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;661&quot; height=&quot;529&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/w640-h529/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Non-Abelian anyons in stabilizer codes.&lt;strong>; a:&lt;/strong>; Example of a knot made by braiding two anyons&#39; world lines. &lt;strong>;b: &lt;/strong>;Single-qubit gates can be used to create and move stabilizers with a value of –1 (red squares). Like bishops in chess, these can only move diagonally and are therefore constrained to one sublattice in the regular surface code. This constraint is broken when D3Vs (yellow triangles) are introduced.&lt;strong>; c: &lt;/strong>;Process to form and move D3Vs (predicted to be non-Abelian anyons). We start with the surface code, where each square corresponds to a joint measurement of the four qubits on its corners (&lt;strong>;left&lt;/strong>; panel). We remove an edge separating two neighboring squares, such that there is now a single joint measurement of all six qubits (&lt;strong>;middle&lt;/strong>; panel). This creates two D3Vs, which are non-Abelian anyons. We move the D3Vs by applying two-qubit gates between neighboring sites (right panel). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now that we have a way to create and move the non-Abelian anyons, we need to verify their anyonic behavior. For this we examine three characteristics that would be expected of non-Abelian anyons: &lt;/p>; &lt;ol>; &lt;li>;The “&lt;a href=&quot;https://en.wikipedia.org/wiki/Fusion_of_anyons&quot;>;fusion rules&lt;/a>;” — What happens when non-Abelian anyons collide with each other? &lt;/li>;&lt;li>;Exchange statistics — What happens when they are braided around one another? &lt;/li>;&lt;li>;Topological quantum computing primitives — Can we encode qubits in the non-Abelian anyons and use braiding to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;two-qubit entangling operations&lt;/a>;? &lt;/li>; &lt;/ol>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The fusion rules of non-Abelian anyons&lt;/h2>; &lt;p>; We investigate fusion rules by studying how a pair of D3Vs interact with the bishop-like plaquette violations introduced above. In particular, we create a pair of these and bring one of them around a D3V by applying single-qubit gates. &lt;/p>; &lt;p>; While the rules of bishops in chess dictate that the plaquette violations can never meet, the dislocation in the checkerboard lattice allows them to break this rule, meet its partner and annihilate with it. The plaquette violations have now disappeared! But bring the non-Abelian anyons back in contact with one another, and the anyons suddenly morph into the missing plaquette violations. As weird as this behavior seems, it is a manifestation of exactly the fusion rules that we expect these entities to obey. This establishes confidence that the D3Vs are, indeed, non-Abelian anyons. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/s982/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;826&quot; data-original-width=&quot;982&quot; height=&quot;538&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/w640-h538/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Demonstration of anyonic fusion rules (starting with &lt;strong>;panel I&lt;/strong>;, in the &lt;strong>;lower left&lt;/strong>;). We form and separate two D3Vs (yellow triangles), then form two adjacent plaquette violations (red squares) and pass one between the D3Vs. The D3Vs deformation of the “chessboard” changes the bishop rules of the plaquette violations. While they used to lie on adjacent squares, they are now able to move along the same diagonals and collide (as shown by the red lines). When they do collide, they annihilate one another. The D3Vs are brought back together and surprisingly morph into the missing adjacent red plaquette violations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Observation of non-Abelian exchange statistics&lt;/h2>; &lt;p>; After establishing the fusion rules, we want to see the real smoking gun of non-Abelian anyons: non-Abelian exchange statistics. We create two pairs of non-Abelian anyons, then braid them by wrapping one from each pair around each other (shown below). When we fuse the two pairs back together, two pairs of plaquette violations appear. The simple act of braiding the anyons around one another changed the observables of our system. In other words, if you closed your eyes while the non-Abelian anyons were being exchanged, you would still be able to tell that they had been exchanged once you opened your eyes. This is the hallmark of non-Abelian statistics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/s1648/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1648&quot; height=&quot;437&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/w640-h437/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Braiding non-Abelian anyons. We make two pairs of D3Vs (&lt;strong>;panel II&lt;/strong>;), then bring one from each pair around each other (&lt;strong>;III-XI&lt;/strong>;). When fusing the two pairs together again in panel XII, two pairs of plaquette violations appear! Braiding the non-Abelian anyons changed the observables of the system from panel I to panel XII; a direct manifestation of non-Abelian exchange statistics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Topological quantum computing&lt;/h2>; &lt;p>; Finally, after establishing their fusion rules and exchange statistics, we demonstrate how we can use these particles in quantum computations. The non-Abelian anyons can be used to encode information, represented by &lt;a href=&quot;https://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;logical qubits&lt;/a>;, which should be distinguished from the actual &lt;em>;physical&lt;/em>; qubits used in the experiment. The number of logical qubits encoded in &lt;em>;N&lt;/em>; D3Vs can be shown to be &lt;em>;N&lt;/em>;/2–1, so we use &lt;em>;N&lt;/em>;=8 D3Vs to encode three logical qubits, and perform braiding to entangle them. By studying the resulting state, we find that the braiding has indeed led to the formation of the desired, well-known quantum entangled state called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state&quot;>;Greenberger-Horne-Zeilinger&lt;/a>; (GHZ) state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/s750/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;544&quot; data-original-width=&quot;750&quot; height=&quot;464&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/w640-h464/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using non-Abelian anyons as logical qubits.&lt;strong>; a, &lt;/strong>;We braid the non-Abelian anyons to entangle three qubits encoded in eight D3Vs. &lt;strong>;b, &lt;/strong>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_tomography&quot;>;Quantum state tomography&lt;/a>; allows for reconstructing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Density_matrix&quot;>;density matrix&lt;/a>;, which can be represented in a 3D bar plot and is found to be consistent with the desired highly entangled GHZ-state.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments show the first observation of non-Abelian exchange statistics, and that braiding of the D3Vs can be used to perform quantum computations. With future additions, including error correction during the braiding procedure, this could be a major step towards topological quantum computation, a long-sought method to endow qubits with intrinsic resilience against fluctuations and noise that would otherwise cause errors in computations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4041992163804186827/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html&quot; rel=&quot;alternate&quot; title=&quot;The world&#39;s first braiding of non-Abelian anyons&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s72-c/Non-Abelian%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9139300663122353070&lt;/id>;&lt;published>;2023-06-18T11:00:00.018-07:00&lt;/published>;&lt;updated>;2023-06-19T10:54:26.149-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at CVPR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shaina Mehta, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s1200/CVPR%20Design-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week marks the beginning of the premier annual &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;Computer Vision and Pattern Recognition&lt;/a>; conference (CVPR 2023), held in-person in Vancouver, BC (with additional virtual content). As a leader in computer vision research and a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Sponsors&quot;>;Platinum Sponsor&lt;/a>;, &lt;a href=&quot;https://research.google/&quot;>;Google Research&lt;/a>; will have a strong presence across CVPR 2023 with ~90 papers being presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers&quot;>;main conference&lt;/a>; and active involvement in over 40 conference &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/workshop-list&quot;>;workshops&lt;/a>; and &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/tutorial-list&quot;>;tutorials&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you are attending CVPR this year, please stop by our booth to chat with our researchers who are actively exploring the latest techniques for application to various areas of &lt;a href=&quot;https://research.google/pubs/?area=machine-perception&quot;>;machine perception&lt;/a>;. Our researchers will also be available to talk about and demo several recent efforts, including on-device ML applications with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;, strategies for differential privacy, neural radiance field technologies and much more. &lt;/p>; &lt;p>; You can also learn more about our research being presented at CVPR 2023 in the list below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>; Board and organizing committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Senior area chairs include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Area chairs include: &lt;em>;&lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Rodrigo Benenson&lt;/strong>;, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Da-Cheng Jua&lt;/strong>;, &lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Stephen Lombardi&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;Saurabh Singh&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Pratul P. Srinivasan&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, &lt;strong>;Jasper Uijlings&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Publicity Chair: &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Demonstration Chair: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Program Advisory Board includes: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Panels&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>;&lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;History and Future of Artificial Intelligence and Computer Vision &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>;&lt;/div>; &lt;p>; &lt;/p>; &lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;Scientific Discovery and the Environment &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;/div>; &lt;/div>; &lt;br />; &lt;h2>;Best Paper Award candidates&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.00277.pdf&quot;>;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhiqin Chen&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11082.pdf&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Qianqian Wang&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Richard Tucker&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.12242.pdf&quot;>;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/a>; &lt;br />; &lt;em>;Nataniel Ruiz*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Kfir Aberman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03142.pdf&quot;>;On Distillation of Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Chenlin Meng, Robin Rombach,&lt;strong>; Ruiqi Gao&lt;/strong>;,&lt;strong>; Diederik Kingma&lt;/strong>;, Stefano Ermon,&lt;strong>; Jonathan Ho&lt;/strong>;,&lt;strong>; Tim Salimans&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Highlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.11217.pdf&quot;>;Connecting Vision and Language with Video Localized Narratives&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Paul Voigtlaender&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.05496.pdf&quot;>;MaskSketch: Unpaired Structure-Guided Masked Image Generation&lt;/a>; &lt;br />; &lt;em>;Dina Bashkirova*,&lt;strong>; Jose Lezama&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11738.pdf&quot;>;SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>; &lt;br />; &lt;em>;Prune Truong*,&lt;strong>; Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05199.pdf&quot;>;MAGVIT: Masked Generative Video Transformer&lt;/a>; &lt;br />; &lt;em>;Lijun Yu*,&lt;strong>; Yong Cheng&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alexander Hauptmann&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf&quot;>;Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.02291.pdf&quot;>;I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification&lt;/a>; &lt;br />; &lt;em>;Muhammad Ferjad Naeem, Gul Zain Khan,&lt;strong>; Yongqin Xian&lt;/strong>;, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool,&lt;strong>; Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12624.pdf&quot;>;Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization&lt;/a>; &lt;br />; &lt;em>;Zifan Wang*,&lt;strong>; Nan Ding&lt;/strong>;, &lt;strong>;Tomer Levinboim&lt;/strong>;, &lt;strong>;Xi Chen&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Su Wang&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Ceslee Montgomery&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Shai Noy&lt;/strong>;, &lt;strong>;Stefano Pellegrini&lt;/strong>;, &lt;strong>;Yasumasa Onoe&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;William Cha&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14306.pdf&quot;>;RUST: Latent Neural Scene Representations from Unposed Imagery&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mehdi SM Sajjadi&lt;/strong>;, &lt;strong>;Aravindh Mahendran&lt;/strong>;, &lt;strong>;Thomas Kipf&lt;/strong>;, &lt;strong>;Etienne Pot&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Mario Lučić&lt;/strong>;, &lt;strong>;Klaus Greff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05221.pdf&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-training with Multi-Source Multimodal Knowledge Memory&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Ziniu Hu*,&lt;strong>; Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Zirui Wang&lt;/strong>;, Kai-Wei Chang,&lt;strong>; Yizhou Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.00833.pdf&quot;>;RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Sara Sabour&lt;/strong>;, &lt;strong>;Suhani Vora&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Ivan Krasin&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09682.pdf&quot;>;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>; &lt;br />; &lt;em>;Yifan Jiang*, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, Dejia Xu, &lt;strong>;Jonathan T. Barron&lt;/strong>;, Zhangyang Wang, Tianfan Xue*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.pdf&quot;>;BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>; &lt;br />; &lt;em>;Kacper Kania, Stephan Garbin, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00583.pdf&quot;>;Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints&lt;/a>; &lt;br />; &lt;em>;Guilherme Potje, Felipe Cadar, &lt;strong>;Andre Araujo&lt;/strong>;, Renato Martins, Erickson Nascimento&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf&quot;>;How Can Objects Help Action Recognition?&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.12652.pdf&quot;>;Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>; &lt;br />; &lt;em>;Peng Dai, &lt;strong>;Yinda Zhang&lt;/strong>;, Xin Yu, Xiaoyang Lyu, Xiaojuan Qi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14396.pdf&quot;>;IFSeg: Image-Free Semantic Segmentation via Vision-Language Model&lt;/a>; &lt;br />; &lt;em>;Sukmin Yun, Seong Park, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-Aware Saliency Modeling&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Shi Chen*, &lt;strong>;Nachiappan Valliappan&lt;/strong>;, &lt;strong>;Shaolei Shen&lt;/strong>;, &lt;strong>;Xinyu Ye&lt;/strong>;, &lt;strong>;Kai Kohlhoff&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09117.pdf&quot;>;MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis&lt;/a>; &lt;br />; &lt;em>;Tianhong Li*,&lt;strong>; Huiwen Chang&lt;/strong>;, Shlok Kumar Mishra,&lt;strong>; Han Zhang&lt;/strong>;, Dina Katabi,&lt;strong>; Dilip Krishnan&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17603.pdf&quot;>;NeRF-Supervised Deep Stereo&lt;/a>; &lt;br />; &lt;em>;Fabio Tosi, &lt;strong>;Alessio Tonioni&lt;/strong>;, Daniele Gregorio, Matteo Poggi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf&quot;>;Omnimatte3D: Associating Objects and their Effects in Unconstrained Monocular Video&lt;/a>; &lt;br />; &lt;em>;Mohammed Suhail, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, Leon Sigal, &lt;strong>;Forrester Cole&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15654.pdf&quot;>;OpenScene: 3D Scene Understanding with Open Vocabularies&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Songyou Peng&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;, &lt;strong>;Chiyu Jiang&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Marc Pollefeys&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08504.pdf&quot;>;PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>; &lt;br />; &lt;em>;Chung-Yi Weng,&lt;strong>; Pratul Srinivasan&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, Kate Saenko, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;AJ Piergiovanni&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01194.pdf&quot;>;Burstormer: Burst Image Restoration and Enhancement Transformer&lt;/a>; &lt;br />; &lt;em>;Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15774.pdf&quot;>;Decentralized Learning with Multi-Headed Distillation&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Andrey Zhmoginov&lt;/strong>;, &lt;strong>;Mark Sandler&lt;/strong>;, &lt;strong>;Nolan Miller&lt;/strong>;, &lt;strong>;Gus Kristiansen&lt;/strong>;, &lt;strong>;Max Vladymyrov&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02163.pdf&quot;>;GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>; &lt;br />; &lt;em>;Bokui Shen, Xinchen Yan, Charles R. Qi, Mahyar Najibi, Boyang Deng,&lt;strong>; Leonidas Guibas&lt;/strong>;, Yin Zhou, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11846.pdf&quot;>;Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions&lt;/a>; &lt;br />; &lt;em>;Yun He, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;, Xiangyang Xue, Yanwei Fu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.11042.pdf&quot;>;Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble&lt;/a>; &lt;br />; &lt;em>;Chun-Han Yao*, Wei-Chih Hung, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.00653.pdf&quot;>;Hyperbolic Contrastive Learning for Visual Representations beyond Objects&lt;/a>; &lt;br />; &lt;em>;Songwei Ge, Shlok Mishra,&lt;strong>; Simon Kornblith&lt;/strong>;, &lt;strong>;Chun-Liang Li, &lt;/strong>;David Jacobs&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09276.pdf&quot;>;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Bahjat Kawar*,&lt;strong>; Shiran Zada&lt;/strong>;, &lt;strong>;Oran Lang&lt;/strong>;, &lt;strong>;Omer Tov&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Tali Dekel&lt;/strong>;, &lt;strong>;Inbar Mosseri&lt;/strong>;, &lt;strong>;Michal Irani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02743.pdf&quot;>;Incremental 3D Semantic Scene Graph Prediction from RGB Sequences&lt;/a>; &lt;br />; &lt;em>;Shun-Cheng Wu, &lt;strong>;Keisuke Tateno&lt;/strong>;, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00575.pdf&quot;>;IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction&lt;/a>; &lt;br />; &lt;em>;Dekai Zhu, Guangyao Zhai, Yan Di, &lt;strong>;Fabian Manhardt&lt;/strong>;, Hendrik Berkemeyer, Tuan Tran, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.10844.pdf&quot;>;Learning to Generate Image Embeddings with User-Level Differential Privacy&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zheng Xu, Maxwell Collins, Yuxiao Wang, Liviu Panait, Sewoong Oh, Sean Augenstein, Ting Liu, Florian Schroff, H. Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05866.pdf&quot;>;NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs&lt;/a>; &lt;br />; &lt;em>;Harsh Rangwani, Lavish Bansal, Kartik Sharma,&lt;strong>; Tejan Karmali&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, Venkatesh Babu Radhakrishnan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09794.pdf&quot;>;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Ron Mokady*, Amir Hertz*, &lt;strong>;Kfir Aberman&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, Daniel Cohen-Or*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14020.pdf&quot;>;SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow&lt;/a>; &lt;br />; &lt;em>;Itai Lang*,&lt;strong>; Dror Aiger&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Shai Avidan&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11674.pdf&quot;>;Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>; &lt;br />; &lt;em>;Dario Pavllo*,&lt;strong>; David Joseph Tan&lt;/strong>;, &lt;strong>;Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12902.pdf&quot;>;TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Hanzhi Chen, &lt;strong>;Fabian Manhardt&lt;/strong>;, Nassir Navab, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf&quot;>;TryOnDiffusion: A Tale of Two UNets&lt;/a>; &lt;br />; &lt;em>;Luyang Zhu*, &lt;strong>;Dawei Yang&lt;/strong>;, &lt;strong>;Tyler Zhu&lt;/strong>;, &lt;strong>;Fitsum Reda&lt;/strong>;, &lt;strong>;William Chan&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03112.pdf&quot;>;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a>; &lt;br />; &lt;em>;Aishwarya Kamath*, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;Su Wang&lt;/strong>;, Jing Yu Koh*, &lt;strong>;Alexander Ku&lt;/strong>;, &lt;strong>;Austin Waters&lt;/strong>;, Yinfei Yang*, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Zarana Parekh&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08045.pdf&quot;>;CLIPPO: Image-and-Language Understanding from Pixels Only&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Basil Mustafa&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.04745.pdf&quot;>;Controllable Light Diffusion for Portraits&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;David Futschik&lt;/strong>;, &lt;strong>;Kelvin Ritland&lt;/strong>;, &lt;strong>;James Vecore&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Daniel Sýkora&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf&quot;>;CUF: Continuous Upsampling Filters&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Cristina Vasconcelos&lt;/strong>;, &lt;strong>;Cengiz Oztireli&lt;/strong>;, &lt;strong>;Mark Matthews&lt;/strong>;, &lt;strong>;Milad Hashemi&lt;/strong>;, &lt;strong>;Kevin Swersky&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01758.pdf&quot;>;Improving Zero-Shot Generalization and Robustness of Multi-modal Models&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge*, &lt;strong>;Jie Ren&lt;/strong>;, &lt;strong>;Andrew Gallagher&lt;/strong>;, &lt;strong>;Yuxiao Wang&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>;, &lt;strong>;Laurent Itti&lt;/strong>;, &lt;strong>;Balaji Lakshminarayanan&lt;/strong>;, &lt;strong>;Jiaping Zhao&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09665.pdf&quot;>;LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding&lt;/a>; &lt;br />; &lt;em>;Gen Li, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, Laura Sevilla-Lara&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03361.pdf&quot;>;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xiaoshuai Zhang&lt;/strong>;, &lt;strong>;Abhijit Kundu&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Hao Su&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01762.pdf&quot;>;Self-Supervised AutoFlow&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hsin-Ping Huang&lt;/strong>;, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf&quot;>;Train-Once-for-All Personalization&lt;/a>; &lt;br />; &lt;em>;Hong-You Chen*,&lt;strong>; Yandong Li&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Mingda Zhang&lt;/strong>;, Wei-Lun Chao,&lt;strong>; Li Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.14115.pdf&quot;>;Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Antoine Yang*, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Antoine Miech, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, Ivan Laptev, Josef Sivic, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14302.pdf&quot;>;VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Junjie Ke&lt;/strong>;, &lt;strong>;Keren Ye&lt;/strong>;, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Yonghui Wu&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Feng Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11152.pdf&quot;>;You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model&lt;/a>; &lt;br />; &lt;em>;Shengkun Tang, &lt;strong>;Yaqing Wang&lt;/strong>;, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, &lt;strong>;Yi Liang&lt;/strong>;, Dongkuan Xu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.05211.pdf&quot;>;Accidental Light Probes&lt;/a>; &lt;br />; &lt;em>;Hong-Xing Yu, Samir Agarwala, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;, &lt;strong>;Noah Snavely, &lt;/strong>;Jiajun Wu, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.09653.pdf&quot;>;FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning&lt;/a>; &lt;br />; &lt;em>;Yuanhao Xiong, Ruochen Wang, Minhao Cheng,&lt;strong>; Felix Yu&lt;/strong>;, Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08013.pdf&quot;>;FlexiViT: One Model for All Patch Sizes&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Lucas Beyer&lt;/strong>;, &lt;strong>;Pavel Izmailov&lt;/strong>;, &lt;strong>;Alexander Kolesnikov&lt;/strong>;, &lt;strong>;Mathilde Caron&lt;/strong>;, &lt;strong>;Simon Kornblith&lt;/strong>;, &lt;strong>;Xiaohua Zhai&lt;/strong>;, &lt;strong>;Matthias Minderer&lt;/strong>;, &lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Ibrahim Alabdulmohsin&lt;/strong>;, &lt;strong>;Filip Pavetic&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03087.pdf&quot;>;Iterative Vision-and-Language Navigation&lt;/a>; &lt;br />; &lt;em>;Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso,&lt;strong>; Peter Anderson&lt;/strong>;, Stefan Lee, Jesse Thomason&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.08010.pdf&quot;>;MoDi: Unconditional Motion Synthesis from Diverse Data&lt;/a>; &lt;br />; &lt;em>;Sigal Raab, Inbal Leibovitch, Peizhuo Li,&lt;strong>; Kfir Aberman&lt;/strong>;, Olga Sorkine-Hornung, Daniel Cohen-Or&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03369.pdf&quot;>;Multimodal Prompting with Missing Modalities for Visual Recognition&lt;/a>; &lt;br />; &lt;em>;Yi-Lun Lee,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>;, Wei-Chen Chiu,&lt;strong>; Chen-Yu Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf&quot;>;Scene-Aware Egocentric 3D Human Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu,&lt;strong>; Kripasindhu Sarkar&lt;/strong>;, Christian Theobalt&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06247.pdf&quot;>;ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-Based Consistency&lt;/a>; &lt;br />; &lt;em>;Zixuan Huang,&lt;strong>; Varun Jampani&lt;/strong>;, Ngoc Anh Thai,&lt;strong>; Yuanzhen Li&lt;/strong>;, Stefan Stojanov, James M. Rehg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05173.pdf&quot;>;Improving Image Recognition by Retrieving from Web-Scale Image-Text Data&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00341.pdf&quot;>;JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>; &lt;br />; &lt;em>;Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, Li Yi,&lt;strong>; Leonidas Guibas&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01436.pdf&quot;>;Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos&lt;/a>; &lt;br />; &lt;em>;Ziqian Bai*,&lt;strong>; Feitong Tan&lt;/strong>;, &lt;strong>;Zeng Huang&lt;/strong>;, &lt;strong>;Kripasindhu Sarkar&lt;/strong>;, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Di Qiu&lt;/strong>;, &lt;strong>;Abhimitra Meka&lt;/strong>;, &lt;strong>;Ruofei Du&lt;/strong>;, &lt;strong>;Mingsong Dou&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;, Ping Tan,&lt;strong>; Thabo Beeler&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.08556.pdf&quot;>;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>; &lt;br />; &lt;em>;Allan Zhou, Mo Jin Kim, Lirui Wang,&lt;strong>; Pete Florence&lt;/strong>;, &lt;strong>;Chelsea Finn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03084.pdf&quot;>;Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*,&lt;strong>; Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13582.pdf&quot;>;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mikaela Uy&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Ke Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06820.pdf&quot;>;Structured 3D Features for Reconstructing Controllable Avatars&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Enric Corona&lt;/strong>;, &lt;strong>;Mihai Zanfir&lt;/strong>;, &lt;strong>;Thiemo Alldieck&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Andrei Zanfir&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09119.pdf&quot;>;Token Turing Machines&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael S. Ryoo&lt;/strong>;, &lt;strong>;Keerthana Gopalakrishnan&lt;/strong>;, &lt;strong>;Kumara Kahatapitiya&lt;/strong>;, &lt;strong>;Ted Xiao&lt;/strong>;, &lt;strong>;Kanishka Rao&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Yao Lu&lt;/strong>;, &lt;strong>;Julian Ibarz&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10957.pdf&quot;>;TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization&lt;/a>; &lt;br />; &lt;em>;Fabrizio Guillaro, Davide Cozzolino,&lt;strong>; Avneesh Sud&lt;/strong>;, &lt;strong>;Nicholas Dufour, &lt;/strong>;Luisa Verdoliva&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.07685.pdf&quot;>;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/a>; &lt;br />; &lt;em>;Sihyun Yu,&lt;strong>; Kihyuk Sohn, &lt;/strong>;Subin Kim, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00990.pdf&quot;>;Visual Prompt Tuning for Generative Transfer Learning&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Luisa Polania&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17811.pdf&quot;>;Zero-Shot Referring Image Segmentation with Global-Local Context Features&lt;/a>; &lt;br />; &lt;em>;Seonghoon Yu,&lt;strong>; Paul Hongsuck Seo&lt;/strong>;, Jeany Son&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.03285.pdf&quot;>;DC2: Dual-Camera Defocus Control by Learning to Refocus&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hadi Alzayer&lt;/strong>;, &lt;strong>;Abdullah Abuolaim&lt;/strong>;, &lt;strong>;Leung Chun Chan&lt;/strong>;, &lt;strong>;Yang Yang&lt;/strong>;, &lt;strong>;Ying Chen Lou&lt;/strong>;, &lt;strong>;Jia-Bin Huang&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf&quot;>;Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision&lt;/a>; &lt;br />; &lt;em>;Aditay Tripathi*,&lt;strong>; Rishubh Singh&lt;/strong>;, Anirban Chakraborty,&lt;strong>; Pradeep Shenoy&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09898.pdf&quot;>;MetaCLUE: Towards Comprehensive Visual Metaphors Research&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arjun R. Akula&lt;/strong>;, &lt;strong>;Brendan Driscoll&lt;/strong>;, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Zhiwei Jia&lt;/strong>;, &lt;strong>;Suyash Damle&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;William T. Freeman&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13824.pdf&quot;>;Multi-Realism Image Compression with a Conditional Generator&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Eirikur Agustsson&lt;/strong>;, &lt;strong>;David Minnen&lt;/strong>;, &lt;strong>;George Toderici&lt;/strong>;, &lt;strong>;Fabian Mentzer&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.03267.pdf&quot;>;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>; &lt;br />; &lt;em>;Congyue Deng, Chiyu Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou,&lt;strong>; Leonidas Guibas&lt;/strong>;, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12053.pdf&quot;>;On Calibrating Semantic Segmentation Models: Analyses and an Algorithm&lt;/a>; &lt;br />; &lt;em>;Dongdong Wang,&lt;strong>; Boqing Gong&lt;/strong>;, Liqiang Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13515.pdf&quot;>;Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>; &lt;br />; &lt;em>;Lucy Chai,&lt;strong>; Richard Tucker&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, Phillip Isola,&lt;strong>; Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13662.pdf&quot;>;Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment&lt;/a>; &lt;br />; &lt;em>;Yiyou Sun*,&lt;strong>; Yaojie Liu&lt;/strong>;, &lt;strong>;Xiaoming Liu&lt;/strong>;, Yixuan Li,&lt;strong>; Wen-Sheng Chu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13277.pdf&quot;>;SINE: Semantic-Driven Image-Based NeRF Editing with Prior-Guided Editing Field&lt;/a>; &lt;br />; &lt;em>;Chong Bao,&lt;strong>; Yinda Zhang&lt;/strong>;, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15533.pdf&quot;>;Sequential Training of GANs Against GAN-Classifiers Reveals Correlated &quot;Knowledge Gaps&quot; Present Among Independently Trained GAN Instances&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arkanath Pathak&lt;/strong>;, &lt;strong>;Nicholas Dufour&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.16991.pdf&quot;>;SparsePose: Sparse-View Camera Pose Regression and Refinement&lt;/a>; &lt;br />; &lt;em>;Samarth Sinha, Jason Zhang,&lt;strong>; Andrea Tagliasacchi&lt;/strong>;, Igor Gilitschenski, David Lindell&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models&lt;/a>; &lt;br />; &lt;em>;Yushi Yao,&lt;strong>; Chang Ye&lt;/strong>;, &lt;strong>;Gamaleldin F. Elsayed&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://cv4mr.github.io/&quot;>;Computer Vision for Mixed Reality&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvpr2023.wad.vision/&quot;>;Workshop on Autonomous Driving (WAD)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://multimodal-content-moderation.github.io/&quot;>;Multimodal Content Moderation (MMCM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Chris Bregler&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mevan Babakar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mcv-workshop.github.io/#updates&quot;>;Medical Computer Vision (MCV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Shekoofeh Azizi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vand-cvpr23/home&quot;>;VAND: Visual Anomaly and Novelty Detection&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Jie Ren&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://struco3d.github.io/cvpr2023/&quot;>;Structural and Compositional Learning on 3D Data&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Fei Xia&lt;/strong>;, &lt;strong>;Amir Hertz&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgvc10&quot;>;Fine-Grained Visual Categorization (FGVC10)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Kimberly Wilber&lt;/strong>;, &lt;strong>;Sara Beery&lt;/strong>;&lt;/em>; &lt;br />; Panelists include: &lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/xrnerf/&quot;>;XRNeRF: Advances in NeRF for the Metaverse&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnilabel-workshop-cvpr23/overview&quot;>;OmniLabel: Infinite Label Spaces for Semantic Understanding via Natural Language&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Long Zhao&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://holistic-video-understanding.github.io/workshops/cvpr2023.html&quot;>;Large Scale Holistic Video Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;David Ross&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nice.lgresearch.ai/&quot;>;New Frontiers for Zero-Shot Image Captioning Evaluation (NICE)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ccd2023.github.io/&quot;>;Computational Cameras and Displays (CCD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ulugbek Kamilov&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mauricio Delbracio&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gazeworkshop.github.io/2023/&quot;>;Gaze Estimation and Prediction in the Wild (GAZE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Thabo Beele&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Erroll Wood&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgahi2023/home&quot;>;Face and Gesture Analysis for Health Informatics (FGAHI)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.cv4animals.com/&quot;>;Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;3D Vision and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation (E2EAD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opendrivelab.com/e2ead/cvpr23&quot;>;End-to-End Autonomous Driving: Emerging Tasks and Challenges&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mula-workshop.github.io/&quot;>;Multi-modal Learning and Applications (MULA)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Aleksander Hołyński&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sdas2023/&quot;>;Synthetic Data for Autonomous Systems (SDAS)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vdu-cvpr23&quot;>;Vision Datasets Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;José Lezama&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vijay Janapa Reddi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ieeecvf-cvpr2023-precognition/&quot;>;Precognition: Seeing Through the Future&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Utsav Prabhu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvlai.net/ntire/2023/&quot;>;New Trends in Image Restoration and Enhancement (NTIRE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://generative-vision.github.io/workshop-CVPR-23/&quot;>;Generative Models for Computer Vision&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://robustart.github.io/&quot;>;Adversarial Machine Learning on Computer Vision: Art of Robustness&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Xinyun Chen&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/wmf2023/home&quot;>;Media Forensics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Nicholas Carlini&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://taodataset.org/workshop/cvpr23/&quot;>;Tracking and Its Many Guises: Tracking Any Object in Open-World&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Paul Voigtlaender&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://scene-understanding.com/&quot;>;3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.es.ele.tue.nl/cvpm23/&quot;>;Computer Vision for Physiological Measurement (CVPM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/&quot;>;Affective Behaviour Analysis In-the-Wild&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision (EC3V)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Rida Qadri&lt;/strong>;, &lt;strong>;Mohammad Havaei&lt;/strong>;, &lt;strong>;Fernando Diaz&lt;/strong>;, &lt;strong>;Emily Denton&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;Negar Rostamzadeh&lt;/strong>;, &lt;strong>;Pamela Peter-Agbia&lt;/strong>;, &lt;strong>;Eva Kozanecka&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vizwiz.org/workshops/2023-workshop/&quot;>;VizWiz Grand Challenge: Describing Images and Videos Taken by Blind People&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Haoran Qi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ecv23/home&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;blog post&lt;/a>;) &lt;br />; Organizers include: &lt;em>;&lt;strong>;Andrew Howard&lt;/strong>;, &lt;strong>;Chas Leichner&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andrew Howard&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vcdw2023/&quot;>;Visual Copy Detection&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Priya Goyal&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dmv2023.github.io/&quot;>;Learning 3D with Multi-View Supervision (3DMV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://image-matching-workshop.github.io/&quot;>;Image Matching: Local Features and Beyond&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vision4allseason.net/&quot;>;Vision for All Seasons: Adverse Weather and Lightning Conditions (V4AS)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/t4v-cvpr23&quot;>;Transformers for Vision (T4V)&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/academic-cv/&quot;>;Scholars vs Big Models — How Can Academics Adapt?&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Jonathan T. Barron&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://www.scan-net.org/cvpr2023workshop/&quot;>;ScanNet Indoor Scene Understanding Challenge&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Tom Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvmi-workshop.github.io/new.html&quot;>;Computer Vision for Microscopy Image Analysis&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Po-Hsuan Cameron Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://embeddedvisionworkshop.wordpress.com/&quot;>;Embedded Vision&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sightsound.org/&quot;>;Sight and Sound&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;William Freeman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4cc.net/&quot;>;AI for Content Creation&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Tim Salimans&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://computer-vision-in-the-wild.github.io/cvpr-2023/https://computer-vision-in-the-wild.github.io/cvpr-2023/&quot;>;Computer Vision in the Wild&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Xiuye Gu&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vispr-workshop.github.io/&quot;>;Visual Pre-training for Robotics&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnicv2023/home&quot;>;Omnidirectional Computer Vision&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://all-things-vits.github.io/atv/&quot;>;All Things ViTs: Understanding and Interpreting Attention in Vision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hila Chefer&lt;/strong>;, &lt;strong>;Sayak Paul&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-tutorial-on-ad/&quot;>;Recent Advances in Anomaly Detection&lt;/a>; &lt;br />; &lt;em>;Guansong Pang, Joey Tianyi Zhou, Radu Tudor Ionescu, Yu Tian,&lt;strong>; Kihyuk Sohn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr-tutorial-2023/home&quot;>;Contactless Healthcare Using Cameras and Wireless Sensors&lt;/a>; &lt;br />; &lt;em>;Wenjin Wang, Xuyu Wang, Jun Luo,&lt;strong>; Daniel McDuff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://osimeoni.github.io/object-localization-for-free/&quot;>;Object Localization for Free: Going Beyond Self-Supervised Learning&lt;/a>; &lt;br />; &lt;em>;Oriane Simeoni,&lt;strong>; &lt;/strong>;Weidi Xie,&lt;strong>; Thomas Kipf&lt;/strong>;, Patrick Pérez&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://prompting-in-vision.github.io/&quot;>;Prompting in Vision&lt;/a>; &lt;br />; &lt;em>;Kaiyang Zhou, Ziwei Liu, Phillip Isola, Hyojin Bahng, Ludwig Schmidt, Sarah Pratt,&lt;strong>; Denny Zhou&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9139300663122353070/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CVPR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s72-c/CVPR%20Design-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2312998356111901143&lt;/id>;&lt;published>;2023-06-15T13:53:00.000-07:00&lt;/published>;&lt;updated>;2023-06-15T13:53:27.418-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Juhyun Lee and Raman Sarokin, Software Engineers, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s700/SpeedHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The proliferation of large &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>; for &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;image generation&lt;/a>; has led to a significant increase in model size and inference workloads. On-device ML inference in mobile environments requires meticulous performance optimization and consideration of trade-offs due to resource constraints. Running inference of large diffusion models (LDMs) on-device, driven by the need for cost efficiency and user privacy, presents even greater challenges due to the substantial memory requirements and computational demands of these models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We address this challenge in our work titled “&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations&lt;/a>;” (to be presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; workshop for &lt;a href=&quot;https://sites.google.com/corp/view/ecv23&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>;) focusing on the optimized execution of a foundational LDM model on a mobile GPU. In this blog post, we summarize the core techniques we employed to successfully execute large diffusion models like &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; at full resolution (512x512 pixels) and 20 iterations on modern smartphones with high-performing inference speed of the original model without distillation of under 12 seconds. As discussed in &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;our previous blog post&lt;/a>;, GPU-accelerated ML inference is often limited by memory performance, and execution of LDMs is no exception. Therefore, the central theme of our optimization is efficient memory input/output (I/O) even if it means choosing memory-efficient algorithms over those that prioritize arithmetic logic unit efficiency. Ultimately, our primary objective is to reduce the overall latency of the ML inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s512/image4.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A sample output of an LDM on Mobile GPU with the prompt text: “a photo realistic and high resolution image of a cute puppy with surrounding flowers”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enhanced attention module for memory efficiency&lt;/h2>; &lt;p>; An ML inference engine typically provides a variety of optimized ML operations. Despite this, achieving optimal performance can still be challenging as there is a certain amount of overhead for executing individual neural net operators on a GPU. To mitigate this overhead, ML inference engines incorporate extensive operator fusion rules that consolidate multiple operators into a single operator, thereby reducing the number of iterations across tensor elements while maximizing compute per iteration. For instance, &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; utilizes operator fusion to combine computationally expensive operations, like convolutions, with subsequent activation functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;rectified linear units&lt;/a>;, into one. &lt;/p>; &lt;p>; A clear opportunity for optimization is the heavily used attention block adopted in the &lt;a href=&quot;https://arxiv.org/pdf/2304.11267.pdf&quot;>;denoiser model&lt;/a>; in the LDM. The attention blocks allow the model to focus on specific parts of the input by assigning higher weights to important regions. There are multiple ways one can optimize the attention modules, and we selectively employ one of the two optimizations explained below depending on which optimization performs better. &lt;/p>; &lt;p>; The first optimization, which we call &lt;em>;partially fused &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;>;softmax&lt;/a>;&lt;/em>;, removes the need for extensive memory writes and reads between the softmax and the matrix multiplication in the attention module. Let the attention block be just a simple matrix multiplication of the form &lt;b>;&lt;em>;Y &lt;/em>;&lt;/b>;= softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) * &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; where &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; are 2D matrices of shape &lt;em>;a&lt;/em>;×&lt;em>;b&lt;/em>; and &lt;em>;b&lt;/em>;×&lt;em>;c&lt;/em>;, respectively (shown below in the top half). &lt;/p>; &lt;p>; For numerical stability, &lt;b>;&lt;em>;T = &lt;/em>;&lt;/b>;softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) is typically calculated in three passes: &lt;/p>; &lt;ol>; &lt;li>;Determine the maximum value in the list, &lt;em>;ie&lt;/em>;.,&lt;em>; &lt;/em>;for each row in matrix &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; &lt;/li>;&lt;li>;Sum up the differences of the exponential of each list item and the maximum value (from pass 1) &lt;/li>;&lt;li>;Divide the exponential of the items minus the maximum value by the sum from pass 2 &lt;/li>; &lt;/ol>; &lt;p>; Carrying out these passes naïvely would result in a huge memory write for the temporary intermediate tensor &lt;strong>;&lt;em>;T&lt;/em>;&lt;/strong>; holding the output of the entire softmax function. We bypass this large memory write if we only store the results of passes 1 and 2, labeled &lt;b>;&lt;em>;m&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;s&lt;/em>;&lt;/b>;, respectively, which are small vectors, with &lt;em>;a&lt;/em>; elements each, compared to &lt;strong>;&lt;em>;T &lt;/em>;&lt;/strong>;which has &lt;em>;a·b &lt;/em>;elements. With this technique, we are able to reduce tens or even hundreds of megabytes of memory consumption by multiple orders of magnitude (shown below in the bottom half). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s568/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;457&quot; data-original-width=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Attention modules. &lt;b>;Top&lt;/b>;: A naïve attention block, composed of a SOFTMAX (with all three passes) and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;MATMUL&lt;/a>;, requires a large memory write for the big intermediate tensor &lt;em>;T&lt;/em>;. &lt;b>;Bottom&lt;/b>;: Our memory-efficient attention block with partially fused softmax in MATMUL only needs to store two small intermediate tensors for &lt;em>;m&lt;/em>; and s.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The other optimization involves employing &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;>;FlashAttention&lt;/a>;, which is an I/O-aware, exact attention algorithm. This algorithm reduces the number of GPU high-bandwidth memory accesses, making it a good fit for our memory bandwidth–limited use case. However, we found this technique to only work for &lt;a href=&quot;https://en.wikipedia.org/wiki/Static_random-access_memory&quot;>;SRAM&lt;/a>; with certain sizes and to require a large number of registers. Therefore, we only leverage this technique for attention matrices with a certain size on a select set of GPUs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Winograd fast convolution for 3×3 convolution layers&lt;/h2>; &lt;p>; The backbone of common LDMs heavily relies on 3×3 convolution layers (convolutions with filter size 3×3), comprising over 90% of the layers in the decoder. Despite increased memory consumption and numerical errors, we found that &lt;a href=&quot;https://arxiv.org/abs/1509.09308&quot;>;Winograd fast convolution&lt;/a>; to be effective at speeding up the convolutions. Distinct from the &lt;em>;filter size&lt;/em>; 3x3 used in convolutions, &lt;em>;tile size&lt;/em>; refers to the size of a sub region of the input tensor that is processed at a time. Increasing the tile size enhances the efficiency of the convolution in terms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_logic_unit&quot;>;arithmetic logic unit&lt;/a>; (ALU) usage. However, this improvement comes at the expense of increased memory consumption. Our tests indicate that a tile size of 4×4 achieves the optimal trade-off between computational efficiency and memory utilization. &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;/td>; &lt;td>;&lt;/td>; &lt;td colspan=&quot;2&quot;>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Memory usage&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tile size&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; savings&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Intermediate tensors&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Weights&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;2×2 &lt;/td>; &lt;td>;2.25× &lt;/td>; &lt;td>;4.00× &lt;/td>; &lt;td>;1.77× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;4×4&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;2.25×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;6×6 &lt;/td>; &lt;td>;5.06× &lt;/td>; &lt;td>;1.80× &lt;/td>; &lt;td>;7.12× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;8×8 &lt;/td>; &lt;td>;5.76× &lt;/td>; &lt;td>;1.56× &lt;/td>; &lt;td>;11.1× &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impact of Winograd with varying tile sizes for 3×3 convolutions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Specialized operator fusion for memory efficiency&lt;/h2>; &lt;p>; We discovered that performantly inferring LDMs on a mobile GPU requires significantly larger fusion windows for commonly employed layers and units in LDMs than current off-the-shelf on-device GPU-accelerated ML inference engines provide. Consequently, we developed specialized implementations that could execute a larger range of neural operators than typical fusion rules would permit. Specifically, we focused on two specializations: the &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;>;Gaussian Error Linear Unit&lt;/a>; (GELU) and the &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;>;group normalization&lt;/a>; layer. &lt;/p>; &lt;p>; An approximation of GELU with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions&quot;>;hyperbolic tangent&lt;/a>; function requires writing to and reading from seven auxiliary intermediate tensors (shown below as light orange rounded rectangles in the figure below), reading from the input tensor &lt;em>;x&lt;/em>; three times, and writing to the output tensor &lt;em>;y&lt;/em>; once across eight GPU programs implementing the labeled operation each (light blue rectangles). A custom GELU implementation that performs the eight operations in a single shader (shown below in the bottom) can bypass all the memory I/O for the intermediate tensors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s958/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;210&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GELU implementations. &lt;strong>;Top&lt;/strong>;: A naïve implementation with built-in operations would require 8 memory writes and 10 reads. &lt;strong>;Bottom&lt;/strong>;: Our custom GELU only requires 1 memory read (for &lt;em>;x&lt;/em>;) and 1 write (for &lt;em>;y&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; After applying all of these optimizations, we conducted tests of Stable Diffusion 1.5 (image resolution 512x512, 20 iterations) on high-end mobile devices. Running Stable Diffusion with our GPU-accelerated ML inference model uses 2,093MB for the weights and 84MB for the intermediate tensors. With latest high-end smartphones, Stable Diffusion can be run in under 12 seconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s522/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stable Diffusion runs on modern smartphones in under 12 seconds. Note that running the decoder after each iteration for displaying the intermediate output in this animated GIF results in a ~2× slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Performing on-device ML inference of large models has proven to be a substantial challenge, encompassing limitations in model file size, extensive runtime memory requirements, and protracted inference latency. By recognizing memory bandwidth usage as the primary bottleneck, we directed our efforts towards optimizing memory bandwidth utilization and striking a delicate balance between ALU efficiency and memory efficiency. As a result, we achieved state-of-the-art inference latency for large diffusion models. You can learn more about this work in &lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank Yu-Hui Chen, Jiuqiang Tang, Frank Barchard, Yang Zhao, Joe Zou, Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, Lu Wang, and Matthias Grundmann.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2312998356111901143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot; rel=&quot;alternate&quot; title=&quot;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s72-c/SpeedHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3072875289355194363&lt;/id>;&lt;published>;2023-06-14T09:00:00.001-07:00&lt;/published>;&lt;updated>;2023-06-22T14:55:31.687-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Reconstructing indoor spaces with NeRF&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Marcos Seefelder, Software Engineer, and Daniel Duckworth, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s320/Immersive%20View%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; When choosing a venue, we often find ourselves with questions like the following: Does this restaurant have the right vibe for a date? Is there good outdoor seating? Are there enough screens to watch the game? While photos and videos may partially answer questions like these, they are no substitute for feeling like you&#39;re there, even when visiting in person isn&#39;t an option. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Immersive experiences that are interactive, photorealistic, and multi-dimensional stand to bridge this gap and recreate the feel and vibe of a space, empowering users to naturally and intuitively find the information they need. To help with this, Google Maps launched &lt;a href=&quot;https://blog.google/products/maps/google-maps-updates-immersive-view-trip-planning&quot;>;Immersive View&lt;/a>;, which uses advances in machine learning (ML) and computer vision to fuse billions of &lt;a href=&quot;https://www.google.com/streetview/&quot;>;Street View&lt;/a>; and aerial images to create a rich, digital model of the world. Beyond that, it layers helpful information on top, like the weather, traffic, and how busy a place is. Immersive View provides indoor views of restaurants, cafes, and other venues to give users a virtual up-close look that can help them confidently decide where to go. &lt;/p>; &lt;p>; Today we describe the work put into delivering these indoor views in Immersive View. We build on &lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;>;neural radiance fields&lt;/a>; (NeRF), a state-of-the-art approach for fusing photos to produce a realistic, multi-dimensional reconstruction within a neural network. We describe our pipeline for creation of NeRFs, which includes custom photo capture of the space using DSLR cameras, image processing and scene reproduction. We take advantage of Alphabet&#39;s &lt;a href=&quot;https://arxiv.org/abs/2008.02268&quot;>;recent&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2111.12077&quot;>;advances&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2202.05263&quot;>;in the field&lt;/a>; to design a method matching or outperforming the prior state-of-the-art in visual fidelity. These models are then embedded as interactive 360° videos following curated flight paths, enabling them to be available on smartphones. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/introduction_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The reconstruction of The Seafood Bar in Amsterdam in Immersive View.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;From photos to NeRFs&lt;/h2>; &lt;p>; At the core of our work is &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;NeRF&lt;/a>;, a recently-developed method for 3D reconstruction and novel view synthesis. Given a collection of photos describing a scene, NeRF distills these photos into a &lt;a href=&quot;https://arxiv.org/abs/2111.11426&quot;>;neural field&lt;/a>;, which can then be used to render photos from viewpoints not present in the original collection. &lt;/p>; &lt;p>; While NeRF largely solves the challenge of reconstruction, a user-facing product based on real-world data brings a wide variety of challenges to the table. For example, reconstruction quality and user experience should remain consistent across venues, from dimly-lit bars to sidewalk cafes to hotel restaurants. At the same time, privacy should be respected and any potentially personally identifiable information should be removed. Importantly, scenes should be captured consistently and efficiently, reliably resulting in high-quality reconstructions while minimizing the effort needed to capture the necessary photographs. Finally, the same natural experience should be available to all mobile users, regardless of the device on hand. &lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/overview_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Immersive View indoor reconstruction pipeline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Capture &amp;amp; preprocessing&lt;/h2>; &lt;p>; The first step to producing a high-quality NeRF is the careful capture of a scene: a dense collection of photos from which 3D geometry and color can be derived. To obtain the best possible reconstruction quality, every surface should be observed from multiple different directions. The more information a model has about an object&#39;s surface, the better it will be in discovering the object&#39;s shape and the way it interacts with lights. &lt;/p>; &lt;p>; In addition, NeRF models place further assumptions on the camera and the scene itself. For example, most of the camera&#39;s properties, such as white balance and aperture, are assumed to be fixed throughout the capture. Likewise, the scene itself is assumed to be frozen in time: lighting changes and movement should be avoided. This must be balanced with practical concerns, including the time needed for the capture, available lighting, equipment weight, and privacy. In partnership with professional photographers, we developed a strategy for quickly and reliably capturing venue photos using DSLR cameras within only an hour timeframe. This approach has been used for all of our NeRF reconstructions to date. &lt;/p>; &lt;p>; Once the capture is uploaded to our system, processing begins. As photos may inadvertently contain sensitive information, we automatically scan and blur personally identifiable content. We then apply a &lt;a href=&quot;https://en.wikipedia.org/wiki/Structure_from_motion&quot;>;structure-from-motion&lt;/a>; pipeline to solve for each photo&#39;s &lt;a href=&quot;https://www.mathworks.com/help/vision/ug/camera-calibration.html&quot;>;camera parameters&lt;/a>;: its position and orientation relative to other photos, along with lens properties like &lt;a href=&quot;https://en.wikipedia.org/wiki/Focal_length&quot;>;focal length&lt;/a>;. These parameters associate each pixel with a point and a direction in 3D space and constitute a key signal in the NeRF reconstruction process. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeRF reconstruction&lt;/h2>; &lt;p>; Unlike many ML models, a new NeRF model is trained from scratch on each captured location. To obtain the best possible reconstruction quality within a target compute budget, we incorporate features from a variety of published works on NeRF developed at Alphabet. Some of these include: &lt;/p>; &lt;ul>; &lt;li>;We build on &lt;a href=&quot;https://jonbarron.info/mipnerf360/&quot;>;mip-NeRF 360&lt;/a>;, one of the best-performing NeRF models to date. While more computationally intensive than Nvidia&#39;s widely-used &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;Instant NGP&lt;/a>;, we find the mip-NeRF 360 consistently produces fewer artifacts and higher reconstruction quality. &lt;/li>;&lt;li>;We incorporate the low-dimensional generative latent optimization (GLO) vectors introduced in &lt;a href=&quot;https://nerf-w.github.io/&quot;>;NeRF in the Wild&lt;/a>; as an auxiliary input to the model&#39;s radiance network. These are learned real-valued latent vectors that embed appearance information for each image. By assigning each image in its own latent vector, the model can capture phenomena such as lighting changes without resorting to cloudy geometry, a common artifact in casual NeRF captures. &lt;/li>;&lt;li>;We also incorporate exposure conditioning as introduced in &lt;a href=&quot;https://waymo.com/research/block-nerf/&quot;>;Block-NeRF&lt;/a>;. Unlike GLO vectors, which are uninterpretable model parameters, exposure is directly derived from a photo&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Exif&quot;>;metadata&lt;/a>; and fed as an additional input to the model&#39;s radiance network. This offers two major benefits: it opens up the possibility of varying &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;>;ISO&lt;/a>; and provides a method for controlling an image&#39;s brightness at inference time. We find both properties invaluable for capturing and reconstructing dimly-lit venues. &lt;/li>; &lt;/ul>; &lt;p>; We train each NeRF model on TPU or GPU accelerators, which provide different trade-off points. As with all Google products, we continue to search for new ways to improve, from reducing compute requirements to improving reconstruction quality. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/side_by_side_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A side-by-side comparison of our method and a mip-NeRF 360 baseline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A scalable user experience&lt;/h2>; &lt;p>; Once a NeRF is trained, we have the ability to produce new photos of a scene from any viewpoint and camera lens we choose. Our goal is to deliver a meaningful and helpful user experience: not only the reconstructions themselves, but guided, interactive tours that give users the freedom to naturally explore spaces from the comfort of their smartphones. &lt;/p>; &lt;p>; To this end, we designed a controllable 360° video player that emulates flying through an indoor space along a predefined path, allowing the user to freely look around and travel forward or backwards. As the first Google product exploring this new technology, 360° videos were chosen as the format to deliver the generated content for a few reasons. &lt;/p>; &lt;p>; On the technical side, &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;real-time inference&lt;/a>; and &lt;a href=&quot;https://phog.github.io/snerg/&quot;>;baked representations&lt;/a>; are still resource intensive on a per-client basis (either on device or cloud computed), and relying on them would limit the number of users able to access this experience. By using videos, we are able to scale the storage and delivery of videos to all users by taking advantage of the same video management and serving infrastructure used by YouTube. On the operations side, videos give us clearer editorial control over the exploration experience and are easier to inspect for quality in large volumes. &lt;/p>; &lt;p>; While we had considered capturing the space with a 360° camera directly, using a NeRF to reconstruct and render the space has several advantages. A virtual camera can fly anywhere in space, including over obstacles and through windows, and can use any desired camera lens. The camera path can also be edited post-hoc for smoothness and speed, unlike a live recording. A NeRF capture also does not require the use of specialized camera hardware. &lt;/p>; &lt;p>; Our 360° videos are rendered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting&quot;>;ray casting&lt;/a>; through each pixel of a virtual, spherical camera and compositing the visible elements of the scene. Each video follows a smooth path defined by a sequence of keyframe photos taken by the photographer during capture. The position of the camera for each picture is computed during structure-from-motion, and the sequence of pictures is smoothly interpolated into a flight path. &lt;/p>; &lt;p>; To keep speed consistent across different venues, we calibrate the distances for each by capturing pairs of images, each of which is 3 meters apart. By knowing measurements in the space, we scale the generated model, and render all videos at a natural velocity. &lt;/p>; &lt;p>; The final experience is surfaced to the user within &lt;a href=&quot;https://blog.google/products/maps/three-maps-updates-io-2022/&quot;>;Immersive View&lt;/a>;: the user can seamlessly fly into restaurants and other indoor venues and discover the space by flying through the photorealistic 360° videos. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open research questions&lt;/h2>; &lt;p>; We believe that this feature is the first step of many in a journey towards universally accessible, AI-powered, immersive experiences. From a NeRF research perspective, more questions remain open. Some of these include: &lt;/p>; &lt;ol>; &lt;li>;Enhancing reconstructions with scene segmentation, adding semantic information to the scenes that could make scenes, for example, searchable and easier to navigate. &lt;/li>;&lt;li>;Adapting NeRF to outdoor photo collections, in addition to indoor. In doing so, we&#39;d unlock similar experiences to every corner of the world and change how users could experience the outdoor world. &lt;/li>;&lt;li>;Enabling real-time, interactive 3D exploration through neural-rendering on-device. &lt;/li>; &lt;/ol>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/outdoors_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reconstruction of an outdoor scene with a NeRF model trained on Street View panoramas.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; As we continue to grow, we look forward to engaging with and contributing to the community to build the next generation of immersive experiences. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Jon Barron, Julius Beres, Daniel Duckworth, Roman Dudko, Magdalena Filak, Mike Harm, Peter Hedman, Claudio Martella, Ben Mildenhall, Cardin Moffett, Etienne Pot, Konstantinos Rematas, Yves Sallat, Marcos Seefelder, Lilyana Sirakovat, Sven Tresp and Peter Zhizhin.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Also, we&#39;d like to extend our thanks to Luke Barrington, Daniel Filip, Tom Funkhouser, Charles Goran, Pramod Gupta, Santi López, Mario Lučić, Isalo Montacute and Dan Thomasset for valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3072875289355194363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot; rel=&quot;alternate&quot; title=&quot;Reconstructing indoor spaces with NeRF&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s72-c/Immersive%20View%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7719238929287079732&lt;/id>;&lt;published>;2023-06-13T10:18:00.004-07:00&lt;/published>;&lt;updated>;2023-06-13T13:54:56.343-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling delightful user experiences via predictive models of human attention&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Junfeng He, Senior Research Scientist, and Kai Kohlhoff, Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s1400/HumanAttention.png&quot; style=&quot;display: none;&quot; />; &lt;p>; People have the remarkable ability to take in a tremendous amount of information (estimated to be ~10&lt;sup>;10&lt;/sup>; bits/s entering the retina) and selectively attend to a few task-relevant and interesting regions for further processing (eg, memory, comprehension, action). Modeling human attention (the result of which is often called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;saliency&lt;/a>; model) has therefore been of interest across the fields of neuroscience, psychology, &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;>;human-computer interaction&lt;/a>; (HCI) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision&quot;>;computer vision&lt;/a>;. The ability to predict which regions are likely to attract attention has numerous important applications in areas like graphics, photography, image compression and processing, and the measurement of visual quality. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We&#39;ve &lt;a href=&quot;https://ai.googleblog.com/2021/05/accelerating-eye-movement-research-for.html&quot;>;previously discussed&lt;/a>; the possibility of accelerating eye movement research using machine learning and smartphone-based gaze estimation, which earlier required specialized hardware costing up to $30,000 per unit. Related research includes “&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/look-to-speak/&quot;>;Look to Speak&lt;/a>;”, which helps users with accessibility needs (eg, people with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;ALS&lt;/a>;) to communicate with their eyes, and the recently published “&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;Differentially private heatmaps&lt;/a>;” technique to compute heatmaps, like those for attention, while protecting users&#39; privacy. &lt;/p>; &lt;p>; In this blog, we present two papers (one from &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;>;CVPR 2022&lt;/a>;, and one just accepted to &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;) that highlight our recent research in the area of human attention modeling: “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” and “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, together with recent research on saliency driven progressive loading for image compression (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;2&lt;/a>;). We showcase how predictive models of human attention can enable delightful user experiences such as image editing to minimize visual clutter, distraction or artifacts, image compression for faster loading of webpages or apps, and guiding ML models towards more intuitive human-like interpretation and model performance. We focus on image editing and image compression, and discuss recent advances in modeling in the context of these applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Attention-guided image editing&lt;/h2>; &lt;p>; Human attention models usually take an image as input (eg, a natural image or a screenshot of a webpage), and &lt;a href=&quot;https://dl.acm.org/doi/10.1109/TPAMI.2019.2935715&quot;>;predict a heatmap as output&lt;/a>;. The predicted heatmap on the image is &lt;a href=&quot;https://www.computer.org/csdl/journal/tp/2019/03/08315047/17D45Vw15wU&quot;>;evaluated against ground-truth attention data&lt;/a>;, which are typically collected by an eye tracker or &lt;a href=&quot;https://bubbleview.namwkim.org/&quot;>;approximated via mouse hovering/clicking&lt;/a>;. Previous models leveraged handcrafted features for visual clues, like color/brightness contrast, edges, and shape, while more recent approaches automatically learn discriminative features based on deep neural networks, from &lt;a href=&quot;https://arxiv.org/abs/1805.01047&quot;>;convolutional&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/1610.01708.pdf&quot;>;recurrent neural networks&lt;/a>; to more recent &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231222004714&quot;>;vision transformer networks&lt;/a>;. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” (more information on this &lt;a href=&quot;https://deep-saliency-prior.github.io/&quot;>;project site&lt;/a>;), we leverage deep saliency models for dramatic yet visually realistic edits, which can significantly change an observer&#39;s attention to different image regions. For example, removing distracting objects in the background can reduce clutter in photos, leading to increased user satisfaction. Similarly, in video conferencing, reducing clutter in the background may increase focus on the main speaker (&lt;a href=&quot;https://deep-saliency-prior.github.io/supplementary/index.html&quot;>;example demo here&lt;/a>;). &lt;/p>; &lt;p>; To explore what types of editing effects can be achieved and how these affect viewers&#39; attention, we developed an optimization framework for guiding visual attention in images using a differentiable, predictive saliency model. Our method employs a state-of-the-art deep saliency model. Given an input image and a binary mask representing the distractor regions, pixels within the mask will be edited under the guidance of the predictive saliency model such that the saliency within the masked region is reduced. To make sure the edited image is natural and realistic, we carefully choose four image editing operators: two standard image editing operations, namely recolorization and image warping (shift); and two learned operators (we do not define the editing operation explicitly), namely a multi-layer convolution filter, and a generative model (&lt;a href=&quot;https://arxiv.org/abs/1912.04958&quot;>;GAN&lt;/a>;). &lt;/p>; &lt;p>; With those operators, our framework can produce a variety of powerful effects, with examples in the figure below, including recoloring, inpainting, camouflage, object editing or insertion, and facial attribute editing. Importantly, all these effects are driven solely by the single, pre-trained saliency model, without any additional supervision or training. Note that our goal is not to compete with dedicated methods for producing each effect, but rather to demonstrate how multiple editing operations can be guided by the knowledge embedded within deep saliency models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of reducing visual distractions, guided by the saliency model with several operators. The distractor region is marked on top of the saliency map (red border) in each example.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enriching experiences with user-aware saliency modeling&lt;/h2>; &lt;p>; Prior research assumes a single saliency model for the whole population. However, human attention varies between individuals — while the detection of salient clues is fairly consistent, their order, interpretation, and gaze distributions can differ substantially. This offers opportunities to create personalized user experiences for individuals or groups. In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, we introduce a user-aware saliency model, the first that can predict attention for one user, a group of users, and the general population, with a single model. &lt;/p>; &lt;p>; As shown in the figure below, core to the model is the combination of each participant&#39;s visual preferences with a per-user attention map and adaptive user masks. This requires per-user attention annotations to be available in the training data, eg, the &lt;a href=&quot;https://www.nature.com/articles/s41467-020-18360-5&quot;>;OSIE mobile gaze dataset for natural images; FiWI&lt;/a>; and &lt;a href=&quot;http://vision.cs.stonybrook.edu/~soura/websaliency.html&quot;>;WebSaliency&lt;/a>; datasets for web pages. Instead of predicting a single saliency map representing attention of all users, this model predicts per-user attention maps to encode individuals&#39; attention patterns. Further, the model adopts a user mask (a binary vector with the size equal to the number of participants) to indicate the presence of participants in the current sample, which makes it possible to select a group of participants and combine their preferences into a single heatmap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s1218/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;724&quot; data-original-width=&quot;1218&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the user aware saliency model framework. The example image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During inference, the user mask allows making predictions for any combination of participants. In the following figure, the first two rows are attention predictions for two different groups of participants (with three people in each group) on an image. A &lt;a href=&quot;https://arxiv.org/pdf/1805.01047.pdf&quot;>;conventional attention prediction model&lt;/a>; will predict identical attention heatmaps. Our model can distinguish the two groups (eg, the second group pays less attention to the face and more attention to the food than the first). Similarly, the last two rows are predictions on a webpage for two distinctive participants, with our model showing different preferences (eg, the second participant pays more attention to the left region than the first). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s961/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;961&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Predicted attention vs. ground truth (GT). EML-Net: predictions from a state-of-the-art model, which will have the same predictions for the two participants/groups. Ours: predictions from our proposed user aware saliency model, which can predict the unique preference of each participant/group correctly. The first image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set, and the second is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html&quot;>;FiWI&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Progressive image decoding centered on salient features&lt;/h2>; &lt;p>; Besides image editing, human attention models can also improve users&#39; browsing experience. One of the most frustrating and annoying user experiences while browsing is waiting for web pages with images to load, especially in conditions with low network connectivity. One way to improve the user experience in such cases is with progressive decoding of images, which decodes and displays increasingly higher-resolution image sections as data are downloaded, until the full-resolution image is ready. Progressive decoding usually proceeds in a sequential order (eg, left to right, top to bottom). With a predictive attention model (&lt;a href=&quot;https://github.com/google/attention-center&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;2&lt;/a>;), we can instead decode images based on saliency, making it possible to send the data necessary to display details of the most salient regions first. For example, in a portrait, bytes for the face can be prioritized over those for the out-of-focus background. Consequently, users perceive better image quality earlier and experience significantly reduced wait times. More details can be found in our open source blog posts (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;post 1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;post 2&lt;/a>;). Thus, predictive attention models can help with image compression and faster loading of web pages with images, improve rendering for large images and streaming/VR applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We&#39;ve shown how predictive models of human attention can enable delightful user experiences via applications such as image editing that can reduce clutter, distractions or artifacts in images or photos for users, and progressive image decoding that can greatly reduce the perceived waiting time for users while images are fully rendered. Our user-aware saliency model can further personalize the above applications for individual users or groups, enabling richer and more unique experiences. &lt;/p>; &lt;p>; Another interesting direction for predictive attention models is whether they can help improve robustness of computer vision models in tasks such as object classification or detection. For example, in “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-generated spatial-attention labels boost robustness and accuracy of contrastive models&lt;/a>;”, we show that a predictive human attention model can guide &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;contrastive learning&lt;/a>; models to achieve better representation and improve the accuracy/robustness of classification tasks (on the &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://paperswithcode.com/dataset/imagenet-c&quot;>;ImageNet-C&lt;/a>; datasets). Further research in this direction could enable applications such as using radiologist&#39;s attention on medical images to improve health screening or diagnosis, or using human attention in complex driving scenarios to guide autonomous driving systems. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of software engineers, researchers, and cross-functional contributors. We&#39;d like to thank all the co-authors of the papers/research, including Kfir Aberman, Gamaleldin F. Elsayed, Moritz Firsching, Shi Chen, Nachiappan Valliappan, Yushi Yao, Chang Ye, Yossi Gandelsman, Inbar Mosseri, David E. Jacobes, Yael Pritch, Shaolei Shen, and Xinyu Ye. We also want to thank team members Oscar Ramirez, Venky Ramachandran and Tim Fujita for their help. Finally, we thank Vidhya Navalpakkam for her technical leadership in initiating and overseeing this body of work.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7719238929287079732/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling delightful user experiences via predictive models of human attention&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s72-c/HumanAttention.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2015281937814138473&lt;/id>;&lt;published>;2023-06-09T12:09:00.001-07:00&lt;/published>;&lt;updated>;2023-06-12T07:58:34.790-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Image Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Imagen Editor and EditBench: Advancing and evaluating text-guided image inpainting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Su Wang and Ceslee Montgomery, Research Engineers, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s320/Imagen%20Editor%20&amp;amp;%20EditBench%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In the last few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Text-to-image_model&quot;>;text-to-image generation&lt;/a>; research has seen an explosion of breakthroughs (notably, &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>;, &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>;, etc.) that have naturally permeated into related topics. In particular, text-guided image editing (TGIE) is a practical task that involves editing generated and photographed visuals rather than completely redoing them. Quick, automated, and controllable editing is a convenient solution when recreating visuals would be time-consuming or infeasible (eg, tweaking objects in vacation photos or perfecting fine-grained details on a cute pup generated from scratch). Further, TGIE represents a substantial opportunity to improve training of foundational models themselves. Multimodal models require diverse data to train properly, and TGIE editing can enable the generation and recombination of high-quality and scalable synthetic data that, perhaps most importantly, can provide methods to optimize the distribution of training data along any given axis. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2212.06909&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>;”, to be presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;Imagen Editor&lt;/a>;, a state-of-the-art solution for the task of masked &lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;inpainting&lt;/a>; — ie, when a user provides text instructions alongside an overlay or “mask” (usually generated within a drawing-type interface) indicating the area of the image they would like to modify. We also introduce &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;EditBench&lt;/a>;, a method that gauges the quality of image editing models. EditBench goes beyond the &lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;commonly&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=bKBhQhPeKaF&quot;>;used&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1910.13321&quot;>;coarse-grained&lt;/a>; “does this image match this text” methods, and drills down to various types of attributes, objects, and scenes for a more fine-grained understanding of model performance. In particular, it puts strong emphasis on the faithfulness of image-text alignment without losing sight of image quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s1261/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an image, a user-defined mask, and a text prompt, Imagen Editor makes localized edits to the designated areas. The model meaningfully incorporates the user&#39;s intent and performs photorealistic edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Imagen Editor&lt;/h2>; &lt;p>; Imagen Editor is a &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html&quot;>;diffusion-based model&lt;/a>; fine-tuned on &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>; for editing. It targets improved representations of linguistic inputs, fine-grained control and high-fidelity outputs. Imagen Editor takes three inputs from the user: 1) the image to be edited, 2) a binary mask to specify the edit region, and 3) a text prompt — all three inputs guide the output samples. &lt;/p>; &lt;p>; Imagen Editor depends on three core techniques for high-quality text-guided image inpainting. First, unlike prior inpainting models (eg, &lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1801.07892&quot;>;Context Attention&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;>;Gated Convolution&lt;/a>;) that apply random box and stroke masks, Imagen Editor employs an object detector masking policy with an &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;object detector module&lt;/a>; that produces object masks during training. Object masks are based on detected objects rather than random patches and allow for more principled alignment between edit text prompts and masked regions. Empirically, the method helps the model stave off the prevalent issue of the text prompt being ignored when masked regions are small or only partially cover an object (eg, &lt;a href=&quot;https://arxiv.org/abs/2204.14217&quot;>;CogView2&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/s648/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;648&quot; height=&quot;328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/w640-h328/image9.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Random masks (&lt;strong>;left&lt;/strong>;) frequently capture background or intersect object boundaries, defining regions that can be plausibly inpainted just from image context alone. Object masks (&lt;strong>;right&lt;/strong>;) are harder to inpaint from image context alone, encouraging models to rely more on text inputs during training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Next, during training and inference, Imagen Editor enhances high resolution editing by conditioning on full resolution (1024×1024 in this work), channel-wise concatenation of the input image and the mask (similar to &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887996&quot;>;SR3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2112.10741&quot;>;GLIDE&lt;/a>;). For the base diffusion 64×64 model and the 64×64→256×256 super-resolution models, we apply a parameterized downsampling convolution (eg, convolution with a stride), which we empirically find to be critical for high fidelity. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s646/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;365&quot; data-original-width=&quot;646&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Imagen is fine-tuned for image editing. All of the diffusion models, ie, the base model and super-resolution (SR) models, are conditioned on high-resolution 1024×1024 image and mask inputs. To this end, new convolutional image encoders are introduced.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Finally, at inference we apply &lt;a href=&quot;https://arxiv.org/abs/2207.12598&quot;>;classifier-free guidance&lt;/a>; (CFG) to bias samples to a particular conditioning, in this case, text prompts. CFG interpolates between the text-conditioned and unconditioned model predictions to ensure strong alignment between the generated image and the input text prompt for text-guided image inpainting. We follow &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;Imagen Video&lt;/a>; and use high guidance weights with guidance oscillation (a guidance schedule that oscillates within a value range of guidance weights). In the base model (the stage-1 64x diffusion), where ensuring strong alignment with text is most critical, we use a guidance weight schedule that oscillates between 1 and 30. We observe that high guidance weights combined with oscillating guidance result in the best trade-off between sample fidelity and text-image alignment. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EditBench&lt;/h2>; &lt;p>; The EditBench dataset for text-guided image inpainting evaluation contains 240 images, with 120 generated and 120 natural images. Generated images are synthesized by &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; and natural images are drawn from the &lt;a href=&quot;https://arxiv.org/abs/1602.07332&quot;>;Visual Genome&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1811.00982&quot;>;Open Images&lt;/a>; datasets. EditBench captures a wide variety of language, image types, and levels of text prompt specificity (ie, simple, rich, and full captions). Each example consists of (1) a masked input image, (2) an input text prompt, and (3) a high-quality output image used as reference for automatic metrics. To provide insight into the relative strengths and weaknesses of different models, EditBench prompts are designed to test fine-grained details along three categories: (1) attributes (eg, material, color, shape, size, count); (2) object types (eg, common, rare, text rendering); and (3) scenes (eg, indoor, outdoor, realistic, or paintings). To understand how different specifications of prompts affect model performance, we provide three text prompt types: a single-attribute (Mask Simple) or a multi-attribute description of the masked object (Mask Rich) – or an entire image description (Full Image). Mask Rich, especially, probes the models&#39; ability to handle complex attribute binding and inclusion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s531/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;531&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The full image is used as a reference for successful inpainting. The mask covers the target object with a free-form, non-hinting shape. We evaluate Mask Simple, Mask Rich and Full Image prompts, consistent with conventional text-to-image models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Due to the intrinsic weaknesses in existing automatic evaluation metrics (&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;CLIPScore&lt;/a>; and &lt;a href=&quot;https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf&quot;>;CLIP-R-Precision&lt;/a>;) for TGIE, we hold human evaluation as the gold standard for EditBench. In the section below, we demonstrate how EditBench is applied to model evaluation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation &lt;/h2>; &lt;p>; We evaluate the Imagen Editor model — with object masking (IM) and with random masking (IM-RM) — against comparable models, &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; (SD) and &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; (DL2). Imagen Editor outperforms these models by substantial margins across all EditBench evaluation categories. &lt;/p>; &lt;p>; For Full Image prompts, &lt;em>;single-image human evaluation&lt;/em>; provides binary answers to confirm if the image matches the caption. For Mask Simple prompts, single-image human evaluation confirms if the object and attribute are properly rendered, and bound correctly (eg, for a red cat, a white cat on a red table would be an incorrect binding). &lt;em>;Side-by-side human evaluation&lt;/em>; uses Mask Rich prompts only for side-by-side comparisons between IM and each of the other three models (IM-RM, DL2, and SD), and indicates which image matches with the caption better for text-image alignment, and which image is most realistic. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/s800/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;703&quot; data-original-width=&quot;800&quot; height=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/w640-h562/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Human evaluation. Full Image prompts elicit annotators&#39; overall impression of text-image alignment; Mask Simple and Mask Rich check for the correct inclusion of particular attributes, objects and attribute binding.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For single-image human evaluation, IM receives the highest ratings across-the-board (10–13% higher than the 2nd-highest performing model). For the rest, the performance order is IM-RM &amp;gt; DL2 &amp;gt; SD (with 3–6% difference) except for with Mask Simple, where IM-RM falls 4-8% behind. As relatively more semantic content is involved in Full and Mask Rich, we conjecture IM-RM and IM are benefited by the higher performing &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 XXL&lt;/a>; text encoder. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s1117/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1117&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations of text-guided image inpainting on EditBench by prompt type. For Mask Simple and Mask Rich prompts, text-image alignment is correct if the edited image accurately includes every attribute and object specified in the prompt, including the correct attribute binding. Note that due to different evaluation designs, Full vs. Mask-only prompts, results are less directly comparable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; EditBench focuses on fine-grained annotation, so we evaluate models for object and attribute types. For object types, IM leads in all categories, performing 10–11% better than the 2nd-highest performing model in common, rare, and text-rendering. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s1086/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations on EditBench Mask Simple by object type. As a cohort, models are better at object rendering than text-rendering.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For attribute types, IM is rated much higher (13–16%) than the 2nd highest performing model, except for in count, where DL2 is merely 1% behind. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s1086/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;607&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations on EditBench Mask Simple by attribute type. Object masking improves adherence to prompt attributes across-the-board (IM vs. IM-RM).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Side-by-side compared with other models one-vs-one, IM leads in text alignment with a substantial margin, being preferred by annotators compared to SD, DL2, and IM-RM. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s636/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;319&quot; data-original-width=&quot;636&quot; src=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Side-by-side human evaluation of image realism &amp;amp; text-image alignment on EditBench Mask Rich prompts. For text-image alignment, Imagen Editor is preferred in all comparisons.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Finally, we illustrate a representative side-by-side comparative for all the models. See the &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;paper&lt;/a>; for more examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s766/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;766&quot; data-original-width=&quot;638&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example model outputs for Mask Simple vs. Mask Rich prompts. Object masking improves Imagen Editor&#39;s fine-grained adherence to the prompt compared to the same model trained with random masking.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented Imagen Editor and EditBench, making significant advancements in text-guided image inpainting and the evaluation thereof. Imagen Editor is a text-guided image inpainting fine-tuned from Imagen. EditBench is a comprehensive systematic benchmark for text-guided image inpainting, evaluating performance across multiple dimensions: attributes, objects, and scenes. Note that due to concerns in relation to responsible AI, we are not releasing Imagen Editor to the public. EditBench on the other hand is &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;released in full&lt;/a>; for the benefit of the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments &lt;/h2>; &lt;p>; &lt;em>;Thanks to Gunjan Baid, Nicole Brichtova, Sara Mahdavi, Kathy Meier-Hellstern, Zarana Parekh, Anusha Ramesh, Tris Warkentin, Austin Waters, and Vijay Vasudevan for their generous support. We give thanks to Igor Karpov, Isabel Kraus-Liang, Raghava Ram Pamidigantam, Mahesh Maddinala, and all the anonymous human annotators for their coordination to complete the human evaluation tasks. We are grateful to Huiwen Chang, Austin Tarango, and Douglas Eck for providing paper feedback.感谢 Erica Moreira 和 Victor Gomes 在资源协调方面提供的帮助。 Finally, thanks to the authors of DALL-E 2 for giving us permission to use their model outputs for research purposes.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2015281937814138473/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot; rel=&quot;alternate&quot; title=&quot;Imagen Editor and EditBench: Advancing and evaluating text-guided image inpainting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s72-c/Imagen%20Editor%20&amp;%20EditBench%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;