<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-08-18T11:31:22.976-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="ICML"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="Responsible AI"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="RAI-HCT Highlights"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1266&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-362086873015740792&lt;/id>;&lt;发布>;2023-08-18T11:28:00.004-07:00&lt;/发布>;&lt;更新>;2023-08- 18T11:30:51.167-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;利用大语言模型进行自主视觉信息搜索&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;作者：胡紫牛，学生Google 研究感知团队研究员和研究科学家 Alireza Fathi&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqN pbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;显示：无；” />; &lt;p>; 在适应大型语言模型 (LLM) 以适应任务的多模式输入方面已经取得了巨大进展，包括 &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~ :text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20them。&quot;>;图像字幕&lt;/a>;，&lt;a href=&quot;https://huggingface.co/tasks/visual-question -answering&quot;>;视觉问答 (VQA)&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;开放词汇识别&lt;/a>;。尽管取得了这些成就，当前最先进的视觉语言模型（VLM）在视觉信息搜索数据集上的表现还不够，例如 &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a >; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;，需要外部知识来回答问题。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe -vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R 9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-宽度=“1999”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpm MLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;需要外部知识来回答问题的视觉信息搜索查询示例。图像取自 OK-VQA 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>; AVIS：使用大型语言模型进行自主视觉信息搜索&lt;/a>;”，我们介绍了一种新颖的方法，该方法在视觉信息搜索任务上取得了最先进的结果。我们的方法将法学硕士与三种类型的工具集成：（i）用于从图像中提取视觉信息的计算机视觉工具，（ii）用于检索开放世界知识和事实的网络搜索工具，以及（iii）用于收集相关信息的图像搜索工具来自与视觉上相似的图像相关的元数据。 AVIS 采用法学硕士支持的规划器在每个步骤中选择工具和查询。它还使用 LLM 支持的推理机来分析工具输出并提取关键信息。工作记忆组件在整个过程中保留信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoS​​FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3i Fnyn7oy8bB81TCbh_D-I3unIwgxswoS​​FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;AVIS 生成的工作流程示例，用于回答具有挑战性的视觉信息搜索问题。输入图像取自 Infoseek 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2 >;与之前的工作比较&lt;/h2>; &lt;p>;最近的研究（例如，&lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;、&lt;a href=&quot;https:// viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; 和 &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;）探索向法学硕士添加工具以实现多模式输入。这些系统遵循两个阶段的过程：规划（将问题分解为结构化程序或指令）和执行（使用工具收集信息）。尽管在基本任务中取得了成功，但这种方法在复杂的现实场景中常常会出现问题。 &lt;/p>; &lt;p>; 人们对应用法学硕士作为自主代理的兴趣也激增（例如，&lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; 和 &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;）。这些代理与环境交互，根据实时反馈进行调整并实现目标。然而，这些方法并没有限制每个阶段可以调用的工具，导致搜索空间巨大。因此，即使是当今最先进的法学硕士也可能陷入无限循环或传播错误。 AVIS 通过指导法学硕士使用来解决这个问题，并受到用户研究中人类决策的影响。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过用户研究为 LLM 决策提供依据&lt;/h2>; &lt;p>; 中的许多视觉问题数据集，例如 &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a >; 即使对人类来说也是一个挑战，通常需要各种工具和 API 的帮助。下面显示了 OK-VQA 数据集中的一个示例问题。我们进行了一项用户研究，以了解使用外部工具时人类的决策。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP 8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWx b9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们进行了一项用户研究，以了解人类在使用外部工具时的决策。图像取自 OK-VQA 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 用户配备了与我们的方法相同的工具集，包括 &lt;a href=&quot;https ://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022 /04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 和&lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;网络搜索&lt;/a >;。他们接收输入图像、问题、检测到的对象作物以及链接到图像搜索结果的按钮。这些按钮提供了有关检测到的对象作物的各种信息，例如知识图实体、相似的图像标题、相关产品标题和相同的图像标题。 &lt;/p>; &lt;p>; 我们记录用户操作和输出，并通过两种关键方式将其用作我们系统的指南。首先，我们通过分析用户做出的决策顺序构建一个转换图（如下所示）。该图定义了不同的状态并限制每个状态下可用的操作集。例如，在启动状态，系统只能采取以下三种操作之一：PALI 字幕、PALI VQA 或对象检测。其次，我们使用人类决策的例子来指导我们的规划者和推理者与相关的上下文实例，以提高我们系统的性能和有效性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-I CB-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png “ style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr 1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.png“宽度=“472”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS 转换图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;总体框架&lt;/h2>; &lt;p>;我们的方法采用动态决策策略，旨在响应视觉信息寻求查询。我们的系统具有三个主要组件。首先，我们有一个规划器来确定后续操作，包括适当的 API 调用及其需要处理的查询。其次，我们有一个&lt;em>;工作内存&lt;/em>;，它保留有关从 API 执行获得的结果的信息。最后，我们有一个推理器，其作用是处理 API 调用的输出。它确定所获得的信息是否足以产生最终响应，或者是否需要额外的数据检索。 &lt;/p>; &lt;p>; 每次需要决定使用哪种工具以及向其发送什么查询时，规划器都会执行一系列步骤。根据当前状态，规划器提供一系列潜在的后续行动。潜在的行动空间可能太大，使得搜索空间变得棘手。为了解决这个问题，规划器参考转换图来消除不相关的动作。规划器还排除之前已经采取并存储在工作记忆中的动作。 &lt;/p>; &lt;p>; 接下来，规划器收集一组相关的上下文示例，这些示例是根据人类之前在用户研究期间做出的决策组合而成的。通过这些示例以及保存从过去的工具交互中收集的数据的工作记忆，规划者可以制定提示。然后，提示被发送到法学硕士，法学硕士返回结构化答案，确定下一个要激活的工具以及要向其分派的查询。这种设计允许在整个过程中多次调用规划器，从而促进动态决策，逐渐导致回答输入查询。 &lt;/p>; &lt;p>; 我们使用推理器来分析工具执行的输出，提取有用的信息并决定工具输出属于哪个类别：提供信息、非信息或最终答案。我们的方法利用法学硕士以及适当的提示和上下文示例来执行推理。如果推理机断定它已准备好提供答案，它将输出最终响应，从而结束任务。如果它确定工具输出没有信息，它将返回给规划器以根据当前状态选择另一个操作。如果它发现工具输出有用，它将修改状态并将控制权转移回规划器以在新状态下做出新决策。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQf WpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height= “470”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSur SEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt; /td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS 采用动态决策策略来响应视觉信息寻求查询。&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估 AVIS &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; 数据集。如下所示，甚至是强大的视觉语言模型，例如 &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog。 com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;，在 Infoseek 上微调时无法产生高精度。我们的方法 (AVIS) 在没有微调的情况下，对该数据集的未见实体分割达到了 50.7% 的准确率。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2E ikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHj qYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Infoseek 数据集上的 AVIS 视觉问答结果。与之前基于 &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; 的基线相比，AVIS 实现了更高的准确率、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https:// arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在 OK-VQA 数据集上的结果如下所示。 AVIS 通过少量上下文示例实现了 60.2% 的准确率，高于之前的大多数作品。与在 OK-VQA 上微调的 PALI 模型相比，AVIS 的准确度较低，但相当。与 Infoseek 相比，AVIS 优于微调后的 PALI，这种差异是由于 OK-VQA 中的大多数问答示例依赖于常识知识而不是细粒度知识。因此，PaLI 能够将此类通用知识编码在模型参数中，并且不需要外部知识。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d 5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a 6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;A-OKVQA 上的视觉问答结果。与之前使用少样本或零样本学习的作品相比，AVIS 实现了更高的准确度，包括 &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single- Visual-language-model&quot;>;火烈鸟&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;巴利语&lt;/ a>; 和 &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;。 AVIS 还比之前在 OK-VQA 数据集上进行微调的大多数作品实现了更高的准确度，包括 &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language- pre.html&quot;>;REVEAL&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/ 2112.08614&quot;>;KAT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;，并获得接近微调&lt;a href=&quot;https的结果://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了一种新颖的方法，使法学硕士能够使用各种用于回答知识密集型视觉问题的工具。我们的方法以从用户研究中收集的人类决策数据为基础，采用了一个结构化框架，该框架使用法学硕士支持的规划器来动态决定工具选择和查询形成。由 LLM 驱动的推理机的任务是从所选工具的输出中处理和提取关键信息。我们的方法迭代地使用规划器和推理器来利用不同的工具，直到收集到回答视觉问题所需的所有必要信息。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Ziniu Hu、Ahmet Iscen 进行、孙晨、张凯伟、孙一洲、David A. Ross、Cordelia Schmid 和 Alireza Fathi。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/ feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/autonomous -visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel =&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; 标题=&quot;使用大语言模型进行自主视觉信息搜索&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqY oAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:总计>;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;发布>;2023-08-17T11:08:00.000 -07:00&lt;/已发布>;&lt;更新>;2023-08-17T11:08:24.346-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”term= &quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;神经网络剪枝组合优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队研究科学家 Hussein Hazimeh 和麻省理工学院研究生 Riade Benbaki&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDN jcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 现代神经网络在各种应用中都取得了令人印象深刻的性能，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model /&quot;>;语言、数学推理&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;愿景&lt;/a>; 。然而，这些网络通常使用需要大量计算资源的大型架构。这使得向用户提供此类模型变得不切实际，特别是在可穿戴设备和智能手机等资源有限的环境中。降低预训练网络的推理成本的&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;广泛使用的方法&lt;/a>;是通过删除一些网络来修剪它们它们的权重，不会显着影响效用。在标准神经网络中，每个权重定义两个神经元之间的连接。因此，在权重被修剪后，输入将通过较小的连接集传播，因此需要较少的计算资源。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw -bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBRYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEg D-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width= “1600”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FonAvjG2Hw J4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;原始网络与修剪后的网络。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;剪枝方法可以应用于网络训练过程的不同阶段：训练后、训练期间或训练之前（即权重初始化后立即）。在这篇文章中，我们重点关注训练后设置：给定一个预先训练的网络，我们如何确定应该修剪哪些权重？一种流行的方法是&lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;幅度剪枝&lt;/a>;，它删除幅度最小的权重。虽然有效，但该方法没有直接考虑删除权重对网络性能的影响。另一个流行的范例是&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;基于优化的剪枝&lt;/a>;，它根据权重的删除对损失函数的影响程度来删除权重。尽管在概念上很有吸引力，但大多数现有的基于优化的方法似乎都面临着性能和计算要求之间的严重权衡。进行粗略近似的方法（例如，假设对角线&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian_matrix&quot;>;Hessian 矩阵&lt;/a>;）可以很好地扩展，但性能相对较低。另一方面，虽然进行较少近似的方法往往表现更好，但它们的可扩展性似乎要差得多。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”中，介绍于 &lt;a href=&quot; https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;，我们描述了如何开发一种基于优化的方法来大规模修剪预训练的神经网络。 CHITA（代表“组合 Hessian 迭代阈值算法”）在可扩展性和性能权衡方面优于现有的剪枝方法，并且它通过利用多个领域的进步来做到这一点，包括 &lt;a href=&quot;https://en. wikipedia.org/wiki/High-Dimensional_statistics&quot;>;高维统计&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;组合优化&lt;/a>;和神经网络修剪。例如，CHITA 的速度比最先进的修剪方法快 20 倍到 1000 倍 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;，并将准确性提高 10 以上% 在许多设置中。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;贡献概述&lt;/h2>; &lt;p>; CHITA 相对于流行方法有两项显着的技术改进：&lt; /p>; &lt;ul>; &lt;li>;&lt;strong>;二阶信息的有效使用&lt;/strong>;：使用二阶信息的修剪方法（即，与&lt;a href=&quot;https://en.wikipedia. org/wiki/Second_derivative&quot;>;二阶导数&lt;/a>;）在许多设置中实现了最先进的技术。在文献中，此信息通常通过计算 Hessian 矩阵或其逆矩阵来使用，这种操作很难扩展，因为 Hessian 大小与权重数量成二次方。通过仔细的重新表述，CHITA 使用二阶信息，而无需显式计算或存储 Hessian 矩阵，从而实现更高的可扩展性。 &lt;/li>;&lt;li>;&lt;strong>;组合优化&lt;/strong>;：流行的基于优化的方法使用一种简单的优化技术，单独修剪权重，即，在决定修剪某个权重时，它们不考虑是否其他权重已被修剪。这可能会导致重要权重的修剪，因为当其他权重被修剪时，孤立地被认为不重要的权重可能会变得重要。 CHITA 通过使用更先进的组合优化算法来避免这个问题，该算法考虑了修剪一个权重如何影响其他权重。 &lt;/li>; &lt;/ul>; &lt;p>; 在下面的部分中，我们讨论 CHITA 的剪枝公式和算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;计算友好的剪枝公式&lt;/h2>; &lt;p>; 有许多可能的剪枝候选者，其中通过仅保留原始网络权重的子集来获得。令&lt;em>;k&lt;/em>;为用户指定的参数，表示要保留的权重数量。剪枝可以自然地表述为一个&lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;最佳子集选择&lt;/a>;（BSS）问题：在所有可能的剪枝候选者（即权重子集）中仅保留 &lt;em>;k&lt;/em>; 个权重，选择损失最小的候选者。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithv XGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY 1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;作为 BSS 问题的剪枝：在具有相同权重总数的所有可能的剪枝候选者中，最佳候选者被定义为损失最小的候选者。该图显示了四个候选者，但这个数字通常要大得多。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;解决原始损失函数上的剪枝BSS问题通常是通过计算来解决的棘手的。因此，与之前的工作类似，例如 &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; 和 &lt;a href=&quot; https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;，我们用 &lt;a href=&quot; 来近似损失https://en.wikipedia.org/wiki/Quadratic_form&quot;>;二次函数&lt;/a>;，使用二阶&lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;泰勒级数&lt; /a>;，其中 Hessian 矩阵是根据经验 &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher 信息矩阵&lt;/a>; 进行估计的。虽然梯度通常可以有效地计算，但由于其庞大的规模，计算和存储 Hessian 矩阵的成本过高。在文献中，通常通过对 Hessian 矩阵（例如，对角矩阵）和算法（例如，单独剪枝权重）做出限制性假设来应对这一挑战。 &lt;/p>; &lt;p>; CHITA 使用修剪问题的有效重新表述（使用二次损失的 BSS），避免显式计算 Hessian 矩阵，同时仍然使用该矩阵中的所有信息。这是通过利用经验费希尔信息矩阵的低&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;秩&lt;/a>;结构来实现的。这种重新表述可以被视为稀疏&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;线性回归&lt;/a>;问题，其中每个回归系数对应于神经网络中的特定权重。获得此回归问题的解决方案后，设置为零的系数将对应于应修剪的权重。我们的回归数据矩阵是 (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;)，其中 &lt;em>;n&lt;/em>; 是批次（子样本）大小，&lt;em>;p&lt;/em>; em>; 是原始网络中的权重数量。通常&lt;em>;n&lt;/em>; &lt;&lt; &lt;em>;p&lt;/em>;，因此使用此数据矩阵进行存储和操作比使用 (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian 操作的常见修剪方法更具可扩展性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdK vBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUf ysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;CHITA 将二次损失近似重新表述为线性回归 (LR) 问题，这需要昂贵的 Hessian 矩阵。 LR 的数据矩阵在 &lt;em>;p&lt;/em>; 中呈线性，这使得重构比原始二次近似更具可扩展性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可扩展的优化算法&lt;/h2>; &lt;p>; CHITA 在以下稀疏性约束下将剪枝减少为线性回归问题：最多 &lt; em>;k&lt;/em>; 个回归系数可以不为零。为了获得此问题的解决方案，我们考虑对著名的&lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;迭代硬阈值&lt;/a>; (IHT) 算法进行修改。 IHT 执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;梯度下降&lt;/a>;，每次更新后执行以下后处理步骤：Top-&lt;em >;k&lt;/em>;（即，具有最大幅度的&lt;em>;k&lt;/em>;系数）设置为零。 IHT 通常会为问题提供良好的解决方案，并且它会迭代探索不同的剪枝候选对象并联合优化权重。 &lt;/p>; &lt;p>; 由于问题的规模，具有恒定&lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;学习率&lt;/a>;的标准 IHT 可能会非常慢收敛。为了更快地收敛，我们开发了一种新的&lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;线搜索&lt;/a>;方法，该方法利用问题结构来找到合适的学习率，即导致损失足够大的减少。我们还采用了多种计算方案来提高 CHITA 的效率和二阶近似的质量，从而产生了一个改进的版本，我们称之为 CHITA++。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们将 CHITA 的运行时间和准确性与几种状态进行比较使用不同架构的最先进的修剪方法，包括 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1704.04861 &quot;>;移动网络&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;运行时间&lt;/strong>;：CHITA 比执行联合优化的同类方法更具可扩展性（而不是单独修剪权重）。例如，CHITA 在剪枝 ResNet 时的加速比可以达到 1000 倍以上。 &lt;/p>; &lt;p>; &lt;strong>;后剪枝精度&lt;/strong>;：&lt;strong>; &lt;/strong>;下面，我们比较了 CHITA 和 CHITA++ 与幅度剪枝（MP）的性能，&lt;a href=&quot;https: //arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF) 和&lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;组合脑外科医生&lt;/a>; (CBS)，用于修剪 70% 的模型权重。总体而言，我们看到 CHITA 和 CHITA++ 取得了良好的改进。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1 aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz- KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlk Fi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ResNet20 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-标题容器&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_ AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img边框=“0”数据原始高度=“742”数据原始宽度=“1200”高度=“396”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA -5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-thHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-CP-w2OUUDsPUNMBhGCTkSBFjV6ODFY60 K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MobileNet 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们报告修剪更大网络的结果：ResNet50（在此网络上，一些ResNet20 图中列出的方法无法扩展）。这里我们与幅度剪枝和&lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;进行比较。下图显示，CHITA 在各种稀疏度水平下实现了更好的测试精度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU 5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6J MfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试使用不同方法获得的修剪网络的准确性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论、局限性和未来工作&lt;/h2>; &lt;p>; 我们提出了 CHITA，一种基于优化的方法用于修剪预先训练的神经网络。 CHITA 通过有效地使用二阶信息并借鉴组合优化和高维统计的思想，提供可扩展性和有竞争力的性能。 &lt;/p>; &lt;p>; CHITA 专为&lt;em>;非结构化修剪&lt;/em>;而设计，可以去除任何重量。理论上，非结构化剪枝可以显着降低计算要求。然而，在实践中实现这些减少需要支持稀疏计算的特殊软件（可能还有硬件）。相比之下，结构化修剪会删除神经元等整个结构，可能会提供在通用软件和硬件上更容易实现的改进。将 CHITA 扩展到结构化修剪将会很有趣。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是 Google 之间研究合作的一部分和麻省理工学院。感谢 Rahul Mazumder、Natalia Ponomareva、Wenyu Chen、Xiang Men、Zhe Zhu 和 Sergei Vassivitskii 在准备这篇文章和论文时提供的帮助。还要感谢 John Guilyard 在本文中创建图形。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4779594044050650231/comments/default&quot; rel= &quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/neural-network-pruning-with.html#comment -form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel= “编辑”类型=“application/atom+xml”/>;&lt;link href=“http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231”rel=“self”type=“application/atom” +xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;组合优化神经网络剪枝&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS 2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K /s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;发布>;2023-08-15T12:59:00.001-07:00&lt;/已发布>;&lt;更新>;2023-08-15T13:00:54.887-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt; /类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“推荐系统”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;社交网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;研究：社会意识暂时因果解码器推荐系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：Google 研究工程师 Eltayeb Ahmed 和高级研究科学家 Subhrajit Roy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt -PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx 8AO1_/s837/study-hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 阅读对于年轻学生有很多好处，例如&lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure。 pdf&quot;>;更好的语言和生活技能&lt;/a>;，并且快乐阅读已被证明与&lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;学业成功相关&lt; /a>;.此外，学生们还表示，阅读&lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;情绪健康得到改善&lt;/a>;，并且&lt;a href=&quot;https:// eric.ed.gov/?id=ED496343&quot;>;更好的常识和对其他文化的更好的理解&lt;/a>;。由于线上和线下有大量的阅读材料，找到适合年龄的、相关的和引人入胜的内容可能是一项具有挑战性的任务，但帮助学生做到这一点是让他们参与阅读的必要步骤。向学生提供相关阅读材料的有效推荐有助于学生持续阅读，而这正是机器学习 (ML) 可以提供帮助的地方。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 机器学习已广泛应用于构建&lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592 -5&quot;>;推荐系统&lt;/a>;，适用于各种类型的数字内容，从视频到书籍再到电子商务项目。推荐系统在一系列数字平台上使用，以帮助向用户展示相关且有吸引力的内容。在这些系统中，机器学习模型经过训练，可以根据用户偏好、用户参与度和推荐项目单独向每个用户推荐项目。这些数据为模型提供了强大的学习信号，使其能够推荐可能感兴趣的项目，从而改善用户体验。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;研究：社交意识暂时因果解码器推荐系统&lt;/a>;”中，我们提出了一个有声读物的内容推荐系统在教育环境中考虑阅读的社会性质。我们与 &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 合作开发了 STUDY 算法，这是一个教育非营利组织，旨在促进阅读障碍学生的阅读，通过学校向学生提供有声读物广泛的订阅计划。利用 Learning Ally 图书馆中的各种有声读物，我们的目标是帮助学生找到合适的内容，以帮助提高他们的阅读体验和参与度。由于一个人的同龄人当前正在阅读的内容会对他们感兴趣的阅读内容产生重大影响，因此我们共同处理同一教室中学生的阅读参与历史。这使得我们的模型能够从有关学生本地社交群体（在本例中为他们的教室）当前趋势的实时信息中受益。 &lt;/p>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 拥有一个大型数字图书馆，其中包含针对以下人群的精选有声读物：学生，使其非常适合构建社会推荐模型，以帮助提高学生的学习成果。我们收到了两年的匿名有声读物消费数据。数据中的所有学生、学校和分组都是匿名的，仅通过随机生成的 ID 进行识别，无法通过 Google 追溯到真实实体。此外，所有潜在可识别元数据仅以聚合形式共享，以保护学生和机构不被重新识别。这些数据包括学生与有声读物互动的带时间戳的记录。对于每次交互，我们都有一个匿名的学生 ID（包括学生的年级和匿名的学校 ID）、有声读物标识符和日期。虽然许多学校将单一年级的学生分布在多个教室中，但我们利用此元数据做出简化的假设，即同一所学校和同一年级的所有学生都在同一教室。虽然这为建立更好的社交推荐模型提供了基础，但值得注意的是，这并不能让我们重新识别个人、班级群体或学校。 &lt;/p>; &lt;br />; &lt;h2>;STUDY 算法&lt;/h2>; &lt;p>; 我们将推荐问题描述为&lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;点击率&lt;/a>; 预测问题，我们对用户与每个特定项目交互的条件概率进行建模，条件是 1）用户和项目特征以及 2）当前用户的项目交互历史序列。 &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;之前的工作&lt;/a>;建议&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;基于 /a>; 的模型是 Google Research 开发的一种广泛使用的模型类，非常适合对这个问题进行建模。当单独处理每个用户时，这将成为一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;自回归序列建模问题&lt;/a>;。我们使用这个概念框架来建模我们的数据，然后扩展这个框架来创建研究方法。 &lt;/p>; &lt;p>; 虽然这种点击率预测方法可以对单个用户过去和未来的项目偏好之间的依赖关系进行建模，并且可以在训练时学习用户之间的相似性模式，但它无法在推理时对不同用户之间的依赖关系进行建模时间。为了认识阅读的社会性并弥补这一缺陷，我们开发了 STUDY 模型，该模型将每个学生阅读的多个书籍序列连接成一个序列，该序列收集单个教室中多个学生的数据。 &lt;/p>; &lt;p>; 然而，如果要通过 Transformer 建模，这种数据表示需要仔细研究。在 Transformer 中，注意力掩码是控制哪些输入可用于通知哪些输出的预测的矩阵。使用序列中的所有先验标记来通知输出预测的模式导致传统上在因果解码器中发现的上三角注意矩阵。然而，由于输入 STUDY 模型的序列不是按时间排序的，即使它的每个组成子序列都是标准的&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;因果解码器&lt;/a>;不再适合这个序列。当尝试预测每个标记时，模型不允许关注序列中位于其之前的每个标记；其中一些令牌可能具有较晚的时间戳，并且包含在部署时不可用的信息。 &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tq Rd-ZWDXOCRpHAy0FZE9sV0xB8reyJ-- Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width = =第1787章pHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot;宽度=&quot; 400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们显示了通常用于因果解码器。每列代表一个输出，每列代表一个输出。特定位置处的矩阵条目的值为 1（显示为蓝色）表示模型在预测相应列的输出时可以观察到该行的输入，而值为 0（显示为白色）表示相反的情况.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; STUDY 模型建立在因果变换器的基础上，通过将三角矩阵注意掩码替换为灵活的注意掩码，其值基于时间戳，以允许关注不同的子序列。 Compared to a regular transformer, which would not allow attention across different subsequences and would have a triangular matrix mask within sequence, STUDY maintains a causal triangular attention matrix within a sequence and has flexible values across sequences with values that depend on timestamps. Hence, predictions at any output point in the sequence are informed by all input points that occurred in the past relative to the current time point, regardless of whether they appear before or after the current input in the sequence. This causal constraint is important because if it is not enforced at train time, the model could potentially learn to make predictions using information from the future, which would not be available for a real world deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In (a) we show a sequential autoregressive transformer with causal attention that processes each user individually; in (b) we show an equivalent joint forward pass that results in the same computation as (a); and finally, in (c) we show that by introducing new nonzero values (shown in purple) to the attention mask we allow information to flow across users. We do this by allowing a prediction to condition on all interactions with an earlier timestamp, irrespective of whether the interaction came from the same user or not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We used the Learning Ally dataset to train the STUDY model along with multiple baselines for comparison. We implemented an autoregressive click-through rate transformer decoder, which we refer to as “Individual”, a &lt;em>;k&lt;/em>;-nearest neighbor baseline (KNN), and a comparable social baseline, social attention memory network (SAMN). We used the data from the first school year for training and we used the data from the second school year for validation and testing. &lt;/p>; &lt;p>; We evaluated these models by measuring the percentage of the time the next item the user actually interacted with was in the model&#39;s top &lt;em>;n&lt;/em>; recommendations, ie, hits@&lt;em>;n,&lt;/em>; for different values of &lt;em>;n&lt;/em>;. In addition to evaluating the models on the entire test set we also report the models&#39; scores on two subsets of the test set that are more challenging than the whole data set. We observed that students will typically interact with an audiobook over multiple sessions, so simply recommending the last book read by the user would be a strong trivial recommendation. Hence, the first test subset, which we refer to as “non-continuation”, is where we only look at each model&#39;s performance on recommendations when the students interact with books that are different from the previous interaction. We also observe that students revisit books they have read in the past, so strong performance on the test set can be achieved by restricting the recommendations made for each student to only the books they have read in the past. Although there might be value in recommending old favorites to students, much value from recommender systems comes from surfacing content that is new and unknown to the user. To measure this we evaluate the models on the subset of the test set where the students interact with a title for the first time. We name this evaluation subset “novel”. &lt;/p>; &lt;p>; We find that STUDY outperforms all other tested models across almost every single slice we evaluated against. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In this figure we compare the performance of four models, Study, Individual, KNN and SAMN. We measure the performance with hits@5, ie, how likely the model is to suggest the next title the user read within the model&#39;s top 5 recommendations. We evaluate the model on the entire test set (all) as well as the novel and non-continuation splits. We see STUDY consistently outperforms the other three models presented across all splits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Importance of appropriate grouping&lt;/h2>; &lt;p>; At the heart of the STUDY algorithm is organizing users into groups and doing joint inference over multiple users who are in the same group in a single forward pass of the model. We conducted an ablation study where we looked at the importance of the actual groupings used on the performance of the model. In our presented model we group together all students who are in the same grade level and school. We then experiment with groups defined by all students in the same grade level and district and also place all students in a single group with a random subset used for each forward pass. We also compare these models against the Individual model for reference. &lt;/p>; &lt;p>; We found that using groups that were more localized was more effective, with the school and grade level grouping outperforming the district and grade level grouping. This supports the hypothesis that the STUDY model is successful because of the social nature of activities such as reading — people&#39;s reading choices are likely to correlate with the reading choices of those around them. Both of these models outperformed the other two models (single group and Individual) where grade level is not used to group students. This suggests that data from users with similar reading levels and interests is beneficial for performance. &lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; This work is limited to modeling recommendations for user populations where the social connections are assumed to be homogenous. In the future it would be beneficial to model a user population where relationships are not homogeneous, ie, where categorically different types of relationships exist or where the relative strength or influence of different relationships is known. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of researchers, software engineers and educational subject matter experts. We thank our co-authors: Diana Mincu, Lauren Harrell, and Katherine Heller from Google. We also thank our colleagues at Learning Ally, Jeff Ho, Akshat Shah, Erin Walker, and Tyler Bastian, and our collaborators at Google, Marc Repnyek, Aki Estrella, Fernando Diaz, Scott Sanner, Emily Salkey and Lev Proleev.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;STUDY: Socially aware temporally causal decoder recommender systems&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;published>;2023-08-09T11:32:00.001-07:00&lt;/published>;&lt;updated>;2023-08-09T12:26:53.831-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advances in document understanding&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sandeep Tata, Software Engineer, Google Research, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; 过去几年，能够自动处理复杂业务文档并将其转换为结构化对象的系统取得了快速进展。一个可以从收据、保险报价等文档中&lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;自动提取数据&lt;/a>;的系统和财务报表，通过避免容易出错的手动工作，有可能显着提高业务工作流程的效率。基于 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; 架构的最新模型已经展示了&lt;a href=&quot; https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;准确度显着提升&lt;/a>;。更大的模型，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;，也被用来进一步简化这些业务工作流程。然而，学术文献中使用的数据集未能捕捉到现实世界用例中遇到的挑战。因此，学术基准报告了很强的模型准确性，但这些相同的模型在用于复杂的现实应用程序时表现不佳。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU：视觉丰富文档理解的基准&lt; /a>;”，在 &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>; 上发布，我们宣布发布新的&lt;a href=&quot;https://research.google /resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集旨在弥补这一差距，帮助研究人员更好地跟踪文档理解任务的进展。根据经常使用文档理解模型的现实文档类型，我们列出了良好的文档理解基准的五个要求。然后，我们描述了研究界当前使用的大多数数据集为何无法满足其中一项或多项要求，而 VRDU 却满足了所有这些要求。我们很高兴地宣布公开发布 VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;数据集&lt;/a>;和&lt;a href=&quot;https ://github.com/google-research-datasets/vrdu&quot;>;评估代码&lt;/a>;，采用&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;知识共享许可&lt;/a>;一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基准要求&lt;/h2>; &lt;p>; 首先，我们比较了最先进的模型准确性（例如，使用 &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/ a>;）关于现实世界用例与学术基准（例如，&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.a>;） net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;）。我们观察到，最先进的模型与学术基准结果不符，并且在现实世界中的准确性要低得多。接下来，我们将经常使用文档理解模型的典型数据集与学术基准进行比较，并确定了五个数据集要求，使数据集能够更好地捕获现实应用程序的复杂性：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;丰富模式：&lt;/strong>;在实践中，我们看到了各种用于结构化提取的丰富模式。实体具有不同的数据类型（数字、字符串、日期等），这些数据类型可能是必需的、可选的或在单个文档中重复，甚至可能是嵌套的。像（标题、问题、答案）这样的简单平面模式的提取任务并不能反映实践中遇到的典型问题。 &lt;/li>;&lt;li>;&lt;strong>;布局丰富的文档：&lt;/strong>;文档应该具有复杂的布局元素。实际设置中的挑战来自以下事实：文档可能包含表格、键值对、在单列和双列布局之间切换、不同部分具有不同的字体大小、包括带有标题甚至脚注的图片。将此与大多数文档以句子、段落和带有节标题的章节组织的数据集进行对比 - 这些文档通常是 &lt;a href=&quot;https://arxiv.org/abs 上经典自然语言处理文献的焦点/2007.14062&quot;>;长&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;输入&lt;/a>;。 &lt;/li>;&lt;li>;&lt;strong>;多样化模板：&lt;/strong>;基准测试应包括不同的结构布局或模板。对于大容量模型来说，通过记忆结构从特定模板中提取数据是微不足道的。然而，在实践中，人们需要能够推广到新的模板/布局，这是基准测试中的训练-测试分割应该衡量的一种能力。 &lt;/li>;&lt;li>;&lt;strong>;高质量 OCR&lt;/strong>;：文档应具有高质量&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;光学字符识别&lt;/a>; (OCR) 结果。我们此基准测试的目标是专注于 VRDU 任务本身，并排除 OCR 引擎选择带来的可变性。 &lt;/li>;&lt;li>;&lt;strong>;令牌级注释&lt;/strong>;：文档应包含可以映射回相应输入文本的真实注释，以便每个令牌都可以注释为相应实体的一部分。这与简单地提供要为实体提取的值的文本形成对比。这是生成干净的训练数据的关键，我们不必担心与给定值的偶然匹配。例如，在某些收据中，如果税额为零，“税前总计”字段可能具有与“总计”字段相同的值。具有令牌级别注释会阻止我们生成训练数据，其中匹配值的两个实例都被标记为“total”字段的真实值，从而产生嘈杂的示例。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn 5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600 /Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6Ai WX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU 数据集和任务&lt;/h2>; &lt;p>; VRDU 数据集是两个公开可用数据集的组合，&lt;a href=&quot;https://efile.40%;&quot;>; fara.gov/ords/fara/f?p=1235:10&quot;>;注册表&lt;/a>;和&lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;广告购买表单&lt;/a>;。这些数据集提供了代表现实世界用例的示例，并满足上述五个基准要求。 &lt;/p>; &lt;p>; Ad-buy Forms 数据集包含 641 个包含政治广告详细信息的文档。每份文件都是由电视台和竞选团队签署的发票或收据。文档使用表格、多列和键值对来记录广告信息，例如产品名称、播放日期、总价、发布日期和时间。 &lt;/p>; &lt;p>; 登记表数据集包含 1,915 个文档，其中包含有关外国代理人在美国政府登记的信息。每份文件都记录了涉及需要公开披露的活动的外国代理人的基本信息。内容包括注册人名称、相关部门地址、活动目的等详细信息。 &lt;/p>; &lt;p>; 我们从公众&lt;a href=&quot;https://www.fcc.gov/&quot;>;联邦通信委员会&lt;/a>; (FCC) 和&lt;a href=&quot; https://www.justice.gov/nsd-fara&quot;>;外国代理人登记法&lt;/a>; (FARA) 网站，并使用 &lt;a href=&quot;https://cloud.google.com/ 将图像转换为文本&quot;>;Google Cloud 的&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;。我们丢弃了少量几页长的文档，并且处理在两分钟内没有完成。这也使我们能够避免发送很长的文档进行手动注释——对于单个文档来说，这项任务可能需要一个多小时。然后，我们为具有文档标记任务经验的注释者团队定义了模式和相应的标记指令。 &lt;/p>; &lt;p>; 还向注释者提供了一些我们自己标记的示例标记文档。该任务要求注释者检查每个文档，围绕每个文档架构中实体的每次出现绘制边界框，并将该边界框与目标实体相关联。第一轮标注后，一组专家被指派对结果进行审核。修正后的结果包含在已发布的 VRDU 数据集中。有关标记协议和每个数据集架构的更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG 2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i7 2av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;现有学术基准 (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.net/ pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;、&lt;a href=&quot;https:// /arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;、&lt;a href=&quot;https ://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;）未达到我们为项目确定的五项要求中的一项或多项良好的文档理解基准。 VRDU 满足了所有这些要求。请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;，了解每个数据集的背景以及它们如何未能满足一项或多项要求的讨论。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们构建了四个不同的模型训练集，分别有 10、50、100 和 200 个样本。然后，我们使用三个任务（如下所述）评估 VRDU 数据集：(1) 单模板学习、(2) 混合模板学习和 (3) 看不见的模板学习。对于每项任务，我们在测试集中包含 300 个文档。我们使用测试集上的 &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 分数&lt;/a>; 来评估模型。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;单模板学习&lt;/em>; (STL)：这是最简单的场景，其中训练、测试和验证集仅包含单个模板。这个简单的任务旨在评估模型处理固定模板的能力。当然，我们期望此任务的 F1 分数非常高（0.90+）。 &lt;/li>;&lt;li>;&lt;em>;混合模板学习&lt;/em>;（MTL）：此任务与大多数相关论文使用的任务类似：训练集、测试集和验证集都包含属于同一集的文档模板。我们从数据集中随机抽取文档并构建分割，以确保每个模板的分布在采样过程中不会改变。 &lt;/li>;&lt;li>;&lt;em>;未见模板学习&lt;/em>;（UTL）：这是最具挑战性的设置，我们评估模型是否可以泛化到未见模板。例如，在注册表数据集中，我们使用三个模板中的两个来训练模型，并使用其余一个来测试模型。训练、测试和验证集中的文档来自不相交的模板集。据我们所知，以前的基准测试和数据集没有明确提供这样的任务，旨在评估模型泛化到训练期间未见过的模板的能力。 &lt;/li>; &lt;/ul>; &lt;p>; 目标是能够评估模型的数据效率。在我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;中，我们比较了使用 STL、MTL 和 UTL 任务的两个最新模型，并提出了三个观察结果。首先，与其他基准测试不同，VRDU 具有挑战性，并表明模型有很大的改进空间。其次，我们表明，即使是最先进的模型，few-shot 性能也低得惊人，即使是最好的模型，其 F1 分数也低于 0.60。第三，我们表明模型很难处理结构化的重复字段，并且在这些字段上的表现特别差。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们发布了新的&lt;a href=&quot;https:// Research.google/resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集，可帮助研究人员更好地跟踪文档理解任务的进度。我们描述了为什么 VRDU 更好地反映了该领域的实际挑战。我们还提出了实验，表明 VRDU 任务具有挑战性，并且与文献中通常使用的数据集（典型的 F1 分数为 0.90+）相比，最近的模型有很大的改进空间。我们希望 VRDU 数据集和评估代码的发布有助于研究团队推进文档理解的最新技术。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;非常感谢王子龙、周一超、魏Wei 和 Chen-Yu Lee 与 Sandeep Tata 共同撰写了这篇论文。感谢 Marc Najork、Riham Mansour 以及 Google Research 和 Cloud AI 团队的众多合作伙伴提供了宝贵的见解。感谢 John Guilyard 创建本文中的动画。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /ai.googleblog.com/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;文档理解方面的进展&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google人工智能&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http ://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/作者>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV 8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;发布>;2023-08-08T14:02:00.000-07:00&lt;/发布>;&lt;更新>;2023-08-08T14： 02:34.659-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;AdaTape：具有自适应计算和动态读写的基础模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：薛福兆、研究实习生和研究科学家 Mostafa Dehghani，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIq JhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;显示：无；” />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;自适应计算&lt;/a>;是指机器学习系统根据环境变化调整其行为的能力。虽然传统的神经网络具有固定的功能和计算能力，即它们花费相同数量的 FLOP 来处理不同的输入，但具有自适应和动态计算的模型会根据输入的复杂性来调整其专用于处理每个输入的计算预算。输入。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 神经网络中的自适应计算具有吸引力有两个关键原因。首先，引入适应性的机制提供了归纳偏差，可以在解决一些具有挑战性的任务中发挥关键作用。例如，为不同的输入启用不同数量的计算步骤对于解决需要不同深度的建模层次结构的算术问题至关重要。其次，它使从业者能够通过动态计算提供的更大灵活性来调整推理成本，因为可以调整这些模型以花费更多的 FLOP 来处理新输入。 &lt;/p>; &lt;p>; 神经网络可以通过对各种输入使用不同的函数或计算预算来实现自适应。深度神经网络可以被认为是一个基于输入及其参数输出结果的函数。为了实现自适应函数类型，根据输入选择性地激活参数子集，这一过程称为条件计算。在 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>; 的研究中探索了基于函数类型的自适应性，其中每个输入的稀疏激活参数样本是通过路由确定的。自适应计算的另一个研究领域涉及动态计算预算。与标准神经网络（例如 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;）不同，&lt;a href= &quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling -to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;其计算预算对于不同的样本是固定的，&lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;最近的研究&lt;/a>;表明，自适应计算预算可以提高 Transformer 不足的任务的性能。其中许多工作通过使用动态深度来分配计算预算来实现自适应性。例如，提出了自适应计算时间（ACT）算法来为递归神经网络提供自适应计算预算。 &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;通用转换器&lt;/a>;将 ACT 算法扩展到转换器，使计算预算取决于用于每个输入示例或标记的转换器层数。最近的研究，例如 &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;，在改进动态停止机制的同时也遵循类似的方法。 &lt;/p>; &lt;p>; 在论文“&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;具有弹性输入序列的自适应计算&lt;/a>;”中，我们介绍了一种利用自适应计算的新模型，称为&lt;em>;AdaTape&lt;/em>;。该模型是基于 Transformer 的架构，使用一组动态令牌来创建弹性输入序列，与以前的作品相比，提供了关于适应性的独特视角。 AdaTape 使用自适应磁带读取机制来根据输入的复杂性确定添加到每个输入的不同数量的磁带标记。 AdaTape 实现起来非常简单，提供了一个有效的旋钮来在需要时提高准确性，而且与&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;其他自适应基线&lt;/a相比也更加高效>; 因为它直接将自适应性注入到输入序列中，而不是模型深度中。最后，Adatape 在标准任务（例如图像分类）以及算法任务上提供更好的性能，同时保持良好的质量和成本权衡。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;具有弹性输入序列的自适应计算转换器&lt;/h2>; &lt;p>; AdaTape 使用两种自适应函数类型和动态计算预算。具体来说，对于标记化后的一批输入序列（例如，来自视觉变换器中的图像的非重叠补丁的线性投影），AdaTape 使用表示每个输入的向量来动态选择可变大小的磁带标记序列。 &lt;/p>; &lt;p>; AdaTape 使用称为“磁带库”的令牌库来存储通过自适应磁带读取机制与模型交互的所有候选磁带令牌。我们探索了两种不同的创建磁带库的方法：输入驱动的磁带库和可学习的磁带库。 &lt;/p>; &lt;p>; 输入驱动库的总体思想是从输入中提取一组令牌，同时采用与原始模型令牌生成器不同的方法将原始输入映射到输入标记的序列。这使得能够动态地按需访问来自使用不同观点（例如，不同图像分辨率或不同抽象级别）获得的输入的信息。 &lt;/p>; &lt;p>; 在某些情况下，不同抽象级别的标记化是不可能的，因此输入驱动的磁带库是不可行的，例如当很难进一步拆分 &lt;a href=&quot; 中的每个节点时https://arxiv.org/abs/2012.09699&quot;>;图形转换器&lt;/a>;。为了解决这个问题，AdaTape 提供了一种更通用的方法，通过使用一组可训练向量作为磁带令牌来生成磁带库。这种方法被称为可学习库，可以被视为嵌入层，其中模型可以根据输入示例的复杂性动态检索标记。可学习的银行使 AdaTape 能够生成更灵活的磁带库，使其能够根据每个输入示例的复杂性动态调整其计算预算，例如，更复杂的示例从磁带库中检索更多的令牌，这使得模型不会只使用存储在银行中的知识，但由于输入现在更大，所以还要花费更多的 FLOP 来处理它。 &lt;/p>; &lt;p>; 最后，选定的磁带标记将附加到原始输入并馈送到以下转换器层。对于每个转换器层，在所有输入和磁带标记上使用相同的多头注意力。然而，使用了两种不同的前馈网络（FFN）：一种用于来自原始输入的所有令牌，另一种用于所有磁带令牌。通过对输入和磁带令牌使用单独的前馈网络，我们观察到质量稍好一些。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfda EU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxf ccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AdaTape 概述。对于不同的样本，我们从磁带库中挑选不同数量的不同标记。磁带库可以由输入驱动，例如通过提取一些额外的细粒度信息，或者它可以是一组可训练向量。自适应磁带读取用于针对不同的输入递归地选择不同长度的磁带令牌序列。然后将这些标记简单地附加到输入并馈送到变压器编码器。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;AdaTape 提供有用的归纳偏差&lt;/h2>; &lt;p>; 我们在奇偶校验上评估 AdaTape，这对于标准 Transformer 来说是一项非常具有挑战性的任务，以研究 AdaTape 中归纳偏差的影响。对于奇偶校验任务，给定序列 1、0 和 -1，模型必须预测序列中 1 数量的偶数或奇数。 Parity 是最简单的非计数器自由或&lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;周期性正则语言&lt;/a>;，但也许令人惊讶的是，标准 Transformer 无法解决该任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3 EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36 zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;对奇偶校验任务的评估。标准 Transformer 和 Universal Transformer 无法执行此任务，两者都显示出随机猜测基线水平的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;尽管进行了短期评估，对于简单的序列，标准 Transformer 和通用 Transformer 都无法执行奇偶校验任务，因为它们无法在模型中维护计数器。然而，AdaTape 的性能优于所有基线，因为它在其输入选择机制中融入了轻量级循环，提供了归纳偏差，可以实现计数器的隐式维护，这在标准 Transformer 中是不可能的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图像分类评估&lt;/h2>; &lt;p>; 我们还在图像分类任务上评估 AdaTape。为此，我们从头开始在 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; 上训练 AdaTape。下图显示了 AdaTape 和基线方法的准确性，包括 &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>; 和 Universal Transformer ViT（UViT 和 U2T）与它们的速度（以每秒由每个代码处理的图像数量来衡量）。在质量和成本权衡方面，AdaTape 的性能比替代自适应变压器基线要好得多。就效率而言，较大的 AdaTape 模型（就参数数量而言）比较小的基线更快。这些结果与&lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;之前的工作&lt;/a>;的发现一致，该发现表明自适应模型深度架构不太适合许多加速器，例如热塑性聚氨酯。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7Z Ih6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H -Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473 “高度=“285”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd- H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; 宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们通过从头开始在 ImageNet 上进行训练来评估 AdaTape 。对于&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;，我们不仅从论文中报告了他们的结果，还通过从头开始训练重新实​​现了A-ViT，即，A-ViT（我们的）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A AdaTape 行为的研究&lt;/h2>; &lt;p>; 除了奇偶校验任务和 ImageNet-1K 上的性能之外，我们还评估了 AdaTape 在 &lt;a href=&quot;https:// /arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>;验证集。为了更好地理解模型的行为，我们将输入驱动银行上的代币选择结果可视化为热图，其中较浅的颜色意味着该位置被更频繁地选择。热图显示 AdaTape 更频繁地选择中心补丁。这与我们的先验知识相一致，因为中心补丁通常包含更多信息，尤其是在具有自然图像的数据集的背景下，其中主要对象位于图像的中间。这一结果凸显了 AdaTape 的智能性，因为它可以有效地识别信息更丰富的补丁并确定优先级，以提高其性能。 &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgk R2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/ image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYq crCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr >;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们可视化 AdaTape-B/32（左）和 AdaTape-B/16（右）的磁带令牌选择热图。颜色越热/越浅，意味着该位置的补丁被更频繁地选择。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; AdaTape 的特点是自适应磁带读取机制生成的弹性序列长度。这还引入了一种新的感应偏置，使 AdaTape 能够解决对标准变压器和现有自适应变压器都具有挑战性的任务。通过对图像识别基准进行全面的实验，我们证明了当计算保持恒定时，AdaTape 优于标准转换器和自适应架构转换器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Mostafa Dehghani ，现供职于 Google DeepMind。 &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTape：具有自适应计算和动态读写的基础模型&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhd Qrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudiEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72- c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4436946873570093049&lt;/id>;&lt;已发布>;2023-08-03T11:24:00.001-07:00&lt;/已发布>;&lt;已更新>; 2023-08-03T11:24:40.465-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“AI”>;&lt;/类别>;&lt;类别方案=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;健康&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;多模式医疗人工智能&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：健康人工智能主管 Greg Corrado 、Google 研究部和 Google 研究部工程与研究副总裁 Yossi Matias&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjm G9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3 /s1600/medpalm.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 医学本质上是一门多模式学科。在提供护理时，临床医生通常会解读各种模式的数据，包括医学图像、临床记录、实验室测试、电子健康记录、基因组学等。在过去十年左右的时间里，人工智能系统在特定模式下的特定任务上取得了专家级的性能——一些人工智能系统&lt;a href=&quot;https://www.nature.com/articles/s41591 -019-0447-x&quot;>;处理 CT 扫描&lt;/a>;，而其他人&lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;分析高倍病理切片&lt;/a>;，还有一些&lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc2112090&quot;>;寻找罕见的遗传变异&lt;/a>;。这些系统的输入往往是复杂的数据，例如图像，它们通常提供结构化输出，无论是离散等级的形式还是&lt;a href=&quot;https://www.nature.com/articles/s41591-018- 0107-6&quot;>;密集图像分割掩模。&lt;/a>;同时，大型语言模型 (LLM) 的容量和功能具有&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;变得如此先进&lt;/a>;，以至于他们通过用简单的语言进行解释和回应，表现出了对医学知识的理解和专业知识。但是，我们如何将这些功能结合在一起来构建可以利用所有这些来源的信息的医疗人工智能系统？ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在今天的博文中，我们概述了为法学硕士带来多模式能力的一系列方法，并分享了关于构建多模式医学法学硕士的可处理性的一些令人兴奋的结果，正如最近的三篇研究论文所述。这些论文反过来概述了如何将&lt;em>;从头&lt;/em>;模式引入法学硕士，如何将最先进的医学成像基础模型移植到对话式法学硕士上，以及建立法学硕士的第一步真正的多模式医疗人工智能系统。如果成功成熟，多模式医学法学硕士可能会成为涵盖专业医学、医学研究和消费者应用的新辅助技术的基础。与我们之前的工作一样，我们强调需要与医学界和医疗保健生态系统合作仔细评估这些技术。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;一系列方法&lt;/h2>; &lt;p>; 最近提出了几种构建多模式法学硕士的方法月 [&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;1&lt;/a>;，&lt;a href=&quot;https:// /www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;2&lt;/a>;，&lt;a href=&quot;https://ai.googleblog.com/2023 /03/palm-e-embodied-multimodal-language.html&quot;>;3&lt;/a>;]，毫无疑问，新方法将在一段时间内继续出现。为了了解为医疗人工智能系统带来新模式的机会，我们将考虑三种广泛定义的方法：工具使用、模型移植和通才系统。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s1600/image1.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8 Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;工具使用&lt;/h2>; &lt;p>; 在 &lt;em >;工具使用方法，一个中心医学法学硕士将各种模式的数据分析外包给一组针对这些任务独立优化的软件子系统：工具。工具使用的常见助记示例是教法学硕士使用计算器而不是自己做算术。在医疗领域，面对胸部 X 光检查的医学法学硕士可以将该图像转发到放射学人工智能系统并整合该响应。这可以通过子系统提供的应用程序编程接口（API）来完成，或者更奇特的是，两个具有不同专业的医疗人工智能系统进行对话来完成。 &lt;/p>; &lt;p>; 这种方法有一些重要的好处。它允许子系统之间实现最大的灵活性和独立性，使卫生系统能够根据经过验证的子系统性能特征在技术提供商之间混合和匹配产品。此外，子系统之间的人类可读通信通道最大限度地提高了可审核性和可调试性。也就是说，在独立子系统之间实现正确的通信可能很棘手，这会缩小信息传输范围，或暴露通信错误和信息丢失的风险。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;模型嫁接&lt;/h2>; &lt;p>; 一种更综合的方法是采用专门用于每个相关领域，并对其进行调整以直接插入法学硕士 - 将视觉模型&lt;em>;嫁接&lt;/em>;到核心推理代理上。与由法学硕士确定所使用的具体工具的工具使用相反，在模型移植中，研究人员可以在开发过程中选择使用、完善或开发特定模型。在谷歌研究中心最近的两篇论文中，我们表明这实际上是可行的。神经法学硕士通常通过首先将单词映射到&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings&quot;>;向量嵌入空间&lt;/一个>;。这两篇论文都基于将数据从新模态映射到法学硕士已经熟悉的输入词嵌入空间的想法。第一篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.09018&quot;>;基于个人特定数据的健康多模式法学硕士&lt;/a>;”表明&lt;a href= “https://www.ukbiobank.ac.uk/&quot;>;英国生物银行&lt;/a>;如果我们首先训练神经网络分类器来解释&lt;a href=&quot;https://www.mayoclinic.org/tests -procedures/spirometry/about/pac-20385201&quot;>;呼吸图&lt;/a>;（一种用于评估呼吸能力的方式），然后调整该网络的输出作为 LLM 的输入。 &lt;/p>; &lt;p>; 第二篇论文，“&lt;a href=&quot;https://arxiv.org/abs/2308.01317&quot;>;ELIXR：通过大语言模型和放射学的结合实现通用 X 射线人工智能系统视觉编码器&lt;/a>;”，采用相同的策略，但将其应用于放射学中的全尺寸图像编码器模型。从&lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;了解胸部 X 光检查的基础模型&lt;/a>;开始，已展示为了成为在此模式中构建各种分类器的良好基础，本文描述了训练一个轻量级的医疗信息适配器，该适配器将基础模型的顶层输出重新表示为一系列标记LLM 的输入嵌入空间。尽管对视觉编码器和语言模型都没有进行微调，但生成的系统显示了未经训练的功能，包括&lt;a href=&quot;https://arxiv.org/abs/2210.10163&quot;>;语义搜索&lt;/a>;和&lt;a href=&quot;https://www.nature.com/articles/sdata2018251&quot;>;视觉问答&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFM aMMzTSUu4fif7t9SVsr9MduEcISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s1600/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFMaMMzTSUu4fif7t9SVsr9MduEc ISLA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;我们嫁接模型的方法是通过训练医疗信息适配器来将现有或改进的图像编码器的输出映射为法学硕士可理解的形式。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 模型嫁接有很多优点。它使用相对适度的计算资源来训练适配器层，但允许法学硕士在每个数据域中现有的高度优化和验证的模型上构建。将问题模块化为编码器、适配器和 LLM 组件还可以方便在开发和部署此类系统时对各个软件组件进行测试和调试。相应的缺点是，专业编码器和 LLM 之间的通信不再是人类可读的（作为一系列高维向量），并且嫁接过程不仅需要为每个特定领域的编码器构建一个新的适配器，还需要为每个每个编码器的&lt;em>;修订&lt;/em>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;通才系统&lt;/h2>; &lt;p>; 多模式医疗人工智能最根本的方法是构建一个集成的系统，完全通才的系统本身就能够吸收所有来源的信息。在该领域的第三篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;迈向通才生物医学人工智能&lt;/a>;”中，我们没有为每种数据模态使用单独的编码器和适配器，以 &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>; 为基础构建，PaLM-E 是最近发布的多模式模型单个 LLM (&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;) 和 &lt; a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;单视觉编码器 (ViT)&lt;/a>;。在此设置中，LLM 文本编码器涵盖文本和表格数据模式，但现在所有其他数据都被视为图像并馈送到视觉编码器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42 Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s1600/image2.gif&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ 2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Med-PaLM M 是一个大型多模态生成模型，可以使用相同的模型权重灵活地编码和解释生物医学数据，包括临床语言、成像和基因组学。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们通过在论文中描述的医学数据集上微调整套模型参数，将 PaLM-E 专门应用于医学领域。由此产生的通用医疗人工智能系统是 &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; 的多模式版本，我们称之为 Med-PaLM M。灵活的多模式序列到序列的架构使我们能够在一次交互中交织各种类型的多模态生物医学信息。据我们所知，这是单个统一模型的首次演示，该模型可以解释多模式生物医学数据并在所有任务中使用相同的模型权重集来处理各种任务（论文中有详细评估）。 &lt;/p>; &lt;p>; 这种多模态通才系统方法是我们描述的方法中最雄心勃勃、同时也是最优雅的方法。原则上，这种直接方法最大限度地提高了模式之间的灵活性和信息传输。由于没有 API 来维持兼容性，也没有适配器层的激增，通用方法可以说是最简单的设计。但同样的优雅也是其一些缺点的根源。计算成本通常更高，并且由于单一视觉编码器服务于多种模式，领域专业化或系统可调试性可能会受到影响。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;多模式医疗人工智能的现实&lt;/h2>; &lt;p>;为了在医学中充分利用人工智能，我们需要将经过预测人工智能训练的专家系统的优势与通过生成人工智能实现的灵活性结合起来。哪种方法（或方法组合）在该领域最有用取决于许多尚未评估的因素。通才模型的灵活性和简单性是否比模型嫁接或工具使用的模块化更有价值？哪种方法可以为特定的实际用例提供最高质量的结果？支持医学研究或医学教育与增强医疗实践的首选方法是否不同？回答这些问题需要持续进行严格的实证研究，并与医疗保健提供者、医疗机构、政府实体和医疗保健行业合作伙伴继续进行广泛的直接合作。我们期待着共同寻找答案。 &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4436946873570093049/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/08/multimodal-medical-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com /2023/08/multimodal-medical-ai.html&quot; rel=&quot;alternate&quot; title=&quot;多模式医疗 AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72“网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL _N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s72-c/medpalm.png“宽度=“72”xmlns：媒体=“http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-5980448298988153366&lt;/id>;&lt;发布>;2023-07-26T09:33:00.003-07:00&lt;/发布>;&lt;更新>;2023-08-01T08:31:25.216-07:00&lt;/更新>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;寻找无源域的通用方法适应&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Eleni Triantafillou 和学生研究员 Malik Boudiaf &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgy M4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s1500/notela.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 深度学习最近在广泛的问题和应用中取得了巨大进展，但模型在部署在未知的领域或分布中时常常会出现不可预测的失败。 &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;无源域适应&lt;/a>; (SFDA) 是一个研究领域，旨在设计适应预训练模型的方法（在一个“源域”）到一个新的“目标域”，仅使用后者的未标记数据。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 设计深度模型的适应方法是一个重要的研究领域。虽然模型和训练数据集规模的不断扩大是其成功的关键因素，但这种趋势的负面后果是训练此类模型的计算成本越来越高，在某些情况下使得大型模型训练&lt;a href=&quot;https:// spectrum.ieee.org/deep-learning-computational-cost&quot;>;不太容易访问&lt;/a>;并且不必要&lt;a href=&quot;https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy- and-resource-strain&quot;>;增加碳足迹&lt;/a>;。缓解这一问题的一个途径是通过设计技术来利用和重用已经训练好的模型来处理新任务或推广到新领域。事实上，在&lt;a href=&quot;https://arxiv.org/abs/1911.02685&quot;>;迁移学习&lt;/a>;的框架下，对模型适应新任务进行了广泛的研究。 &lt;/p>; &lt;p>; SFDA 是这项研究的一个特别实用的领域，因为一些需要适应的现实应用程序会遇到无法获得来自目标域的标记示例的问题。事实上，SFDA 正受到越来越多的关注 [&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot; >;2&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2110.04202&quot;>;3&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;4 &lt;/a>;]。然而，尽管出于雄心勃勃的目标，大多数 SFDA 研究都基于一个非常狭窄的框架，考虑图像分类任务中简单的&lt;a href=&quot;https://arxiv.org/abs/2108.13624&quot;>;分布变化&lt;/a>;。 &lt;/p>; &lt;p>; 与这一趋势有很大的不同，我们将注意力转向生物声学领域，其中自然发生的分布变化无处不在，通常以目标标记数据不足为特征，这对从业者来说是一个障碍。因此，在此应用中研究 SFDA 不仅可以让学术界了解现​​有方法的普遍性并确定开放的研究方向，而且还可以直接使该领域的从业者受益，并有助于解决本世纪最大的挑战之一：生物多样性保存。 &lt;/p>; &lt;p>; 在这篇文章中，我们宣布“&lt;a href=&quot;https://arxiv.org/abs/2302.06658&quot;>;寻找一种通用的无源域适应方法&lt;/a>;”，出现在 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023。&lt;/a>;我们表明，最先进的 SFDA 方法在面对现实的分布变化时可能表现不佳甚至崩溃在生物声学中。此外，现有方法相对于彼此的表现与视觉基准中观察到的不同，并且令人惊讶的是，有时表现比根本没有适应更差。我们还提出了NOTELA，这是一种新的简单方法，它在这些转变上优于现有方法，同时在一系列视觉数据集上表现出强大的性能。总的来说，我们得出的结论是，（仅）在常用数据集和分布变化上评估 SFDA 方法，让我们对其相对性能和普遍性产生了短视的看法。为了兑现他们的承诺，SFDA 方法需要在更广泛的分布变化上进行测试，我们主张考虑可以有利于高影响力应用的自然发生的方法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;生物声学中的分布变化&lt;/h2>; &lt;p>; 自然发生的分布变化在生物声学中普遍存在。最大的鸟类歌曲标记数据集是 &lt;a href=&quot;https://www.researchgate.net/publication/280978332_The_Xeno-canto_collection_and_its_relation_to_sound_recognition_and_classification&quot;>;Xeno-Canto&lt;/a>; (XC)，这是用户贡献的野生鸟类录音的集合来自世界各地的鸟类。 XC 中的录音是“焦点”的：它们针对的是在自然条件下捕获的个体，其中所识别的鸟的歌声位于前景。然而，出于持续监控和跟踪的目的，从业者通常更感兴趣的是在通过全向麦克风获得的被动录音（“声景”）中识别鸟类。这是一个&lt;a href=&quot;https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments&quot;>;最近&lt;/a>;&lt;a href=&quot;https://www.sciencedirect.com/science/ Article/pii/S1574954121000273&quot;>;工作&lt;/a>; 展示非常具有挑战性。受到这个现实应用的启发，我们使用在 XC 上预先训练的鸟类分类器作为源模型，以及来自不同地理位置的几个“声景”来研究 SFDA 的生物声学 — &lt;a href=&quot;https://doi. org/10.5281/zenodo.7050013&quot;>;内华达山脉&lt;/a>;（内华达州）； &lt;a href=&quot;https://doi.org/10.1002/ecy.3329&quot;>;Powdermill&lt;/a>; 自然保护区，美国宾夕法尼亚州； &lt;a href=&quot;https://doi.org/10.5281/zenodo.7078498&quot;>;夏威夷&lt;/a>;；美国加利福尼亚州卡普尔斯分水岭； &lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;Sapsucker Woods&lt;/a>;，美国纽约 (SSW)；和 &lt;a href=&quot;https://www.google.com/url?q=https://zenodo.org/record/7525349%23.ZB8z_-xudhE&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1688498539746392&amp;amp; usg=AOvVaw07CsIKIE-dcNyMKFT-n_JT&quot;>;哥伦比亚&lt;/a>; — 作为我们的目标域。 &lt;/p>; &lt;p>; 这种从聚焦域到无源域的转变是巨大的：后者的录音通常具有低得多的信噪比，几只鸟同时发声，以及明显的干扰因素和环境噪音，例如雨或风。此外，不同的音景源自不同的地理位置，从而导致极端的标签变化，因为 XC 中的物种的一小部分将出现在给定位置。此外，正如现实世界数据中常见的那样，源域和目标域都存在明显的类别不平衡，因为某些物种比其他物种更常见。此外，我们还考虑了多标签分类问题，因为每个记录中可能会识别出几只鸟类，这与通常研究 SFDA 的标准单标签图像分类场景有很大不同。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeG afX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s1378/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;1378&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQ OzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;“焦点→音景”转变的图示。在聚焦域中，录音通常由前景中的单个鸟类发声组成，以高信噪比 (SNR) 捕获，尽管背景中可能还有其他鸟类发声。&lt;strong>; &lt;/strong>;另一方面，音景包含全向麦克风的录音，可以由多只鸟同时发声以及昆虫、雨、汽车、飞机等环境噪音组成。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/表>; &lt;br />; &lt;表align =“中心”cellpadding =“0”cellspacing =“0”class =“tr-caption-container”>; &lt;tbody>; &lt;tr>; &lt;td style =“text-align：左; &quot;>;&lt;b>;音频文件&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;焦点域&lt;em>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https:// /storage.googleapis.com/chirp-public-bucket/notela-blog-post/XC417991%20-%20Yellow-throated%20Vireo%20-%20Vireo%20flavifrons.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/ em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;音景域名&lt;em>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>; &lt;/sup>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/ Yetvir-soundscape.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;频谱图图像&lt;/ b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR 4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/s936/left.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0 NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/w640-h546/left.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;td>; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iy jBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/s936/右.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTC mlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/w640-h546/right.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;table对齐=“中心”cellpadding=“0”cellspacing=“0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从焦点域（&lt;b>;向左&lt;/ b>;）到音景域（&lt;b>;右&lt;/b>;），根据代表性的音频文件（&lt;b>;顶部&lt;/b>;）和频谱图图像（&lt;b>;底部&lt;/b>;）每个数据集的录音。请注意，在第二个音频剪辑中，鸟鸣声非常微弱；这是音景录音中的一个常见属性，其中鸟叫声不在“前景”。学分：&lt;b>;左：&lt;/b>; XC Sue Riffe &lt;a href=&quot;https://xeno-canto.org/417991&quot;>;录音&lt;/a>; (&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/ &quot;>;CC-BY-NC 许可证&lt;/a>;）。&lt;b>;右：&lt;/b>;摘自 Kahl、Charif 和 Klinck 提供的录音。(2022)“完整注释的音景录音合集来自美国东北部的 SSW 音景数据集的 [&lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;链接&lt;/a>;] (&lt;a href=&quot;https://creativecommons.7018483&quot;>;链接&lt;/a>;) org/licenses/by/4.0/&quot;>;CC-BY 许可证&lt;/a>;)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;最先进的 SFDA 模型在生物声学变化方面表现不佳&lt;/h2>; &lt;p>; 作为起点，我们对六种最先进的 SFDA 方法进行了基准测试我们的生物声学基准，并将它们与&lt;em>;未适应&lt;/em>;基线（源模型）进行比较。我们的发现令人惊讶：毫无例外，现有方法无法在所有目标领域始终优于源模型。事实上，他们的表现常常明显不佳。 &lt;/p>; &lt;p>; 举个例子，&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>; 是一种最新的方法，旨在通过以下方式使模型对每个示例产生可信的预测：减少模型输出概率的不确定性。虽然 Tent 在各种任务中表现良好，但它对于我们的生物声学任务却效果不佳。在单标签场景中，最小化熵迫使模型自信地为每个示例选择一个类别。然而，在我们的多标签场景中，不存在任何类都应该被选择为存在的约束。再加上显着的分布变化，这可能会导致模型崩溃，导致所有类别的概率为零。其他基准测试方法，例如 &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;、&lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt; /a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;帐篷&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98 -Paper.pdf&quot;>;NRC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;灰尘&lt;/a>;和&lt;a href=&quot;https://www.researchgate .net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;伪标签&lt;/a>;是标准 SFDA 基准的强大基线，但也难以完成这项生物声学任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj 38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s1434/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;730&quot; data-original-width=&quot;1434&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1 aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试的演变&lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_ precision&quot;>;平均精度&lt;/a>; (mAP)，一种多标签分类的标准度量，贯穿六个音景数据集的适应过程。我们对我们提出的NOTELA和Dropout Student（见下文）以及&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;、&lt;a href=&quot;https://openreview .net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;帐篷&lt;/a>;、&lt;a href=&quot;https://proceedings .neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;灰尘&lt;/a>;和&lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;伪标签&lt;/a>;。除了NOTELA之外，所有其他方法都无法持续改进源模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;引入带有拉普拉斯调整的 NOisy 学生 TEacher (NOTELA)&lt;/h2>; &lt;p>; 尽管如此，一个令人惊讶的积极结果脱颖而出：不太著名的 &lt;a href=&quot;https://arxiv.org/abs/ 1911.04252&quot;>;吵闹的学生&lt;/a>;原则看起来很有前途。这种无监督方法鼓励模型在某些目标数据集上重建自己的预测，但在随机噪声的应用下。虽然噪声可能通过各种渠道引入，但我们力求简单并使用&lt;a href=&quot;https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9&quot;>;模型丢失&lt;/a>;作为唯一的噪声源：因此，我们将这种方法称为&lt;em>;辍学学生（DS）&lt;/em>;。简而言之，它鼓励模型在对特定目标数据集进行预测时限制单个神经元（或过滤器）的影响。 &lt;/p>; &lt;p>; DS 虽然有效，但在各种目标域上都面临模型崩溃问题。我们假设发生这种情况是因为源模型最初对这些目标域缺乏信心。我们建议通过直接使用特征空间作为辅助事实来源来提高 DS 稳定性。 NOTELA 的灵感来自于 &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC 的方法&lt; /a>; 和拉普拉斯&lt;a href=&quot;https://www.researchgate.net/publication/220319905_Manifold_Regularization_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples&quot;>;正则化&lt;/a>;。这种简单的方法如下图所示，在音频和视觉任务中始终显着优于源模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4 VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s1870/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1870&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb 5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; &quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-对齐：center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9Q nJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s1080/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92 969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;NOTELA 的实际应用。通过完整模型转发音频记录以获得第一组预测，然后通过拉普拉斯正则化（一种基于聚类附近点的后处理形式）对这些预测进行细化。最后，细化的预测为用作&lt;em>;噪声模型&lt;/em>;重建的目标。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 标准的人工图像分类基准无意中限制了我们对 SFDA 方法的真正通用性和鲁棒性的理解。我们主张扩大范围并采用新的评估框架，其中包含生物声学自然发生的分布变化。我们还希望NOTELA 能够成为促进该方向研究的坚实基础。 NOTELA 的强劲表现也许表明了两个因素可以导致开发更通用的模型：首先，开发着眼于更困难问题的方法，其次，支持简单的建模原则。然而，未来仍需开展工作来查明和理解现有方法在解决更困难问题时的失效模式。我们相信，我们的研究代表了朝这个方向迈出的重要一步，为设计具有更大普适性的 SFDA 方法奠定了基础。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Eleni Triantafillou ，现供职于 Google DeepMind。我们代表 NOTELA 论文的作者发布此博文：Malik Boudiaf、Tom Denton、Bart van Merriënboer、Vincent Dumoulin*、Eleni Triantafillou*（其中 * 表示同等贡献）。我们感谢合著者为本文所做的辛勤工作，以及 Perch 团队其他成员的支持和反馈。&lt;/em>; &lt;/p>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/ a>;&lt;/sup>;请注意，在此音频片段中，鸟鸣声非常微弱；这是音景录音中的一个常见属性，鸟叫声不在“前景”。&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5980448298988153366/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/in-search-of-generalized-method-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /ai.googleblog.com/2023/07/in-search-of-generalized-method-for.html&quot; rel=&quot;alternate&quot; title=&quot;寻找无源域适应的通用方法&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4ep C-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/ s72-c/notela.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;已发布>;2023-07-23T14:13:00.002-07:00&lt;/已发布>;&lt;更新>;2023-08-07T10:08:27.713-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 在 ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：项目经理 Cat Armato 、谷歌&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4n vXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s320/Google-ICML-hero.jpg&quot;样式=“显示：无；” />; &lt;p>; Google 各个团队积极开展机器学习 (ML) 领域的理论和应用研究。我们构建机器学习系统来解决语言、音乐、视觉处理、算法开发等领域的深层科学和工程挑战。我们的目标是通过开源工具和数据集、发布我们的工作并积极参加会议，与更广泛的机器学习研究社区建立一个更具协作性的生态系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Google 很荣幸成为&lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;钻石赞助商&lt;/a >; 第 40 届&lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;国际机器学习会议&lt;/a>; (ICML 2023)，这是一次重要的年度会议，将于今年举行夏威夷檀香山一周。作为 ML 研究领域的领导者，Google 在今年的会议上表现强劲，收到了 120 多篇论文，并积极参与了许多研讨会和教程。 Google 还很荣幸成为 &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; 和 &lt;a href=&quot;https://sites .google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;机器学习中的女性&lt;/a>;研讨会。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;p>; 已注册 ICML 2023？我们希望您能够参观 Google 展位，详细了解解决该领域最有趣挑战的部分令人兴奋的工作、创造力和乐趣。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解 Google 展位活动（例如演示和问答环节）。请参阅 &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind 博客&lt;/a>;，了解他们在 ICML 2023 上的技术参与情况。&lt;/ p>; &lt;p>; 请参阅下文，了解有关 ICML 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。&lt;/p>; &lt;br>; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 董事会成员包括：&lt;strong>;&lt;em >;Corinna Cortes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; 辅导主席包括：&lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong >; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin- left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75KA&quot;>;将视觉 Transformer 扩展到 220 亿个参数&lt;/a>;（请参阅 &lt;a href=&quot;https:// /ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;约西普·乔隆加&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;巴兹尔·穆斯塔法&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;皮奥特·帕德鲁斯基&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;乔纳森·希克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯汀·吉尔默&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;安德烈亚斯·斯坦纳&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伯特·盖尔霍斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;易卜拉欣·阿拉卜杜尔莫辛&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Michael Tschannen&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;阿努拉格·阿纳布&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;小王&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡洛斯·里克尔梅&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Joan Puigcerver、&lt;/em>;&lt;em>;Utku Evci&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;阿拉文德·马亨德兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿维塔尔·奥利弗&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;芳汀·胡特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯米恩·巴斯廷斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马克·帕特里克·科利尔&lt;/em>;&lt;/strong>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alexey Gritsenko&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Cristina Vasconcelos&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;托马斯·门辛克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;亚历山大·科列斯尼科夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Filip Pavetić&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;达斯汀·特兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;托马斯Kipf&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mario Lučić&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·凯泽斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;耶利米·哈姆森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔·霍尔斯比&lt;/em>;&lt;br />;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;通过推理解码从 Transformers 进行快速推理&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em >;&lt;/strong>;、&lt;strong>;&lt;em>;马坦·卡尔曼&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尤西·马蒂亚斯&lt;/em>;&lt;/strong>;&lt;/p>;&lt;p>;&lt;a href= “https://openreview.net/pdf?id=bUFUaawOTk&quot;>;两全其美的策略优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;，&lt;em>;陈-于伟&lt;/em>;，&lt;strong>;&lt;em>;朱利安·齐默特&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf? id=9PJ2V6qvQL&quot;>;机器学习中的流入、流出和互惠&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Walid Krichene&lt;/em>; &lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;Transformers 通过梯度下降在上下文中学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;约翰内斯·冯·奥斯瓦尔德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;艾文德·尼克拉森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;埃托雷·兰达佐&lt;/em>;&lt;/strong>; >;、&lt;em>;若昂·萨克拉门托&lt;/em>;、&lt;strong>; &lt;em>;亚历山大·莫德文采夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈·日莫吉诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Max Vladymyrov&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;算术采样：并行多样化解码大型语言模型&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;em>;Patrick Murray* &lt;/em>;、&lt;em>;亚历山大·帕索斯*&lt;/em>;、&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https: //openreview.net/pdf?id=ayBKRjGDEI&quot;>;具有可证明近似保证的差分私有层次聚类&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/05/ Differentially-private- clustering-for.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;em>;Jacob Imola&lt;/em>;*,&lt;strong>; &lt;em>;Alessandro Epasto&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; >;穆罕默德·马赫迪安&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;文森特·科恩-阿达德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong >; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2ToHR5&quot;>;用于私有机器学习的多历元矩阵分解机制&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;H.布伦丹·麦克马汉&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;基思·拉什&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;阿卜拉迪普·塔库塔&lt;/em>;&lt;br />;&lt;/strong>; &lt;/ p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;无论模型选择如何，随机分类噪声都不会击败所有凸潜在助推器&lt;/a>; &lt;br>; &lt;strong>;&lt; em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;理查德·诺克&lt;/em>;&lt;/strong>;、&lt;em>;罗伯特·威廉姆森&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;单纯形随机特征&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;，&lt;strong>; &lt;em>;Krzysztof Choromanski&lt;/ em>;&lt;/strong>;、&lt;em>;Valerii Likhosherstov&lt;/em>;、&lt;em>;阿德里安·韦勒&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/ pdf?id=bF1LVbP493&quot;>;Pix2Struct：屏幕截图解析作为视觉语言理解的预训练&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mandar Joshi&lt; /em>;&lt;/strong>;、&lt;em>;Iulia Turc&lt;/em>;、&lt;strong>; &lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;、&lt;em>;刘芳宇&lt;/em>;、&lt;strong>; &lt;em>; >;Julian Eisenschlos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;张明伟&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Kristina Tooutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net /pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM：多任务、多语言语音和语言模型&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yong Cheng&lt;/​​em>;&lt;/strong>;, &lt;strong>; &lt;em>;张宇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;梅尔文·约翰逊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;沃尔夫冈·马切里&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;稳健的预算使用单个样本调整节奏&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;、&lt;em>;Rachitesh Kumar&lt;/em>;*、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/ em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王迪&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p >;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;基于检索的模型的统计视角&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;、&lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot; https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;张量分解的近似最佳核心形状&lt;/a>; &lt;br>; &lt;em>;Mehrdad Ghadiri&lt;/em>;，&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;傅刚&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;使用批次的高效列表可解码回归&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>; &lt;/strong>;、&lt;em>;Ayush Jain&lt;/em>;*、&lt;strong>; &lt;em>;孔伟豪&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt; /strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;使用少样本学习高效训练语言模型&lt;/a>; &lt;br >; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;金承妍&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp ;name=pdf&quot;>;拟阵上的完全动态子模最大化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt; em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/ strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;用于学习组合潜变量模型的 GFlowNet-EM&lt;/a>; &lt;br>; &lt; em>;Edward J Hu&lt;/em>;、&lt;em>;Nikolay Malkin&lt;/em>;、&lt;em>;Moksh Jain&lt;/em>;、&lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;、&lt;em>; Alexandros Graikos&lt;/em>;、&lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;改进在线学习广告拍卖中点击率预测算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;冯哲&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;, &lt;em >;Zixin Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;大型语言模型难以学习长尾知识&lt;/ a>; &lt;br>; &lt;em>;尼基尔·坎德帕尔&lt;/em>;、&lt;em>;邓海康&lt;/em>;、&lt;strong>; &lt;em>;亚当·罗伯茨&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;埃里克·华莱士&lt;/em>;&lt;/strong>;，&lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;多渠道自动出价预算和投资回报率限制&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yuan Deng&lt;/​​em>;&lt;/strong>;、&lt;em>;Negin Golrezaei&lt;/em>;、&lt;em>;Patrick Jaillet&lt;/em>;、&lt;em>; >;杰森·卓南梁&lt;/em>;，&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= ZMvv6laV5b&amp;amp;name=pdf&quot;>;多层神经网络作为希尔伯特空间的可训练阶梯&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;陈正道&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;关于用户级私有凸优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;强>;&lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;拉维库马尔&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;， &lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张驰远&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;通过不变表示进行 PAC 泛化&lt;/a>; &lt;br>; &lt;em>;Advait U Parulekar&lt;/em>;，&lt;strong>; &lt;em>;Karthikeyan Shanmugam&lt;/em>;&lt; /strong>;, &lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;正则化和方差加权回归在线性 MDP 中实现极小极大最优：理论与实践&lt;/a>; &lt;br>; &lt;em>;Toshinori Kitamura&lt;/em>;、&lt;em>;Tadashi Kozuno&lt;/em>;、&lt;em>;Yunhao Tang&lt;/em>; 、&lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;、&lt;em>;Michal Valko&lt;/em>;、&lt;em>;杨文浩&lt;/em>;、&lt;strong>;&lt;em>;梅金城&lt;/em>; &lt;/strong>;、&lt;em>;皮埃尔·梅纳德&lt;/em>;、&lt;em>;Mohammad Gheshlaghi Azar&lt;/em>;、&lt;em>;雷米·穆诺斯&lt;/em>;、&lt;strong>; &lt;em>;奥利维耶·皮埃奎因&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;Csaba Szepesvari&lt;/em>;、&lt;em>;Wataru Kumagai&lt;/em>;、&lt;em>;松尾裕&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt39VUw&amp;amp;name=pdf&quot;>;通过最小违规排列加速 Bellman Ford&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;西尔维奥·拉坦齐&lt;/em>;&lt;/strong>;、&lt;em>;奥拉·斯文森&lt;/em>;、&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/attachment?id=LxodbQa62n&amp;amp;name=pdf&quot;>;学习算法的统计不可区分性&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;，&lt;strong>; &lt;em>;Amin Karbasi&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢伊·莫兰&lt;/em>;&lt;/strong>;、&lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/pdf?id=G5vKSJVhJL&quot;>;以时隙为中心的模型进行测试时调整&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;、&lt;em>;Deepak Pathak&lt;/em>;、&lt;em>;Katerina Fragkiadaki>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;用户级隐私下直方图估计的边界贡献算法&lt;/a>; &lt;br>; &lt;em>;刘玉涵&lt;/em>;*、&lt;strong>;&lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;朱文楠&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Peter Kairouz&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment ?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;带有提示和查询的 Bandit 在线线性优化&lt;/a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;、&lt;em>;Ashok Cutkosky&lt;/em>;、&lt;strong>; &lt;em >;拉维·库马尔&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;曼尼什·普罗希特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR：通过无监督任务表示学习进行课程学习&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;，&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/ &lt;em>;贾斯帕·埃姆霍夫&lt;/em>;、&lt;em>;纳撒尼尔·亚历克西斯&lt;/em>;、&lt;strong>; &lt;em>;亚历山大·福斯特&lt;/em>;&lt;/strong>;、&lt;em>;彼得·阿贝尔&lt;/em>;、 &lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP：自我监督对比空间预训练地理空间视觉表示&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;, &lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;, &lt;em>;Yutong He&lt;/em>;, &lt; em>;宋嘉明&lt;/em>;、&lt;em>;斯特凡诺·埃尔蒙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;埃瓦尔德-基于分子图的远程消息传递&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;，&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;，&lt;em>;Nicholas Gau&lt; /em>;, &lt;em>;Stephan Günnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;快 (1+ε) -二元矩阵分解的近似算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;、&lt;em>;Maximilian Vötsch&lt;/em>;、&lt;strong>;&lt;em>;David Woodruff&lt; /em>;&lt;/strong>;，&lt;em>;Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;联合线性具有用户级差分隐私的上下文强盗&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;、&lt;em>;Huanyu Zhang&lt;/em>;、&lt;em>;Luca Melis&lt;/em>;、&lt;em>;Milan Shen &lt;/em>;，&lt;strong>; &lt;em>;Meisam Hejazinia&lt;/em>;&lt;/strong>;，&lt;em>;杨静&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;研究基于模型的学习在探索和迁移中的作用&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;、&lt;em>;Eszter Vértes&lt;/em>; >;、&lt;em>;李亚哲&lt;/em>;、&lt;strong>; &lt;em>;加布里埃尔·杜拉克-阿诺德&lt;/em>;&lt;/strong>;、&lt;em>;Ankesh Anand&lt;/em>;、&lt;em>;Theophane Weber&lt;/em>; ,&lt;em>; Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;标记差异隐私和私人训练数据发布&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;罗伯特·布萨-费科特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈斯·穆尼奥斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;奥马尔·赛义德&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name= pdf&quot;>;与分销专业专家一起进行终身语言预训练&lt;/a>; &lt;br>; &lt;em>;Wuyang Chen&lt;/em>;*, &lt;strong>;&lt;em>;Yanqi Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Nan Du&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄艳萍&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;James Laudon&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;陈志峰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;崔嘉儿&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;低排名奖励的多用户强化学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;，&lt;em>;Suhas S Kowshik&lt;/em>;、&lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;用于视觉机器人操作的多视图蒙版世界模型&lt;/a >; &lt;br>;&lt;em>;Younggyo Seo&lt;/em>;、&lt;em>;Junsu Kim&lt;/em>;、&lt;em>;Stephen James&lt;/em>;、&lt;strong>;&lt;em>;Kimin Lee&lt;/em>;&lt;/strong>; 、 &lt;em>;申振宇&lt;/em>;、&lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot; >;PaLM-E：体现的多模态语言模型&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;夏飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科里·林奇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aakanksha Chowdery&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Brian Ichter&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em >;Quan Vuong&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;余天河&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄文龙&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Daniel Duckworth&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;文森特·范霍克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡罗尔·豪斯曼&lt;/em>;&lt;/strong>;、&lt;em>; >;Marc Toussaint&lt;/em>;、&lt;strong>; &lt;em>;Klaus Greff&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曾安迪&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Igor Mordatch &lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;皮特·弗洛伦斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs &quot;>;采用自动调整压缩的私有联合学习&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*，&lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;使用线性函数逼近的对抗性 MDP 的精致遗憾&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;、&lt;em>;罗海鹏&lt;/em>;、&lt;em>;魏震宇&lt;/em>;、&lt;strong>;&lt;em>;朱利安·齐默特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp ;name=pdf&quot;>;使用恒定内存将数据集蒸馏扩展到 ImageNet-1K&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;、&lt;em>; Ruoche Wan&lt;/em>;、&lt;strong>;&lt;em>;丝丝&lt;/em>;&lt;/strong>;，&lt;em>;谢祖瑞&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name= pdf&quot;>;采用 AdaGrad 步长的 SGD：对未知参数、无界梯度和仿射方差具有高概率的完全自适应性&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;分位数时间差异学习对价值估计的统计优势&lt;/a>; &lt;br >; &lt;em>;马克·罗兰&lt;/em>;、&lt;em>;唐云浩&lt;/em>;、&lt;em>;克莱尔·莱尔&lt;/em>;、&lt;em>;雷米·穆诺斯&lt;/em>;、&lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;、&lt;em>;威尔·达布尼&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;透过图像特征的迷雾揭开位置信息模式的面具&lt;/a>; &lt;br>; &lt;em>;林杰休伯特&lt;/em>;、&lt;em>;曾鸿宇&lt;/em>;、&lt;em>;新英Lee&lt;/em>;、&lt;em>;Maneesh Kumar Singh&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/pdf?id=4UStsbnfVT&quot;>;具有最佳速率的用户级私有随机凸优化&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;，&lt;strong>; &lt;em>;孙子腾&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;一种改进文本中提示集成的简单零样本提示加权技术-图像模型&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*、&lt;strong>;&lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>; em>;&lt;/strong>;,&lt;strong>;&lt;em>;顾秀野&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;崔银&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Dustin Tran&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘哲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/attachment?id=mXv2aVqUGG&amp;amp;name=pdf&quot;>;大型语言模型能否推理程序不变量？&lt;/a>; &lt;br>; &lt;strong>;裴可欣&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;大卫·比伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;石肯森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;查尔斯·萨顿&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;殷鹏程&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name =pdf&quot;>;持续观察下的并发随机差异隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong >;、&lt;strong>;&lt;em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;常量：差分隐私持续观察中的细粒度误差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/强>;，&lt;em>;莫妮卡·亨辛格&lt;/em>;，&lt;em>;Jalaj Upadhyay&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name= pdf&quot;>;交叉熵损失函数：理论分析与应用&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;，&lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;，&lt;em>; Yutaozhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;使用在线函数逼近的对抗性上下文 MDP 的高效速率最优遗憾&lt; /a>; &lt;br>; &lt;em>;奥林·利维&lt;/em>;、&lt;strong>;&lt;em>;阿隆·科恩&lt;/em>;&lt;/strong>;、&lt;em>;阿萨夫·卡塞尔&lt;/em>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;拟阵约束下流子模最大化的公平性&lt; /a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Jakab Tardos&lt;/em>;&lt;/strong>;，&lt;em>;Jakub Tarnawski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name= pdf&quot;>;Flan Collection：设计有效指令调优的数据和方法&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open。 html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;em>;Shayne Longpre&lt;/em>;、&lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tu Vu&lt;/em>; em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿尔伯特·韦伯森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;郑亨元&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;巴雷特·佐夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾森·魏&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚当·罗伯茨&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;通过双层优化进行网络控制的图强化学习&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/ em>;、&lt;strong>;&lt;em>;詹姆斯·哈里森&lt;/em>;&lt;/strong>;、&lt;em>;杨凯迪&lt;/em>;、&lt;em>;Marco Pavone&lt;/em>;、&lt;em>;Filipe Rodrigues&lt;/em>;、 &lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;多分位数的学习增强私有算法发布&lt;/a>; &lt;br>; &lt;em>;米哈伊尔·霍达克&lt;/em>;*、&lt;strong>;&lt;em>;卡里姆·阿明&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;特拉维斯·迪克&lt;/em>;&lt;/强>;，&lt;强>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>; LegendreTron：起义正确的多类损失学习&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;、&lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;、&lt;em>;Spiridon Penev&lt;/em>; >;,&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;测量编程语言分布的影响&lt;/a>; &lt;br>; &lt;em>;Gabriel Orlanski&lt;/em>;*, &lt;strong>;&lt;em>;肖克凡&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xavier Garcia&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;许志锋&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·豪兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·马尔莫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·奥斯汀&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;、&lt;em>;米歇尔·卡塔斯塔&lt;/em>;&lt;/strong>; em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;分布倾斜下的多任务差分隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;Walid Krichene&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;宋爽&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张莉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse：通过 Masked Generative Transformers 生成文本到图像&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Huiwen Chang &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张翰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾里德·巴伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;José Lezama&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陆江&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;凯文·墨菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;威廉·弗里曼&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;李元珍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迪利普·克里希南&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;关于联合平均与循环客户端参与的融合&lt;/a>; &lt;br>; &lt;em>; Yae Jee Cho&lt;/em>;、&lt;em>;Pranay Sharma&lt;/em>;、&lt;em>;Gauri Joshi&lt;/em>;、&lt;strong>;&lt;em>;郑旭&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Satyen Kale&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;张童&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=GimajxXNc0&amp;amp;name=pdf&quot;>;通过在线到非凸转换实现最优随机非平滑非凸优化&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;，&lt;strong>;&lt;em>;严厉的梅塔&lt;/em>;&lt;/strong>;，&lt;em>;弗朗西斯科·奥拉博纳&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot; >;通过定向增强实现域外鲁棒性&lt;/a>; &lt;br>; &lt;em>;Irena Gau&lt;/em>;、&lt;em>;Shiori Sakawa&lt;/em>;、&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>; &lt;/strong>;、&lt;em>;桥本龙典&lt;/em>;、&lt;em>;梁培西&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=b6Hxt4Jw10&quot; >;无界高斯混合模型的多项式时间和私人学习&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;、&lt;em>;Hassan Ashtiani&lt;/em>;、&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;预计算内存还是即时编码？检索增强的混合方法可充分利用您的计算&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;、&lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;尼古拉斯·菲茨杰拉德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;约书亚·安斯利&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;沙飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;威廉·科恩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;用于迭代生成的可扩展自适应计算&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*、&lt;strong>;&lt;em>;David J. Fleet&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*、&lt;strong>;&lt;em>;David J. Fleet&lt;/a>; em>;&lt;/strong>;,&lt;strong>; &lt;em>;陈婷&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;缩放球形 CNN&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;、&lt;em>;Jean-Jacques Slotine&lt;/em>;、&lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;步骤：在前提条件下从头开始学习 N:M 结构化稀疏掩模&lt; /a>; &lt;br>; &lt;em>;吕玉成&lt;/em>;、&lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;奥列格·雷巴科夫&lt;/em>;&lt;/strong>;、&lt;em>;克里斯托弗·德萨&lt;/em>;、&lt;em>;阿米尔·亚兹丹巴赫什&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;带有拒绝的分层对抗鲁棒性&lt;/a>; &lt;br>; &lt;em>;Jiefeng Chen&lt;/em>;，&lt;em>;Jayaram Raghuram&lt;/em>;， &lt;em>;Jihye Choi&lt;/em>;、&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;、&lt;em>;梁英宇&lt;/em>;、&lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;特权信息何时解释消除标签噪音？&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez &lt;/em>;*、&lt;strong>; &lt;em>;马克·科利尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;阿南特·纳瓦尔加利亚&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚历山大·达莫尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杰西·贝伦特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;鲁道夫·杰纳顿&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;埃夫罗西尼Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;具有弹性输入序列的自适应计算&lt;/a>; &lt;br>; &lt;em>;薛福兆&lt;/em>;*、&lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;、&lt;strong>; >; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;、&lt;em>;杨游&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;神经网络记忆可以本地化吗？&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;,&lt;strong>; &lt; em>;Michael C. Mozer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>;、&lt;em>;Zachary C. Lipton&lt;/em>;、&lt;em>;J. Zico Kolter&lt;/em>;，&lt;strong>; &lt;em>;张驰远&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name =pdf&quot;>;可控性感知无监督技能发现&lt;/a>; &lt;br>; &lt;em>;Seohong Park&lt;/em>;，&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;，&lt;em>;Youngwoon Lee&lt; /em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;基于网格的高效学习Bi-Stride多尺度图神经网络物理模拟&lt;/a>; &lt;br>; &lt;em>;曹亚迪&lt;/em>;，&lt;strong>; &lt;em>;柴梦雷&lt;/em>;&lt;/strong>;，&lt;em>;Minchen李&lt;/em>;，&lt;em>;蒋陈凡富&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;联邦重量级选手恢复线性草图下&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;、&lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;、&lt;strong>; &lt;em>;孙子腾&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf? id=SpA7YFu02k&quot;>;用于对图神经网络进行基准测试的图生成模型&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;、&lt;em>;Yue Wu&lt;/em>;、&lt;strong>;&lt;em>;John Palowitch&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;成对错误排名损失代理的 H 一致性界限&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;，&lt;strong>;&lt;em>;Mehryar Mohri&lt;/ em>;&lt;/strong>;, &lt;em>;钟宇涛&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;改进遗憾使用线性函数逼近的高效在线强化学习&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yishay Mansour &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;不变槽注意力：使用以槽为中心的参考进行对象发现框架&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*，&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Gamaleldin Fathy Elsayed&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;从 Bandit 反馈中进行多任务离策略学习&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;、&lt;em>;Branislav Kveton&lt;/em>;、&lt;em>;Manzil Zaheer&lt;/em>;、&lt;em>;Sumeet Katariya&lt;/em>;、&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;片面 Lipschitz 函数的最佳无悔学习&lt;/ a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jon Schneider&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;王睿智&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name =pdf&quot;>;平均场游戏中高效和独立学习的政策镜像上升&lt;/a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;、&lt;em>;Semih Cayci&lt;/em>;、&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;鸟何&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;广义和马尔可夫博弈中的遗憾最小化和均衡收敛&lt;/a>; &lt;br>; &lt;em>;Liad Erez&lt;/em>;, &lt;em>;Tal Lancewicki&lt;/em>;, &lt;em>;Uri Sherman&lt;/em>;, &lt;强>;&lt;em>;托默·科伦&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;伊莎伊·曼苏尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;通过多重奖励，强化学习可以更加高效&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf ?id=rdOuTlTUMX&quot;>;利用依赖于历史的动态上下文进行强化学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;、&lt;em>;Nadav Merlis&lt;/em>;、&lt;strong >;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马丁·姆拉德诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdhcjMzhHN&amp;amp;name=pdf&quot;>;物理动力系统扩散模型中的用户定义事件采样和不确定性量化&lt;/a>; &lt;br >; &lt;em>;马克·安东·芬齐&lt;/em>;*、&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;、&lt;em>;安德鲁·戈登·威尔逊&lt;/em>;、&lt;strong>; &lt;em>;飞沙&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;莱昂纳多·泽佩达-努涅斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= LDBIVZCnLl&amp;amp;name=pdf&quot;>;离散键值瓶颈&lt;/a>; &lt;br>; &lt;em>;Frederik Träuble&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;em>;Nasim Rahaman&lt;/em>;、&lt;强>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;、&lt;em>;Kenji Kawaguchi&lt;/em>;、&lt;em>;Yoshua Bengio&lt;/em>;、&lt;em>;Bernhard Schölkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA：采用通信最优精确共识算法的去中心化 SGD&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/ em>;, &lt;em>;金可欣&lt;/em>;,&lt;strong>; &lt;em>;碧城英&lt;/em>;&lt;/strong>;, &lt;em>;袁坤&lt;/em>;, &lt;em>;殷窝涛&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer：图的稀疏变换器&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/ em>;、&lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balaji Venkatachalam&lt;/em>;&lt;/strong>;、&lt;em>;Danica J. Sutherland&lt;/em>;、&lt; strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;快速、可微分和稀疏 Top-k：凸分析视角&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>;*、&lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Josip Djolonga&lt;/em>;&lt;/strong>;、&lt;em>;加布里埃尔·佩雷&lt;/em>;、&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= “https://openreview.net/pdf?id=priTMs7n6e&quot;>;改进算法资源分配随机试验的政策评估&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;， &lt;em>;布莱恩·怀尔德&lt;/em>;、&lt;strong>; &lt;em>;阿帕娜·塔内贾&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;寻找一种通用的无源域适应方法&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em >;*、&lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mSofpvUxCL&amp;amp;name=pdf &quot;>;存在分布偏移的学习率表&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;并非所有语义都是平等的：具有自动温度个性化的对比自我监督学习&lt;/a>; &lt;br>; &lt;em>;邱子浩&lt;/em>; , &lt;em>;胡全琪&lt;/em>;, &lt;em>;袁卓宁&lt;/em>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;张丽君&lt;/em>;, &lt;em>; >;杨天宝&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;论解释与预测之间的关系：因果观&lt; /a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*、&lt;em>;Krikamol Muandet&lt;/em>;、&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;、&lt;em>;Bernhard Schölkopf&lt;/em>;，&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name= pdf&quot;>;关于注意力在提示调整中的作用&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;，&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;，&lt;em>; Mahdi Soltanolkotabi&lt;/em>;、&lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;播放：参数化使用潜在扩散生成条件布局&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/​​em>;&lt;/strong>;, &lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;李刚&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;李杨&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;用于非线性策略优化的学习局部线性模型的力量&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;、&lt;em>;Max Simchowitz&lt;/em>; , &lt;em>;Tyler Westenbroek&lt;/em>;, &lt;em>;Nikolai Matni&lt;/em>;,&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;解释图神经网络的相关步行搜索&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;，&lt;em>;Thomas Schnake&lt;/ em>;、&lt;em>;Michael Gastegger&lt;/em>;、&lt;em>;Grégoire Montavon&lt;/em>;、&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;、&lt;em>;中岛新一&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;大型代码语言模型的存储库级提示生成&lt;/a>; &lt;br>; &lt; em>;Disha Shrivastava&lt;/em>;、&lt;strong>; &lt;em>;雨果·拉罗谢尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·塔洛&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;稳健且私密的随机线性强盗&lt;/a>; &lt;br>; &lt;em>;Vasileios Charisopoulos&lt;/em>;*, &lt;strong>; &lt;em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;简单扩散：高分辨率图像的端到端扩散&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;,&lt;strong >; &lt;em>;乔纳森·希克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;蒂姆·萨利曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;捆绑增强：控制表示相似性改进数据增强&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;、&lt;strong>;&lt;em>;李子超&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;为什么私人模特训练需要公开预训练？&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt; /em>;&lt;/strong>;、&lt;em>;Mahdi Haghifam&lt;/em>;*、&lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/em>;&lt;/em>; &lt;strong>;、&lt;strong>; &lt;em>;托马斯·斯坦克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;王伦&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf &quot;>;强化学习中一步强化学习与批评正则化之间的联系&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;em>;鲁斯兰·萨拉胡迪诺夫&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview. net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;差分隐私优化中的超越统一 Lipschitz 条件&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*，&lt;strong>;&lt;em>;Satyen Kale&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;徐峥&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;张潼&lt;/em>;&lt;/strong>;, &lt;em>;Sujay Sanghavi&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;高效图场积分器满足点云&lt;/a>; &lt;br>; &lt;strong>;&lt;em>; Krzysztof Choromanski&lt;/em>;&lt;/strong>;、&lt;em>;Arijit Sehanobish&lt;/em>;、&lt;em>;林瀚&lt;/em>;、&lt;em>;赵云帆&lt;/em>;、&lt;em>;Eli Berger、&lt;/em>; >; &lt;em>;Tetiana Parshakova&lt;/em>;、&lt;em>;Alvin Pan&lt;/em>;、&lt;em>;David Watkins&lt;/em>;、&lt;em>;张天一&lt;/em>;、&lt;em>;Valerii Likhoherstov&lt;/em>; 、&lt;em>;Somnath Basu Roy Chowdhury&lt;/em>;、&lt;strong>;&lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;塔马斯·萨洛斯&lt;/em>;&lt;/strong>;、&lt;em>;Snigdha Chaturvedi&lt;/em>;、&lt;em>;阿德里安·韦勒&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>;快如 CHITA：组合优化的神经网络剪枝&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;、&lt;em>;Wenyu Chen&lt;/em>; , &lt;em>;孟翔&lt;/em>;, &lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;赵哲&lt;/em>;&lt;/strong>;，&lt;em>;拉胡尔·马宗德&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf &quot;>;快速启动强化学习&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;博文&lt;/a>;）&lt;br >; &lt;em>;Ikechukwu Uchendu&lt;/em>;*、&lt;strong>;&lt;em>;Ted Shaw&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;姚璐&lt;/em>;&lt;/strong>;、&lt;em>;Banghua朱&lt;/em>;、&lt;em>;严孟源&lt;/em>;、&lt;em>;Joséphine Simon&lt;/em>;、&lt;em>;Matthew Bennice&lt;/em>;、&lt;em>;付楚源&lt;/em>;、&lt;em>;丛马&lt;/em>;、&lt;em>;焦建涛&lt;/em>;、&lt;strong>; &lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡罗尔·豪斯曼&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;POMDP 中的学习具有样本效率和事后可观察性&lt;/a>; &lt;br>; &lt;em>;乔纳森·李&lt;/em>;、&lt;strong>;&lt;em>;阿勒克·阿加瓦尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;克里斯托夫·丹恩&lt;/em>;&lt;/strong>;、&lt;em>;张潼&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;使用 ES-Single 展开计算图中的低方差梯度估计&lt;/a>; &lt;br >; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;掩蔽轨迹预测、表示和控制模型&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;、&lt;em>;Arjun Majumdar&lt;/em>;、&lt;em>;Kevin Stone&lt;/em>;、&lt;em>;Yixin Lin &lt;/em>;、&lt;strong>;&lt;em>;伊戈尔·莫达奇&lt;/em>;&lt;/strong>;、&lt;em>;彼得·阿贝尔&lt;/em>;、&lt;em>;Aravind Rajeswaran&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;使用特征筛克服深度网络中的简单性偏差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;普拉迪普·谢诺伊&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name= pdf&quot;>;广告拍卖中福利最大化点击率预测的成对排名损失&lt;/a>; &lt;br>; &lt;em>;吕博相&lt;/em>;,&lt;strong>; &lt;em>;冯哲&lt;/em>;&lt;/strong >;、&lt;em>;扎卡里·罗伯逊&lt;/em>;、&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Faster Ford-Fulkerson 的预测流程&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;、&lt;em>;Benjamin Moseley&lt;/em>;、&lt;strong>; &lt;em>;Sergei瓦西尔维茨基&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;王雨燕&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= SVCYSBgFIr&amp;amp;name=pdf&quot;>;多语言神经机器翻译的缩放定律&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Behrooz Ghorbani&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;泽维尔·加西亚&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马库斯·弗雷塔格&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;奥尔罕·菲拉特&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;用于时间序列结构发现的顺序蒙特卡罗学习&lt;/a>; &lt; br>; &lt;strong>;&lt;em>;费拉斯·萨德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;布莱恩·巴顿&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马修·道格拉斯·霍夫曼&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;随机梯度成功实现强盗&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;梅金城&lt;/em>;&lt;/strong>;, &lt; em>;钟子鑫&lt;/em>;、&lt;strong>;&lt;em>;戴波&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;、&lt;em>;Csaba Szepesvari&lt;/ em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;基于子集的实例私人估计中的最优性&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; >;孙子腾&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment ?id=zvCSNsoyKW&amp;amp;name=pdf&quot;>;机器翻译的少样本学习的不合理有效性&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;、&lt;em>;Yamini Bansal&lt;/em>;、&lt;strong >; &lt;em>;科林·切里&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔治·福斯特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;马克西姆·克里昆&lt;/em>;&lt;/strong>;、&lt; em>;梅尔文·约翰逊&lt;/em>;、&lt;em>;奥尔罕·菲拉特&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot; >;视觉中的自我监督学习：从研究进展到最佳实践&lt;/a>; &lt;br>; &lt;em>;Xinlei Chen&lt;/em>;、&lt;em>;Ishan Misra&lt;/em>;、&lt;em>;Randall Balestriero&lt;/em>; 、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;em>;克里斯托夫·费希滕霍夫&lt;/em>;、&lt;em>;马克·易卜拉欣&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://icml.cc/virtual/2023/tutorial/21560&quot;>;如何 DP-fy ML：差异隐私机器学习实用教程&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog .com/2023/05/making-ml-models- Differentially-private.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;徐峥&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/ virtual/2023/tutorial/21558&quot;>;神经网络泛化理论的最新进展&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;马腾宇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alex Damian &lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;世博日研讨会&lt;/h2 >; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Tensorflow 中的图神经网络：A实用指南&lt;/a>; &lt;br>; 研讨会组织者包括：&lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;布兰登·梅尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·哈尔克劳&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 赞助的亲和力研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www .latinxinai.org/icml-2023&quot;>;人工智能中的拉丁语&lt;/a>; (LAXAI) &lt;br>;白金赞助商&lt;br>;主题演讲者：&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>;小组成员：&lt;strong>;&lt;em>;姚勤&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop- 2023/call-for-participation?authuser=0&quot;>;机器学习领域的女性&lt;/a>; (WiML) &lt;br>;白金赞助商&lt;br>;小组成员：&lt;strong>;&lt;em>;姚勤&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github.io/&quot;>;联邦学习和分析实践：算法、系统、应用程序和机会&lt;/a>; &lt;br>; 组织者： &lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;徐铮&lt;/em>;&lt;/strong>; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Brendan McMahan&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home?authuser=1&quot;>;医疗保健领域的可解释机器学习&lt;/a>; (IMLH) &lt; br>; 组织者：&lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;数据驱动学习时代的知识与逻辑推理&lt;/a>; &lt;br>; 主办方：&lt;strong>;&lt;em>;Beliz Günel&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;基于偏好的学习的方方面面&lt;/a>; (MFPL) &lt;br>;组织者：&lt;strong>;&lt;em>;罗伯特·布萨-费科特&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;穆罕默德·加瓦姆扎德&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://syns -ml.github.io/2023/&quot;>;科学和机器学习建模的协同作用&lt;/a>; (SynS &amp;amp; ML）&lt;br>;演讲者：&lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot; >;沟通主体的心智理论&lt;/a>; &lt;br>; 主办方：&lt;strong>;&lt;em>;周培&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //sites.google.com/corp/view/aihci/home&quot;>;人工智能与人工智能人机交互&lt;/a>; &lt;br>; 主办方：&lt;em>; &lt;strong>;杨丽&lt;/strong>;&lt;/em>;、&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt; /p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;以数据为中心的机器学习研究&lt;/a>; (DMLR) &lt;br>; 组织者：&lt;strong>;&lt;em>;Alicia Parrish&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>; Isabelle Guyon&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;神经压缩：从信息理论到应用&lt;/a >; &lt;br>; 演讲者：&lt;strong>;&lt;em>;Johannes Ballé&lt;/em>;&lt;/strong>; &lt;br>; 小组成员：&lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p >; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;神经会话人工智能研讨会 - 还剩下什么可以教（值得信赖、增强、适应性强、有能力和以人为中心）聊天机器人？&lt;/a>; &lt;br>; 组织者：&lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /sites.google.com/corp/view/scis-workshop-23&quot;>;虚假相关性、不变性和稳定性&lt;/a>; (SCIS) &lt;br>;组织者：&lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展位活动&lt;/h2>; &lt;div style=&quot;margin-左：20px;&quot;>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; 标题：无监督图嵌入 @ Google（&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;论文&lt;/a>;，&lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO 研讨会&lt;/a>;) &lt;br>; 7 月 25 日星期二上午 10:30 HST &lt;/p>; &lt;p >; 演讲者：&lt;strong>;&lt;em>;徐峥&lt;/em>;&lt;/strong>; &lt;br>; 标题：具有差异隐私的 Gboard 语言模型的联邦学习 (&lt;a href=&quot;https://openreview.net/attachment?id =d8LTNXt97w&amp;amp;name=pdf&quot;>;论文 1&lt;/a>;、&lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;论文 2&lt;/a>;、&lt;a href= &quot;https://ai.googleblog.com/2023/05/making-ml-models- Differentially-private.html&quot;>;博客文章&lt;/a>;) &lt;br>; 7 月 25 日星期二下午 3:30 HST &lt;/ p>; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; 标题：自监督场景理解 (&lt;a href=&quot;https://arxiv.org/abs/2302.04973 &quot;>;论文 1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2203.11194&quot;>;论文 2&lt;/a>;) &lt;br>; 7 月 26 日星期三上午 10:30 HST &lt;/p >; &lt;p>; 演讲者：&lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; 标题：变形金刚在情境中学习通过梯度下降（&lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;论文&lt;/a>;）&lt;br>; 7 月 26 日星期三下午 3:30 HST &lt;/p>; &lt;/div >; &lt;br>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>; *&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2519516457542613363/comments/default&quot; rel=&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html#comment-表单&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;编辑&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+ xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google 在 ICML 2023&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJg Lj1nhzPr3JesD4nvXkj- FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>;&lt;thr:总计>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;发布>;2023-07-20T09:22:00.002- 07:00&lt;/已发布>;&lt;更新>;2023-07-20T15:31:26.737-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= RAI-HCT 亮点&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用社会情境知识，以促进人工智能的负责任应用&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：技术项目经理、社会情境理解工具负责人 Donald Martin, Jr.和解决方案 (SCOUTS)，Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4Y RykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;显示：无；” />; &lt;p>; 人工智能相关产品和技术是在&lt;em>;社会背景&lt;/em>;中构建和部署的：即社会、文化、历史、政治和经济环境的动态且复杂的集合。由于社会环境本质上是动态的、复杂的、非线性的、有争议的、主观的和高度定性的，因此将它们转化为主导标准机器学习 (ML) 方法和负责任的人工智能产品开发的定量表示、方法和实践具有挑战性做法。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;人工智能产品开发的第一个阶段是&lt;em>;问题理解&lt;/em>;，这个阶段对问题的解决方式有着巨大的影响（例如，越来越多的问题）癌症筛查的可用性和准确性）是为 ML 系统制定的，以解决许多其他下游决策，例如数据集和 ML 架构选择。当产品运行的社会环境没有得到充分阐明，无法形成强有力的问题理解时，最终的机器学习解决方案可能会很脆弱，甚至会传播不公平的偏见。 &lt;/p>; &lt;p>; 当人工智能产品开发人员缺乏在开发过程中有效理解和考虑社会背景所需的知识和工具时，他们往往会将其抽象化。这种抽象使他们对他们寻求解决的问题有一个浅层的、定量的理解，而产品用户和社会利益相关者——他们最接近这些问题并嵌入相关的社会背景——往往对这些相同的问题有深刻的定性理解。这种理解复杂问题的定性-定量差异将产品用户和社会与开发人员区分开来，这就是我们所说的“问题理解鸿沟”。 &lt;/p>; &lt;p>; 这种鸿沟在现实世界中产生了影响：例如，它是&lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;种族歧视的根本原因广泛使用的医疗保健算法发现了偏差，旨在解决为特殊计划选择具有最复杂医疗保健需求的患者的问题。对算法运行的社会背景的不完全理解导致系统设计者对关键问题因素形成了错误且过于简单化的因果理论。关键的社会结构因素，包括缺乏医疗保健机会、对医疗保健系统缺乏信任以及由于人为偏见导致的诊断不足，都被排除在外，而医疗保健支出则被强调为复杂的预测因素。健康需要。为了负责任地弥合问题理解鸿沟，人工智能产品开发人员需要工具，将经过社区验证的、关于复杂社会问题的社会背景的结构化知识放在他们的指尖——从问题理解开始，而且贯穿整个产品开发过程。生命周期。为此，&lt;a href=&quot;https://sites.research.google/scouts/&quot;>;社会情境理解工具和解决方案&lt;/a>; (SCOUTS) — &lt;a href=&quot;https://research .google/teams/responsible-ai/&quot;>;Google 研究院内的负责任的人工智能和以人为本的技术&lt;/a>; (RAI-HCT) 团队是一个专门的研究团队，致力于“为人们提供可扩展、值得信赖的技术”实现负责任、强大的人工智能并解决世界上最复杂的社会问题所需的社会背景知识。” SCOUTS 的动力来自于阐明社会背景的重大挑战，它进行创新的基础和应用研究，以产生结构化的社会背景知识，并将其整合到人工智能相关产品开发生命周期的所有阶段。去年，我们&lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;宣布&lt;/a>;&lt;a href=&quot;https:// jigsaw.google.com/&quot;>;Jigsaw&lt;/a>; 是 Google 的技术孵化器，旨在探索开放社会威胁的解决方案，在模型开发的数据准备和评估阶段，利用我们的结构化社会背景知识方法来扩大偏见缓解范围他们广泛使用的 &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; 毒性分类器。展望未来，SCOUTS 的研究议程重点关注人工智能相关产品开发的问题理解阶段，目标是弥合问题理解鸿沟。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;弥合人工智能问题理解鸿沟&lt;/h2>; &lt;p>;弥合人工智能问题理解鸿沟需要两个关键要素：1）用于组织结构化社会背景知识的参考框架；2）参与式非提取方法，以获取有关复杂问题的社区专业知识并将其表示为结构化知识。 SCOUTS 在这两个领域都发表了创新研究。&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot;loop=&quot;&quot; muted=&quot;&quot;playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0% ;” width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt; div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;理解鸿沟问题的说明。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;社会背景参考框架&lt;/h3>; &lt;p >; 产生结构化知识的一个基本要素是创建组织结构的分类法。 SCOUTS 与其他 RAI-HCT 团队合作 (&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;、&lt;a href =&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;影响实验室&lt;/a>;)，&lt;a href=&quot;https://www.deepmind. com/&quot;>;Google DeepMind&lt;/a>; 和外部系统动力学专家&lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;针对社会背景开发分类参考框架&lt;/a>;。为了应对社会环境的复杂性、动态性和适应性，我们利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_adaptive_system&quot;>;复杂适应性系统&lt;/a>; (CAS) 理论提出用于组织社会背景知识的高级分类模型。该模型精确指出了社会环境的三个关键要素以及将它们结合在一起的动态反馈循环：代理、戒律和工件。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;代理人&lt;/em>;：可以是个人或机构。 &lt;/li>;&lt;li>;&lt;em>;戒律&lt;/em>;：约束和驱动主体行为的先入之见，包括信仰、价值观、刻板印象和偏见。基本规则的一个例子是“所有篮球运动员都超过 6 英尺高”。这种限制性假设可能会导致无法识别身材较小的篮球运动员。 &lt;/li>;&lt;li>;&lt;em>;人工制品&lt;/em>;：主体行为产生多种人工制品，包括语言、数据、技术、社会问题和产品。 &lt;/li>; &lt;/ul>; &lt;p>; 这些实体之间的关系是动态且复杂的。我们的工作假设戒律是社会环境中最关键的元素，并且我们强调&lt;em>;人们感知到的问题&lt;/em>;以及&lt;em>;他们所持有的关于这些问题为何存在的因果理论&lt;/em>;作为特别有影响力的戒律，是理解社会背景的核心。例如，在前面描述的医疗算法中存在种族偏见的情况下，设计者所持有的因果理论规则是，复杂的健康问题将导致所有人群的医疗支出增加。这一不正确的规则直接导致选择医疗保健支出作为模型预测复杂医疗保健需求的代理变量，这反过来又导致该模型对黑人患者产生偏见，这些黑人患者由于缺乏医疗保健和缺乏医疗保健等社会因素而存在偏见。由于平均偏差而导致诊断不足，当他们有复杂的医疗保健需求时，并不总是在医疗保健上花费更多。一个关键的开放问题是，我们如何以道德和公平的方式从最接近不平等问题的人和社区中引出因果理论，并将其转化为有用的结构化知识？ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0E uHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4Zay BDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;社会背景参考框架的说明性版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding= &quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB- Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNC KRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWilmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUISHdlors19GZ0WuikJGz 6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;社会背景参考框架的分类版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;与社区合作，促进人工智能在医疗保健领域的负责任应用&lt;/h3>; &lt;p>; 自成立以来，SCOUTS 一直致力于&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive -ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>;在历史上被边缘化的社区进行能力建设&lt;/a>;，以阐明与重要的复杂问题有关的更广泛的社会背景他们使用一种称为基于社区的系统动力学（CBSD）的实践。 &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;系统动力学&lt;/a>;（SD）是一种阐明复杂问题因果理论的方法论，无论是&lt;em>;定性&lt;strong>;&lt;/强>;&lt;/em>;作为因果循环和&lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;库存和流量图表&lt;/a>;（分别为 CLD 和 SFD）和&lt;em>;定量&lt;/em>;作为仿真模型。视觉定性工具、定量方法和协作模型构建的固有支持使其成为弥合问题理解鸿沟的理想成分。 CBSD 是一个&lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;基于社区的，SD 的参与式变体&lt;/a>;特别注重社区内部的能力建设，以协作方式将他们面临的问题描述和建模为因果理论，而无需中介。通过 CBSD，我们见证了社区团体在 2 小时内学习基础知识并开始绘制 CLD。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM 13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyY rvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; 社区成员学习系统动态。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;p>;人工智能在&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms% 20可以%20分析%20医疗、疾病%20更多%20准确%20并且快速%20。&quot;>;改善医疗诊断&lt;/a>;。但人工智能相关健康诊断算法的安全性、公平性和可靠性取决于多样化且平衡的训练数据集。健康诊断领域的一个公开挑战是缺乏来自历史边缘群体的训练样本数据。 SCOUTS 与 &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; 社区和 CBSD 专家合作制作了&lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot; >;数据差距问题的定性和定量因果理论&lt;/a>;。这些理论包括构成健康诊断更广泛社会背景的关键因素，包括死亡的文化记忆和对医疗的信任。 &lt;/p>; &lt;p>; 下图描绘了上述协作期间生成的因果理论作为 CLD。它假设对医疗保健的信任会影响这个复杂系统的所有部分，并且是增加筛查的关键杠杆，而筛查反过来又会生成数据以克服数据多样性差距。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQ Df-c0Tv-JIUAI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI /s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jiIUaI9FFsbeiza BGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动” ;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2Vq XJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCedzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y /s1072/image2.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcuaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzI iJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;健康诊断数据差距的因果循环图&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;这些社区来源的因果理论是弥合问题理解鸿沟与值得信赖的社会的第一步&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 正如本博客中所讨论的，理解鸿沟问题是负责任的人工智能领域的一项关键的开放挑战。SCOUTS 与 Google 研究院的其他团队、外部社区以及跨多个学科的学术合作伙伴合作进行探索性和应用性研究，以在解决该问题方面取得有意义的进展。展望未来，我们的工作将重点关注三个关键要素，以我们的&lt;a href=&quot;http://ai.google/principles&quot;>;人工智能原则&lt;/a>;为指导：&lt;/p>; &lt;ol>; &lt;li>;提高意识和通过讲座、出版物和培训了解问题、理解鸿沟及其影响。 &lt;/li>;&lt;li>;开展基础和应用研究，将社会背景知识表示并整合到人工智能产品开发工具和工作流程中，从概念到监测、评估和适应。 &lt;/li>;&lt;li>;将基于社区的因果建模方法应用于人工智能健康公平领域，以实现影响并增强社会和 Google 生成和利用全球范围社会背景知识的能力，以实现负责任的人工智能。 &lt;/li>; &lt;/ol>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto；margin-right：auto；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47Nw zxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73Day9XL-CEtVDRJfZIwhBpa99m/s960/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEG CcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;SCOUTS 飞轮用于弥合问题理解鸿沟。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 John Guilyard 的图形开发、SCOUTS 中的每个人以及我们所有的合作者和赞助商。&lt;/em>; &lt;/p>; &lt; /content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/ 2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;利用社会背景知识促进人工智能的负责任应用&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name >;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRy kesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot;宽度= “72” xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;发布>;2023-07-18T13:15:00.000-07:00&lt;/发布>;&lt;更新>;2023-07-18T13:15： 15.633-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>; blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt; /category>;&lt;title type=&quot;text&quot;>;SimPer：周期性目标的简单自我监督学习&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Staff Research 的 Daniel McDuff科学家和学生研究员 Yuzhe Yang，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNg x0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer %20hero.jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 从周期性数据（重复的信号，例如心跳或地球表面的每日温度变化）中学习对于许多现实世界的应用程序至关重要，来自 &lt;a href=&quot;https://cloud.google .com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;监控天气系统&lt;/a>;到&lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and -wellness-your-phone/&quot;>;检测生命体征&lt;/a>;。例如，在环境遥感领域，通常需要定期学习来实现环境变化的即时预报，例如&lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to -nowcast.html&quot;>;降水模式或地表温度&lt;/a>;。在健康领域，从视频测量中学习已证明可以提取（准）周期性生命体征，例如心房颤动&lt;/a>; 和&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;睡眠呼吸暂停发作&lt;/a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 类似&lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos. html&quot;>;RepNet&lt;/a>; 强调了此类任务的重要性，并提出了一种识别单个视频中重复活动的解决方案。然而，这些是受监督的方法，需要大量数据来捕获重复活动，所有数据都标记为指示操作重复的次数。标记此类数据通常具有挑战性且需要大量资源，需要研究人员手动捕获与感兴趣的模态（例如视频或卫星图像）同步的黄金标准时间测量。 &lt;/p>; &lt;p>; 或者，自我监督学习（SSL）方法（例如，&lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html &quot;>;SimCLR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;)，它们利用大量未标记的数据来学习捕获周期性或准周期的表示-周期性时间动态，在&lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;解决分类任务&lt;/a>;方面取得了成功。然而，他们忽视了数据中固有的周期性（即识别帧是否是周期性过程的一部分的能力），并且无法学习捕获周期性或频率属性的鲁棒表示。这是因为定期学习表现出与普遍学习任务不同的特征。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5 ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/ s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ _o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= &quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;与静态特征（例如图像）相比，周期性表示上下文中的特征相似性是不同的。例如，因短时间延迟而偏移或反转的视频应与原始样本相似，而按因子 &lt;em>;x&lt;/em>; 上采样或下采样的视频应与原始样本相差&lt;em>;x&lt;/em>; 的因子。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 为了应对这些挑战，在“&lt;a href=&quot;https:// openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer：周期性目标的简单自我监督学习&lt;/a>;”，发表于第十一届&lt;a href=&quot;https://iclr.cc/&quot;>;国际学习会议表示&lt;/a>;（ICLR 2023），我们引入了一种用于学习数据中周期性信息的自监督对比框架。具体来说，SimPer 使用时间自对比学习来利用周期性目标的时间属性，其中正样本和负样本是通过来自相同目标的周期性不变和周期性变化增强来获得的输入实例。我们提出&lt;em>;周期性特征相似性&lt;/em>;，它明确定义了如何在周期性学习的背景下测量相似性。此外，我们设计了一种广义对比损失&lt;em>;&lt;/em>;，将经典的&lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE损失&lt;/a>;扩展到软回归变体可以对连续标签（频率）进行对比。接下来，我们证明与最先进的 SSL 方法相比，SimPer 可以有效地学习周期特征表示，并强调其有趣的特性，包括更好的数据效率、对虚假相关性的鲁棒性以及对分布变化的泛化。最后，我们很高兴与研究社区一起发布 &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer 代码存储库&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SimPer 框架&lt;/h2>; &lt;p>; SimPer 引入了一个时间自我对比学习框架。正样本和负样本是通过同一输入实例的周期性不变和周期性变化增强获得的。对于时间视频示例，周期性不变的变化是裁剪、旋转或翻转，而周期性变化的变化涉及增加或降低视频的速度。 &lt;/p>; &lt;p>; 为了明确定义如何在周期性学习的背景下测量相似性，SimPer 提出了周期性特征相似性。这种结构使我们能够将训练制定为对比学习任务。可以使用没有任何标签的数据来训练模型，然后根据需要进行微调，以将学习到的特征映射到特定的频率值。 &lt;/p>; &lt;p>; 给定一个输入序列&lt;em>;x&lt;/em>;，我们知道存在一个潜在的相关周期信号。然后，我们变换&lt;em>;x&lt;/em>;来创建一系列速度或频率改变的样本，这改变了潜在的周期性目标，从而创建了不同的负面视图。尽管原始频率未知，但我们有效地为未标记的输入 x 设计伪速度或频率标签。 &lt;/p>; &lt;p>; 传统的&lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;相似性度量&lt;/a>;，例如&lt;a href=&quot;https://en.wikipedia.org /wiki/Cosine_similarity&quot;>;余弦相似度&lt;/a>;强调两个特征向量之间的严格接近，并且对索引移位特征（代表不同时间戳）、反转特征和频率变化的特征敏感。相反，对于具有较小时间偏移和/或反向索引的样本，周期性特征相似性应该很高，同时在特征频率变化时捕获连续的相似性变化。这可以通过频域中的相似性度量来实现，例如两个&lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;傅立叶变换&lt;/a>;之间的距离。 &lt;/p>; &lt;p>; 为了利用频域中增强样本的内在连续性，SimPer 设计了一种广义对比损失，它扩展了经典的 &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>; InfoNCE&lt;/a>; 损失为软回归变体，可以对连续标签（频率）进行对比。这使得它适合回归任务，其目标是恢复连续信号，例如心跳。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUv HgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm60 7Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;SimPer 通过频域变换构建数据的负面视图。输入序列&lt;em>;x&lt;/em>;具有底层关联的周期信号。 SimPer 转换&lt;em>;x&lt;/em>;来创建一系列速度或频率改变的样本，这改变了底层的周期性目标，从而创建了不同的负面视图。尽管原始频率未知，但我们有效地为未标记的输入&lt;em>;x&lt;/em>;设计了伪速度或频率标签（周期变量增强&lt;em>;τ&lt;/em>;）。 SimPer 采用不改变输入身份的变换，并将其定义为周期性不变的增强&lt;em>;σ&lt;/em>;，从而创建样本的不同正面视图。然后，它将这些增强视图发送到编码器&lt;em>;f&lt;/em>;，&lt;em>; &lt;/em>;提取相应的特征。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 为了评估 SimPer 的性能，我们将其与最先进的 SSL 方案（例如，&lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;、 &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;，&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e- Paper.pdf&quot;>;BYOL&lt;/a>;、&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;）在一组六个不同的周期上用于人类行为分析、环境遥感和医疗保健中常见现实任务的学习数据集。具体来说，下面我们展示了视频中的心率测量和运动重复计数的结果。结果表明，SimPer 在所有六个数据集上都优于最先进的 SSL 方案，突显了其在数据效率、对虚假相关性的鲁棒性以及对未见目标的泛化方面的卓越性能。 &lt;/p>; &lt;p>; 在这里，我们使用 SimPer 显示了两个代表性数据集的定量结果，这些数据集使用各种 SSL 方法进行预训练，并对标记数据进行微调。首先，我们使用 &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ 预训练 SimPer。 Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) 数据集，人类&lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;光电体积描记术&lt;/a>;和心率预测数据集，并将其性能与最先进的 SSL 方法进行比较。我们观察到 SimPer 优于 SimCLR、MoCo v2、BYOL 和 CVRL 方法。人类行为计数数据集 &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>; 的结果进一步证实了 SimPer 相对于其他方法的优势，因为它的性能明显优于其他方法监督基线。有关特征评估结果以及在其他数据集上的表现，请参阅&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorce K9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1 YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Univ 上 SimCLR、MoCo v2、BYOL、CVRL 和 SimPer 的结果。勃艮第弗朗什孔泰远程光电体积描记法 (UBFC) 和 Countix 数据集。心率和重复次数表现报告为&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;平均绝对误差&lt;/a>; (MAE)。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和应用&lt;/h2>; &lt;p>; 我们提出 SimPer，一个用于学习数据中周期性信息的自监督对比框架。我们证明，通过结合时间自对比学习框架、周期性不变和周期性变化增强以及连续周期性特征相似性，SimPer 提供了一种直观且灵活的方法来学习周期性信号的强特征表示。此外，SimPer可以应用于从环境遥感到医疗保健的各个领域。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢杨宇哲、刘鑫、Ming-Zher Poh、Jiang Wu、Silviu Borac 和 Dina Katabi 对这项工作的贡献。 &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel =&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/ >;&lt;link href=&quot;http://ai.googleblog.com/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer：周期性目标的简单自监督学习&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com &lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded .gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN -dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR _u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;已发布>;2023-07-13T14:01 :00.001-07:00&lt;/已发布>;&lt;更新>;2023-07-13T14:01:18.428-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;机器智能&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;符号调整可改善语言模型中的上下文学习&lt;/stitle>;&lt;content type= &quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究中心学生研究员 Jerry Wei 和首席科学家 Denny Zhou&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRicUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiI j-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人类智能的一个关键特征是，人类可以仅使用几个例子进行推理来学习执行新任务。 Scaling up language models has unlocked a range of new applications and paradigms in machine learning, including the ability to perform challenging reasoning tasks via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>;. Language models, however, are still sensitive to the way that prompts are given, indicating that they are not reasoning in a robust manner. For instance, language models often require heavy prompt engineering or phrasing tasks as instructions, and they exhibit unexpected behaviors such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;performance on tasks being unaffected even when shown incorrect labels&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;Symbol tuning improves in-context learning in language models&lt;/a>;”, we propose a simple fine-tuning procedure that we call &lt;em>;symbol tuning&lt;/em>;, which can improve in-context learning by emphasizing input–label mappings. We experiment with symbol tuning across &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Flan-PaLM&lt;/a>; models and observe benefits across various settings. &lt;/p>; &lt;ul>; &lt;li>;Symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. &lt;/li>;&lt;li>;Symbol-tuned models are much stronger at algorithmic reasoning tasks. &lt;/li>;&lt;li>;Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of symbol tuning, where models are fine-tuned on tasks where natural language labels are replaced with arbitrary symbols. Symbol tuning relies on the intuition that when instruction and relevant labels are not available, models must use in-context examples to learn the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Motivation&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a common fine-tuning method that has been shown to improve performance and allow models to better follow in-context examples. One shortcoming, however, is that models are not forced to learn to use the examples because the task is redundantly defined in the evaluation example via instructions and natural language labels. For example, on the left in the figure above, although the examples can help the model understand the task (sentiment analysis), they are not strictly necessary since the model could ignore the examples and just read the instruction that indicates what the task is. &lt;/p>; &lt;p>; In symbol tuning, the model is fine-tuned on examples where the instructions are removed and natural language labels are replaced with semantically-unrelated labels (eg, “Foo,” “Bar,” etc.). In this setup, the task is unclear without looking at the in-context examples. For example, on the right in the figure above, multiple in-context examples would be needed to figure out the task. Because symbol tuning teaches the model to reason over the in-context examples, symbol-tuned models should have better performance on tasks that require reasoning between in-context examples and their labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Datasets and task types used for symbol tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Symbol-tuning procedure&lt;/h2>; &lt;p>; We selected 22 publicly-available &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) datasets that we use for our symbol-tuning procedure. These tasks have been widely used in the past, and we only chose classification-type tasks since our method requires discrete labels. We then remap labels to a random label from a set of ~30K arbitrary labels selected from one of three categories: integers, character combinations, and words. &lt;/p>; &lt;p>; For our experiments, we symbol tune &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, the instruction-tuned variants of &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;. We use three different sizes of Flan-PaLM models: Flan-PaLM-8B, Flan-PaLM-62B, and Flan-PaLM-540B. We also tested &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>; (Flan-PaLM-62B at 1.3T tokens instead of 780B tokens), which we abbreviate as 62B-c. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;861&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a set of ∼300K arbitrary symbols from three categories (integers, character combinations, and words). ∼30K symbols are used during tuning and the rest are held out for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; We want to evaluate a model&#39;s ability to perform unseen tasks, so we cannot evaluate on tasks used in symbol tuning (22 datasets) or used during instruction tuning (1.8K tasks). Hence, we choose 11 NLP datasets that were not used during fine-tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;In-context learning&lt;/h2>; &lt;p>; In the symbol-tuning procedure, models must learn to reason with in-context examples in order to successfully perform tasks because prompts are modified to ensure that tasks cannot simply be learned from relevant labels or instructions. Symbol-tuned models should perform better in settings where tasks are unclear and require reasoning between in-context examples and their labels. To explore these settings, we define four in-context learning settings that vary the amount of reasoning required between inputs and labels in order to learn the task (based on the availability of instructions/relevant labels) &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the availability of instructions and relevant natural language labels, models may need to do varying amounts of reasoning with in-context examples. When these features are not available, models must reason with the given in-context examples to successfully perform the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Symbol tuning improves performance across all settings for models 62B and larger, with small improvements in settings with relevant natural language labels (+0.8% to +4.2%) and substantial improvements in settings without relevant natural language labels (+5.5% to +15.5%). Strikingly, when relevant labels are unavailable, symbol-tuned Flan-PaLM-8B outperforms FlanPaLM-62B, and symbol-tuned Flan-PaLM-62B outperforms Flan-PaLM-540B. This performance difference suggests that symbol tuning can allow much smaller models to perform as well as large models on these tasks (effectively saving ∼10X inference compute). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Large-enough symbol-tuned models are better at in-context learning than baselines, especially in settings where relevant labels are not available. Performance is shown as average model accuracy (%) across eleven tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithmic reasoning&lt;/h2>; &lt;p>; We also experiment on algorithmic reasoning tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;. There are two main groups of tasks: 1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;List functions&lt;/a>; — identify a transformation function (eg, remove the last element in a list) between input and output lists containing non-negative integers; and 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;simple turing concepts&lt;/a>; — reason with binary strings to learn the concept that maps an input to an output (eg, swapping 0s and 1s in a string). &lt;/p>; &lt;p>; On the list function and simple turing concept tasks, symbol tuning results in an average performance improvement of 18.2% and 15.3%, respectively. Additionally, Flan-cont-PaLM-62B with symbol tuning outperforms Flan-PaLM-540B on the list function tasks on average, which is equivalent to a ∼10x reduction in inference compute. These improvements suggest that symbol tuning strengthens the model&#39;s ability to learn in-context for unseen task types, as symbol tuning did not include any algorithmic data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models achieve higher performance on list function tasks and simple turing concept tasks. (A–E): categories of list functions tasks. (F): simple turing concepts task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In the flipped-label experiment, labels of in-context and evaluation examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override prior knowledge. &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;Previous work&lt;/a>; has shown that while pre-trained models (without instruction tuning) can, to some extent, follow flipped labels presented in-context, instruction tuning degraded this ability. &lt;/p>; &lt;p>; We see that there is a similar trend across all model sizes — symbol-tuned models are much more capable of following flipped labels than instruction-tuned models. We found that after symbol tuning, Flan-PaLM-8B sees an average improvement across all datasets of 26.5%, Flan-PaLM-62B sees an improvement of 33.7%, and Flan-PaLM-540B sees an improvement of 34.0%. Additionally, symbol-tuned models achieve similar or better than average performance as pre-training–only models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models are much better at following flipped labels presented in-context than instruction-tuned models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented symbol tuning, a new method of tuning models on tasks where natural language labels are remapped to arbitrary symbols. Symbol tuning is based off of the intuition that when models cannot use instructions or relevant labels to determine a presented task, it must do so by instead learning from in-context examples. We tuned four language models using our symbol-tuning procedure, utilizing a tuning mixture of 22 datasets and approximately 30K arbitrary symbols as labels. &lt;/p>; &lt;p>; We first showed that symbol tuning improves performance on unseen in-context learning tasks, especially when prompts do not contain instructions or relevant labels. We also found that symbol-tuned models were much better at algorithmic reasoning tasks, despite the lack of numerical or algorithmic data in the symbol-tuning procedure. Finally, in an in-context learning setting where inputs have flipped labels, symbol tuning (for some datasets) restores the ability to follow flipped labels that was lost during instruction tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; Through symbol tuning, we aim to increase the degree to which models can examine and learn from input–label mappings during in-context learning. We hope that our results encourage further work towards improving language models&#39; ability to reason over symbols presented in-context. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. This work was conducted by Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. We would like to thank our colleagues at Google Research and Google DeepMind for their advice and helpful discussions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2013323302512835157/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning-improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Symbol tuning improves in-context learning in language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;published>;2023-07-11T10:00:00.010-07:00&lt;/published>;&lt;updated>;2023-07-11T10:44:15.111-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An open-source gymnasium for machine learning assisted computer architecture design&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Amir Yazdanbakhsh, Research Scientist, and Vijay Janapa Reddi, Visiting Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;Computer Architecture&lt;/a>; research has a long history of developing simulators and tools to evaluate and shape the design of computer systems. For example, the &lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNvNONNQ&quot;>;SimpleScalar&lt;/a>; simulator was introduced in the late 1990s and allowed researchers to explore various &lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;microarchitectural&lt;/a>; ideas. Computer architecture simulators and tools, such as &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>;, and many more have played a significant role in advancing computer architecture research. Since then, these shared resources and infrastructure have benefited industry and academia and have enabled researchers to systematically build on each other&#39;s work, leading to significant advances in the field. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Nonetheless, computer architecture research is evolving, with industry and academia turning towards machine learning (ML) optimization to meet stringent domain-specific requirements, such as &lt;a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;ML for computer architecture&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2201.01863.pdf&quot;>;ML for TinyML acceleration&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN&lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;accelerator&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3503222.3507767&quot;>;datapath optimization&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z46Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;memory controllers&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;power consumption&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7430287&quot;>;security&lt;/a>;, and &lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;privacy&lt;/a>;. Although prior work has demonstrated the benefits of ML in design optimization, the lack of strong, reproducible baselines hinders fair and objective comparison across different methods and poses several challenges to their deployment. To ensure steady progress, it is imperative to understand and tackle these challenges collectively. &lt;/p>; &lt;p>; To alleviate these challenges, in “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design&lt;/a>;”, accepted at &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>;, we introduced ArchGym, which includes a variety of computer architecture simulators and ML algorithms. Enabled by ArchGym, our results indicate that with a sufficiently large number of samples, any of a diverse collection of ML algorithms are capable of finding the optimal set of architecture design parameters for each target problem; &lt;i>;no one solution is necessarily better than another&lt;/i>;. These results further indicate that selecting the optimal hyperparameters for a given ML algorithm is essential for finding the optimal architecture design, but choosing them is non-trivial. We &lt;a href=&quot;https://bit.ly/ArchGym&quot;>;release&lt;/a>; the code and dataset across multiple computer architecture simulations and ML algorithms.&lt;/p>; &lt;br />; &lt;h2>;Challenges in ML-assisted architecture research &lt;/h2>; &lt;p>; ML-assisted architecture research poses several challenges, including: &lt;/p>; &lt;ol>; &lt;li>;For a specific ML-assisted computer architecture problem (eg, finding an optimal solution for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; controller) there is no systematic way to identify optimal ML algorithms or hyperparameters (eg, learning rate, warm-up steps, etc.). There is a wider range of ML and heuristic methods, from &lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;random walk&lt;/a>; to &lt;a href=&quot;http://incompleteideas.net/book/RLbook2020.pdf&quot;>;reinforcement learning&lt;/a>; (RL), that can be employed for &lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;design space exploration&lt;/a>; (DSE). While these methods have shown noticeable performance improvement over their choice of baselines, it is not evident whether the improvements are because of the choice of optimization algorithms or hyperparameters.&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;Thus, to ensure reproducibility and facilitate widespread adoption of ML-aided architecture DSE, it is necessary to outline a systematic benchmarking methodology.&lt;/em>; &lt;br />;&lt;br />; &lt;/li>; &lt;li>;While computer architecture simulators have been the backbone of architectural innovations, there is an emerging need to address the trade-offs between accuracy, speed, and cost in architecture exploration. The accuracy and speed of performance estimation widely varies from one simulator to another, depending on the underlying modeling details (eg, &lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;cycle&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;accurate&lt;/a>; vs. &lt;a href=&quot;https://arxiv.org/abs/2210.03894&quot;>;ML&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;based&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1808.07412&quot;>;proxy&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;models&lt;/a>;). While analytical or ML-based proxy models are nimble by virtue of discarding low-level details, they generally suffer from high prediction error. Also, due to commercial licensing, there can be strict &lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;limits on the number of runs collected from a simulator&lt;/a>;. Overall, these constraints exhibit distinct performance vs. sample efficiency trade-offs, affecting the choice of optimization algorithm for architecture exploration. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;It is challenging to delineate how to systematically compare the effectiveness of various ML algorithms under these constraints.&lt;/em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;Finally, the landscape of ML algorithms is rapidly evolving and some ML algorithms need data to be useful. Additionally, rendering the outcome of DSE into meaningful artifacts such as datasets is critical for drawing insights about the design space. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;In this rapidly evolving ecosystem, it is consequential to ensure how to amortize the overhead of search algorithms for architecture exploration. It is not apparent, nor systematically studied how to leverage exploration data while being agnostic to the underlying search algorithm.&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym design&lt;/h2>; &lt;p>; ArchGym addresses these challenges by providing a unified framework for evaluating different ML-based search algorithms fairly. It comprises two main components: 1) the ArchGym environment and 2) the ArchGym agent. The environment is an encapsulation of the architecture cost model — which includes latency, throughput, area, energy, etc., to determine the computational cost of running the workload, given a set of architectural parameters — paired with the target workload(s). The agent is an encapsulation of the ML algorithm used for the search and consists of hyperparameters and a guiding policy. The hyperparameters are intrinsic to the algorithm for which the model is to be optimized and can significantly influence performance. The policy, on the other hand, determines how the agent selects a parameter iteratively to optimize the target objective. &lt;/p>; &lt;p>; Notably, ArchGym also includes a standardized interface that connects these two components, while also saving the exploration data as the ArchGym Dataset. At its core, the interface entails three main signals: &lt;i>;hardware state&lt;/i>;, &lt;i>;hardware parameters&lt;/i>;, and &lt;i>;metrics&lt;/i>;. These signals are the bare minimum to establish a meaningful communication channel between the environment and the agent.&amp;nbsp;Using these signals, the agent observes the state of the hardware and suggests a set of hardware parameters to iteratively optimize a (user-defined) reward. The reward is a function of hardware performance metrics, such as performance, energy consumption, etc.&amp;nbsp;&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1226&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym-animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym comprises two main components: the ArchGym environment and the ArchGym agent. The ArchGym environment encapsulates the cost model and the agent is an abstraction of a policy and hyperparameters. With a standardized interface that connects these two components, ArchGym provides a unified framework for evaluating different ML-based search algorithms fairly while also saving the exploration data as the ArchGym Dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ML algorithms could be equally favorable to meet user-defined target specifications&lt;/h2>; &lt;p>; Using ArchGym, we empirically demonstrate that across different optimization objectives and DSE problems, &lt;em>;at least one set of hyperparameters exists that results in the same hardware performance as other ML algorithms&lt;/em>;. A poorly selected (random selection) hyperparameter for the ML algorithm or its baseline can lead to a misleading conclusion that a particular family of ML algorithms is better than another. We show that with sufficient hyperparameter tuning, different search algorithms, even &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;random walk&lt;/a>; (RW), are able to identify the best possible reward. However, note that finding the right set of hyperparameters may require exhaustive search or even luck to make it competitive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With a sufficient number of samples, there exists at least one set of hyperparameters that results in the same performance across a range of search algorithms. Here the dashed line represents the maximum normalized reward. &lt;i>;Cloud-1&lt;/i>;, &lt;i>;cloud-2&lt;/i>;, &lt;i>;stream&lt;/i>;, and &lt;i>;random&lt;/i>; indicate four different memory traces for &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; (DRAM subsystem design space exploration framework).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset construction and high-fidelity proxy model training&lt;/h2>; &lt;p>; Creating a unified interface using ArchGym also enables the creation of datasets that can be used to design better data-driven ML-based proxy architecture cost models to improve the speed of architecture simulation. To evaluate the benefits of datasets in building an ML model to approximate architecture cost, we leverage ArchGym&#39;s ability to log the data from each run from DRAMSys to create four dataset variants, each with a different number of data points. For each variant, we create two categories: (a) Diverse Dataset, which represents the data collected from different agents (&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/a>;), and (b) ACO only, which shows the data collected exclusively from the ACO agent, both of which are released along with ArchGym. We train a proxy model on each dataset using &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;random forest regression&lt;/a>; with the objective to predict the latency of designs for a &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM simulator&lt;/a>;. Our results show that: &lt;/p>; &lt;ol>; &lt;li>;As we increase the dataset size, the average normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;root mean squared error&lt;/a>; (RMSE) slightly decreases. &lt;/li>;&lt;li>;However, as we introduce diversity in the dataset (eg, collecting data from different agents), we observe 9× to 42× lower RMSE across different dataset sizes. &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1143&quot; data-original-width=&quot;1826&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diverse dataset collection across different agents using ArchGym interface.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/s1803/ArchGym1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The impact of a diverse dataset and dataset size on the normalized RMSE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;The need for a community-driven ecosystem for ML-assisted architecture research&lt;/h2>; &lt;p>;While, ArchGym is an initial effort towards creating an open-source ecosystem that (1) connects a broad range of search algorithms to computer architecture simulators in an unified and easy-to-extend manner, (2) facilitates research in ML-assisted computer architecture, and (3) forms the scaffold to develop reproducible baselines, there are a lot of open challenges that need community-wide support. Below we outline some of the open challenges in ML-assisted architecture design. Addressing these challenges requires a well coordinated effort and a community driven ecosystem.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot; data-original-width=&quot;1999&quot; height=&quot;310&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key challenges in ML-assisted architecture design.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We call this ecosystem &lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/&quot;>;Architecture 2.0&lt;/a>;.&amp;nbsp;We outline the key challenges and a vision for building an inclusive ecosystem of interdisciplinary researchers to tackle the long-standing open problems in applying ML for computer architecture research.&amp;nbsp;If you are interested in helping shape this ecosystem, please fill out the &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/viewform&quot;>;interest survey&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>; is an open source gymnasium for ML architecture DSE and enables an standardized interface that can be readily extended to suit different use cases. Additionally, ArchGym enables fair and reproducible comparison between different ML algorithms and helps to establish stronger baselines for computer architecture research problems. &lt;/p>; &lt;p>; We invite the computer architecture community as well as the ML community to actively participate in the development of ArchGym. We believe that the creation of a gymnasium-type environment for computer architecture research would be a significant step forward in the field and provide a platform for researchers to use ML to accelerate research and lead to new and innovative designs. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This blogpost is based on joint work with several co-authors at Google and Harvard University.&amp;nbsp;&lt;/i>;&lt;i>;We would like to acknowledge and highlight Srivatsan Krishnan (Harvard) who contributed several ideas to this project in collaboration with Shvetank Prakash (Harvard), Jason Jabbour (Harvard), Ikechukwu Uchendu (Harvard), Susobhan Ghosh (Harvard), Behzad Boroujerdian (Harvard), Daniel Richins (Harvard), Devashree Tripathy (Harvard), and Thierry Thambe (Harvard).&amp;nbsp; In addition, we would also like to thank James Laudon, Douglas Eck, Cliff Young, and Aleksandra Faust for their support, feedback, and motivation for this work. We would also like to thank John Guilyard for the animated figure used in this post. Amir Yazdanbakhsh is now a Research Scientist at Google DeepMind and Vijay Janapa Reddi is an Associate Professor at Harvard.&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;An open-source gymnasium for machine learning assisted computer architecture design&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;published>;2023-07-10T06:02:00.005-07:00&lt;/published>;&lt;updated>;2023-07-21T13:24:14.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://2023.aclweb.org/&quot;>;61st annual meeting&lt;/a>; of the &lt;a href=&quot;https://www.aclweb.org/&quot;>;Association for Computational Linguistics&lt;/a>; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place in Vancouver, BC. As a leader in natural language processing and understanding, and a&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;Diamond Level sponsor&lt;/a>;&amp;nbsp;of&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, Google will showcase the latest research in the field with over 50 publications, and active involvement in a variety of workshops and tutorials.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;If you&#39;re registered for ACL 2023, we hope that you&#39;ll visit the Google booth to learn more about the projects at Google that go into solving interesting problems for billions of people.您还可以在下面了解有关 Google 参与的更多信息（Google 隶属关系以&lt;b>;粗体&lt;/b>;显示）。&lt;/p>; &lt;br />; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left : 20px;&quot;>; &lt;p>; 区域椅子包括：&lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; 工作坊椅子包括：&lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/ strong>; &lt;br />; 出版主席包括：&lt;strong>;&lt;em>;雷舒&lt;/em>;&lt;/strong>; &lt;br />; 程序委员会包括：&lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>; 、&lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;聚光灯论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd：印度尼西亚 NLP 资源开源倡议&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya、Holy Lovenia、Alham Fikri Aji、Genta Winata、Bryan Wilie、Fajri Koto、Rahmad Mahendra、Christian Wibisono、Ade Romadhony、Karissa Vincentio、Jennifer Santoso、David Moeljadi、Cahya Wirawan , Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda、Ryan Hadiwijaya、Ryandito Diandaru、Tiezheng Yu、Vito Ghifari、戴文亮、Yan Xu、Dyah Damapuspita、Haryo Wibowo、Cuk Tho、Ichwanul Karo Karo、Tirana Fatyanosa、Ziwei Ji、Graham Neubig、Timothy Baldwin、&lt;strong>;Sebastian Ruder&lt;/strong>;、Pascale Fung、Herry Sujaini、Sakriani Sakti、Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;优化测试-密集检索的时间查询表示&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung、Jungsoo Park、Jaewoo Kang、Danqi Chen、&lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt：用于命题级分割和蕴涵识别的大型语料库&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;、&lt;strong>;Alex Fabrikant&lt;/strong>;、Dan Roth、&lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2305.02301.pdf&quot;>;逐步蒸馏！使用较少的训练数据和较小的模型规模超越较大的语言模型&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*、&lt;strong>;Chun-Liang Li&lt;/strong>;、&lt;strong>;Chih-Kuan Yeh&lt;/ &lt;strong>;、&lt;strong>;Hootan Nakhost&lt;/strong>;、&lt;strong>;藤井康久&lt;/strong>;、Alex Ratner、Ranjay Krishna、&lt;strong>;李振宇&lt;/strong>;、&lt;strong>;托马斯·普菲斯特&lt;/strong>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;具有可控工作记忆的大型语言模型&lt;/a>; &lt;br />; &lt;em>;&lt; b>;李大良&lt;/b>;、&lt;b>;Ankit Singh Rawat&lt;/b>;、&lt;b>;Manzil Zaheer&lt;/b>;、&lt;b>;王鑫&lt;/b>;、&lt;b>;Michal Lukasik&lt;/b>;、 &lt;b>;Andreas Veit&lt;/b>;、&lt;b>;Felix Yu&lt;/b>;、&lt;b>;Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2212.10791.pdf&quot;>;OpineSum：基于蕴含的抽象意见总结自我训练&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;，&lt;b>;Joshua Maynez&lt;/ b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE：利用检索技术进行摘要评估&lt;/a>; &lt;br />; &lt; em>;&lt;b>;David Uthus&lt;/b>;、&lt;b>;倪建模&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools -public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;充满信心地跟随领导者（董事会）：利用项目和响应方差从单个测试集中估计 p 值&lt;/a>; &lt;br />; &lt;i>; Shira Wein *，克里斯托弗·霍曼，&lt;strong>;洛拉·阿罗约&lt;/strong>;，&lt;strong>;克里斯·韦尔蒂&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/ 2306.02516.pdf&quot;>;SamToNe：改进具有同塔负数的双编码器检索模型的对比损失&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;、&lt;strong>;Gustavo Hernandez Abrego&lt;/strong>; 、&lt;strong>;彼得·多恩巴赫&lt;/strong>;、&lt;strong>;伊梅德·齐图尼&lt;/strong>;、&lt;strong>;恩里克·阿方塞卡&lt;/strong>;、&lt;strong>;董哲&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/ div>; &lt;br />; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;大海捞针：偶然双语在 PaLM 翻译能力中的作用&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou、&lt;strong>;Colin Cherry&lt;/strong>;、&lt;strong>;George Foster&lt;/strong>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;提示PaLM进行翻译：评估策略和性能&lt;/a>; &lt;br />; &lt;em >;&lt;b>;大卫·维拉尔&lt;/b>;、&lt;b>;马库斯·弗雷塔格&lt;/b>;、&lt;b>;科林·切里&lt;/b>;、&lt;b>;罗家明&lt;/b>;、&lt;b>;维雷什·拉特纳卡尔&lt;/b>; , &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;闭卷长篇的查询细化提示-表格质量检查&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;、&lt;b>;Kellie Webster&lt;/b>;、&lt;b>;Michael Collins&lt;/b>;、&lt;b>;Dipanjan Das&lt; /b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;改编或注释：开放域问答中域适应的挑战和干预&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*、&lt;strong>;Emma Strubell&lt;/strong>;、Sameer Singh、&lt;strong>;Pat Verga&lt;/strong>;&lt; /em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT：少样本区域感知机器翻译的基准&lt;/a>;（参见&lt; a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;博客文章&lt;/a>;）&lt;br />; &lt;em>;&lt;b>;帕克·莱利、&lt;b>;蒂莫西·多扎特&lt;/b>;、&lt;b>;Jan A. Botha&lt;/b>;、&lt;b>;泽维尔·加西亚&lt;/b>;、&lt;b>;丹·加勒特&lt;/b>;、&lt; b>;杰森·里萨&lt;/b>;，&lt;b>;奥尔罕·菲拉特&lt;/b>;，&lt;b>;诺亚·康斯坦特&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/2207.00397.pdf&quot;>;带有问答蓝图的条件生成&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;、&lt;b>;Joshua Maynez&lt;/b>;、&lt;b >;雷纳尔德·金·安普莱奥&lt;/b>;、&lt;b>;库兹曼·甘切夫&lt;/b>;、&lt;b>;安妮·路易斯&lt;/b>;、&lt;b>;芳汀·胡特&lt;/b>;、&lt;b>;安德斯·桑德霍尔姆&lt;/b>;、&lt; b>;Dipanjan Das&lt;/b>;、&lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;参考文献通过基于 Seq2Seq 转换的系统进行解析&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;、&lt;b>;Chris Alberti&lt;/b>;、&lt;b>;Michael Collins&lt;/b>;&lt;/ em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;使用特定于语言的子网进行低资源依存解析的跨语言传输&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni、&lt;strong>;Dan Garrette&lt;/strong>;、Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot; >;DAMP：用于面向任务的对话的双重对齐多语言解析器&lt;/a>; &lt;br />; &lt;em>;William Held*、&lt;strong>;Christopher Hidey&lt;/strong>;、&lt;strong>;Fei Liu&lt;/strong>;、&lt;strong>; Eric Zhu&lt;/strong>;、&lt;strong>;Rahul Goel&lt;/strong>;、Diyi Yang、&lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2210.08726.pdf&quot;>;RARR：研究和修订语言模型所说的内容，使用语言模型&lt;/a>; &lt;br />;高鲁宇*，&lt;strong>;戴竹云&lt;/strong>;，&lt;strong>;Panupong Pasupat &lt;/strong>;、Anthony Chen*、&lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;、&lt;strong>;范一成&lt;/strong>;、&lt;strong>;赵文山&lt;/strong>;、&lt;strong>;倪老&lt;/strong>; >;、&lt;strong>;李洪来&lt;/strong>;、&lt;strong>;胡安大成&lt;/strong>;、&lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2306.16793.pdf&quot;>;条件生成大型语言模型能力的基准测试&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez、Priyanka Agrawal、&lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;通过多任务微调实现跨语言泛化&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff、Thomas Wang、Lintang Sutawika、&lt;strong>;Adam Roberts&lt;/strong>;、Stella Biderman、Teven Le Scao、M. Saiful Bari、Sheng Shen、郑欣勇、Hailey Schoelkopf、Xiangru Tang、Dragomir Radev、Alham Fikri Aji、Khalid Almubarak、Samuel Albanie、 Zaid Alyafeai、Albert Webson、Edward Raff、Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA：用以下方法解开参数和上下文知识：反事实问答&lt;/a>; &lt;br />; &lt;em>;Ella Neeman、&lt;strong>;Roee Aharoni&lt;/strong>;、Or Honovich、Leshem Choshen、&lt;strong>;Idan Szpektor&lt;/strong>;、Omri Abend&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;解析实体选择的间接引用表达式&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini &lt;/b>;、&lt;b>;菲利普·拉德林斯基&lt;/b>;、&lt;b>;西尔维娅·帕雷蒂&lt;/b>;、&lt;b>;安妮·路易斯&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL：利用生成模型的具有广泛地理文化覆盖范围的刻板印象基准&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*，&lt;strong>;Aida Mostafazadeh Davani &lt;/strong>;、Chandan K Reddy、&lt;strong>;Shachi Dave&lt;/strong>;、&lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;、&lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;尾巴摇狗：社会偏见基准的数据集构建偏差&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam，&lt;strong>;Sunipa Dev&lt;/strong>;、Daniel Khashabi、Tushar Khot、Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;角色感知模型改善视觉文本渲染&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;、&lt;b>;Dan Garrette&lt;/b>;、&lt;b>;Chitwan Saharia&lt;/b>;、&lt;b>;William陈&lt;/b>;、&lt;b>;亚当·罗伯茨&lt;/b>;、&lt;b>;Sharan Narang&lt;/b>;、&lt;b>;伊琳娜·布洛克&lt;/b>;、&lt;b>;RJ Mical&lt;/b>;、&lt;b>;穆罕默德Norouzi&lt;/b>;，&lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;冷启动数据更好的小样本语言模型微调的选择：一种基于提示的不确定性传播方法&lt;/a>; &lt;br />; &lt;em>;Yue Yu，Rongzhi Zhang，Ran Xu，Jieyu Zhu，&lt;strong>;Jiaming Shen&lt;/strong >;，张超&lt;/em>; &lt;/p>; &lt;p>; 涵盖不常见的领域：针对答案评估的差距聚焦问题生成&lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;，&lt;b>;Alexandre Djerbetian&lt; /b>;、&lt;b>;罗伊·英格伯格&lt;/b>;、&lt;b>;利丹·哈克蒙&lt;/b>;、&lt;b>;加尔·埃利丹&lt;/b>;、&lt;b>;罗伊特·沙法蒂&lt;/b>;、&lt;b>;阿米尔·格洛伯森&lt; /b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2：表单文档信息提取的多模态图对比学习&lt;/a>; &lt; br />; &lt;em>;&lt;b>;李震宇&lt;/b>;、&lt;b>;李春亮&lt;/b>;、&lt;b>;张浩&lt;/b>;、&lt;b>;Timothy Dozat&lt;/b>;、 &lt;b>;文森特·佩罗&lt;/b>;、&lt;b>;苏国龙&lt;/b>;、&lt;b>;张翔&lt;/b>;、&lt;b>;基赫克·索恩&lt;/b>;、&lt;b>;尼古拉·格卢希涅夫&lt;/b>;、 &lt;b>;王仁深&lt;/b>;、&lt;b>;Joshua Ainslie&lt;/b>;、&lt;b>;龙尚邦&lt;/b>;、&lt;b>;秦思阳&lt;/b>;、&lt;b>;藤井泰久&lt;/b>;、 &lt;b>;南华&lt;/b>;，&lt;b>;托马斯·普菲斯特&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;生成文本的方言鲁棒评估&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy多扎特&lt;/strong>;、&lt;strong>;丹·加勒特&lt;/strong>;、&lt;strong>;阿迪亚·西丹特&lt;/strong>;、&lt;strong>;雅各布·爱森斯坦&lt;/strong>;、&lt;strong>;塞巴斯蒂安·格尔曼&lt;/strong>;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;性别错误：大型语言模型在理解代词方面的局限性&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain， &lt;strong>;Sunipa Dev&lt;/strong>;，Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA：用于自动推理的反向链接自然语言&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;、&lt;b>;Najoung Kim&lt;/b>;、&lt;b>;Deepti Bhatia&lt;/b>;、&lt;b>;Xin Xu&lt; /b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT：高效的多段通过层可调交互在变形金刚中进行编码&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*、&lt;strong>;Annie Louis&lt;/strong>;、&lt;strong>;Mohammad Javad Hosseini&lt;/strong>;、&lt;strong>;Alex Fabrikant&lt; /strong>;、&lt;strong>;唐纳德·梅茨勒&lt;/strong>;、&lt;strong>;塔尔·舒斯特&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392 .pdf&quot;>;通过代码生成进行模块化视觉问答&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;博客帖子&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian、Medhini Narasimhan、Kushal Khangaonkar、Kevin Yang、&lt;strong>;Arsha Nagrani&lt;/strong>;、&lt;strong>;Cordelia Schmid&lt;/strong>;、&lt;strong>;Andy Zeng &lt;/strong>;，Trevor Darrell，Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;理解思想链提示：对重要因素的实证研究&lt;/a>; &lt;br />; &lt;em>;Boshi Wang、Sewon Min、Xiang Deng、&lt;strong>;Jiaming Shen&lt;/strong>;、&lt;strong>;You Wu&lt;/strong>;、Luke Zettlemoyer 和 Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;通过自适应提示实现更好的零样本推理&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*、&lt;strong>;孙若曦&lt;/strong>;、Hanjun Dai、&lt;strong>;Sercan Ö。 Arik&lt;/strong>;、&lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;事实上一致的总结带有文本蕴涵反馈的强化学习&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;、&lt;b>;Johan Ferret&lt;/b>;、&lt;b>;Lior Shani&lt;/b>;、&lt;b>;罗伊·阿哈罗尼&lt;/b>;、&lt;b>;杰弗里·西德龙&lt;/b>;、&lt;b>;罗伯特·达达什&lt;/b>;、&lt;b>;马蒂厄·盖斯特&lt;/b>;、&lt;b>;塞尔坦·吉尔金&lt;/b>;、&lt;b>;莱昂纳德·胡塞诺&lt;/b>;、&lt;b>;奥加德·凯勒&lt;/b>;、&lt;b>;尼古拉·莫切夫&lt;/b>;、&lt;b>;萨贝拉·拉莫斯&lt;/b>;、&lt;b>;皮奥特·斯坦奇克&lt;/b>;、&lt;b>;尼诺·维埃拉德&lt;/b>;、&lt;b>;奥利维尔·巴赫姆&lt;/b>;、&lt;b>;加尔·埃利丹&lt;/b>;、&lt;b>;阿维纳坦·哈西丁&lt;/b>;、&lt;b>;奥利维尔·皮特昆&lt;/b>;、&lt;b>; Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;交互式数据科学笔记本中的自然语言到代码生成&lt;/a >; &lt;br />; &lt;em>;&lt;b>;殷鹏程&lt;/b>;、&lt;b>;李文鼎&lt;/b>;、&lt;b>;肖克凡&lt;/b>;、&lt;b>;Abhishek Rao&lt;/b>;、 &lt;b>;温业明&lt;/b>;、&lt;b>;石肯森&lt;/b>;、&lt;b>;约书亚·霍兰&lt;/b>;、&lt;b>;佩奇·贝利&lt;/b>;、&lt;b>;米歇尔·卡塔斯塔&lt;/b>;、 &lt;b>;亨利克·米哈勒夫斯基&lt;/b>;、&lt;b>;亚历山大·波洛佐夫&lt;/b>;、&lt;b>;查尔斯·萨顿&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2212.08410.pdf&quot;>;教授小语言模型进行推理&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*、&lt;strong>;Jonathan Mallinson&lt;/strong>;、&lt;strong>;Jakub Adamek&lt;/strong >;、&lt;strong>;埃里克·马尔米&lt;/strong>;、&lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf /papers/NeuPSL.pdf&quot;>;利用领域知识通过神经概率软逻辑引导对话结构归纳&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*、&lt;strong>;Quan Yuan&lt;/strong>;、&lt;strong>;Jeremiah刘&lt;/strong>;、&lt;strong>;Mehran Kazemi&lt;/strong>;、&lt;strong>;Deepak Ramachandran&lt;/strong>;、&lt;strong>;Tania Bedrax-Weiss&lt;/strong>;、Lise Getoor&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;大海捞针：MTurk 上高共识工作者分析总结&lt;/a>; &lt;br />; &lt;em>;Lining张，Simon Mille，侯玉芳，&lt;strong>;Daniel Deutsch&lt;/strong>;，&lt;strong>;Elizabeth Clark&lt;/strong>;，刘一鑫，Saad Mahamood，&lt;strong>;Sebastian Gehrmann&lt;/strong>;，Miruna Clinciu，Khyathi Raghavi Chandu和 João Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;行业跟踪论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2305.18465.pdf&quot;>;具有差异隐私的Gboard语言模型联邦学习&lt;/a>; &lt;br />; &lt;em>;&lt;b>;徐峥&lt;/b>;，&lt;b>;张彦翔&lt;/b>;、&lt;b>;盖伦·安德鲁&lt;/b>;、&lt;b>;克里斯托弗·乔奎特&lt;/b>;、&lt;b>;彼得·凯鲁兹&lt;/b>;、&lt;b>;布兰登·麦克马汉&lt;/b>;、&lt;b>;杰西·罗森斯托克&lt;/b>;、&lt;b>;张远波&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA：重新思考图片广告通过视觉语言模型的知识增强特征适应进行理解&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>; Garima Pruthi&lt;/strong>;、Hao Su、&lt;strong>;Sugato Basu&lt;/strong>;、&lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL 研究结果论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;具有事实一致性评估的多语言摘要&lt;/a >; &lt;br />; &lt;em>;&lt;b>;罗伊·阿哈罗尼&lt;/b>;、&lt;b>;沙什·纳拉扬&lt;/b>;、&lt;b>;约书亚·梅内斯&lt;/b>;、&lt;b>;乔纳森·赫齐格&lt;/b>;、&lt;b >;伊丽莎白·克拉克&lt;/b>;，&lt;b>;米雷拉·拉帕塔&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;参数-高效微调实现稳健的持续多语言学习&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;、&lt;b>;Shachi Dave&lt;/b>;、&lt;b>;Partha Talukdar&lt;/b>;&lt; /em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO：融合解码器经过优化，可实现更强的性能和更快的推理&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*、&lt;strong>;Yury Zemlyanskiy&lt;/strong>;、&lt;strong>;Joshua Ainslie&lt;/strong>;、&lt;strong>;Nicholas FitzGerald&lt;/strong>;、&lt;strong>;Sumit Sanghai&lt;/strong>;、 &lt;strong>;飞沙&lt;/strong>;、&lt;strong>;威廉·科恩&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;一种简单而有效的方法来查找代码生成中的偏差&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos、Mateusz Malinowski、&lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;具有挑战性的大基准任务以及思想链是否可以解决它们&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun &lt;/b>;、&lt;b>;内森·斯凯尔斯&lt;/b>;、&lt;b>;纳撒内尔·沙利&lt;/b>;、&lt;b>;塞巴斯蒂安·格尔曼&lt;/b>;、&lt;b>;Yi Tay&lt;/b>;、&lt;b>;Hyung Won钟&lt;/b>;、&lt;b>;Aakanksha Chowdhery&lt;/b>;、&lt;b>;Quoc Le&lt;/b>;、&lt;b>;Ed Chi&lt;/b>;、&lt;b>;Denny Zhou&lt;/b>;、&lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm：一个简单的零样本实体查询框架&lt;/a >; &lt;br />; &lt;em>;王子峰*、&lt;strong>;张子钊&lt;/strong>;、&lt;strong>;Jacob Devlin&lt;/strong>;、&lt;strong>;李振宇&lt;/strong>;、&lt;strong>;苏国龙&lt; /strong>;、&lt;strong>;张浩&lt;/strong>;、Jennifer Dy、&lt;strong>;文森特·佩罗&lt;/strong>;、&lt;strong>;托马斯·菲斯特&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen：通过训练数据生成和渐进密集检索进行零样本文本分类&lt;/a>; &lt;br />; &lt;em>;Yue Yu，Yuchen Zhuang，Rongzhi张雨萌，&lt;strong>;沉家明&lt;/strong>;，张超&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;多语言序列希伯来语 NLP 的序列模型&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;、&lt;b>;Hila Noga&lt;/b>;、&lt;b>;Roee Aharoni&lt;/b>;、&lt; b>;Idan Szpektor&lt;/b>;、&lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;触发使用软提示和随机游走的语言模型中问答的多跳推理&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*、&lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;、Siamak Shakeri&lt;/em>; &lt; /p>; &lt;/div>; &lt;br />; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/ program/tutorials/&quot;>;自然语言中的复杂推理&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao、Mor Geva、Bill Yuchen Lin、Michihiro Yasunaga、&lt;strong>;Aman Madaan&lt;/strong>;、Tao Yu&lt;/em >; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;从语言模型生成文本&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini &lt;/b>;、&lt;b>;瑞安·科特雷尔&lt;/b>;、&lt;b>;约翰·休伊特&lt;/b>;、&lt;b>;克拉拉·梅斯特&lt;/b>;、&lt;b>;蒂亚戈·皮门特尔&lt;/b>;&lt;/em>; &lt;/ p>; &lt;/div>; &lt;br />; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp /view/sustainlp2023&quot;>;简单高效的自然语言处理（SustaiNLP）&lt;/a>; &lt;br />;组织者包括：&lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://www.workshopononlineabuse.com/&quot;>;网络虐待和伤害研讨会 (WOAH)&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;基于文档的对话和对话式问答 (DialDoc)&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4concai/home?authuser=0 &quot;>;对话式人工智能 NLP&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl .wellformedness.com/&quot;>;计算和书面语言 (CAWL)&lt;/a>; &lt;br />; 组织者包括：&lt;em>;&lt;b>;Kyle Gorman&lt;/b>;、&lt;b>;Brian Roark&lt;/b>;、&lt;b >;理查德·斯普罗特&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;计算形态学和音系学 (SIGMORPHON)&lt;/a>; &lt;br />; 演讲者包括：&lt;strong>;&lt;em>;凯尔·戈尔曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023 &quot;>;叙事理解研讨会（WNU）&lt;/a>; &lt;br />; 组织者包括：&lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--脚注- ->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>; 完成的工作在 Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type= &quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://ai.googleblog.com/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel= “http://schemas.google.com/g/2005#thumbnail” src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>; &lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYN zpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023% 20多伦多%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;已发布>;2023-07-07T11:01:00.001-07:00&lt;/已发布>; &lt;更新>;2023-07-07T11:09:20.724-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns #&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;通过代码生成进行模块化视觉问答&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：加州大学伯克利分校博士生 Sanjay Subramanian 和 Google 感知团队研究科学家 Arsha Nagrani&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7 HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382 -4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;视觉问答&lt;/a>; (VQA) 是一项机器学习任务，需要模型回答有关图像或&lt;a href =&quot;https://covr-dataset.github.io/&quot;>;一组图像&lt;/a>;。传统的 VQA 方法需要大量带标签的训练数据，其中包含数千个与图像相关的人工注释问答对。近年来，大规模预训练的进步促进了 VQA 方法的发展，这些方法在 &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a -single-visual-language-model&quot;>;少于 50 个训练示例&lt;/a>;（少量）和&lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions- for-visual.html&quot;>;没有任何人工注释的 VQA 训练数据&lt;/a>;（零样本）。然而，这些方法与最先进的完全监督 VQA 方法之间仍然存在显着的性能差距，例如 &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple- Vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;。特别是，少样本方法在空间推理、计数和多跳推理方面遇到了困难。此外，少样本方法通常仅限于回答有关单个图像的问题。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了提高涉及复杂推理的 VQA 示例的准确性，请参见“&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;通过代码生成实现模块化视觉问答&lt;/a>;”，出现在 &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>; 上，我们推出了 CodeVQA，一个回答视觉问题的框架使用程序综合。具体来说，当给出有关图像或图像集的问题时，CodeVQA 会生成一个具有简单视觉功能的 Python 程序（代码），使其能够处理图像，并执行该程序以确定答案。我们证明，在少数样本设置中，CodeVQA 在 &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; 数据集上的性能比之前的工作高约 3%，在&lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; 数据集。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; CodeVQA 方法使用代码编写大型语言模型（LLM ），例如 &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;，以生成 &lt;a href= “https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; 程序（代码）。我们通过制作由这些函数的描述和少于 15 个视觉问题的“上下文”示例以及相关 Python 代码组成的提示来指导法学硕士正确使用视觉函数。为了选择这些示例，我们计算输入问题以及我们已注释程序的所有问题（随机选择的 50 个）的嵌入。然后，我们选择与输入最相似的问题并将其用作上下文示例。给定我们想要回答的提示和问题，法学硕士会生成一个代表该问题的 Python 程序。 &lt;/p>; &lt;p>; 我们使用三个可视化函数实例化 CodeVQA 框架：(1) &lt;code>;query&lt;/code>;、(2) &lt;code>;get_pos&lt;/code>; 和 (3) &lt;code>;find_matching_image&lt; /代码>;。 &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;，它回答有关单个图像的问题，是使用少镜头&lt;a href=&quot;https://arxiv.org/abs/ 实现的2210.08773&quot;>;即插即用 VQA&lt;/a>; (PnP-VQA) 方法。 PnP-VQA 使用 &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; 生成字幕 - 图像字幕 &lt;a href=&quot;https://ai.googleblog.com/2017 /08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; 在数百万个图像标题对上进行了预训练，并将它们输入到 LLM 中，输出问题的答案。 &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>; 是一个对象定位器，它将对象的描述作为输入并返回其在图像中的位置，是使用 &lt;a href=&quot;https:// 实现的arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;。具体来说，描述和图像通过 BLIP 联合文本图像编码器，该编码器预测图像文本匹配分数。 GradCAM 采用该分数相对于图像特征的梯度来查找与文本最相关的区域。 &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;，用于多图像问题中查找与给定输入短语最匹配的图像，是通过使用 BLIP 文本和图像编码器计算文本嵌入来实现的对于短语和每个图像的图像嵌入。然后文本嵌入与每个图像嵌入的点积表示每个图像与短语的相关性，我们选择最大化这种相关性的图像。 &lt;/li>; &lt;/ul>; &lt;p>; 这三个功能可以使用需要很少注释的模型来实现（例如，从网络收集的文本和图像文本对以及少量的 VQA 示例）。此外，除了这些功能之外，CodeVQA 框架还可以轻松推广到用户可能实现的其他功能（例如，对象检测、图像分割或知识库检索）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_Gs RRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW 6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;CodeVQA 方法的图示。首先，大型语言模型生成Python程序（代码），它调用代表问题的视觉函数。在此示例中，使用简单的 VQA 方法 (&lt;code>;query&lt;/code>;) 来回答问题的一部分，并使用对象定位器 (&lt;code>;get_pos&lt;/code>;) 来查找问题的位置提到的对象。然后程序通过组合这些函数的输出来生成原始问题的答案。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%; &quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; CodeVQA 框架不仅可以正确生成和执行单图像问题的 Python 程序，还可以正确生成和执行多图像问题的 Python 程序。例如，如果给定两张图像，每张图像显示两只熊猫，人们可能会问的一个问题是：“真的有四只熊猫吗？”在这种情况下，LLM 将有关图像对的计数问题转换为一个程序，其中为每个图像获取对象计数（使用 &lt;em>;query&lt;/em>; 函数）。然后将两个图像的计数相加来计算总计数，然后将其与原始问题中的数字进行比较以得出是或否的答案。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4F rp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UPuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGR4V9K4QXfHSm4w LFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在三个视觉推理数据集上评估 CodeVQA：&lt;a href=&quot;https://cs.stanford.edu/people/ dorarad/gqa/about.html&quot;>;GQA&lt;/a>;（单图像）、&lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>;（多图像）、和 &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>;（多图像）。对于 GQA，我们为每种方法提供 12 个上下文示例，对于 COVR 和 NLVR2，我们为每种方法提供 6 个上下文示例。下表显示，CodeVQA 在所有三个数据集上均比基线少样本 VQA 方法有了持续改进。 &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text -align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;方法&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;少镜头 PnP-VQA &lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td >;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/ td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt; /td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GQA、COVR 和 NLVR2 数据集上的结果表明 CodeVQA 持续改进在少样本 PnP-VQA 上。该指标是精确匹配精度，即预测答案与真实答案完全匹配的示例的百分比。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;p>; 我们发现，在 GQA 中，CodeVQA 在空间推理问题上的准确率比基线高出大约 30%，在“与”问题上高出 4%，在“或”问题上高出 3%。类别包括多跳问题，例如“图片中是否有盐瓶或滑板？”，生成的程序如下所示。&lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-左：40 像素；右边距：40px； white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;有盐瓶吗？&quot;)skateboards_exist = query(img, &quot;有滑板吗？&quot;) if salt_shakers_exist == &quot;yes&quot; 或skateboards_exist == &quot;yes&quot;:answer = &quot;yes&quot; else:answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>;在COVR中，我们发现当输入图像数量较多时，CodeVQA相对于基线的增益较高，如下表所示。这种趋势表明将问题分解为单图像问题是有益的。&lt;/p>; &lt; br />; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”style=“margin-left：auto;右边距：自动；文本对齐：居中;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;图片数量&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;方法&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td >; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br / >; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;少量 PnP-VQA&lt;/em>;&amp;nbsp ; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt; /tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出CodeVQA，一个用于少量视觉问答的框架，依赖于代码生成来执行多步骤视觉推理。未来工作的令人兴奋的方向包括扩展所使用的模块集以及为 VQA 之外的视觉任务创建类似的框架。我们注意到，在考虑是否部署 CodeVQA 等系统时应谨慎，因为像我们的视觉函数中使用的视觉语言模型&lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;具有已被证明表现出社会偏见&lt;/a>;。同时，与整体模型相比，CodeVQA 提供了额外的可解释性（通过 Python 程序）和可控性（通过修改提示或视觉功能），这在生产系统中非常有用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究是 &lt;a href= &quot;https://bair.berkeley.edu/baircommons.html&quot;>;加州大学伯克利分校人工智能研究实验室&lt;/a>; (BAIR) 和 Google Research，由 Sanjay Subramanian、Medin Narasimhan、Kushal Khangaonkar、Kevin Yang、Arsha 主持Nagrani、Cordelia Schmid、Andy Zeng、Trevor Darrell 和 Dan Klein。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type= &quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;模块化视觉通过代码生成回答问题&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNRABjVYgqA9xmCanWTTyYw 4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-561916049750681872&lt;/id>;&lt;发布>;2023-07-06T12：50：00.000- 07:00&lt;/发布>;&lt;更新>;2023-07-06T12:50:04.339-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;多模态学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;Pic2Word：将图片映射到文字以进行零样本合成图像检索&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Kuniaki Saito，学生Google 研究院云 AI 团队研究员和 Google 研究院研究科学家 Kihyuk Sohn&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIGz -JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4BOpEtfW -hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s1100/Pic2Word.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 图像检索在搜索引擎中起着至关重要的作用。通常，他们的用户依靠图像或文本作为查询来检索所需的目标图像。然而，基于文本的检索有其局限性，因为使用文字准确描述目标图像可能具有挑战性。例如，当搜索时尚商品时，用户可能想要其特定属性（例如徽标的颜色或徽标本身）与他们在网站中找到的不同的商品。然而，在现有搜索引擎中搜索该商品并非易事，因为通过文本精确描述时尚商品可能具有挑战性。为了解决这个问题，&lt;a href=&quot;https://arxiv.org/pdf/1812.07119.pdf&quot;>;组合图像检索&lt;/a>; (CIR) 基于组合图像和文本样本的查询来检索图像它提供了有关如何修改图像以适合预期检索目标的说明。因此，CIR 可以通过图像和文本的结合来精确检索目标图像。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 然而，CIR 方法需要大量标记数据，即 1) 查询图像、2) 描述和 3) 目标图像的三元组。收集此类标记数据的成本很高，并且根据这些数据训练的模型通常是针对特定用例量身定制的，这限制了它们泛化到不同数据集的能力。 &lt;/p>; &lt;p>; 为了解决这些挑战，请参阅“&lt;a href=&quot;https://arxiv.org/abs/2302.03084&quot;>;Pic2Word：将图片映射到单词以进行零样本合成图像检索&lt;/a>;” ，我们提出了一个称为零样本 CIR (ZS-CIR) 的任务。在ZS-CIR中，我们的目标是构建一个执行各种CIR任务的单一CIR模型，例如&lt;a href=&quot;https://www.zheyuanliu.me/CIRR/&quot;>;对象组合，&lt;/a>; &lt; a href=&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;属性编辑&lt;/a>;，或域转换，无需标记的三元组数据。相反，我们建议使用大规模图像标题对和未标记图像来训练检索模型，这些图像比大规模监督 CIR 数据集更容易收集。为了鼓励可重复性并进一步推进这一领域的发展，我们还&lt;a href=&quot;https://github.com/google-research/composed_image_retrieval&quot;>;发布代码&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyuc IiQWqa-0FgWYq2BBUGvD79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s1062/image9.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;342&quot; data-original-width=&quot;1062&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgstvc3MJRt1_D572XfScwQFn9a6_U0yAmov2AClUkUrW9LyjVN-us_vqvSdhiuhC4PC3zaPSpWnd86T4tM6fyucIiQWqa-0FgWYq2BBUGvD 79eW44s5rdYsF7U_HGapSax0WtlPr-ddbAZfh8fxluWTrDCbWhMfYWWv_RSCZXBTnjvkfp61Voc82M_IGN2/s16000/image9.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;现有组合图像检索模型的描述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyE p3G9- DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9ALCHCW​​i9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s949/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;476&quot; data-original-width=&quot;949&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEitOBOJ1qoJGbJaQpzdvHv4yLV047FB4aX1Ns9XTyMlCQ70sGOGBSKle5qZyWI29He9DGvbO60eyEp3G9-DfCO9Fgs7PNNcJzug2f8CYIJUzMxvqqDQmTjPfp6GO1yv1rc9 ALCHCW​​i9YbVFJSUP8U_zrsANSssKC2BOXj76M_oWwuiIs3JDDYskabB6LDTi/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们仅使用图像标题数据训练组合图像检索模型。我们的模型检索与查询图像和文本的组合对齐的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;方法概述&lt;/h2>; &lt;p>; 我们建议利用 &lt;a href=&quot;https://arxiv 中语言编码器的语言功能.org/abs/2103.00020&quot;>;对比语言图像预训练模型&lt;/a>;（CLIP），它擅长为各种文本概念和属性生成语义上有意义的语言嵌入。为此，我们使用了一个轻量级的模型CLIP中的映射子模块，被设计为将输入图片（例如，猫的照片）从图像嵌入空间映射到文本输入空间中的单词标记（例如，“猫”）。整个网络通过视觉语言对比损失进行优化，以再次确保视觉和文本嵌入空间在给定一对图像及其文本描述的情况下尽可能接近。然后，可以将查询图像视为一个单词。这使得语言编码器能够灵活、无缝地组合查询图像特征和文本描述。我们将我们的方法称为 Pic2Word 并在下图中概述了其训练过程。我们希望映射的标记&lt;em>;s&lt;/em>;以单词标记的形式表示输入图像。然后，我们训练映射网络来重建语言嵌入中的图像嵌入，&lt;em>;p&lt;/em>;。具体来说，我们优化了 CLIP 中提出的在视觉嵌入 &lt;em>;v&lt;/em>; 和文本嵌入 &lt;em>;p&lt;/em>; 之间计算的对比损失。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiM6UvNkDUIIICX-SUaDVgAXYBtRvbgmd8cXv29no7UCc7_3Wkq7Hb-L72qLXiJLwfHfRGyqjUIU5p8-gMlMLYH XfOdOCnkADSiEP0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDscoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s611 /image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;第611章0dl3t1s8nvbQwEerDkIg20eVJLT2NXjDScoLU3mjVk0mOzyNhb-bZobIZa9VX5S6CjBrZlmFNh7-m6NdyIJT7Xq3k1/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用以下方法训练映射网络 (&lt;em>;f&lt;sub>;M&lt;/sub>;&lt;/em>;)仅限未标记的图像。我们仅使用冻结的视觉和文本编码器优化映射网络。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;给定经过训练的映射网络，我们可以将图像视为单词标记和对与文字描述灵活组合图文联合查询，如下图所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_i E9Ove-RupgS0e50E5StD1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s827/image3 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;827&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkH5SDYcnCCNSbv4tyJ7lEaZp4W0SsMVP2rBTx8-AnXGM2eYaY04UX9sczYL07-z9TPvcbKP5wF6huVyWe6SOQqqz_iE9Ove-RupgS0e50E5St D1A_yKF2KQtrVgy01J6WaLUZ4rYatFQqgBEnoltPBRXAqTgcGmuD8hVJ3BBkEi55ASVhMy35-_j1yCjs/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;通过训练好的映射网络，我们将图像视为单词标记，并将其与文本描述配对以灵活地组成联合图像-文本查询。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;评估&lt;/h2>; &lt;p >; 我们进行了各种实验来评估 Pic2Word 在各种 CIR 任务上的性能。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;域转换&lt;/h3>; &lt;p>; 我们首先评估所提出的方法在域上的组合能力转换——给定图像和所需的新图像域（例如，雕塑、折纸、卡通、玩具），系统的输出应该是具有相同内容但具有新的所需图像域或风格的图像。如下图所示，我们评估分别以图像和文本形式给出的类别信息和领域描述的能力。我们使用 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; 和 &lt;a href=&quot;https://github.com/hendrycks 评估从真实图像到四个域的转换/imagenet-r&quot;>;ImageNet-R&lt;/a>;。 &lt;/p>; &lt;p>; 为了与不需要监督训练数据的方法进行比较，我们选择了三种方法：(i) &lt;em>;仅图像&lt;/em>;仅通过视觉嵌入执行检索，(ii) &lt;em>;文本only &lt;/em>;仅采用文本嵌入，并且 (iii)&lt;em>;图像 + 文本&lt;/em>;对视觉和文本嵌入进行平均以构成查询。与（iii）的比较显示了使用语言编码器组合图像和文本的重要性。我们还与 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;Combiner&lt;/a>; 进行了比较，后者在 &lt;a href=&quot;https 上训练 CIR 模型://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRR&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdF FCCacbAu2mt6CD1vgea5oS_eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s965/image6.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;513&quot; data-original-width=&quot;965&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6DGgCKaYmU6tCML-WOn9C0uXwfRgRck3-w2R5SkE4hs_WCqLdwDaWS-ccoCc7mjK8nXJsgvrc0gN4m_IdFFCCacbAu2mt6CD1vgea5oS_ eOZQVyCB6fJ6ldH5BON_ZX2nIUbbc2OKFScHnz4jCOXo0wF8jZSruw_0cHfWz68GL041zjctO61AwKWgkhyj/s16000/image6.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的目标是将输入查询图像的域转换为用文本描述的域，例如折纸。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; 如下图所示，我们提出的方法大大优于基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2 I6A0UJMq0VMcir0oARMQpxxYvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s754/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;754&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJ-Y_U7Tg25uC3fam-2yUk_tklOrx6DLARCA7TaHdlv8C5ZRo2kVjFnlzF-d0bhBqTmFEr5VwtqH5rYR3wH2I6A0UJMq0VMcir0oARMQpxx YvKZhLHZPQwczz4d338MNxPQEWE3kWyCtRU0QfSIsnDHM_YszosF5wsUTGqFCIBOUO9jacXWELBy4sBiYy6/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;结果（&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;召回&lt;/a>;@10，即，检索到的前 10 个图像中相关实例的百分比。）用于域转换的合成图像检索。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h3>;时尚属性构成&lt;/h3>; &lt;p>; 接下来，我们使用&lt;a href =&quot;https://arxiv.org/pdf/1905.12794.pdf&quot;>;Fashion-IQ&lt;/a>; 数据集。下图说明了给定查询的所需输出。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1R yZbrzZzyKvwqCaeXSrL4B_JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s818/image8.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;309&quot; data-original-width=&quot;818&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggJ9K9W1jC_Y0Y5y8n-xGVuGOafb59VwHL0ShpgCF-_ny1oMNQn6X3Ji57AltOUN_CfduBl5ektSt6Htp0iYg_1RyZbrzZzyKvwqCaeXSrL4B _JazXPNZvvXdszAHHzI1waa-xKrt3UxT2BOFVmYrw_KNgQdEVvGFnXAfTc_SkmVyy4edYmoZ2IkjLp8nY/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;时尚属性的 CIR 概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在下图中，我们与基线进行比较，包括利用三元组训练 CIR 模型的监督基线：(i) &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Baldrati_Effective_Conditioned_and_Composed_Image_Retrieval_Combining_CLIP-Based_Features_CVPR_2022_paper.pdf&quot;>;CB &lt;/a>; 使用与我们的方法相同的架构，(ii) &lt;a href=&quot;https://arxiv.org/pdf/2108.04024.pdf&quot;>;CIRPLANT&lt;/a>;，&lt;a href=&quot;https:// arxiv.org/pdf/2203.08101.pdf&quot;>;ALTEMIS&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2007.00145.pdf&quot;>;MAAF&lt;/a>; 使用较小的主干网，例如 ResNet50 。与这些方法的比较将使我们了解零样本方法在此任务上的表现如何。 &lt;/p>; &lt;p>; 虽然 CB 优于我们的方法，但我们的方法比具有较小主干网的监督基线表现更好。这一结果表明，通过利用强大的 CLIP 模型，我们可以训练高效的 CIR 模型，而无需带注释的三元组。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXb FK41NohRIZu9vzj5_tvUOOA8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s807/image4.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;498&quot; data-original-width=&quot;807&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxykcjevVjWwmJLW2lFx3qEmykC6ujUDvSvMwiYBpzeU7Knr6djcCVLKo__cwe6KJTeK2Q6LIYmyb43NcVguXbFK41NohRIZu9vzj5_tvUOOA 8l6jGI8Nt0UEw5RrUig3mfLH2kW4ET6J_ePgMfvt5c2XFsc0Ab84Wucq38jIKWvR1H-ElbPU_4W5bE1z6/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;结果（&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;召回&lt;/a>;@10，即第一个中相关实例的百分比检索了 10 个图像。）关于 Fashion-IQ 数据集的合成图像检索（越高越好）。浅蓝色条使用三元组训练模型。请注意，我们的方法与这些具有浅（较小）主干的监督基线的性能相当。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br >; &lt;/div>; &lt;h3>;定性结果&lt;/h3>; &lt;p>; 我们在下图中展示了几个示例。与不需要监督训练数据（文本+图像特征平均）的基线方法相比，我们的方法在正确检索目标图像方面做得更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90 NvPIO1rVgBTNEHy1eVIOmNdyZzd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s1999/image7.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;1999&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqeltDVuEJfgEanY_84kEbxJUI5vgOd0OIqx2uoNxixTiy0BIxPhHmGHLyiBS2t1ShKDspP6t4vl94_PEEc0NE90NvPIO1rVgBTNEHy1eVIOmNdyZ zd3tynUz6g1stmYw053BMUGnL-m1qjMOYBCo8XjX_JzYJT_4NdUrndgK9It1E6cESKyq44QGDe2o/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;不同查询图像和文本描述的定性结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>; 在本文中，我们介绍了 Pic2Word，一种用于 ZS-CIR 将图片映射到文字的方法。我们建议将图像转换为单词标记，以仅使用图像标题数据集来实现 CIR 模型。通过各种实验，我们验证了训练模型在各种 CIR 任务上的有效性，表明在图像字幕数据集上进行训练可以构建强大的 CIR 模型。未来一个潜在的研究方向是利用标题数据来训练映射网络，尽管我们在目前的工作中仅使用图像数据。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Kuniaki Saito、Kihyuk Sohn 进行，张翔、李春亮、李震宇、Kate Saenko 和 Tomas Pfister。同时感谢 Zizhao 张和 Sergey Ioffe 提供的宝贵反馈。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/561916049750681872/comments/default&quot; rel= &quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for .html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 561916049750681872&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/561916049750681872&quot; rel=&quot;self&quot; type= “application/atom+xml”/>;&lt;link href=&quot;http://ai.googleblog.com/2023/07/pic2word-mapping-pictures-to-words-for.html&quot; rel=&quot;alternate&quot; title=&quot; Pic2Word：将图片映射到文字以进行零样本合成图像检索” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6B2QkPYBxWvwycmetnYj3hn1h6R9lhon2guJb7jNE3lSCmGXWSe-tm_AdTC6pJ-ORJSIsGz-JuRHpFqflOHvk02R_1AVd0s4Z0JvZ4m1SV6OtiPx0b6DF4 BOPEtfW-hkTKt1VhoTPbQKDQCBF3ihZ5rQG9GGe9AQ6xVH8vMYtUYIWBc2hIIvs0mwQh70r/s72-c/Pic2Word.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/ &quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-667019077470952746&lt;/id>;&lt;发布>;2023-06-29T14:08:00.000-07:00&lt;/发布>;&lt;更新>;2023-06-29T14:08:24.386-07:00&lt;/更新>;&lt;category schema=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;差异隐私&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;安全与隐私&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;宣布首届机器遗忘挑战&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Fabian Pedregosa 和 Eleni Triantafillou&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-m NrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s900/Unlearning.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 深度学习最近在一系列应用中取得了巨大进步，包括&lt;a href=&quot;https://imagen.research.google/&quot;>;真实图像生成&lt;/a>;和&lt;a href= “https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;令人印象深刻的检索系统&lt;/a>;到&lt;a href=&quot;https://blog.google/technology /ai/bard-google-ai-search-updates/&quot;>;可以进行类人对话的语言模型&lt;/a>;。虽然这一进展非常令人兴奋，但深度神经网络模型的广泛使用需要谨慎：在 Google AI &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;原则&lt;/a>;的指导下，我们寻求通过了解和减轻潜在风险（例如不公平偏见的传播和放大以及保护用户隐私）来负责任地开发人工智能技术。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 完全消除请求删除的数据的影响具有挑战性，因为除了简单地将其从存储的数据库中删除之外，还需要删除该数据对其他工件（例如训练有素的机器学习模型）的影响。此外，最近的研究 [&lt;a href=&quot;https://arxiv.org/abs/1610.05820&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;2&lt;/ a>;] 表明，在某些情况下，可以使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Adversarial_machine_learning#Model_extraction 高精度地推断示例是否用于训练机器学习模型&quot;>;成员推理攻击&lt;/a>; (MIA)。这可能会引起隐私问题，因为这意味着即使从数据库中删除个人数据，仍然可以推断该个人的数据是否用于训练模型。 &lt;/p>; &lt;p>; 鉴于上述情况，&lt;em>;机器遗忘&lt;/em>;是机器学习的一个新兴子领域，旨在消除训练样本的特定子集（“遗忘集”）的影响模型。此外，理想的去学习算法将消除某些示例的影响，同时保持其他有益的属性，例如训练集其余部分的准确性以及对保留示例的泛化。生成这种未学习模型的一种直接方法是在调整后的训练集上重新训练模型，该训练集排除了遗忘集中的样本。然而，这并不总是一个可行的选择，因为重新训练深度模型的计算成本可能很高。理想的去学习算法会使用已经训练的模型作为起点，并有效地进行调整以消除所请求数据的影响。 &lt;/p>; &lt;p>; 今天，我们很高兴地宣布，我们已经与众多学术和工业研究人员合作组织了&lt;a href=&quot;https://unlearning-challenge.github.io/&quot; >;首届机器遗忘挑战&lt;/a>;。比赛考虑了一个现实场景，在训练后，必须忘记训练图像的某个子集，以保护相关个人的隐私或权利。比赛将在 &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; 上举办，提交的作品将根据遗忘质量和模型实用性自动评分。我们希望这次竞赛将有助于推进机器去学习的最先进水平，并鼓励开发高效、有效和合乎道德的去学习算法。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;机器取消学习应用&lt;/h2>; &lt;p>; 机器取消学习除了保护用户隐私之外还有其他应用。 For instance, one can use unlearning to erase inaccurate or outdated information from trained models (eg, due to errors in labeling or changes in the environment) or remove harmful, manipulated, or outlier data. &lt;/p>; &lt;p>; The field of machine unlearning is related to other areas of machine learning such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1802.07569&quot;>;life-long learning&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&quot;>;fairness&lt;/a>;. Differential privacy aims to guarantee that no particular training example has too large an influence on the trained model; a stronger goal compared to that of unlearning, which only requires erasing the influence of the designated forget set. Life-long learning research aims to design models that can learn continuously while maintaining previously-acquired skills. As work on unlearning progresses, it may also open additional ways to boost fairness in models, by correcting unfair biases or disparate treatment of members belonging to different groups (eg, demographics, age groups, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s720/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRnut8P03hlk5tKJPEEsqUl1DSlqN2ScdJeiaRfC3mWbQ_PBBwf7wBU9xgxuzr1GoqgkB6MwCa6Zrdo6LQxSOIPXIUrl1Yug73k2Q2zFI61VDAi9K21JOPox0Hc1CIh6ShKxW9Tgy45TYV3p3r5IiI7yxzzzOpzvbJ-5o3QVtjZn6vhDZLntnCcUSi1mb_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Anatomy of unlearning.&lt;/b>; An unlearning algorithm takes as input a pre-trained model and one or more samples from the train set to unlearn (the &quot;forget set&quot;). From the model, forget set, and retain set, the unlearning algorithm produces an updated model. An ideal unlearning algorithm produces a model that is indistinguishable from the model trained without the forget set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Challenges of machine unlearning&lt;/h2>; &lt;p>; The problem of unlearning is complex and multifaceted as it involves several conflicting objectives: forgetting the requested data, maintaining the model&#39;s utility (eg, accuracy on retained and held-out data), and efficiency. Because of this, existing unlearning algorithms make different trade-offs. For example, full retraining achieves successful forgetting without damaging model utility, but with poor efficiency, while &lt;a href=&quot;https://arxiv.org/abs/2007.02923&quot;>;adding noise&lt;/a>; to the weights achieves forgetting at the expense of utility. &lt;/p>; &lt;p>; Furthermore, the evaluation of forgetting algorithms in the literature has so far been highly inconsistent. While some &lt;a href=&quot;https://arxiv.org/abs/1911.04933&quot;>;works&lt;/a>; report the classification accuracy on the samples to unlearn, &lt;a href=&quot;https://proceedings.mlr.press/v119/wu20b.html&quot;>;others&lt;/a>; report distance to the fully retrained model, and yet others use the error rate of membership inference attacks as a metric for forgetting quality [&lt;a href=&quot;https://arxiv.org/abs/2302.09880&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.10981&quot;>;5&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.02205&quot;>;6&lt;/a>;]. &lt;/p>; &lt;p>; We believe that the inconsistency of evaluation metrics and the lack of a standardized protocol is a serious impediment to progress in the field — we are unable to make direct comparisons between different unlearning methods in the literature. This leaves us with a myopic view of the relative merits and drawbacks of different approaches, as well as open challenges and opportunities for developing improved algorithms. To address the issue of inconsistent evaluation and to advance the state of the art in the field of machine unlearning, we&#39;ve teamed up with a broad group of academic and industrial researchers to organize the first unlearning challenge. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Announcing the first Machine Unlearning Challenge&lt;/h2>; &lt;p>; We are pleased to announce the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>;, which will be held as part of the &lt;a href=&quot;https://neurips.cc/Conferences/2023/CompetitionTrack&quot;>;NeurIPS 2023 Competition Track.&lt;/a>; The goal of the competition is twofold. First, by unifying and standardizing the evaluation metrics for unlearning, we hope to identify the strengths and weaknesses of different algorithms through apples-to-apples comparisons. Second, by opening this competition to everyone, we hope to foster novel solutions and shed light on open challenges and opportunities. &lt;/p>; &lt;p>; The competition will be hosted on &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; and run between mid-July 2023 and mid-September 2023. As part of the competition, today we&#39;re announcing the availability of the &lt;a href=&quot;https://github.com/unlearning-challenge/starting-kit&quot;>;starting kit&lt;/a>;. This starting kit provides a foundation for participants to build and test their unlearning models on a toy dataset. &lt;/p>; &lt;p>; The competition considers a realistic scenario in which an age predictor has been trained on face images, and, after training, a certain subset of the training images must be forgotten to protect the privacy or rights of the individuals concerned. For this, we will make available as part of the starting kit a dataset of synthetic faces (samples shown below) and we&#39;ll also use several real-face datasets for evaluation of submissions. The participants are asked to submit code that takes as input the trained predictor, the forget and retain sets, and outputs the weights of a predictor that has unlearned the designated forget set. We will evaluate submissions based on both the strength of the forgetting algorithm and model utility. We will also enforce a hard cut-off that rejects unlearning algorithms that run slower than a fraction of the time it takes to retrain. A valuable outcome of this competition will be to characterize the trade-offs of different unlearning algorithms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s2000/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;2000&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijGdpNGKrQ9AskeRnXVSjPcFrjFPWs5TvXIAeD0gkJVL0hizxuJ4LL24rdKuNPUr86ivbaJZ5x-3dHBBQzLTbFYUWQ9p3ER5THVgv6xpOvK45_67ueGCtJsJVHrlkBKSfbz-21PrI2nkNGmoPcOkO_rqjR9W1-eDTxcjM6NNqqJkxMXMpRym_SYt3v6Wwn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Excerpt images from the &lt;a href=&quot;https://github.com/microsoft/FaceSynthetics&quot;>;Face Synthetics&lt;/a>; dataset together with age annotations. The competition considers the scenario in which an age predictor has been trained on face images like the above, and, after training, a certain subset of the training images must be forgotten.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For evaluating forgetting, we will use tools inspired by MIAs, such as &lt;a href=&quot;https://arxiv.org/abs/2112.03570&quot;>;LiRA&lt;/a>;. MIAs were first developed in the privacy and security literature and their goal is to infer which examples were part of the training set. Intuitively, if unlearning is successful, the unlearned model contains no traces of the forgotten examples, causing MIAs to fail: the attacker would be &lt;em>;unable&lt;/em>; to infer that the forget set was, in fact, part of the original training set. In addition, we will also use statistical tests to quantify how different the distribution of unlearned models (produced by a particular submitted unlearning algorithm) is compared to the distribution of models retrained from scratch. For an ideal unlearning algorithm, these two will be indistinguishable. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Machine unlearning is a powerful tool that has the potential to address several open problems in machine learning. As research in this area continues, we hope to see new methods that are more efficient, effective, and responsible. We are thrilled to have the opportunity via this competition to spark interest in this field, and we are looking forward to sharing our insights and findings with the community. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We are writing this blog post on behalf of the organization team of the Unlearning Competition: Eleni Triantafillou*, Fabian Pedregosa* (*equal contribution), Meghdad Kurmanji, Kairan Zhao, Gintare Karolina Dziugaite, Peter Triantafillou, Ioannis Mitliagkas, Vincent Dumoulin, Lisheng Sun Hosoya, Peter Kairouz, Julio CS Jacques Junior, Jun Wan, Sergio Escalera and Isabelle Guyon.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/667019077470952746/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/667019077470952746&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html&quot; rel=&quot;alternate&quot; title=&quot;Announcing the first Machine Unlearning Challenge&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnYgC13GY7TTT4dhlutperGlCU07bWBJJr3yICTpKX4kxu8Beso89UtRLslnKi7XhD3T2kzkjHvKLNfXG_hztQxmbDFLafmLscIDZKGzOqWbOUxcYWjTDYgudshWu7v-mNrbhlegtEj7I-9woJvwgtOSfi01nKsalbOiYZmP1YN2FnQw_dTwobwwQvSrsv/s72-c/Unlearning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9161751123508054082&lt;/id>;&lt;published>;2023-06-29T12:10:00.004-07:00&lt;/published>;&lt;updated>;2023-07-18T14:55:41.583-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device diffusion plugins for conditioned text-to-image generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao and Tingbo Hou, Software Engineers, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s800/MediaPipeDiffusion-hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown great success in text-to-image generation, achieving high image quality, improved inference performance, and expanding our creative inspiration. Nevertheless, it is still challenging to efficiently control the generation, especially with conditions that are difficult to describe with text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we announce &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; diffusion plugins, which enable controllable text-to-image generation to be run on-device. Expanding upon our &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;prior work&lt;/a>; on GPU inference for on-device large generative models, we introduce new low-cost solutions for controllable text-to-image generation that can be plugged into existing diffusion models and their Low-Rank Adaptation (&lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;LoRA&lt;/a>;) variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s1080/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV7MOp8-3-FSJQpmuovwRm6gcc64-i3T4CW370aiey5MWHThNtr_IPedqYh0aJl9rbolgzGdV-eRf2EDlhpWZN759usJt0wbzD5Gvdhx_6yZU5x6TnunYdXBBgzovXaT0oWgWma81L49g9G8nW8sgHD95I-0_J23jkYQ4LBPYEfI2N1d8PIYsWJgFaLpY/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Text-to-image generation with control plugins running on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Background&lt;/h2>; &lt;p>; With diffusion models, image generation is modeled as an iterative denoising process. Starting from a noise image, at each step, the diffusion model gradually denoises the image to reveal an image of the target concept. Research shows that leveraging language understanding via text prompts can greatly improve image generation. For text-to-image generation, the text embedding is connected to the model via cross-attention layers. Yet, some information is difficult to describe by text prompts, eg, the position and pose of an object. To address this problem, researchers add additional models into the diffusion to inject control information from a condition image. &lt;/p>; &lt;p>; Common approaches for controlled text-to-image generation include &lt;a href=&quot;https://arxiv.org/abs/2211.12572&quot;>;Plug-and-Play&lt;/a>;, &lt;a href=&quot;https://github.com/lllyasviel/ControlNet&quot;>;ControlNet&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.08453&quot;>;T2I Adapter&lt;/a>;. Plug-and-Play applies a widely used denoising diffusion implicit model (&lt;a href=&quot;https://arxiv.org/abs/2010.02502&quot;>;DDIM&lt;/a>;) inversion approach that reverses the generation process starting from an input image to derive an initial noise input, and then employs a copy of the diffusion model (860M parameters for Stable Diffusion 1.5) to encode the condition from an input image. Plug-and-Play extracts spatial features with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>; from the copied diffusion, and injects them into the text-to-image diffusion. ControlNet creates a trainable copy of the encoder of a diffusion model, which connects via a convolution layer with zero-initialized parameters to encode conditioning information that is conveyed to the decoder layers. However, as a result, the size is large, half that of the diffusion model (430M parameters for Stable Diffusion 1.5). T2I Adapter is a smaller network (77M parameters) and achieves similar effects in controllable generation. T2I Adapter only takes the condition image as input, and its output is shared across all diffusion iterations. Yet, the adapter model is not designed for portable devices. &lt;/p>; &lt;br />; &lt;h2>;The MediaPipe diffusion plugins&lt;/h2>; &lt;p>; To make conditioned generation efficient, customizable, and scalable, we design the MediaPipe diffusion plugin as a separate network that is: &lt;/p>; &lt;ul>; &lt;li>;&lt;i>;Plugable&lt;/i>;: It can be easily connected to a pre-trained base model. &lt;/li>;&lt;li>;&lt;i>;Trained from scratch&lt;/i>;: It does not use pre-trained weights from the base model. &lt;/li>;&lt;li>;&lt;i>;Portable&lt;/i>;: It runs outside the base model on mobile devices, with negligible cost compared to the base model inference. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Parameter Size&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Plugable&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;From Scratch&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Portable&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Plug-and-Play &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;860M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;430M* &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;T2I Adapter &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;77M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;❌ &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6M &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;✔️ &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Plug-and-Play, ControlNet, T2I Adapter, and the MediaPipe diffusion plugin.&lt;br />;* The number varies depending on the particulars of the diffusion model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The MediaPipe diffusion plugin is a portable on-device model for text-to-image generation. It extracts multiscale features from a conditioning image, which are added to the encoder of a diffusion model at corresponding levels. When connecting to a text-to-image diffusion model, the plugin model can provide an extra conditioning signal to the image generation. We design the plugin network to be a lightweight model with only 6M parameters. It uses depth-wise convolutions and inverted bottlenecks from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetv2&lt;/a>; for fast inference on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s1724/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;628&quot; data-original-width=&quot;1724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjunHsjDk5nhV0Ihky8QlPL-GWyr-vx6M-DYs36lK63AxxT_QZZrt4HNBmirqc_hY29OqzRxvMMVERk-kPpsFzfkpgTLExoq1TtliirmvH_lGp1dOYcKXIzgezB4Vgjj7XVVWHagFeZ3GBPNVrhZJuVl2z-p8prz5ex4jGQYqxOKwzbkoOk0lxTYmxZLg4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the MediaPipe diffusion model plugin. The plugin is a separate network, whose output can be plugged into a pre-trained text-to-image generation model. Features extracted by the plugin are applied to the associated downsampling layer of the diffusion model (blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Unlike ControlNet, we inject the same control features in all diffusion iterations. That is, we only run the plugin once for one image generation, which saves computation. We illustrate some intermediate results of a diffusion process below. The control is effective at every diffusion step and enables controlled generation even at early steps. More iterations improve the alignment of the image with the text prompt and generate more detail. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s1280/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9yb97pLIbtrew_dyuoCFH8n_JMJVMiTr8Fxr65sanG7jV8J-L8Ciy_YUOXvsGZbD_9YhEtUN9DGe0_Z-djE2Z-irvqGqshbuK-E2wCUKORJigLemEjJ9WZ4fPbvaUnIBXIlQ0pYRPRxtVeZy25E9JoRst92Fo0FDkkaMmRhoMBEYv1WjQQO7X7y7U2M4/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the generation process using the MediaPipe diffusion plugin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Examples&lt;/h2>; &lt;p>; In this work, we developed plugins for a diffusion-based text-to-image generation model with MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmark&lt;/a>;, MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmark&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_map&quot;>;depth maps&lt;/a>;, and &lt;a href=&quot;https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html&quot;>;Canny edge&lt;/a>;. For each task, we select about 100K images from a &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;web-scale image-text dataset&lt;/a>;, and compute control signals using corresponding MediaPipe solutions. We use refined captions from &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; for training the plugins. &lt;/p>; &lt;br />; &lt;h3>;Face Landmark&lt;/h3>; &lt;p>; The MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_landmarker&quot;>;Face Landmarker&lt;/a>; task computes 478 landmarks (with attention) of a human face. We use the &lt;a href=&quot;https://github.com/google/mediapipe/blob/26a7ca5c64cd885978677931a7218d33cd7d1dec/mediapipe/python/solutions/drawing_utils.py&quot;>;drawing utils&lt;/a>; in MediaPipe to render a face, including face contour, mouth, eyes, eyebrows, and irises, with different colors. The following table shows randomly generated samples by conditioning on face mesh and prompts. As a comparison, both ControlNet and Plugin can control text-to-image generation with given conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s1346/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1018&quot; data-original-width=&quot;1346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5HVe80CJwcW0VAHJTw8p7gaY00rZejs39WM_udfGW5vsBmltAjHHKBlCPIOpIhHo3yhWs8uJLo79g9R6Lo9MG-IYLqImfjcCrEJPea2nlwF17_9a5-gv5vo7BwSXgMsYU1XJ7l3KYvpkAC-ifUpZP6TNO-yVwgVAlix4WeQN6yaqmxdFiQFaFaMCG7e4/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Face-landmark plugin for text-to-image generation, compared with ControlNet.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Holistic Landmark&lt;/h3>; &lt;p>; MediaPipe &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker&quot;>;Holistic Landmarker&lt;/a>; task includes landmarks of body pose, hands, and face mesh. Below, we generate various stylized images by conditioning on the holistic features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s1351/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;970&quot; data-original-width=&quot;1351&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_cmhhm6zxHOo6cWGTDJpOXDTPGSQ4uYhxLLHVAXrnBivDq4AzcSAFP9p0MO62Kan3Nk22YMy9Rn3lpCw4rFXu5T-zQ788Y1yImejy7PvWV5kDi19h8QlJeEO92NkIScQv9OjfMB6HaTFCKXg6T4odr-GVyph4IGoDuQl5r8CZ574_qwBgMGBGBjFi-M/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Holistic-landmark plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Depth&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s1344/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1344&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfp0hIP0WS9Lv5MZ0YheoV0nOtviyeBVChi6I5gjnm7fKy_dYWPzgMe84SA-7vWLRH0nJp2FWpUK_be9CDHfH4ZN8qvWrXmDAM-YGktYMLuMnaEfRCMU_gkfZDqDTesV-D6FEVMghPhJsHCbNMJwXNHfPfhCoeNBZescmQEN3gBMeJR-q42twwuGKGvlc/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depth-plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Canny Edge&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s1345/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;955&quot; data-original-width=&quot;1345&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGHhAasVg0tIxGklm_EpfRFdiiE6gFCWwMyfBu5FPfnWUdVDzIs9GpbQjAimEkYH9uIykptgcMVi06mfoCUjCYkfgaxB9mPpfnCh5iZoSNYbb4mXKn66XXKFGs5e1VKpSTeRdKI1L1xIcvgfh-9mDvVJ6HTXTVobNn53AO8Kew7u5oqqmwpiJxmVOoQ50/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Canny-edge plugin for text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We conduct a quantitative study of the &lt;em>;face landmark&lt;/em>; plugin to demonstrate the model&#39;s performance. The evaluation dataset contains 5K human images. We compare the generation quality as measured by the widely used metrics, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance&quot;>;Fréchet Inception Distance&lt;/a>; (FID) and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; scores. The base model is a pre-trained text-to-image diffusion model. We use &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>; v1.5 here. &lt;/p>; &lt;p>; As shown in the following table, both ControlNet and the MediaPipe diffusion plugin produce much better sample quality than the base model, in terms of FID and CLIP scores. Unlike ControlNet, which needs to run at every diffusion step, the MediaPipe plugin only runs once for each image generated. We measured the performance of the three models on a server machine (with Nvidia V100 GPU) and a mobile phone (Galaxy S23). On the server, we run all three models with 50 diffusion steps, and on mobile, we run 20 diffusion steps using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/image_generator&quot;>;MediaPipe image generation app&lt;/a>;. Compared with ControlNet, the MediaPipe plugin shows a clear advantage in inference efficiency while preserving the sample quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Model&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;FID↓&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td rowspan=&quot;2&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;CLIP↑&lt;/strong>; &lt;/td>; &lt;td rowspan=&quot;2&quot;>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Inference Time (s)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Nvidia V100&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;10.32 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.26 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.5 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + ControlNet &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.51 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.31 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.4 (+48%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;18.2 (+58.3%) &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Base + MediaPipe Plugin &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;6.50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.30 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.0 (+0.2%) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.8 (+2.6%) &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quantitative comparison on FID, CLIP, and inference time.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We test the performance of the plugin on a wide range of mobile devices from mid-tier to high-end. We list the results on some representative devices in the following table, covering both Android and iOS. &lt;/p>; &lt;p>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td rowspan=&quot;2&quot;>;&lt;strong>;Device&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;7&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Android&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;iOS&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 4&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 6&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Pixel 7&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Galaxy S23&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 12 Pro&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;iPhone 13 Pro&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;strong>;Time&lt;/strong>; (ms) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;128 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;68 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;48 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;73 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;63 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Inference time (ms) of the plugin on different mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we present MediaPipe, a portable plugin for conditioned text-to-image generation. It injects features extracted from a condition image to a diffusion model, and consequently controls the image generation. Portable plugins can be connected to pre-trained diffusion models running on servers or devices. By running text-to-image generation and plugins fully on-device, we enable more flexible applications of generative AI. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank all team members who contributed to this work: Raman Sarokin and Juhyun Lee for the GPU inference solution; Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann for leadership. Special thanks to Jiuqiang Tang&lt;/em>;&lt;i>;, Joe Zou and Lu wang,&lt;/i>;&lt;em>;&amp;nbsp;who made this technology and all the demos running on-device.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9161751123508054082/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9161751123508054082&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/on-device-diffusion-plugins-for.html&quot; rel=&quot;alternate&quot; title=&quot;On-device diffusion plugins for conditioned text-to-image generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWYz8RL4gwfO_N5cGmW4m-wglXWgC2UxgAJg70bS6arMkhdFdN-sWs66tOCm4tuw7w7Kuow4pHnLksYqRUz_rH2LplGJ7Wk5rm7wztNEoSsTiPIZKYPhZsnEe2OxNvOBDWYd889WJqAQ59-pnPayHiStWADTqpcYzZidBjf8wvM9_NTTL82iK19yjLbvTz/s72-c/MediaPipeDiffusion-hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6036162561497757163&lt;/id>;&lt;published>;2023-06-27T14:19:00.000-07:00&lt;/published>;&lt;updated>;2023-06-27T14:19:04.029-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unifying image-caption and image-classification datasets with prefix conditioning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kuniaki Saito, Student Researcher, Cloud AI Team, and Kihyuk Sohn, Research Scientist, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s320/Prefix%20conditioning%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Pre-training visual language (VL) models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pre-training on image classification data. Image-caption datasets are considered to be more “open-domain” because they contain broader scene types and vocabulary words, which result in models with &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;strong performance in few- and zero-shot recognition tasks&lt;/a>;. However, images with fine-grained class descriptions can be rare, and the class distribution can be imbalanced since image-caption datasets do not go through manual curation. By contrast, large-scale classification datasets, such as &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>;, are often curated and can thus provide fine-grained categories with a balanced label distribution. While it may sound promising, directly combining caption and classification datasets for pre-training is often unsuccessful as it can result in biased representations that do not generalize well to various downstream tasks. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2206.01125&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we demonstrate a pre-training strategy that uses both classification and caption datasets to provide complementary benefits. First, we show that naïvely unifying the datasets results in sub-optimal performance on downstream zero-shot recognition tasks as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem during training through &lt;em>;prefix conditioning&lt;/em>;, a novel simple and effective method that uses prefix tokens to disentangle dataset biases from visual concepts. This approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Prefix conditioning is a generic method that can be easily integrated into existing VL pre-training objectives, such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;Contrastive Language-Image Pre-training&lt;/a>; (CLIP) or &lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;Unified Contrastive Learning&lt;/a>; (UniCL). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;High-level idea&lt;/h2>; &lt;p>; We note that classification datasets tend to be biased in at least two ways: (1) the images mostly contain single objects from restricted domains, and (2) the vocabulary is limited and lacks the linguistic flexibility required for zero-shot learning. For example, the class embedding of “a photo of a dog” optimized for ImageNet usually results in a photo of one dog in the center of the image pulled from the ImageNet dataset, which does not generalize well to other datasets containing images of multiple dogs in different spatial locations or a dog with other subjects. &lt;/p>; &lt;p>; By contrast, caption datasets contain a wider variety of scene types and vocabularies. As shown below, if a model simply learns from two datasets, the language embedding can entangle the bias from the image classification and caption dataset, which can decrease the generalization in zero-shot classification. If we can disentangle the bias from two datasets, we can use language embeddings that are tailored for the caption dataset to improve generalization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/s1150/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1150&quot; data-original-width=&quot;856&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/w476-h640/image1.png&quot; width=&quot;476&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Top&lt;/strong>;: Language embedding entangling the bias from image classification and caption dataset. &lt;strong>;Bottom&lt;/strong>;: Language embeddings disentangles the bias from two datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prefix conditioning&lt;/h2>; &lt;p>; Prefix conditioning is partially inspired by &lt;a href=&quot;https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html&quot;>;prompt tuning&lt;/a>;, which prepends learnable tokens to the input token sequences to instruct a pre-trained model backbone to learn task-specific knowledge that can be used to solve downstream tasks. The prefix conditioning approach differs from prompt tuning in two ways: (1) it is designed to unify image-caption and classification datasets by disentangling the dataset bias, and (2) it is applied to VL pre-training while the standard prompt tuning is used to fine-tune models. Prefix conditioning is an explicit way to specifically steer the behavior of model backbones based on the type of datasets provided by users. This is especially helpful in production when the number of different types of datasets is known ahead of time. &lt;/p>; &lt;p>; During training, prefix conditioning learns a text token (prefix token) for each dataset type, which absorbs the bias of the dataset and allows the remaining text tokens to focus on learning visual concepts. Specifically, it prepends prefix tokens for each dataset type to the input tokens that inform the language and visual encoder of the input data type (eg, classification vs. caption). Prefix tokens are trained to learn the dataset-type-specific bias, which enables us to disentangle that bias in language representations and utilize the embedding learned on the image-caption dataset during test time, even without an input caption. &lt;/p>; &lt;p>; We utilize prefix conditioning for CLIP using a language and visual encoder. During test time, we employ the prefix used for the image-caption dataset since the dataset is supposed to cover broader scene types and vocabulary words, leading to better performance in zero-shot recognition. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s1038/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;1038&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the Prefix Conditioning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We apply prefix conditioning to two types of contrastive loss, &lt;a href=&quot;http://proceedings.mlr.press/v139/radford21a&quot;>;CLIP&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf&quot;>;UniCL,&lt;/a>; and evaluate their performance on zero-shot recognition tasks compared to models trained with &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet21K&lt;/a>; (IN21K) and &lt;a href=&quot;https://github.com/google-research-datasets/conceptual-12m&quot;>;Conceptual 12M&lt;/a>; (CC12M). CLIP and UniCL models trained with two datasets using prefix conditioning show large improvements in zero-shot classification accuracy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s440/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Zero-shot classification accuracy of models trained with only IN21K or CC12M compared to CLIP and UniCL models trained with both two datasets using prefix conditioning (“Ours”). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on test-time prefix&lt;/h3>; &lt;p>; The table below describes the performance change by the prefix used during test time. We demonstrate that by using the same prefix used for the classification dataset (“Prompt”), the performance on the classification dataset (&lt;a href=&quot;https://www.image-net.org/&quot;>;IN-1K&lt;/a>;) improves. When using the same prefix used for the image-caption dataset (“Caption”), the performance on other datasets (Zero-shot AVG) improves. This analysis illustrates that if the prefix is tailored for the image-caption dataset, it achieves better generalization of scene types and vocabulary words. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/w640-h396/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis of the prefix used for test-time. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study on robustness to image distribution shift &lt;/h3>; &lt;p>; We study the shift in image distribution using ImageNet variants. We see that the “Caption” prefix performs better than “Prompt” in &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>; (IN-R) and &lt;a href=&quot;https://github.com/HaohanWang/ImageNet-Sketch&quot;>;ImageNet-Sketch&lt;/a>; (IN-S), but underperforms in &lt;a href=&quot;https://github.com/modestyachts/ImageNetV2&quot;>;ImageNet-V2&lt;/a>; (IN-V2). This indicates that the “Caption” prefix achieves generalization on domains far from the classification dataset. Therefore, the optimal prefix probably differs by how far the test domain is from the classification dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVybxgVUvsT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUxN/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Analysis on the robustness to image-level distribution shift. IN: ImageNet, IN-V2: ImageNet-V2, IN-R: Art, Cartoon style ImageNet, IN-S: ImageNet Sketch. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce prefix conditioning, a technique for unifying image caption and classification datasets for better zero-shot classification. We show that this approach leads to better zero-shot classification accuracy and that the prefix can control the bias in the language embedding. One limitation is that the prefix learned on the caption dataset is not necessarily optimal for the zero-shot classification. Identifying the optimal prefix for each test dataset is an interesting direction for future work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Thanks to Zizhao Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6036162561497757163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html&quot; rel=&quot;alternate&quot; title=&quot;Unifying image-caption and image-classification datasets with prefix conditioning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s72-c/Prefix%20conditioning%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8471212041038116532&lt;/id>;&lt;published>;2023-06-23T12:24:00.001-07:00&lt;/published>;&lt;updated>;2023-06-23T12:34:21.337-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Preference learning with automated feedback for cache eviction&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ramki Gummadi, Software Engineer, Google and Kevin Chen, Software Engineer, YouTube&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s320/Halp%20hero%20gif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Caching is a ubiquitous idea in computer science that significantly improves the performance of storage and retrieval systems by storing a subset of popular items closer to the client based on request patterns. An important algorithmic piece of cache management is the decision policy used for dynamically updating the set of items being stored, which has been extensively optimized over several decades, resulting in several &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;>;efficient and robust heuristics&lt;/a>;. While applying machine learning to cache policies has shown promising results in recent years (eg, &lt;a href=&quot;https://www.usenix.org/conference/nsdi20/presentation/song&quot;>;LRB&lt;/a>;, &lt;a href=&quot;https://www.usenix.org/system/files/conference/nsdi18/nsdi18-beckmann.pdf&quot;>;LHD&lt;/a>;, &lt;a href=&quot;https://www.pdl.cmu.edu/PDL-FTP/Storage/MLSys2021-zhou.pdf&quot;>;storage applications&lt;/a>;), it remains a challenge to outperform robust heuristics in a way that can generalize reliably beyond benchmarks to production settings, while maintaining competitive compute and memory overheads. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c9ebaf640152028a6cdedb684577a7c9e52b6f10.pdf&quot;>;HALP: Heuristic Aided Learned Preference Eviction Policy for YouTube Content Delivery Network&lt;/a>;”, presented at &lt;a href=&quot;https://www.usenix.org/conference/nsdi23&quot;>;NSDI 2023&lt;/a>;, we introduce a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. The Heuristic Aided Learned Preference (HALP) framework is a meta-algorithm that uses randomization to merge a lightweight heuristic baseline eviction rule with a learned reward model. The reward model is a lightweight neural network that is continuously trained with ongoing automated feedback on preference comparisons designed to mimic the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;offline oracle&lt;/a>;. We discuss how HALP has improved infrastructure efficiency and user video playback latency for YouTube&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Content_delivery_network&quot;>;content delivery network&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learned preferences for cache eviction decisions&lt;/h2>; &lt;p>; The HALP framework computes cache eviction decisions based on two components: (1) a neural reward model trained with automated feedback via preference learning, and (2) a meta-algorithm that combines a learned reward model with a fast heuristic. As the cache observes incoming requests, HALP continuously trains a small neural network that predicts a scalar reward for each item by formulating this as a preference learning method via &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_to_rank#Pairwise_approach&quot;>;pairwise&lt;/a>; preference feedback. This aspect of HALP is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&quot;>;reinforcement learning from human feedback&lt;/a>; (RLHF) systems, but with two important distinctions: &lt;/p>; &lt;ul>; &lt;li>;Feedback is &lt;em>;automated&lt;/em>; and leverages well-known results about the structure of offline &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0020019006003449&quot;>;optimal&lt;/a>; cache eviction policies. &lt;/li>;&lt;li>;The model is &lt;em>;learned continuously&lt;/em>; using a transient buffer of training examples constructed from the automated feedback process. &lt;/li>; &lt;/ul>; &lt;p>; The eviction decisions rely on a filtering mechanism with two steps. First, a small subset of candidates is selected using a heuristic that is efficient, but suboptimal in terms of performance. Then, a re-ranking step optimizes from within the baseline candidates via the sparing use of a neural network scoring function to “boost” the quality of the final decision. &lt;/p>; &lt;p>; As a production ready cache policy implementation, HALP not only makes eviction decisions, but also subsumes the end-to-end process of sampling pairwise preference queries used to efficiently construct relevant feedback and update the model to power eviction decisions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A neural reward model&lt;/h2>; &lt;p>; HALP uses a light-weight two-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) as its reward model to selectively score individual items in the cache. The features are constructed and managed as a metadata-only “ghost cache” (similar to classical policies like &lt;a href=&quot;https://en.wikipedia.org/wiki/Adaptive_replacement_cache&quot;>;ARC&lt;/a>;). After any given lookup request, in addition to regular cache operations, HALP conducts the book-keeping (eg, tracking and updating feature metadata in a capacity-constrained key-value store) needed to update the dynamic internal representation. This includes: (1) externally tagged features provided by the user as input, along with a cache lookup request, and (2) internally constructed dynamic features (eg, time since last access, average time between accesses) constructed from lookup times observed on each item. &lt;/p>; &lt;p>; HALP learns its reward model fully online starting from a random weight initialization. This might seem like a bad idea, especially if the decisions are made exclusively for optimizing the reward model. However, the eviction decisions rely on both the learned reward model and a suboptimal but simple and robust heuristic like &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU&quot;>;LRU&lt;/a>;. This allows for optimal performance when the reward model has fully generalized, while remaining robust to a temporarily uninformative reward model that is yet to generalize, or in the process of catching up to a changing environment. &lt;/p>; &lt;p>; Another advantage of online training is specialization. Each cache server runs in a potentially different environment (eg, geographic location), which influences local network conditions and what content is locally popular, among other things. Online training automatically captures this information while reducing the burden of generalization, as opposed to a single offline training solution.&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scoring samples from a randomized priority queue &lt;/h2>; &lt;p>; It can be impractical to optimize for the quality of eviction decisions with an exclusively learned objective for two reasons. &lt;/p>; &lt;ol>; &lt;li>;Compute efficiency constraints: Inference with a learned network can be significantly more expensive than the computations performed in practical cache policies operating at scale. This limits not only the expressivity of the network and features, but also how often these are invoked during each eviction decision. &lt;/li>;&lt;li>;Robustness for generalizing out-of-distribution: HALP is deployed in a setup that involves continual learning, where a quickly changing workload might generate request patterns that might be temporarily out-of-distribution with respect to previously seen data. &lt;/li>; &lt;/ol>; &lt;p>; To address these issues, HALP first applies an inexpensive heuristic scoring rule that corresponds to an eviction priority to identify a small candidate sample. This process is based on efficient random sampling that &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;approximates&lt;/a>; exact &lt;a href=&quot;https://en.wikipedia.org/wiki/Priority_queue&quot;>;priority queues&lt;/a>;. The priority function for generating candidate samples is intended to be quick to compute using existing manually-tuned algorithms, eg, LRU. However, this is configurable to approximate other cache replacement heuristics by editing a simple cost function. Unlike &lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;prior work&lt;/a>;, where the randomization was used to tradeoff approximation for efficiency, HALP also &lt;em>;relies&lt;/em>; on the inherent randomization in the sampled candidates across time steps for providing the necessary &lt;a href=&quot;https://lilianweng.github.io/posts/2020-06-07-exploration-drl/&quot;>;exploratory&lt;/a>; diversity in the sampled candidates for both training and inference. &lt;/p>; &lt;p>; The final evicted item is chosen from among the supplied candidates, equivalent to the &lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;>;best-of-n&lt;/a>; reranked sample, corresponding to maximizing the predicted&lt;em>; &lt;/em>;preference score according to the neural reward model. The same pool of candidates used for eviction decisions is also used to construct the pairwise preference queries for automated feedback, which helps minimize the training and inference skew between samples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s1470/Halp.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s16000/Halp.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the two-stage process invoked for each eviction decision.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Online preference learning with automated feedback&lt;/h2>; &lt;p>; The reward model is learned using online feedback, which is based on automatically assigned preference labels that indicate, wherever feasible, the ranked preference ordering for the time taken to receive future re-accesses, starting from a given snapshot in time among each queried sample of items. This is similar to the oracle optimal policy, which, at any given time, evicts an item with the farthest future access from all the items in the cache. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generation of the automated feedback for learning the reward model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To make this feedback process informative, HALP constructs pairwise preference queries that are most likely to be relevant for eviction decisions. In sync with the usual cache operations, HALP issues a small number of pairwise preference queries while making each eviction decision, and appends them to a set of &lt;em>;pending comparisons. &lt;/em>;The labels for these pending comparisons can only be resolved at a random future time. To operate online, HALP also performs some additional book-keeping after each lookup request to process any pending comparisons that can be labeled incrementally after the current request. HALP indexes the pending comparison buffer with each element involved in the comparison, and recycles the memory consumed by stale comparisons (neither of which may ever get a re-access) to ensure that the memory overhead associated with feedback generation remains bounded over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/s1468/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1032&quot; data-original-width=&quot;1468&quot; height=&quot;450&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/w640-h450/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of all main components in HALP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results: Impact on the YouTube CDN&lt;/h2>; &lt;p>; Through empirical analysis, we show that HALP compares favorably to state-of-the-art cache policies on public benchmark traces in terms of cache miss rates. However, while public benchmarks are a useful tool, they are rarely sufficient to capture all the usage patterns across the world over time, not to mention the diverse hardware configurations that we have already deployed. &lt;/p>; &lt;p>; Until recently, YouTube servers used an optimized LRU-variant for memory cache eviction. HALP increases YouTube&#39;s memory egress/ingress — the ratio of the total bandwidth egress served by the CDN to that consumed for retrieval (ingress) due to cache misses — by roughly 12% and memory hit rate by 6%. This reduces latency for users, since memory reads are faster than disk reads, and also improves egressing capacity for disk-bounded machines by shielding the disks from traffic. &lt;/p>; &lt;p>; The figure below shows a visually compelling reduction in the &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usits97/full_papers/cao/cao_html/node12.html&quot;>;byte miss ratio&lt;/a>; in the days following HALP&#39;s final rollout on the YouTube CDN, which is now serving significantly more content from within the cache with lower latency to the end user, and without having to resort to more expensive retrieval that increases the operating costs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s505/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;257&quot; data-original-width=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Aggregate worldwide YouTube byte miss ratio before and after rollout (vertical dashed line).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An aggregated performance improvement could still hide important regressions. In addition to measuring overall impact, we also conduct an analysis in the paper to understand its impact on different racks using a machine level analysis, and find it to be overwhelmingly positive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduced a scalable state-of-the-art cache eviction framework that is based on learned rewards and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;preference learning&lt;/a>; with automated feedback. Because of its design choices, HALP can be deployed in a manner similar to any other cache policy without the operational overhead of having to separately manage the labeled examples, training procedure and the model versions as additional offline pipelines common to most machine learning systems. Therefore, it incurs only a small extra overhead compared to other classical algorithms, but has the added benefit of being able to take advantage of additional features to make its eviction decisions and continuously adapt to changing access patterns. &lt;/p>; &lt;p>; This is the first large-scale deployment of a learned cache policy to a widely used and heavily trafficked CDN, and has significantly improved the CDN infrastructure efficiency while also delivering a better quality of experience to users. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Ramki Gummadi is now part of Google DeepMind. We would like to thank John Guilyard for help with the illustrations and Richard Schooler for feedback on this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8471212041038116532/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html&quot; rel=&quot;alternate&quot; title=&quot;Preference learning with automated feedback for cache eviction&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s72-c/Halp%20hero%20gif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3798161588273442525&lt;/id>;&lt;published>;2023-06-22T11:33:00.000-07:00&lt;/published>;&lt;updated>;2023-06-22T11:33:53.578-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Audio&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SoundStorm: Efficient parallel audio generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zalán Borsos, Research Software Engineer, and Marco Tagliasacchi, Senior Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s2400/SoundStorm.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The recent progress in generative AI unlocked the possibility of creating new content in several different domains, including text, vision and audio. These models often rely on the fact that raw data is first converted to a compressed format as a sequence of tokens. In the case of audio, neural audio codecs (eg, &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;SoundStream&lt;/a>; or &lt;a href=&quot;https://github.com/facebookresearch/encodec&quot;>;EnCodec&lt;/a>;) can efficiently compress waveforms to a compact representation, which can be inverted to reconstruct an approximation of the original audio signal. Such a representation consists of a sequence of discrete audio tokens, capturing the local properties of sounds (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Phoneme&quot;>;phonemes&lt;/a>;) and their temporal structure (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;>;prosody&lt;/a>;). By representing audio as a sequence of discrete tokens, audio generation can be performed with &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based sequence-to-sequence models — this has unlocked rapid progress in speech continuation (eg, with &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;), text-to-speech (eg, with &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;), and general audio and music generation (eg, &lt;a href=&quot;https://felixkreuk.github.io/audiogen/&quot;>;AudioGen&lt;/a>; and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;). Many generative audio models, including AudioLM, rely on auto-regressive decoding, which produces tokens one by one. While this method achieves high acoustic quality, inference (ie, calculating an output) can be slow, especially when decoding long sequences. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To address this issue, in “&lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm: Efficient Parallel Audio Generation&lt;/a>;”, we propose a new method for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on two novel elements: 1) an architecture adapted to the specific nature of audio tokens as produced by the SoundStream neural codec, and 2) a decoding scheme inspired by &lt;a href=&quot;https://arxiv.org/abs/2202.04200&quot;>;MaskGIT&lt;/a>;, a recently proposed method for image generation, which is tailored to operate on audio tokens. Compared to the autoregressive decoding approach of AudioLM, SoundStorm is able to generate tokens in parallel, thus decreasing the inference time by 100x for long sequences, and produces audio of the same quality and with higher consistency in voice and acoustic conditions. Moreover, we show that SoundStorm, coupled with the text-to-semantic modeling stage of &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>;, can synthesize high-quality, natural dialogues, allowing one to control the spoken content (via transcripts), speaker voices (via short voice prompts) and speaker turns (via transcript annotations), as demonstrated by the examples below: &lt;/p>; &lt;br>; &lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/SoundStorm_promo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;br>; &lt;br>; &lt;table>; &lt;tr>; &lt;td>;&lt;b>;Input: Text&lt;em>; (transcript used to drive the audio generation in bold)&lt;/em>;&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;Something really funny happened to me this morning. | Oh wow, what? | &lt;b>;Well, uh I woke up as usual. | Uhhuh | Went downstairs to have uh breakfast. | Yeah | Started eating. Then uh 10 minutes later I realized it was the middle of the night. | Oh no way, that&#39;s so funny!&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size: small;&quot;>;&lt;em>;I didn&#39;t sleep well last night. |不好了。发生了什么？ | &lt;b>;I don&#39;t know. II just couldn&#39;t seem to uh to fall asleep somehow, I kept tossing and turning all night. |这太糟糕了。 Maybe you should uh try going to bed earlier tonight or uh maybe you could try reading a book. | Yeah, thanks for the suggestions, I hope you&#39;re right. |没问题。 II hope you get a good night&#39;s sleep&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Input: Audio prompt&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;Output: Audio prompt + generated audio&lt;/b>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/mp_joke_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;SoundStorm design&lt;/h2>; &lt;p>; In our previous work on AudioLM, we showed that audio generation can be decomposed into two steps: 1) semantic modeling, which generates semantic tokens from either previous semantic tokens or a conditioning signal (eg, a transcript as in SPEAR-TTS, or a text prompt as in MusicLM), and 2) acoustic modeling, which generates acoustic tokens from semantic tokens. With SoundStorm we specifically address this second, acoustic modeling step, replacing slower autoregressive decoding with faster parallel decoding. &lt;/p>; &lt;p>; SoundStorm relies on a bidirectional attention-based &lt;a href=&quot;https://arxiv.org/abs/2005.08100&quot;>;Conformer&lt;/a>;, a model architecture that combines a Transformer with convolutions to capture both local and global structure of a sequence of tokens. Specifically, the model is trained to predict audio tokens produced by SoundStream given a sequence of semantic tokens generated by AudioLM as input. When doing this, it is important to take into account the fact that, at each time step &lt;em>;t&lt;/em>;, SoundStream uses up to &lt;em>;Q&lt;/em>; tokens to represent the audio using a method known as &lt;em>;residual vector quantization&lt;/em>; (RVQ), as illustrated below on the right. The key intuition is that the quality of the reconstructed audio progressively increases as the number of generated tokens at each step goes from 1 to &lt;em>;Q&lt;/em>;. &lt;/p>; &lt;p>; At inference time, given the semantic tokens as input conditioning signal, SoundStorm starts with all audio tokens masked out, and fills in the masked tokens over multiple iterations, starting from the coarse tokens at RVQ level &lt;em>;q = 1&lt;/em>; and proceeding level-by-level with finer tokens until reaching level &lt;em>;q = Q&lt;/em>;. &lt;/p>; &lt;p>; There are two crucial aspects of SoundStorm that enable fast generation: 1) tokens are predicted in parallel during a single iteration within a RVQ level and, 2) the model architecture is designed in such a way that the complexity is only mildly affected by the number of levels &lt;em>;Q&lt;/em>;. To support this inference scheme, during training a carefully designed masking scheme is used to mimic the iterative process used at inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;951&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SoundStorm model architecture. &lt;em>;T&lt;/em>; denotes the number of time steps and &lt;em>;Q&lt;/em>; the number of RVQ levels used by SoundStream. The semantic tokens used as conditioning are time-aligned with the SoundStream frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Measuring SoundStorm performance&lt;/h2>; &lt;p>; We demonstrate that SoundStorm matches the quality of AudioLM&#39;s acoustic generator, replacing both AudioLM&#39;s stage two (coarse acoustic model) and stage three (fine acoustic model). Furthermore, SoundStorm produces audio 100x faster than AudioLM&#39;s hierarchical autoregressive acoustic generator (top half below) with matching quality and improved consistency in terms of speaker identity and acoustic conditions (bottom half below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1122&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Runtimes of SoundStream decoding, SoundStorm and different stages of AudioLM on a TPU-v4.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s1312/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1312&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Acoustic consistency between the prompt and the generated audio. The shaded area represents the inter-quartile range.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Safety and risk mitigation&lt;/h2>; &lt;p>; We acknowledge that the audio samples produced by the model may be influenced by the unfair biases present in the training data, for instance in terms of represented accents and voice characteristics. In our generated samples, we demonstrate that we can reliably and responsibly control speaker characteristics via prompting, with the goal of avoiding unfair biases. A thorough analysis of any training data and its limitations is an area of future work in line with our responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. &lt;/p>; &lt;p>; In turn, the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification and using the model for the purpose of impersonation. Thus, it is crucial to put in place safeguards against potential misuse: to this end, we have verified that the audio generated by SoundStorm remains detectable by a dedicated classifier using the same classifier as described in our original AudioLM paper. Hence, as a component of a larger system, we believe that SoundStorm would be unlikely to introduce additional risks to those discussed in our earlier papers on AudioLM and SPEAR-TTS. At the same time, relaxing the memory and computational requirements of AudioLM would make research in the domain of audio generation more accessible to a wider community. In the future, we plan to explore other approaches for detecting synthesized speech, eg, with the help of audio watermarking, so that any potential product usage of this technology strictly follows our responsible AI Principles. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have introduced SoundStorm, a model that can efficiently synthesize high-quality audio from discrete conditioning tokens. When compared to the acoustic generator of &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;, SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>; with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker turns, controlling both the voices of the speakers and the generated content. SoundStorm is not limited to generating speech. For example, MusicLM uses SoundStorm to synthesize longer outputs efficiently (&lt;a href=&quot;https://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot;>;as seen at I/O&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The work described here was authored by Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour and Marco Tagliasacchi. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3798161588273442525/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html&quot; rel=&quot;alternate&quot; title=&quot;SoundStorm: Efficient parallel audio generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s72-c/SoundStorm.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3542229559799425219&lt;/id>;&lt;published>;2023-06-21T13:57:00.004-07:00&lt;/published>;&lt;updated>;2023-06-22T11:02:35.392-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Automatic Speech Recognition&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: AI for Social Good&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jimmy Tobin and Katrin Tomanek, Software Engineers, Google Research, AI for Social Good&lt;/span>; &lt;p>; Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>; team consists of researchers, engineers, volunteers, and others with a shared focus on positive social impact. Our mission is to demonstrate AI&#39;s societal benefit by enabling real-world value, with projects spanning work in &lt;a href=&quot;https://blog.google/technology/health/making-data-useful-public-health/&quot;>;public health&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/making-android-more-accessible/&quot;>;accessibility&lt;/a>;, &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;crisis response&lt;/a>;, &lt;a href=&quot;https://sustainability.google/&quot;>;climate and energy,&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;nature and society&lt;/a>;. We believe that the best way to drive positive change in underserved communities is by partnering with change-makers and the organizations they serve. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post we discuss work done by &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, a team within &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>;, that aims to &lt;a href=&quot;https://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html&quot;>;improve automatic speech recognition&lt;/a>; (ASR) for people with disordered speech. For people with typical speech, an ASR model&#39;s word error rate (WER) can be less than 10%. But for people with disordered speech patterns, such as stuttering, &lt;a href=&quot;https://www.mayoclinic.org/diseases-conditions/dysarthria/symptoms-causes/syc-20371994&quot;>;dysarthria&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Apraxia&quot;>;apraxia&lt;/a>;, the WER could reach 50% or even 90% depending on the etiology and severity. To help address this problem, we worked with more than 1,000 participants to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-euphonia-1000-hours-speech-recordings/&quot;>;collect over 1,000 hours of disordered speech samples&lt;/a>; and used the data to show that ASR &lt;a href=&quot;https://ai.googleblog.com/2021/09/personalized-asr-models-from-large-and.html&quot;>;personalization&lt;/a>; is a viable avenue for bridging the performance gap for users with disordered speech. We&#39;ve shown that personalization can be successful with as little as &lt;a href=&quot;https://arxiv.org/abs/2110.04612&quot;>;3-4 minutes of training speech&lt;/a>;&amp;nbsp;using &lt;a href=&quot;https://arxiv.org/abs/1907.13511&quot;>;layer freezing techniques&lt;/a>;. &lt;/p>; &lt;p>; This work led to the development of &lt;a href=&quot;https://sites.research.google/relate/&quot;>;Project Relate&lt;/a>;&amp;nbsp;for anyone with atypical speech who could benefit from a personalized speech model. Built in partnership with &lt;a href=&quot;https://research.google/research-areas/speech-processing/&quot;>;Google&#39;s Speech team&lt;/a>;, Project Relate enables people who find it hard to be understood by other people and technology to train their own models. People can use these personalized models to communicate more effectively and gain more independence. To make ASR more accessible and usable, we describe how we fine-tuned Google&#39;s &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; (USM) to better understand disordered speech out of the box, without personalization, for use with digital assistant technologies, dictation apps, and in conversations. &lt;/p>; &lt;br />; &lt;h2>;Addressing the challenges&lt;/h2>; &lt;p>; Working closely with Project Relate users, it became clear that personalized models can be very useful, but for many users, recording dozens or hundreds of examples can be challenging. In addition, the personalized models did not always perform well in freeform conversation. &lt;/p>; &lt;p>; To address these challenges, Euphonia&#39;s research efforts have been focusing on &lt;em>;speaker independent&lt;/em>; ASR (SI-ASR) to make models work better out of the box for people with disordered speech so that no additional training is necessary. &lt;/p>; &lt;br />; &lt;h2>;Prompted Speech dataset for SI-ASR&lt;/h2>; &lt;p>; The first step in building a robust SI-ASR model was to create representative dataset splits. We created the Prompted Speech dataset by splitting the &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2021/macdonald21_interspeech.pdf&quot;>;Euphonia corpus&lt;/a>; into train, validation and test portions, while ensuring that each split spanned a range of speech impairment severity and underlying etiology and that no speakers or phrases appeared in multiple splits. The training portion consists of over 950k speech utterances from over 1,000 speakers with disordered speech. The test set contains around 5,700 utterances from over 350 speakers. Speech-language pathologists manually reviewed all of the utterances in the test set for transcription accuracy and audio quality. &lt;/p>; &lt;br />; &lt;h2>;Real Conversation test set&lt;/h2>; &lt;p>; Unprompted or conversational speech differs from prompted speech in several ways. In conversation, people speak faster and enunciate less. They repeat words, repair misspoken words, and use a more expansive vocabulary that is specific and personal to themselves and their community. To improve a model for this use case, we created the Real Conversation test set to benchmark performance. &lt;/p>; &lt;p>; The Real Conversation test set was created with the help of trusted testers who recorded themselves speaking during conversations. The audio was reviewed, any personally identifiable information (PII) was removed, and then that data was transcribed by speech-language pathologists. The Real Conversation test set contains over 1,500 utterances from 29 speakers. &lt;/p>; &lt;br />; &lt;h2>;Adapting USM to disordered speech&lt;/h2>; &lt;p>; We then tuned USM on the training split of the Euphonia Prompted Speech set to improve its performance on disordered speech. Instead of fine-tuning the full model, our tuning was based on &lt;a href=&quot;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&quot;>;residual adapters&lt;/a>;, a parameter-efficient tuning approach that adds tunable bottleneck layers as residuals between the transformer layers. Only these layers are tuned, while the rest of the model weights are untouched. We have &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.541/&quot;>;previously shown&lt;/a>; that this approach works very well to adapt ASR models to disordered speech. Residual adapters were only added to the encoder layers, and the bottleneck dimension was set to 64. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate the adapted USM, we compared it to older ASR models using the two test sets described above. For each test, we compare adapted USM to the pre-USM model best suited to that task: (1) For short prompted speech, we compare to Google&#39;s production ASR model optimized for short form ASR; (2) for longer Real Conversation speech, we compare to a model &lt;a href=&quot;https://arxiv.org/abs/2005.03271&quot;>;trained for long form ASR&lt;/a>;. USM improvements over pre-USM models can be explained by USM&#39;s relative size increase, 120M to 2B parameters, and other improvements discussed in the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;USM blog post&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model word error rates (WER) for each test set (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see that the USM adapted with disordered speech significantly outperforms the other models. The adapted USM&#39;s WER on Real Conversation is 37% better than the pre-USM model, and on the Prompted Speech test set, the adapted USM performs 53% better. &lt;/p>; &lt;p>; These findings suggest that the adapted USM is significantly more usable for an end user with disordered speech. We can demonstrate this improvement by looking at transcripts of Real Conversation test set recordings from a trusted tester of Euphonia and Project Relate (see below). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; -->; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example audio and transcriptions of a trusted tester&#39;s speech from the Real Conversation test set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A comparison of the Pre-USM and adapted USM transcripts revealed some key advantages: &lt;/p>; &lt;ul>;&lt;li>; The first example shows that Adapted USM is better at recognizing disordered speech patterns. The baseline misses key words like “XBox” and “controller” that are important for a listener to understand what they are trying to say. &lt;/li>; &lt;li>; The second example is a good example of how deletions are a primary issue with ASR models that are not trained with disordered speech. Though the baseline model did transcribe a portion correctly, a large part of the utterance was not transcribed, losing the speaker&#39;s intended message. &lt;/li>;&lt;/ul>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that this work is an important step towards making speech recognition more accessible to people with disordered speech. We are continuing to work on improving the performance of our models. With the rapid advancements in ASR, we aim to ensure people with disordered speech benefit as well. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Key contributors to this project include Fadi Biadsy, Michael Brenner, Julie Cattiau, Richard Cave, Amy Chung-Yu Chou, Dotan Emanuel, Jordan Green, Rus Heywood, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Bob MacDonald, Philip Nelson, Katie Seaver, Joel Shor, Jimmy Tobin, Katrin Tomanek, and Subhashini Venugopalan. We gratefully acknowledge the support Project Euphonia received from members of the USM research team including Yu Zhang, Wei Han, Nanxin Chen, and many others. Most importantly, we wanted to say a huge thank you to the 2,200+ participants who recorded speech samples and the many &lt;a href=&quot;https://sites.research.google/euphonia/about/#thank-partners&quot;>;advocacy groups&lt;/a>; who helped us connect with these participants.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Audio volume has been adjusted for ease of listening, but the original files would be more consistent with those used in training and would have pauses, silences, variable volume, etc.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3542229559799425219/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: AI for Social Good&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s72-w640-h396-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4041992163804186827&lt;/id>;&lt;published>;2023-06-21T10:29:00.000-07:00&lt;/published>;&lt;updated>;2023-06-21T10:29:53.804-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;The world&#39;s first braiding of non-Abelian anyons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Trond Andersen and Yuri Lensky, Research Scientists, Google Quantum AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s320/Non-Abelian%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine you&#39;re shown two identical objects and then asked to close your eyes. When you open your eyes, you see the same two objects in the same position. How can you determine if they have been swapped back and forth? Intuition and the laws of &lt;a href=&quot;https://en.wikipedia.org/wiki/Identical_particles#Quantum_mechanical_description_of_identical_particles&quot;>;quantum mechanics agree&lt;/a>;: If the objects are truly identical, there is no way to tell. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While this sounds like common sense, it only applies to our familiar three-dimensional world. Researchers have predicted that for a special type of particle, called an &lt;a href=&quot;https://en.wikipedia.org/wiki/Anyon&quot;>;anyon&lt;/a>;, that is restricted to move only in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Plane_(mathematics)&quot;>;two-dimensional&lt;/a>; (2D) plane, quantum mechanics allows for something quite different. Anyons are indistinguishable from one another and some, non-Abelian anyons, have a special property that causes observable differences in the shared quantum state under exchange, making it possible to tell when they have been exchanged, despite being fully indistinguishable from one another. While researchers have managed to detect their relatives, Abelian anyons, whose change under exchange is more subtle and impossible to directly detect, realizing “non-Abelian exchange behavior” has proven more difficult due to challenges with both control and detection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05954-4&quot;>;Non-Abelian braiding of graph vertices in a superconducting processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we report the observation of this non-Abelian exchange behavior for the first time. Non-Abelian anyons could open a new avenue for quantum computation, in which quantum operations are achieved by swapping particles around one another like strings are swapped around one another to create braids. Realizing this new exchange behavior on our &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;superconducting quantum processor&lt;/a>; could be an alternate route to so-called &lt;a href=&quot;https://arxiv.org/abs/quant-ph/9707021&quot;>;topological quantum computation&lt;/a>;, which benefits from being robust against environmental noise. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Exchange statistics and non-Abelian anyons&lt;/h2>; &lt;p>; In order to understand how this strange non-Abelian behavior can occur, it&#39;s helpful to consider an analogy with the braiding of two strings. Take two identical strings and lay them parallel next to one another. Swap their ends to form a double-helix shape. The strings are identical, but because they wrap around one another when the ends are exchanged, it is very clear when the two ends are swapped. &lt;/p>; &lt;p>; The exchange of non-Abelian anyons can be visualized in a similar way, where the strings are made from extending the particles&#39; positions into the time dimension to form “world-lines.” Imagine plotting two particles&#39; locations vs. time. If the particles stay put, the plot would simply be two parallel lines, representing their constant locations. But if we exchange the locations of the particles, the world lines wrap around one another. Exchange them a second time, and you&#39;ve made a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knot_theory&quot;>;knot&lt;/a>;. &lt;/p>; &lt;p>; While a bit difficult to visualize, knots in four dimensions (three spatial plus one time dimension) can always easily be undone. They are trivial — like a shoelace, simply pull one end and it unravels. But when the particles are restricted to two spatial dimensions, the knots are in three total dimensions and — as we know from our everyday 3D lives — cannot always be easily untied. The braiding of the non-Abelian anyons&#39; world lines can be used as quantum computing operations to transform the state of the particles. &lt;/p>; &lt;p>; A key aspect of non-Abelian anyons is “degeneracy”: the full state of several separated anyons is not completely specified by local information, allowing the same anyon configuration to represent superpositions of several quantum states. Winding non-Abelian anyons about each other can change the encoded state. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;How to make a non-Abelian anyon&lt;/h2>; &lt;p>; So how do we realize non-Abelian braiding with one of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sycamore_processor&quot;>;Google&#39;s quantum processors&lt;/a>;? We start with the familiar surface code, which we recently used to achieve a &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestone in quantum error correction&lt;/a>;, where qubits are arranged on the vertices of a checkerboard pattern. Each color square of the checkerboard represents one of two possible joint measurements that can be made of the qubits on the four corners of the square. These so-called “stabilizer measurements” can return a value of either + or – 1. The latter is referred to as a plaquette violation, and can be created and moved diagonally — just like bishops in chess — by applying single-qubit &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_logic_gate&quot;>;X- and Z-gates&lt;/a>;. Recently, we showed that these bishop-like &lt;a href=&quot;https://arxiv.org/abs/2104.01180&quot;>;plaquette violations are Abelian anyons&lt;/a>;. In contrast to non-Abelian anyons, the state of Abelian anyons changes only subtly when they are swapped — so subtly that it is impossible to directly detect. While Abelian anyons are interesting, they do not hold the same promise for topological quantum computing that non-Abelian anyons do. &lt;/p>; &lt;p>; To produce non-Abelian anyons, we need to control the degeneracy (ie, the number of &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function&quot;>;wavefunctions&lt;/a>; that causes all &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;stabilizer&lt;/a>; measurements to be +1). Since a stabilizer measurement returns two possible values, each stabilizer cuts the degeneracy of the system in half, and with sufficiently many stabilizers, only one wave function satisfies the criterion. Hence, a simple way to increase the degeneracy is to merge two stabilizers together. In the process of doing so, we remove one edge in the stabilizer grid, giving rise to two points where only three edges intersect. These points, referred to as “&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;degree-3 vertices&lt;/a>;” (D3Vs), are predicted to be non-Abelian anyons. &lt;/p>; &lt;p>; In order to braid the D3Vs, we have to move them, meaning that we have to stretch and squash the stabilizers into new shapes. We accomplish this by implementing two-qubit gates between the anyons and their neighbors (middle and right panels shown below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/s661/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;661&quot; height=&quot;529&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/w640-h529/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Non-Abelian anyons in stabilizer codes.&lt;strong>; a:&lt;/strong>; Example of a knot made by braiding two anyons&#39; world lines. &lt;strong>;b: &lt;/strong>;Single-qubit gates can be used to create and move stabilizers with a value of –1 (red squares). Like bishops in chess, these can only move diagonally and are therefore constrained to one sublattice in the regular surface code. This constraint is broken when D3Vs (yellow triangles) are introduced.&lt;strong>; c: &lt;/strong>;Process to form and move D3Vs (predicted to be non-Abelian anyons). We start with the surface code, where each square corresponds to a joint measurement of the four qubits on its corners (&lt;strong>;left&lt;/strong>; panel). We remove an edge separating two neighboring squares, such that there is now a single joint measurement of all six qubits (&lt;strong>;middle&lt;/strong>; panel). This creates two D3Vs, which are non-Abelian anyons. We move the D3Vs by applying two-qubit gates between neighboring sites (right panel). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now that we have a way to create and move the non-Abelian anyons, we need to verify their anyonic behavior. For this we examine three characteristics that would be expected of non-Abelian anyons: &lt;/p>; &lt;ol>; &lt;li>;The “&lt;a href=&quot;https://en.wikipedia.org/wiki/Fusion_of_anyons&quot;>;fusion rules&lt;/a>;” — What happens when non-Abelian anyons collide with each other? &lt;/li>;&lt;li>;Exchange statistics — What happens when they are braided around one another? &lt;/li>;&lt;li>;Topological quantum computing primitives — Can we encode qubits in the non-Abelian anyons and use braiding to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;two-qubit entangling operations&lt;/a>;? &lt;/li>; &lt;/ol>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The fusion rules of non-Abelian anyons&lt;/h2>; &lt;p>; We investigate fusion rules by studying how a pair of D3Vs interact with the bishop-like plaquette violations introduced above. In particular, we create a pair of these and bring one of them around a D3V by applying single-qubit gates. &lt;/p>; &lt;p>; While the rules of bishops in chess dictate that the plaquette violations can never meet, the dislocation in the checkerboard lattice allows them to break this rule, meet its partner and annihilate with it. The plaquette violations have now disappeared! But bring the non-Abelian anyons back in contact with one another, and the anyons suddenly morph into the missing plaquette violations. As weird as this behavior seems, it is a manifestation of exactly the fusion rules that we expect these entities to obey. This establishes confidence that the D3Vs are, indeed, non-Abelian anyons. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/s982/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;826&quot; data-original-width=&quot;982&quot; height=&quot;538&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/w640-h538/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Demonstration of anyonic fusion rules (starting with &lt;strong>;panel I&lt;/strong>;, in the &lt;strong>;lower left&lt;/strong>;). We form and separate two D3Vs (yellow triangles), then form two adjacent plaquette violations (red squares) and pass one between the D3Vs. The D3Vs deformation of the “chessboard” changes the bishop rules of the plaquette violations. While they used to lie on adjacent squares, they are now able to move along the same diagonals and collide (as shown by the red lines). When they do collide, they annihilate one another. The D3Vs are brought back together and surprisingly morph into the missing adjacent red plaquette violations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Observation of non-Abelian exchange statistics&lt;/h2>; &lt;p>; After establishing the fusion rules, we want to see the real smoking gun of non-Abelian anyons: non-Abelian exchange statistics. We create two pairs of non-Abelian anyons, then braid them by wrapping one from each pair around each other (shown below). When we fuse the two pairs back together, two pairs of plaquette violations appear. The simple act of braiding the anyons around one another changed the observables of our system. In other words, if you closed your eyes while the non-Abelian anyons were being exchanged, you would still be able to tell that they had been exchanged once you opened your eyes. This is the hallmark of non-Abelian statistics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/s1648/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1648&quot; height=&quot;437&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/w640-h437/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Braiding non-Abelian anyons. We make two pairs of D3Vs (&lt;strong>;panel II&lt;/strong>;), then bring one from each pair around each other (&lt;strong>;III-XI&lt;/strong>;). When fusing the two pairs together again in panel XII, two pairs of plaquette violations appear! Braiding the non-Abelian anyons changed the observables of the system from panel I to panel XII; a direct manifestation of non-Abelian exchange statistics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Topological quantum computing&lt;/h2>; &lt;p>; Finally, after establishing their fusion rules and exchange statistics, we demonstrate how we can use these particles in quantum computations. The non-Abelian anyons can be used to encode information, represented by &lt;a href=&quot;https://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;logical qubits&lt;/a>;, which should be distinguished from the actual &lt;em>;physical&lt;/em>; qubits used in the experiment. The number of logical qubits encoded in &lt;em>;N&lt;/em>; D3Vs can be shown to be &lt;em>;N&lt;/em>;/2–1, so we use &lt;em>;N&lt;/em>;=8 D3Vs to encode three logical qubits, and perform braiding to entangle them. By studying the resulting state, we find that the braiding has indeed led to the formation of the desired, well-known quantum entangled state called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state&quot;>;Greenberger-Horne-Zeilinger&lt;/a>; (GHZ) state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/s750/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;544&quot; data-original-width=&quot;750&quot; height=&quot;464&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/w640-h464/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using non-Abelian anyons as logical qubits.&lt;strong>; a, &lt;/strong>;We braid the non-Abelian anyons to entangle three qubits encoded in eight D3Vs. &lt;strong>;b, &lt;/strong>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_tomography&quot;>;Quantum state tomography&lt;/a>; allows for reconstructing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Density_matrix&quot;>;density matrix&lt;/a>;, which can be represented in a 3D bar plot and is found to be consistent with the desired highly entangled GHZ-state.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments show the first observation of non-Abelian exchange statistics, and that braiding of the D3Vs can be used to perform quantum computations. With future additions, including error correction during the braiding procedure, this could be a major step towards topological quantum computation, a long-sought method to endow qubits with intrinsic resilience against fluctuations and noise that would otherwise cause errors in computations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4041992163804186827/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html&quot; rel=&quot;alternate&quot; title=&quot;The world&#39;s first braiding of non-Abelian anyons&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s72-c/Non-Abelian%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9139300663122353070&lt;/id>;&lt;published>;2023-06-18T11:00:00.018-07:00&lt;/published>;&lt;updated>;2023-06-19T10:54:26.149-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at CVPR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shaina Mehta, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s1200/CVPR%20Design-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week marks the beginning of the premier annual &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;Computer Vision and Pattern Recognition&lt;/a>; conference (CVPR 2023), held in-person in Vancouver, BC (with additional virtual content). As a leader in computer vision research and a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Sponsors&quot;>;Platinum Sponsor&lt;/a>;, &lt;a href=&quot;https://research.google/&quot;>;Google Research&lt;/a>; will have a strong presence across CVPR 2023 with ~90 papers being presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers&quot;>;main conference&lt;/a>; and active involvement in over 40 conference &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/workshop-list&quot;>;workshops&lt;/a>; and &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/tutorial-list&quot;>;tutorials&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you are attending CVPR this year, please stop by our booth to chat with our researchers who are actively exploring the latest techniques for application to various areas of &lt;a href=&quot;https://research.google/pubs/?area=machine-perception&quot;>;machine perception&lt;/a>;. Our researchers will also be available to talk about and demo several recent efforts, including on-device ML applications with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;, strategies for differential privacy, neural radiance field technologies and much more. &lt;/p>; &lt;p>; You can also learn more about our research being presented at CVPR 2023 in the list below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>; Board and organizing committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Senior area chairs include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Area chairs include: &lt;em>;&lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Rodrigo Benenson&lt;/strong>;, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Da-Cheng Jua&lt;/strong>;, &lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Stephen Lombardi&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;Saurabh Singh&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Pratul P. Srinivasan&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, &lt;strong>;Jasper Uijlings&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Publicity Chair: &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Demonstration Chair: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Program Advisory Board includes: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Panels&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>;&lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;History and Future of Artificial Intelligence and Computer Vision &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>;&lt;/div>; &lt;p>; &lt;/p>; &lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;Scientific Discovery and the Environment &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;/div>; &lt;/div>; &lt;br />; &lt;h2>;Best Paper Award candidates&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.00277.pdf&quot;>;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhiqin Chen&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11082.pdf&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Qianqian Wang&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Richard Tucker&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.12242.pdf&quot;>;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/a>; &lt;br />; &lt;em>;Nataniel Ruiz*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Kfir Aberman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03142.pdf&quot;>;On Distillation of Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Chenlin Meng, Robin Rombach,&lt;strong>; Ruiqi Gao&lt;/strong>;,&lt;strong>; Diederik Kingma&lt;/strong>;, Stefano Ermon,&lt;strong>; Jonathan Ho&lt;/strong>;,&lt;strong>; Tim Salimans&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Highlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.11217.pdf&quot;>;Connecting Vision and Language with Video Localized Narratives&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Paul Voigtlaender&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.05496.pdf&quot;>;MaskSketch: Unpaired Structure-Guided Masked Image Generation&lt;/a>; &lt;br />; &lt;em>;Dina Bashkirova*,&lt;strong>; Jose Lezama&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11738.pdf&quot;>;SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>; &lt;br />; &lt;em>;Prune Truong*,&lt;strong>; Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05199.pdf&quot;>;MAGVIT: Masked Generative Video Transformer&lt;/a>; &lt;br />; &lt;em>;Lijun Yu*,&lt;strong>; Yong Cheng&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alexander Hauptmann&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf&quot;>;Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.02291.pdf&quot;>;I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification&lt;/a>; &lt;br />; &lt;em>;Muhammad Ferjad Naeem, Gul Zain Khan,&lt;strong>; Yongqin Xian&lt;/strong>;, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool,&lt;strong>; Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12624.pdf&quot;>;Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization&lt;/a>; &lt;br />; &lt;em>;Zifan Wang*,&lt;strong>; Nan Ding&lt;/strong>;, &lt;strong>;Tomer Levinboim&lt;/strong>;, &lt;strong>;Xi Chen&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Su Wang&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Ceslee Montgomery&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Shai Noy&lt;/strong>;, &lt;strong>;Stefano Pellegrini&lt;/strong>;, &lt;strong>;Yasumasa Onoe&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;William Cha&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14306.pdf&quot;>;RUST: Latent Neural Scene Representations from Unposed Imagery&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mehdi SM Sajjadi&lt;/strong>;, &lt;strong>;Aravindh Mahendran&lt;/strong>;, &lt;strong>;Thomas Kipf&lt;/strong>;, &lt;strong>;Etienne Pot&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Mario Lučić&lt;/strong>;, &lt;strong>;Klaus Greff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05221.pdf&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-training with Multi-Source Multimodal Knowledge Memory&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Ziniu Hu*,&lt;strong>; Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Zirui Wang&lt;/strong>;, Kai-Wei Chang,&lt;strong>; Yizhou Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.00833.pdf&quot;>;RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Sara Sabour&lt;/strong>;, &lt;strong>;Suhani Vora&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Ivan Krasin&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09682.pdf&quot;>;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>; &lt;br />; &lt;em>;Yifan Jiang*, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, Dejia Xu, &lt;strong>;Jonathan T. Barron&lt;/strong>;, Zhangyang Wang, Tianfan Xue*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.pdf&quot;>;BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>; &lt;br />; &lt;em>;Kacper Kania, Stephan Garbin, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00583.pdf&quot;>;Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints&lt;/a>; &lt;br />; &lt;em>;Guilherme Potje, Felipe Cadar, &lt;strong>;Andre Araujo&lt;/strong>;, Renato Martins, Erickson Nascimento&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf&quot;>;How Can Objects Help Action Recognition?&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.12652.pdf&quot;>;Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>; &lt;br />; &lt;em>;Peng Dai, &lt;strong>;Yinda Zhang&lt;/strong>;, Xin Yu, Xiaoyang Lyu, Xiaojuan Qi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14396.pdf&quot;>;IFSeg: Image-Free Semantic Segmentation via Vision-Language Model&lt;/a>; &lt;br />; &lt;em>;Sukmin Yun, Seong Park, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-Aware Saliency Modeling&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Shi Chen*, &lt;strong>;Nachiappan Valliappan&lt;/strong>;, &lt;strong>;Shaolei Shen&lt;/strong>;, &lt;strong>;Xinyu Ye&lt;/strong>;, &lt;strong>;Kai Kohlhoff&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09117.pdf&quot;>;MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis&lt;/a>; &lt;br />; &lt;em>;Tianhong Li*,&lt;strong>; Huiwen Chang&lt;/strong>;, Shlok Kumar Mishra,&lt;strong>; Han Zhang&lt;/strong>;, Dina Katabi,&lt;strong>; Dilip Krishnan&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17603.pdf&quot;>;NeRF-Supervised Deep Stereo&lt;/a>; &lt;br />; &lt;em>;Fabio Tosi, &lt;strong>;Alessio Tonioni&lt;/strong>;, Daniele Gregorio, Matteo Poggi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf&quot;>;Omnimatte3D: Associating Objects and their Effects in Unconstrained Monocular Video&lt;/a>; &lt;br />; &lt;em>;Mohammed Suhail, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, Leon Sigal, &lt;strong>;Forrester Cole&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15654.pdf&quot;>;OpenScene: 3D Scene Understanding with Open Vocabularies&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Songyou Peng&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;, &lt;strong>;Chiyu Jiang&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Marc Pollefeys&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08504.pdf&quot;>;PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>; &lt;br />; &lt;em>;Chung-Yi Weng,&lt;strong>; Pratul Srinivasan&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, Kate Saenko, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;AJ Piergiovanni&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01194.pdf&quot;>;Burstormer: Burst Image Restoration and Enhancement Transformer&lt;/a>; &lt;br />; &lt;em>;Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15774.pdf&quot;>;Decentralized Learning with Multi-Headed Distillation&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Andrey Zhmoginov&lt;/strong>;, &lt;strong>;Mark Sandler&lt;/strong>;, &lt;strong>;Nolan Miller&lt;/strong>;, &lt;strong>;Gus Kristiansen&lt;/strong>;, &lt;strong>;Max Vladymyrov&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02163.pdf&quot;>;GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>; &lt;br />; &lt;em>;Bokui Shen, Xinchen Yan, Charles R. Qi, Mahyar Najibi, Boyang Deng,&lt;strong>; Leonidas Guibas&lt;/strong>;, Yin Zhou, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11846.pdf&quot;>;Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions&lt;/a>; &lt;br />; &lt;em>;Yun He, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;, Xiangyang Xue, Yanwei Fu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.11042.pdf&quot;>;Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble&lt;/a>; &lt;br />; &lt;em>;Chun-Han Yao*, Wei-Chih Hung, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.00653.pdf&quot;>;Hyperbolic Contrastive Learning for Visual Representations beyond Objects&lt;/a>; &lt;br />; &lt;em>;Songwei Ge, Shlok Mishra,&lt;strong>; Simon Kornblith&lt;/strong>;, &lt;strong>;Chun-Liang Li, &lt;/strong>;David Jacobs&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09276.pdf&quot;>;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Bahjat Kawar*,&lt;strong>; Shiran Zada&lt;/strong>;, &lt;strong>;Oran Lang&lt;/strong>;, &lt;strong>;Omer Tov&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Tali Dekel&lt;/strong>;, &lt;strong>;Inbar Mosseri&lt;/strong>;, &lt;strong>;Michal Irani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02743.pdf&quot;>;Incremental 3D Semantic Scene Graph Prediction from RGB Sequences&lt;/a>; &lt;br />; &lt;em>;Shun-Cheng Wu, &lt;strong>;Keisuke Tateno&lt;/strong>;, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00575.pdf&quot;>;IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction&lt;/a>; &lt;br />; &lt;em>;Dekai Zhu, Guangyao Zhai, Yan Di, &lt;strong>;Fabian Manhardt&lt;/strong>;, Hendrik Berkemeyer, Tuan Tran, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.10844.pdf&quot;>;Learning to Generate Image Embeddings with User-Level Differential Privacy&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zheng Xu, Maxwell Collins, Yuxiao Wang, Liviu Panait, Sewoong Oh, Sean Augenstein, Ting Liu, Florian Schroff, H. Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05866.pdf&quot;>;NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs&lt;/a>; &lt;br />; &lt;em>;Harsh Rangwani, Lavish Bansal, Kartik Sharma,&lt;strong>; Tejan Karmali&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, Venkatesh Babu Radhakrishnan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09794.pdf&quot;>;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Ron Mokady*, Amir Hertz*, &lt;strong>;Kfir Aberman&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, Daniel Cohen-Or*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14020.pdf&quot;>;SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow&lt;/a>; &lt;br />; &lt;em>;Itai Lang*,&lt;strong>; Dror Aiger&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Shai Avidan&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11674.pdf&quot;>;Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>; &lt;br />; &lt;em>;Dario Pavllo*,&lt;strong>; David Joseph Tan&lt;/strong>;, &lt;strong>;Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12902.pdf&quot;>;TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Hanzhi Chen, &lt;strong>;Fabian Manhardt&lt;/strong>;, Nassir Navab, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf&quot;>;TryOnDiffusion: A Tale of Two UNets&lt;/a>; &lt;br />; &lt;em>;Luyang Zhu*, &lt;strong>;Dawei Yang&lt;/strong>;, &lt;strong>;Tyler Zhu&lt;/strong>;, &lt;strong>;Fitsum Reda&lt;/strong>;, &lt;strong>;William Chan&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03112.pdf&quot;>;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a>; &lt;br />; &lt;em>;Aishwarya Kamath*, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;Su Wang&lt;/strong>;, Jing Yu Koh*, &lt;strong>;Alexander Ku&lt;/strong>;, &lt;strong>;Austin Waters&lt;/strong>;, Yinfei Yang*, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Zarana Parekh&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08045.pdf&quot;>;CLIPPO: Image-and-Language Understanding from Pixels Only&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Basil Mustafa&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.04745.pdf&quot;>;Controllable Light Diffusion for Portraits&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;David Futschik&lt;/strong>;, &lt;strong>;Kelvin Ritland&lt;/strong>;, &lt;strong>;James Vecore&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Daniel Sýkora&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf&quot;>;CUF: Continuous Upsampling Filters&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Cristina Vasconcelos&lt;/strong>;, &lt;strong>;Cengiz Oztireli&lt;/strong>;, &lt;strong>;Mark Matthews&lt;/strong>;, &lt;strong>;Milad Hashemi&lt;/strong>;, &lt;strong>;Kevin Swersky&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01758.pdf&quot;>;Improving Zero-Shot Generalization and Robustness of Multi-modal Models&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge*, &lt;strong>;Jie Ren&lt;/strong>;, &lt;strong>;Andrew Gallagher&lt;/strong>;, &lt;strong>;Yuxiao Wang&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>;, &lt;strong>;Laurent Itti&lt;/strong>;, &lt;strong>;Balaji Lakshminarayanan&lt;/strong>;, &lt;strong>;Jiaping Zhao&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09665.pdf&quot;>;LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding&lt;/a>; &lt;br />; &lt;em>;Gen Li, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, Laura Sevilla-Lara&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03361.pdf&quot;>;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xiaoshuai Zhang&lt;/strong>;, &lt;strong>;Abhijit Kundu&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Hao Su&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01762.pdf&quot;>;Self-Supervised AutoFlow&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hsin-Ping Huang&lt;/strong>;, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf&quot;>;Train-Once-for-All Personalization&lt;/a>; &lt;br />; &lt;em>;Hong-You Chen*,&lt;strong>; Yandong Li&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Mingda Zhang&lt;/strong>;, Wei-Lun Chao,&lt;strong>; Li Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.14115.pdf&quot;>;Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Antoine Yang*, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Antoine Miech, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, Ivan Laptev, Josef Sivic, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14302.pdf&quot;>;VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Junjie Ke&lt;/strong>;, &lt;strong>;Keren Ye&lt;/strong>;, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Yonghui Wu&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Feng Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11152.pdf&quot;>;You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model&lt;/a>; &lt;br />; &lt;em>;Shengkun Tang, &lt;strong>;Yaqing Wang&lt;/strong>;, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, &lt;strong>;Yi Liang&lt;/strong>;, Dongkuan Xu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.05211.pdf&quot;>;Accidental Light Probes&lt;/a>; &lt;br />; &lt;em>;Hong-Xing Yu, Samir Agarwala, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;, &lt;strong>;Noah Snavely, &lt;/strong>;Jiajun Wu, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.09653.pdf&quot;>;FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning&lt;/a>; &lt;br />; &lt;em>;Yuanhao Xiong, Ruochen Wang, Minhao Cheng,&lt;strong>; Felix Yu&lt;/strong>;, Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08013.pdf&quot;>;FlexiViT: One Model for All Patch Sizes&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Lucas Beyer&lt;/strong>;, &lt;strong>;Pavel Izmailov&lt;/strong>;, &lt;strong>;Alexander Kolesnikov&lt;/strong>;, &lt;strong>;Mathilde Caron&lt;/strong>;, &lt;strong>;Simon Kornblith&lt;/strong>;, &lt;strong>;Xiaohua Zhai&lt;/strong>;, &lt;strong>;Matthias Minderer&lt;/strong>;, &lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Ibrahim Alabdulmohsin&lt;/strong>;, &lt;strong>;Filip Pavetic&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03087.pdf&quot;>;Iterative Vision-and-Language Navigation&lt;/a>; &lt;br />; &lt;em>;Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso,&lt;strong>; Peter Anderson&lt;/strong>;, Stefan Lee, Jesse Thomason&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.08010.pdf&quot;>;MoDi: Unconditional Motion Synthesis from Diverse Data&lt;/a>; &lt;br />; &lt;em>;Sigal Raab, Inbal Leibovitch, Peizhuo Li,&lt;strong>; Kfir Aberman&lt;/strong>;, Olga Sorkine-Hornung, Daniel Cohen-Or&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03369.pdf&quot;>;Multimodal Prompting with Missing Modalities for Visual Recognition&lt;/a>; &lt;br />; &lt;em>;Yi-Lun Lee,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>;, Wei-Chen Chiu,&lt;strong>; Chen-Yu Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf&quot;>;Scene-Aware Egocentric 3D Human Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu,&lt;strong>; Kripasindhu Sarkar&lt;/strong>;, Christian Theobalt&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06247.pdf&quot;>;ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-Based Consistency&lt;/a>; &lt;br />; &lt;em>;Zixuan Huang,&lt;strong>; Varun Jampani&lt;/strong>;, Ngoc Anh Thai,&lt;strong>; Yuanzhen Li&lt;/strong>;, Stefan Stojanov, James M. Rehg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05173.pdf&quot;>;Improving Image Recognition by Retrieving from Web-Scale Image-Text Data&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00341.pdf&quot;>;JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>; &lt;br />; &lt;em>;Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, Li Yi,&lt;strong>; Leonidas Guibas&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01436.pdf&quot;>;Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos&lt;/a>; &lt;br />; &lt;em>;Ziqian Bai*,&lt;strong>; Feitong Tan&lt;/strong>;, &lt;strong>;Zeng Huang&lt;/strong>;, &lt;strong>;Kripasindhu Sarkar&lt;/strong>;, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Di Qiu&lt;/strong>;, &lt;strong>;Abhimitra Meka&lt;/strong>;, &lt;strong>;Ruofei Du&lt;/strong>;, &lt;strong>;Mingsong Dou&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;, Ping Tan,&lt;strong>; Thabo Beeler&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.08556.pdf&quot;>;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>; &lt;br />; &lt;em>;Allan Zhou, Mo Jin Kim, Lirui Wang,&lt;strong>; Pete Florence&lt;/strong>;, &lt;strong>;Chelsea Finn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03084.pdf&quot;>;Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*,&lt;strong>; Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13582.pdf&quot;>;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mikaela Uy&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Ke Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06820.pdf&quot;>;Structured 3D Features for Reconstructing Controllable Avatars&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Enric Corona&lt;/strong>;, &lt;strong>;Mihai Zanfir&lt;/strong>;, &lt;strong>;Thiemo Alldieck&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Andrei Zanfir&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09119.pdf&quot;>;Token Turing Machines&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael S. Ryoo&lt;/strong>;, &lt;strong>;Keerthana Gopalakrishnan&lt;/strong>;, &lt;strong>;Kumara Kahatapitiya&lt;/strong>;, &lt;strong>;Ted Xiao&lt;/strong>;, &lt;strong>;Kanishka Rao&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Yao Lu&lt;/strong>;, &lt;strong>;Julian Ibarz&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10957.pdf&quot;>;TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization&lt;/a>; &lt;br />; &lt;em>;Fabrizio Guillaro, Davide Cozzolino,&lt;strong>; Avneesh Sud&lt;/strong>;, &lt;strong>;Nicholas Dufour, &lt;/strong>;Luisa Verdoliva&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.07685.pdf&quot;>;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/a>; &lt;br />; &lt;em>;Sihyun Yu,&lt;strong>; Kihyuk Sohn, &lt;/strong>;Subin Kim, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00990.pdf&quot;>;Visual Prompt Tuning for Generative Transfer Learning&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Luisa Polania&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17811.pdf&quot;>;Zero-Shot Referring Image Segmentation with Global-Local Context Features&lt;/a>; &lt;br />; &lt;em>;Seonghoon Yu,&lt;strong>; Paul Hongsuck Seo&lt;/strong>;, Jeany Son&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.03285.pdf&quot;>;DC2: Dual-Camera Defocus Control by Learning to Refocus&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hadi Alzayer&lt;/strong>;, &lt;strong>;Abdullah Abuolaim&lt;/strong>;, &lt;strong>;Leung Chun Chan&lt;/strong>;, &lt;strong>;Yang Yang&lt;/strong>;, &lt;strong>;Ying Chen Lou&lt;/strong>;, &lt;strong>;Jia-Bin Huang&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf&quot;>;Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision&lt;/a>; &lt;br />; &lt;em>;Aditay Tripathi*,&lt;strong>; Rishubh Singh&lt;/strong>;, Anirban Chakraborty,&lt;strong>; Pradeep Shenoy&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09898.pdf&quot;>;MetaCLUE: Towards Comprehensive Visual Metaphors Research&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arjun R. Akula&lt;/strong>;, &lt;strong>;Brendan Driscoll&lt;/strong>;, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Zhiwei Jia&lt;/strong>;, &lt;strong>;Suyash Damle&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;William T. Freeman&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13824.pdf&quot;>;Multi-Realism Image Compression with a Conditional Generator&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Eirikur Agustsson&lt;/strong>;, &lt;strong>;David Minnen&lt;/strong>;, &lt;strong>;George Toderici&lt;/strong>;, &lt;strong>;Fabian Mentzer&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.03267.pdf&quot;>;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>; &lt;br />; &lt;em>;Congyue Deng, Chiyu Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou,&lt;strong>; Leonidas Guibas&lt;/strong>;, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12053.pdf&quot;>;On Calibrating Semantic Segmentation Models: Analyses and an Algorithm&lt;/a>; &lt;br />; &lt;em>;Dongdong Wang,&lt;strong>; Boqing Gong&lt;/strong>;, Liqiang Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13515.pdf&quot;>;Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>; &lt;br />; &lt;em>;Lucy Chai,&lt;strong>; Richard Tucker&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, Phillip Isola,&lt;strong>; Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13662.pdf&quot;>;Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment&lt;/a>; &lt;br />; &lt;em>;Yiyou Sun*,&lt;strong>; Yaojie Liu&lt;/strong>;, &lt;strong>;Xiaoming Liu&lt;/strong>;, Yixuan Li,&lt;strong>; Wen-Sheng Chu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13277.pdf&quot;>;SINE: Semantic-Driven Image-Based NeRF Editing with Prior-Guided Editing Field&lt;/a>; &lt;br />; &lt;em>;Chong Bao,&lt;strong>; Yinda Zhang&lt;/strong>;, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15533.pdf&quot;>;Sequential Training of GANs Against GAN-Classifiers Reveals Correlated &quot;Knowledge Gaps&quot; Present Among Independently Trained GAN Instances&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arkanath Pathak&lt;/strong>;, &lt;strong>;Nicholas Dufour&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.16991.pdf&quot;>;SparsePose: Sparse-View Camera Pose Regression and Refinement&lt;/a>; &lt;br />; &lt;em>;Samarth Sinha, Jason Zhang,&lt;strong>; Andrea Tagliasacchi&lt;/strong>;, Igor Gilitschenski, David Lindell&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models&lt;/a>; &lt;br />; &lt;em>;Yushi Yao,&lt;strong>; Chang Ye&lt;/strong>;, &lt;strong>;Gamaleldin F. Elsayed&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://cv4mr.github.io/&quot;>;Computer Vision for Mixed Reality&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvpr2023.wad.vision/&quot;>;Workshop on Autonomous Driving (WAD)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://multimodal-content-moderation.github.io/&quot;>;Multimodal Content Moderation (MMCM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Chris Bregler&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mevan Babakar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mcv-workshop.github.io/#updates&quot;>;Medical Computer Vision (MCV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Shekoofeh Azizi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vand-cvpr23/home&quot;>;VAND: Visual Anomaly and Novelty Detection&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Jie Ren&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://struco3d.github.io/cvpr2023/&quot;>;Structural and Compositional Learning on 3D Data&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Fei Xia&lt;/strong>;, &lt;strong>;Amir Hertz&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgvc10&quot;>;Fine-Grained Visual Categorization (FGVC10)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Kimberly Wilber&lt;/strong>;, &lt;strong>;Sara Beery&lt;/strong>;&lt;/em>; &lt;br />; Panelists include: &lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/xrnerf/&quot;>;XRNeRF: Advances in NeRF for the Metaverse&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnilabel-workshop-cvpr23/overview&quot;>;OmniLabel: Infinite Label Spaces for Semantic Understanding via Natural Language&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Long Zhao&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://holistic-video-understanding.github.io/workshops/cvpr2023.html&quot;>;Large Scale Holistic Video Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;David Ross&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nice.lgresearch.ai/&quot;>;New Frontiers for Zero-Shot Image Captioning Evaluation (NICE)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ccd2023.github.io/&quot;>;Computational Cameras and Displays (CCD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ulugbek Kamilov&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mauricio Delbracio&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gazeworkshop.github.io/2023/&quot;>;Gaze Estimation and Prediction in the Wild (GAZE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Thabo Beele&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Erroll Wood&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgahi2023/home&quot;>;Face and Gesture Analysis for Health Informatics (FGAHI)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.cv4animals.com/&quot;>;Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;3D Vision and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation (E2EAD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opendrivelab.com/e2ead/cvpr23&quot;>;End-to-End Autonomous Driving: Emerging Tasks and Challenges&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mula-workshop.github.io/&quot;>;Multi-modal Learning and Applications (MULA)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Aleksander Hołyński&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sdas2023/&quot;>;Synthetic Data for Autonomous Systems (SDAS)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vdu-cvpr23&quot;>;Vision Datasets Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;José Lezama&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vijay Janapa Reddi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ieeecvf-cvpr2023-precognition/&quot;>;Precognition: Seeing Through the Future&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Utsav Prabhu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvlai.net/ntire/2023/&quot;>;New Trends in Image Restoration and Enhancement (NTIRE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://generative-vision.github.io/workshop-CVPR-23/&quot;>;Generative Models for Computer Vision&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://robustart.github.io/&quot;>;Adversarial Machine Learning on Computer Vision: Art of Robustness&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Xinyun Chen&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/wmf2023/home&quot;>;Media Forensics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Nicholas Carlini&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://taodataset.org/workshop/cvpr23/&quot;>;Tracking and Its Many Guises: Tracking Any Object in Open-World&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Paul Voigtlaender&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://scene-understanding.com/&quot;>;3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.es.ele.tue.nl/cvpm23/&quot;>;Computer Vision for Physiological Measurement (CVPM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/&quot;>;Affective Behaviour Analysis In-the-Wild&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision (EC3V)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Rida Qadri&lt;/strong>;, &lt;strong>;Mohammad Havaei&lt;/strong>;, &lt;strong>;Fernando Diaz&lt;/strong>;, &lt;strong>;Emily Denton&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;Negar Rostamzadeh&lt;/strong>;, &lt;strong>;Pamela Peter-Agbia&lt;/strong>;, &lt;strong>;Eva Kozanecka&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vizwiz.org/workshops/2023-workshop/&quot;>;VizWiz Grand Challenge: Describing Images and Videos Taken by Blind People&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Haoran Qi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ecv23/home&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;blog post&lt;/a>;) &lt;br />; Organizers include: &lt;em>;&lt;strong>;Andrew Howard&lt;/strong>;, &lt;strong>;Chas Leichner&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andrew Howard&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vcdw2023/&quot;>;Visual Copy Detection&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Priya Goyal&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dmv2023.github.io/&quot;>;Learning 3D with Multi-View Supervision (3DMV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://image-matching-workshop.github.io/&quot;>;Image Matching: Local Features and Beyond&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vision4allseason.net/&quot;>;Vision for All Seasons: Adverse Weather and Lightning Conditions (V4AS)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/t4v-cvpr23&quot;>;Transformers for Vision (T4V)&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/academic-cv/&quot;>;Scholars vs Big Models — How Can Academics Adapt?&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Jonathan T. Barron&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://www.scan-net.org/cvpr2023workshop/&quot;>;ScanNet Indoor Scene Understanding Challenge&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Tom Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvmi-workshop.github.io/new.html&quot;>;Computer Vision for Microscopy Image Analysis&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Po-Hsuan Cameron Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://embeddedvisionworkshop.wordpress.com/&quot;>;Embedded Vision&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sightsound.org/&quot;>;Sight and Sound&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;William Freeman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4cc.net/&quot;>;AI for Content Creation&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Tim Salimans&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://computer-vision-in-the-wild.github.io/cvpr-2023/https://computer-vision-in-the-wild.github.io/cvpr-2023/&quot;>;Computer Vision in the Wild&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Xiuye Gu&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vispr-workshop.github.io/&quot;>;Visual Pre-training for Robotics&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnicv2023/home&quot;>;Omnidirectional Computer Vision&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://all-things-vits.github.io/atv/&quot;>;All Things ViTs: Understanding and Interpreting Attention in Vision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hila Chefer&lt;/strong>;, &lt;strong>;Sayak Paul&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-tutorial-on-ad/&quot;>;Recent Advances in Anomaly Detection&lt;/a>; &lt;br />; &lt;em>;Guansong Pang, Joey Tianyi Zhou, Radu Tudor Ionescu, Yu Tian,&lt;strong>; Kihyuk Sohn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr-tutorial-2023/home&quot;>;Contactless Healthcare Using Cameras and Wireless Sensors&lt;/a>; &lt;br />; &lt;em>;Wenjin Wang, Xuyu Wang, Jun Luo,&lt;strong>; Daniel McDuff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://osimeoni.github.io/object-localization-for-free/&quot;>;Object Localization for Free: Going Beyond Self-Supervised Learning&lt;/a>; &lt;br />; &lt;em>;Oriane Simeoni,&lt;strong>; &lt;/strong>;Weidi Xie,&lt;strong>; Thomas Kipf&lt;/strong>;, Patrick Pérez&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://prompting-in-vision.github.io/&quot;>;Prompting in Vision&lt;/a>; &lt;br />; &lt;em>;Kaiyang Zhou, Ziwei Liu, Phillip Isola, Hyojin Bahng, Ludwig Schmidt, Sarah Pratt,&lt;strong>; Denny Zhou&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9139300663122353070/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CVPR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s72-c/CVPR%20Design-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2312998356111901143&lt;/id>;&lt;published>;2023-06-15T13:53:00.000-07:00&lt;/published>;&lt;updated>;2023-06-15T13:53:27.418-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Juhyun Lee and Raman Sarokin, Software Engineers, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s700/SpeedHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The proliferation of large &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>; for &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;image generation&lt;/a>; has led to a significant increase in model size and inference workloads. On-device ML inference in mobile environments requires meticulous performance optimization and consideration of trade-offs due to resource constraints. Running inference of large diffusion models (LDMs) on-device, driven by the need for cost efficiency and user privacy, presents even greater challenges due to the substantial memory requirements and computational demands of these models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We address this challenge in our work titled “&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations&lt;/a>;” (to be presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; workshop for &lt;a href=&quot;https://sites.google.com/corp/view/ecv23&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>;) focusing on the optimized execution of a foundational LDM model on a mobile GPU. In this blog post, we summarize the core techniques we employed to successfully execute large diffusion models like &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; at full resolution (512x512 pixels) and 20 iterations on modern smartphones with high-performing inference speed of the original model without distillation of under 12 seconds. As discussed in &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;our previous blog post&lt;/a>;, GPU-accelerated ML inference is often limited by memory performance, and execution of LDMs is no exception. Therefore, the central theme of our optimization is efficient memory input/output (I/O) even if it means choosing memory-efficient algorithms over those that prioritize arithmetic logic unit efficiency. Ultimately, our primary objective is to reduce the overall latency of the ML inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s512/image4.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A sample output of an LDM on Mobile GPU with the prompt text: “a photo realistic and high resolution image of a cute puppy with surrounding flowers”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enhanced attention module for memory efficiency&lt;/h2>; &lt;p>; An ML inference engine typically provides a variety of optimized ML operations. Despite this, achieving optimal performance can still be challenging as there is a certain amount of overhead for executing individual neural net operators on a GPU. To mitigate this overhead, ML inference engines incorporate extensive operator fusion rules that consolidate multiple operators into a single operator, thereby reducing the number of iterations across tensor elements while maximizing compute per iteration. For instance, &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; utilizes operator fusion to combine computationally expensive operations, like convolutions, with subsequent activation functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;rectified linear units&lt;/a>;, into one. &lt;/p>; &lt;p>; A clear opportunity for optimization is the heavily used attention block adopted in the &lt;a href=&quot;https://arxiv.org/pdf/2304.11267.pdf&quot;>;denoiser model&lt;/a>; in the LDM. The attention blocks allow the model to focus on specific parts of the input by assigning higher weights to important regions. There are multiple ways one can optimize the attention modules, and we selectively employ one of the two optimizations explained below depending on which optimization performs better. &lt;/p>; &lt;p>; The first optimization, which we call &lt;em>;partially fused &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;>;softmax&lt;/a>;&lt;/em>;, removes the need for extensive memory writes and reads between the softmax and the matrix multiplication in the attention module. Let the attention block be just a simple matrix multiplication of the form &lt;b>;&lt;em>;Y &lt;/em>;&lt;/b>;= softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) * &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; where &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; are 2D matrices of shape &lt;em>;a&lt;/em>;×&lt;em>;b&lt;/em>; and &lt;em>;b&lt;/em>;×&lt;em>;c&lt;/em>;, respectively (shown below in the top half). &lt;/p>; &lt;p>; For numerical stability, &lt;b>;&lt;em>;T = &lt;/em>;&lt;/b>;softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) is typically calculated in three passes: &lt;/p>; &lt;ol>; &lt;li>;Determine the maximum value in the list, &lt;em>;ie&lt;/em>;.,&lt;em>; &lt;/em>;for each row in matrix &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; &lt;/li>;&lt;li>;Sum up the differences of the exponential of each list item and the maximum value (from pass 1) &lt;/li>;&lt;li>;Divide the exponential of the items minus the maximum value by the sum from pass 2 &lt;/li>; &lt;/ol>; &lt;p>; Carrying out these passes naïvely would result in a huge memory write for the temporary intermediate tensor &lt;strong>;&lt;em>;T&lt;/em>;&lt;/strong>; holding the output of the entire softmax function. We bypass this large memory write if we only store the results of passes 1 and 2, labeled &lt;b>;&lt;em>;m&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;s&lt;/em>;&lt;/b>;, respectively, which are small vectors, with &lt;em>;a&lt;/em>; elements each, compared to &lt;strong>;&lt;em>;T &lt;/em>;&lt;/strong>;which has &lt;em>;a·b &lt;/em>;elements. With this technique, we are able to reduce tens or even hundreds of megabytes of memory consumption by multiple orders of magnitude (shown below in the bottom half). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s568/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;457&quot; data-original-width=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Attention modules. &lt;b>;Top&lt;/b>;: A naïve attention block, composed of a SOFTMAX (with all three passes) and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;MATMUL&lt;/a>;, requires a large memory write for the big intermediate tensor &lt;em>;T&lt;/em>;. &lt;b>;Bottom&lt;/b>;: Our memory-efficient attention block with partially fused softmax in MATMUL only needs to store two small intermediate tensors for &lt;em>;m&lt;/em>; and s.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The other optimization involves employing &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;>;FlashAttention&lt;/a>;, which is an I/O-aware, exact attention algorithm. This algorithm reduces the number of GPU high-bandwidth memory accesses, making it a good fit for our memory bandwidth–limited use case. However, we found this technique to only work for &lt;a href=&quot;https://en.wikipedia.org/wiki/Static_random-access_memory&quot;>;SRAM&lt;/a>; with certain sizes and to require a large number of registers. Therefore, we only leverage this technique for attention matrices with a certain size on a select set of GPUs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Winograd fast convolution for 3×3 convolution layers&lt;/h2>; &lt;p>; The backbone of common LDMs heavily relies on 3×3 convolution layers (convolutions with filter size 3×3), comprising over 90% of the layers in the decoder. Despite increased memory consumption and numerical errors, we found that &lt;a href=&quot;https://arxiv.org/abs/1509.09308&quot;>;Winograd fast convolution&lt;/a>; to be effective at speeding up the convolutions. Distinct from the &lt;em>;filter size&lt;/em>; 3x3 used in convolutions, &lt;em>;tile size&lt;/em>; refers to the size of a sub region of the input tensor that is processed at a time. Increasing the tile size enhances the efficiency of the convolution in terms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_logic_unit&quot;>;arithmetic logic unit&lt;/a>; (ALU) usage. However, this improvement comes at the expense of increased memory consumption. Our tests indicate that a tile size of 4×4 achieves the optimal trade-off between computational efficiency and memory utilization. &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;/td>; &lt;td>;&lt;/td>; &lt;td colspan=&quot;2&quot;>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Memory usage&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tile size&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; savings&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Intermediate tensors&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Weights&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;2×2 &lt;/td>; &lt;td>;2.25× &lt;/td>; &lt;td>;4.00× &lt;/td>; &lt;td>;1.77× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;4×4&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;2.25×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;6×6 &lt;/td>; &lt;td>;5.06× &lt;/td>; &lt;td>;1.80× &lt;/td>; &lt;td>;7.12× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;8×8 &lt;/td>; &lt;td>;5.76× &lt;/td>; &lt;td>;1.56× &lt;/td>; &lt;td>;11.1× &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impact of Winograd with varying tile sizes for 3×3 convolutions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Specialized operator fusion for memory efficiency&lt;/h2>; &lt;p>; We discovered that performantly inferring LDMs on a mobile GPU requires significantly larger fusion windows for commonly employed layers and units in LDMs than current off-the-shelf on-device GPU-accelerated ML inference engines provide. Consequently, we developed specialized implementations that could execute a larger range of neural operators than typical fusion rules would permit. Specifically, we focused on two specializations: the &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;>;Gaussian Error Linear Unit&lt;/a>; (GELU) and the &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;>;group normalization&lt;/a>; layer. &lt;/p>; &lt;p>; An approximation of GELU with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions&quot;>;hyperbolic tangent&lt;/a>; function requires writing to and reading from seven auxiliary intermediate tensors (shown below as light orange rounded rectangles in the figure below), reading from the input tensor &lt;em>;x&lt;/em>; three times, and writing to the output tensor &lt;em>;y&lt;/em>; once across eight GPU programs implementing the labeled operation each (light blue rectangles). A custom GELU implementation that performs the eight operations in a single shader (shown below in the bottom) can bypass all the memory I/O for the intermediate tensors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s958/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;210&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GELU implementations. &lt;strong>;Top&lt;/strong>;: A naïve implementation with built-in operations would require 8 memory writes and 10 reads. &lt;strong>;Bottom&lt;/strong>;: Our custom GELU only requires 1 memory read (for &lt;em>;x&lt;/em>;) and 1 write (for &lt;em>;y&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; After applying all of these optimizations, we conducted tests of Stable Diffusion 1.5 (image resolution 512x512, 20 iterations) on high-end mobile devices. Running Stable Diffusion with our GPU-accelerated ML inference model uses 2,093MB for the weights and 84MB for the intermediate tensors. With latest high-end smartphones, Stable Diffusion can be run in under 12 seconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s522/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stable Diffusion runs on modern smartphones in under 12 seconds. Note that running the decoder after each iteration for displaying the intermediate output in this animated GIF results in a ~2× slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Performing on-device ML inference of large models has proven to be a substantial challenge, encompassing limitations in model file size, extensive runtime memory requirements, and protracted inference latency. By recognizing memory bandwidth usage as the primary bottleneck, we directed our efforts towards optimizing memory bandwidth utilization and striking a delicate balance between ALU efficiency and memory efficiency. As a result, we achieved state-of-the-art inference latency for large diffusion models. You can learn more about this work in &lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank Yu-Hui Chen, Jiuqiang Tang, Frank Barchard, Yang Zhao, Joe Zou, Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, Lu Wang, and Matthias Grundmann.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2312998356111901143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot; rel=&quot;alternate&quot; title=&quot;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s72-c/SpeedHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3072875289355194363&lt;/id>;&lt;published>;2023-06-14T09:00:00.001-07:00&lt;/published>;&lt;updated>;2023-06-22T14:55:31.687-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Reconstructing indoor spaces with NeRF&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Marcos Seefelder, Software Engineer, and Daniel Duckworth, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s320/Immersive%20View%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; When choosing a venue, we often find ourselves with questions like the following: Does this restaurant have the right vibe for a date? Is there good outdoor seating? Are there enough screens to watch the game? While photos and videos may partially answer questions like these, they are no substitute for feeling like you&#39;re there, even when visiting in person isn&#39;t an option. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Immersive experiences that are interactive, photorealistic, and multi-dimensional stand to bridge this gap and recreate the feel and vibe of a space, empowering users to naturally and intuitively find the information they need. To help with this, Google Maps launched &lt;a href=&quot;https://blog.google/products/maps/google-maps-updates-immersive-view-trip-planning&quot;>;Immersive View&lt;/a>;, which uses advances in machine learning (ML) and computer vision to fuse billions of &lt;a href=&quot;https://www.google.com/streetview/&quot;>;Street View&lt;/a>; and aerial images to create a rich, digital model of the world. Beyond that, it layers helpful information on top, like the weather, traffic, and how busy a place is. Immersive View provides indoor views of restaurants, cafes, and other venues to give users a virtual up-close look that can help them confidently decide where to go. &lt;/p>; &lt;p>; Today we describe the work put into delivering these indoor views in Immersive View. We build on &lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;>;neural radiance fields&lt;/a>; (NeRF), a state-of-the-art approach for fusing photos to produce a realistic, multi-dimensional reconstruction within a neural network. We describe our pipeline for creation of NeRFs, which includes custom photo capture of the space using DSLR cameras, image processing and scene reproduction. We take advantage of Alphabet&#39;s &lt;a href=&quot;https://arxiv.org/abs/2008.02268&quot;>;recent&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2111.12077&quot;>;advances&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2202.05263&quot;>;in the field&lt;/a>; to design a method matching or outperforming the prior state-of-the-art in visual fidelity. These models are then embedded as interactive 360° videos following curated flight paths, enabling them to be available on smartphones. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/introduction_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The reconstruction of The Seafood Bar in Amsterdam in Immersive View.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;From photos to NeRFs&lt;/h2>; &lt;p>; At the core of our work is &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;NeRF&lt;/a>;, a recently-developed method for 3D reconstruction and novel view synthesis. Given a collection of photos describing a scene, NeRF distills these photos into a &lt;a href=&quot;https://arxiv.org/abs/2111.11426&quot;>;neural field&lt;/a>;, which can then be used to render photos from viewpoints not present in the original collection. &lt;/p>; &lt;p>; While NeRF largely solves the challenge of reconstruction, a user-facing product based on real-world data brings a wide variety of challenges to the table. For example, reconstruction quality and user experience should remain consistent across venues, from dimly-lit bars to sidewalk cafes to hotel restaurants. At the same time, privacy should be respected and any potentially personally identifiable information should be removed. Importantly, scenes should be captured consistently and efficiently, reliably resulting in high-quality reconstructions while minimizing the effort needed to capture the necessary photographs. Finally, the same natural experience should be available to all mobile users, regardless of the device on hand. &lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/overview_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Immersive View indoor reconstruction pipeline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Capture &amp;amp; preprocessing&lt;/h2>; &lt;p>; The first step to producing a high-quality NeRF is the careful capture of a scene: a dense collection of photos from which 3D geometry and color can be derived. To obtain the best possible reconstruction quality, every surface should be observed from multiple different directions. The more information a model has about an object&#39;s surface, the better it will be in discovering the object&#39;s shape and the way it interacts with lights. &lt;/p>; &lt;p>; In addition, NeRF models place further assumptions on the camera and the scene itself. For example, most of the camera&#39;s properties, such as white balance and aperture, are assumed to be fixed throughout the capture. Likewise, the scene itself is assumed to be frozen in time: lighting changes and movement should be avoided. This must be balanced with practical concerns, including the time needed for the capture, available lighting, equipment weight, and privacy. In partnership with professional photographers, we developed a strategy for quickly and reliably capturing venue photos using DSLR cameras within only an hour timeframe. This approach has been used for all of our NeRF reconstructions to date. &lt;/p>; &lt;p>; Once the capture is uploaded to our system, processing begins. As photos may inadvertently contain sensitive information, we automatically scan and blur personally identifiable content. We then apply a &lt;a href=&quot;https://en.wikipedia.org/wiki/Structure_from_motion&quot;>;structure-from-motion&lt;/a>; pipeline to solve for each photo&#39;s &lt;a href=&quot;https://www.mathworks.com/help/vision/ug/camera-calibration.html&quot;>;camera parameters&lt;/a>;: its position and orientation relative to other photos, along with lens properties like &lt;a href=&quot;https://en.wikipedia.org/wiki/Focal_length&quot;>;focal length&lt;/a>;. These parameters associate each pixel with a point and a direction in 3D space and constitute a key signal in the NeRF reconstruction process. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeRF reconstruction&lt;/h2>; &lt;p>; Unlike many ML models, a new NeRF model is trained from scratch on each captured location. To obtain the best possible reconstruction quality within a target compute budget, we incorporate features from a variety of published works on NeRF developed at Alphabet. Some of these include: &lt;/p>; &lt;ul>; &lt;li>;We build on &lt;a href=&quot;https://jonbarron.info/mipnerf360/&quot;>;mip-NeRF 360&lt;/a>;, one of the best-performing NeRF models to date. While more computationally intensive than Nvidia&#39;s widely-used &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;Instant NGP&lt;/a>;, we find the mip-NeRF 360 consistently produces fewer artifacts and higher reconstruction quality. &lt;/li>;&lt;li>;We incorporate the low-dimensional generative latent optimization (GLO) vectors introduced in &lt;a href=&quot;https://nerf-w.github.io/&quot;>;NeRF in the Wild&lt;/a>; as an auxiliary input to the model&#39;s radiance network. These are learned real-valued latent vectors that embed appearance information for each image. By assigning each image in its own latent vector, the model can capture phenomena such as lighting changes without resorting to cloudy geometry, a common artifact in casual NeRF captures. &lt;/li>;&lt;li>;We also incorporate exposure conditioning as introduced in &lt;a href=&quot;https://waymo.com/research/block-nerf/&quot;>;Block-NeRF&lt;/a>;. Unlike GLO vectors, which are uninterpretable model parameters, exposure is directly derived from a photo&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Exif&quot;>;metadata&lt;/a>; and fed as an additional input to the model&#39;s radiance network. This offers two major benefits: it opens up the possibility of varying &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;>;ISO&lt;/a>; and provides a method for controlling an image&#39;s brightness at inference time. We find both properties invaluable for capturing and reconstructing dimly-lit venues. &lt;/li>; &lt;/ul>; &lt;p>; We train each NeRF model on TPU or GPU accelerators, which provide different trade-off points. As with all Google products, we continue to search for new ways to improve, from reducing compute requirements to improving reconstruction quality. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/side_by_side_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A side-by-side comparison of our method and a mip-NeRF 360 baseline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A scalable user experience&lt;/h2>; &lt;p>; Once a NeRF is trained, we have the ability to produce new photos of a scene from any viewpoint and camera lens we choose. Our goal is to deliver a meaningful and helpful user experience: not only the reconstructions themselves, but guided, interactive tours that give users the freedom to naturally explore spaces from the comfort of their smartphones. &lt;/p>; &lt;p>; To this end, we designed a controllable 360° video player that emulates flying through an indoor space along a predefined path, allowing the user to freely look around and travel forward or backwards. As the first Google product exploring this new technology, 360° videos were chosen as the format to deliver the generated content for a few reasons. &lt;/p>; &lt;p>; On the technical side, &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;real-time inference&lt;/a>; and &lt;a href=&quot;https://phog.github.io/snerg/&quot;>;baked representations&lt;/a>; are still resource intensive on a per-client basis (either on device or cloud computed), and relying on them would limit the number of users able to access this experience. By using videos, we are able to scale the storage and delivery of videos to all users by taking advantage of the same video management and serving infrastructure used by YouTube. On the operations side, videos give us clearer editorial control over the exploration experience and are easier to inspect for quality in large volumes. &lt;/p>; &lt;p>; While we had considered capturing the space with a 360° camera directly, using a NeRF to reconstruct and render the space has several advantages. A virtual camera can fly anywhere in space, including over obstacles and through windows, and can use any desired camera lens. The camera path can also be edited post-hoc for smoothness and speed, unlike a live recording. A NeRF capture also does not require the use of specialized camera hardware. &lt;/p>; &lt;p>; Our 360° videos are rendered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting&quot;>;ray casting&lt;/a>; through each pixel of a virtual, spherical camera and compositing the visible elements of the scene. Each video follows a smooth path defined by a sequence of keyframe photos taken by the photographer during capture. The position of the camera for each picture is computed during structure-from-motion, and the sequence of pictures is smoothly interpolated into a flight path. &lt;/p>; &lt;p>; To keep speed consistent across different venues, we calibrate the distances for each by capturing pairs of images, each of which is 3 meters apart. By knowing measurements in the space, we scale the generated model, and render all videos at a natural velocity. &lt;/p>; &lt;p>; The final experience is surfaced to the user within &lt;a href=&quot;https://blog.google/products/maps/three-maps-updates-io-2022/&quot;>;Immersive View&lt;/a>;: the user can seamlessly fly into restaurants and other indoor venues and discover the space by flying through the photorealistic 360° videos. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open research questions&lt;/h2>; &lt;p>; We believe that this feature is the first step of many in a journey towards universally accessible, AI-powered, immersive experiences. From a NeRF research perspective, more questions remain open. Some of these include: &lt;/p>; &lt;ol>; &lt;li>;Enhancing reconstructions with scene segmentation, adding semantic information to the scenes that could make scenes, for example, searchable and easier to navigate. &lt;/li>;&lt;li>;Adapting NeRF to outdoor photo collections, in addition to indoor. In doing so, we&#39;d unlock similar experiences to every corner of the world and change how users could experience the outdoor world. &lt;/li>;&lt;li>;Enabling real-time, interactive 3D exploration through neural-rendering on-device. &lt;/li>; &lt;/ol>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/outdoors_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reconstruction of an outdoor scene with a NeRF model trained on Street View panoramas.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; As we continue to grow, we look forward to engaging with and contributing to the community to build the next generation of immersive experiences. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Jon Barron, Julius Beres, Daniel Duckworth, Roman Dudko, Magdalena Filak, Mike Harm, Peter Hedman, Claudio Martella, Ben Mildenhall, Cardin Moffett, Etienne Pot, Konstantinos Rematas, Yves Sallat, Marcos Seefelder, Lilyana Sirakovat, Sven Tresp and Peter Zhizhin.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Also, we&#39;d like to extend our thanks to Luke Barrington, Daniel Filip, Tom Funkhouser, Charles Goran, Pramod Gupta, Santi López, Mario Lučić, Isalo Montacute and Dan Thomasset for valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3072875289355194363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot; rel=&quot;alternate&quot; title=&quot;Reconstructing indoor spaces with NeRF&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s72-c/Immersive%20View%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;