<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-09-08T15:59:21.449-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Android"></category><category term="TPU"></category><category term="Awards"></category><category term="ICML"></category><category term="ML"></category><category term="Structured Data"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Responsible AI"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1277&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-5967099570177693866&lt;/id>;&lt;发布>;2023-09-07T15:03:00.000-07:00&lt;/发布>;&lt;更新>;2023-09- 07T15:03:10.510-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;用于湍流研究的新型计算流体动力学框架&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Shantanu Shahane，软件Athena 团队工程师和研究科学家 Matthias Ihme&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4 r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1。 png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 湍流在环境和工程流体流动中普遍存在，并且在日常生活中经常遇到。更好地了解这些湍流过程可以为各个研究领域提供有价值的见解——改善对大气输送引起的云形成和湍流能量交换引起的野火蔓延的预测，了解河流中沉积物的沉积，以及提高燃烧效率例如，在飞机发动机中减少排放。然而，尽管它很重要，但我们目前的理解和可靠预测此类流量的能力仍然有限。这主要归因于高度混沌的性质以及这些流体流占据的巨大的空间和时间尺度，范围从高端的几米量级的高能大规模运动，其中能量被注入流体流，在低端一直降至微米 (μm)，湍流通过粘性摩擦耗散成热量。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 理解这些湍流的一个强大工具是&lt;a href=&quot;https://en.wikipedia.org/wiki/Direct_numerical_simulation&quot;>;直接数值模拟（DNS），它提供了非定常三维流场的详细表示，而不进行任何近似或简化。更具体地说，这种方法利用具有足够小的网格间距的离散网格来捕获控制系统动力学的基础连续方程（在这种情况下，可变密度&lt;a href=&quot;https://en.wikipedia.org/ wiki/Navier%E2%80%93Stokes_equations&quot;>;纳维-斯托克斯方程&lt;/a>;，它控制所有流体流动动力学）。当网格间距足够小时，离散网格点足以表示真实（连续）方程，而不会损失精度。虽然这很有吸引力，但此类模拟需要大量的计算资源才能捕获如此广泛的空间尺度上的正确流体流动行为。 &lt;/p>; &lt;p>; 必须应用直接数值计算的空间分辨率的实际跨度取决于任务，并由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Reynolds_number&quot;>; 确定雷诺数&lt;/a>;，将惯性力与粘性力进行比较。通常，雷诺数的范围为 10&lt;sup>;2&lt;/sup>; 到 10&lt;sup>;7 &lt;/sup>;（对于大气或星际问题甚至更大）。在 3D 中，所需分辨率的网格大小大致与雷诺数的 4.5 次方成正比！由于这种强烈的缩放依赖性，模拟此类流动通常仅限于具有中等雷诺数的流动状态，并且通常需要访问 &lt;a href=&quot;https://www.top500.org/lists/top500/2023/06/&quot; >;具有数百万个 CPU/GPU 核心的高性能计算系统&lt;/a>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;用于科学计算流体流动的 TensorFlow 模拟框架张量处理单元&lt;/a>;”，我们引入了一种新的模拟框架，可以使用 TPU 计算流体流动。通过利用 &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>; 软件和 TPU 硬件架构的最新进展，该软件工具可以以前所未有的规模对湍流进行详细的大规模模拟，突破科学发现和湍流分析的界限。我们证明了该框架可以有效地扩展以适应问题的规模，或者改进运行时间，这是值得注意的，因为大多数大规模分布式计算框架都会因扩展而降低效率。该软件作为 &lt;a href=&quot;https://github.com/google-research/swirl-lm&quot;>;GitHub&lt;/a>; 上的开源项目提供。 &lt;/p>; &lt;br />; &lt;h2>;使用加速器进行大规模科学计算&lt;/h2>; &lt;p>; 该软件使用 TensorFlow 框架在 TPU 架构上求解变密度纳维-斯托克斯方程。采用&lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;>;单指令、多数据&lt;/a>; (SIMD) 方法来并行化 TPU 求解器实现。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_difference_method&quot;>;有限差分&lt;/a>;运算符利用 TPU 的矩阵乘法单元 (MXU)，将共置&lt;/a>;结构化网格用作 TensorFlow 卷积函数的过滤器。该框架利用了 TPU 加速器之间的低延迟高带宽芯片间互连 (ICI)。此外，通过加速线性代数 (XLA) 编译器利用单精度浮点计算和高度优化的可执行文件，可以在 TPU 硬件架构上执行具有出色扩展性的大规模模拟。 &lt;/p>; &lt;p>; 这项研究工作表明，基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;图&lt;/a>;的 TensorFlow 与新型 ML 特殊用途相结合硬件，可以用作编程范例来求解表示多物理场流的偏微分方程。后者是通过用物理模型增强纳维-斯托克斯方程来实现的，以考虑化学反应、传热和密度变化，从而能够模拟&lt;a href=&quot;https://arxiv.org/abs/ 2301.04698&quot;>;云层形成&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;野火&lt;/a>;。 &lt;/p>; &lt;p>; 值得注意的是，该框架是第一个开源&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_fluid_dynamics&quot;>;计算流体动力学&lt;/a>; (CFD) 框架用于高性能、大规模模拟，以充分利用近年来随着机器学习 (ML) 的进步而变得常见（并成为商品）的云加速器。虽然我们的工作重点是使用 TPU 加速器，但可以轻松地针对其他加速器（例如 GPU 集群）调整代码。 &lt;/p>; &lt;p>; 该框架展示了一种大大降低与运行大规模科学 CFD 模拟相关的成本和周转时间的方法，并在气候和天气研究等领域实现更高的迭代速度。由于该框架是使用 TensorFlow（一种 ML 语言）实现的，因此它还可以与 ML 方法轻松集成，并允许探索 CFD 问题的 ML 方法。由于 TPU 和 GPU 硬件的普遍可访问性，这种方法降低了研究人员为我们理解大规模湍流系统做出贡献的障碍。 &lt;/p>; &lt;br />; &lt;h2>;框架验证和均匀各向同性湍流&lt;/h2>; &lt;p>; 除了展示性能和扩展能力之外，验证该框架的正确性也很重要，以确保当它是用于 CFD 问题，我们得到了合理的结果。为此，研究人员通常在 CFD 求解器开发过程中使用理想化基准问题，我们在工作中采用了其中许多问题（更多详细信息，请参见 &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii /S0010465522000108？via%3Dihub&quot;>;论文&lt;/a>;）。 &lt;/p>; &lt;p>; 湍流分析的基准之一是&lt;a href=&quot;https://en.wikipedia.org/wiki/Homogeneous_isotropic_turbulence&quot;>;均匀各向同性湍流&lt;/a>;&lt;em>; &lt;/em>;(HIT ），这是一个经过充分研究的规范流，其中动能等统计特性在坐标轴的平移和旋转下保持不变。通过将分辨率提升到当前技术水平的极限，我们能够执行超过 80 亿度自由度的直接数值模拟，相当于沿三个方向各有 2,048 个网格点的三维网格。我们使用 512 个 TPU-v4 核心，将沿 x、y 和 z 轴的网格点的计算分别分布到 [2,2,128] 个核心上，针对 TPU 上的性能进行了优化。每个时间步长的挂钟时间约为 425 毫秒，总共模拟了 400,000 个时间步长的流程。 50 TB 数据（包括速度和密度字段）存储 400 个时间步长（每 1,000 个步长）。据我们所知，这是迄今为止进行的同类最大的湍流模拟之一。 &lt;/p>; &lt;p>; 由于湍流流场的复杂性、混沌性，其分辨率跨越多个数量级，因此有必要以高分辨率模拟系统。因为我们采用了具有 80 亿个点的高分辨率网格，所以我们能够准确地解析场。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLST4r1oj5zaGDi3Jy gwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;894&quot; data-original-width=&quot;990&quot; height=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ 5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/w400-h361/image1.png“宽度=”400“/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;速度沿 &lt;em>;z&lt; 方向的 &lt;em>;x&lt;/em>; 分量的轮廓/em>; 中板。模拟的高分辨率对于准确表示湍流场至关重要。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;湍流动能和耗散率是常用于分析湍流场的两个统计量。湍流。这些属性在没有额外能量注入的湍流场中的时间衰减是由于粘性耗散造成的，并且衰减渐近线遵循预期的分析&lt;a href=&quot;https://en.wikipedia.org/wiki/Energy_cascade&quot;>;幂律&lt; /a>;.这与文献中报告的理论渐近线和观察结果一致，从而验证了我们的框架。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7 fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s562/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;562&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1 CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;实线：湍流动能 (&lt;em>;k&lt;/em>;) 的时间演化。虚线：衰减均匀各向同性湍流的解析幂律 (&lt;em>;n&lt;/em>;=1.3) (&lt;em>;Ⲧ&lt;sub>;l&lt;/sub>;&lt;/em>;: &lt;a href=&quot;https://物理.stackexchange.com/questions/79184/what-are-the-length-and-time-scales-in-turbulence&quot;>;涡流周转时间&lt;/a>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4Ds Muf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s565/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;565&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAij lsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;实线：耗散率 (&lt;em>;ε&lt;/em>;) 的时间演化。虚线：衰减均匀各向同性湍流的解析幂律 (n=1.3)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 湍流的能谱表示 &lt;a 上的能量含量href=&quot;https://en.wikipedia.org/wiki/Wavenumber&quot;>;波数&lt;/a>;，其中波数&lt;i>;k&lt;/i>;与波长&lt;i>;λ&lt;/i>;成正比（即，&lt;i>;k&lt;/i>; ∝ 1/&lt;i>;λ&lt;/i>;）。一般来说，光谱可以定性地分为三个范围：源范围、惯性范围和粘性耗散范围（波数轴上从左到右，如下）。源范围内的最低波数对应于最大的湍流涡流，其能量含量最高。这些大涡流将能量传递给中间波数（惯性范围）内的湍流，该湍流在统计上是各向同性的（即在所有方向上基本均匀）。最小的涡流对应于最大的波数，通过流体的粘度耗散成热能。凭借在三个空间方向上每个方向都有 2,048 个点的精细网格，我们能够将流场解析到发生粘性耗散的长度尺度。这种直接数值模拟方法是最准确的，因为它不需要任何闭合模型来近似网格尺寸以下的能量级联。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW 8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N -k6DcBbh912IOG/s569/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;569 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NY dIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s16000/image2.png&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;不同时间实例的湍流动能谱。谱图通过瞬时积分长度 (&lt;em>;l&lt;/em>;) 和湍流动能 (&lt;em>;k&lt;/em>;) 进行归一化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;湍流研究的新时代&lt;/h2>; &lt;p>; 最近，我们扩展了这个框架来预测 &lt; a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;野火&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;大气流动&lt;/a>;，这是与气候风险评估相关。除了实现复杂湍流的高保真模拟之外，该模拟框架还提供&lt;a href=&quot;https://www.osti.gov/servlets/purl/1478744&quot;>;科学机器学习&lt;/a>; (SciML ） - 例如，从精细网格向下采样到粗网格（&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_order_reduction&quot;>;模型缩减&lt;/a>;）或构建以较低分辨率运行的模型仍然捕获正确的动态行为。它还可以为进一步的科学发现提供途径，例如构建基于机器学习的模型，以更好地参数化湍流的微观物理，包括温度、压力、蒸汽分数等之间的物理关系，并且可以改进各种控制任务，例如减少建筑物的能源消耗或找到更高效的螺旋桨形状。尽管很有吸引力，但 SciML 的一个主要瓶颈是训练数据的可用性。为了探索这一点，我们一直与斯坦福大学和 Kaggle 的团队合作，通过社区托管的网络平台 &lt;a href=&quot;https://blastnet.github.io&quot; 提供来自高分辨率 HIT 模拟的数据。 >;BLASTNet&lt;/a>;，通过数据集网络方法向研究界提供对高保真数据的广泛访问。我们希望这些新兴的高保真模拟工具与社区驱动的数据集的结合将导致流体力学各个领域的重大进步。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢 Qing Wang、Yi-Fan Chen 和 John Anderson 提供的咨询和建议，感谢 Tyler Russell 和 Carla Bromberg 提供的项目管理。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5967099570177693866/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html&quot; rel=&quot;alternate&quot; title=&quot;一种用于湍流研究的新型计算流体动力学框架&quot; type=&quot;text/html &quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图片高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16” &quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLST4r1oj 5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/ s72-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8763480087701216145&lt;/id>;&lt;已发布>;2023-09-06T12:47:00.000-07:00&lt;/已发布>;&lt;更新>;2023-09-06T12:47:22.739-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;TSMixer：用于时间序列预测的全 MLP 架构&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：云人工智能团队学生研究员陈思安和云人工智能团队研究科学家李春亮&lt;/ span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4 BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s320/hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;时间序列&lt;/a>;预测对于各种实际应用至关重要，来自 &lt;a href=&quot;https:// /www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;需求预测&lt;/a>;到&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/29606177/&quot;>;大流行病传播预测&lt; /a>;.在&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;多变量时间序列预测&lt;/a>;（同时预测多个变量）中，可以将现有方法分为两类：单变量模型和多变量模型楷模。单变量模型侧重于序列间相互作用或时间模式，其中包含具有单个变量的时间序列上的趋势和季节性模式。这种趋势和季节性模式的例子可能是抵押贷款利率因通货膨胀而增加的方式，以及高峰时段交通高峰的方式。除了系列间模式之外，多元模型还处理系列内特征，称为跨变量信息，当一个系列是另一个系列的高级指标时，这特别有用。例如，体重增加可能导致血压升高，产品价格上涨可能导致销量下降。多变量模型&lt;a href=&quot;https://research.google/pubs/pub49697/&quot;>;最近成为多变量预测的流行解决方案&lt;/a>;，因为从业者相信他们处理跨变量信息的能力可能会带来更好的性能。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 近年来，基于深度学习 Transformer 的架构因其在序列任务上的卓越性能而成为多元预测模型的热门选择。然而，高级多变量模型&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;在常用的&lt;a href=&quot;https://github.com/2205.13504&quot;>;上比简单的单变量线性模型表现出奇差&lt;/a>;。 com/thuml/Autoformer&quot;>;长期预测基准&lt;/a>;，例如&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;电力变压器温度&lt;/a>; (ETT)、&lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+conspiration&quot;>;电力&lt;/a>;，&lt;a href=&quot;https://pems.dot.ca .gov/&quot;>;交通&lt;/a>;和&lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;天气&lt;/a>;。这些结果提出了两个问题：&lt;/p>; &lt;ul>; &lt;li>;跨变量信息是否有利于时间序列预测？ &lt;/li>;&lt;li>;当跨变量信息没有好处时，多元模型仍然可以像单变量模型一样表现吗？ &lt;/li>; &lt;/ul>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.06053&quot;>;TSMixer：用于时间序列预测的全 MLP 架构&lt;/a>;”中，我们分析了单变量线性模型的优点并揭示其有效性。根据此分析的见解，我们开发了时间序列混合器 (TSMixer)，这是一种先进的多元模型，它利用线性模型特征并在长期预测基准上表现良好。据我们所知，TSMixer 是第一个在长期预测基准上与最先进的单变量模型表现一样好的多变量模型，我们表明跨变量信息的益处较小。为了证明跨变量信息的重要性，我们评估了一个更具挑战性的现实应用程序，&lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;M5&lt;/a>;。最后，实证结果表明 TSMixer 优于最先进的模型，例如 &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/2201.12740&quot;>;Fedformer&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;、&lt;a href=&quot;https:// arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TSMixer 架构&lt;/h2>; &lt;p>; 线性模型和 Transformer 之间的主要区别在于它们捕获的方式时间模式。一方面，线性模型应用固定且与时间步长相关的权重来捕获静态时间模式，并且无法处理跨变量信息。另一方面，Transformers 使用&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;注意力机制&lt;/a>;，在每个时间步应用动态和数据相关的权重，捕获动态时间模式并启用他们来处理跨变量信息。 &lt;/p>; &lt;p>; 在我们的分析中，我们表明，在时间模式的常见假设下，线性模型具有简单的解决方案来完美地恢复时间序列或对误差设置界限，这意味着它们是学习静态时间模式的绝佳解决方案更有效地分析单变量时间序列。相比之下，为注意力机制找到类似的解决方案并非易事，因为应用于每个时间步的权重是动态的。因此，我们通过用线性层替换 Transformer 注意力层来开发一种新的架构。由此产生的 TSMixer 模型类似于计算机视觉 &lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;>;MLP-Mixer&lt;/a>; 方法，在不同的多层感知器应用之间交替。方向，我们分别称之为&lt;em>;时间混合&lt;/em>;和&lt;em>;特征混合&lt;/em>;。 TSMixer 架构有效地捕获时间模式和跨变量信息，如下图所示。残差设计确保 TSMixer 保留时间线性模型的能力，同时仍然能够利用跨变量信息。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY 3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s1999/ image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiL sL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption &quot; style=&quot;text-align: center;&quot;>;Transformer 块和 TSMixer 块架构。 TSMixer 用时间混合代替了多头注意力层，时间混合是一种应用于时间维度的线性模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQizn bfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/s1646/image4.png&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1646&quot; height=&quot;195&quot; src=&quot;https://blogger.googleusercontent.com/img/b /R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC 5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/w400-h195/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;数据相关（注意力机制）和时间步相关（线性模型）之间的比较。这是通过学习前三个时间步的权重来预测下一个时间步的示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;长期预测基准评估&lt;/h2>; &lt;p>; 我们使用七个流行的&lt;a href=&quot;https://github.com/thuml /Autoformer&quot;>;长期预测数据集&lt;/a>; (&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm1&lt;/a>;, &lt;a href=&quot;https://github.com /zhouhaoyi/ETDataset&quot;>;ETTm2&lt;/a>;、&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh1&lt;/a>;、&lt;a href=&quot;https://github.com/zhouhaoyi /ETDataset&quot;>;ETTh2&lt;/a>;，&lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+conspiration&quot;>;电力&lt;/a>;，&lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;交通&lt;/a>;和&lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;天气&lt;/a>; a>;），其中&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;最近的研究&lt;/a>;表明，单变量线性模型的性能优于具有较大裕度的高级多元模型。我们将 TSMixer 与最先进的多变量模型进行比较（&lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;、&lt;a href=&quot;https://arxiv.org /abs/2201.12740&quot;>;FEDformer&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs /2012.07436&quot;>;Informer&lt;/a>;）和单变量模型，包括&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;线性模型&lt;/a>;和&lt;a href=&quot;https:// arxiv.org/abs/2211.14730&quot;>;补丁TST&lt;/a>;。下图显示了 TSMixer 与其他方法相比的&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;均方误差&lt;/a>; (MSE) 的平均改进。平均值是跨数据集和多个预测范围计算的。我们证明 TSMixer 的性能显着优于其他多变量模型，并且与最先进的单变量模型相当。这些结果表明，多变量模型的性能与单变量模型一样好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec 3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s1200/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQK HbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;与其他基线相比，TSMixer 的平均 MSE 改进。红色条显示多变量方法，蓝色条显示单变量方法。 TSMixer 比其他多变量模型取得了显着改进，并取得了与单变量模型相当的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;消融研究&lt;/h2>; &lt;p>; 我们进行了消融研究，以将 TSMixer 与 TMix-Only 进行比较，TMix-Only 是仅由时间混合层组成的 TSMixer 变体。结果表明，TMix-Only 的性能几乎与 TSMixer 相同，这意味着额外的特征混合层不会提高性能，并证实跨变量信息在流行的基准测试中不太有利。结果验证了&lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;之前的研究&lt;/a>;中显示的卓越的单变量模型性能。然而，现有的长期预测基准不能很好地代表一些现实世界应用中对跨变量信息的需求，在这些应用中，时间序列可能是间歇性或稀疏的，因此时间模式可能不足以进行预测。因此，仅根据这些基准来评估多元预测模型可能是不合适的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;M5评估：跨变量信息的有效性&lt;/h2>; &lt;p>;进一步论证为了利用多变量模型的优势，我们在具有挑战性的 M5 基准上评估 TSMixer，这是一个包含关键跨变量交互的大规模零售数据集。 M5包含5年来收集的30,490个产品的信息。每个产品描述都包含时间序列数据，例如每日销售额、售价、促销活动信息和静态（非时间序列）特征，例如商店位置和产品类别。目标是预测未来 28 天每种产品的每日销售额，使用&lt;a href=&quot;https://mofc.unic.ac.cy/m5-competition/&quot;>;加权均方根缩放误差&lt; /a>; (WRMSSE) 来自 M5 竞赛。零售业的复杂性使得仅使用专注于时间模式的单变量模型进行预测变得更具挑战性，因此具有跨变量信息甚至辅助特征的多变量模型更加重要。 &lt;/p>; &lt;p>; 首先，我们将 TSMixer 与其他方法进行比较，仅考虑历史数据，例如每日销售额和历史销售价格。结果表明，多变量模型显着优于单变量模型，表明跨变量信息的有用性。在所有比较的方法中，TSMixer 有效地利用了跨变量信息并实现了最佳性能。 &lt;/p>; &lt;p>; 此外，为了利用 M5 中提供的更多信息，例如静态特征（例如，商店位置、产品类别）和未来时间序列（例如，未来几天安排的促销活动），我们提出了一个原则设计扩展 TSMixer。扩展的TSMixer将不同类型的特征对齐到相同的长度，然后将多个混合层应用于连接的特征以进行预测。扩展的 TSMixer 架构优于工业应用中流行的模型，包括 &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs /1912.09363&quot;>;TFT&lt;/a>;，展示了其对现实世界影响的强大潜力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQ gaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/s1950/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1950&quot; data-original-width=&quot;1760&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6t c22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/w578-h640/image2.png&quot; width=&quot;578&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;扩展 TSMixer 的架构。在第一阶段（对齐阶段），它将不同类型的特征对齐到相同的长度，然后再将它们连接起来。在第二阶段（混合阶段），它应用以静态特征为条件的多个混合层。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbK ZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKP JQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_ha46iTaDBpogHn7iVJ615O/ s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;M5 上的 WRMSSE。前三种方法（&lt;strong>;蓝色&lt;/strong>;）是单变量模型。中间三种方法（&lt;strong>;橙色&lt;/strong>;）是仅考虑历史特征的多元模型。最后三种方法（&lt;strong>;红色&lt;/strong>;）是考虑历史、未来和静态特征的多元模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 TSMixer，这是一种先进的多元模型，它利用线性模型特征，并且性能与状态一样好。长期预测基准的最先进的单变量模型。 TSMixer 通过深入了解跨变量和辅助信息在现实场景中的重要性，为时间序列预测架构的开发创造了新的可能性。实证结果强调，在未来的研究中需要考虑更现实的多元预测模型基准。我们希望这项工作能够激发时间序列预测领域的进一步探索，并导致开发更强大、更有效的模型，以应用于实际应用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Si-An Chen 进行，李春亮、Nate Yoder、Sercan O. Arik 和 Tomas Pfister。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google /feeds/8763480087701216145/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/ tsmixer-all-mlp-architecture-for-time.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger .com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/8763480087701216145&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time .html&quot; rel=&quot;alternate&quot; title=&quot;TSMixer：用于时间序列预测的全 MLP 架构&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http:// /www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/ 2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd2 4e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http:// /search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626 .post-6223088894812086013&lt;/id>;&lt;发布>;2023-08-31T10:14:00.000-07:00&lt;/发布>;&lt;更新>;2023-08-31T10:14:27.378-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= &quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;WeatherBench 2：下一代数据驱动天气模型的基准&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;发布者：Google 研究部研究科学家 Stephan Rasp 和项目负责人 Carla Bromberg&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFd jdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s800 /weatherbenchgif.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 1950 年，当研究人员使用第一台可编程通用计算机时，天气预报开始了数字革命&lt;a href=&quot;https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC %20(%2F%CB%88%C9%9Bni,of%20them%20in%20one%20computer.&quot;>;ENIAC&lt;/a>; 用于求解描述天气如何演变的数学方程。在此后的 70 多年里，不断进步计算能力的进步和模型公式的改进导致了天气预报技能的稳步提高：今天的 7 天预报大约与 2000 年的 5 天预报和 1980 年的 3 天预报一样准确。同时提高预报准确性以大约每十年一天的速度似乎没什么大不了的，每天的改进对于影响深远的用例都很重要，例如物流规划、灾害管理、农业和能源生产。这个&lt;a href=&quot;https: //www.nature.com/articles/nature14956&quot;>;“安静”革命&lt;/a>;对社会具有巨大价值，拯救了生命并为许多部门提供了经济价值。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 现在，我们看到天气预报领域的另一场革命正在开始，这一次是由机器学习 (ML) 的进步推动的。我们的想法不是对物理方程的近似值进行硬编码，而是让算法通过查看大量过去的天气数据来了解天气是如何演变的。这样做的早期尝试可以追溯到 &lt;a href=&quot;https://gmd.copernicus.org/articles/11/3999/2018/&quot;>;2018&lt;/a>;，但在过去两年中，步伐大大加快几个大型机器学习模型展示了与最好的基于物理的模型相媲美的天气预报能力。 Google 的 MetNet [&lt;a href=&quot;https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;1&lt;/a>;、&lt;a href=&quot;例如，https://arxiv.org/abs/2306.06079&quot;>;2&lt;/a>;] 展示了预测未来一天区域天气的最先进功能。对于全局预测，Google DeepMind 创建了 &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>;，这是一个图神经网络，可以在 25 公里的水平分辨率下进行 10 天的预测，与许多技能指标中最好的基于物理的模型。 &lt;/p>; &lt;p>; 除了可能提供更准确的预测之外，此类机器学习方法的一个关键优势是，一旦经过训练，它们就可以在廉价的硬件上在几分钟内创建预测。相比之下，传统天气预报需要每天运行数小时的大型超级计算机。显然，机器学习为天气预报界带来了巨大的机遇。这也得到了领先的天气预报中心的认可，例如&lt;a href=&quot;https://www.ecmwf.int/&quot;>;欧洲中期天气预报中心&lt;/a>; (ECMWF) &lt;a href =&quot;https://www.ecmwf.int/en/elibrary/81207-machine-learning-ecmwf-roadmap-next-10-years&quot;>;机器学习路线图&lt;/a>;或&lt;a href=&quot;https:// /www.noaa.gov/&quot;>;美国国家海洋和大气管理局&lt;/a>; (NOAA) &lt;a href=&quot;https://sciencecouncil.noaa.gov/wp-content/uploads/2023/04/2020-AI- Strategy.pdf&quot;>;人工智能战略&lt;/a>;。 &lt;/p>; &lt;p>; 为了确保机器学习模型受到信任并针对正确的目标进行优化，预测评估至关重要。然而，评估天气预报并不简单，因为天气是一个令人难以置信的多方面问题。不同的最终用户对预测的不同属性感兴趣，例如，可再生能源生产商关心风速和太阳辐射，而危机应对团队则关心潜在气旋或即将到来的热浪的轨迹。换句话说，没有单一的指标来确定什么是“好的”天气预报，评估必须反映天气及其下游应用的多方面性质。此外，精确评估设置的差异（例如，使用哪种分辨率和地面实况数据）可能会导致模型比较变得困难。有一种方法以公平且可重复的方式比较新颖的方法和已建立的方法对于衡量该领域的进展至关重要。 &lt;/p>; &lt;p>; 为此，我们宣布推出 &lt;a href=&quot;https://arxiv.org/abs/2308.15560&quot;>;WeatherBench 2&lt;/a>; (WB2)，这是下一代数据的基准驱动的全球天气模型。 WB2 是对 2020 年发布的&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002203&quot;>;原始基准&lt;/a>;的更新，该基准基于初始的较低分辨率的机器学习楷模。 WB2 的目标是通过提供一个可信的、可重复的框架来评估和比较不同的方法，从而加速数据驱动的天气模型的进展。 &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;官方网站&lt;/a>;包含来自多个最先进模型的分数（在撰写本文时，这些是&lt;a href= &quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler (2022)&lt;/a>;，一种早期图神经网络，Google DeepMind 的 &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast &lt;/a>; 和华为的&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06185-3&quot;>;盘古天气&lt;/a>;（基于 Transformer 的 ML 模型）。此外，还包括 ECMWF 高分辨率和集合预报系统的预报，它们代表了一些最好的传统天气预报模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3h PB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s1960/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;372&quot; data-original-width=&quot;1960&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFi ySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;让评估更容易&lt; /h2>; &lt;p>; WB2 的关键组件是一个&lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;开源评估框架&lt;/a>;，它允许用户评估他们的预测与其他基线相同的方式。高分辨率的天气预报数据可能非常大，甚至使评估成为计算挑战。因此，我们在 &lt;a href=&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; 上构建了评估代码，它允许用户将计算分割成更小的块并以分布式方式评估它们，例如在 Google Cloud 上使用 &lt;a href=&quot;https://cloud.google.com/dataflow&quot;>;DataFlow&lt;/a>;。该代码附带&lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/evaluation.html&quot;>;快速入门指南&lt;/a>;，可帮助人们快速上手。 &lt;/p>; &lt;p>; 此外，我们&lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/data-guide.html&quot;>;提供&lt;/a>;大部分真实数据和基线数据在 Google Cloud Storage 上以不同分辨率的云优化 &lt;a href=&quot;https://zarr.dev/&quot;>;Zarr&lt;/a>; 格式存储，例如 &lt;a href=&quot;https:// rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803&quot;>;ERA5&lt;/a>; 数据集用于训练大多数机器学习模型。这是 Google 更大努力的一部分，旨在为研究提供&lt;a href=&quot;https://github.com/google-research/arco-era5&quot;>;分析就绪、云优化的天气和气候数据集&lt;/a>;社区以及&lt;a href=&quot;https://github.com/google-research/arco-era5#roadmap&quot;>;超越&lt;/a>;。由于从各自的档案中下载这些数据并进行转换可能非常耗时且需要大量计算，因此我们希望这能够大大降低社区的进入门槛。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;评估预测技能&lt;/h2>; &lt;p>; 与我们的合作者一起 &lt;a href=&quot;https ://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;，我们定义了一组最能反映全球天气预报质量的标题分数。如下图所示，一些基于 ML 的预测的误差低于 &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts&quot;>;关于确定性指标的最先进的物理模型&lt;/a>;。这适用于一系列变量和区域，并强调了基于机器学习的方法的竞争力和前景。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9Xtx fFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7 YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;此记分卡显示了与 ECMWF 相比的不同模型的技能&lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and -support/changes-ecmwf-model&quot;>;综合预报系统&lt;/a>; (IFS)，针对多个变量的最佳基于物理的天气预报之一。 IFS 预测是根据 IFS 分析进行评估的。所有其他模型均根据 ERA5 进行评估。 ML 模型的顺序反映了发布日期。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实现可靠的概率预测&lt;/h2>; &lt;p>; 然而，单一预测通常是不够的。由于&lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;蝴蝶效应&lt;/a>;，天气本质上是混乱的。出于这个原因，运营气象中心现在运行大约 50 个稍微扰动的模型实现（称为集合），以估计各种情景下的预报概率分布。例如，如果人们想知道极端天气发生的可能性，这一点很重要。 &lt;/p>; &lt;p>; 创建可靠的概率预测将是全球机器学习模型的下一个关键挑战之一。区域机器学习模型，例如 Google 的 &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet&lt;/a>; 已经估算了概率。为了预测下一代全球模型，WB2 已经提供了概率指标和基线，其中&lt;a href=&quot;https://www.ecmwf.int/en/about/media-centre/focus/2017/fact-sheet- ensemble-weather-forecasting&quot;>;ECMWF 的 IFS 集合&lt;/a>;，以加速这一方向的研究。 &lt;/p>; &lt;p>; 如上所述，天气预报有很多方面，虽然标题指标试图捕捉预报技能最重要的方面，但它们还远远不够。一个例子是预测现实主义。目前，面对大气的内在不确定性，许多机器学习预测模型倾向于“两面下注”。换句话说，他们倾向于预测平均误差较低的平滑场，但并不代表真实的、物理一致的大气状态。下面的动画就是一个例子。 Pangu-Weather 和 GraphCast（下）这两个数据驱动模型可以很好地预测大气的大规模演化。然而，与地面实况或物理预测模型 IFS HRES（上）相比，它们的小规模结构也较少。在 WB2 中，我们包含了一系列此类案例研究以及量化此类模糊的光谱指标。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-Zkx F_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MSCBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s1200/image2 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1020&quot; data-original-width=&quot;1200&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCd YUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;2020 年 1 月 3 日启动的锋线穿过美国大陆的预报。地图显示气压为 850 度 &lt;a href=&quot;https: //sites.research.google/weatherbench/faq/&quot;>;hPa&lt;/a>;（大约相当于 1.5 公里的海拔）和 &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;等值线中压力水平为 500 hPa（约 5.5 公里）的 >;位势&lt;/a>;。 ERA5是相应的ground-truth分析，IFS HRES是ECMWF基于物理的预测模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; WeatherBench 2 将继续随着 ML 模型的开发而发展。 &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;官方网站&lt;/a>;将更新最新的最先进模型。 （要提交模型，请按照&lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/submit.html&quot;>;这些说明&lt;/a>;操作）。我们还邀请社区通过 &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;WB2 GitHub 页面&lt;/a>;上的问题和拉取请求提供反馈和改进建议。 &lt;/p>; &lt;p>; 精心设计评估并瞄准正确的指标对于确保机器学习天气模型尽快造福社会至关重要。现在的 WeatherBench 2 只是一个起点。我们计划在未来扩展它，以解决基于机器学习的天气预报未来的关键问题。具体来说，我们希望添加站点观测和更好的降水数据集。此外，我们将探索将临近预报和次季节到季节预测纳入基准。 &lt;/p>; &lt;p>; 随着天气预报的不断发展，我们希望 WeatherBench 2 能够为研究人员和最终用户提供帮助。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;WeatherBench 2 是多个不同领域合作的成果Google 的团队和 ECMWF 的外部合作者。我们要感谢 ECMWF 的 Matthew Chantry、Zied Ben Bouallegue 和 Peter Dueben。我们谨向 Google 致谢该项目的核心贡献者：Stephan Rasp、Stephan Hoyer、Peter Battaglia、Alex Merose、Ian Langmore、Tyler Russell、Alvaro Sanchez、Antonio Lobato、Laurence Chiu、Rob Carver、Vivian Yang、Shreya Agrawal 、托马斯·特恩布尔、杰森·希基、卡拉·布隆伯格、贾里德·西斯克、​​卢克·巴林顿、亚伦·贝尔和沙飞。我们还要感谢 Kunal Shah、Rahul Mahrsee、Aniket Rawat 和 Satish Kumar。感谢 John Anderson 对 WeatherBench 2 的赞助。此外，我们还要感谢盘古天气团队的毕凯峰和 Ryan Keisler 将模型添加到 WeatherBench 2 的帮助。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href=&quot;http://blog.research.google/feeds/6223088894812086013/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>; &lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08 /weatherbench-2-benchmark-for-next.html&quot; rel=&quot;alternate&quot; title=&quot;WeatherBench 2：下一代数据驱动天气模型的基准&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel =&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image >;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ 43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s72-c/weatherbenchgif.gif&quot;宽度=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger .com，1999：blog-8474926331452026626.post-1342410539466537463&lt;/id>;&lt;发布>;2023-08-30T12:34:00.002-07:00&lt;/发布>;&lt;更新>;2023-08-30T12:39:33.664- 07 ：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“HCI”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com” /atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;建模并提高实时字幕中的文本稳定性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;发布者：Google 增强现实研究科学家 Vikas Bahirwani 和软件工程师 Susan Xu&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45 -muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s23 00/stabilizedcaptions.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 自动语音识别 (ASR) 技术通过远程会议软件、移动应用程序和 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3379337.3415817&quot; 中的实时字幕使对话变得更加容易>;头戴式​​显示器&lt;/a>;。然而，为了保持实时响应能力，实时字幕系统通常会显示临时预测，这些预测会随着收到新话语而更新。这可能会导致文本不稳定（之前显示的文本被更新时出现“闪烁”，如下面视频左侧的字幕所示），这可能会因分心、疲劳、以及跟不上谈话的困难。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://research.google/pubs/pub52450/&quot;>;建模并提高实时字幕中的文本稳定性&lt;/” a>;”，在 &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/98883&quot;>;ACM CHI 2023&lt;/a>; 上提出，我们通过一些方法将文本稳定性问题形式化关键贡献。首先，我们通过采用基于视觉的闪烁指标来量化文本不稳定性，该指标使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Luminance&quot;>;亮度&lt;/a>;对比度和&lt;a href=&quot;https ://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;>;离散傅里叶变换&lt;/a>;。其次，我们还引入了一种稳定性算法，通过标记对齐、语义合并和平滑动画来稳定实时字幕的渲染。最后，我们进行了一项用户研究 (N=123)，以了解观众对实时字幕的体验&lt;strong>;。 &lt;/strong>;我们的统计分析表明，我们提出的闪烁指标与观众体验之间存在很强的相关性。此外，它表明我们提出的稳定技术显着改善了观看者的体验（例如，上面视频中右侧的字幕）。 &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/-hiAnT6QkmU?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;-hiAnT6QkmU&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Raw ASR captions vs. stabilized captions&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Metric&lt;/h2>; &lt;p>; Inspired by &lt;a href=&quot;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5203/0000/Toward-perceptual-metrics-for-video-watermark-evaluation/10.1117/12.512550.full&quot;>;previous work&lt;/a>;, we propose a flicker-based metric to quantify text stability and objectively evaluate the performance of live captioning systems. Specifically, our goal is to quantify the flicker in a grayscale live caption video. We achieve this by comparing the difference in luminance between individual frames (frames in the figures below) that constitute the video. Large visual changes in luminance are obvious (eg, addition of the word “bright” in the figure on the bottom), but subtle changes (eg, update from “... this gold. Nice..” to “... this. Gold is nice”) may be difficult to discern for readers. However, converting the change in luminance to its constituting frequencies exposes both the obvious and subtle changes. &lt;/p>; &lt;p>; Thus, for each pair of contiguous frames, we convert the difference in luminance into its constituting frequencies using discrete Fourier transform. We then sum over each of the low and high frequencies to quantify the flicker in this pair. Finally, we average over all of the frame-pairs to get a per-video flicker. &lt;/p>; &lt;p>; For instance, we can see below that two identical frames (top) yield a flicker of 0, while two non-identical frames (bottom) yield a non-zero flicker. It is worth noting that higher values of the metric indicate high flicker in the video and thus, a worse user experience than lower values of the metric. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s1399/noflicker.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;268&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s16000/noflicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the flicker metric between two identical frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s1399/flicker.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;263&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s16000/flicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the flicker between two non-identical frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Stability algorithm &lt;/h2>; &lt;p>; To improve the stability of live captions, we propose an algorithm that takes as input already rendered sequence of tokens (eg, “Previous” in the figure below) and the new sequence of ASR predictions, and outputs an updated stabilized text (eg, “Updated text (with stabilization)” below). It considers both the natural language understanding (NLU) aspect as well as the ergonomic aspect (display, layout, etc.) of the user experience in deciding when and how to produce a stable updated text. Specifically, our algorithm performs tokenized alignment, semantic merging, and smooth animation to achieve this goal. In what follows, a token is defined as a word or punctuation produced by ASR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s1995/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;1995&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show (a) the previously already rendered text, (b) the baseline layout of updated text without our merging algorithm, and (c) the updated text as generated by our stabilization algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />;&lt;p>;&lt;br />;&lt;/p>;&lt;p>;&lt;br />;&lt;/p>; &lt;p>; Our algorithm address the challenge of producing stabilized updated text by first identifying three classes of changes (highlighted in red, green, and blue below): &lt;/p>; &lt;ol type=&quot;A&quot;>; &lt;li>;Red: Addition of tokens to the end of previously rendered captions (eg, &quot;How about&#39;&#39;). &lt;/li>; &lt;li>;Green: Addition / deletion of tokens, in the middle of already rendered captions. &lt;ul>; &lt;li>;B1: Addition of tokens (eg, &quot;I&#39;&#39; and &quot;friends&#39;&#39;). These may or may not affect the overall comprehension of the captions, but may lead to layout change. Such layout changes are not desired in live captions as they cause significant jitter and poorer user experience. Here “I” does not add to the comprehension but “friends” does. Thus, it is important to balance updates with stability specially for B1 type tokens. &lt;/li>; &lt;li>;B2: Removal of tokens, eg, &quot;in&#39;&#39; is removed in the updated sentence. &lt;/li>; &lt;/ul>; &lt;/li>; &lt;li>;Blue: Re-captioning of tokens: This includes token edits that may or may not have an impact on the overall comprehension of the captions. &lt;/li>;&lt;ul>; &lt;li>;C1: Proper nouns like &quot;disney land&#39;&#39; are updated to &quot;Disneyland&#39;&#39;. &lt;/li>; &lt;li>;C2: Grammatical shorthands like &quot;it&#39;s&#39;&#39; are updated to &quot;It was&#39;&#39;. &lt;/li>; &lt;/ul>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Classes of changes between previously displayed and updated text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Alignment, merging, and smoothing &lt;/h2>; &lt;p>; To maximize text stability, our goal is to align the old sequence with the new sequence using updates that make minimal changes to the existing layout while ensuring accurate and meaningful captions. To achieve this, we leverage a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#:~:text=The%20Needleman%E2%80%93Wunsch%20algorithm%20is%20still%20widely%20used%20for%20optimal,alignments%20having%20the%20highest%20score.&quot;>;Needleman-Wunsch algorithm&lt;/a>; with dynamic programming to merge the two sequences depending on the class of tokens as defined above: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Case A tokens:&lt;/b>; We directly add case A tokens, and line breaks as needed to fit the updated captions. &lt;/li>;&lt;li>;&lt;b>;Case B tokens:&lt;/b>; Our preliminary studies showed that users preferred stability over accuracy for previously displayed captions. Thus, we only update case B tokens if the updates do not break an existing line layout. &lt;/li>;&lt;li>;&lt;b>;Case C tokens:&lt;/b>; We compare the semantic similarity of case C tokens by transforming original and updated sentences into sentence embeddings, measuring their dot-product, and updating them only if they are semantically different (similarity &amp;lt; 0.85) and the update will not cause new line breaks. &lt;/li>; &lt;/ul>; &lt;p>; Finally, we leverage animations to reduce visual jitter. We implement smooth scrolling and fading of newly added tokens to further stabilize the overall layout of the live captions. &lt;/p>; &lt;h2>;User evaluation&lt;/h2>; &lt;p>; We conducted a user study with 123 participants to (1) examine the correlation of our proposed flicker metric with viewers&#39; experience of the live captions, and (2) assess the effectiveness of our stabilization techniques. &lt;/p>; &lt;p>; We manually selected 20 videos in YouTube to obtain a broad coverage of topics including video conferences, documentaries, academic talks, tutorials, news, comedy, and more. For each video, we selected a 30-second clip with at least 90% speech. &lt;/p>; &lt;p>; We prepared four types of renderings of live captions to compare: &lt;/p>; &lt;ol>; &lt;li>;Raw ASR: raw speech-to-text results from a speech-to-text API. &lt;/li>;&lt;li>;Raw ASR + thresholding: only display interim speech-to-text result if its confidence score is higher than 0.85. &lt;/li>;&lt;li>;Stabilized captions: captions using our algorithm described above with alignment and merging. &lt;/li>;&lt;li>;Stabilized and smooth captions: stabilized captions with smooth animation (scrolling + fading) to assess whether softened display experience helps improve the user experience. &lt;/li>; &lt;/ol>; &lt;p>; We collected user ratings by asking the participants to watch the recorded live captions and rate their assessments of comfort, distraction, ease of reading, ease of following the video, fatigue, and whether the captions impaired their experience. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Correlation between flicker metric and user experience&lt;/h3>; &lt;p>; We calculated &lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;>;Spearman&#39;s coefficient&lt;/a>; between the flicker metric and each of the behavioral measurements (values range from -1 to 1, where negative values indicate a negative relationship between the two variables, positive values indicate a positive relationship, and zero indicates no relationship). Shown below, our study demonstrates statistically significant (𝑝 &amp;lt; 0.001) correlations between our flicker metric and users&#39; ratings. The absolute values of the coefficient are around 0.3, indicating a moderate relationship. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Behavioral Measurement&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;&lt;b>;Correlation to Flickering Metric*&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Comfort &lt;/td>; &lt;td align=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Distraction &lt;/td>; &lt;td align=&quot;center&quot;>; 0.33 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Easy to read &lt;/td>; &lt;td align=&quot;center&quot;>; -0.31 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Easy to follow videos &lt;/td>; &lt;td align=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Fatigue &lt;/td>; &lt;td align=&quot;center&quot;>; 0.36 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Impaired Experience &lt;/td>; &lt;td align=&quot;center&quot;>; 0.31 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spearman correlation tests of our proposed flickering metric. *&lt;em>;p&lt;/em>; &amp;lt; 0.001.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Stabilization of live captions&lt;/h3>; &lt;p>; Our proposed technique (stabilized smooth captions) received consistently better ratings, significant as measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;>;Mann-Whitney U test&lt;/a>; (&lt;em>;p&lt;/em>; &amp;lt; 0.01 in the figure below), in five out of six aforementioned survey statements. That is, users considered the stabilized captions with smoothing to be more comfortable and easier to read, while feeling less distraction, fatigue, and impairment to their experience than other types of rendering. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s960/graph.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;387&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s16000/graph.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User ratings from 1 (Strongly Disagree) – 7 (Strongly Agree) on survey statements. (**: p&amp;lt;0.01, ***: p&amp;lt;0.001; ****: p&amp;lt;0.0001; ns: non-significant)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future direction&lt;/h2>; &lt;p>; Text instability in live captioning significantly impairs users&#39; reading experience. This work proposes a vision-based metric to model caption stability that statistically significantly correlates with users&#39; experience, and an algorithm to stabilize the rendering of live captions. Our proposed solution can be potentially integrated into existing ASR systems to enhance the usability of live captions for a variety of users, including those with translation needs or those with hearing accessibility needs. &lt;/p>; &lt;p>; Our work represents a substantial step towards measuring and improving text stability. This can be evolved to include language-based metrics that focus on the consistency of the words and phrases used in live captions over time. These metrics may provide a reflection of user discomfort as it relates to language comprehension and understanding in real-world scenarios. We are also interested in conducting &lt;a href=&quot;https://en.wikipedia.org/wiki/Eye_tracking&quot;>;eye-tracking&lt;/a>; studies (eg, videos shown below) to track viewers&#39; gaze patterns, such as eye fixation and saccades, allowing us to better understand the types of errors that are most distracting and how to improve text stability for those. &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowedfullscreen=&quot;&quot; youtube-src-id =&quot;1E2jGUscWyc&quot;宽度=&quot;640&quot;高度=&quot;360&quot; src=&quot;https://www.youtube.com/embed/1E2jGUscWyc?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of tracking a viewer&#39;s gaze when reading raw ASR captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowfullscreen=&quot;&quot; youtube-src-id=&quot;YfotwPIznL8&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/YfotwPIznL8?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of tracking a viewer&#39;s gaze when reading stabilized and smoothed captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;p>; By improving text stability in live captions, we can create more effective communication tools and improve how people connect in everyday conversations in familiar or, through translation, unfamiliar languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Key contributors include Xingyu “Bruce” Liu, Jun Zhang, Leonardo Ferrer, Susan Xu, Vikas Bahirwani, Boris Smus, Alex Olwal, and Ruofei Du. We wish to extend our thanks to our colleagues who provided assistance, including Nishtha Bhatia, Max Spear, and Darcy Philippon. We would also like to thank Lin Li, Evan Parker, and CHI 2023 reviewers.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1342410539466537463/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html&quot; rel=&quot;alternate&quot; title=&quot;Modeling and improving text stability in live captions&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s72-c/stabilizedcaptions.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8352967842060136321&lt;/id>;&lt;published>;2023-08-29T12:57:00.001-07:00&lt;/published>;&lt;updated>;2023-08-30T17:14:40.708-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SayTap: Language to quadrupedal locomotion&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yujin Tang and Wenhao Yu, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s320/SayTap%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Simple and effective interaction between human and quadrupedal robots paves the way towards creating intelligent and capable helper robots, forging a future where technology enhances our lives in ways beyond our imagination. Key to such human-robot interaction systems is enabling quadrupedal robots to respond to natural language instructions. Recent developments in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) have demonstrated the potential to perform &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;high-level planning&lt;/a>;. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques, especially for inherently unstable legged robots, necessitating high-frequency control signals. Consequently, most &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing&lt;/a>; &lt;a href=&quot;https://code-as-policies.github.io/&quot;>;work&lt;/a>; presumes the provision of high-level APIs for LLMs to dictate robot behavior, inherently limiting the system&#39;s expressive capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://saytap.github.io&quot;>;SayTap: Language to Quadrupedal Locomotion&lt;/a>;”, to be presented at &lt;a href=&quot;https://www.corl2023.org/&quot;>;CoRL 2023&lt;/a>;, we propose an approach that uses foot contact patterns (which refer to the sequence and manner in which a four-legged agent places its feet on the ground while moving) as an interface to bridge human commands in natural language and a locomotion controller that outputs low-level commands. This results in an interactive quadrupedal robot system that allows users to flexibly craft diverse locomotion behaviors (eg, a user can ask the robot to walk, run, jump or make other movements using simple language). We contribute an LLM prompt design, a reward function, and a method to expose the SayTap controller to the feasible distribution of contact patterns. We demonstrate that SayTap is a controller capable of achieving diverse locomotion patterns that can be transferred to real robot hardware. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SayTap method&lt;/h2>; &lt;p>; The SayTap approach uses a contact pattern template, which is a 4 X &lt;i>;T&lt;/i>; matrix of 0s and 1s, with 0s representing an agent&#39;s feet in the air and 1s for feet on the ground. From top to bottom, each row in the matrix gives the foot contact patterns of the front left (FL), front right (FR), rear left (RL) and rear right (RR) feet. SayTap&#39;s control frequency is 50 Hz, so each 0 or 1 lasts 0.02 seconds. In this work, a desired foot contact pattern is defined by a cyclic sliding window of size L&lt;sub>;w&lt;/sub>; and of shape 4 X L&lt;sub>;w&lt;/sub>;. The sliding window extracts from the contact pattern template four foot ground contact flags, which indicate if a foot is on the ground or in the air between &lt;i>;t&lt;/i>; + 1 and &lt;i>;t&lt;/i>; + L&lt;sub>;w&lt;/sub>;. The figure below provides an overview of the SayTap method. &lt;/p>; &lt;p>; SayTap introduces these desired foot contact patterns as a new interface between natural language user commands and the locomotion controller. The locomotion controller is used to complete the main task (eg, following specified velocities) and to place the robot&#39;s feet on the ground at the specified time, such that the realized foot contact patterns are as close to the desired contact patterns as possible. To achieve this, the locomotion controller takes the desired foot contact pattern at each time step as its input in addition to the robot&#39;s proprioceptive sensory data (eg, joint positions and velocities) and task-related inputs (eg, user-specified velocity commands). We use &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;>;deep reinforcement learning&lt;/a>; to train the locomotion controller and represent it as a deep neural network. During controller training, a random generator samples the desired foot contact patterns, the policy is then optimized to output low-level robot actions to achieve the desired foot contact pattern. Then at test time a LLM translates user commands into foot contact patterns. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/s935/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;935&quot; height=&quot;297&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/w640-h297/image13.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap approach overview.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/saytap_blog.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap uses foot contact patterns (eg, 0 and 1 sequences for each foot in the inset, where 0s are foot in the air and 1s are foot on the ground) as an interface that bridges natural language user commands and low-level control commands. With a reinforcement learning-based locomotion controller that is trained to realize the desired contact patterns, SayTap allows a quadrupedal robot to take both simple and direct instructions (eg, “Trot forward slowly.”) as well as vague user commands (eg, “Good news, we are going to a picnic this weekend!”) and react accordingly.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We demonstrate that the LLM is capable of accurately mapping user commands into foot contact pattern templates in specified formats when given properly designed prompts, even in cases when the commands are unstructured or vague. In training, we use a random pattern generator to produce contact pattern templates that are of various pattern lengths &lt;i>;T&lt;/i>;, foot-ground contact ratios within a cycle based on a given gait type &lt;i>;G&lt;/i>;, so that the locomotion controller gets to learn on a wide distribution of movements leading to better generalization. See the &lt;a href=&quot;https://arxiv.org/abs/2306.07580&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; With a simple prompt that contains only three in-context examples of commonly seen foot contact patterns, an LLM can translate various human commands accurately into contact patterns and even generalize to those that do not explicitly specify how the robot should react. &lt;/p>; &lt;p>; SayTap prompts are concise and consist of four components: (1) general instruction that describes the tasks the LLM should accomplish; (2) gait definition that reminds the LLM of basic knowledge about quadrupedal gaits and how they can be related to emotions; (3) output format definition; and (4) examples that give the LLM chances to learn in-context. We also specify five velocities that allow a robot to move forward or backward, fast or slow, or remain still. &lt;/p>; &lt;span style=&quot;font-size: small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#0000ff&quot;>; General instruction block&lt;/font>; You are a dog foot contact pattern expert. Your job is to give a velocity and a foot contact pattern based on the input. You will always give the output in the correct format no matter what the input is. &lt;font color=&quot;#0000ff&quot;>;Gait definition block&lt;/font>; The following are description about gaits: 1. Trotting is a gait where two diagonally opposite legs strike the ground at the same time. 2. Pacing is a gait where the two legs on the left/right side of the body strike the ground at the same time. 3. Bounding is a gait where the two front/rear legs strike the ground at the same time. It has a longer suspension phase where all feet are off the ground, for example, for at least 25% of the cycle length. This gait also gives a happy feeling. &lt;font color=&quot;#0000ff&quot;>;Output format definition block&lt;/font>; The following are rules for describing the velocity and foot contact patterns: 1. You should first output the velocity, then the foot contact pattern. 2. There are five velocities to choose from: [-1.0, -0.5, 0.0, 0.5, 1.0]. 3. A pattern has 4 lines, each of which represents the foot contact pattern of a leg. 4. Each line has a label. &quot;FL&quot; is front left leg, &quot;FR&quot; is front right leg, &quot;RL&quot; is rear left leg, and &quot;RR&quot; is rear right leg. 5. In each line, &quot;0&quot; represents foot in the air, &quot;1&quot; represents foot on the ground. &lt;font color=&quot;#0000ff&quot;>;Example block&lt;/font>; Input: Trot slowly Output: 0.5 FL: 11111111111111111000000000 FR: 00000000011111111111111111 RL: 00000000011111111111111111 RR: 11111111111111111000000000 Input: Bound in place Output: 0.0 FL: 11111111111100000000000000 FR: 11111111111100000000000000 RL: 00000011111111111100000000 RR: 00000011111111111100000000 Input: Pace backward fast Output: -1.0 FL: 11111111100001111111110000 FR: 00001111111110000111111111 RL: 11111111100001111111110000 RR: 00001111111110000111111111 Input: &lt;/pre>;&lt;/span>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap prompt to the LLM. Texts in blue are used for illustration and are not input to LLM.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Following simple and direct commands&lt;/h3>; &lt;p>; We demonstrate in the videos below that the SayTap system can successfully perform tasks where the commands are direct and clear. Although some commands are not covered by the three in-context examples, we are able to guide the LLM to express its internal knowledge from the pre-training phase via the “Gait definition block” (see the second block in our prompt above) in the prompt. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Following unstructured or vague commands&lt;/h3>; &lt;p>; But what is more interesting is SayTap&#39;s ability to process unstructured and vague instructions. With only a little hint in the prompt to connect certain gaits with general impressions of emotions, the robot bounds up and down when hearing exciting messages, like “We are going to a picnic!” Furthermore, it also presents the scenes accurately (eg, moving quickly with its feet barely touching the ground when told the ground is very hot). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We present SayTap, an interactive system for quadrupedal robots that allows users to flexibly craft diverse locomotion behaviors. SayTap introduces desired foot contact patterns as a new interface between natural language and the low-level controller. This new interface is straightforward and flexible, moreover, it allows a robot to follow both direct instructions and commands that do not explicitly state how the robot should react. &lt;/p>; &lt;p>; One interesting direction for future work is to test if commands that imply a specific feeling will allow the LLM to output a desired gait. In the gait definition block shown in the results section above, we provide a sentence that connects a happy mood with bounding gaits. We believe that providing more information can augment the LLM&#39;s interpretations (eg, implied feelings). In our evaluation, the connection between a happy feeling and a bounding gait led the robot to act vividly when following vague human commands. Another interesting direction for future work is to introduce multi-modal inputs, such as videos and audio. Foot contact patterns translated from those signals will, in theory, still work with our pipeline and will unlock many more interesting use cases. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust and Tatsuya Harada conducted this research. This work was conceived and performed while the team was in Google Research and will be continued at Google DeepMind. The authors would like to thank Tingnan Zhang, Linda Luu, Kuang-Huei Lee, Vincent Vanhoucke and Douglas Eck for their valuable discussions and technical support in the experiments.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8352967842060136321/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot; rel=&quot;alternate&quot; title=&quot;SayTap: Language to quadrupedal locomotion&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s72-c/SayTap%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-781660063956467509&lt;/id>;&lt;published>;2023-08-28T09:59:00.003-07:00&lt;/published>;&lt;updated>;2023-08-29T01:37:19.871-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s1100/RO-ViT-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection task&lt;/a>; (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniques such as &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-text pre-training&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;knowledge distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.09106&quot;>;pseudo labeling&lt;/a>;, and frozen models, often employing &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) backbones, have been proposed. With the growing popularity of &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;>;vision transformers&lt;/a>; (ViTs), it is important to explore their potential for building proficient open-vocabulary detectors. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The existing approaches assume the availability of pre-trained &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;vision-language models&lt;/a>; (VLMs) and focus on fine-tuning or &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;distillation&lt;/a>; from these models to address the disparity between image-level pre-training and object-level fine-tuning. However, as VLMs are primarily designed for image-level tasks like classification and retrieval, they do not fully leverage the concept of objects or regions during the pre-training phase. Thus, it could be beneficial for open-vocabulary detection if we build locality information into the image-text pre-training. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.07011&quot;>;RO-ViT: Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce a simple method to pre-train vision transformers in a region-aware manner to improve open-vocabulary detection. In vision transformers, positional embeddings are added to image patches to encode information about the spatial position of each patch within the image. Standard pre-training typically uses full-image positional embeddings, which does not generalize well to detection tasks. Thus, we propose a new positional embedding scheme, called “cropped positional embedding”, that better aligns with the use of region crops in detection fine-tuning. In addition, we replace the &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Neural_networks&quot;>;softmax cross entropy loss&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;>;focal loss&lt;/a>; in contrastive image-text learning, allowing us to learn from more challenging and informative examples. Finally, we leverage recent advances in novel object proposals to enhance open-vocabulary detection fine-tuning, which is motivated by the observation that existing methods often miss novel objects during the proposal stage due to overfitting to foreground categories. We are also releasing the code &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm/rovit&quot;>;here&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Region-aware image-text pre-training&lt;/h2>; &lt;p>; Existing VLMs are trained to match an image as a whole to a text description.然而，我们观察到现有对比预训练方法和开放词汇检测中使用位置嵌入的方式之间存在不匹配。位置嵌入对于转换器很重要，因为它们提供了集合中每个元素来自何处的信息。此信息通常对下游识别和定位任务有用。预训练方法通常在训练期间应用全图像位置嵌入，并对下游任务（例如零样本识别）使用相同的位置嵌入。 However, the recognition occurs at region-level for open-vocabulary detection fine-tuning, which requires the full-image positional embeddings to generalize to regions that they never see during the pre-training. &lt;/p>; &lt;p>; To address this, we propose &lt;em>;cropped positional embeddings&lt;/em>; (CPE). With CPE, we upsample positional embeddings from the image size typical for pre-training, eg, 224x224 pixels, to that typical for detection tasks, eg, 1024x1024 pixels. Then we randomly crop and resize a region, and use it as the image-level positional embeddings during pre-training. The position, scale, and aspect ratio of the crop is randomly sampled. Intuitively, this causes the model to view an image not as a full image in itself, but as a region crop from some larger unknown image. This better matches the downstream use case of detection where recognition occurs at region- rather than image-level. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s952/RO-ViT-img6.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;952&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s16000/RO-ViT-img6.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;For the pre-training, we propose&amp;nbsp;&lt;em>;cropped positional embedding&lt;/em>;&amp;nbsp;(CPE) which randomly crops and resizes a region of positional embeddings instead of using the whole-image positional embedding (PE). In addition, we use focal loss instead of the common softmax cross entropy loss for contrastive learning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also find it beneficial to learn from hard examples with a focal loss.与 softmax 交叉熵损失相比，焦点损失可以更好地控制样本的权重。我们在图像到文本和文本到图像的损失中采用焦点损失并将其替换为 softmax 交叉熵损失。 CPE 和焦点损失都没有引入额外的参数并且计算成本最小。 &lt;/p>; &lt;br />; &lt;h2>;Open-vocabulary detector fine-tuning&lt;/h2>; &lt;p>; An open-vocabulary detector is trained with the detection labels of &#39;base&#39; categories, but needs to detect the union of &#39;base&#39; and &#39;novel&#39; (unlabeled) categories at test time. Despite the backbone features pre-trained from the vast open-vocabulary data, the added detector layers (neck and heads) are newly trained with the downstream detection dataset. Existing approaches often miss novel/unlabeled objects in the object proposal stage because the proposals tend to classify them as background. To remedy this, we leverage recent advances in a novel object proposal method and adopt the localization quality-based objectness (ie, &lt;a href=&quot;https://arxiv.org/abs/2108.06753&quot;>;centerness&lt;/a>; score) instead of object-or-not binary classification score, which is combined with the detection score. During training, we compute the detection scores for each detected region as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; between the region&#39;s embedding (computed via &lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks&quot;>;RoI-Align&lt;/a>; operation) and the text embeddings of the base categories. At test time, we append the text embeddings of novel categories, and the detection score is now computed with the union of the base and novel categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s682/RO-ViT-img4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;682&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s16000/RO-ViT-img4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The pre-trained ViT backbone is transferred to the downstream open-vocabulary detection by replacing the global average pooling with detector heads. The&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks#:~:text=new%20method%20called-,ROIAlign,-%2C%20which%20can%20represent&quot; style=&quot;text-align: left;&quot;>;RoI-Align&lt;/a>;&amp;nbsp;embeddings are matched with the cached category embeddings to obtain the VLM score, which is combined with the detection score into the open-vocabulary detection score.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate RO-ViT on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, our best model achieves 33.6 box&amp;nbsp;&lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>;&amp;nbsp;on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;AP&lt;sub>;r&lt;/sub>;&lt;/a>;) and 32.1 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask AP&lt;sub>;r&lt;/sub>;&lt;/a>;, which outperforms&amp;nbsp;the best existing ViT-based approach OWL-ViT by 8.0 AP&lt;sub>;r&lt;/sub>;&amp;nbsp;and the best CNN-based approach ViLD-Ens by 5.8 mask AP&lt;sub>;r&lt;/sub>;. It also exceeds the performance of many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT outperforms both the state-of-the-art (SOTA)&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2205.06230&quot; style=&quot;text-align: left;&quot;>;ViT-based&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot; style=&quot;text-align: left;&quot;>;CNN-based&lt;/a>;&amp;nbsp;methods on LVIS open-vocabulary detection benchmark. We show mask AP on rare categories (AP&lt;sub>;r&lt;/sub>;) , except for SOTA ViT-based (OwL-ViT) where we show box AP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Apart from evaluating region-level representation through open-vocabulary detection, we evaluate the image-level representation of RO-ViT in image-text retrieval through the &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1505.04870&quot;>;Flickr30K&lt;/a>; benchmarks. Our model with 303M ViT outperforms the state-of-the-art CoCa model with 1B ViT on MS COCO, and is on par on Flickr30K. This shows that our pre-training method not only improves the region-level representation but also the global image-level representation for retrieval. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s1692/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show zero-shot image-text retrieval on MS COCO and Flickr30K benchmarks, and compare with dual-encoder methods. We report recall@1 (top-1 recall) on image-to-text (I2T) and text-to-image (T2I) retrieval tasks. RO-ViT outperforms the state-of-the-art&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot; style=&quot;text-align: left;&quot;>;CoCa&lt;/a>;&amp;nbsp;with the same backbone.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s1476/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;362&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT open-vocabulary detection on LVIS.为了清楚起见，我们仅显示新颖的类别。 RO-ViT detects many novel categories that it has never seen during detection training: “fishbowl”, “sombrero”, “persimmon”, “gargoyle”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualization of positional embeddings&lt;/h2>; &lt;p>; We visualize and compare the learned positional embeddings of RO-ViT with the baseline.每个图块是一个补丁与所有其他补丁的位置嵌入之间的余弦相似度。例如，左上角的图块（以红色标记）可视化了位置（行=1，列=1）的位置嵌入与 2D 中所有其他位置的位置嵌入之间的相似性。补丁的亮度表示不同位置的学习位置嵌入的接近程度。 RO-ViT 在不同的斑块位置形成更明显的簇，在中心斑块周围显示对称的全局模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANdQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s1686/RO-ViT-img1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1686&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANdQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s16000/RO-ViT-img1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Each tile shows the cosine similarity between the positional embedding of the patch (at the indicated row-column position) and the positional embeddings of all other patches. ViT-B/16 backbone is used.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present RO-ViT, a contrastive image-text pre-training framework to bridge the gap between image-level pre-training and open-vocabulary detection fine-tuning.我们的方法简单、可扩展，并且易于应用于任何对比骨干网，计算开销最小，并且不增加参数。 RO-ViT 在 LVIS 开放词汇检测基准和图像文本检索基准上实现了最先进的水平，表明学习的表示不仅在区域级别有益，而且在图像级别也非常有效。我们希望这项研究能够帮助从图像文本预训练的角度进行开放词汇检测的研究，这对区域级和图像级任务都有好处。 &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Dahun Kim, Anelia Angelova, and Weicheng Kuo conducted this work and are now at Google DeepMind.我们要感谢 Google Research 的同事提供的建议和有益的讨论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/781660063956467509/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s72-c/RO-ViT-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3964459643854458124&lt;/id>;&lt;published>;2023-08-25T10:38:00.003-07:00&lt;/published>;&lt;updated>;2023-08-25T10:46:59.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Perception Fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Susanna Ricco and Utsav Prabhu, co-leads, Perception Fairness Team, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI research&lt;/a>; is built on a foundation of collaboration — between teams with diverse backgrounds and expertise, between researchers and product developers, and ultimately with the community at large. The Perception Fairness team drives progress by combining deep subject-matter expertise in both computer vision and machine learning (ML) fairness with direct connections to the researchers building the perception systems that power products across Google and beyond. Together, we are working to intentionally design our systems to be inclusive from the ground up, guided by &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/s948/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;214&quot; data-original-width=&quot;948&quot; height=&quot;144&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/w640-h144/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Perception Fairness research spans the design, development, and deployment of advanced multimodal models including the latest foundation and generative models powering Google&#39;s products.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our team&#39;s mission is to advance the frontiers of fairness and inclusion in multimodal ML systems, especially related to &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;foundation&lt;/a>; models and &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>;. This encompasses core technology components including classification, localization, captioning, retrieval, visual question answering, text-to-image or text-to-video generation, and generative image and video editing. We believe that fairness and inclusion can and should be top-line performance goals for these applications. Our research is focused on unlocking novel analyses and mitigations that enable us to proactively design for these objectives throughout the development cycle. We answer core questions, such as: How can we use ML to responsibly and faithfully model human perception of demographic, cultural, and social identities in order to promote fairness and inclusion? What kinds of system biases (eg, underperforming on images of people with certain skin tones) can we measure and how can we use these metrics to design better algorithms? How can we build more inclusive algorithms and systems and react quickly when failures occur? &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Measuring representation of people in media&lt;/h2>; &lt;p>; ML systems that can edit, curate or create images or videos can affect anyone exposed to their outputs, shaping or reinforcing the beliefs of viewers around the world. Research to reduce representational harms, such as reinforcing stereotypes or denigrating or erasing groups of people, requires a deep understanding of both the content and the &lt;a href=&quot;https://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html&quot;>;societal context&lt;/a>;. It hinges on how different observers perceive themselves, their communities, or how others are represented. There&#39;s considerable debate in the field regarding which social categories should be studied with computational tools and how to do so responsibly. Our research focuses on working toward scalable solutions that are informed by sociology and social psychology, are aligned with human perception, embrace the subjective nature of the problem, and enable nuanced measurement and mitigation. One example is our research on &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;differences in human perception and annotation of skin tone in images&lt;/a>; using the &lt;a href=&quot;skintone.google&quot;>;Monk Skin Tone scale&lt;/a>;. &lt;/p>; &lt;p>; Our tools are also used to study representation in large-scale content collections. Through our Media Understanding for Social Exploration (MUSE) project, we&#39;ve partnered with academic researchers, nonprofit organizations, and major consumer brands to understand patterns in mainstream media and advertising content. We first published this work in 2017, with a co-authored study analyzing &lt;a href=&quot;https://about.google/intl/ALL_au/main/gender-equality-films/&quot;>;gender equity in Hollywood movies&lt;/a>;. Since then, we&#39;ve increased the scale and depth of our analyses. In 2019, we released findings based on &lt;a href=&quot;https://www.thinkwithgoogle.com/feature/diversity-inclusion/&quot;>;over 2.7 million YouTube advertisements&lt;/a>;. In the &lt;a href=&quot;https://blog.google/technology/ai/using-ai-to-study-12-years-of-representation-in-tv/&quot;>;latest study&lt;/a>;, we examine representation across intersections of perceived gender presentation, perceived age, and skin tone in over twelve years of popular US television shows. These studies provide insights for content creators and advertisers and further inform our own research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/s800/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/w640-h360/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration (not actual data) of computational signals that can be analyzed at scale to reveal representational patterns in media collections. [Video Collection / Getty Images]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Moving forward, we&#39;re expanding the ML fairness concepts on which we focus and the domains in which they are responsibly applied. Looking beyond photorealistic images of people, we are working to develop tools that model the representation of communities and cultures in illustrations, abstract depictions of humanoid characters, and even images with no people in them at all. Finally, we need to reason about not just who is depicted, but how they are portrayed — what narrative is communicated through the surrounding image content, the accompanying text, and the broader cultural context. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Analyzing bias properties of perceptual systems&lt;/h2>; &lt;p>; Building advanced ML systems is complex, with multiple stakeholders informing various criteria that decide product behavior. Overall quality has historically been defined and measured using summary statistics (like overall accuracy) over a test dataset as a proxy for user experience. But not all users experience products in the same way. &lt;br />; &lt;/p>; &lt;p>; Perception Fairness enables practical measurement of nuanced system behavior beyond summary statistics, and makes these metrics core to the system quality that directly informs product behaviors and launch decisions. This is often much harder than it seems. Distilling complex bias issues (eg, disparities in performance across intersectional subgroups or instances of stereotype reinforcement) to a small number of metrics without losing important nuance is extremely challenging. Another challenge is balancing the interplay between fairness metrics and other product metrics (eg, user satisfaction, accuracy, latency), which are often phrased as conflicting despite being compatible. It is common for researchers to describe their work as optimizing an &quot;accuracy-fairness&quot; tradeoff when in reality widespread user satisfaction is aligned with meeting fairness and inclusion objectives. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s640/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We built and released the &lt;a href=&quot;https://storage.googleapis.com/openimages/web/extended.html#miap&quot;>;MIAP dataset&lt;/a>; as part of &lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;Open Images&lt;/a>;, leveraging our research on perception of socially relevant concepts and detection of biased behavior in complex systems to create a resource that furthers ML fairness research in computer vision. Original photo credits — left: &lt;a href=&quot;https://www.flickr.com/photos/boston_public_library/8242010414/&quot;>;Boston Public Library&lt;/a>;; middle: &lt;a href=&quot;https://www.flickr.com/photos/jenrobinson/20183915655/&quot;>;jen robinson&lt;/a>;; right: &lt;a href=&quot;https://www.flickr.com/photos/mrgarin/2484859086/&quot;>;Garin Fons&lt;/a>;; all used with permission under the &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;>;CC- BY 2.0 license&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To these ends, our team focuses on two broad research directions. First, democratizing access to well-understood and widely-applicable fairness analysis tooling, engaging partner organizations in adopting them into product workflows, and informing leadership across the company in interpreting results. This work includes developing broad benchmarks, &lt;a href=&quot;https://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html&quot;>;curating widely-useful high-quality test datasets&lt;/a>; and tooling centered around techniques such as sliced analysis and counterfactual testing — often building on the core representation signals work described earlier. Second, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3461702.3462557&quot;>;advancing novel approaches&lt;/a>; towards fairness analytics — including partnering with &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;product efforts&lt;/a>; that may result in breakthrough findings or &lt;a href=&quot;https://services.google.com/fh/files/blogs/bsr-google-cr-api-hria-executive-summary.pdf&quot;>;inform launch strategy&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Advancing AI responsibly&lt;/h2>; &lt;p>; Our work does not stop with analyzing model behavior. Rather, we use this as a jumping-off point for identifying algorithmic improvements in collaboration with other researchers and engineers on product teams. Over the past year we&#39;ve launched upgraded components that power Search and &lt;a href=&quot;https://blog.google/products/photos/google-photos-memories-view/&quot;>;Memories&lt;/a>; features in Google Photos, leading to more consistent performance and drastically improving robustness through added layers that keep mistakes from cascading through the system. We are working on improving ranking algorithms in Google Images to diversify representation. We updated algorithms that may reinforce historical stereotypes, using additional signals responsibly, such that it&#39;s more likely for &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;everyone to see themselves reflected in Search results and find what they&#39;re looking for&lt;/a>;. &lt;/p>; &lt;p>; This work naturally carries over to the world of generative AI, where &lt;a href=&quot;https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text/&quot;>;models can create collections of images or videos seeded from image and text prompts&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;can answer questions about images and videos&lt;/a>;. We&#39;re excited about the potential of these technologies to &lt;a href=&quot;https://blog.google/products/shopping/ai-virtual-try-on-google-shopping/&quot;>;deliver new experiences to users&lt;/a>; and as tools to further our own research. To enable this, we&#39;re collaborating across the research and responsible AI communities to develop guardrails that mitigate failure modes. We&#39;re leveraging our tools for understanding representation to power scalable benchmarks that can be combined with human feedback, and investing in research from pre-training through deployment to steer the models to generate higher quality, more inclusive, and more controllable output. We want these models to inspire people, producing diverse outputs, translating concepts without relying on tropes or stereotypes, and providing consistent behaviors and responses across counterfactual variations of prompts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Opportunities and ongoing work&lt;/h2>; &lt;p>; Despite over a decade of focused work, the field of perception fairness technologies still seems like a nascent and fast-growing space, rife with opportunities for breakthrough techniques. We continue to see opportunities to contribute technical advances backed by interdisciplinary scholarship. The gap between what we can measure in images versus the underlying aspects of human identity and expression is large — closing this gap will require increasingly complex media analytics solutions. Data metrics that indicate true representation, situated in the appropriate context and heeding a diversity of viewpoints, remains an open challenge for us. Can we reach a point where we can reliably identify depictions of nuanced stereotypes, continually update them to reflect an ever-changing society, and discern situations in which they could be offensive? Algorithmic advances driven by human feedback point a promising path forward. &lt;/p>; &lt;p>; Recent focus on AI safety and ethics in the context of modern large model development has spurred new ways of thinking about measuring systemic biases. We are exploring multiple avenues to use these models — along with recent developments in concept-based explainability methods, causal inference methods, and cutting-edge UX research — to quantify and minimize undesired biased behaviors. We look forward to tackling the challenges ahead and developing technology that is built for everybody. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank every member of the Perception Fairness team, and all of our collaborators.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3964459643854458124/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Perception Fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6350776943545232437&lt;/id>;&lt;published>;2023-08-24T15:10:00.000-07:00&lt;/published>;&lt;updated>;2023-08-24T15:10:57.268-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;How to compare a noisy quantum processor to a classical computer&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sergio Boixo and Vadim Smelyanskiy, Principal Scientists, Google Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; A full-scale error-corrected quantum computer will be able to solve some problems that are impossible for classical computers, but building such a device is a huge endeavor. We are proud of the &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestones&lt;/a>; that we have achieved toward a fully error-corrected quantum computer, but that large-scale computer is still some number of years away. Meanwhile, we are using our current noisy quantum processors as flexible platforms for quantum experiments. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In contrast to an error-corrected quantum &lt;em>;computer&lt;/em>;, experiments in noisy quantum &lt;em>;processors&lt;/em>; are currently limited to a few thousand quantum operations or gates, before noise degrades the quantum state. In 2019 we implemented a specific computational task called &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1666-5&quot;>;random circuit sampling&lt;/a>; on our quantum processor &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;and showed&lt;/a>; for the first time that it outperformed state-of-the-art classical supercomputing. &lt;/p>; &lt;p>; Although they have not yet reached beyond-classical capabilities, we have also used our processors to observe novel physical phenomena, such as &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04257-w&quot;>;time crystals&lt;/a>; and &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;Majorana edge modes&lt;/a>;, and have made new experimental discoveries, such as robust &lt;a href=&quot;https://ai.googleblog.com/2022/12/formation-of-robust-bound-states-of.html&quot;>;bound states&lt;/a>; of interacting photons and the &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;noise-resilience&lt;/a>; of Majorana edge modes of Floquet evolutions. &lt;/p>; &lt;p>; We expect that even in this intermediate, noisy regime, we will find applications for the quantum processors in which useful quantum experiments can be performed much faster than can be calculated on classical supercomputers — we call these &quot;computational applications&quot; of the quantum processors. No one has yet demonstrated such a beyond-classical computational application. So as we aim to achieve this milestone, the question is: What is the best way to compare a quantum experiment run on such a quantum processor to the computational cost of a classical application? &lt;/p>; &lt;p>; We already know how to compare an error-corrected quantum algorithm to a classical algorithm. In that case, the field of &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity&quot;>;computational complexity&lt;/a>; tells us that we can compare their respective computational costs — that is, the number of operations required to accomplish the task. But with our current experimental quantum processors, the situation is not so well defined. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments&lt;/a>;”, we provide a framework for measuring the computational cost of a quantum experiment, introducing the experiment&#39;s “effective quantum volume”, which is the number of quantum operations or gates that contribute to a measurement outcome. We apply this framework to evaluate the computational cost of three recent experiments: our &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;random circuit sampling&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;experiment&lt;/a>;, our&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>; experiment measuring quantities known as “out of time order correlators” (OTOCs)&lt;/a>;, and a &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;recent experiment on a Floquet evolution&lt;/a>; related to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;>;Ising model&lt;/a>;. We are particularly excited about OTOCs because they provide a direct way to experimentally measure the effective quantum volume of a circuit (a sequence of quantum gates or operations), which is itself a computationally difficult task for a classical computer to estimate precisely. OTOCs are also important in &lt;a href=&quot;https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance&quot;>;nuclear magnetic resonance&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Electron_paramagnetic_resonance&quot;>;electron spin resonance spectroscopy&lt;/a>;. Therefore, we believe that OTOC experiments are a promising candidate for a first-ever computational application of quantum processors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/s1280/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;1280&quot; height=&quot;297&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/w640-h297/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Plot of computational cost and impact of some recent quantum experiments. While some (eg, &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;QC-QMC 2022&lt;/a>;) have had high impact and others (eg, &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;RCS 2023&lt;/a>;) have had high computational cost, none have yet been both useful and hard enough to be considered a “computational application.” We hypothesize that our future OTOC experiment could be the first to pass this threshold. Other experiments plotted are referenced in the text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Random circuit sampling: Evaluating the computational cost of a noisy circuit&lt;/h2>; &lt;p>; When it comes to running a quantum circuit on a noisy quantum processor, there are two competing considerations. On one hand, we aim to do something that is difficult to achieve classically. The computational cost — the number of operations required to accomplish the task on a classical computer — depends on the quantum circuit&#39;s &lt;em>;effective quantum volume&lt;/em>;: the larger the volume, the higher the computational cost, and the more a quantum processor can outperform a classical one. &lt;/p>; &lt;p>; But on the other hand, on a noisy processor, each quantum gate can introduce an error to the calculation. The more operations, the higher the error, and the lower the fidelity of the quantum circuit in measuring a quantity of interest. Under this consideration, we might prefer simpler circuits with a smaller effective volume, but these are easily simulated by classical computers. The balance of these competing considerations, which we want to maximize, is called the &quot;computational resource&quot;, shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/s973/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;653&quot; data-original-width=&quot;973&quot; height=&quot;430&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/w640-h430/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Graph of the tradeoff between quantum volume and noise in a quantum circuit, captured in a quantity called the “computational resource.” For a noisy quantum circuit, this will initially increase with the computational cost, but eventually, noise will overrun the circuit and cause it to decrease. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; We can see how these competing considerations play out in a simple &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;“hello world” program&lt;/a>; for quantum processors, known as random circuit sampling (RCS), which was the first demonstration of a quantum processor outperforming a classical computer. Any error in any gate is likely to make this experiment fail. Inevitably, this is a hard experiment to achieve with significant fidelity, and thus it also serves as a benchmark of system fidelity. But it also corresponds to the highest known computational cost achievable by a quantum processor. We recently reported the &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;most powerful RCS&lt;/a>; experiment performed to date, with a low measured experimental fidelity of 1.7x10&lt;sup>;-3&lt;/sup>;, and a high theoretical computational cost of ~10&lt;sup>;23&lt;/sup>;. These quantum circuits had 700 two-qubit gates. We estimate that this experiment would take ~47 years to simulate in the world&#39;s largest supercomputer. While this checks one of the two boxes needed for a computational application — it outperforms a classical supercomputer — it is not a particularly useful application &lt;em>;per se&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;OTOCs and Floquet evolution: The effective quantum volume of a local observable&lt;/h2>; &lt;p>; There are many open questions in quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-body_problem&quot;>;many-body physics&lt;/a>; that are classically intractable, so running some of these experiments on our quantum processor has great potential. We typically think of these experiments a bit differently than we do the RCS experiment. Rather than measuring the quantum state of all qubits at the end of the experiment, we are usually concerned with more specific, local physical observables. Because not every operation in the circuit necessarily impacts the observable, a local observable&#39;s effective quantum volume might be smaller than that of the full circuit needed to run the experiment. &lt;/p>; &lt;p>; We can understand this by applying the concept of a light cone from &lt;a href=&quot;https://en.wikipedia.org/wiki/Special_relativity&quot;>;relativity&lt;/a>;, which determines which events in space-time can be causally connected: some events cannot possibly influence one another because information takes time to propagate between them. We say that two such events are outside their respective light cones. In a quantum experiment, we replace the light cone with something called a “butterfly cone,” where the growth of the cone is determined by the butterfly speed — the speed with which information spreads throughout the system. (This speed is characterized by measuring OTOCs, discussed later.) The effective quantum volume of a local observable is essentially the volume of the butterfly cone, including only the quantum operations that are causally connected to the observable. So, the faster information spreads in a system, the larger the effective volume and therefore the harder it is to simulate classically. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/s1392/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1392&quot; data-original-width=&quot;1092&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/w502-h640/image2.png&quot; width=&quot;502&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A depiction of the effective volume V&lt;sub>;eff&lt;/sub>; of the gates contributing to the local observable B. A related quantity called the effective area A&lt;sub>;eff&lt;/sub>; is represented by the cross-section of the plane and the cone. The perimeter of the base corresponds to the front of information travel that moves with the butterfly velocity v&lt;sub>;B&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; We apply this framework to a recent &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;experiment&lt;/a>; implementing a so-called Floquet Ising model, a physical model related to the time crystal and Majorana experiments. From the data of this experiment, one can directly estimate an effective fidelity of 0.37 for the largest circuits. With the measured gate error rate of ~1%, this gives an estimated effective volume of ~100. This is much smaller than the light cone, which included two thousand gates on 127 qubits. So, the butterfly velocity of this experiment is quite small. Indeed, we argue that the effective volume covers only ~28 qubits, not 127, using numerical simulations that obtain a larger precision than the experiment. This small effective volume has also been &lt;a href=&quot;https://arxiv.org/abs/2306.17839&quot;>;corroborated&lt;/a>; with the OTOC technique. Although this was a deep circuit, the estimated computational cost is 5x10&lt;sup>;11&lt;/sup>;, almost one trillion times less than the recent RCS experiment. Correspondingly, this experiment can be &lt;a href=&quot;https://arxiv.org/abs/2306.15970&quot;>;simulated&lt;/a>; in less than a second per data point on a single A100 GPU. So, while this is certainly a useful application, it does not fulfill the second requirement of a computational application: substantially outperforming a classical simulation. &lt;/p>; &lt;p>; Information scrambling experiments with OTOCs are a promising avenue for a computational application. OTOCs can tell us important physical information about a system, such as the butterfly velocity, which is critical for precisely measuring the effective quantum volume of a circuit. OTOC experiments with fast entangling gates offer a potential path for a first beyond-classical demonstration of a computational application with a quantum processor. Indeed, in our &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;experiment&lt;/a>; from 2021 we achieved an effective fidelity of F&lt;sub>;eff &lt;/sub>;~ 0.06 with an experimental signal-to-noise ratio of ~1, corresponding to an effective volume of ~250 gates and a computational cost of 2x10&lt;sup>;12&lt;/sup>;. &lt;/p>; &lt;p>; While these early OTOC experiments are not sufficiently complex to outperform classical simulations, there is a deep physical reason why OTOC experiments are good candidates for the first demonstration of a computational application. Most of the interesting quantum phenomena accessible to near-term quantum processors that are hard to simulate classically correspond to a quantum circuit exploring many, many quantum energy levels. Such evolutions are typically chaotic and standard time-order correlators (TOC) decay very quickly to a purely random average in this regime. There is no experimental signal left. This does not happen for &lt;a href=&quot;https://arxiv.org/abs/2101.08870&quot;>;OTOC measurements&lt;/a>;, which allows us to grow complexity at will, only limited by the error per gate. We anticipate that a reduction of the error rate by half would double the computational cost, pushing this experiment to the beyond-classical regime. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Using the effective quantum volume framework we have developed, we have determined the computational cost of our RCS and OTOC experiments, as well as a recent Floquet evolution experiment. While none of these meet the requirements yet for a computational application, we expect that with improved error rates, an OTOC experiment will be the first beyond-classical, useful application of a quantum processor. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6350776943545232437/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html&quot; rel=&quot;alternate&quot; title=&quot;How to compare a noisy quantum processor to a classical computer&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3589347607411181063&lt;/id>;&lt;published>;2023-08-24T12:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-24T12:33:37.720-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Teaching language models to reason algorithmically&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hattie Zhou, Graduate Student at MILA, Hanie Sedghi, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs), such as &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have shown impressive progress in recent years, which have been driven by &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;scaling up models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;>;training data sizes&lt;/a>;. Nonetheless, a long standing debate has been whether LLMs can reason symbolically (ie, manipulating symbols based on logical rules). For example, LLMs are able to perform simple arithmetic operations when numbers are small, but struggle to perform with large numbers. This suggests that LLMs have not learned the underlying rules needed to perform these arithmetic operations. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While neural networks have powerful &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;>;pattern matching capabilities&lt;/a>;, they are prone to overfitting to spurious statistical patterns in the data. This does not hinder good performance when the training data is large and diverse and the evaluation is in-distribution. However, for tasks that require rule-based reasoning (such as addition), LLMs struggle with out-of-distribution generalization as spurious correlations in the training data are often much easier to exploit than the true rule-based solution. As a result, despite significant progress in a variety of natural language processing tasks, performance on simple arithmetic tasks like addition has remained a challenge. Even with modest improvement of &lt;a href=&quot;https://openai.com/research/gpt-4&quot;>;GPT-4&lt;/a>; on the &lt;a href=&quot;https://arxiv.org/abs/2103.03874&quot;>;MATH&lt;/a>; dataset, &lt;a href=&quot;https://arxiv.org/abs/2303.12712&quot;>;errors are still largely due to arithmetic and calculation mistakes&lt;/a>;. Thus, an important question is whether LLMs are capable of algorithmic reasoning, which involves solving a task by applying a set of abstract rules that define the algorithm. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;Teaching Algorithmic Reasoning via In-Context Learning&lt;/a>;”, we describe an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; to enable algorithmic reasoning capabilities in LLMs. In-context learning refers to a model&#39;s ability to perform a task after seeing a few examples of it within the context of the model. The task is specified to the model using a prompt, without the need for weight updates. We also present a novel algorithmic prompting technique that enables general purpose language models to achieve strong &lt;a href=&quot;https://arxiv.org/abs/2207.04901&quot;>;generalization&lt;/a>; on arithmetic problems that are more difficult than those seen in the prompt. Finally, we demonstrate that a model can reliably execute algorithms on out-of-distribution examples with an appropriate choice of prompting strategy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/s1200/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200&quot; height=&quot;89&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/w640-h89/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By providing algorithmic prompts, we can teach a model the rules of arithmetic via in-context learning. In this example, the LLM (word predictor) outputs the correct answer when prompted with an easy addition question (eg, 267+197), but fails when asked a similar addition question with longer digits. However, when the more difficult question is appended with an algorithmic prompt for addition (blue box with white +&lt;strong>; &lt;/strong>;shown below the word predictor), the model is able to answer correctly. Moreover, the model is capable of simulating the multiplication algorithm (&lt;strong>;X&lt;/strong>;) by composing a series of addition calculations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Teaching an algorithm as a skill&lt;/h2>; &lt;p>; In order to teach a model an algorithm as a skill, we develop algorithmic prompting, which builds upon other rationale-augmented approaches (eg, &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>;). Algorithmic prompting extracts algorithmic reasoning abilities from LLMs, and has two notable distinctions compared to other prompting approaches: (1) it solves tasks by outputting the steps needed for an algorithmic solution, and (2) it explains each algorithmic step with sufficient detail so there is no room for misinterpretation by the LLM. &lt;/p>; &lt;p>; To gain intuition for algorithmic prompting, let&#39;s consider the task of two-number addition.在便笺式提示中，我们从右到左处理每个数字，并在每一步跟踪进位值（即，如果当前数字大于 9，我们将在下一个数字上加 1）。然而，在只看到几个进位值的例子后，进位规则就很模糊了。我们发现，包含显式方程来描述进位规则有助于模型关注相关细节并更准确地解释提示。我们利用这种洞察力开发了两个数字加法的算法提示，其中我们为每个计算步骤提供了明确的方程，并以明确的格式描述了各种索引操作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/s1584/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1584&quot; height=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/w640-h505/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of various prompt strategies for addition.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using only three prompt examples of addition with answer length up to five digits, we evaluate performance on additions of up to 19 digits.准确性是通过在整个答案范围内均匀采样的 2,000 个示例来衡量的。如下所示，使用算法提示可以在比提示中看到的时间更长的时间内保持问题的高精度，这表明该模型确实通过执行与输入无关的算法来解决任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s1600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;548&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy on addition questions of increasing length for different prompting methods. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Leveraging algorithmic skills as tool use&lt;/h2>; &lt;p>; To evaluate if the model can leverage algorithmic reasoning in a broader reasoning process, we evaluate performance using grade school math word problems (&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;GSM8k&lt;/a>;). We specifically attempt to replace addition calculations from GSM8k with an algorithmic solution. &lt;/p>; &lt;p>; Motivated by context length limitations and possible interference between different algorithms, we explore a strategy where differently-prompted models interact with one another to solve complex tasks. In the context of GSM8k, we have one model that specializes in informal mathematical reasoning using &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain-of-thought prompting&lt;/a>;, and a second model that specializes in addition using &lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;algorithmic prompting&lt;/a>;.非正式数学推理模型被提示输出专门的标记，以便调用加法提示模型来执行算术步骤。我们提取标记之间的查询，将它们发送到加法模型并将答案返回到第一个模型，之后第一个模型继续其输出。我们使用 GSM8k (GSM8k-Hard) 中的一个难题来评估我们的方法，其中我们随机选择 50 个仅加法问题并增加问题中的数值。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/s1268/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;1268&quot; height=&quot;167&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/w640-h167/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example from the GSM8k-Hard dataset. The chain-of-thought prompt is augmented with brackets to indicate when an algorithmic call should be performed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We find that using separate contexts and models with specialized prompts is an effective way to tackle GSM8k-Hard.下面，我们观察到使用算法调用加法的模型的性能是思想链基线的 2.3 倍。最后，该策略提供了一个通过情境学习促进专门从事不同技能的法学硕士之间的互动来解决复杂任务的示例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/s716/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;716&quot; height=&quot;108&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/w400-h108/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Chain-of-thought (CoT) performance on GSM8k-Hard with or without algorithmic call.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; and a novel algorithmic prompting technique to unlock algorithmic reasoning abilities in LLMs. Our results suggest that it may be possible to transform longer context into better reasoning performance by providing more detailed explanations. Thus, these findings point to the ability of using or otherwise simulating long contexts and generating more informative rationales as promising research directions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Behnam Neyshabur, Azade Nova, Hugo Larochelle and Aaron Courville for their valuable contributions to the paper and great feedback on the blog.我们感谢 Tom Small 创建本文中的动画。 This work was done during Hattie Zhou&#39;s internship at Google Research.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3589347607411181063/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot; rel=&quot;alternate&quot; title=&quot;Teaching language models to reason algorithmically&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6641308922483151364&lt;/id>;&lt;published>;2023-08-22T11:47:00.004-07:00&lt;/published>;&lt;updated>;2023-08-24T10:34:33.888-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Language to rewards for robotic skill synthesis&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Wenhao Yu and Fei Xia, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s320/Language%20to%20reward.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Empowering end-users to interactively teach robots to perform novel tasks is a crucial capability for their successful integration into real-world applications. For example, a user may want to teach a robot dog to perform a new trick, or teach a manipulator robot how to organize a lunch box based on user preferences. The recent advancements in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) pre-trained on extensive internet data have shown a promising path towards achieving this goal. Indeed, researchers have explored diverse ways of leveraging LLMs for robotics, from &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;step-by-step planning&lt;/a>; and &lt;a href=&quot;https://interactive-language.github.io/&quot;>;goal-oriented dialogue&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2022/11/robots-that-write-their-own-code.html&quot;>;robot-code-writing agents&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While these methods impart new modes of compositional generalization, they focus on using language to link together new behaviors from an &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing library of control primitives&lt;/a>; that are either manually engineered or learned &lt;em>;a priori&lt;/em>;. Despite having internal knowledge about robot motions, LLMs struggle to directly output low-level robot commands due to the limited availability of relevant training data. As a result, the expression of these methods are bottlenecked by the breadth of the available primitives, the design of which often requires extensive expert knowledge or massive data collection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.08647&quot;>;Language to Rewards for Robotic Skill Synthesis&lt;/a>;”, we propose an approach to enable users to teach robots novel actions through natural language input. To do so, we leverage reward functions as an interface that bridges the gap between language and low-level robot actions. We posit that reward functions provide an ideal interface for such tasks given their richness in semantics, modularity, and interpretability. They also provide a direct connection to low-level policies through black-box optimization or reinforcement learning (RL). We developed a language-to-reward system that leverages LLMs to translate natural language user instructions into reward-specifying code and then applies &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; to find optimal low-level robot actions that maximize the generated reward function. We demonstrate our language-to-reward system on a variety of robotic control tasks in simulation using a quadruped robot and a dexterous manipulator robot. We further validate our method on a physical robot manipulator. &lt;/p>; &lt;p>; The language-to-reward system consists of two core components: (1) a Reward Translator, and (2) a Motion Controller&lt;em>;. &lt;/em>;The&lt;em>; &lt;/em>;Reward Translator maps natural language instruction from users to reward functions represented as &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;python code&lt;/a>;. The Motion Controller optimizes the given reward function using &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;receding horizon optimization&lt;/a>; to find the optimal low-level robot actions, such as the amount of torque that should be applied to each robot motor. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s720/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLMs cannot directly generate low-level robotic actions due to lack of data in pre-training dataset. We propose to use reward functions to bridge the gap between language and low-level robot actions, and enable novel complex robot motions from natural language instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Reward Translator: Translating user instructions to reward functions&lt;/h2>; &lt;p>; The Reward Translator module was built with the goal of mapping natural language user instructions to reward functions. Reward tuning is highly domain-specific and requires expert knowledge, so it was not surprising to us when we found that LLMs trained on generic language datasets are unable to directly generate a reward function for a specific hardware. To address this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;in-context learning&lt;/a>; ability of LLMs. Furthermore, we split the Reward Translator into two sub-modules: &lt;em>;Motion Descriptor&lt;/em>; and &lt;em>;Reward Coder&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion Descriptor&lt;/h3>; &lt;p>; First, we design a Motion Descriptor that interprets input from a user and expands it into a natural language description of the desired robot motion following a predefined template. This Motion Descriptor turns potentially ambiguous or vague user instructions into more specific and descriptive robot motions, making the reward coding task more stable. Moreover, users interact with the system through the motion description field, so this also provides a more interpretable interface for users compared to directly showing the reward function. &lt;/p>; &lt;p>; To create the Motion Descriptor, we use an LLM to translate the user input into a detailed description of the desired robot motion. We design &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_motion_descriptor.txt&quot;>;prompts&lt;/a>; that guide the LLMs to output the motion description with the right amount of details and format. By translating a vague user instruction into a more detailed description, we are able to more reliably generate the reward function with our system. This idea can also be potentially applied more generally beyond robotics tasks, and is relevant to &lt;a href=&quot;https://innermonologue.github.io/&quot;>;Inner-Monologue&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought prompting&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reward Coder &lt;/h3>; &lt;p>; In the second stage, we use the same LLM from Motion Descriptor for Reward Coder, which translates generated motion description into the reward function. Reward functions are represented using python code to benefit from the LLMs&#39; knowledge of reward, coding, and code structure. &lt;/p>; &lt;p>; Ideally, we would like to use an LLM to directly generate a reward function &lt;i>;R&lt;/i>; (&lt;i>;s&lt;/i>;, &lt;i>;t&lt;/i>;) that maps the robot state &lt;i>;s&lt;/i>; and time &lt;i>;t&lt;/i>; into a scalar reward value. However, generating the correct reward function from scratch is still a challenging problem for LLMs and correcting the errors requires the user to understand the generated code to provide the right feedback. As such, we pre-define a set of reward terms that are commonly used for the robot of interest and allow LLMs to composite different reward terms to formulate the final reward function. To achieve this, we design a &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_reward_coder.txt&quot;>;prompt&lt;/a>; that specifies the reward terms and guide the LLM to generate the correct reward function for the task.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s1293/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;1293&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The internal structure of the Reward Translator, which is tasked to map user inputs to reward functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Motion Controller: Translating reward functions to robot actions&lt;/h2>; &lt;p>; The Motion Controller takes the reward function generated by the Reward Translator and synthesizes a controller that maps robot observation to low-level robot actions. To do this, we formulate the controller synthesis problem as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP), which can be solved using different strategies, including RL, &lt;a href=&quot;https://en.wikipedia.org/wiki/Trajectory_optimization&quot;>;offline trajectory optimization&lt;/a>;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;model predictive control&lt;/a>; (MPC). Specifically, we use an open-source implementation based on the &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; (MJPC). &lt;/p>; &lt;p>; MJPC has demonstrated the interactive creation of diverse behaviors, such as legged locomotion, grasping, and finger-gaiting, while supporting multiple planning algorithms, such as &lt;a href=&quot;https://homes.cs.washington.edu/~todorov/papers/TodorovACC05.pdf&quot;>;iterative linear–quadratic–Gaussian&lt;/a>; (iLQG) and &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;predictive sampling&lt;/a>;. More importantly, the frequent re-planning in MJPC empowers its robustness to uncertainties in the system and enables an interactive motion synthesis and correction system when combined with LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples &lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Robot dog&lt;/h3>; &lt;p>; In the first example, we apply the language-to-reward system to a simulated quadruped robot and teach it to perform various skills. For each skill, the user will provide a concise instruction to the system, which will then synthesize the robot motion by using reward functions as an intermediate interface. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_quadruped_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Dexterous manipulator&lt;/h3>; &lt;p>; We then apply the language-to-reward system to a dexterous manipulator robot to perform a variety of manipulation tasks. The dexterous manipulator has 27 degrees of freedom, which is very challenging to control. Many of these tasks require manipulation skills beyond grasping, making it difficult for pre-designed primitives to work. We also include an example where the user can interactively instruct the robot to place an apple inside a drawer. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_manipulation_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Validation on real robots&lt;/h2>; &lt;p>; We also validate the language-to-reward method using a real-world manipulation robot to perform tasks such as picking up objects and opening a drawer. To perform the optimization in Motion Controller, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/ARTag&quot;>;AprilTag&lt;/a>;, a fiducial marker system, and &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;F-VLM&lt;/a>;, an open-vocabulary object detection tool, to identify the position of the table and objects being manipulated. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;yes&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/real/l2r_demo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we describe a new paradigm for interfacing an LLM with a robot through reward functions, powered by a low-level model predictive control tool, MuJoCo MPC. Using reward functions as the interface enables LLMs to work in a semantic-rich space that plays to the strengths of LLMs, while ensuring the expressiveness of the resulting controller. To further improve the performance of the system, we propose to use a structured motion description template to better extract internal knowledge about robot motions from LLMs. We demonstrate our proposed system on two simulated robot platforms and one real robot for both locomotion and manipulation tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our co-authors Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, and Yuval Tassa for their help and support in various aspects of the project. We would also like to acknowledge Ken Caluwaerts, Kristian Hartikainen, Steven Bohez, Carolina Parada, Marc Toussaint, and the teams at Google DeepMind for their feedback and contributions.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6641308922483151364/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot; rel=&quot;alternate&quot; title=&quot;Language to rewards for robotic skill synthesis&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s72-c/Language%20to%20reward.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6990000750711940626&lt;/id>;&lt;published>;2023-08-21T00:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-21T00:35:57.579-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Interspeech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at Interspeech 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s1075/Interspeech2023-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the 24th Annual &lt;a href=&quot;https://interspeech2023.org/&quot;>;Conference of the International Speech Communication Association&lt;/a>; (INTERSPEECH 2023) is being held in Dublin, Ireland, representing one of the world&#39;s most extensive conferences on research and technology of spoken language understanding and processing. Experts in speech-related research fields gather to take part in oral presentations and poster sessions and to build collaborations across the globe. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to be a &lt;a href=&quot;https://interspeech2023.org/sponsorship-exhibition/sponsors/&quot;>;Platinum Sponsor&lt;/a>; of INTERSPEECH 2023, where we will be showcasing more than 20 research publications and supporting a number of workshops and special sessions. We welcome in-person attendees to drop by the Google Research booth to meet our researchers and participate in Q&amp;amp;As and demonstrations of some of our latest speech technologies, which help to improve accessibility and provide convenience in communication for billions of users. In addition, online attendees are encouraged to visit our &lt;a href=&quot;http://topia.io/google-research&quot;>;virtual booth in Topia&lt;/a>; where you can get up-to-date information on research and opportunities at Google. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). You can also learn more about the Google research being presented at INTERSPEECH 2023 below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; ISCA 董事会、技术委员会主席：&lt;strong>;&lt;em>;Bhuvana Ramabhadran &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; 区域主席包括：&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;语音和音频信号分析：&lt;strong>;&lt;em>;Richard Rose&lt;/em>; &lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;语音合成和口语生成：&lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp ;特殊区域：&lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;卫星事件&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/competition2023.html&quot;>;2023 年 VoxCeleb 演讲者识别挑战赛&lt;/a>; (VoxSRC-23)&lt; br />; 主办方包括：&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ssw2023.org/&quot;>;ISCA 语音合成研讨会&lt;/ a>; (SSW12)&lt;br />; 演讲者包括：&lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;主题演讲 – ISCA 奖章获得者&lt;/ h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/narayanan23_interspeech.pdf&quot;>;桥梁语音科学和技术 — 现在与未来&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Shrikanth Narayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;调查谈话&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; AI时代的语音压缩&lt;br />;演讲者：&lt;strong>;&lt;em>;Jan Skoglund&lt;/em>;&lt;br />;&lt;/ strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;特别会议文件&lt;​​/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www .isca-speech.org/archive/pdfs/interspeech_2023/rose23_interspeech.pdf&quot;>;用于在重叠语音上微调 ASR 模型的级联编码器&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Richard Rose&lt;/b>;， &lt;b>;奥斯卡·张&lt;/b>;，&lt;b>;奥利维尔·西奥汉&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs /interspeech_2023/erdogan23_interspeech.pdf&quot;>;TokenSplit：使用离散语音表示进行直接、精炼和转录条件语音分离和识别&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Hakan Erdogan&lt;/b>;，&lt;b >;Scott Wisdom&lt;/b>;、Xuankai Chang*、&lt;b>;Zalán Borsos&lt;/b>;、&lt;b>;Marco Tagliasacchi&lt;/b>;、&lt;b>;Neil Zeghidour&lt;/b>;、&lt;b>;John R. Hershey&lt; /b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;论文&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23d_interspeech.pdf&quot;>;DeePMOS：深度后验平均意见-言语得分&lt;/a>;&lt;br />; &lt;em>;梁新宇， Fredrik Cumlin、&lt;strong>; Christian Schüldt&lt;/strong>;、Saikat Chatterjee&lt;strong>;&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca- peech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf&quot;>;O-1：使用 Oracle 进行自我训练和 1-最佳假设&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Murali Karthick Baskar&lt;/b>; 、&lt;b>;安德鲁·罗森伯格&lt;/b>;、&lt;b>;布瓦纳·拉马巴德兰&lt;/b>;、&lt;b>;卡蒂克·奥德卡西&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot; https://www.isca-speech.org/archive/pdfs/interspeech_2023/huo23b_interspeech.pdf&quot;>;利用特征融合方法重新研究语音基础模型的高效迁移学习&lt;/a>;&lt;br />; &lt;em>;&lt; b>;Zhouyuan Huor&lt;/b>;、&lt;b>;Khe Chai Sim&lt;/b>;、&lt;b>;Dongseong Hwang&lt;/b>;、&lt;b>;Tsendsuren Munkhdalai&lt;/b>;、&lt;b>;Tara N. Sainath&lt;/b>; >;,&lt;b>;佩德罗·莫雷诺&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/camp23_interspeech .pdf&quot;>;MOS 与 AB：使用集群标准误差可靠地评估文本转语音系统&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Joshua Camp&lt;/b>;、&lt;b>;Tom Kenter&lt;/b >;、&lt;b>;列夫·芬克尔斯坦&lt;/b>;、&lt;b>;罗布·克拉克&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech .org/archive/pdfs/interspeech_2023/gong23c_interspeech.pdf&quot;>;LanSER：语言模型支持的语音情感识别&lt;/a>;&lt;br />; &lt;em>;Taesik Kong，&lt;strong>; Josh Belanich&lt;/strong>;，&lt;strong>; Krishna Somandepalli&lt;/strong>;、&lt;strong>;Arsha Nagrani&lt;/strong>;、&lt;strong>;Brian Eoff&lt;/strong>;、&lt;strong>;Brendan Jou&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23fa_interspeech.pdf&quot;>;基于一致性的流媒体 ASR 的模块化域适应&lt;/a>;&lt;br />; &lt;em>;&lt; b>;李秋佳&lt;/b>;、&lt;b>;李波&lt;/b>;、&lt;b>;Dongseong Hwang&lt;/b>;、&lt;b>;Tara N. Sainath&lt;/b>;、&lt;b>;Pedro M.Mengibar&lt;/b>; b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/panchapagesan23_interspeech.pdf&quot;>;关于训练神经残余声学回声抑制器改进的 ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Sankaran Panchapagesan&lt;/b>;、&lt;b>;Turaj Zakizadeh Shabestary&lt;/b>;、&lt;b>;Arun Narayanan&lt;/b>;&lt;br />;&lt;/em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf&quot;>;MD3：对话的多方言数据集&lt;/a>;&lt; br />; &lt;em>;&lt;b>;雅各布·爱森斯坦&lt;/b>;、&lt;b>;Vinodkumar Prabhakaran&lt;/b>;、&lt;b>;克拉拉·里维拉&lt;/b>;、Dorottya Demszky、Devyani Sharma&lt;br />;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf&quot;>;双模式 NAM：端到端的有效 Top-K 上下文注入结束 ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zelin Wu&lt;/b>;、&lt;b>;Tsendsuren Munkhdalai&lt;/b>;、&lt;b>;Pat Rondon&lt;/b>;、&lt;b>;Golan Pundak&lt;/b>; b>;、&lt;b>;Khe Chai Sim&lt;/b>;、&lt;b>;Christopher Li&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca -speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf&quot;>;使用文本注入来改进语音中个人标识符的识别&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yochai Blau&lt;/b>;，&lt;b >;Rohan Agrawal&lt;/b>;、&lt;b>;Lior Madmony&lt;/b>;、&lt;b>;Gary Wang&lt;/b>;、&lt;b>;Andrew Rosenberg&lt;/b>;、&lt;b>;陈哲怀&lt;/b>;、&lt;b >;佐里克·格赫曼&lt;/b>;、&lt;b>;格纳迪·别廖兹金&lt;/b>;、&lt;b>;帕里萨·哈加尼&lt;/b>;、&lt;b>;布瓦纳·拉玛巴德兰&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23j_interspeech.pdf&quot;>;如何估计预训练语音模型的模型可迁移性？&lt;/a>;&lt;br />; &lt;em>;Zih-Ching Chen、Chao-Han Huck Yang*、&lt;strong>;李波&lt;/strong>;、&lt;strong>;张宇&lt;/strong>;、&lt;strong>;陈南新&lt;/strong>;、&lt;strong>;Shuo- yiin Chang&lt;/strong>;、&lt;strong>;Rohit Prabhavalkar、&lt;/strong>;李鸿毅、&lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf&quot;>;在不对齐的情况下改进联合语音文本表示&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Cal Peyser &lt;/b>;、&lt;b>;钟猛&lt;/b>;、&lt;b>;胡可&lt;/b>;、&lt;b>;Rohit Prabhavalkar&lt;/b>;、&lt;b>;安德鲁·罗森伯格&lt;/b>;、&lt;b>;Tara N .Sainath&lt;/b>;、Michael Picheny、Kyunghyun Cho &lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/ bijwadia23_interspeech.pdf&quot;>;语音模型中大写和轮流预测的文本注入&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shaan Bijwadia&lt;/b>;，&lt;b>;Shuo-yiin Chang&lt;/b>;， &lt;b>;王蔚然&lt;/b>;、&lt;b>;孟忠&lt;/b>;、&lt;b>;张浩&lt;/b>;、&lt;b>;Tara N. Sainath&lt;/b>;&lt;br />;&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23_interspeech.pdf&quot;>;用于设备上语音到语音转换的流式 Parrotron&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;、&lt;b>;Fadi Biadsy&lt;/b>;、&lt;b>;张霞&lt;/b>;、&lt;b>;蒋立阳&lt;/b>;、&lt;b>;凤凰草地鹨&lt;/b>;、&lt;b>;Shivani Agrawal&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/ pdfs/interspeech_2023/huang23b_interspeech.pdf&quot;>;使用双向语言模型的语义分割改进了长格式 ASR&lt;/a>;&lt;br />; &lt;strong>;W. Ronny Huang&lt;/strong>;、&lt;strong>;张浩&lt;/strong>;、&lt;strong>;Shankar Kumar&lt;/strong>;、&lt;strong>;张硕仪&lt;/strong>;、&lt;strong>;Tara N. Sainath&lt;br />; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/taguchi23_interspeech.pdf&quot;>;通用自动语音转录为国际音标&lt;/ a>;&lt;br />; &lt;em>;田口千寻、酒井佑介、&lt;strong>;Parisa Haghani&lt;/strong>;、姜大卫&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf&quot;>;用于流式多语言 ASR 的混合专家一致性&lt;/a>;&lt;br />; &lt;em>;&lt;b>;胡柯&lt;/b>; , &lt;b>;李波&lt;/b>;,&lt;b>;Tara N. Sainath&lt;/b>;,&lt;b>;张宇&lt;/b>;,&lt;b>;Francoise Beaufays&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23c_interspeech.pdf&quot;>;手机实时频谱图反演&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;、&lt;b>;Marco Tagliasacchi&lt;/b>;、&lt;b>;李云鹏&lt;/b>;、&lt;b>;蒋立阳&lt;/b>;、&lt;b>;张霞&lt;/ b>;、&lt;b>;Fadi Biadsy&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/ rybakov23b_interspeech.pdf&quot;>;自动语音识别的 2 位一致性量化&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;、&lt;b>;Phoenix Meadowlark&lt;/b>;、&lt;b>;Shaojin Ding &lt;/b>;、&lt;b>;邱大卫&lt;/b>;、&lt;b>;李健&lt;/b>;、&lt;b>;林大卫&lt;/b>;、&lt;b>;何彦章&lt;/b>;&lt;br />;&lt;/ em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/koizumi23_interspeech.pdf&quot;>;LibriTTS-R：恢复的多扬声器文本到-语音语料库&lt;/a>;&lt;br />; &lt;em>;&lt;b>;小泉佑马&lt;/b>;、&lt;b>;平贺禅&lt;/b>;、&lt;b>;成田茂树&lt;/b>;、&lt;b>;丁一帆&lt;/ b>;、矢田部航平、&lt;b>;盛冈伸之&lt;/b>;、&lt;b>;Michiel Bacchiani&lt;/b>;、&lt;b>;张宇&lt;/b>;、&lt;b>;韩伟&lt;/b>;、&lt;b>;Ankur Bapna&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf&quot;>;PronScribe：语音和文本的高精度多模态音素转录&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yang Yu&lt;/b>;、Matthew Perez*、&lt;b>;Ankur Bapna&lt;/b>;、&lt;b>;Fadi Haik&lt; /b>;、&lt;b>;Siamak Tazari&lt;/b>;、&lt;b>;张宇&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca -speech.org/archive/pdfs/interspeech_2023/vashishth23_interspeech.pdf&quot;>;用于语言识别的标签感知语音表示学习&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shikhar Vashishth&lt;/b>;、&lt;b>;Shikhar Bharadwaj &lt;/b>;、&lt;b>;Sriram Ganapathy&lt;/b>;、&lt;b>;安库尔·巴普纳&lt;/b>;、&lt;b>;马敏&lt;/b>;、&lt;b>;韩伟&lt;/b>;、&lt;b>;维拉·阿克塞尔罗德&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p >; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>;&lt;/content>; &lt;link href=&quot;http://blog.research.google/feeds/6990000750711940626/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://blog.research.google/2023/08/google-at-interspeech-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google -at-interspeech-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at Interspeech 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http:// www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005 #thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJ VLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s72-c/Interspeech2023-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http: //search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog- 8474926331452026626.post-362086873015740792&lt;/id>;&lt;发布>;2023-08-18T11:28:00.004-07:00&lt;/发布>;&lt;更新>;2023-08-18T11:30:51.167-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“计算机视觉”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模态学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用大语言模型进行自主视觉信息搜索&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：学生研究员 Ziniu Hu 和 Google 研究感知团队研究科学家 Alireza Fathi &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUF zF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在适应大型语言模型 (LLM) 以适应任务的多模式输入方面已经取得了巨大进展，包括 &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~ :text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20them。&quot;>;图像字幕&lt;/a>;，&lt;a href=&quot;https://huggingface.co/tasks/visual-question -answering&quot;>;视觉问答 (VQA)&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;开放词汇识别&lt;/a>;。尽管取得了这些成就，当前最先进的视觉语言模型（VLM）在视觉信息搜索数据集上的表现还不够，例如 &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a >; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;，需要外部知识来回答问题。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe -vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R 9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-宽度=“1999”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpm MLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;需要外部知识来回答问题的视觉信息搜索查询示例。图像取自 OK-VQA 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>; AVIS：使用大型语言模型进行自主视觉信息搜索&lt;/a>;”，我们介绍了一种新颖的方法，该方法在视觉信息搜索任务上取得了最先进的结果。我们的方法将法学硕士与三种类型的工具集成：（i）用于从图像中提取视觉信息的计算机视觉工具，（ii）用于检索开放世界知识和事实的网络搜索工具，以及（iii）用于收集相关信息的图像搜索工具来自与视觉上相似的图像相关的元数据。 AVIS 采用法学硕士支持的规划器在每个步骤中选择工具和查询。它还使用 LLM 支持的推理机来分析工具输出并提取关键信息。工作记忆组件在整个过程中保留信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoS​​FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3i Fnyn7oy8bB81TCbh_D-I3unIwgxswoS​​FcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;AVIS 生成的工作流程示例，用于回答具有挑战性的视觉信息搜索问题。输入图像取自 Infoseek 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2 >;与之前的工作比较&lt;/h2>; &lt;p>;最近的研究（例如，&lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;、&lt;a href=&quot;https:// viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; 和 &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;）探索向法学硕士添加工具以实现多模式输入。这些系统遵循两个阶段的过程：规划（将问题分解为结构化程序或指令）和执行（使用工具收集信息）。尽管在基本任务中取得了成功，但这种方法在复杂的现实场景中常常会出现问题。 &lt;/p>; &lt;p>; 人们对应用法学硕士作为自主代理的兴趣也激增（例如，&lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; 和 &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;）。这些代理与环境交互，根据实时反馈进行调整并实现目标。然而，这些方法并没有限制每个阶段可以调用的工具，导致搜索空间巨大。因此，即使是当今最先进的法学硕士也可能陷入无限循环或传播错误。 AVIS 通过指导法学硕士使用来解决这个问题，并受到用户研究中人类决策的影响。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过用户研究为 LLM 决策提供依据&lt;/h2>; &lt;p>; 中的许多视觉问题数据集，例如 &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a >; 即使对人类来说也是一个挑战，通常需要各种工具和 API 的帮助。下面显示了 OK-VQA 数据集中的一个示例问题。我们进行了一项用户研究，以了解使用外部工具时人类的决策。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP 8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWx b9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们进行了一项用户研究，以了解人类在使用外部工具时的决策。图像取自 OK-VQA 数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 用户配备了与我们的方法相同的工具集，包括 &lt;a href=&quot;https ://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022 /04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 和&lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;网络搜索&lt;/a >;。他们接收输入图像、问题、检测到的对象作物以及链接到图像搜索结果的按钮。这些按钮提供了有关检测到的对象作物的各种信息，例如知识图实体、相似的图像标题、相关产品标题和相同的图像标题。 &lt;/p>; &lt;p>; 我们记录用户操作和输出，并通过两种关键方式将其用作我们系统的指南。首先，我们通过分析用户做出的决策顺序构建一个转换图（如下所示）。该图定义了不同的状态并限制每个状态下可用的操作集。例如，在启动状态，系统只能采取以下三种操作之一：PALI 字幕、PALI VQA 或对象检测。其次，我们使用人类决策的例子来指导我们的规划者和推理者与相关的上下文实例，以提高我们系统的性能和有效性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-I CB-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png “ style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr 1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.png“宽度=“472”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS 转换图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;总体框架&lt;/h2>; &lt;p>;我们的方法采用动态决策策略，旨在响应视觉信息寻求查询。我们的系统具有三个主要组件。首先，我们有一个规划器来确定后续操作，包括适当的 API 调用及其需要处理的查询。其次，我们有一个&lt;em>;工作内存&lt;/em>;，它保留有关从 API 执行获得的结果的信息。最后，我们有一个推理器，其作用是处理 API 调用的输出。它确定所获得的信息是否足以产生最终响应，或者是否需要额外的数据检索。 &lt;/p>; &lt;p>; 每次需要决定使用哪种工具以及向其发送什么查询时，规划器都会执行一系列步骤。根据当前状态，规划器提供一系列潜在的后续行动。潜在的行动空间可能太大，使得搜索空间变得棘手。为了解决这个问题，规划器参考转换图来消除不相关的动作。规划器还排除之前已经采取并存储在工作记忆中的动作。 &lt;/p>; &lt;p>; 接下来，规划器收集一组相关的上下文示例，这些示例是根据人类之前在用户研究期间做出的决策组合而成的。通过这些示例以及保存从过去的工具交互中收集的数据的工作记忆，规划者可以制定提示。然后，提示被发送到法学硕士，法学硕士返回结构化答案，确定下一个要激活的工具以及要向其分派的查询。这种设计允许在整个过程中多次调用规划器，从而促进动态决策，逐渐导致回答输入查询。 &lt;/p>; &lt;p>; 我们使用推理器来分析工具执行的输出，提取有用的信息并决定工具输出属于哪个类别：提供信息、非信息或最终答案。我们的方法利用法学硕士以及适当的提示和上下文示例来执行推理。如果推理机断定它已准备好提供答案，它将输出最终响应，从而结束任务。如果它确定工具输出没有信息，它将返回给规划器以根据当前状态选择另一个操作。如果它发现工具输出有用，它将修改状态并将控制权转移回规划器以在新状态下做出新决策。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQf WpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height= “470”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSur SEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt; /td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS 采用动态决策策略来响应视觉信息寻求查询。&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估 AVIS &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; 和 &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; 数据集。如下所示，甚至是强大的视觉语言模型，例如 &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog。 com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;，在 Infoseek 上微调时无法产生高精度。我们的方法 (AVIS) 在没有微调的情况下，对该数据集的未见实体分割达到了 50.7% 的准确率。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2E ikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHj qYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Infoseek 数据集上的 AVIS 视觉问答结果。与之前基于 &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; 的基线相比，AVIS 实现了更高的准确度、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https:// arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们在 OK-VQA 数据集上的结果如下所示。 AVIS 通过少量上下文示例实现了 60.2% 的准确率，高于之前的大多数作品。与在 OK-VQA 上微调的 PALI 模型相比，AVIS 的准确度较低，但相当。与 Infoseek 相比，AVIS 优于微调后的 PALI，这种差异是由于 OK-VQA 中的大多数问答示例依赖于常识知识而不是细粒度知识。因此，PaLI 能够将此类通用知识编码在模型参数中，并且不需要外部知识。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d 5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a 6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;A-OKVQA 上的视觉问答结果。与之前使用少样本或零样本学习的作品相比，AVIS 实现了更高的准确度，包括 &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single- Visual-language-model&quot;>;火烈鸟&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;巴利语&lt;/ a>; 和 &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;。 AVIS 还比之前在 OK-VQA 数据集上进行微调的大多数作品实现了更高的准确度，包括 &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language- pre.html&quot;>;REVEAL&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/ 2112.08614&quot;>;KAT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;，并获得接近微调&lt;a href=&quot;https的结果://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了一种新颖的方法，使法学硕士能够使用各种用于回答知识密集型视觉问题的工具。我们的方法以从用户研究中收集的人类决策数据为基础，采用了一个结构化框架，该框架使用法学硕士支持的规划器来动态决定工具选择和查询形成。由 LLM 驱动的推理机的任务是从所选工具的输出中处理和提取关键信息。我们的方法迭代地使用规划器和推理器来利用不同的工具，直到收集到回答视觉问题所需的所有必要信息。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Ziniu Hu、Ahmet Iscen 进行、孙晨、张凯伟、孙一舟、David A. Ross、Cordelia Schmid 和 Alireza Fathi。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/ feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous -visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel =&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; 标题=&quot;使用大语言模型进行自主视觉信息搜索&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqY oAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:总计>;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;发布>;2023-08-17T11:08:00.000 -07:00&lt;/已发布>;&lt;更新>;2023-08-17T11:08:24.346-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”term= &quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;神经网络剪枝组合优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队研究科学家 Hussein Hazimeh 和麻省理工学院研究生 Riade Benbaki&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDN jcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 现代神经网络在各种应用中都取得了令人印象深刻的性能，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model /&quot;>;语言、数学推理&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;愿景&lt;/a>; 。然而，这些网络通常使用需要大量计算资源的大型架构。这使得向用户提供此类模型变得不切实际，特别是在可穿戴设备和智能手机等资源有限的环境中。降低预训练网络的推理成本的&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;广泛使用的方法&lt;/a>;是通过删除一些网络来修剪它们它们的权重，不会显着影响效用。在标准神经网络中，每个权重定义两个神经元之间的连接。因此，在权重被修剪后，输入将通过较小的连接集传播，因此需要较少的计算资源。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw -bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBRYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEg D-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width= “1600”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FonAvjG2Hw J4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;原始网络与修剪后的网络。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;剪枝方法可以应用于网络训练过程的不同阶段：训练后、训练期间或训练之前（即权重初始化后立即）。在这篇文章中，我们重点关注训练后设置：给定一个预先训练的网络，我们如何确定应该修剪哪些权重？一种流行的方法是&lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;幅度剪枝&lt;/a>;，它删除幅度最小的权重。虽然有效，但该方法没有直接考虑删除权重对网络性能的影响。另一个流行的范例是&lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;基于优化的剪枝&lt;/a>;，它根据权重的删除对损失函数的影响程度来删除权重。尽管在概念上很有吸引力，但大多数现有的基于优化的方法似乎都面临着性能和计算要求之间的严重权衡。进行粗略近似的方法（例如，假设对角线&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian_matrix&quot;>;Hessian 矩阵&lt;/a>;）可以很好地扩展，但性能相对较低。另一方面，虽然进行较少近似的方法往往表现更好，但它们的可扩展性似乎要差得多。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”中，介绍于 &lt;a href=&quot; https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;，我们描述了如何开发一种基于优化的方法来大规模修剪预训练的神经网络。 CHITA（代表“组合 Hessian 迭代阈值算法”）在可扩展性和性能权衡方面优于现有的剪枝方法，并且它通过利用多个领域的进步来做到这一点，包括 &lt;a href=&quot;https://en. wikipedia.org/wiki/High-Dimensional_statistics&quot;>;高维统计&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;组合优化&lt;/a>;和神经网络修剪。例如，CHITA 的速度比最先进的修剪方法快 20 倍到 1000 倍 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;，并将准确性提高 10 以上% 在许多设置中。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;贡献概述&lt;/h2>; &lt;p>; CHITA 相对于流行方法有两项显着的技术改进：&lt; /p>; &lt;ul>; &lt;li>;&lt;strong>;二阶信息的有效使用&lt;/strong>;：使用二阶信息的修剪方法（即，与&lt;a href=&quot;https://en.wikipedia. org/wiki/Second_derivative&quot;>;二阶导数&lt;/a>;）在许多设置中实现了最先进的技术。在文献中，此信息通常通过计算 Hessian 矩阵或其逆矩阵来使用，这种操作很难扩展，因为 Hessian 大小与权重数量成二次方。通过仔细的重新表述，CHITA 使用二阶信息，而无需显式计算或存储 Hessian 矩阵，从而实现更高的可扩展性。 &lt;/li>;&lt;li>;&lt;strong>;组合优化&lt;/strong>;：流行的基于优化的方法使用一种简单的优化技术，单独修剪权重，即，在决定修剪某个权重时，它们不考虑是否其他权重已被修剪。这可能会导致重要权重的修剪，因为当其他权重被修剪时，孤立地被认为不重要的权重可能会变得重要。 CHITA 通过使用更先进的组合优化算法来避免这个问题，该算法考虑了修剪一个权重如何影响其他权重。 &lt;/li>; &lt;/ul>; &lt;p>; 在下面的部分中，我们讨论 CHITA 的剪枝公式和算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;计算友好的剪枝公式&lt;/h2>; &lt;p>; 有许多可能的剪枝候选者，其中通过仅保留原始网络权重的子集来获得。令&lt;em>;k&lt;/em>;为用户指定的参数，表示要保留的权重数量。剪枝可以自然地表述为一个&lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;最佳子集选择&lt;/a>;（BSS）问题：在所有可能的剪枝候选者（即权重子集）中仅保留 &lt;em>;k&lt;/em>; 个权重，选择损失最小的候选者。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithv XGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY 1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;作为 BSS 问题的剪枝：在具有相同权重总数的所有可能的剪枝候选者中，最佳候选者被定义为损失最小的候选者。该图显示了四个候选者，但这个数字通常要大得多。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;解决原始损失函数上的剪枝BSS问题通常是通过计算来解决的棘手的。因此，与之前的工作类似，例如 &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; 和 &lt;a href=&quot; https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;，我们用 &lt;a href=&quot; 来近似损失https://en.wikipedia.org/wiki/Quadratic_form&quot;>;二次函数&lt;/a>;，使用二阶&lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;泰勒级数&lt; /a>;，其中 Hessian 矩阵是根据经验 &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher 信息矩阵&lt;/a>; 进行估计的。虽然梯度通常可以有效地计算，但由于其庞大的规模，计算和存储 Hessian 矩阵的成本过高。在文献中，通常通过对 Hessian 矩阵（例如，对角矩阵）和算法（例如，单独剪枝权重）做出限制性假设来应对这一挑战。 &lt;/p>; &lt;p>; CHITA 使用修剪问题的有效重新表述（使用二次损失的 BSS），避免显式计算 Hessian 矩阵，同时仍然使用该矩阵中的所有信息。这是通过利用经验费希尔信息矩阵的低&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;秩&lt;/a>;结构来实现的。这种重新表述可以被视为稀疏&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;线性回归&lt;/a>;问题，其中每个回归系数对应于神经网络中的特定权重。获得此回归问题的解决方案后，设置为零的系数将对应于应修剪的权重。我们的回归数据矩阵是 (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;)，其中 &lt;em>;n&lt;/em>; 是批次（子样本）大小，&lt;em>;p&lt;/em>; em>; 是原始网络中的权重数量。通常&lt;em>;n&lt;/em>; &lt;&lt; &lt;em>;p&lt;/em>;，因此使用此数据矩阵进行存储和操作比使用 (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian 操作的常见修剪方法更具可扩展性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdK vBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUf ysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;CHITA 将二次损失近似重新表述为线性回归 (LR) 问题，这需要昂贵的 Hessian 矩阵。 LR 的数据矩阵在 &lt;em>;p&lt;/em>; 中呈线性，这使得重构比原始二次近似更具可扩展性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可扩展的优化算法&lt;/h2>; &lt;p>; CHITA 在以下稀疏性约束下将剪枝减少为线性回归问题：最多 &lt; em>;k&lt;/em>; 个回归系数可以不为零。为了获得此问题的解决方案，我们考虑对著名的&lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;迭代硬阈值&lt;/a>; (IHT) 算法进行修改。 IHT 执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;梯度下降&lt;/a>;，每次更新后执行以下后处理步骤：Top-&lt;em >;k&lt;/em>;（即，具有最大幅度的&lt;em>;k&lt;/em>;系数）设置为零。 IHT 通常会为问题提供良好的解决方案，并且它会迭代探索不同的剪枝候选对象并联合优化权重。 &lt;/p>; &lt;p>; 由于问题的规模，具有恒定&lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;学习率&lt;/a>;的标准 IHT 可能会非常慢收敛。为了更快地收敛，我们开发了一种新的&lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;线搜索&lt;/a>;方法，该方法利用问题结构来找到合适的学习率，即导致损失足够大的减少。我们还采用了多种计算方案来提高 CHITA 的效率和二阶近似的质量，从而产生了一个改进的版本，我们称之为 CHITA++。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们将 CHITA 的运行时间和准确性与几种状态进行比较使用不同架构的最先进的修剪方法，包括 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/1704.04861 &quot;>;移动网络&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;运行时间&lt;/strong>;：CHITA 比执行联合优化的同类方法更具可扩展性（而不是单独修剪权重）。例如，CHITA 在剪枝 ResNet 时的加速比可以达到 1000 倍以上。 &lt;/p>; &lt;p>; &lt;strong>;后剪枝精度&lt;/strong>;：&lt;strong>; &lt;/strong>;下面，我们比较了 CHITA 和 CHITA++ 与幅度剪枝（MP）的性能，&lt;a href=&quot;https: //arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF) 和&lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;组合脑外科医生&lt;/a>; (CBS)，用于修剪 70% 的模型权重。总体而言，我们看到 CHITA 和 CHITA++ 取得了良好的改进。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1 aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz- KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlk Fi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ResNet20 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-标题容器&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_ AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img边框=“0”数据原始高度=“742”数据原始宽度=“1200”高度=“396”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA -5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-thHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-CP-w2OUUDsPUNMBhGCTkSBFjV6ODFY60 K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MobileNet 上各种方法的后剪枝精度。报告修剪 70% 模型权重的结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们报告修剪更大网络的结果：ResNet50（在此网络上，一些ResNet20 图中列出的方法无法扩展）。这里我们与幅度剪枝和&lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;进行比较。下图显示，CHITA 在各种稀疏度水平下实现了更好的测试精度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU 5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6J MfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试使用不同方法获得的修剪网络的准确性。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论、局限性和未来工作&lt;/h2>; &lt;p>; 我们提出了 CHITA，一种基于优化的方法用于修剪预先训练的神经网络。 CHITA 通过有效地使用二阶信息并借鉴组合优化和高维统计的思想，提供可扩展性和有竞争力的性能。 &lt;/p>; &lt;p>; CHITA 专为&lt;em>;非结构化修剪&lt;/em>;而设计，可以去除任何重量。理论上，非结构化剪枝可以显着降低计算要求。然而，在实践中实现这些减少需要支持稀疏计算的特殊软件（可能还有硬件）。相比之下，结构化修剪会删除神经元等整个结构，可能会提供在通用软件和硬件上更容易实现的改进。将 CHITA 扩展到结构化修剪将会很有趣。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是 Google 之间研究合作的一部分和麻省理工学院。感谢 Rahul Mazumder、Natalia Ponomareva、Wenyu Chen、Xiang Men、Zhe Zhu 和 Sergei Vassivitskii 在准备这篇文章和论文时提供的帮助。还要感谢 John Guilyard 在本文中创建图形。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4779594044050650231/comments/default&quot; rel= &quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html#comment -form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel= “编辑”类型=“application/atom+xml”/>;&lt;link href=“http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231”rel=“self”type=“application/atom” +xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;组合优化神经网络剪枝&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS 2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K /s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr ：总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-8689679451447865270&lt;/id>;&lt;发布>;2023-08-15T12:59:00.001-07:00&lt;/已发布>;&lt;更新>;2023-08-15T13:00:54.887-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt; /类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“推荐系统”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;社交网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;研究：社会意识暂时因果解码器推荐系统&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：Google 研究工程师 Eltayeb Ahmed 和高级研究科学家 Subhrajit Roy&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt -PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqaJM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx 8AO1_/s837/study-hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 阅读对于年轻学生有很多好处，例如&lt;a href=&quot;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/284286/reading_for_pleasure。 pdf&quot;>;更好的语言和生活技能&lt;/a>;，并且快乐阅读已被证明与&lt;a href=&quot;https://files.eric.ed.gov/fulltext/ED541404.pdf&quot;>;学业成功相关&lt; /a>;.此外，学生们还表示，阅读&lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/1361454042000312284&quot;>;情绪健康得到改善&lt;/a>;，并且&lt;a href=&quot;https:// eric.ed.gov/?id=ED496343&quot;>;更好的常识和对其他文化的更好的理解&lt;/a>;。由于线上和线下有大量的阅读材料，找到适合年龄的、相关的和引人入胜的内容可能是一项具有挑战性的任务，但帮助学生做到这一点是让他们参与阅读的必要步骤。向学生提供相关阅读材料的有效推荐有助于学生持续阅读，而这正是机器学习 (ML) 可以提供帮助的地方。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 机器学习已广泛应用于构建&lt;a href=&quot;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592 -5&quot;>;推荐系统&lt;/a>;，适用于各种类型的数字内容，从视频到书籍再到电子商务项目。推荐系统在一系列数字平台上使用，以帮助向用户展示相关且有吸引力的内容。在这些系统中，机器学习模型经过训练，可以根据用户偏好、用户参与度和推荐项目单独向每个用户推荐项目。这些数据为模型提供了强大的学习信号，使其能够推荐可能感兴趣的项目，从而改善用户体验。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.07946&quot;>;研究：社交意识暂时因果解码器推荐系统&lt;/a>;”中，我们提出了一个有声读物的内容推荐系统在教育环境中考虑阅读的社会性质。我们与 &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 合作开发了 STUDY 算法，这是一个教育非营利组织，旨在促进阅读障碍学生的阅读，通过学校向学生提供有声读物广泛的订阅计划。利用 Learning Ally 图书馆中的各种有声读物，我们的目标是帮助学生找到合适的内容，以帮助提高他们的阅读体验和参与度。由于一个人的同龄人当前正在阅读的内容会对他们感兴趣的阅读内容产生重大影响，因此我们共同处理同一教室中学生的阅读参与历史。这使得我们的模型能够从有关学生本地社交群体（在本例中为他们的教室）当前趋势的实时信息中受益。 &lt;/p>; &lt;br />; &lt;h2>;数据&lt;/h2>; &lt;p>; &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; 拥有一个大型数字图书馆，其中包含针对以下人群的精选有声读物：学生，使其非常适合构建社会推荐模型，以帮助提高学生的学习成果。我们收到了两年的匿名有声读物消费数据。数据中的所有学生、学校和分组都是匿名的，仅通过随机生成的 ID 进行识别，无法通过 Google 追溯到真实实体。此外，所有潜在可识别元数据仅以聚合形式共享，以保护学生和机构不被重新识别。这些数据包括学生与有声读物互动的带时间戳的记录。对于每次交互，我们都有一个匿名的学生 ID（包括学生的年级和匿名的学校 ID）、有声读物标识符和日期。虽然许多学校将单一年级的学生分布在多个教室中，但我们利用此元数据做出简化的假设，即同一所学校和同一年级的所有学生都在同一教室。虽然这为建立更好的社交推荐模型提供了基础，但值得注意的是，这并不能让我们重新识别个人、班级群体或学校。 &lt;/p>; &lt;br />; &lt;h2>;STUDY 算法&lt;/h2>; &lt;p>; 我们将推荐问题描述为&lt;a href=&quot;https://arxiv.org/abs/2104.10584&quot;>;点击率&lt;/a>; 预测问题，我们对用户与每个特定项目交互的条件概率进行建模，条件是 1）用户和项目特征以及 2）当前用户的项目交互历史序列。 &lt;a href=&quot;https://arxiv.org/abs/1905.06874&quot;>;之前的工作&lt;/a>;建议&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;>;Transformer&lt;基于 /a>; 的模型是 Google Research 开发的一种广泛使用的模型类，非常适合对这个问题进行建模。当单独处理每个用户时，这将成为一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;自回归序列建模问题&lt;/a>;。我们使用这个概念框架来建模我们的数据，然后扩展这个框架来创建研究方法。 &lt;/p>; &lt;p>; 虽然这种点击率预测方法可以对单个用户过去和未来的项目偏好之间的依赖关系进行建模，并且可以在训练时学习用户之间的相似性模式，但它无法在推理时对不同用户之间的依赖关系进行建模时间。为了认识阅读的社会性并弥补这一缺陷，我们开发了 STUDY 模型，该模型将每个学生阅读的多个书籍序列连接成一个序列，该序列收集单个教室中多个学生的数据。 &lt;/p>; &lt;p>; 然而，如果要通过 Transformer 建模，这种数据表示需要仔细研究。在 Transformer 中，注意力掩码是控制哪些输入可用于通知哪些输出的预测的矩阵。使用序列中的所有先验标记来通知输出预测的模式导致传统上在因果解码器中发现的上三角注意矩阵。然而，由于输入 STUDY 模型的序列不是按时间排序的，即使它的每个组成子序列都是标准的&lt;a href=&quot;https://arxiv.org/abs/1807.03819&quot;>;因果解码器&lt;/a>;不再适合这个序列。当尝试预测每个标记时，模型不允许关注序列中位于其之前的每个标记；其中一些令牌可能具有较晚的时间戳，并且包含在部署时不可用的信息。 &lt;br />; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYtgiddj84RymezoC-hVklA8QnD4y2_v5vZpmcEca2ZrLYWhB6dd2n17h5EXFKjYDf_7-E_tyA7i_tq Rd-ZWDXOCRpHAy0FZE9sV0xB8reyJ-- Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/s1787/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1651&quot; data-original-width = =第1787章pHAy0FZE9sV0xB8reyJ--Xpm3bYITc9YdTu6542F1QH1ziERca6ZKzPn95CW5MyZ5-MXf_FqaxjdnjpSbQpDDaLw0jfILnusxXpB4/w400-h370/image1.png&quot;宽度=&quot; 400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们显示了通常用于因果解码器。每列代表一个输出，每列代表一个输出。特定位置处的矩阵条目的值为 1（显示为蓝色）表示模型在预测相应列的输出时可以观察到该行的输入，而值为 0（显示为白色）表示相反的情况.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; STUDY 模型建立在因果变换器的基础上，通过将三角矩阵注意掩码替换为灵活的注意掩码，其值基于时间戳，以允许关注不同的子序列。与常规 Transformer 相比，常规 Transformer 不允许跨不同子序列进行关注，并且在序列内具有三角矩阵掩码，而 STUDY 在序列内维护因果三角关注矩阵，并且跨序列具有灵活的值，其值取决于时间戳。因此，序列中任何输出点的预测都是由过去相对于当前时间点发生的所有输入点通知的，无论它们是出现在序列中当前输入之前还是之后。这种因果约束很重要，因为如果不在训练时强制执行，模型可能会学习使用未来的信息进行预测，而这对于现实世界的部署来说是不可用的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6Zm UAfS2rExoTevwFqk0EItFhANO2eJdeFMIWt7K58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s1104/image3.jpg&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEhU139mxAtpr1qLEVp93A62fLgeWEDWyHzGJuGDqqEF6w3gKUSMzCA8a1wMiiyCgXaMuIrnZcyQIUZiS0f1MQisyFb6ZmUAfS2rExoTevwFqk0EItFHANO2eJdeFMIWt7K 58TYxMn8jroJF9IkeCDr7UAYUu0wnfjfPG3WLE2r3xddQr1eHALIaMMkVrMTC/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;在（a）中，我们展示了一个具有因果注意力的顺序自回归变压器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQf vFM43UlqVpHukBZDMszjzcqIRb2aKB8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s1035/Study.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1035&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjP2mcUTkycIilVpYw2VlVD6zei9A54MSLKxsE4_UT47WsUFt3fkV4x-KFxdm_MnQ9lKgu7-mwqn_ZhjEdbf7zQfvFM43UlqVpHukBZDMszjzcqIRb2aKB 8ztSLo8jR3VepFfKb9R_YpDrofhkcMC-9os0Ibdt1oSepXzlq5dohM3OhUk6mere35MgW58vv/s16000/Study.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; !--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt; tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2 _wNPxEjeApzumh4ksesE5X9FdcaJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/s1104/image3.png&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1104&quot; data-original-width=&quot;548&quot; height=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcePiVSL8fNX136KdFzmsPt1jHI9CfLwWmm6T6n3lTewlW-OVPLdRSFdL3_qYJsOhMzBf1Zb63p4NTVYqbaP4Qi2_wNPxEjeApzumh4ksesE5X9Fd caJHHVnF0WTvAk9VyCtYubcD9CHQvWwgLFN1BmHjs5zHHWzHt7c5Oj_WLZ6rn0RIfj-2k78sEBYBe/w318-h640/image3.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 (a) 中，我们展示了一个具有因果注意力的顺序自回归转换器，可以单独处理每个用户；在（b）中，我们展示了等效的联合前向传递，其结果与（a）相同的计算；最后，在（c）中，我们表明，通过向注意力掩模引入新的非零值（以紫色显示），我们允许信息在用户之间流动。我们通过允许预测以具有较早时间戳的所有交互为条件来实现这一点，无论交互是否来自同一用户。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验&lt;/h2>; &lt;p>; 我们使用 Learning Ally 数据集来训练 STUDY 模型以及多个基线以进行比较。我们实现了一个自回归点击率转换器解码器，我们将其称为“个体”，一个&lt;em>;k&lt;/em>;最近邻基线（KNN），以及一个可比较的社交基线，社交注意力记忆网络（SAMN） 。我们使用第一学年的数据进行训练，并使用第二学年的数据进行验证和测试。 &lt;/p>; &lt;p>; 我们通过测量用户实际交互的下一个项目在模型的前 &lt;em>;n&lt;/em>; 个推荐中的时间百分比来评估这些模型，即，hits@&lt;em>;n， &lt;/em>;对于&lt;em>;n&lt;/em>;的不同值。除了在整个测试集上评估模型之外，我们还报告了模型在测试集的两个子集上的得分，这两个子集比整个数据集更具挑战性。我们观察到，学生通常会在多个会话中与有声读物进行交互，因此简单地推荐用户最近读过的一本书将是一个强有力的简单推荐。因此，第一个测试子集（我们称之为“非延续”）是指当学生与与之前交互不同的书籍进行交互时，我们仅查看每个模型在推荐上的表现。我们还观察到学生会重温他们过去读过的书籍，因此通过将针对每个学生的推荐仅限于他们过去读过的书籍，可以在测试集上取得出色的表现。尽管向学生推荐旧的收藏夹可能很有价值，但推荐系统的大部分价值来自于向用户展示新的和未知的内容。为了衡量这一点，我们在学生第一次与标题交互的测试集子集上评估模型。我们将这个评估子集命名为“小说”。 &lt;/p>; &lt;p>; 我们发现，在我们评估的几乎每个切片中，STUDY 都优于所有其他测试模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr 7k67KRDgWH96Q-OC8UsSVx0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/s1526/Study-img2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1486&quot; data-original-width=&quot;1526&quot; height=&quot;390&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUC1VrNR_Wrz4ZJGhL3rLSqizzVFA4WPkKuYTTnU1pEry-jDApsTcZF_6HmG06z_wse94Sr1YX5zVwzF7abgfTr7k67KRDgWH96Q-OC8UsSVx 0_H2URPwHIuM3GgOIAiYoppBdO99JnP77WcAlxUVZrhxZ27KKG5114kFoXayJhJe5BS51wzGlkNHo_OQW/w400-h390/Study-img2.png&quot;宽度=“400”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在此图中，我们比较了 Study、Individual、KNN 和 SAMN 四种模型的性能。我们用 hist@5 来衡量性能，即模型在模型的前 5 个推荐中建议用户阅读的下一个标题的可能性有多大。我们在整个测试集（全部）以及新颖的和非连续的分割上评估模型。我们发现 STUDY 在所有分组中始终优于其他三个模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;适当分组的重要性&lt;/h2>; &lt;p>; STUDY 算法的核心是将用户组织成组，并在模型的单次前向传递中对同一组中的多个用户进行联合推理。我们进行了一项消融研究，研究了实际分组对模型性能的重要性。在我们提出的模型中，我们将同一年级和学校的所有学生分组在一起。然后，我们对由同一年级和学区的所有学生定义的组进行实验，并将所有学生放入一个组中，并在每次前向传递中使用随机子集。我们还将这些模型与个体模型进行比较以供参考。 &lt;/p>; &lt;p>; 我们发现使用更加本地化的分组更为有效，学校和年级分组的效果优于地区和年级分组。这支持了这样的假设：学习模型之所以成功，是因为阅读等活动的社会性质——人们的阅读选择可能与周围人的阅读选择相关。这两种模型都优于其他两种模型（单组和个人），其中不使用年级水平对学生进行分组。这表明来自具有相似阅读水平和兴趣的用户的数据有利于性能。 &lt;/p>; &lt;br />; &lt;h2>;未来的工作&lt;/h2>; &lt;p>; 这项工作仅限于为假设社交关系同质的用户群进行推荐建模。将来，对关系不均匀的用户群体进行建模将是有益的，即，存在明显不同类型的关系或已知不同关系的相对强度或影响。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作涉及由研究人员、软件工程师和教育主题专家组成的多学科团队的协作努力。我们感谢我们的合著者：来自 Google 的 Diana Mincu、Lauren Harrell 和 Katherine Heller。我们还要感谢 Learning Ally 的同事 Jeff Ho、Akshat Shah、Erin Walker 和 Tyler Bastian，以及 Google 的合作者 Marc Repnyek、Aki Estrella、Fernando Diaz、Scott Sanner、Emily Salkey 和 Lev Proleev。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8689679451447865270/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; 类型=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;链接 href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8689679451447865270&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/08/study-socially-aware-temporally-causal.html&quot; rel=&quot;alternate&quot; title=&quot;研究：社会意识时间因果解码器推荐系统&quot; type=&quot;text/html&quot;/>;&lt;author >;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16 “rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>;&lt;/gd :image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtKv9JocvNhtzu5Cxb5h_vVAz4y-OOQJ9YRj8gmvLlt-PLgnxqXM5KytIsUWkdtHtEvqmTvyiUqOqa JM1R4096YBLtUYQmv2nEQR0CMZvPc2ccfCIriJFGVCp94fe24etHhZrZh4JtzAV6GpumD657Q3qqPinhVpJ1Zn3UqJPE7BDa8GxE9h8CqAx8AO1_/s72-c/study-hero.png “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-3240052415427459327&lt;/id>;&lt;发布>;2023-08-09T11:32:00.001-07:00&lt;/发布>;&lt;更新>;2023-08-09T12 :26:53.831-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言理解&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;文档理解方面的进展&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Athena 团队 Google 研究部软件工程师 Sandeep Tata&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV8P1-X9deoXISeAfq85z UbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s320/Document%20understanding%20hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 过去几年，能够自动处理复杂业务文档并将其转换为结构化对象的系统取得了快速进展。一个可以从收据、保险报价等文档中&lt;a href=&quot;https://ai.googleblog.com/2020/06/extracting-structured-data-from.html&quot;>;自动提取数据&lt;/a>;的系统和财务报表，通过避免容易出错的手动工作，有可能显着提高业务工作流程的效率。基于 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; 架构的最新模型已经展示了&lt;a href=&quot; https://ai.googleblog.com/2022/04/formnet-beyond-sequential-modeling-for.html&quot;>;准确度显着提升&lt;/a>;。更大的模型，例如 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>;，也被用来进一步简化这些业务工作流程。然而，学术文献中使用的数据集未能捕捉到现实世界用例中遇到的挑战。因此，学术基准报告了很强的模型准确性，但这些相同的模型在用于复杂的现实应用程序时表现不佳。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;VRDU：视觉丰富文档理解的基准&lt; /a>;”，在 &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>; 上发布，我们宣布发布新的&lt;a href=&quot;https://research.google /resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集旨在弥补这一差距，帮助研究人员更好地跟踪文档理解任务的进展。根据经常使用文档理解模型的现实文档类型，我们列出了良好的文档理解基准的五个要求。然后，我们描述了研究界当前使用的大多数数据集为何无法满足其中一项或多项要求，而 VRDU 却满足了所有这些要求。我们很高兴地宣布公开发布 VRDU &lt;a href=&quot;https://research.google/resources/datasets/visually-rich-document-understanding/&quot;>;数据集&lt;/a>;和&lt;a href=&quot;https ://github.com/google-research-datasets/vrdu&quot;>;评估代码&lt;/a>;，采用&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;>;知识共享许可&lt;/a>;一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基准要求&lt;/h2>; &lt;p>; 首先，我们比较了最先进的模型准确性（例如，使用 &lt;a href=&quot;https://arxiv.org/abs/2203.08411&quot;>;FormNet&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2012.14740&quot;>;LayoutLMv2&lt;/ a>;）关于现实世界用例与学术基准（例如，&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.a>;） net/pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;）。我们观察到，最先进的模型与学术基准结果不符，并且在现实世界中的准确性要低得多。接下来，我们将经常使用文档理解模型的典型数据集与学术基准进行比较，并确定了五个数据集要求，使数据集能够更好地捕获现实应用程序的复杂性：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;丰富模式：&lt;/strong>;在实践中，我们看到了各种用于结构化提取的丰富模式。实体具有不同的数据类型（数字、字符串、日期等），这些数据类型可能是必需的、可选的或在单个文档中重复，甚至可能是嵌套的。像（标题、问题、答案）这样的简单平面模式的提取任务并不能反映实践中遇到的典型问题。 &lt;/li>;&lt;li>;&lt;strong>;布局丰富的文档：&lt;/strong>;文档应该具有复杂的布局元素。实际设置中的挑战来自以下事实：文档可能包含表格、键值对、在单列和双列布局之间切换、不同部分具有不同的字体大小、包括带有标题甚至脚注的图片。将此与大多数文档以句子、段落和带有节标题的章节组织的数据集进行对比 - 这些文档通常是 &lt;a href=&quot;https://arxiv.org/abs 上经典自然语言处理文献的焦点/2007.14062&quot;>;长&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2004.08483&quot;>;输入&lt;/a>;。 &lt;/li>;&lt;li>;&lt;strong>;多样化模板：&lt;/strong>;基准测试应包括不同的结构布局或模板。对于大容量模型来说，通过记忆结构从特定模板中提取数据是微不足道的。然而，在实践中，人们需要能够推广到新的模板/布局，这是基准测试中的训练-测试分割应该衡量的一种能力。 &lt;/li>;&lt;li>;&lt;strong>;高质量 OCR&lt;/strong>;：文档应具有高质量&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;光学字符识别&lt;/a>; (OCR) 结果。我们此基准测试的目标是专注于 VRDU 任务本身，并排除 OCR 引擎选择带来的可变性。 &lt;/li>;&lt;li>;&lt;strong>;令牌级注释&lt;/strong>;：文档应包含可以映射回相应输入文本的真实注释，以便每个令牌都可以注释为相应实体的一部分。这与简单地提供要为实体提取的值的文本形成对比。这是生成干净的训练数据的关键，我们不必担心与给定值的偶然匹配。例如，在某些收据中，如果税额为零，“税前总计”字段可能具有与“总计”字段相同的值。具有令牌级别注释会阻止我们生成训练数据，其中匹配值的两个实例都被标记为“total”字段的真实值，从而产生嘈杂的示例。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：自动；margin-right：自动；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn 5yzv-9KAnyJqeLJ5Ugw7k6AiWX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s1600 /Benchmark%20GIF.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdzxo26YbqaeX_nfDIxXKS-x2sxK-lyctkpuLQFCoBvfvFjZY8mJi1dPHWeMKAJbhr3_x2lUCWeJz2a4cn5yzv-9KAnyJqeLJ5Ugw7k6Ai WX2zyGb_otR7GsnnhHcrPRzhmUL7wGcSrp3vksUt001NYaWBgpjb3FZ3D8dj7PdP3Q11Ler3VhCnUn3Z4rCW/s16000/Benchmark%20GIF.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;VRDU 数据集和任务&lt;/h2>; &lt;p>; VRDU 数据集是两个公开可用数据集的组合，&lt;a href=&quot;https://efile.40%;&quot;>; fara.gov/ords/fara/f?p=1235:10&quot;>;注册表&lt;/a>;和&lt;a href=&quot;https://publicfiles.fcc.gov/&quot;>;广告购买表单&lt;/a>;。这些数据集提供了代表现实世界用例的示例，并满足上述五个基准要求。 &lt;/p>; &lt;p>; Ad-buy Forms 数据集包含 641 个包含政治广告详细信息的文档。每份文件都是由电视台和竞选团队签署的发票或收据。文档使用表格、多列和键值对来记录广告信息，例如产品名称、播放日期、总价、发布日期和时间。 &lt;/p>; &lt;p>; 登记表数据集包含 1,915 个文档，其中包含有关外国代理人在美国政府登记的信息。每份文件都记录了涉及需要公开披露的活动的外国代理人的基本信息。内容包括注册人名称、相关部门地址、活动目的等详细信息。 &lt;/p>; &lt;p>; 我们从公众&lt;a href=&quot;https://www.fcc.gov/&quot;>;联邦通信委员会&lt;/a>; (FCC) 和&lt;a href=&quot; https://www.justice.gov/nsd-fara&quot;>;外国代理人登记法&lt;/a>; (FARA) 网站，并使用 &lt;a href=&quot;https://cloud.google.com/ 将图像转换为文本&quot;>;Google Cloud 的&lt;/a>; &lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot;>;OCR&lt;/a>;。我们丢弃了少量几页长的文档，并且处理在两分钟内没有完成。这也使我们能够避免发送很长的文档进行手动注释——对于单个文档来说，这项任务可能需要一个多小时。然后，我们为具有文档标记任务经验的注释者团队定义了模式和相应的标记指令。 &lt;/p>; &lt;p>; 还向注释者提供了一些我们自己标记的示例标记文档。该任务要求注释者检查每个文档，围绕每个文档架构中实体的每次出现绘制边界框，并将该边界框与目标实体相关联。第一轮标注后，一组专家被指派对结果进行审核。修正后的结果包含在已发布的 VRDU 数据集中。有关标记协议和每个数据集架构的更多详细信息，请参阅&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG 2OuFPH6Z5qh63VVHv5q7p5i72av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s1600/Chart.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpTa1feVHH8NlQlSf7sdKAAS2_JFHvGctLBTVeKfKbHa0vq5vUmfLj_RWXyfE_wTETB229_YCGbTSIWkul08cQLawG2OuFPH6Z5qh63VVHv5q7p5i7 2av-_ZqUB_DadodtfaivuXMOY2ORxf2xKvh87Tbza-jrznwSOERzXHFPW0WtdX01wh04i6WYjbx3/s16000/Chart.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;现有学术基准 (&lt;a href=&quot;https://arxiv.org/abs/1905.13538&quot;>;FUNSD&lt;/a>;、&lt;a href=&quot;https://openreview.net/ pdf?id=SJl3z659UH&quot;>;CORD&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=13&quot;>;SROIE&lt;/a>;、&lt;a href=&quot;https:// /arxiv.org/abs/2105.05796&quot;>;Kleister-NDA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2105.05796&quot;>;Kleister-Charity&lt;/a>;、&lt;a href=&quot;https ://wandb.ai/stacey/deepform_v1/reports/DeepForm-Understand-Structured-Documents-at-Scale--VmlldzoyODQ3Njg&quot;>;DeepForm&lt;/a>;）未达到我们为项目确定的五项要求中的一项或多项良好的文档理解基准。 VRDU 满足了所有这些要求。请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;，了解每个数据集的背景以及它们如何未能满足一项或多项要求的讨论。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们构建了四个不同的模型训练集，分别有 10、50、100 和 200 个样本。然后，我们使用三个任务（如下所述）评估 VRDU 数据集：(1) 单模板学习、(2) 混合模板学习和 (3) 看不见的模板学习。对于每项任务，我们在测试集中包含 300 个文档。我们使用测试集上的 &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1 分数&lt;/a>; 来评估模型。 &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;单模板学习&lt;/em>; (STL)：这是最简单的场景，其中训练、测试和验证集仅包含单个模板。这个简单的任务旨在评估模型处理固定模板的能力。当然，我们期望此任务的 F1 分数非常高（0.90+）。 &lt;/li>;&lt;li>;&lt;em>;混合模板学习&lt;/em>;（MTL）：此任务与大多数相关论文使用的任务类似：训练集、测试集和验证集都包含属于同一集的文档模板。我们从数据集中随机抽取文档并构建分割，以确保每个模板的分布在采样过程中不会改变。 &lt;/li>;&lt;li>;&lt;em>;未见模板学习&lt;/em>;（UTL）：这是最具挑战性的设置，我们评估模型是否可以泛化到未见模板。例如，在注册表数据集中，我们使用三个模板中的两个来训练模型，并使用其余一个来测试模型。训练、测试和验证集中的文档来自不相交的模板集。据我们所知，以前的基准测试和数据集没有明确提供这样的任务，旨在评估模型泛化到训练期间未见过的模板的能力。 &lt;/li>; &lt;/ul>; &lt;p>; 目标是能够评估模型的数据效率。在我们的&lt;a href=&quot;https://arxiv.org/abs/2211.15421&quot;>;论文&lt;/a>;中，我们比较了使用 STL、MTL 和 UTL 任务的两个最新模型，并提出了三个观察结果。首先，与其他基准测试不同，VRDU 具有挑战性，并表明模型有很大的改进空间。其次，我们表明，即使是最先进的模型，few-shot 性能也低得惊人，即使是最好的模型，其 F1 分数也低于 0.60。第三，我们表明模型很难处理结构化的重复字段，并且在这些字段上的表现特别差。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们发布了新的&lt;a href=&quot;https:// Research.google/resources/datasets/visually-rich-document-understanding/&quot;>;视觉丰富文档理解&lt;/a>; (VRDU) 数据集，可帮助研究人员更好地跟踪文档理解任务的进度。我们描述了为什么 VRDU 更好地反映了该领域的实际挑战。我们还提出了实验，表明 VRDU 任务具有挑战性，并且与文献中通常使用的数据集（典型的 F1 分数为 0.90+）相比，最近的模型有很大的改进空间。我们希望 VRDU 数据集和评估代码的发布有助于研究团队推进文档理解的最新技术。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;非常感谢王子龙、周一超、魏Wei 和 Chen-Yu Lee 与 Sandeep Tata 共同撰写了这篇论文。感谢 Marc Najork、Riham Mansour 以及 Google Research 和 Cloud AI 团队的众多合作伙伴提供了宝贵的见解。感谢 John Guilyard 创建本文中的动画。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3240052415427459327/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/advances-in-document-understanding.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3240052415427459327&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/08/advances-in-document-understanding.html&quot; rel=&quot;alternate&quot; title=&quot;文档理解方面的进展&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google人工智能&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http ://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/作者>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Zs3cysjsJQsPfOZML1cR03gCO18NmC-KMsoQtctvWr7v_bwJ6MmQMXesUafsi7w53SY50YZOtnBdpAcBaplHXOPV 8P1-X9deoXISeAfq85zUbcPXUOCPJSTsaIanCEIWUUkBNXE9JTU6q3OIgMZi5JqykbiN1x36LR78x4jEka2pL5MBjTdMr_Sn3FNv/s72-c/Document%20understanding%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-595171581401765238&lt;/id>;&lt;发布>;2023-08-08T14:02:00.000-07:00&lt;/发布>;&lt;更新>;2023-08-08T14： 02:34.659-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/ category>;&lt;title type=&quot;text&quot;>;AdaTape：具有自适应计算和动态读写的基础模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：薛福兆、研究实习生和研究科学家 Mostafa Dehghani，Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIq JhdQrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudIEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s2000/adatape.png&quot; style=&quot;显示：无；” />; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1603.08983&quot;>;自适应计算&lt;/a>;是指机器学习系统根据环境变化调整其行为的能力。虽然传统的神经网络具有固定的功能和计算能力，即它们花费相同数量的 FLOP 来处理不同的输入，但具有自适应和动态计算的模型会根据输入的复杂性来调整其专用于处理每个输入的计算预算。输入。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 神经网络中的自适应计算具有吸引力有两个关键原因。首先，引入适应性的机制提供了归纳偏差，可以在解决一些具有挑战性的任务中发挥关键作用。例如，为不同的输入启用不同数量的计算步骤对于解决需要不同深度的建模层次结构的算术问题至关重要。其次，它使从业者能够通过动态计算提供的更大灵活性来调整推理成本，因为可以调整这些模型以花费更多的 FLOP 来处理新输入。 &lt;/p>; &lt;p>; 神经网络可以通过对各种输入使用不同的函数或计算预算来实现自适应。深度神经网络可以被认为是一个基于输入及其参数输出结果的函数。为了实现自适应函数类型，根据输入选择性地激活参数子集，这一过程称为条件计算。在 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;mixture-of-experts&lt;/a>; 的研究中探索了基于函数类型的自适应性，其中每个输入的稀疏激活参数样本是通过路由确定的。自适应计算的另一个研究领域涉及动态计算预算。与标准神经网络（例如 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;）不同，&lt;a href= &quot;https://arxiv.org/abs/2005.14165&quot;>;GPT-3&lt;/a>;、&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling -to.html&quot;>;PaLM&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;ViT&lt;/a>;其计算预算对于不同的样本是固定的，&lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;最近的研究&lt;/a>;表明，自适应计算预算可以提高 Transformer 不足的任务的性能。其中许多工作通过使用动态深度来分配计算预算来实现自适应性。例如，提出了自适应计算时间（ACT）算法来为递归神经网络提供自适应计算预算。 &lt;a href=&quot;https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html&quot;>;通用转换器&lt;/a>;将 ACT 算法扩展到转换器，使计算预算取决于用于每个输入示例或标记的转换器层数。最近的研究，例如 &lt;a href=&quot;https://arxiv.org/abs/2107.05407&quot;>;PonderNet&lt;/a>;，在改进动态停止机制的同时也遵循类似的方法。 &lt;/p>; &lt;p>; 在论文“&lt;a href=&quot;https://arxiv.org/abs/2301.13195&quot;>;具有弹性输入序列的自适应计算&lt;/a>;”中，我们介绍了一种利用自适应计算的新模型，称为&lt;em>;AdaTape&lt;/em>;。该模型是基于 Transformer 的架构，使用一组动态令牌来创建弹性输入序列，与以前的作品相比，提供了关于适应性的独特视角。 AdaTape 使用自适应磁带读取机制来根据输入的复杂性确定添加到每个输入的不同数量的磁带令牌。 AdaTape 实现起来非常简单，提供了一个有效的旋钮来在需要时提高准确性，而且与&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;其他自适应基线&lt;/a相比也更加高效>; 因为它直接将自适应性注入到输入序列中，而不是模型深度中。最后，Adatape 在标准任务（例如图像分类）以及算法任务上提供更好的性能，同时保持良好的质量和成本权衡。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;具有弹性输入序列的自适应计算转换器&lt;/h2>; &lt;p>; AdaTape 使用两种自适应函数类型和动态计算预算。具体来说，对于标记化后的一批输入序列（例如，来自视觉变换器中的图像的非重叠补丁的线性投影），AdaTape 使用表示每个输入的向量来动态选择可变大小的磁带标记序列。 &lt;/p>; &lt;p>; AdaTape 使用称为“磁带库”的令牌库来存储通过自适应磁带读取机制与模型交互的所有候选磁带令牌。我们探索了两种不同的创建磁带库的方法：输入驱动的磁带库和可学习的磁带库。 &lt;/p>; &lt;p>; 输入驱动库的总体思想是从输入中提取一组令牌，同时采用与原始模型令牌生成器不同的方法将原始输入映射到输入标记的序列。这使得能够动态地按需访问来自使用不同观点（例如，不同图像分辨率或不同抽象级别）获得的输入的信息。 &lt;/p>; &lt;p>; 在某些情况下，不同抽象级别的标记化是不可能的，因此输入驱动的磁带库是不可行的，例如当很难进一步拆分 &lt;a href=&quot; 中的每个节点时https://arxiv.org/abs/2012.09699&quot;>;图形转换器&lt;/a>;。为了解决这个问题，AdaTape 提供了一种更通用的方法，通过使用一组可训练向量作为磁带令牌来生成磁带库。这种方法被称为可学习库，可以被视为嵌入层，其中模型可以根据输入示例的复杂性动态检索标记。可学习的银行使 AdaTape 能够生成更灵活的磁带库，使其能够根据每个输入示例的复杂性动态调整其计算预算，例如，更复杂的示例从磁带库中检索更多的令牌，这使得模型不会只使用存储在银行中的知识，但由于输入现在更大，所以还要花费更多的 FLOP 来处理它。 &lt;/p>; &lt;p>; 最后，选定的磁带标记将附加到原始输入并馈送到以下转换器层。对于每个转换器层，在所有输入和磁带标记上使用相同的多头注意力。然而，使用了两种不同的前馈网络（FFN）：一种用于来自原始输入的所有令牌，另一种用于所有磁带令牌。通过对输入和磁带令牌使用单独的前馈网络，我们观察到质量稍好一些。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfda EU1-w8kv7USzuc-VBwGumvHxfccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s1786/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;871&quot; data-original-width=&quot;1786&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCPwSsjmuF0q-H_xnuDxj_ucAX7mBS6TrwqKUczBhxLxIKmxbfh7bwTIgvGAAk73_4vWtxGttp-SEKBTdaYDfdaEU1-w8kv7USzuc-VBwGumvHxf ccBOgk5EBulmfRVu4Ude2Ku8Dp_fME3IS_9TUyAt66k3lrZSsFgnn9_vrjg9U8KnR_wzGReZ2NodfR/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AdaTape 概述。对于不同的样本，我们从磁带库中挑选不同数量的不同标记。磁带库可以由输入驱动，例如通过提取一些额外的细粒度信息，或者它可以是一组可训练向量。自适应磁带读取用于针对不同的输入递归地选择不同长度的磁带令牌序列。然后将这些标记简单地附加到输入并馈送到变压器编码器。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;AdaTape 提供有用的归纳偏差&lt;/h2>; &lt;p>; 我们在奇偶校验上评估 AdaTape，这对于标准 Transformer 来说是一项非常具有挑战性的任务，以研究 AdaTape 中归纳偏差的影响。对于奇偶校验任务，给定序列 1、0 和 -1，模型必须预测序列中 1 数量的偶数或奇数。 Parity 是最简单的非计数器自由或&lt;a href=&quot;https://arxiv.org/abs/2009.11264&quot;>;周期性正则语言&lt;/a>;，但也许令人惊讶的是，标准 Transformer 无法解决该任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3 EoYw4UhzcKK0fGIscOJSv36zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/s866/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;432&quot; data-original-width=&quot;866&quot; height=&quot;319&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSbA6ptb743-TspzKql_9n4i1U1Fpj2jxJUcuXkfcJ4uFxph3UU6Ow8fsqpn51sPigsPWZGdhI6oMp3EoYw4UhzcKK0fGIscOJSv36 zuMbbim1sSKH9DJ1L1I5Li1JQg6XrK_SAChGKT35tDs5_l3mewLdLEenGWGi4p34M-tt3YmloLwbqXj8pdpFd7No/w640-h319/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;对奇偶校验任务的评估。标准 Transformer 和 Universal Transformer 无法执行此任务，两者都显示出随机猜测基线水平的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;尽管进行了短期评估，对于简单的序列，标准 Transformer 和通用 Transformer 都无法执行奇偶校验任务，因为它们无法在模型中维护计数器。然而，AdaTape 的性能优于所有基线，因为它在其输入选择机制中融入了轻量级循环，提供了归纳偏差，可以实现计数器的隐式维护，这在标准 Transformer 中是不可能的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图像分类评估&lt;/h2>; &lt;p>; 我们还在图像分类任务上评估 AdaTape。为此，我们从头开始在 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet-1K&lt;/a>; 上训练 AdaTape。下图显示了 AdaTape 和基线方法的准确性，包括 &lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>; 和 Universal Transformer ViT（UViT 和 U2T）与它们的速度（以每秒由每个代码处理的图像数量来衡量）。在质量和成本权衡方面，AdaTape 的性能比替代自适应变压器基线要好得多。就效率而言，较大的 AdaTape 模型（就参数数量而言）比较小的基线更快。这些结果与&lt;a href=&quot;https://arxiv.org/abs/2110.12894&quot;>;之前的工作&lt;/a>;的发现一致，该发现表明自适应模型深度架构不太适合许多加速器，例如热塑性聚氨酯。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7Z Ih6Rujxnd-H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H -Hxzek9ziXZsw5adSEMZU/s1473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;657&quot; data-original-width=&quot;1473 “高度=“285”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidr0KyW-d8bm2pqTPVrREQTqWzUVHYXvi4Vb9Uq-U5_KT-ksLPZD4YaQ9mN-L7JULw6B5dYbElvKbd8azus7ZIh6Rujxnd- H9ZD-fCiWpI_W0uvAYwugJMW79rng6HHCo2mTB5rj06Pcnf2_Io8zrv2IxGpsJNt6N3he8tA7H-Hxzek9ziXZsw5adSEMZU/w640-h285/image2.png&quot; 宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们通过从头开始在 ImageNet 上进行训练来评估 AdaTape 。对于&lt;a href=&quot;https://arxiv.org/abs/2112.07658&quot;>;A-ViT&lt;/a>;，我们不仅从论文中报告了他们的结果，还通过从头开始训练重新实​​现了A-ViT，即，A-ViT（我们的）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A AdaTape 行为的研究&lt;/h2>; &lt;p>; 除了奇偶校验任务和 ImageNet-1K 上的性能之外，我们还评估了 AdaTape 在 &lt;a href=&quot;https:// /arxiv.org/abs/1707.02968&quot;>;JFT-300M&lt;/a>;验证集。为了更好地理解模型的行为，我们将输入驱动银行上的代币选择结果可视化为热图，其中较浅的颜色意味着该位置被更频繁地选择。热图显示 AdaTape 更频繁地选择中心补丁。这与我们的先验知识相一致，因为中心补丁通常包含更多信息，尤其是在具有自然图像的数据集的背景下，其中主要对象位于图像的中间。这一结果凸显了 AdaTape 的智能性，因为它可以有效地识别信息更丰富的补丁并确定优先级，以提高其性能。 &lt;/p>; &lt;p>;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgk R2VcDoKTicWQ_hufXwrlAcPMoeYqcrCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/s839/ image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;839&quot; height=&quot;270 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIIK51uyt9oTlSp09y6mSx0bgd7dOEbtki2bkzIo_aqK5EOITOgLdRMqIA5InUS2MZsw_0LtguHgkR2VcDoKTicWQ_hufXwrlAcPMoeYq crCMd0QmpbFmyglBc7BNQ1o8xFrbBubjwj2YAlyFlz-OwwUp22FS4XNqmxUTp7o173eDJp_d0ow_I9by15N4g/w640-h270/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr >;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们可视化 AdaTape-B/32（左）和 AdaTape-B/16（右）的磁带令牌选择热图。颜色越热/越浅，意味着该位置的补丁被更频繁地选择。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; AdaTape 的特点是自适应磁带读取机制生成的弹性序列长度。这还引入了一种新的感应偏置，使 AdaTape 能够解决对标准变压器和现有自适应变压器都具有挑战性的任务。通过对图像识别基准进行全面的实验，我们证明了当计算保持恒定时，AdaTape 优于标准转换器和自适应架构转换器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Mostafa Dehghani ，现供职于 Google DeepMind。 &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/595171581401765238/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/595171581401765238&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html&quot; rel=&quot;alternate&quot; title=&quot;AdaTape：具有自适应计算和动态读写的基础模型&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlzhnBmcyTQ940NZQduhyS5G17r9FghwVjYOPW2Ly0pRzug9DdZ7p02z1iK1g9b93xIqJhd Qrg5XVmlcUQqUcSOwQXOGSm6lhcScopZX5J3OTxIsThxmAudiEpkrshjAhipDV4VKFL7Vv1r0Qad77VSiH7rGUD9-E5Jgg1HnzmSkIBt24rd3j1rPN2Ee2N/s72- c/adatape.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-4436946873570093049&lt;/id>;&lt;已发布>;2023-08-03T11:24:00.001-07:00&lt;/已发布>;&lt;已更新>; 2023-08-03T11:24:40.465-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“AI”>;&lt;/类别>;&lt;类别方案=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= &quot;健康&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;多模式医疗人工智能&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：健康人工智能主管 Greg Corrado 、Google 研究部和 Google 研究部工程与研究副总裁 Yossi Matias&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjm G9lDTBWdy5kH_bR3-tqN8KzdhgmrLL_N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3 /s1600/medpalm.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 医学本质上是一门多模式学科。在提供护理时，临床医生通常会解读各种模式的数据，包括医学图像、临床记录、实验室测试、电子健康记录、基因组学等。在过去十年左右的时间里，人工智能系统在特定模式下的特定任务上取得了专家级的性能——一些人工智能系统&lt;a href=&quot;https://www.nature.com/articles/s41591 -019-0447-x&quot;>;处理 CT 扫描&lt;/a>;，而其他人&lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;分析高倍病理切片&lt;/a>;，还有一些&lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/NEJMc2112090&quot;>;寻找罕见的遗传变异&lt;/a>;。这些系统的输入往往是复杂的数据，例如图像，它们通常提供结构化输出，无论是离散等级的形式还是&lt;a href=&quot;https://www.nature.com/articles/s41591-018- 0107-6&quot;>;密集图像分割掩模。&lt;/a>;同时，大型语言模型 (LLM) 的容量和功能具有&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;变得如此先进&lt;/a>;，以至于他们通过用简单的语言进行解释和回应，表现出了对医学知识的理解和专业知识。但是，我们如何将这些功能结合在一起来构建可以利用所有这些来源的信息的医疗人工智能系统？ &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在今天的博文中，我们概述了为法学硕士带来多模式能力的一系列方法，并分享了关于构建多模式医学法学硕士的可处理性的一些令人兴奋的结果，正如最近的三篇研究论文所述。这些论文反过来概述了如何将&lt;em>;从头&lt;/em>;模式引入法学硕士，如何将最先进的医学成像基础模型移植到对话式法学硕士上，以及建立法学硕士的第一步真正的多模式医疗人工智能系统。如果成功成熟，多模式医学法学硕士可能会成为涵盖专业医学、医学研究和消费者应用的新辅助技术的基础。与我们之前的工作一样，我们强调需要与医学界和医疗保健生态系统合作仔细评估这些技术。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;一系列方法&lt;/h2>; &lt;p>; 最近提出了几种构建多模式法学硕士的方法月 [&lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;1&lt;/a>;，&lt;a href=&quot;https:// /www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;2&lt;/a>;，&lt;a href=&quot;https://ai.googleblog.com/2023 /03/palm-e-embodied-multimodal-language.html&quot;>;3&lt;/a>;]，毫无疑问，新方法将在一段时间内继续出现。为了了解为医疗人工智能系统带来新模式的机会，我们将考虑三种广泛定义的方法：工具使用、模型移植和通才系统。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ ZKRMY82JiKLD26G2skhiJEEh8Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s1600/image1.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvoUA0mVph9X7CO01Jonhg0o4mcrKKTcZj2QY69W7xhwxPt0a7DJqq8Az1kOUYQsQXDDFmab1xZIdvGZzvl7P_ZKRMY82JiKLD26G2skhiJEEh8 Hv-PqMRfJNXPx79ts8Q9r1FfPIP8MMpvneShLnXMfy8JNnMLcXMsfvWGXKbKYcBWAriLkaza4u0Lu77/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;工具使用&lt;/h2>; &lt;p>; 在 &lt;em >;工具使用方法，一个中心医学法学硕士将各种模式的数据分析外包给一组针对这些任务独立优化的软件子系统：工具。工具使用的常见助记示例是教法学硕士使用计算器而不是自己做算术。在医疗领域，面对胸部 X 光检查的医学法学硕士可以将该图像转发到放射学人工智能系统并整合该响应。这可以通过子系统提供的应用程序编程接口（API）来完成，或者更奇特的是，两个具有不同专业的医疗人工智能系统进行对话来完成。 &lt;/p>; &lt;p>; 这种方法有一些重要的好处。它允许子系统之间实现最大的灵活性和独立性，使卫生系统能够根据经过验证的子系统性能特征在技术提供商之间混合和匹配产品。此外，子系统之间的人类可读通信通道最大限度地提高了可审核性和可调试性。也就是说，在独立子系统之间实现正确的通信可能很棘手，这会缩小信息传输范围，或暴露通信错误和信息丢失的风险。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;模型嫁接&lt;/h2>; &lt;p>; 一种更综合的方法是采用专门用于每个相关领域，并对其进行调整以直接插入法学硕士 - 将视觉模型&lt;em>;嫁接&lt;/em>;到核心推理代理上。与由法学硕士确定所使用的具体工具的工具使用相反，在模型移植中，研究人员可以在开发过程中选择使用、完善或开发特定模型。在谷歌研究中心最近的两篇论文中，我们表明这实际上是可行的。神经法学硕士通常通过首先将单词映射到&lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings&quot;>;向量嵌入空间&lt;/一个>;。这两篇论文都基于将数据从新模态映射到法学硕士已经熟悉的输入词嵌入空间的想法。第一篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.09018&quot;>;基于个人特定数据的健康多模式法学硕士&lt;/a>;”表明&lt;a href= “https://www.ukbiobank.ac.uk/&quot;>;英国生物银行&lt;/a>;如果我们首先训练神经网络分类器来解释&lt;a href=&quot;https://www.mayoclinic.org/tests -procedures/spirometry/about/pac-20385201&quot;>;呼吸图&lt;/a>;（一种用于评估呼吸能力的方式），然后调整该网络的输出作为 LLM 的输入。 &lt;/p>; &lt;p>; 第二篇论文，“&lt;a href=&quot;https://arxiv.org/abs/2308.01317&quot;>;ELIXR：通过大语言模型和放射学的结合实现通用 X 射线人工智能系统视觉编码器&lt;/a>;”，采用相同的策略，但将其应用于放射学中的全尺寸图像编码器模型。从&lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;了解胸部 X 光检查的基础模型&lt;/a>;开始，已展示为了成为在此模式中构建各种分类器的良好基础，本文描述了训练一个轻量级的医疗信息适配器，该适配器将基础模型的顶层输出重新表示为一系列标记LLM 的输入嵌入空间。尽管对视觉编码器和语言模型都没有进行微调，但生成的系统显示了未经训练的功能，包括&lt;a href=&quot;https://arxiv.org/abs/2210.10163&quot;>;语义搜索&lt;/a>;和&lt;a href=&quot;https://www.nature.com/articles/sdata2018251&quot;>;视觉问答&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFM aMMzTSUu4fif7t9SVsr9MduEcISlA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s1600/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheBucnFOb5C0jzsIah0hyrqru1C6nYPfOOvkMWZ4Rtddx83ElVgGQmCNXLIIwK-0oWFr_Ge2SkZpmedDuNd849eZuImFMaMMzTSUu4fif7t9SVsr9MduEc ISLA9waxskgrI78cjcW3j51A2fciHhPzpp1cTnYXzUoBWfX_KLremukXY04Gh9NNvU5FTShp/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;我们嫁接模型的方法是通过训练医疗信息适配器来将现有或改进的图像编码器的输出映射为法学硕士可理解的形式。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 模型嫁接有很多优点。它使用相对适度的计算资源来训练适配器层，但允许法学硕士在每个数据域中现有的高度优化和验证的模型上构建。将问题模块化为编码器、适配器和 LLM 组件还可以方便在开发和部署此类系统时对各个软件组件进行测试和调试。相应的缺点是，专业编码器和 LLM 之间的通信不再是人类可读的（作为一系列高维向量），并且嫁接过程不仅需要为每个特定领域的编码器构建一个新的适配器，还需要为每个每个编码器的&lt;em>;修订&lt;/em>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;通才系统&lt;/h2>; &lt;p>; 多模式医疗人工智能最根本的方法是构建一个集成的系统，完全通才的系统本身就能够吸收所有来源的信息。在该领域的第三篇论文“&lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;迈向通才生物医学人工智能&lt;/a>;”中，我们没有为每种数据模态使用单独的编码器和适配器，以 &lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>; 为基础构建，PaLM-E 是最近发布的多模式模型单个 LLM (&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;) 和 &lt; a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;单视觉编码器 (ViT)&lt;/a>;。在此设置中，LLM 文本编码器涵盖文本和表格数据模式，但现在所有其他数据都被视为图像并馈送到视觉编码器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42 Hepv4DNsEWebRmtorN05xdpkJ2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s1600/image2.gif&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmrEL1xFdZsVYkVHM16m0yXqYa5SFcdf0HB3iKMABeXqrhWoo9WvPezzWPWxA6t-PVjM_iQYgumiYAshgUQydl42Hepv4DNsEWebRmtorN05xdpkJ 2Ouq6W7mVpgMyrjZSVvSDy9kKgKzQnmYgGtPIpYzx6_50h7LZAgz-puJkvYgZ6yChiczLYrkk9d2I/s16000/image2.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Med-PaLM M 是一个大型多模态生成模型，可以使用相同的模型权重灵活地编码和解释生物医学数据，包括临床语言、成像和基因组学。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们通过在论文中描述的医学数据集上微调整套模型参数，将 PaLM-E 专门应用于医学领域。由此产生的通用医疗人工智能系统是 &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; 的多模式版本，我们称之为 Med-PaLM M。灵活的多模式序列到序列的架构使我们能够在一次交互中交织各种类型的多模态生物医学信息。据我们所知，这是单个统一模型的首次演示，该模型可以解释多模式生物医学数据并在所有任务中使用相同的模型权重集来处理各种任务（论文中有详细评估）。 &lt;/p>; &lt;p>; 这种多模态通才系统方法是我们描述的方法中最雄心勃勃、同时也是最优雅的方法。原则上，这种直接方法最大限度地提高了模式之间的灵活性和信息传输。由于没有 API 来维持兼容性，也没有适配器层的激增，通用方法可以说是最简单的设计。但同样的优雅也是其一些缺点的根源。计算成本通常更高，并且由于单一视觉编码器服务于多种模式，领域专业化或系统可调试性可能会受到影响。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;多模式医疗人工智能的现实&lt;/h2>; &lt;p>;为了在医学中充分利用人工智能，我们需要将经过预测人工智能训练的专家系统的优势与通过生成人工智能实现的灵活性结合起来。哪种方法（或方法组合）在该领域最有用取决于许多尚未评估的因素。通才模型的灵活性和简单性是否比模型嫁接或工具使用的模块化更有价值？哪种方法可以为特定的实际用例提供最高质量的结果？支持医学研究或医学教育与增强医疗实践的首选方法是否不同？回答这些问题需要持续进行严格的实证研究，并与医疗保健提供者、医疗机构、政府实体和医疗保健行业合作伙伴继续进行广泛的直接合作。我们期待着共同寻找答案。 &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4436946873570093049/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/multimodal-medical-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/ html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot; http://www.blogger.com/feeds/8474926331452026626/posts/default/4436946873570093049&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google /2023/08/multimodal-medical-ai.html&quot; rel=&quot;alternate&quot; title=&quot;多模式医疗 AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http ://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/ g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot; 72“网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhU2ypPfvmlgGQW4yp3EbUlJ4rLlIukRC9TDstIe7RV5JTxMo-THDgKPhFYbBUV4m0vKVjmG9lDTBWdy5kH_bR3-tqN8KzdhgmrLL _N2e_glc0WG-HkSm5Nouk7-MU65hu0RH5QWP0nHFNcZpERq9_agfaMqtHjhChbu_dPvWsJfZ8DsxZWnx15hogprRb3/s72-c/medpalm.png“宽度=“72”xmlns：媒体=“http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-5980448298988153366&lt;/id>;&lt;发布>;2023-07-26T09:33:00.003-07:00&lt;/发布>;&lt;更新>;2023-08-01T08:31:25.216-07:00&lt;/更新>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Acoustic Modeling&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;寻找无源域的通用方法适应&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Eleni Triantafillou 和学生研究员 Malik Boudiaf &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4epC-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgy M4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/s1500/notela.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 深度学习最近在广泛的问题和应用中取得了巨大进展，但模型在部署在未知的领域或分布中时常常会出现不可预测的失败。 &lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;无源域适应&lt;/a>; (SFDA) 是一个研究领域，旨在设计适应预训练模型的方法（在一个“源域”）到一个新的“目标域”，仅使用后者的未标记数据。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 设计深度模型的适应方法是一个重要的研究领域。虽然模型和训练数据集规模的不断扩大是其成功的关键因素，但这种趋势的负面后果是训练此类模型的计算成本越来越高，在某些情况下使得大型模型训练&lt;a href=&quot;https:// spectrum.ieee.org/deep-learning-computational-cost&quot;>;不太容易访问&lt;/a>;并且不必要&lt;a href=&quot;https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy- and-resource-strain&quot;>;增加碳足迹&lt;/a>;。缓解这一问题的一个途径是通过设计技术来利用和重用已经训练好的模型来处理新任务或推广到新领域。事实上，在&lt;a href=&quot;https://arxiv.org/abs/1911.02685&quot;>;迁移学习&lt;/a>;的框架下，对模型适应新任务进行了广泛的研究。 &lt;/p>; &lt;p>; SFDA 是这项研究的一个特别实用的领域，因为一些需要适应的现实应用程序会遇到无法获得来自目标域的标记示例的问题。事实上，SFDA 正受到越来越多的关注 [&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;1&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot; >;2&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2110.04202&quot;>;3&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2302.11803&quot;>;4 &lt;/a>;]。然而，尽管出于雄心勃勃的目标，大多数 SFDA 研究都基于一个非常狭窄的框架，考虑图像分类任务中简单的&lt;a href=&quot;https://arxiv.org/abs/2108.13624&quot;>;分布变化&lt;/a>;。 &lt;/p>; &lt;p>; 与这一趋势有很大的不同，我们将注意力转向生物声学领域，其中自然发生的分布变化无处不在，通常以目标标记数据不足为特征，这对从业者来说是一个障碍。因此，在此应用中研究 SFDA 不仅可以让学术界了解现​​有方法的普遍性并确定开放的研究方向，而且还可以直接使该领域的从业者受益，并有助于解决本世纪最大的挑战之一：生物多样性保存。 &lt;/p>; &lt;p>; 在这篇文章中，我们宣布“&lt;a href=&quot;https://arxiv.org/abs/2302.06658&quot;>;寻找一种通用的无源域适应方法&lt;/a>;”，出现在 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023。&lt;/a>;我们表明，最先进的 SFDA 方法在面对现实的分布变化时可能表现不佳甚至崩溃在生物声学中。此外，现有方法相对于彼此的表现与视觉基准中观察到的不同，并且令人惊讶的是，有时表现比根本没有适应更差。我们还提出了NOTELA，这是一种新的简单方法，它在这些转变上优于现有方法，同时在一系列视觉数据集上表现出强大的性能。总的来说，我们得出的结论是，（仅）在常用数据集和分布变化上评估 SFDA 方法，让我们对其相对性能和普遍性产生了短视的看法。为了兑现他们的承诺，SFDA 方法需要在更广泛的分布变化上进行测试，我们主张考虑可以有利于高影响力应用的自然发生的方法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;生物声学中的分布变化&lt;/h2>; &lt;p>; 自然发生的分布变化在生物声学中普遍存在。最大的鸟类歌曲标记数据集是 &lt;a href=&quot;https://www.researchgate.net/publication/280978332_The_Xeno-canto_collection_and_its_relation_to_sound_recognition_and_classification&quot;>;Xeno-Canto&lt;/a>; (XC)，这是用户贡献的野生鸟类录音的集合来自世界各地的鸟类。 XC 中的录音是“焦点”的：它们针对的是在自然条件下捕获的个体，其中所识别的鸟的歌声位于前景。然而，出于持续监控和跟踪的目的，从业者通常更感兴趣的是在通过全向麦克风获得的被动录音（“声景”）中识别鸟类。这是一个&lt;a href=&quot;https://www.researchgate.net/publication/344893963_Overview_of_BirdCLEF_2020_Bird_Sound_Recognition_in_Complex_Acoustic_Environments&quot;>;最近&lt;/a>;&lt;a href=&quot;https://www.sciencedirect.com/science/ Article/pii/S1574954121000273&quot;>;工作&lt;/a>; 展示非常具有挑战性。受到这个现实应用的启发，我们使用在 XC 上预先训练的鸟类分类器作为源模型，以及来自不同地理位置的几个“声景”来研究 SFDA 的生物声学 — &lt;a href=&quot;https://doi. org/10.5281/zenodo.7050013&quot;>;内华达山脉&lt;/a>;（内华达州）； &lt;a href=&quot;https://doi.org/10.1002/ecy.3329&quot;>;Powdermill&lt;/a>; 自然保护区，美国宾夕法尼亚州； &lt;a href=&quot;https://doi.org/10.5281/zenodo.7078498&quot;>;夏威夷&lt;/a>;；美国加利福尼亚州卡普尔斯分水岭； &lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;Sapsucker Woods&lt;/a>;，美国纽约 (SSW)；和 &lt;a href=&quot;https://www.google.com/url?q=https://zenodo.org/record/7525349%23.ZB8z_-xudhE&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1688498539746392&amp;amp; usg=AOvVaw07CsIKIE-dcNyMKFT-n_JT&quot;>;哥伦比亚&lt;/a>; — 作为我们的目标域。 &lt;/p>; &lt;p>; 这种从聚焦域到无源域的转变是巨大的：后者的录音通常具有低得多的信噪比，几只鸟同时发声，以及明显的干扰因素和环境噪音，例如雨或风。此外，不同的音景源自不同的地理位置，从而导致极端的标签变化，因为 XC 中的物种的一小部分将出现在给定位置。此外，正如现实世界数据中常见的那样，源域和目标域都存在明显的类别不平衡，因为某些物种比其他物种更常见。此外，我们还考虑了多标签分类问题，因为每个记录中可能会识别出几只鸟类，这与通常研究 SFDA 的标准单标签图像分类场景有很大不同。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeG afX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQOzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s1378/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;1378&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiE8DlB3xzMeulFglBEgJpnE27myD_Cp5NwLTwpY2K2EmvzdTXfJgaQrtpWgJjnFqQEmm6YyqxUUwpdcBqqgeGafX0c3uU5mLwpGnyQg3BvBIxOlexEClnaWQ OzMZz2KBcHWdfmKHqmdQRAqtSw2xT7U41eyXBRgxFaMk34AfgEKDCJc3WP-k7DNd6VYBOu/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;“焦点→音景”转变的图示。在聚焦域中，录音通常由前景中的单个鸟类发声组成，以高信噪比 (SNR) 捕获，尽管背景中可能还有其他鸟类发声。&lt;strong>; &lt;/strong>;另一方面，音景包含全向麦克风的录音，可以由多只鸟同时发声以及昆虫、雨、汽车、飞机等环境噪音组成。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/表>; &lt;br />; &lt;表align =“中心”cellpadding =“0”cellspacing =“0”class =“tr-caption-container”>; &lt;tbody>; &lt;tr>; &lt;td style =“text-align：左; &quot;>;&lt;b>;音频文件&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align : left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;焦点域&lt;em>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https:// /storage.googleapis.com/chirp-public-bucket/notela-blog-post/XC417991%20-%20Yellow-throated%20Vireo%20-%20Vireo%20flavifrons.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/ em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: left;&quot;>; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em>;音景域名&lt;em>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>; &lt;/sup>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://storage.googleapis.com/chirp-public-bucket/notela-blog-post/ Yetvir-soundscape.mp3&quot;>;&lt;/audio>; &lt;/em>;&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;频谱图图像&lt;/ b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR 4VQQSIFLm6u1t0NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/s936/left.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEhPDc-DkqKwGbNYnk6xu-VhZHd-lDJC1O6J_4Y18jLs9P7FhD6hqvfYlkAwBLcmkVVQZ5htZ8O-3jQVWDWNMiB3baYB9i6RorVvd5dz1JR4VQQSIFLm6u1t0 NgtRBmYfu9zOvOWRRpqyGe0JXMUdoCTA1we9HCvCa2xtdijJcANOkJNVTP1_7aMW3lO_Ey1/w640-h546/left.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;td>; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp ;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iy jBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTCmlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/s936/右.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;936&quot; height=&quot;546&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEiy3DspN9H9uCbzWErZ_MO3cYXAlzSrZkleadyYn8TB2LOFzkHUzBbz8xgDYMI1nIif_UuJdpDD2H5iyjBgD8-aiK9-znIZOSjvU9iYSnnK_q1qsZzXFn5wTC mlyXi_jwXSveGA8EfS8fVUS9sOPbYtNTcoHXMdrZxlgFpsXTh5F87F5FSEIoj1SUjdYglq/w640-h546/right.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;table对齐=“中心”cellpadding=“0”cellspacing=“0”类=“tr-caption-container”样式=“margin-left：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从焦点域（&lt;b>;向左&lt;/ b>;）到音景域（&lt;b>;右&lt;/b>;），根据代表性的音频文件（&lt;b>;顶部&lt;/b>;）和频谱图图像（&lt;b>;底部&lt;/b>;）每个数据集的录音。请注意，在第二个音频剪辑中，鸟鸣声非常微弱；这是音景录音中的一个常见属性，其中鸟叫声不在“前景”。学分：&lt;b>;左：&lt;/b>; XC Sue Riffe &lt;a href=&quot;https://xeno-canto.org/417991&quot;>;录音&lt;/a>; (&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/ &quot;>;CC-BY-NC 许可证&lt;/a>;）。&lt;b>;右：&lt;/b>;摘自 Kahl、Charif 和 Klinck 提供的录音。(2022)“完整注释的音景录音合集来自美国东北部的 SSW 音景数据集的 [&lt;a href=&quot;https://doi.org/10.5281/zenodo.7018483&quot;>;链接&lt;/a>;] (&lt;a href=&quot;https://creativecommons.7018483&quot;>;链接&lt;/a>;) org/licenses/by/4.0/&quot;>;CC-BY 许可证&lt;/a>;)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;最先进的 SFDA 模型在生物声学变化方面表现不佳&lt;/h2>; &lt;p>; 作为起点，我们对六种最先进的 SFDA 方法进行了基准测试我们的生物声学基准，并将它们与&lt;em>;未适应&lt;/em>;基线（源模型）进行比较。我们的发现令人惊讶：毫无例外，现有方法无法在所有目标领域始终优于源模型。事实上，他们的表现常常明显不佳。 &lt;/p>; &lt;p>; 举个例子，&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;Tent&lt;/a>; 是一种最新的方法，旨在通过以下方式使模型对每个示例产生可信的预测：减少模型输出概率的不确定性。虽然 Tent 在各种任务中表现良好，但它对于我们的生物声学任务却效果不佳。在单标签场景中，最小化熵迫使模型自信地为每个示例选择一个类别。然而，在我们的多标签场景中，不存在任何类都应该被选择为存在的约束。再加上显着的分布变化，这可能会导致模型崩溃，导致所有类别的概率为零。其他基准测试方法，例如 &lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;、&lt;a href=&quot;https://openreview.net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt; /a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;帐篷&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98 -Paper.pdf&quot;>;NRC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;灰尘&lt;/a>;和&lt;a href=&quot;https://www.researchgate .net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;伪标签&lt;/a>;是标准 SFDA 基准的强大基线，但也难以完成这项生物声学任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj 38druynuW9zh26no15rOJCj1aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s1434/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;730&quot; data-original-width=&quot;1434&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjivFWRLMTUTu-HZTQ1YZWT-gUl6cJh_9yBsfobahcX3QTTC2Nxc_-34BApQcWcTZ-tmOxgmhwYsu0m5IEIREralqj38druynuW9zh26no15rOJCj1 aThkFnDy3Ai4rVHfTdCfvBghNppEF1Yl3UKXliDS9IcjpHa2Dnq4dpv2_ozg2bAsy5f2IOf1dnK73/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;测试的演变&lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_ precision&quot;>;平均精度&lt;/a>; (mAP)，一种多标签分类的标准度量，贯穿六个音景数据集的适应过程。我们对我们提出的NOTELA和Dropout Student（见下文）以及&lt;a href=&quot;https://arxiv.org/abs/2002.08546&quot;>;SHOT&lt;/a>;、&lt;a href=&quot;https://openreview .net/forum?id=Hk6dkJQFx&quot;>;AdaBN&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10726&quot;>;帐篷&lt;/a>;、&lt;a href=&quot;https://proceedings .neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC&lt;/a>;、&lt;a href=&quot;https://arxiv.org/pdf/2011.13439.pdf&quot;>;灰尘&lt;/a>;和&lt;a href=&quot;https://www.researchgate.net/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks&quot;>;伪标签&lt;/a>;。除了NOTELA之外，所有其他方法都无法持续改进源模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;引入带有拉普拉斯调整的 NOisy 学生 TEacher (NOTELA)&lt;/h2>; &lt;p>; 尽管如此，一个令人惊讶的积极结果脱颖而出：不太著名的 &lt;a href=&quot;https://arxiv.org/abs/ 1911.04252&quot;>;吵闹的学生&lt;/a>;原则看起来很有前途。这种无监督方法鼓励模型在某些目标数据集上重建自己的预测，但在随机噪声的应用下。虽然噪声可能通过各种渠道引入，但我们力求简单并使用&lt;a href=&quot;https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9&quot;>;模型丢失&lt;/a>;作为唯一的噪声源：因此，我们将这种方法称为&lt;em>;辍学学生（DS）&lt;/em>;。简而言之，它鼓励模型在对特定目标数据集进行预测时限制单个神经元（或过滤器）的影响。 &lt;/p>; &lt;p>; DS 虽然有效，但在各种目标域上都面临模型崩溃问题。我们假设发生这种情况是因为源模型最初对这些目标域缺乏信心。我们建议通过直接使用特征空间作为辅助事实来源来提高 DS 稳定性。 NOTELA 的灵感来自于 &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/f5deaeeae1538fb6c45901d524ee2f98-Paper.pdf&quot;>;NRC 的方法&lt; /a>; 和拉普拉斯&lt;a href=&quot;https://www.researchgate.net/publication/220319905_Manifold_Regularization_A_Geometric_Framework_for_Learning_from_Labeled_and_Unlabeled_Examples&quot;>;正则化&lt;/a>;。这种简单的方法如下图所示，在音频和视觉任务中始终显着优于源模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4 VZgRaN-w1jovOwJ4gEXoupksTEpvb5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s1870/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;124&quot; data-original-width=&quot;1870&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd2UZLjX89CQQzQa4p2J6nP5PKEj8dfSkGRfjJ3X354sLPnOpqQjz_1isSCPPSLDaeagai4o2c17qFwsndUL60J4VZgRaN-w1jovOwJ4gEXoupksTEpvb 5HSu2Uz5XALtnph21SoKKK5aG7F0uRN_Z_531v8NRp1G9-c9k3K9Gs2hWL_czsXUjg4eBdPF2/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; &quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-对齐：center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9Q nJb5JxwLFwbmJJN_3jmlGB9-CTug92969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s1080/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgtjOC4ChHgZVlz8iywJQB9G2O3bBtJX_3sheHvZLdH-0ijMrD0qw5Ld2tktfZWgW0RXia6orRurRI0DxpNYRy7nUld-A4z9QnJb5JxwLFwbmJJN_3jmlGB9-CTug92 969vQN3mYl0S5U1xcb7M_Y4z0N3u6Ot8TJqY1uqtvwXozq1xlMGNjFuiS4NWErJL/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;NOTELA 的实际应用。通过完整模型转发音频记录以获得第一组预测，然后通过拉普拉斯正则化（一种基于聚类附近点的后处理形式）对这些预测进行细化。最后，细化的预测为用作&lt;em>;噪声模型&lt;/em>;重建的目标。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 标准的人工图像分类基准无意中限制了我们对 SFDA 方法的真正通用性和鲁棒性的理解。我们主张扩大范围并采用新的评估框架，其中包含生物声学自然发生的分布变化。我们还希望NOTELA 能够成为促进该方向研究的坚实基础。 NOTELA 的强劲表现也许表明了两个因素可以导致开发更通用的模型：首先，开发着眼于更困难问题的方法，其次，支持简单的建模原则。然而，未来仍需开展工作来查明和理解现有方法在解决更困难问题时的失效模式。我们相信，我们的研究代表了朝这个方向迈出的重要一步，为设计具有更大普适性的 SFDA 方法奠定了基础。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本文作者之一，Eleni Triantafillou ，现供职于 Google DeepMind。我们代表 NOTELA 论文的作者发布此博文：Malik Boudiaf、Tom Denton、Bart van Merriënboer、Vincent Dumoulin*、Eleni Triantafillou*（其中 * 表示同等贡献）。我们感谢合著者为本文所做的辛勤工作，以及 Perch 团队其他成员的支持和反馈。&lt;/em>; &lt;/p>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/ a>;&lt;/sup>;请注意，在此音频片段中，鸟鸣声非常微弱；这是音景录音中的一个常见属性，鸟叫声不在“前景”。&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5980448298988153366/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/in-search-of-generalized-method-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论“ type =“text / html”/>;&lt;link href =“http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366”rel =“edit”type =“application/atom+xml”/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5980448298988153366&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:// /blog.research.google/2023/07/in-search-of-generalized-method-for.html&quot; rel=&quot;alternate&quot; title=&quot;寻找无源域适应的通用方法&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnKLO5myVKCSpTQa17lSR3Jj3i3D5Ll87Me9l6CHJ4eyQe_1feJitNR6CYsDURNb7OobVrh3MRU49C4ep C-kkkEL7-kgiJ4MXEIvlxIxc8G7NXZxjzjgyM4nY06lQWVIGEL2yoKnK_mR9P8UyK5T_4b1pnQPOnjW2fhJVYgQkVTk7gxthW-n5WwKDdgmiA/ s72-c/notela.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>; &lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-2519516457542613363&lt;/id>;&lt;已发布>;2023-07-23T14:13:00.002-07:00&lt;/已发布>;&lt;更新>;2023-08-07T10:08:27.713-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 在 ICML 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：项目经理 Cat Armato 、谷歌&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4n vXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s320/Google-ICML-hero.jpg&quot;样式=“显示：无；” />; &lt;p>; Google 各个团队积极开展机器学习 (ML) 领域的理论和应用研究。我们构建机器学习系统来解决语言、音乐、视觉处理、算法开发等领域的深层科学和工程挑战。我们的目标是通过开源工具和数据集、发布我们的工作并积极参加会议，与更广泛的机器学习研究社区建立一个更具协作性的生态系统。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Google 很荣幸成为&lt;a href=&quot;https://icml.cc/virtual/2023/sponsor_list&quot;>;钻石赞助商&lt;/a >; 第 40 届&lt;a href=&quot;https://icml.cc/virtual/2023/index.html&quot;>;国际机器学习会议&lt;/a>; (ICML 2023)，这是一次重要的年度会议，将于今年举行夏威夷檀香山一周。作为 ML 研究领域的领导者，Google 在今年的会议上表现强劲，收到了 120 多篇论文，并积极参与了许多研讨会和教程。 Google 还很荣幸成为 &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; 和 &lt;a href=&quot;https://sites .google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;机器学习中的女性&lt;/a>;研讨会。我们期待分享我们的一些广泛的机器学习研究，并扩大我们与更广泛的机器学习研究社区的合作伙伴关系。 &lt;/p>; &lt;p>; 已注册 ICML 2023？我们希望您能够参观 Google 展位，详细了解解决该领域最有趣挑战的部分令人兴奋的工作、创造力和乐趣。 Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions).请参阅 &lt;a href=&quot;http://www.deepmind.com/blog/google-deepmind-research-at-icml-2023&quot;>;Google DeepMind 博客&lt;/a>;，了解他们在 ICML 2023 上的技术参与情况。&lt;/ p>; &lt;p>; 请参阅下文，了解有关 ICML 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。&lt;/p>; &lt;br>; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会和组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 董事会成员包括：&lt;strong>;&lt;em >;Corinna Cortes&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>; &lt;br>; 辅导主席包括：&lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong >; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin- left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Lhyy8H75KA&quot;>;将视觉 Transformer 扩展到 220 亿个参数&lt;/a>;（请参阅 &lt;a href=&quot;https:// /ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;约西普·乔隆加&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;巴兹尔·穆斯塔法&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;皮奥特·帕德鲁斯基&lt;/em>;&lt;/strong>; >;、&lt;strong>;&lt;em>;乔纳森·希克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯汀·吉尔默&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;安德烈亚斯·斯坦纳&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;玛蒂尔德·卡隆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;罗伯特·盖尔霍斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;易卜拉欣·阿拉卜杜尔莫辛&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Michael Tschannen&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;阿努拉格·阿纳布&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;小王&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡洛斯·里克尔梅&lt;/em>; >;&lt;/strong>;、&lt;strong>;&lt;em>;Matthias Minderer&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Joan Puigcerver、&lt;/em>;&lt;em>;Utku Evci&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Manoj Kumar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Gamaleldin F. Elsayed&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;阿拉文德·马亨德兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Fisher Yu&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿维塔尔·奥利弗&lt;/em>; &lt;/strong>;、&lt;strong>;&lt;em>;芳汀·胡特&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;贾斯米恩·巴斯廷斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;马克·帕特里克·科利尔&lt;/em>;&lt;/strong>; em>;&lt;/strong>;、&lt;strong>; &lt;em>;Alexey Gritsenko&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vighnesh Birodkar&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Cristina Vasconcelos&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;托马斯·门辛克&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;亚历山大·科列斯尼科夫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Filip Pavetić&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;达斯汀·特兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;托马斯Kipf&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mario Lučić&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;丹尼尔·凯泽斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;耶利米·哈姆森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔·霍尔斯比&lt;/em>;&lt;br />;&lt;/strong>; &lt;/ p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=C9NEblP8vS&quot;>;通过推理解码从 Transformers 进行快速推理&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yaniv Leviathan&lt;/em >;&lt;/strong>;、&lt;strong>;&lt;em>;马坦·卡尔曼&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尤西·马蒂亚斯&lt;/em>;&lt;/strong>;&lt;/p>;&lt;p>;&lt;a href= “https://openreview.net/pdf?id=bUFUaawOTk&quot;>;两全其美的策略优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;，&lt;em>;陈-于伟&lt;/em>;，&lt;strong>;&lt;em>;朱利安·齐默特&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf? id=9PJ2V6qvQL&quot;>;机器学习中的流入、流出和互惠&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Walid Krichene&lt;/em>; &lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;Transformers 通过梯度下降在上下文中学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;约翰内斯·冯·奥斯瓦尔德&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;艾文德·尼克拉森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;埃托雷·兰达佐&lt;/em>;&lt;/strong>; >;、&lt;em>;若昂·萨克拉门托&lt;/em>;、&lt;strong>; &lt;em>;亚历山大·莫德文采夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈·日莫吉诺夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Max Vladymyrov&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=EfhmBBrXY2&quot;>;算术采样：并行多样化解码大型语言模型&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Luke Vilnis&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;、&lt;em>;Patrick Murray* &lt;/em>;、&lt;em>;亚历山大·帕索斯*&lt;/em>;、&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https: //openreview.net/pdf?id=ayBKRjGDEI&quot;>;具有可证明近似保证的差分私有层次聚类&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/05/ Differentially-private- clustering-for.html&quot;>;博客文章&lt;/a>;) &lt;br>; &lt;em>;Jacob Imola&lt;/em>;*,&lt;strong>; &lt;em>;Alessandro Epasto&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; >;穆罕默德·马赫迪安&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;文森特·科恩-阿达德&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong >; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=ZVxT2ToHR5&quot;>;用于私有机器学习的多历元矩阵分解机制&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;H.布伦丹·麦克马汉&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;基思·拉什&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;阿卜拉迪普·塔库塔&lt;/em>;&lt;br />;&lt;/strong>; &lt;/ p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=1UaGAhLAsL&quot;>;无论模型选择如何，随机分类噪声都不会击败所有凸潜在助推器&lt;/a>; &lt;br>; &lt;strong>;&lt; em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;理查德·诺克&lt;/em>;&lt;/strong>;、&lt;em>;罗伯特·威廉姆森&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=qw8zAw6mzJ&quot;>;单纯形随机特征&lt;/a>; &lt;br>; &lt;em>;Isaac Reid&lt;/em>;，&lt;strong>; &lt;em>;Krzysztof Choromanski&lt;/ em>;&lt;/strong>;、&lt;em>;Valerii Likhosherstov&lt;/em>;、&lt;em>;阿德里安·韦勒&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/ pdf?id=bF1LVbP493&quot;>;Pix2Struct：屏幕截图解析作为视觉语言理解的预训练&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mandar Joshi&lt; /em>;&lt;/strong>;、&lt;em>;Iulia Turc&lt;/em>;、&lt;strong>; &lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;、&lt;em>;刘芳宇&lt;/em>;、&lt;strong>; &lt;em>; >;Julian Eisenschlos&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Peter Shaw&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;张明伟&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Kristina Tooutanova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net /pdf?id=eIQIcUKs0T&quot;>;Mu&lt;sup>;2&lt;/sup>;SLAM：多任务、多语言语音和语言模型&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yong Cheng&lt;/​​em>;&lt;/strong>;, &lt;strong>; &lt;em>;张宇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;梅尔文·约翰逊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;沃尔夫冈·马切里&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;Ankur Bapna&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/pdf?id=5h42xM0pwn&quot;>;稳健的预算使用单个样本调整节奏&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Santiago Balseiro&lt;/em>;&lt;/strong>;、&lt;em>;Rachitesh Kumar&lt;/em>;*、&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/ em>;&lt;/strong>;、&lt;strong>;&lt;em>;Balasubramanian Sivan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;王迪&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p >;&lt;a href=&quot;https://openreview.net/attachment?id=0bR5JuxaoN&amp;amp;name=pdf&quot;>;基于检索的模型的统计视角&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Soumya Basu&lt;/ em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;、&lt;em>;Manzil Zaheer&lt;/em>;&lt;br />; &lt;/p>; &lt;p>;&lt;a href=&quot; https://openreview.net/pdf?id=XjTcC4EA4P&quot;>;张量分解的近似最佳核心形状&lt;/a>; &lt;br>; &lt;em>;Mehrdad Ghadiri&lt;/em>;，&lt;strong>; &lt;em>;Matthew Fahrbach&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;傅刚&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=rWGp9FbS0Q&amp;amp;name=pdf&quot;>;使用批次的高效列表可解码回归&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Abhimanyu Das&lt;/em>; &lt;/strong>;、&lt;em>;Ayush Jain&lt;/em>;*、&lt;strong>; &lt;em>;孔伟豪&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rajat Sen&lt;/em>;&lt;br />;&lt; /strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=SpFIO5Mdso&amp;amp;name=pdf&quot;>;使用少样本学习高效训练语言模型&lt;/a>; &lt;br >; &lt;strong>;&lt;em>;Sashank J. Reddi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sobhan Miryoosefi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Stefani Karp&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Krishnan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;金承妍&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Sanjiv Kumar&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=Bj76bauv1Q&amp;amp ;name=pdf&quot;>;拟阵上的完全动态子模最大化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt; em>;Silvio Lattanzi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Morteza Zadimoghaddam&lt;/em>;&lt;br />;&lt;/ strong>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://openreview.net/attachment?id=VlEAJkmlMs&amp;amp;name=pdf&quot;>;用于学习组合潜变量模型的 GFlowNet-EM&lt;/a>; &lt;br>; &lt; em>;Edward J Hu&lt;/em>;、&lt;em>;Nikolay Malkin&lt;/em>;、&lt;em>;Moksh Jain&lt;/em>;、&lt;strong>;&lt;em>;Katie Everett&lt;/em>;&lt;/strong>;、&lt;em>; Alexandros Graikos&lt;/em>;、&lt;em>;Yoshua Bengio&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rB0VaD44FZ&quot;>;改进在线学习广告拍卖中点击率预测算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;冯哲&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>;&lt;/strong>;, &lt;em >;Zixin Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sfdKdeczaw&amp;amp;name=pdf&quot;>;大型语言模型难以学习长尾知识&lt;/ a>; &lt;br>; &lt;em>;尼基尔·坎德帕尔&lt;/em>;、&lt;em>;邓海康&lt;/em>;、&lt;strong>; &lt;em>;亚当·罗伯茨&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;埃里克·华莱士&lt;/em>;&lt;/strong>;，&lt;em>;Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=UdiUd99I81&quot;>;多渠道自动出价预算和投资回报率限制&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Yuan Deng&lt;/​​em>;&lt;/strong>;、&lt;em>;Negin Golrezaei&lt;/em>;、&lt;em>;Patrick Jaillet&lt;/em>;、&lt;em>; >;杰森·卓南梁&lt;/em>;，&lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id= ZMvv6laV5b&amp;amp;name=pdf&quot;>;多层神经网络作为希尔伯特空间的可训练阶梯&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;陈正道&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfkSyUJyqg&quot;>;关于用户级私有凸优化&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Badih Ghazi&lt;/em>;&lt;/strong>;,&lt;强>;&lt;em>;Pritish Kamath&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;拉维库马尔&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;Raghu Meka&lt;/em>;&lt;/strong>;， &lt;strong>;&lt;em>;Pasin Manurangsi&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张驰远&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview .net/attachment?id=zAgouWgI7b&amp;amp;name=pdf&quot;>;通过不变表示进行 PAC 泛化&lt;/a>; &lt;br>; &lt;em>;Advait U Parulekar&lt;/em>;，&lt;strong>; &lt;em>;Karthikeyan Shanmugam&lt;/em>;&lt; /strong>;, &lt;em>;Sanjay Shakkottai&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0rZvMIfECW&amp;amp;name=pdf&quot;>;正则化和方差加权回归在线性 MDP 中实现极小极大最优：理论与实践&lt;/a>; &lt;br>; &lt;em>;Toshinori Kitamura&lt;/em>;、&lt;em>;Tadashi Kozuno&lt;/em>;、&lt;em>;Yunhao Tang&lt;/em>; 、&lt;strong>;&lt;em>;Nino Vieillard&lt;/em>;&lt;/strong>;、&lt;em>;Michal Valko&lt;/em>;、&lt;em>;杨文浩&lt;/em>;、&lt;strong>;&lt;em>;梅金城&lt;/em>; &lt;/strong>;、&lt;em>;皮埃尔·梅纳德&lt;/em>;、&lt;em>;Mohammad Gheshlaghi Azar&lt;/em>;、&lt;em>;雷米·穆诺斯&lt;/em>;、&lt;strong>; &lt;em>;奥利维耶·皮埃奎因&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;、&lt;em>;Csaba Szepesvari&lt;/em>;、&lt;em>;Wataru Kumagai&lt;/em>;、&lt;em>;松尾裕&lt;/em>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mrykt39VUw&amp;amp;name=pdf&quot;>;通过最小违规排列加速 Bellman Ford&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;西尔维奥·拉坦齐&lt;/em>;&lt;/strong>;、&lt;em>;奥拉·斯文森&lt;/em>;、&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://openreview.net/attachment?id=LxodbQa62n&amp;amp;name=pdf&quot;>;学习算法的统计不可区分性&lt;/a>; &lt;br>; &lt;em>;Alkis Kalavasis&lt;/em>;，&lt;strong>; &lt;em>;Amin Karbasi&lt; /em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢伊·莫兰&lt;/em>;&lt;/strong>;、&lt;em>;Grigoris Velegkas&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:// openreview.net/pdf?id=G5vKSJVhJL&quot;>;以时隙为中心的模型进行测试时调整&lt;/a>; &lt;br>; &lt;em>;Mihir Prabhudesai&lt;/em>;、&lt;em>;Anirudh Goyal&lt;/em>;、&lt;strong>; &lt;em>;Sujoy Paul&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Gaurav Aggarwal&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>;、&lt;em>;Deepak Pathak&lt;/em>;、&lt;em>;Katerina Fragkiadaki>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=2WEMW6rGgG&quot;>;用户级隐私下直方图估计的边界贡献算法&lt;/a>; &lt;br>; &lt;em>;刘玉涵&lt;/em>;*、&lt;strong>;&lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;朱文楠&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Peter Kairouz&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Marco Gruteser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment ?id=SgeIqUvo4w&amp;amp;name=pdf&quot;>;带有提示和查询的 Bandit 在线线性优化&lt;/a>; &lt;br>; &lt;em>;Aditya Bhaskara&lt;/em>;、&lt;em>;Ashok Cutkosky&lt;/em>;、&lt;strong>; &lt;em >;拉维·库马尔&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;曼尼什·普罗希特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=wagsJnR5GO&amp;amp;name=pdf&quot;>;CLUTR：通过无监督任务表示学习进行课程学习&lt;/a>; &lt;br>; &lt;em>;Abdus Salam Azad&lt;/em>;，&lt;strong>; &lt;em>;Izzeddin Gur&lt;/em>;&lt;/ &lt;em>;贾斯帕·埃姆霍夫&lt;/em>;、&lt;em>;纳撒尼尔·亚历克西斯&lt;/em>;、&lt;strong>; &lt;em>;亚历山大·福斯特&lt;/em>;&lt;/strong>;、&lt;em>;彼得·阿贝尔&lt;/em>;、 &lt;em>;Ion Stoica&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=R3WrLjtzG8&amp;amp;name=pdf&quot;>;CSP：自我监督对比空间预训练地理空间视觉表示&lt;/a>; &lt;br>; &lt;em>;Gengchen Mai&lt;/em>;, &lt;strong>;&lt;em>;Ni Lao&lt;/em>;&lt;/strong>;, &lt;em>;Yutong He&lt;/em>;, &lt; em>;宋嘉明&lt;/em>;、&lt;em>;斯特凡诺·埃尔蒙&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=vd5JYAml0A&amp;amp;name=pdf&quot;>;埃瓦尔德-基于分子图的远程消息传递&lt;/a>; &lt;br>; &lt;em>;Arthur Kosmala&lt;/em>;，&lt;strong>; &lt;em>;Johannes Gasteiger&lt;/em>;&lt;/strong>;，&lt;em>;Nicholas Gau&lt; /em>;, &lt;em>;Stephan Günnemann&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Iey50XHA3g&amp;amp;name=pdf&quot;>;快 (1+ε) -二元矩阵分解的近似算法&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;、&lt;em>;Maximilian Vötsch&lt;/em>;、&lt;strong>;&lt;em>;David Woodruff&lt; /em>;&lt;/strong>;，&lt;em>;Samson Zhou&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=b9opfVNw6O&amp;amp;name=pdf&quot;>;联合线性具有用户级差分隐私的上下文强盗&lt;/a>; &lt;br>; &lt;em>;Ruiquan Huang&lt;/em>;、&lt;em>;Huanyu Zhang&lt;/em>;、&lt;em>;Luca Melis&lt;/em>;、&lt;em>;Milan Shen &lt;/em>;，&lt;strong>; &lt;em>;Meisam Hejazinia&lt;/em>;&lt;/strong>;，&lt;em>;杨静&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net /attachment?id=m21SgZnBWZ&amp;amp;name=pdf&quot;>;研究基于模型的学习在探索和迁移中的作用&lt;/a>; &lt;br>; &lt;em>;Jacob C Walker&lt;/em>;、&lt;em>;Eszter Vértes&lt;/em>; >;、&lt;em>;李亚哲&lt;/em>;、&lt;strong>; &lt;em>;加布里埃尔·杜拉克-阿诺德&lt;/em>;&lt;/strong>;、&lt;em>;Ankesh Anand&lt;/em>;、&lt;em>;Theophane Weber&lt;/em>; ,&lt;em>; Jessica B Hamrick&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=K1sJiHvy02&quot;>;标记差异隐私和私人训练数据发布&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;罗伯特·布萨-费科特&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈斯·穆尼奥斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;奥马尔·赛义德&lt;/em>; >;&lt;/strong>;,&lt;strong>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Q4QFG5Fe4O&amp;amp;name= pdf&quot;>;与分销专业专家一起进行终身语言预训练&lt;/a>; &lt;br>; &lt;em>;Wuyang Chen&lt;/em>;*, &lt;strong>;&lt;em>;Yanqi Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Nan Du&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄艳萍&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;James Laudon&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;陈志峰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;崔嘉儿&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=06djx2x2Rf&amp;amp;name=pdf&quot;>;低排名奖励的多用户强化学习&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Dheeraj Mysore Nagaraj&lt;/em>;&lt;/strong>;，&lt;em>;Suhas S Kowshik&lt;/em>;、&lt;strong>;&lt;em>;Naman Agarwal&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Praneeth Netrapalli&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DwOUndjwiV&amp;amp;name=pdf&quot;>;用于视觉机器人操作的多视图蒙版世界模型&lt;/a >; &lt;br>;&lt;em>;Younggyo Seo&lt;/em>;、&lt;em>;Junsu Kim&lt;/em>;、&lt;em>;Stephen James&lt;/em>;、&lt;strong>;&lt;em>;Kimin Lee&lt;/em>;&lt;/strong>; 、 &lt;em>;申振宇&lt;/em>;、&lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VTpHpqM3Cf&amp;amp;name=pdf&quot; >;PaLM-E：体现的多模态语言模型&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;strong>;&lt;em>;Danny Driess&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;夏飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科里·林奇&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aakanksha Chowdery&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; Brian Ichter&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Ayzaan Wahid&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jonathan Tompson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em >;Quan Vuong&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;余天河&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;黄文龙&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Yevgen Chebotar&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pierre Sermanet&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Daniel Duckworth&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;谢尔盖·莱文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;文森特·范霍克&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卡罗尔·豪斯曼&lt;/em>;&lt;/strong>;、&lt;em>; >;Marc Toussaint&lt;/em>;、&lt;strong>; &lt;em>;Klaus Greff&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;曾安迪&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Igor Mordatch &lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;皮特·弗洛伦斯&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=y8qAZhWbNs &quot;>;采用自动调整压缩的私有联合学习&lt;/a>; &lt;br>; &lt;em>;Enayat Ullah&lt;/em>;*，&lt;strong>;&lt;em>;Christopher A. Choquette-Choo&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/ Attachment?id=7WdMBofQFx&amp;amp;name=pdf&quot;>;使用线性函数逼近的对抗性 MDP 的精致遗憾&lt;/a>; &lt;br>; &lt;em>;Yan Dai&lt;/em>;、&lt;em>;罗海鹏&lt;/em>;、&lt;em>;魏震宇&lt;/em>;、&lt;strong>;&lt;em>;朱利安·齐默特&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ccwSdYv1GI&amp;amp ;name=pdf&quot;>;使用恒定内存将数据集蒸馏扩展到 ImageNet-1K&lt;/a>; &lt;br>; &lt;em>;Justin Cui&lt;/em>;、&lt;em>; Ruoche Wan&lt;/em>;、&lt;strong>;&lt;em>;丝丝&lt;/em>;&lt;/strong>;，&lt;em>;谢祖瑞&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=X7jMTrwuCz&amp;amp;name= pdf&quot;>;采用 AdaGrad 步长的 SGD：对未知参数、无界梯度和仿射方差具有高概率的完全自适应性&lt;/a>; &lt;br>; &lt;em>;Amit Attia&lt;/em>;、&lt;strong>;&lt;em>;Tomer Koren&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6EVUnWGBMU&quot;>;分位数时间差异学习对价值估计的统计优势&lt;/a>; &lt;br >; &lt;em>;马克·罗兰&lt;/em>;、&lt;em>;唐云浩&lt;/em>;、&lt;em>;克莱尔·莱尔&lt;/em>;、&lt;em>;雷米·穆诺斯&lt;/em>;、&lt;strong>;&lt;em>;Marc G. Bellemare&lt;/em>;&lt;/strong>;、&lt;em>;威尔·达布尼&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=iMHNLJRSVz&amp;amp;name=pdf&quot;>;透过图像特征的迷雾揭开位置信息模式的面具&lt;/a>; &lt;br>; &lt;em>;林杰休伯特&lt;/em>;、&lt;em>;曾鸿宇&lt;/em>;、&lt;em>;新英Lee&lt;/em>;、&lt;em>;Maneesh Kumar Singh&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/pdf?id=4UStsbnfVT&quot;>;具有最佳速率的用户级私有随机凸优化&lt;/a>; &lt;br>; &lt;em>;Raef Bassily&lt;/em>;，&lt;strong>; &lt;em>;孙子腾&lt;/em>; >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6MU5xdrO7t&amp;amp;name=pdf&quot;>;一种改进文本中提示集成的简单零样本提示加权技术-图像模型&lt;/a>; &lt;br>; &lt;em>;James Urquhart Allingham&lt;/em>;*、&lt;strong>;&lt;em>;任杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael W Dusenberry&lt;/em>; em>;&lt;/strong>;,&lt;strong>;&lt;em>;顾秀野&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;崔银&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Dustin Tran&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘哲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://openreview.net/attachment?id=mXv2aVqUGG&amp;amp;name=pdf&quot;>;大型语言模型能否推理程序不变量？&lt;/a>; &lt;br>; &lt;strong>;裴可欣&lt;/em >;&lt;/strong>;、&lt;strong>; &lt;em>;大卫·比伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;石肯森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;查尔斯·萨顿&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;殷鹏程&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=pWeQdceMHL&amp;amp;name =pdf&quot;>;持续观察下的并发随机差异隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jay Tenenbaum&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Haim Kaplan&lt;/em>;&lt;/strong >;、&lt;strong>;&lt;em>;伊谢·曼苏尔&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Uri Stemmer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /openreview.net/attachment?id=Xqedp0Iu1S&amp;amp;name=pdf&quot;>;常量：差分隐私持续观察中的细粒度误差&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Hendrik Fichtenberger&lt;/em>;&lt;/强>;，&lt;em>;莫妮卡·亨辛格&lt;/em>;，&lt;em>;Jalaj Upadhyay&lt;/em>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=NfCA622s8O&amp;amp;name= pdf&quot;>;交叉熵损失函数：理论分析与应用&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;，&lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;，&lt;em>; Yutaozhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=5UZYtGEPTt&amp;amp;name=pdf&quot;>;使用在线函数逼近的对抗性上下文 MDP 的高效速率最优遗憾&lt; /a>; &lt;br>; &lt;em>;奥林·利维&lt;/em>;、&lt;strong>;&lt;em>;阿隆·科恩&lt;/em>;&lt;/strong>;、&lt;em>;阿萨夫·卡塞尔&lt;/em>;、&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KrsaROSs8b&amp;amp;name=pdf&quot;>;拟阵约束下流子模最大化的公平性&lt; /a>; &lt;br>; &lt;em>;Marwa El Halabi&lt;/em>;、&lt;em>;Federico Fusco&lt;/em>;、&lt;strong>;&lt;em>;Ashkan Norouzi-Fard&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Jakab Tardos&lt;/em>;&lt;/strong>;，&lt;em>;Jakub Tarnawski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZX4uS605XV&amp;amp;name= pdf&quot;>;Flan Collection：设计有效指令调优的数据和方法&lt;/a>;（请参阅&lt;a href=&quot;https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open。 html&quot;>;博客文章&lt;/a>;) &lt;br>;&lt;em>;Shayne Longpre&lt;/em>;、&lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Tu Vu&lt;/em>; em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿尔伯特·韦伯森&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;郑亨元&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Yi Tay &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Quoc V Le&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;巴雷特·佐夫&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾森·魏&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚当·罗伯茨&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://openreview.net/attachment?id=rzN05i4GOE&amp;amp;name=pdf&quot;>;通过双层优化进行网络控制的图强化学习&lt;/a>; &lt;br>; &lt;em>;Daniele Gammelli&lt;/ em>;、&lt;strong>;&lt;em>;詹姆斯·哈里森&lt;/em>;&lt;/strong>;、&lt;em>;杨凯迪&lt;/em>;、&lt;em>;Marco Pavone&lt;/em>;、&lt;em>;Filipe Rodrigues&lt;/em>;、 &lt;em>;Francisco C. Pereira&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Fgn23Fsmtv&amp;amp;name=pdf&quot;>;多分位数的学习增强私有算法发布&lt;/a>; &lt;br>; &lt;em>;米哈伊尔·霍达克&lt;/em>;*、&lt;strong>;&lt;em>;卡里姆·阿明&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;特拉维斯·迪克&lt;/em>;&lt;/强>;，&lt;强>; &lt;em>;谢尔盖·瓦西尔维茨基&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=uSHBQdWmuC&amp;amp;name=pdf&quot;>; LegendreTron：起义正确的多类损失学习&lt;/a>; &lt;br>; &lt;em>;Kevin H Lam&lt;/em>;、&lt;strong>;&lt;em>;Christian Walder&lt;/em>;&lt;/strong>;、&lt;em>;Spiridon Penev&lt;/em>; >;,&lt;strong>; &lt;em>;Richard Nock&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=VnGIZsmxDG&amp;amp;name=pdf&quot;>;测量编程语言分布的影响&lt;/a>; &lt;br>; &lt;em>;Gabriel Orlanski&lt;/em>;*, &lt;strong>;&lt;em>;肖克凡&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xavier Garcia&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;许志锋&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;约书亚·豪兰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;乔纳森·马尔莫&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;雅各布·奥斯汀&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Rishabh Singh&lt;/em>;&lt;/strong>;、&lt;em>;米歇尔·卡塔斯塔&lt;/em>;&lt;/strong>; em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=f69OtekDi4&quot;>;分布倾斜下的多任务差分隐私&lt;/a>; &lt;br>; &lt;strong>;&lt;em >;Walid Krichene&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;宋爽&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Mukund Sundararajan&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhradeep Thakurta&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张莉&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=hi9UssZdHR&quot;>;Muse：通过 Masked Generative Transformers 生成文本到图像&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Huiwen Chang &lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张翰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;贾里德·巴伯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;AJ Maschinot&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;José Lezama&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陆江&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;凯文·墨菲&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;威廉·弗里曼&lt;/em>;&lt;/strong>;、&lt; strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;李元珍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迪利普·克里希南&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;关于联合平均与循环客户端参与的融合&lt;/a>; &lt;br>; &lt;em>; Yae Jee Cho&lt;/em>;、&lt;em>;Pranay Sharma&lt;/em>;、&lt;em>;Gauri Joshi&lt;/em>;、&lt;strong>;&lt;em>;郑旭&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;Satyen Kale&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;张童&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment? id=GimajxXNc0&amp;amp;name=pdf&quot;>;通过在线到非凸转换实现最优随机非平滑非凸优化&lt;/a>; &lt;br>; &lt;em>;Ashok Cutkosky&lt;/em>;，&lt;strong>;&lt;em>;严厉的梅塔&lt;/em>;&lt;/strong>;，&lt;em>;弗朗西斯科·奥拉博纳&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=4SHQv4cp3I&amp;amp;name=pdf&quot; >;通过定向增强实现域外鲁棒性&lt;/a>; &lt;br>; &lt;em>;Irena Gau&lt;/em>;、&lt;em>;Shiori Sakawa&lt;/em>;、&lt;strong>; &lt;em>;Pang Wei Koh&lt;/em>; &lt;/strong>;、&lt;em>;桥本龙典&lt;/em>;、&lt;em>;梁培西&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=b6Hxt4Jw10&quot; >;无界高斯混合模型的多项式时间和私人学习&lt;/a>; &lt;br>; &lt;em>;Jamil Arbas&lt;/em>;、&lt;em>;Hassan Ashtiani&lt;/em>;、&lt;strong>; &lt;em>;Christopher Liaw&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=nlUAvrMbUZ&amp;amp;name=pdf&quot;>;预计算内存还是即时编码？ A Hybrid Approach to Retrieval Augmentation Makes the Most of Your Compute&lt;/a>; &lt;br>; &lt;em>;Michiel de Jong&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas FitzGerald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;William W. Cohen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1FldU7JzGh&amp;amp;name=pdf&quot;>;Scalable Adaptive Computation for Iterative Generation&lt;/a>; &lt;br>; &lt;em>;Allan Jabri&lt;/em>;*, &lt;strong>;&lt;em>;David J. Fleet&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ting Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HiKPaeowPB&quot;>;Scaling Spherical CNNs&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;, &lt;em>;Jean-Jacques Slotine&lt;/em>;, &lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=0O7b2Y198V&amp;amp;name=pdf&quot;>;STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition&lt;/a>; &lt;br>; &lt;em>;Yucheng Lu&lt;/em>;, &lt;strong>;&lt;em>;Shivani Agrawal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Suvinay Subramanian&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Oleg Rybakov&lt;/em>;&lt;/strong>;, &lt;em>;Christopher De Sa&lt;/em>;, &lt;em>;Amir Yazdanbakhsh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LZt1HIEoAf&amp;amp;name=pdf&quot;>;Stratified Adversarial Robustness with Rejection&lt;/a>; &lt;br>; &lt;em>;Jiefeng Chen&lt;/em>;, &lt;em>;Jayaram Raghuram&lt;/em>;, &lt;em>;Jihye Choi&lt;/em>;,&lt;strong>; &lt;em>;Xi Wu&lt;/em>;&lt;/strong>;, &lt;em>;Yingyu Liang&lt;/em>;, &lt;em>;Somesh Jha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=CnHxxjqkMi&amp;amp;name=pdf&quot;>;When Does Privileged information Explain Away Label Noise?&lt;/a>; &lt;br>; &lt;em>;Guillermo Ortiz-Jimenez&lt;/em>;*,&lt;strong>; &lt;em>;Mark Collier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anant Nawalgaria&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexander D&#39;Amour&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jesse Berent&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rodolphe Jenatton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Effrosyni Kokiopoulou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2bGTacOn8v&amp;amp;name=pdf&quot;>;Adaptive Computation with Elastic Input Sequence&lt;/a>; &lt;br>; &lt;em>;Fuzhao Xue&lt;/em>;*, &lt;strong>;&lt;em>;Valerii Likhosherstov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;em>;Yang You&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Pbaiy3fRCt&amp;amp;name=pdf&quot;>;Can Neural Network Memorization Be Localized?&lt;/a>; &lt;br>; &lt;em>;Pratyush Maini&lt;/em>;,&lt;strong>; &lt;em>;Michael C. Mozer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hanie Sedghi&lt;/em>;&lt;/strong>;, &lt;em>;Zachary C. Lipton&lt;/em>;, &lt;em>;J. Zico Kolter&lt;/em>;,&lt;strong>; &lt;em>;Chiyuan Zhang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Ct2N6RWZpQ&amp;amp;name=pdf&quot;>;Controllability-Aware Unsupervised Skill Discovery&lt;/a>; &lt;br>; &lt;em>;Seohong Park&lt;/em>;,&lt;strong>; &lt;em>;Kimin Lee&lt;/em>;&lt;/strong>;, &lt;em>;Youngwoon Lee&lt;/em>;, &lt;em>;Pieter Abbeel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2Mbo7IEtZW&amp;amp;name=pdf&quot;>;Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network&lt;/a>; &lt;br>; &lt;em>;Yadi Cao&lt;/em>;,&lt;strong>; &lt;em>;Menglei Chai&lt;/em>;&lt;/strong>;, &lt;em>;Minchen Li&lt;/em>;, &lt;em>;Chenfanfu Jiang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zN4oRCrlnM&amp;amp;name=pdf&quot;>;Federated Heavy Hitter Recovery Under Linear Sketching&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Adria Gascon&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Peter Kairouz&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;Graph Generative Model for Benchmarking Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Minji Yoon&lt;/em>;, &lt;em>;Yue Wu&lt;/em>;, &lt;strong>;&lt;em>;John Palowitch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;em>;Russ Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=IFhGrPAn8f&amp;amp;name=pdf&quot;>;H-Consistency Bounds for Pairwise Misranking Loss Surrogates&lt;/a>; &lt;br>; &lt;em>;Anqi Mao&lt;/em>;, &lt;strong>;&lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>;, &lt;em>;Yutao Zhong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DF6ypWrepg&amp;amp;name=pdf&quot;>;Improved Regret for Efficient Online Reinforcement Learning with Linear Function Approximation&lt;/a>; &lt;br>; &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ZXeTCRZJp9&amp;amp;name=pdf&quot;>;Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames&lt;/a>; &lt;br>; &lt;em>;Ondrej Biza&lt;/em>;*,&lt;strong>; &lt;em>;Sjoerd van Steenkiste&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehdi SM Sajjadi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gamaleldin Fathy Elsayed&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aravindh Mahendran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=a35tteW8if&quot;>;Multi-task Off-Policy Learning from Bandit Feedback&lt;/a>; &lt;br>; &lt;em>;Joey Hong&lt;/em>;, &lt;em>;Branislav Kveton&lt;/em>;, &lt;em>;Manzil Zaheer&lt;/em>;, &lt;em>;Sumeet Katariya&lt;/em>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=yv8GUQREda&amp;amp;name=pdf&quot;>;Optimal No-Regret Learning for One-Sided Lipschitz Functions&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Duetting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Guru Guruganesh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jon Schneider&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Joshua Ruizhi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=AwxfYvdPZV&amp;amp;name=pdf&quot;>;Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games&lt;/a>; &lt;br>; &lt;em>;Batuhan Yardim&lt;/em>;, &lt;em>;Semih Cayci&lt;/em>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;em>;Niao He&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=ILMHlUn4k6&amp;amp;name=pdf&quot;>;Regret Minimization and Convergence to Equilibria in General-Sum Markov Games&lt;/a>; &lt;br>; &lt;em>;Liad Erez&lt;/em>;, &lt;em>;Tal Lancewicki&lt;/em>;, &lt;em>;Uri Sherman&lt;/em>;, &lt;strong>;&lt;em>;Tomer Koren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=skDVsmXjPR&amp;amp;name=pdf&quot;>;Reinforcement Learning Can Be More Efficient with Multiple Rewards&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yishay Mansour&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mehryar Mohri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rdOuTlTUMX&quot;>;Reinforcement Learning with History-Dependent Dynamic Contexts&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Guy Tennenholtz&lt;/em>;&lt;/strong>;, &lt;em>;Nadav Merlis&lt;/em>;, &lt;strong>;&lt;em>;Lior Shani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Martin Mladenov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Craig Boutlier&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=sdhcjMzhHN&amp;amp;name=pdf&quot;>;User-Defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems&lt;/a>; &lt;br>; &lt;em>;Marc Anton Finzi&lt;/em>;*,&lt;strong>; &lt;em>;Anudhyan Boral&lt;/em>;&lt;/strong>;, &lt;em>;Andrew Gordon Wilson&lt;/em>;,&lt;strong>; &lt;em>;Fei Sha&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Leonardo Zepeda-Nunez&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=LDBIVZCnLl&amp;amp;name=pdf&quot;>;Discrete Key-Value Bottleneck&lt;/a>; &lt;br>; &lt;em>;Frederik Träuble&lt;/em>;, &lt;em>;Anirudh Goyal&lt;/em>;, &lt;em>;Nasim Rahaman&lt;/em>;,&lt;strong>; &lt;em>;Michael Curtis Mozer&lt;/em>;&lt;/strong>;, &lt;em>;Kenji Kawaguchi&lt;/em>;, &lt;em>;Yoshua Bengio&lt;/em>;, &lt;em>;Bernhard Schölkopf&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nVO6YTca8O&quot;>;DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm&lt;/a>; &lt;br>; &lt;em>;Lisang Ding&lt;/em>;, &lt;em>;Kexin Jin&lt;/em>;,&lt;strong>; &lt;em>;Bicheng Ying&lt;/em>;&lt;/strong>;, &lt;em>;Kun Yuan&lt;/em>;, &lt;em>;Wotao Yin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3Ge74dgjjU&amp;amp;name=pdf&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>; &lt;br>; &lt;em>;Hamed Shirzad&lt;/em>;, &lt;strong>;&lt;em>;Ameya Velingker&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Balaji Venkatachalam&lt;/em>;&lt;/strong>;, &lt;em>;Danica J. Sutherland&lt;/em>;,&lt;strong>; &lt;em>;Ali Kemal Sinop&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=dolp65Z6re&amp;amp;name=pdf&quot;>;Fast, Differentiable and Sparse Top-k: A Convex Analysis Perspective&lt;/a>; &lt;br>; &lt;em>;Michael Eli Sander&lt;/em>;*, &lt;strong>;&lt;em>;Joan Puigcerver&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Josip Djolonga&lt;/em>;&lt;/strong>;, &lt;em>;Gabriel Peyré&lt;/em>;,&lt;strong>; &lt;em>;Mathieu Blondel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=priTMs7n6e&quot;>;Improved Policy Evaluation for Randomized Trials of Algorithmic Resource Allocation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Aditya Mate&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wilder&lt;/em>;,&lt;strong>; &lt;em>;Aparna Taneja&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Milind Tambe&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Yh9sFZQk7Y&amp;amp;name=pdf&quot;>;In Search for a Generalizable Method for Source Free Domain Adaptation&lt;/a>; &lt;br>; &lt;em>;Malik Boudiaf&lt;/em>;*, &lt;strong>;&lt;em>;Tom Denton&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bart van Merrienboer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Dumoulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Eleni Triantafillou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=mSofpvUxCL&amp;amp;name=pdf&quot;>;Learning Rate Schedules in the Presence of Distribution Shift&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Matthew Fahrbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adel Javanmard&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratik Worah&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=slM2r4bRD1&amp;amp;name=pdf&quot;>;Not All Semantics Are Created Equal: Contrastive Self-Supervised Learning with Automatic Temperature Individualization&lt;/a>; &lt;br>; &lt;em>;Zi-Hao Qiu&lt;/em>;, &lt;em>;Quanqi Hu&lt;/em>;, &lt;em>;Zhuoning Yuan&lt;/em>;,&lt;strong>; &lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Lijun Zhang&lt;/em>;, &lt;em>;Tianbao Yang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=EUQsBO975P&amp;amp;name=pdf&quot;>;On the Relationship Between Explanation and Prediction: A Causal View&lt;/a>; &lt;br>; &lt;em>;Amir-Hossein Karimi&lt;/em>;*, &lt;em>;Krikamol Muandet&lt;/em>;,&lt;strong>; &lt;em>;Simon Kornblith&lt;/em>;&lt;/strong>;, &lt;em>;Bernhard Schölkopf&lt;/em>;,&lt;strong>; &lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=qorOnDor89&amp;amp;name=pdf&quot;>;On the Role of Attention in Prompt-Tuning&lt;/a>; &lt;br>; &lt;em>;Samet Oymak&lt;/em>;,&lt;strong>; &lt;em>;Ankit Singh Rawat&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Soltanolkotabi&lt;/em>;, &lt;em>;Christos Thrampoulidis&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2jvwyTm6Pk&amp;amp;name=pdf&quot;>;PLay: Parametrically Conditioned Layout Generation Using Latent Diffusion&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Chin-Yi Cheng&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=631FTQB0UB&amp;amp;name=pdf&quot;>;The Power of Learned Locally Linear Models for Nonlinear Policy Optimization&lt;/a>; &lt;br>; &lt;em>;Daniel Pfrommer&lt;/em>;, &lt;em>;Max Simchowitz&lt;/em>;, &lt;em>;Tyler Westenbroek&lt;/em>;, &lt;em>;Nikolai Matni&lt;/em>;,&lt;strong>; &lt;em>;Stephen Tu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=BDYIci7bVs&amp;amp;name=pdf&quot;>;Relevant Walk Search for Explaining Graph Neural Networks&lt;/a>; &lt;br>; &lt;em>;Ping Xiong&lt;/em>;, &lt;em>;Thomas Schnake&lt;/em>;, &lt;em>;Michael Gastegger&lt;/em>;, &lt;em>;Grégoire Montavon&lt;/em>;,&lt;strong>; &lt;em>;Klaus Robert Muller&lt;/em>;&lt;/strong>;,&lt;em>;Shinichi Nakajima&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RX70NHEPE0&amp;amp;name=pdf&quot;>;Repository-Level Prompt Generation for Large Language Models of Code&lt;/a>; &lt;br>; &lt;em>;Disha Shrivastava&lt;/em>;,&lt;strong>; &lt;em>;Hugo Larochelle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel Tarlow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=r3M5cBtpYq&amp;amp;name=pdf&quot;>;Robust and Private Stochastic Linear Bandits&lt;/a>; &lt;br>; &lt;em>;Vasileios Charisopoulos&lt;/em>;*, &lt;strong>;&lt;em>;Hossein Esfandiari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vahab Mirrokni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=6l9YG3wHA9&amp;amp;name=pdf&quot;>;Simple Diffusion: End-to-End Diffusion for High Resolution Images&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Emiel Hoogeboom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Heek&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tim Salimans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=cw6Zb0sEiT&amp;amp;name=pdf&quot;>;Tied-Augment: Controlling Representation Similarity Improves Data Augmentation&lt;/a>; &lt;br>; &lt;em>;Emirhan Kurtulus&lt;/em>;, &lt;strong>;&lt;em>;Zichao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yann Dauphin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ekin D. Cubuk&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=1d3O0b1rbL&amp;amp;name=pdf&quot;>;Why Is Public Pre-Training Necessary for Private Model Training?&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Arun Ganesh&lt;/em>;&lt;/strong>;, &lt;em>;Mahdi Haghifam&lt;/em>;*, &lt;strong>;&lt;em>;Milad Nasr&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sewoong Oh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Steinke&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Om Thakkar&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Abhradeep Guha Thakurta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lun Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XXC601YWgq&amp;amp;name=pdf&quot;>;A Connection Between One-Step RL and Critic Regularization in Reinforcement Learning&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Benjamin Eysenbach&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthieu Geist&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;, &lt;em>;Ruslan Salakhutdinov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;Beyond Uniform Lipschitz Condition in Differentially Private Optimization&lt;/a>; &lt;br>; &lt;em>;Rudrajit Das&lt;/em>;*, &lt;strong>;&lt;em>;Satyen Kale&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tong Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Sujay Sanghavi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Y5jGkbZ0W3&amp;amp;name=pdf&quot;>;Efficient Graph Field Integrators Meet Point Clouds&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Krzysztof Choromanski&lt;/em>;&lt;/strong>;, &lt;em>;Arijit Sehanobish&lt;/em>;, &lt;em>;Han Lin&lt;/em>;, &lt;em>;Yunfan Zhao&lt;/em>;, &lt;em>;Eli Berger,&lt;/em>; &lt;em>;Tetiana Parshakova&lt;/em>;, &lt;em>;Alvin Pan&lt;/em>;, &lt;em>;David Watkins&lt;/em>;, &lt;em>;Tianyi Zhang&lt;/em>;, &lt;em>;Valerii Likhosherstov&lt;/em>;, &lt;em>;Somnath Basu Roy Chowdhury&lt;/em>;,&lt;strong>; &lt;em>;Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Deepali Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tamas Sarlos&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>;, &lt;em>;Adrian Weller&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=RAeN6s9RZV&amp;amp;name=pdf&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>; &lt;br>; &lt;em>;Riade Benbaki&lt;/em>;, &lt;em>;Wenyu Chen&lt;/em>;, &lt;em>;Xiang Meng&lt;/em>;, &lt;strong>;&lt;em>;Hussein Hazimeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhe Zhao&lt;/em>;&lt;/strong>;, &lt;em>;Rahul Mazumder&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=2M7lwN0DTp&amp;amp;name=pdf&quot;>;Jump-Start Reinforcement Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;em>;Ikechukwu Uchendu&lt;/em>;*, &lt;strong>;&lt;em>;Ted Xiao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Lu&lt;/em>;&lt;/strong>;, &lt;em>;Banghua Zhu&lt;/em>;, &lt;em>;Mengyuan Yan&lt;/em>;, &lt;em>;Joséphine Simon&lt;/em>;, &lt;em>;Matthew Bennice&lt;/em>;, &lt;em>;Chuyuan Fu&lt;/em>;, &lt;em>;Cong Ma&lt;/em>;, &lt;em>;Jiantao Jiao&lt;/em>;,&lt;strong>; &lt;em>;Sergey Levine&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Karol Hausman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=WPjMrOi1KE&amp;amp;name=pdf&quot;>;Learning in POMDPs is Sample-Efficient with Hindsight Observability&lt;/a>; &lt;br>; &lt;em>;Jonathan Lee&lt;/em>;,&lt;strong>; &lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christoph Dann&lt;/em>;&lt;/strong>;, &lt;em>;Tong Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=K0InBsKODr&amp;amp;name=pdf&quot;>;Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Paul Vicol&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=Qh0Gbq3lkh&amp;amp;name=pdf&quot;>;Masked Trajectory Models for Prediction, Representation, and Control&lt;/a>; &lt;br>; &lt;em>;Philipp Wu&lt;/em>;, &lt;em>;Arjun Majumdar&lt;/em>;, &lt;em>;Kevin Stone&lt;/em>;, &lt;em>;Yixin Lin&lt;/em>;,&lt;strong>; &lt;em>;Igor Mordatch&lt;/em>;&lt;/strong>;, &lt;em>;Pieter Abbeel&lt;/em>;, &lt;em>;Aravind Rajeswaran&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=DnTVBs6zbz&amp;amp;name=pdf&quot;>;Overcoming Simplicity Bias in Deep Networks Using a Feature Sieve&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pradeep Shenoy&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=KKaTURYcKG&amp;amp;name=pdf&quot;>;Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions&lt;/a>; &lt;br>; &lt;em>;Boxiang Lyu&lt;/em>;,&lt;strong>; &lt;em>;Zhe Feng&lt;/em>;&lt;/strong>;, &lt;em>;Zachary Robertson&lt;/em>;,&lt;strong>; &lt;em>;Sanmi Koyejo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=UTtYSDO1MK&amp;amp;name=pdf&quot;>;Predictive Flows for Faster Ford-Fulkerson&lt;/a>; &lt;br>; &lt;em>;Sami Davies&lt;/em>;, &lt;em>;Benjamin Moseley&lt;/em>;,&lt;strong>; &lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuyan Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=SVCYSBgFIr&amp;amp;name=pdf&quot;>;Scaling Laws for Multilingual Neural Machine Translation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Patrick Fernandes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Behrooz Ghorbani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=msZrQQAlBA&amp;amp;name=pdf&quot;>;Sequential Monte Carlo Learning for Time Series Structure Discovery&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Feras Saad&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Patton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Douglas Hoffman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rif A. Saurous&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vikash Mansinghka&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=XqyXhjVRxR&amp;amp;name=pdf&quot;>;Stochastic Gradient Succeeds for Bandits&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Jincheng Mei&lt;/em>;&lt;/strong>;, &lt;em>;Zixin Zhong&lt;/em>;,&lt;strong>; &lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alekh Agarwal&lt;/em>;&lt;/strong>;, &lt;em>;Csaba Szepesvari&lt;/em>;,&lt;strong>; &lt;em>;Dale Schuurmans&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nm4NwFfp7a&quot;>;Subset-Based Instance Optimality in Private Estimation&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Travis Dick&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Kulesza&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ziteng Sun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ananda Theertha Suresh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/attachment?id=zvCSNsoyKW&amp;amp;name=pdf&quot;>;The Unreasonable Effectiveness of Few-Shot Learning for Machine Translation&lt;/a>; &lt;br>; &lt;em>;Xavier Garcia&lt;/em>;, &lt;em>;Yamini Bansal&lt;/em>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Maxim Krikun&lt;/em>;&lt;/strong>;, &lt;em>;Melvin Johnson&lt;/em>;, &lt;em>;Orhan Firat&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21552&quot;>;Self-Supervised Learning in Vision: from Research Advances to Best Practices&lt;/a>; &lt;br>; &lt;em>;Xinlei Chen&lt;/em>;, &lt;em>;Ishan Misra&lt;/em>;, &lt;em>;Randall Balestriero&lt;/em>;, &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;, &lt;em>;Christoph Feichtenhofer&lt;/em>;, &lt;em>;Mark Ibrahim&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21560&quot;>;How to DP-fy ML: A Practical Tutorial to Machine Learning with Differential Privacy&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; &lt;strong>;&lt;em>;Sergei Vassilvitskii&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Natalia Ponomareva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://icml.cc/virtual/2023/tutorial/21558&quot;>;Recent Advances in the Generalization Theory of Neural Networks&lt;/a>; &lt;br>; &lt;strong>;&lt;em>;Tengyu Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Damian&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EXPO Day workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;Graph Neural Networks in Tensorflow: A Practical Guide&lt;/a>; &lt;br>; Workshop Organizers include: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan Halcrow&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google sponsored affinity workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/icml-2023&quot;>;LatinX in AI&lt;/a>; (LAXAI) &lt;br>; Platinum Sponsor &lt;br>; Keynote Speaker:&lt;em>; &lt;strong>;Monica Ribero&lt;/strong>;&lt;/em>; &lt;br>; Panelist: &lt;strong>;&lt;em>;Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/wimlworkshop.org/wiml-unworkshop-2023/call-for-participation?authuser=0&quot;>;Women in Machine Learning&lt;/a>; (WiML) &lt;br>; Platinum Sponsor &lt;br>; Panelists:&lt;strong>;&lt;em>; Yao Qin&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://fl-icml2023.github.io/&quot;>;Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Peter Kairouz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/imlh2023/home?authuser=1&quot;>;Interpretable Machine Learning in Healthcare&lt;/a>; (IMLH) &lt;br>; Organizer: &lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://klr-icml2023.github.io/&quot;>;Knowledge and Logical Reasoning in the Era of Data-Driven Learning&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Beliz Günel&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mfpl-icml-2023&quot;>;The Many Facets of Preference-Based Learning&lt;/a>; (MFPL) &lt;br>; Organizer: &lt;strong>;&lt;em>;Robert Busa-Fekete&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mohammad Ghavamzadeh&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://syns-ml.github.io/2023/&quot;>;The Synergy of Scientific and Machine Learning Modelling&lt;/a>; (SynS &amp;amp; ML) &lt;br>; Speaker: &lt;strong>;&lt;em>;Sercan Arik&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://tomworkshop.github.io/&quot;>;Theory of Mind in Communicating Agents&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Pei Zhou&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/aihci/home&quot;>;Artificial Intelligence &amp;amp; Human Computer Interaction&lt;/a>; &lt;br>; Organizer:&lt;em>; &lt;strong>;Yang Li&lt;/strong>;&lt;/em>;,&lt;strong>; &lt;em>;Forrest Huang&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://dmlr.ai/&quot;>;Data-Centric Machine Learning Research&lt;/a>; (DMLR) &lt;br>; Organizer: &lt;strong>;&lt;em>;Alicia Parrish&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Peter Mattson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Isabelle Guyon&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://neuralcompression.github.io/workshop23&quot;>;Neural Compression: from Information Theory to Applications&lt;/a>; &lt;br>; Speaker: &lt;strong>;&lt;em>;Johannes Ballé&lt;/em>;&lt;/strong>; &lt;br>; Panelist: &lt;strong>;&lt;em>;George Toderici&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/teach-icml-23/home&quot;>;Neural Conversational AI Workshop - What&#39;s Left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric) Chatbots?&lt;/a>; &lt;br>; Organizer: &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/scis-workshop-23&quot;>;Spurious Correlations, Invariance and Stability&lt;/a>; (SCIS) &lt;br>; Organizer: &lt;strong>;&lt;em>;Amir Feder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Bryan Perozzi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anton Tsitsulin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brandon Mayer&lt;/em>;&lt;/strong>; &lt;br>; Title: Unsupervised Graph Embedding @ Google (&lt;a href=&quot;https://openreview.net/pdf?id=SpA7YFu02k&quot;>;paper&lt;/a>;, &lt;a href=&quot;https://icml.cc/Expo/Conferences/2023/talk%20panel/25682&quot;>;EXPO workshop&lt;/a>;) &lt;br>; Tuesday, July 25th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Zheng Xu&lt;/em>;&lt;/strong>; &lt;br>; Title: Federated Learning of Gboard Language Models with Differential Privacy (&lt;a href=&quot;https://openreview.net/attachment?id=d8LTNXt97w&amp;amp;name=pdf&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://openreview.net/attachment?id=3QIUvovsgJ&amp;amp;name=pdf&quot;>;paper 2&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot;>;blog post&lt;/a>;) &lt;br>; Tuesday, July 25th at 3:30 PM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Thomas Kipf&lt;/em>;&lt;/strong>; &lt;br>; Title: Self-supervised scene understanding (&lt;a href=&quot;https://arxiv.org/abs/2302.04973&quot;>;paper 1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.11194&quot;>;paper 2&lt;/a>;) &lt;br>; Wednesday, July 26th at 10:30 AM HST &lt;/p>; &lt;p>; Presenters: &lt;strong>;&lt;em>;Johannes von Oswald&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Max Vladymyrov&lt;/em>;&lt;/strong>; &lt;br>; Title: Transformers learn in-context by gradient descent (&lt;a href=&quot;https://openreview.net/pdf?id=tHvXrFQma5&quot;>;paper&lt;/a>;) &lt;br>; Wednesday, July 26th at 3:30 PM HST &lt;/p>; &lt;/div>; &lt;br>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2519516457542613363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-icml-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2519516457542613363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-icml-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICML 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFdIolpEmmBVh-IZFfIHWjpGm5M-7N6hhQ4yBUFTBWZfQ_Wa4Reyz-YmsST7TbfiloQVKIlCaPhJgLj1nhzPr3JesD4nvXkj-FzGykvtGM7oe4MVV_Fidc0q6FuqvHXa8hrMj36TNRn_oP2_42lTJmWl3mGmaCNvqi5IQBx5PCfHKnpegwX-cVf4r3LUkU/s72-c/Google-ICML-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4421751509192561388&lt;/id>;&lt;published>;2023-07-20T09:22:00.002-07:00&lt;/published>;&lt;updated>;2023-07-20T15:31:26.737-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using societal context knowledge to foster the responsible application of AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Donald Martin, Jr., Technical Program Manager, Head of Societal Context Understanding Tools and Solutions (SCOUTS), Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s1100/scouts.png&quot; style=&quot;display: none;&quot; />; &lt;p>; AI-related products and technologies are constructed and deployed in a &lt;em>;societal context&lt;/em>;: that is, a dynamic and complex collection of social, cultural, historical, political and economic circumstances. Because societal contexts by nature are dynamic, complex, non-linear, contested, subjective, and highly qualitative, they are challenging to translate into the quantitative representations, methods, and practices that dominate standard machine learning (ML) approaches and responsible AI product development practices. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The first phase of AI product development is &lt;em>;problem understanding&lt;/em>;, and this phase has tremendous influence over how problems (eg, increasing cancer screening availability and accuracy) are formulated for ML systems to solve as well many other downstream decisions, such as dataset and ML architecture choice. When the societal context in which a product will operate is not articulated well enough to result in robust problem understanding, the resulting ML solutions can be fragile and even propagate unfair biases. &lt;/p>; &lt;p>; When AI product developers lack access to the knowledge and tools necessary to effectively understand and consider societal context during development, they tend to abstract it away. This abstraction leaves them with a shallow, quantitative understanding of the problems they seek to solve, while product users and society stakeholders — who are proximate to these problems and embedded in related societal contexts — tend to have a deep qualitative understanding of those same problems. This qualitative–quantitative divergence in ways of understanding complex problems that separates product users and society from developers is what we call the &lt;em>;problem understanding chasm&lt;/em>;. &lt;/p>; &lt;p>; This chasm has repercussions in the real world: for example, it was the root cause of &lt;a href=&quot;https://www.science.org/doi/10.1126/science.aax2342&quot;>;racial bias discovered by a widely used healthcare algorithm&lt;/a>; intended to solve the problem of choosing patients with the most complex healthcare needs for special programs. Incomplete understanding of the societal context in which the algorithm would operate led system designers to form incorrect and oversimplified causal theories about what the key problem factors were. Critical socio-structural factors, including lack of access to healthcare, lack of trust in the health care system, and underdiagnosis due to human bias,&lt;em>; &lt;/em>;were left out while spending on healthcare was highlighted as a predictor of complex health need. &lt;/p>; &lt;p>; To bridge the problem understanding chasm responsibly, AI product developers need tools that put community-validated and structured knowledge of societal context about complex societal problems at their fingertips — starting with problem understanding, but also throughout the product development lifecycle. To that end, &lt;a href=&quot;https://sites.research.google/scouts/&quot;>;Societal Context Understanding Tools and Solutions&lt;/a>; (SCOUTS) — part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research — is a dedicated research team focused on the mission to “empower people with the scalable, trustworthy societal context knowledge required to realize responsible, robust AI and solve the world&#39;s most complex societal problems.” SCOUTS is motivated by the significant challenge of articulating societal context, and it conducts innovative foundational and applied research to produce structured societal context knowledge and to integrate it into all phases of the AI-related product development lifecycle. Last year we &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;announced&lt;/a>; that &lt;a href=&quot;https://jigsaw.google.com/&quot;>;Jigsaw&lt;/a>;, Google&#39;s incubator for building technology that explores solutions to threats to open societies, leveraged our structured societal context knowledge approach during the data preparation and evaluation phases of model development to scale bias mitigation for their widely used &lt;a href=&quot;https://perspectiveapi.com/&quot;>;Perspective API&lt;/a>; toxicity classifier. Going forward SCOUTS&#39; research agenda focuses on the problem understanding phase of AI-related product development with the goal of bridging the problem understanding chasm. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Bridging the AI problem understanding chasm&lt;/h2>; &lt;p>; Bridging the AI problem understanding chasm requires two key ingredients: 1) a reference frame for organizing structured societal context knowledge and 2) participatory, non-extractive methods to elicit community expertise about complex problems and represent it as structured knowledge. SCOUTS has published innovative research in both areas.&lt;/p>; &lt;br>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 0%; margin-right: 0%;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://sites.research.google/scouts/videos/scouts_pull_intro.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br>; &lt;div style=&quot;line-height: 80%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A societal context reference frame&lt;/h3>; &lt;p>; An essential ingredient for producing structured knowledge is a taxonomy for creating the structure to organize it. SCOUTS collaborated with other RAI-HCT teams (&lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;TasC&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/responsible-ai-at-google-research.html&quot;>;Impact Lab&lt;/a>;), &lt;a href=&quot;https://www.deepmind.com/&quot;>;Google DeepMind&lt;/a>;, and external system dynamics experts to &lt;a href=&quot;https://arxiv.org/abs/2006.09663&quot;>;develop a taxonomic reference frame&lt;/a>; for societal context. To contend with the complex, dynamic, and adaptive nature of societal context, we leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_adaptive_system&quot;>;complex adaptive systems&lt;/a>; (CAS) theory to propose a high-level taxonomic model for organizing societal context knowledge. The model pinpoints three key elements of societal context and the dynamic feedback loops that bind them together&lt;strong>;: &lt;/strong>;agents, precepts, and artifacts. &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Agents&lt;/em>;: These can be individuals or institutions. &lt;/li>;&lt;li>;&lt;em>;Precepts&lt;/em>;: The preconceptions — including beliefs, values, stereotypes and biases — that constrain and drive the behavior of agents. An example of a basic precept is that “all basketball players are over 6 feet tall.” That limiting assumption can lead to failures in identifying basketball players of smaller stature. &lt;/li>;&lt;li>;&lt;em>;Artifacts&lt;/em>;: Agent behaviors produce many kinds of artifacts, including language, data, technologies, societal problems and products. &lt;/li>; &lt;/ul>; &lt;p>; The relationships between these entities are dynamic and complex. Our work hypothesizes that precepts are the most critical element of societal context and we highlight &lt;em>;the problems people perceive&lt;/em>; and &lt;em>;the causal theories they hold about why those problems exist&lt;/em>; as particularly influential precepts that are core to understanding societal context. For example, in the case of racial bias in a medical algorithm described earlier, the causal theory precept held by designers was that&lt;em>; complex health problems would cause healthcare expenditures to go up for all populations&lt;/em>;. That incorrect precept directly led to the choice of healthcare spending as the proxy variable for the model to predict complex healthcare need, which in turn led to the model being biased against Black patients who, due to societal factors such as lack of access to healthcare and underdiagnosis due to bias on average, do not always spend more on healthcare when they have complex healthcare needs. A key open question is how can we ethically and equitably elicit causal theories from the people and communities who are most proximate to problems of inequity and transform them into useful structured knowledge? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimqS89AlspWAfZQKzy834IfSmwnQQWDS_O5HL6Z1YLXHzGzk-xHclJtICHZQ3JDxDmkk1EnNagK_BQtAbX4xFztb0EuHKISLx_O3JuU3tiSxtTn4ZayBDdiagae2fPJm-ohpqOw8q4_NQn2Ekbd1l1HejgQeEqh786iE9rHsQb-1I15U-HdqNRvvRRe23R/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s1600/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFrhnPWJB-Km7ZfAu2U2tra2ZLb7Eh9GzuQpiHT3WeUlnf0AgUfWBj3c_UkPd1_sYYFTNrZWIlmuRU2wocznJ7llhUGWYUYbM0rh8X8pLkehxeUiSHdlors19GZ0WuikJGz6ZX5n_izjMXOYkFdvjfykuNtNCKRaBq4UPt4-WVUx47XDpe8z6kWIkH9xQd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Taxonomic version of societal context reference frame.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Working with communities to foster the responsible application of AI to healthcare&lt;/h3>; &lt;p>; Since its inception, SCOUTS has worked&lt;a href=&quot;https://accelerate.withgoogle.com/stories/exploring-systems-dynamics-inclusive-ml-and-societal-impact-meet-googlers-donald-martin-and-jamaal-sebastian-barnes&quot;>; to build capacity&lt;/a>; in historically marginalized communities to articulate the broader societal context of the complex problems that matter to them using a practice called community based system dynamics (CBSD). &lt;a href=&quot;https://en.wikipedia.org/wiki/System_dynamics&quot;>;System dynamics&lt;/a>; (SD) is a methodology for articulating causal theories about complex problems, both &lt;em>;qualitatively&lt;strong>; &lt;/strong>;&lt;/em>;as causal loop and &lt;a href=&quot;https://online.visual-paradigm.com/knowledge/business-design/what-is-stock-and-flow-diagram/&quot;>;stock and flow diagrams&lt;/a>; (CLDs and SFDs, respectively) and &lt;em>;quantitatively&lt;/em>; as simulation models. The inherent support of visual qualitative tools, quantitative methods, and collaborative model building makes it an ideal ingredient for bridging the problem understanding chasm. CBSD is a &lt;a href=&quot;https://medium.com/people-ai-research/qa-donald-martin-on-community-based-system-dynamics-and-machine-learning-3fa21e42c680&quot;>;community-based, participatory variant of SD&lt;/a>; specifically focused on building capacity within communities to collaboratively describe and model the problems they face as causal theories, directly without intermediaries. With CBSD we&#39;ve witnessed community groups learn the basics and begin drawing CLDs within 2 hours. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s1147/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;863&quot; data-original-width=&quot;1147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2cVBSKbnXB_CLVi4VFBmsRh5i69wYqPk0bPUz-3gVF2ch5-nAbrrmx6vR5XnkAqdNQGXGN0c3YgIZqRBC88EiVM13DHEalidgKkR6wlOfSrBFLHUi1muyYrvhzVaG0QmOxzOvr0P0ELZikJF1Lv2laoTycVlCPoAyHu8kzQfqRJgIRViSNmuMNTE2xfjz/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community members learning system dynamics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There is a huge potential for AI to &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9955430/#:~:text=AI%20algorithms%20can%20analyze%20medical,diseases%20more%20accurately%20and%20quickly.&quot;>;improve medical diagnosis&lt;/a>;. But the safety, equity, and reliability of AI-related health diagnostic algorithms depends on diverse and balanced training datasets. An open challenge in the health diagnostic space is the dearth of training sample data from historically marginalized groups. SCOUTS collaborated with the &lt;a href=&quot;https://d4bl.org/&quot;>;Data 4 Black Lives&lt;/a>; community and CBSD experts to produce &lt;a href=&quot;https://arxiv.org/abs/2305.13485&quot;>;qualitative and quantitative causal theories&lt;/a>; for the data gap problem. The theories include critical factors that make up the broader societal context surrounding health diagnostics, including cultural memory of death and trust in medical care. &lt;/p>; &lt;p>; The figure below depicts the causal theory generated during the collaboration described above as a CLD. It hypothesizes that trust in medical care influences all parts of this complex system and is the key lever for increasing screening, which in turn generates data to overcome the data diversity gap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s1024/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;835&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooXyKzBCIbg_CBBMw78gF09_hZv-riaVnInp9tRQNKQEJTpep80vIeVr-HSgoIJ-97aD7uz4Up6SG8IMwAYkGuHWQDf-c0Tv-jIUaI9FFsbeizaBGuJvd09xT5V3WAthpwXnGL_E2XPhAHq6TFKtjvY_EKvpng-nqpJFpJV4efXnVtGcol_VNkcKWGOkI/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s1072/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;561&quot; data-original-width=&quot;1072&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfYxvQjXR_6zmQtooSuGcUaGP3-Zg1W991_OcmfRvC_pKN52eFGOaHW0e-OH39qGM9fY3Q2VqXJA6VZCfz8OVBbR1WnMytixErRGQ4d650dPO6530-WAZYSHzIiJoKrVmS0H4U3oc2CVBVmbQFCEdzQIe_SArF6IZp2Cv5NH7cje6XDh9ibAKiQY5oKK6y/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Causal loop diagram of the health diagnostics data gap&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; These community-sourced causal theories are a first step to bridge the problem understanding chasm with trustworthy societal context knowledge. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; As discussed in this blog, the problem understanding chasm is a critical open challenge in responsible AI. SCOUTS conducts exploratory and applied research in collaboration with other teams within Google Research, external community, and academic partners across multiple disciplines to make meaningful progress solving it. Going forward our work will focus on three key elements, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;: &lt;/p>; &lt;ol>; &lt;li>;Increase awareness and understanding of the problem understanding chasm and its implications through talks, publications, and training. &lt;/li>;&lt;li>;Conduct foundational and applied research for representing and integrating societal context knowledge into AI product development tools and workflows, from conception to monitoring, evaluation and adaptation. &lt;/li>;&lt;li>;Apply community-based causal modeling methods to the AI health equity domain to realize impact and build society&#39;s and Google&#39;s capability to produce and leverage global-scale societal context knowledge to realize responsible AI. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s960/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKYJZra1pZj6fEuZ_IuVTPBliEAJcGtVO5KI6LeXLqR0dr4kT0VCJU0wwe1S8vBxcmyazkNpI34bu5R47NwzxWmm44YBynVUFUOPZGLcG6jB6LuIWdEGCcfBSCxKAx3LGLwBvqP0Vf5qRm8qQjQcvvWW2eCUrH3a8LR73DaY9XL-CEtVDRJfZIwhBpA99m/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SCOUTS flywheel for bridging the problem understanding chasm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thank you to John Guilyard for graphics development, everyone in SCOUTS, and all of our collaborators and sponsors.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4421751509192561388/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/using-societal-context-knowledge-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4421751509192561388&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot; rel=&quot;alternate&quot; title=&quot;Using societal context knowledge to foster the responsible application of AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoaJIKwp3izoj_P77_py-4_y4ng5B_HEW6HUB0QpkS_t4zc7p1w8FuG8x1IRK7Rw0R6uJIgUheqqr0yvz4YRykesH-IRKiV_PaXCr7MdBuaLzrlbqTgiIm3UM0rYcmVmKUlA5KqOjbqRdI3mwbTSyusxGhWisrXNS-C62JbiCHJTNh826JMQ2KtD9nu1vu/s72-c/scouts.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8123471024413959829&lt;/id>;&lt;published>;2023-07-18T13:15:00.000-07:00&lt;/published>;&lt;updated>;2023-07-18T13:15:15.633-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SimPer: Simple self-supervised learning of periodic targets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Daniel McDuff, Staff Research Scientist, and Yuzhe Yang, Student Researcher, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s320/SimPer%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning from periodic data (signals that repeat, such as a heart beat or the daily temperature changes on Earth&#39;s surface) is crucial for many real-world applications, from &lt;a href=&quot;https://cloud.google.com/blog/topics/sustainability/weather-prediction-with-ai&quot;>;monitoring weather systems&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/&quot;>;detecting vital signs&lt;/a>;. For example, in the environmental remote sensing domain, periodic learning is often needed to enable nowcasting of environmental changes, such as &lt;a href=&quot;https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html&quot;>;precipitation patterns or land surface temperature&lt;/a>;. In the health domain, learning from video measurement has shown to extract (quasi-)periodic vital signs such as &lt;a href=&quot;https://www.ahajournals.org/doi/full/10.1161/JAHA.118.008585&quot;>;atrial fibrillation&lt;/a>; and &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9298820&quot;>;sleep apnea episodes&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Approaches like &lt;a href=&quot;https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html&quot;>;RepNet&lt;/a>; highlight the importance of these types of tasks, and present a solution that recognizes repetitive activities within a single video. However, these are supervised approaches that require a significant amount of data to capture repetitive activities, all labeled to indicate the number of times an action was repeated. Labeling such data is often challenging and resource-intensive, requiring researchers to manually capture gold-standard temporal measurements that are synchronized with the modality of interest (eg, video or satellite imagery). &lt;/p>; &lt;p>; Alternatively, self-supervised learning (SSL) methods (eg, &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;), which leverage a large amount of unlabeled data to learn representations that capture periodic or quasi-periodic temporal dynamics, have demonstrated success in &lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j.html&quot;>;solving classification tasks&lt;/a>;. However, they overlook the intrinsic periodicity (ie, the ability to identify if a frame is part of a periodic process) in data and fail to learn robust representations that capture periodic or frequency attributes. This is because periodic learning exhibits characteristics that are distinct from prevailing learning tasks. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s1338/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1338&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNgzwAxx5KRHGUrD3VQuCNzT9xfn_GGD5EB1JtS6UiZoz0YTZoysDcoiOl7HBKeJ3c7y_zjWCqsdJZ5ajZ3h7LZQ-jpiotuXKst9fkSWVJ1rmQ_o27DsfO2jNB9K-dbNy3INpnEf5UrgwDKS0uPN2UV5N49EeF2locr2dqm2KczMrIBq7MJ6KRgcjUSr7N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature similarity is different in the context of periodic representations as compared to static features (eg, images). For example, videos that are offset by short time delays or are reversed should be similar to the original sample, whereas videos that have been upsampled or downsampled by a factor &lt;em>;x&lt;/em>; should be different from the original sample by a factor of &lt;em>;x&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To address these challenges, in “&lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>;”, published at the eleventh &lt;a href=&quot;https://iclr.cc/&quot;>;International Conference on Learning Representations&lt;/a>; (ICLR 2023), we introduced a self-supervised contrastive framework for learning periodic information in data. Specifically, SimPer leverages the temporal properties of periodic targets using &lt;em>;temporal self-contrastive learning&lt;/em>;, where positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the &lt;em>;same&lt;/em>; input instance. We propose &lt;em>;periodic feature similarity&lt;/em>; that explicitly defines how to measure similarity in the context of periodic learning. Moreover, we design a generalized contrastive loss&lt;em>; &lt;/em>;that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE loss&lt;/a>; to a soft regression variant that enables contrasting over continuous labels (frequency). Next, we demonstrate that SimPer effectively learns period feature representations compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts. Finally, we are excited to release the &lt;a href=&quot;https://github.com/YyzHarry/SimPer&quot;>;SimPer code repo&lt;/a>; with the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The SimPer framework&lt;/h2>; &lt;p>; SimPer introduces a temporal self-contrastive learning framework. Positive and negative samples are obtained through periodicity-invariant and periodicity-variant augmentations from the same input instance. For temporal video examples, periodicity-invariant changes are cropping, rotation or flipping, whereas periodicity-variant changes involve increasing or decreasing the speed of a video. &lt;/p>; &lt;p>; To explicitly define how to measure similarity in the context of periodic learning, SimPer proposes periodic feature similarity. This construction allows us to formulate training as a contrastive learning task. A model can be trained with data without any labels and then fine-tuned if necessary to map the learned features to specific frequency values. &lt;/p>; &lt;p>; Given an input sequence &lt;em>;x&lt;/em>;, we know there&#39;s an underlying associated periodic signal. We then transform &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo- speed or frequency labels for the unlabeled input x. &lt;/p>; &lt;p>; Conventional &lt;a href=&quot;https://en.wikipedia.org/wiki/Similarity_measure&quot;>;similarity measures&lt;/a>; such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; emphasize strict proximity between two feature vectors, and are sensitive to index shifted features (which represent different time stamps), reversed features, and features with changed frequencies. In contrast, periodic feature similarity should be high for samples with small temporal shifts and or reversed indexes, while capturing a continuous similarity change when the feature frequency varies. This can be achieved via a similarity metric in the frequency domain, such as the distance between two &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;>;Fourier transforms&lt;/a>;. &lt;/p>; &lt;p>; To harness the intrinsic continuity of augmented samples in the frequency domain, SimPer designs a generalized contrastive loss that extends the classic &lt;a href=&quot;https://arxiv.org/pdf/1807.03748.pdf&quot;>;InfoNCE&lt;/a>; loss to a soft regression variant that enables contrasting over continuous labels (frequency). This makes it suitable for regression tasks, where the goal is to recover a continuous signal, such as a heart beat. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;361&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyJVTOhQLRJWYg_cmUF12sBZogdM1JmY6hAgtCS4QOTj38dko6vuHe1jD71yBMInHreHZGwO62nkA7ip5AdJn514SUvHgAMX9yAQ6PASq4CB8nK-T9JpCm607Xdwd_Vt6aO8_w5dBcJ86sFh4qYJCX121vi504HVNl6vFeFfghwWXKxP8kmlLcOvmJ578-/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SimPer constructs negative views of data through transformations in the frequency domain. The input sequence &lt;em>;x&lt;/em>; has an underlying associated periodic signal. SimPer transforms &lt;em>;x&lt;/em>; to create a series of speed or frequency altered samples, which changes the underlying periodic target, thus creating different negative views. Although the original frequency is unknown, we effectively devise pseudo speed or frequency labels for unlabeled input &lt;em>;x&lt;/em>; (periodicity-variant augmentations &lt;em>;τ&lt;/em>;). SimPer takes transformations that do not change the identity of the input and defines these as periodicity-invariant augmentations &lt;em>;σ&lt;/em>;, thus creating different positive views of the sample. Then, it sends these augmented views to the encoder &lt;em>;f&lt;/em>;,&lt;em>; &lt;/em>;which extracts corresponding features. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate SimPer&#39;s performance, we benchmarked it against state-of-the-art SSL schemes (eg, &lt;a href=&quot;https://github.com/google-research/simclr&quot;>;SimCLR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo v2&lt;/a>;, &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf&quot;>;BYOL&lt;/a>;, &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Spatiotemporal_Contrastive_Video_Representation_Learning_CVPR_2021_paper.html&quot;>;CVRL&lt;/a>;) on a set of six diverse periodic learning datasets for common real-world tasks in human behavior analysis, environmental remote sensing, and healthcare. Specifically, below we present results on heart rate measurement and exercise repetition counting from video. The results show that SimPer outperforms the state-of-the-art SSL schemes across all six datasets, highlighting its superior performance in terms of data efficiency, robustness to spurious correlations, and generalization to unseen targets. &lt;/p>; &lt;p>; Here we show quantitative results on two representative datasets using SimPer pre-trained using various SSL methods and fine-tuned on the labeled data. First, we pre-train SimPer using the &lt;a href=&quot;https://sites.google.com/corp/view/ybenezeth/ubfcrppg&quot;>;Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy&lt;/a>; (UBFC) dataset, a human &lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram#Remote_photoplethysmography&quot;>;photoplethysmography&lt;/a>; and heart rate prediction dataset, and compare its performance to state-of-the-art SSL methods. We observe that SimPer outperforms SimCLR, MoCo v2, BYOL, and CVRL methods. The results on the human action counting dataset, &lt;a href=&quot;https://sites.google.com/corp/view/repnet&quot;>;Countix&lt;/a>;, further confirm the benefits of SimPer over others methods as it notably outperforms the supervised baseline. For the feature evaluation results and performance on other datasets, please refer to the &lt;a href=&quot;https://openreview.net/forum?id=EKpMeEV0hOo&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiApM7aTyk5jFFe41tGmnwcEAGdJeOwDYawhzuCm1qvU-qJSF7qSaiOq-z0ifa1ecQSaBzfkUgBP5XCsk0iYorceK9d3jOmsnfyKugH4NE4o9MGTYhgr1YERtiT3r8woAomWfVcAj0Luo69YkTpElHx7MdRdg90eYZT4_DffXMdqHh8wNo5b8jS65F3z9Bt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results of SimCLR, MoCo v2, BYOL, CVRL and SimPer on the Univ. Bourgogne Franche-Comté Remote PhotoPlethysmoGraphy (UBFC) and Countix datasets. Heart rate and repetition count performance is reported as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and applications&lt;/h2>; &lt;p>; We present SimPer, a self-supervised contrastive framework for learning periodic information in data. We demonstrate that by combining a temporal self-contrastive learning framework, periodicity-invariant and periodicity-variant augmentations, and continuous periodic feature similarity, SimPer provides an intuitive and flexible approach for learning strong feature representations for periodic signals. Moreover, SimPer can be applied to various fields, ranging from environmental remote sensing to healthcare. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Yuzhe Yang, Xin Liu, Ming-Zher Poh, Jiang Wu, Silviu Borac, and Dina Katabi for their contributions to this work. &lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8123471024413959829/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/simper-simple-self-supervised-learning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8123471024413959829&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/simper-simple-self-supervised-learning.html&quot; rel=&quot;alternate&quot; title=&quot;SimPer: Simple self-supervised learning of periodic targets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipB_9hAxZvnElIMZ-TN-dvR0POGa65v8yaZCPs44rLweLTIuHtvXe9knDYpU3h4ydbjKk9F-bLE7WTNgx0MgzUMxHa-RXTg7Ch4nGU7rqSAMYpdxzDI7xuirzahNzDKHR9olCqeXv5vK0dTtCQPm1Ws6_364n0_6-2dR_u0zB0Qiabo_g92yjjDcc4SEhz/s72-c/SimPer%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2013323302512835157&lt;/id>;&lt;published>;2023-07-13T14:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-13T14:01:18.428-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Symbol tuning improves in-context learning in language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s1200/SymbolTuning.png&quot; style=&quot;display: none;&quot; />; &lt;p>; A key feature of human intelligence is that humans can learn to perform new tasks by reasoning using only a few examples. Scaling up language models has unlocked a range of new applications and paradigms in machine learning, including the ability to perform challenging reasoning tasks via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>;. Language models, however, are still sensitive to the way that prompts are given, indicating that they are not reasoning in a robust manner. For instance, language models often require heavy prompt engineering or phrasing tasks as instructions, and they exhibit unexpected behaviors such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;performance on tasks being unaffected even when shown incorrect labels&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.08298&quot;>;Symbol tuning improves in-context learning in language models&lt;/a>;”, we propose a simple fine-tuning procedure that we call &lt;em>;symbol tuning&lt;/em>;, which can improve in-context learning by emphasizing input–label mappings. We experiment with symbol tuning across &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Flan-PaLM&lt;/a>; models and observe benefits across various settings. &lt;/p>; &lt;ul>; &lt;li>;Symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. &lt;/li>;&lt;li>;Symbol-tuned models are much stronger at algorithmic reasoning tasks. &lt;/li>;&lt;li>;Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s1035/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;663&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj2oPVvCdXLd5kCDz5SqLMrvTnRbdgkV3JErHCDOO9o5ktcjnISfNMA9-qhB85Tf1WP-Nl0o1mt5mj-Q33OhlyzxNtLSVv6a5GyZbqlLtn8eg26jahmN0tWgL81Ae-pX0o83AkulO14loLuhumBj4kjWp1Hc94kIYHJuYpkj6B4AIfnA5XRUlBKYstkYO5C/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of symbol tuning, where models are fine-tuned on tasks where natural language labels are replaced with arbitrary symbols. Symbol tuning relies on the intuition that when instruction and relevant labels are not available, models must use in-context examples to learn the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Motivation&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a common fine-tuning method that has been shown to improve performance and allow models to better follow in-context examples. One shortcoming, however, is that models are not forced to learn to use the examples because the task is redundantly defined in the evaluation example via instructions and natural language labels. For example, on the left in the figure above, although the examples can help the model understand the task (sentiment analysis), they are not strictly necessary since the model could ignore the examples and just read the instruction that indicates what the task is. &lt;/p>; &lt;p>; In symbol tuning, the model is fine-tuned on examples where the instructions are removed and natural language labels are replaced with semantically-unrelated labels (eg, “Foo,” “Bar,” etc.). In this setup, the task is unclear without looking at the in-context examples. For example, on the right in the figure above, multiple in-context examples would be needed to figure out the task. Because symbol tuning teaches the model to reason over the in-context examples, symbol-tuned models should have better performance on tasks that require reasoning between in-context examples and their labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s1035/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;237&quot; data-original-width=&quot;1035&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh14anaT3eoh7u4OwgANyXgXFJhpeGvMRuGzAX19N_uwbNXVD42DhPPR7BExQbeBtxS_RJPFFq6lTCjjEsK0WRpkHGD5wn-dhblwvaPR-dyFvSTFV6-cfXwhkpwxhybEHLx8UFgAZ-lQ752hlVLNCXzGcbXGsLI5WsW6_iYMBrQ7HhK3gPJ0-TCjjVMiZYi/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Datasets and task types used for symbol tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Symbol-tuning procedure&lt;/h2>; &lt;p>; We selected 22 publicly-available &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) datasets that we use for our symbol-tuning procedure. These tasks have been widely used in the past, and we only chose classification-type tasks since our method requires discrete labels. We then remap labels to a random label from a set of ~30K arbitrary labels selected from one of three categories: integers, character combinations, and words. &lt;/p>; &lt;p>; For our experiments, we symbol tune &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, the instruction-tuned variants of &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;. We use three different sizes of Flan-PaLM models: Flan-PaLM-8B, Flan-PaLM-62B, and Flan-PaLM-540B. We also tested &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;Flan-cont-PaLM-62B&lt;/a>; (Flan-PaLM-62B at 1.3T tokens instead of 780B tokens), which we abbreviate as 62B-c. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s861/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;168&quot; data-original-width=&quot;861&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlcUmIJh5QKWV54Q2_T0w7_E6L2jfVdmi-pqRql9qyxuAfaeQEj0jT-NVwmD9uy0YCbhiimcu-o6oHfx-KYDmkppbTrj1MPeYbXQcnCH9LWfrUF6P5BZDA_uAyNpwuGhxIG-mB29g9Sy8BBLIe1J30fQPGkfL_ihpjSJeAJXEA1dejbNp2SMMkid9y2JjE/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a set of ∼300K arbitrary symbols from three categories (integers, character combinations, and words). ∼30K symbols are used during tuning and the rest are held out for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; We want to evaluate a model&#39;s ability to perform unseen tasks, so we cannot evaluate on tasks used in symbol tuning (22 datasets) or used during instruction tuning (1.8K tasks). Hence, we choose 11 NLP datasets that were not used during fine-tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;In-context learning&lt;/h2>; &lt;p>; In the symbol-tuning procedure, models must learn to reason with in-context examples in order to successfully perform tasks because prompts are modified to ensure that tasks cannot simply be learned from relevant labels or instructions. Symbol-tuned models should perform better in settings where tasks are unclear and require reasoning between in-context examples and their labels. To explore these settings, we define four in-context learning settings that vary the amount of reasoning required between inputs and labels in order to learn the task (based on the availability of instructions/relevant labels) &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s936/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;936&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiv8nn0it0JSInLKSqKdNZ1wSWXabbu2ZDhSLpwS9igKhzUr7Gv3c9UJW0Uv1C_rjk1QkBeziPmpmRcJ-l1IoGR0w-C-W464xL9-LLL3iT2ldA-LMIzyGMOqLbVUTf726KZOddAEt1_X7HER2jwZiPZWE-HLVC-sTir8iOyzR_jQ0SHHmZ-ecoJhwJeCWXK/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the availability of instructions and relevant natural language labels, models may need to do varying amounts of reasoning with in-context examples. When these features are not available, models must reason with the given in-context examples to successfully perform the task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Symbol tuning improves performance across all settings for models 62B and larger, with small improvements in settings with relevant natural language labels (+0.8% to +4.2%) and substantial improvements in settings without relevant natural language labels (+5.5% to +15.5%). Strikingly, when relevant labels are unavailable, symbol-tuned Flan-PaLM-8B outperforms FlanPaLM-62B, and symbol-tuned Flan-PaLM-62B outperforms Flan-PaLM-540B. This performance difference suggests that symbol tuning can allow much smaller models to perform as well as large models on these tasks (effectively saving ∼10X inference compute). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s900/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;495&quot; data-original-width=&quot;900&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinG4f3gWmKCS7dA9L_yevNCcIAlo5BmKkUbexBlxmUeeEIWL0RwmxQjzRmL_5dtAhMUlne3BOoMwcWYgec9FSXIg7iHwxwZbQZ5gB1sUiziuOIlBOyZst3t-UNogDPj-9YY590gdHcrSIPbwsekUrAZCr2GP027XUkCXmUODak4tTPso64v-iXOzRncj5z/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Large-enough symbol-tuned models are better at in-context learning than baselines, especially in settings where relevant labels are not available. Performance is shown as average model accuracy (%) across eleven tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithmic reasoning&lt;/h2>; &lt;p>; We also experiment on algorithmic reasoning tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;. There are two main groups of tasks: 1) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/list_functions&quot;>;List functions&lt;/a>; — identify a transformation function (eg, remove the last element in a list) between input and output lists containing non-negative integers; and 2) &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/simp_turing_concept&quot;>;simple turing concepts&lt;/a>; — reason with binary strings to learn the concept that maps an input to an output (eg, swapping 0s and 1s in a string). &lt;/p>; &lt;p>; On the list function and simple turing concept tasks, symbol tuning results in an average performance improvement of 18.2% and 15.3%, respectively. Additionally, Flan-cont-PaLM-62B with symbol tuning outperforms Flan-PaLM-540B on the list function tasks on average, which is equivalent to a ∼10x reduction in inference compute. These improvements suggest that symbol tuning strengthens the model&#39;s ability to learn in-context for unseen task types, as symbol tuning did not include any algorithmic data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s908/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;496&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjK0_IZeIVT02P7f-DHZppYY3mnCThReIhYFWI-qUAlU-wYeCunSQt-RK9-tiPTMnXEqQz1NBPsjpyq9fUTaaA2J5XN9DWlDaL_Yd029OgdGjksYj86u-V0k_ZB-jv86kD9O6zgBqDZZLHz2KjVltl07za0uHd2iTFiUkUh7jIyy7iK9DENrSr9rGpvFbaL/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models achieve higher performance on list function tasks and simple turing concept tasks. (A–E): categories of list functions tasks. (F): simple turing concepts task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In the flipped-label experiment, labels of in-context and evaluation examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override prior knowledge. &lt;a href=&quot;https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot;>;Previous work&lt;/a>; has shown that while pre-trained models (without instruction tuning) can, to some extent, follow flipped labels presented in-context, instruction tuning degraded this ability. &lt;/p>; &lt;p>; We see that there is a similar trend across all model sizes — symbol-tuned models are much more capable of following flipped labels than instruction-tuned models. We found that after symbol tuning, Flan-PaLM-8B sees an average improvement across all datasets of 26.5%, Flan-PaLM-62B sees an improvement of 33.7%, and Flan-PaLM-540B sees an improvement of 34.0%. Additionally, symbol-tuned models achieve similar or better than average performance as pre-training–only models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s914/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;914&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh12Wit8rudLwjP_3qoZD-ZNKfgdfq-M_Za0iQzZpJR6h-e7xpA-QANn89kZvj_2S-Rb8-cfFJOeVQwrOSK4VS-3TlqSJuy0c1X7eLyYBBrZAavdTrwgZMpt4thR0aJ2EmdpZG6gbek1IoHUsu7YBX7FRdRWKfNHnEczoppaTQZ33dXZekcConSyYb3hyNb/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Symbol-tuned models are much better at following flipped labels presented in-context than instruction-tuned models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented symbol tuning, a new method of tuning models on tasks where natural language labels are remapped to arbitrary symbols. Symbol tuning is based off of the intuition that when models cannot use instructions or relevant labels to determine a presented task, it must do so by instead learning from in-context examples. We tuned four language models using our symbol-tuning procedure, utilizing a tuning mixture of 22 datasets and approximately 30K arbitrary symbols as labels. &lt;/p>; &lt;p>; We first showed that symbol tuning improves performance on unseen in-context learning tasks, especially when prompts do not contain instructions or relevant labels. We also found that symbol-tuned models were much better at algorithmic reasoning tasks, despite the lack of numerical or algorithmic data in the symbol-tuning procedure. Finally, in an in-context learning setting where inputs have flipped labels, symbol tuning (for some datasets) restores the ability to follow flipped labels that was lost during instruction tuning. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; Through symbol tuning, we aim to increase the degree to which models can examine and learn from input–label mappings during in-context learning. We hope that our results encourage further work towards improving language models&#39; ability to reason over symbols presented in-context. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. This work was conducted by Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. We would like to thank our colleagues at Google Research and Google DeepMind for their advice and helpful discussions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2013323302512835157/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/symbol-tuning-improves-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2013323302512835157&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/symbol-tuning-improves-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Symbol tuning improves in-context learning in language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpcvWqT7OBMHzleUaxiCaADL7SGlXJOakpKo6HsXwWHuERHv1mYDtz0UaLaKNoY6f7cgS7CVack55eHRIcUrDPd9ZY01EKCCUTsxkVAXh3qD7rSw0x2VWj17yKWoTYQD6xiIj-7Zp2vsPaT9ew4UpT6ec4LI0R0nKfb4Sbd1vEjyQEQW0lvbroBBFWfZ1h/s72-c/SymbolTuning.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8984134419460793359&lt;/id>;&lt;published>;2023-07-11T10:00:00.010-07:00&lt;/published>;&lt;updated>;2023-07-11T10:44:15.111-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An open-source gymnasium for machine learning assisted computer architecture design&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Amir Yazdanbakhsh, Research Scientist, and Vijay Janapa Reddi, Visiting Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s1200/ArchGym-animation2.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_architecture&quot;>;Computer Architecture&lt;/a>; research has a long history of developing simulators and tools to evaluate and shape the design of computer systems. For example, the &lt;a href=&quot;https://ieeexplore.ieee.org/iel5/2/21180/00982917.pdf?casa_token=M_ZAQmgCbKkAAAAA:Y9wFTB9OQBwzXXpw7kNbq4asdlCCHYRaq7qsqcRBpYyh6734aHmr57ll4Vb1_zcG5ukNvNONNQ&quot;>;SimpleScalar&lt;/a>; simulator was introduced in the late 1990s and allowed researchers to explore various &lt;a href=&quot;https://en.wikipedia.org/wiki/Microarchitecture&quot;>;microarchitectural&lt;/a>; ideas. Computer architecture simulators and tools, such as &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;gem5&lt;/a>;, &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>;, and many more have played a significant role in advancing computer architecture research. Since then, these shared resources and infrastructure have benefited industry and academia and have enabled researchers to systematically build on each other&#39;s work, leading to significant advances in the field. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Nonetheless, computer architecture research is evolving, with industry and academia turning towards machine learning (ML) optimization to meet stringent domain-specific requirements, such as &lt;a href=&quot;https://ai.googleblog.com/2021/02/machine-learning-for-computer.html&quot;>;ML for computer architecture&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2201.01863.pdf&quot;>;ML for TinyML acceleration&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2022/03/offline-optimization-for-architecting.html&quot;>;DNN&lt;/a>; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9804604&quot;>;accelerator&lt;/a>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3503222.3507767&quot;>;datapath optimization&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1394608.1382172?casa_token=3g46kAHeN0QAAAAA:5pKdYXamA_vstsO5LqA0_4rRy5o2Z46Ks-OJLDUe7ZSLtDfkaSS5i0zLfMe3Y7gAuY2bFMZ1yGOEMA&quot;>;memory controllers&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40&quot;>;power consumption&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/7430287&quot;>;security&lt;/a>;, and &lt;a href=&quot;https://www.sigarch.org/tag/privacy-preserving-computing/&quot;>;privacy&lt;/a>;. Although prior work has demonstrated the benefits of ML in design optimization, the lack of strong, reproducible baselines hinders fair and objective comparison across different methods and poses several challenges to their deployment. To ensure steady progress, it is imperative to understand and tackle these challenges collectively. &lt;/p>; &lt;p>; To alleviate these challenges, in “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3579371.3589049&quot;>;ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design&lt;/a>;”, accepted at &lt;a href=&quot;https://www.iscaconf.org/isca2023/program/&quot;>;ISCA 2023&lt;/a>;, we introduced ArchGym, which includes a variety of computer architecture simulators and ML algorithms. Enabled by ArchGym, our results indicate that with a sufficiently large number of samples, any of a diverse collection of ML algorithms are capable of finding the optimal set of architecture design parameters for each target problem; &lt;i>;no one solution is necessarily better than another&lt;/i>;. These results further indicate that selecting the optimal hyperparameters for a given ML algorithm is essential for finding the optimal architecture design, but choosing them is non-trivial. We &lt;a href=&quot;https://bit.ly/ArchGym&quot;>;release&lt;/a>; the code and dataset across multiple computer architecture simulations and ML algorithms.&lt;/p>; &lt;br />; &lt;h2>;Challenges in ML-assisted architecture research &lt;/h2>; &lt;p>; ML-assisted architecture research poses several challenges, including: &lt;/p>; &lt;ol>; &lt;li>;For a specific ML-assisted computer architecture problem (eg, finding an optimal solution for a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_random-access_memory&quot;>;DRAM&lt;/a>; controller) there is no systematic way to identify optimal ML algorithms or hyperparameters (eg, learning rate, warm-up steps, etc.). There is a wider range of ML and heuristic methods, from &lt;a href=&quot;https://arxiv.org/abs/2008.03639&quot;>;random walk&lt;/a>; to &lt;a href=&quot;http://incompleteideas.net/book/RLbook2020.pdf&quot;>;reinforcement learning&lt;/a>; (RL), that can be employed for &lt;a href=&quot;https://en.wikipedia.org/wiki/Design_space_exploration&quot;>;design space exploration&lt;/a>; (DSE). While these methods have shown noticeable performance improvement over their choice of baselines, it is not evident whether the improvements are because of the choice of optimization algorithms or hyperparameters.&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;Thus, to ensure reproducibility and facilitate widespread adoption of ML-aided architecture DSE, it is necessary to outline a systematic benchmarking methodology.&lt;/em>; &lt;br />;&lt;br />; &lt;/li>; &lt;li>;While computer architecture simulators have been the backbone of architectural innovations, there is an emerging need to address the trade-offs between accuracy, speed, and cost in architecture exploration. The accuracy and speed of performance estimation widely varies from one simulator to another, depending on the underlying modeling details (eg, &lt;a href=&quot;https://biblio.ugent.be/publication/2968322/file/6776727.pdf&quot;>;cycle&lt;/a>;-&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2024716.2024718?casa_token=lAA5J1sHkdUAAAAA:844lNoz81Z6gH1_CkFvWIxg2J_mF54e3xwE7qEQ1cXf73EakxY16wHgMed-f-zIkC1q6_OUX11TzJQ&quot;>;accurate&lt;/a>; vs. &lt;a href=&quot;https://arxiv.org/abs/2210.03894&quot;>;ML&lt;/a>;-&lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;based&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1808.07412&quot;>;proxy&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1803.02329&quot;>;models&lt;/a>;). While analytical or ML-based proxy models are nimble by virtue of discarding low-level details, they generally suffer from high prediction error. Also, due to commercial licensing, there can be strict &lt;a href=&quot;https://ieeexplore.ieee.org/document/7945172&quot;>;limits on the number of runs collected from a simulator&lt;/a>;. Overall, these constraints exhibit distinct performance vs. sample efficiency trade-offs, affecting the choice of optimization algorithm for architecture exploration. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;It is challenging to delineate how to systematically compare the effectiveness of various ML algorithms under these constraints.&lt;/em>; &lt;br />; &lt;br />; &lt;/li>; &lt;li>;Finally, the landscape of ML algorithms is rapidly evolving and some ML algorithms need data to be useful. Additionally, rendering the outcome of DSE into meaningful artifacts such as datasets is critical for drawing insights about the design space. &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;em>;In this rapidly evolving ecosystem, it is consequential to ensure how to amortize the overhead of search algorithms for architecture exploration. It is not apparent, nor systematically studied how to leverage exploration data while being agnostic to the underlying search algorithm.&lt;/em>; &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;ArchGym design&lt;/h2>; &lt;p>; ArchGym addresses these challenges by providing a unified framework for evaluating different ML-based search algorithms fairly. It comprises two main components: 1) the ArchGym environment and 2) the ArchGym agent. The environment is an encapsulation of the architecture cost model — which includes latency, throughput, area, energy, etc., to determine the computational cost of running the workload, given a set of architectural parameters — paired with the target workload(s). The agent is an encapsulation of the ML algorithm used for the search and consists of hyperparameters and a guiding policy. The hyperparameters are intrinsic to the algorithm for which the model is to be optimized and can significantly influence performance. The policy, on the other hand, determines how the agent selects a parameter iteratively to optimize the target objective. &lt;/p>; &lt;p>; Notably, ArchGym also includes a standardized interface that connects these two components, while also saving the exploration data as the ArchGym Dataset. At its core, the interface entails three main signals: &lt;i>;hardware state&lt;/i>;, &lt;i>;hardware parameters&lt;/i>;, and &lt;i>;metrics&lt;/i>;. These signals are the bare minimum to establish a meaningful communication channel between the environment and the agent.&amp;nbsp;Using these signals, the agent observes the state of the hardware and suggests a set of hardware parameters to iteratively optimize a (user-defined) reward. The reward is a function of hardware performance metrics, such as performance, energy consumption, etc.&amp;nbsp;&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s1226/ArchGym-animation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;722&quot; data-original-width=&quot;1226&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvRR9Pr_oSRVs3Cj415xJE-WawaTq96dTM5hM_yK0JKFz6m8953OZS9O1_lXW41E-32-fiWTSiJCF4j5mbC4DU4GinQsi7Aom0EJNcSW6TM2HdJxoKxE-k7m1nF08G135gkOqa81sgYLOBqbg4hoqbMpOcBPnAvhO7f8DakSAlAIGN0Z3lMEHWBnuqCTJ4/s16000/ArchGym-animation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;ArchGym comprises two main components: the ArchGym environment and the ArchGym agent. The ArchGym environment encapsulates the cost model and the agent is an abstraction of a policy and hyperparameters. With a standardized interface that connects these two components, ArchGym provides a unified framework for evaluating different ML-based search algorithms fairly while also saving the exploration data as the ArchGym Dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ML algorithms could be equally favorable to meet user-defined target specifications&lt;/h2>; &lt;p>; Using ArchGym, we empirically demonstrate that across different optimization objectives and DSE problems, &lt;em>;at least one set of hyperparameters exists that results in the same hardware performance as other ML algorithms&lt;/em>;. A poorly selected (random selection) hyperparameter for the ML algorithm or its baseline can lead to a misleading conclusion that a particular family of ML algorithms is better than another. We show that with sufficient hyperparameter tuning, different search algorithms, even &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;random walk&lt;/a>; (RW), are able to identify the best possible reward. However, note that finding the right set of hyperparameters may require exhaustive search or even luck to make it competitive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1158&quot; data-original-width=&quot;1999&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5rDADw1hFu1S10kKW9nyHvd2ugneOa-dmB_Go7aWkNfChkiX6ciGL06GFkc9JxE-hxRv8ULs_xn4ctC7bSIZ6bk_ZdpAR3Vsg8KDRnf5JofK4bypztLinlak2JwSP1t_2BGJ7fOn5e5W-J_r5jHXkwPjFDWJLT_I-3m9l4RZSdKMG5TZR55-gi5W-p8/w640-h371/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;With a sufficient number of samples, there exists at least one set of hyperparameters that results in the same performance across a range of search algorithms. Here the dashed line represents the maximum normalized reward. &lt;i>;Cloud-1&lt;/i>;, &lt;i>;cloud-2&lt;/i>;, &lt;i>;stream&lt;/i>;, and &lt;i>;random&lt;/i>; indicate four different memory traces for &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAMSys&lt;/a>; (DRAM subsystem design space exploration framework).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset construction and high-fidelity proxy model training&lt;/h2>; &lt;p>; Creating a unified interface using ArchGym also enables the creation of datasets that can be used to design better data-driven ML-based proxy architecture cost models to improve the speed of architecture simulation. To evaluate the benefits of datasets in building an ML model to approximate architecture cost, we leverage ArchGym&#39;s ability to log the data from each run from DRAMSys to create four dataset variants, each with a different number of data points. For each variant, we create two categories: (a) Diverse Dataset, which represents the data collected from different agents (&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;>;ACO&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;>;GA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;>;RW&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.&quot;>;BO&lt;/a>;), and (b) ACO only, which shows the data collected exclusively from the ACO agent, both of which are released along with ArchGym. We train a proxy model on each dataset using &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;>;random forest regression&lt;/a>; with the objective to predict the latency of designs for a &lt;a href=&quot;https://github.com/tukl-msd/DRAMSys&quot;>;DRAM simulator&lt;/a>;. Our results show that: &lt;/p>; &lt;ol>; &lt;li>;As we increase the dataset size, the average normalized &lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;root mean squared error&lt;/a>; (RMSE) slightly decreases. &lt;/li>;&lt;li>;However, as we introduce diversity in the dataset (eg, collecting data from different agents), we observe 9× to 42× lower RMSE across different dataset sizes. &lt;/li>; &lt;/ol>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/s1826/ArchGym2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1143&quot; data-original-width=&quot;1826&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkxfrylQFqrxqu42CeUG38HlHgRSJnUm-tUGKnbFuIq6Uoc6QtmTMATkH6GdpOAYxZ6Ux6-fXp82jCJZQrqLc67DI-feQMrcgD14HH_cXjIszCkKN3_I9CSqi_bY5OG-Y7CNM6sQT0QtfAeuWZrXlpjPTg_JmNVAQpcMXqM4UoyCl9KBHqURb6sgSlR9iD/w640-h401/ArchGym2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diverse dataset collection across different agents using ArchGym interface.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/s1803/ArchGym1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;1803&quot; height=&quot;407&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1nusrQvP0fLpgt7R6B4ZcTtWeAhZIadd2Tg6AkuSLokzm3-XhIqrHhl_7bteAkKKcebVD2yiN7elFFEoBP6zFhxMwwb1bcoubcIY0DoyOBW8S-Iqzbgw2hcKVND_q5uuv7t7zuLfH4ZZf20QKiAvnJwyBO9lzkEie8AWA0RuXSzt3um7prjjIHm-YWt9d/w640-h407/ArchGym1.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The impact of a diverse dataset and dataset size on the normalized RMSE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;The need for a community-driven ecosystem for ML-assisted architecture research&lt;/h2>; &lt;p>;While, ArchGym is an initial effort towards creating an open-source ecosystem that (1) connects a broad range of search algorithms to computer architecture simulators in an unified and easy-to-extend manner, (2) facilitates research in ML-assisted computer architecture, and (3) forms the scaffold to develop reproducible baselines, there are a lot of open challenges that need community-wide support. Below we outline some of the open challenges in ML-assisted architecture design. Addressing these challenges requires a well coordinated effort and a community driven ecosystem.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;969&quot; data-original-width=&quot;1999&quot; height=&quot;310&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijxvhgUpcNJjDOdEsk97dGf3_fI0uF652AJwEeGIu_HA8wzQOn36FydrtQr6dNVUiuy7L-Oy-YPAztzOZ1nPabg1S9GKL-TR2mcbpt__GR69NnAjOWhI8olwAp2DHnUcI8qj-yKuRYlFwnmjcEwZWRkD_sFinQhMOvu81mx8mUFXMXh3ImZLbnVVQneOUf/w640-h310/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Key challenges in ML-assisted architecture design.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We call this ecosystem &lt;a href=&quot;https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/&quot;>;Architecture 2.0&lt;/a>;.&amp;nbsp;We outline the key challenges and a vision for building an inclusive ecosystem of interdisciplinary researchers to tackle the long-standing open problems in applying ML for computer architecture research.&amp;nbsp;If you are interested in helping shape this ecosystem, please fill out the &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSfIYeSBoEi-DIHizPx4-FTEcZUSY_uUcKe0rdHC0tkCWp3Gag/viewform&quot;>;interest survey&lt;/a>;.&lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>;&lt;a href=&quot;https://bit.ly/ArchGym&quot;>;ArchGym&lt;/a>; is an open source gymnasium for ML architecture DSE and enables an standardized interface that can be readily extended to suit different use cases. Additionally, ArchGym enables fair and reproducible comparison between different ML algorithms and helps to establish stronger baselines for computer architecture research problems. &lt;/p>; &lt;p>; We invite the computer architecture community as well as the ML community to actively participate in the development of ArchGym. We believe that the creation of a gymnasium-type environment for computer architecture research would be a significant step forward in the field and provide a platform for researchers to use ML to accelerate research and lead to new and innovative designs. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;This blogpost is based on joint work with several co-authors at Google and Harvard University.&amp;nbsp;&lt;/i>;&lt;i>;We would like to acknowledge and highlight Srivatsan Krishnan (Harvard) who contributed several ideas to this project in collaboration with Shvetank Prakash (Harvard), Jason Jabbour (Harvard), Ikechukwu Uchendu (Harvard), Susobhan Ghosh (Harvard), Behzad Boroujerdian (Harvard), Daniel Richins (Harvard), Devashree Tripathy (Harvard), and Thierry Thambe (Harvard).&amp;nbsp; In addition, we would also like to thank James Laudon, Douglas Eck, Cliff Young, and Aleksandra Faust for their support, feedback, and motivation for this work. We would also like to thank John Guilyard for the animated figure used in this post. Amir Yazdanbakhsh is now a Research Scientist at Google DeepMind and Vijay Janapa Reddi is an Associate Professor at Harvard.&lt;/i>;&lt;/p>;&lt;div>;&lt;br />;&lt;/div>;&lt;br />;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8984134419460793359/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/an-open-source-gymnasium-for-computer.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8984134419460793359&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/an-open-source-gymnasium-for-computer.html&quot; rel=&quot;alternate&quot; title=&quot;An open-source gymnasium for machine learning assisted computer architecture design&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMJx6osjDEhIpBGYohScAOpBU1CJmTsafUF9GgeM6BhBQ0KBjhSGirW0WY_8hu1boJvi-oqfbDlcHMO7RsrVOs1voUVsyE0f4uVSsBM2LgrSjGbFtuyWVXRbX7StUb4xbNgX7ZIfFDtfmjtJcEPvz6VGD_zGo1aEcQvbewZwSSwvMoHZP7ZW1Fob8tb86h/s72-c/ArchGym-animation2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7093565002706757508&lt;/id>;&lt;published>;2023-07-10T06:02:00.005-07:00&lt;/published>;&lt;updated>;2023-07-21T13:24:14.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ACL 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s1040/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://2023.aclweb.org/&quot;>;61st annual meeting&lt;/a>; of the &lt;a href=&quot;https://www.aclweb.org/&quot;>;Association for Computational Linguistics&lt;/a>; (ACL), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, is taking place in Vancouver, BC. As a leader in natural language processing and understanding, and a&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/sponsors/&quot;>;Diamond Level sponsor&lt;/a>;&amp;nbsp;of&amp;nbsp;&lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, Google will showcase the latest research in the field with over 50 publications, and active involvement in a variety of workshops and tutorials.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;If you&#39;re registered for ACL 2023, we hope that you&#39;ll visit the Google booth to learn more about the projects at Google that go into solving interesting problems for billions of people. You can also learn more about Google&#39;s participation below (Google affiliations in &lt;b>;bold&lt;/b>;).&lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Area chairs include: &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>; &lt;br />; Workshop chairs include: &lt;strong>;&lt;em>;Annie Louis&lt;/em>;&lt;/strong>; &lt;br />; Publication chairs include: &lt;strong>;&lt;em>;Lei Shu&lt;/em>;&lt;/strong>; &lt;br />; Program Committee includes: &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Spotlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09648.pdf&quot;>;NusaCrowd: Open Source Initiative for Indonesian NLP Resources&lt;/a>; &lt;br />; &lt;em>;Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Septiandri, James Jaya, Kaustubh Dhole, Arie Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Wibowo, Cuk Tho, Ichwanul Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, &lt;strong>;Sebastian Ruder&lt;/strong>;, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2205.12680.pdf&quot;>;Optimizing Test-Time Query Representations for Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, &lt;strong>;Jinhyuk Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10750.pdf&quot;>;PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition&lt;/a>; &lt;br />; &lt;em>;Sihao Chen*, &lt;strong>;Senaka Buthpitiya&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, Dan Roth, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02301.pdf&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>; &lt;br />; &lt;em>;Cheng-Yu Hsieh*, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chih-Kuan Yeh&lt;/strong>;, &lt;strong>;Hootan Nakhost&lt;/strong>;, &lt;strong>;Yasuhisa Fujii&lt;/strong>;, Alex Ratner, Ranjay Krishna, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05110.pdf&quot;>;Large Language Models with Controllable Working Memory&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Daliang Li&lt;/b>;, &lt;b>;Ankit Singh Rawat&lt;/b>;, &lt;b>;Manzil Zaheer&lt;/b>;, &lt;b>;Xin Wang&lt;/b>;, &lt;b>;Michal Lukasik&lt;/b>;, &lt;b>;Andreas Veit&lt;/b>;, &lt;b>;Felix Yu&lt;/b>;,&lt;b>; Sanjiv Kumar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10791.pdf&quot;>;OpineSum: Entailment-Based Self-Training for Abstractive Opinion Summarization&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Annie Louis&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08775.pdf&quot;>;RISE: Leveraging Retrieval Techniques for Summarization Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Uthus&lt;/b>;, &lt;b>;Jianmo Ni&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b5b9bb4ec1e1d7416f88f8b3a1649d1235d747b8.pdf&quot;>;Follow the Leader(board) with Confidence: Estimating p-Values from a Single Test Set with Item and Response Variance&lt;/a>; &lt;br />; &lt;i>; Shira Wein*, Christopher Homan, &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Chris Welty&lt;/strong>;&lt;/i>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.02516.pdf&quot;>;SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives&lt;/a>; &lt;br />; &lt;i>;&lt;strong>;Fedor Moiseev&lt;/strong>;, &lt;strong>;Gustavo Hernandez Abrego&lt;/strong>;, &lt;strong>;Peter Dornbach&lt;/strong>;, &lt;strong>;Imed Zitouni&lt;/strong>;, &lt;strong>;Enrique Alfonseca&lt;/strong>;, &lt;strong>;Zhe Dong&lt;/strong>;&lt;/i>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10266.pdf&quot;>;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability&lt;/a>; &lt;br />; &lt;em>;Eleftheria Briakou, &lt;strong>;Colin Cherry&lt;/strong>;, &lt;strong>;George Foster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09102.pdf&quot;>;Prompting PaLM for Translation: Assessing Strategies and Performance&lt;/a>; &lt;br />; &lt;em>;&lt;b>;David Vilar&lt;/b>;, &lt;b>;Markus Freitag&lt;/b>;, &lt;b>;Colin Cherry&lt;/b>;, &lt;b>;Jiaming Luo&lt;/b>;, &lt;b>;Viresh Ratnakar&lt;/b>;, &lt;b>;George Foster&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.17525.pdf&quot;>;Query Refinement Prompts for Closed-Book Long-Form QA&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kellie Webster&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10381.pdf&quot;>;To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering&lt;/a>; &lt;br />; &lt;em>;Dheeru Dua*, &lt;strong>;Emma Strubell&lt;/strong>;, Sameer Singh, &lt;strong>;Pat Verga&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00193.pdf&quot;>;FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;b>;Parker Riley&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Jan A. Botha&lt;/b>;, &lt;b>;Xavier Garcia&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Jason Riesa&lt;/b>;, &lt;b>;Orhan Firat&lt;/b>;,&lt;b>; Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.00397.pdf&quot;>;Conditional Generation with a Question-Answering Blueprint&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Reinald Kim Amplayo&lt;/b>;, &lt;b>;Kuzman Ganchev&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;, &lt;b>;Fantine Huot&lt;/b>;, &lt;b>;Anders Sandholm&lt;/b>;, &lt;b>;Dipanjan Das&lt;/b>;, &lt;b>;Mirella Lapata&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12142.pdf&quot;>;Coreference Resolution Through a Seq2Seq Transition-Based System&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Bernd Bohnet&lt;/b>;, &lt;b>;Chris Alberti&lt;/b>;, &lt;b>;Michael Collins&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00106.pdf&quot;>;Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni, &lt;strong>;Dan Garrette&lt;/strong>;, Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08054.pdf&quot;>;DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue&lt;/a>; &lt;br />; &lt;em>;William Held*, &lt;strong>;Christopher Hidey&lt;/strong>;, &lt;strong>;Fei Liu&lt;/strong>;, &lt;strong>;Eric Zhu&lt;/strong>;, &lt;strong>;Rahul Goel&lt;/strong>;, Diyi Yang, &lt;strong>;Rushin Shah&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.08726.pdf&quot;>;RARR: Researching and Revising What Language Models Say, Using Language Models&lt;/a>; &lt;br />; Luyu Gao*, &lt;strong>;Zhuyun Dai&lt;/strong>;, &lt;strong>;Panupong Pasupat&lt;/strong>;, Anthony Chen*, &lt;strong>;Arun Tejasvi Chaganty&lt;/strong>;, &lt;strong>;Yicheng Fan&lt;/strong>;, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Ni Lao&lt;/strong>;, &lt;strong>;Hongrae Lee&lt;/strong>;, &lt;strong>;Da-Cheng Juan&lt;/strong>;, &lt;strong>;Kelvin Guu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.16793.pdf&quot;>;Benchmarking Large Language Model Capabilities for Conditional Generation&lt;/a>; &lt;br />; &lt;em>;Joshua Maynez, Priyanka Agrawal, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.01786.pdf&quot;>;Crosslingual Generalization Through Multitask Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Niklas Muennighoff, Thomas Wang, Lintang Sutawika, &lt;strong>;Adam Roberts&lt;/strong>;, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.05655.pdf&quot;>;DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering&lt;/a>; &lt;br />; &lt;em>;Ella Neeman, &lt;strong>;Roee Aharoni&lt;/strong>;, Or Honovich, Leshem Choshen, &lt;strong>;Idan Szpektor&lt;/strong>;, Omri Abend&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10933.pdf&quot;>;Resolving Indirect Referring Expressions for Entity Selection&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mohammad Javad Hosseini&lt;/b>;, &lt;b>;Filip Radlinski&lt;/b>;, &lt;b>;Silvia Pareti&lt;/b>;, &lt;b>;Annie Louis&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11840.pdf&quot;>;SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models&lt;/a>; &lt;br />; &lt;em>;Akshita Jha*, &lt;strong>;Aida Mostafazadeh Davani&lt;/strong>;, Chandan K Reddy, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Sunipa Dev&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.10040.pdf&quot;>;The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks&lt;/a>; &lt;br />; &lt;em>;Nikil Selvam, &lt;strong>;Sunipa Dev&lt;/strong>;, Daniel Khashabi, Tushar Khot, Kai-Wei Chang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10562.pdf&quot;>;Character-Aware Models Improve Visual Text Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Rosanne Liu&lt;/b>;, &lt;b>;Dan Garrette&lt;/b>;, &lt;b>;Chitwan Saharia&lt;/b>;, &lt;b>;William Chan&lt;/b>;, &lt;b>;Adam Roberts&lt;/b>;, &lt;b>;Sharan Narang&lt;/b>;, &lt;b>;Irina Blok&lt;/b>;, &lt;b>;RJ Mical&lt;/b>;, &lt;b>;Mohammad Norouzi&lt;/b>;, &lt;b>;Noah Constant&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06995.pdf&quot;>;Cold-Start Data Selection for Better Few-Shot Language Model Fine-Tuning: A Prompt-Based Uncertainty Propagation Approach&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment &lt;br />; &lt;em>;&lt;b>;Roni Rabin&lt;/b>;, &lt;b>;Alexandre Djerbetian&lt;/b>;, &lt;b>;Roee Engelberg&lt;/b>;, &lt;b>;Lidan Hackmon&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;, &lt;b>;Amir Globerson&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02549.pdf&quot;>;FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Chen-Yu Lee&lt;/b>;, &lt;b>;Chun-Liang Li&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Timothy Dozat&lt;/b>;, &lt;b>;Vincent Perot&lt;/b>;, &lt;b>;Guolong Su&lt;/b>;, &lt;b>;Xiang Zhang&lt;/b>;,&lt;b>; Kihyuk Sohn&lt;/b>;, &lt;b>;Nikolay Glushinev&lt;/b>;, &lt;b>;Renshen Wang&lt;/b>;, &lt;b>;Joshua Ainslie&lt;/b>;, &lt;b>;Shangbang Long&lt;/b>;, &lt;b>;Siyang Qin&lt;/b>;,&lt;b>; Yasuhisa Fujii&lt;/b>;, &lt;b>;Nan Hua&lt;/b>;, &lt;b>;Tomas Pfister&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00922.pdf&quot;>;Dialect-Robust Evaluation of Generated Text&lt;/a>; &lt;br />; &lt;em>;Jiao Sun*, &lt;strong>;Thibault Sellam&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Tu Vu*, &lt;strong>;Timothy Dozat&lt;/strong>;, &lt;strong>;Dan Garrette&lt;/strong>;, &lt;strong>;Aditya Siddhant&lt;/strong>;, &lt;strong>;Jacob Eisenstein&lt;/strong>;, &lt;strong>;Sebastian Gehrmann&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.03950.pdf&quot;>;MISGENDERED: Limits of Large Language Models in Understanding Pronouns&lt;/a>; &lt;br />; &lt;em>;Tamanna Hossain, &lt;strong>;Sunipa Dev&lt;/strong>;, Sameer Singh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13894.pdf&quot;>;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mehran Kazemi&lt;/b>;, &lt;b>;Najoung Kim&lt;/b>;, &lt;b>;Deepti Bhatia&lt;/b>;, &lt;b>;Xin Xu&lt;/b>;, &lt;b>;Deepak Ramachandran&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.19585.pdf&quot;>;LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction&lt;/a>; &lt;br />; &lt;em>;Jeremiah Milbauer*, &lt;strong>;Annie Louis&lt;/strong>;, &lt;strong>;Mohammad Javad Hosseini&lt;/strong>;, &lt;strong>;Alex Fabrikant&lt;/strong>;, &lt;strong>;Donald Metzler&lt;/strong>;, &lt;strong>;Tal Schuster&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05392.pdf&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Andy Zeng&lt;/strong>;, Trevor Darrell, Dan Klein&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10001.pdf&quot;>;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters&lt;/a>; &lt;br />; &lt;em>;Boshi Wang, Sewon Min, Xiang Deng, &lt;strong>;Jiaming Shen&lt;/strong>;, &lt;strong>;You Wu&lt;/strong>;, Luke Zettlemoyer and Huan Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14106.pdf&quot;>;Better Zero-Shot Reasoning with Self-Adaptive Prompting&lt;/a>; &lt;br />; &lt;em>;Xingchen Wan*, &lt;strong>;Ruoxi Sun&lt;/strong>;, Hanjun Dai, &lt;strong>;Sercan Ö. Arik&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.00186.pdf&quot;>;Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Paul Roit&lt;/b>;, &lt;b>;Johan Ferret&lt;/b>;, &lt;b>;Lior Shani&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Geoffrey Cideron&lt;/b>;, &lt;b>;Robert Dadashi&lt;/b>;, &lt;b>;Matthieu Geist&lt;/b>;,&lt;b>; Sertan Girgin&lt;/b>;, &lt;b>;Léonard Hussenot&lt;/b>;, &lt;b>;Orgad Keller&lt;/b>;, &lt;b>;Nikola Momchev&lt;/b>;, &lt;b>;Sabela Ramos&lt;/b>;, &lt;b>;Piotr Stanczyk&lt;/b>;, &lt;b>;Nino Vieillard&lt;/b>;, &lt;b>;Olivier Bachem&lt;/b>;, &lt;b>;Gal Elidan&lt;/b>;, &lt;b>;Avinatan Hassidim&lt;/b>;, &lt;b>;Olivier Pietquin&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09248.pdf&quot;>;Natural Language to Code Generation in Interactive Data Science Notebooks&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Pengcheng Yin&lt;/b>;, &lt;b>;Wen-Ding Li&lt;/b>;, &lt;b>;Kefan Xiao&lt;/b>;, &lt;b>;Abhishek Rao&lt;/b>;, &lt;b>;Yeming Wen&lt;/b>;, &lt;b>;Kensen Shi&lt;/b>;, &lt;b>;Joshua Howland&lt;/b>;,&lt;b>; Paige Bailey&lt;/b>;, &lt;b>;Michele Catasta&lt;/b>;, &lt;b>;Henryk Michalewski&lt;/b>;, &lt;b>;Oleksandr Polozov&lt;/b>;, &lt;b>;Charles Sutton&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08410.pdf&quot;>;Teaching Small Language Models to Reason&lt;/a>; &lt;br />; &lt;em>;Lucie Charlotte Magister*, &lt;strong>;Jonathan Mallinson&lt;/strong>;, &lt;strong>;Jakub Adamek&lt;/strong>;, &lt;strong>;Eric Malmi&lt;/strong>;, &lt;strong>;Aliaksei Severyn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/assets/pdf/papers/NeuPSL.pdf&quot;>;Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic&lt;/a>; &lt;br />; &lt;em>;Connor Pryor*, &lt;strong>;Quan Yuan&lt;/strong>;, &lt;strong>;Jeremiah Liu&lt;/strong>;, &lt;strong>;Mehran Kazemi&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>;, &lt;strong>;Tania Bedrax-Weiss&lt;/strong>;, Lise Getoor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10397.pdf&quot;>;A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization&lt;/a>; &lt;br />; &lt;em>;Lining Zhang, Simon Mille, Yufang Hou, &lt;strong>;Daniel Deutsch&lt;/strong>;, &lt;strong>;Elizabeth Clark&lt;/strong>;, Yixin Liu, Saad Mahamood, &lt;strong>;Sebastian Gehrmann&lt;/strong>;, Miruna Clinciu, Khyathi Raghavi Chandu and João Sedoc&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Industry Track papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18465.pdf&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Zheng Xu&lt;/b>;, &lt;b>;Yanxiang Zhang&lt;/b>;, &lt;b>;Galen Andrew&lt;/b>;, &lt;b>;Christopher Choquette&lt;/b>;, &lt;b>;Peter Kairouz&lt;/b>;, &lt;b>;Brendan McMahan&lt;/b>;, &lt;b>;Jesse Rosenstock&lt;/b>;, &lt;b>;Yuanbo Zhang&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.18373.pdf&quot;>;KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models&lt;/a>; &lt;br />; &lt;em>;Zhiwei Jia*, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Arjun Akula&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, Hao Su, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;ACL Findings papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10622.pdf&quot;>;Multilingual Summarization with Factual Consistency Evaluation&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Shashi Narayan&lt;/b>;, &lt;b>;Joshua Maynez&lt;/b>;, &lt;b>;Jonathan Herzig&lt;/b>;, &lt;b>;Elizabeth Clark&lt;/b>;, &lt;b>;Mirella Lapata &lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.06767.pdf&quot;>;Parameter-Efficient Fine-Tuning for Robust Continual Multilingual Learning&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Kartikeya Badola&lt;/b>;, &lt;b>;Shachi Dave&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08153.pdf&quot;>;FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference&lt;/a>; &lt;br />; &lt;em>;Michiel de Jong*, &lt;strong>;Yury Zemlyanskiy&lt;/strong>;, &lt;strong>;Joshua Ainslie&lt;/strong>;, &lt;strong>;Nicholas FitzGerald&lt;/strong>;, &lt;strong>;Sumit Sanghai&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;William Cohen&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00609.pdf&quot;>;A Simple, Yet Effective Approach to Finding Biases in Code Generation&lt;/a>; &lt;br />; &lt;em>;Spyridon Mouselinos, Mateusz Malinowski, &lt;strong>;Henryk Michalewski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09261.pdf&quot;>;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Mirac Suzgun&lt;/b>;, &lt;b>;Nathan Scales&lt;/b>;, &lt;b>;Nathanael Scharli&lt;/b>;, &lt;b>;Sebastian Gehrmann&lt;/b>;, &lt;b>;Yi Tay&lt;/b>;, &lt;b>;Hyung Won Chung&lt;/b>;,&lt;b>; Aakanksha Chowdhery&lt;/b>;, &lt;b>;Quoc Le&lt;/b>;, &lt;b>;Ed Chi&lt;/b>;, &lt;b>;Denny Zhou&lt;/b>;, &lt;b>;Jason Wei&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.07730.pdf&quot;>;QueryForm: A Simple Zero-Shot Form Entity Query Framework&lt;/a>; &lt;br />; &lt;em>;Zifeng Wang*, &lt;strong>;Zizhao Zhang&lt;/strong>;, &lt;strong>;Jacob Devlin&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Guolong Su&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, Jennifer Dy, &lt;strong>;Vincent Perot&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10703.pdf&quot;>;ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval&lt;/a>; &lt;br />; &lt;em>;Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, &lt;strong>;Jiaming Shen&lt;/strong>;, Chao Zhang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09682.pdf&quot;>;Multilingual Sequence-to-Sequence Models for Hebrew NLP&lt;/a>; &lt;br />; &lt;em>;&lt;b>;Matan Eyal&lt;/b>;, &lt;b>;Hila Noga&lt;/b>;, &lt;b>;Roee Aharoni&lt;/b>;, &lt;b>;Idan Szpektor&lt;/b>;, &lt;b>;Reut Tsarfaty&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.04009.pdf&quot;>;Triggering Multi-Hop Reasoning for Question Answering in Language Models Using Soft Prompts and Random Walks&lt;/a>; &lt;br />; &lt;em>;Kanishka Misra*, &lt;strong>;Cicero Nogueira dos Santos&lt;/strong>;, Siamak Shakeri&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Complex Reasoning in Natural Language&lt;/a>;&lt;br />; &lt;em>;Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, &lt;strong>;Aman Madaan&lt;/strong>;, Tao Yu&lt;/em>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://2023.aclweb.org/program/tutorials/&quot;>;Generating Text from Language Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Afra Amini&lt;/b>;, &lt;b>;Ryan Cotterell&lt;/b>;, &lt;b>;John Hewitt&lt;/b>;, &lt;b>;Clara Meister&lt;/b>;, &lt;b>;Tiago Pimentel&lt;/b>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/sustainlp2023&quot;>;Simple and Efficient Natural Language Processing (SustaiNLP)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Tal Schuster&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.workshopononlineabuse.com/&quot;>;Workshop on Online Abuse and Harms (WOAH)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Aida Mostafazadeh Davani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://doc2dial.github.io/workshop2023/&quot;>;Document-Grounded Dialogue and Conversational Question Answering (DialDoc)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/5thnlp4convai/home?authuser=0&quot;>;NLP for Conversational AI&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cawl.wellformedness.com/&quot;>;Computation and Written Language (CAWL)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;b>;Kyle Gorman&lt;/b>;, &lt;b>;Brian Roark&lt;/b>;, &lt;b>;Richard Sproat&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sigmorphon.github.io/workshops/2023/&quot;>;Computational Morphology and Phonology (SIGMORPHON)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Kyle Gorman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/umass.edu/wnu2023&quot;>;Workshop on Narrative Understanding (WNU)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7093565002706757508/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-acl-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7093565002706757508&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/google-at-acl-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ACL 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw7WA5JOMQQ05WvmPHeEPic7mT0BGyihQVlVoNvFEfIthv_RelDHg5WcFhB6yAyNbnygg74aWk22g1q1GSP5DhVYNzpwsO-WVerYItc62cMhuxVOP6HHOxEs1Qqd8KLq3Lz0k7zjsb-dsNA9DAMPgFbp2aarFS-JA4_h3dl_TOJjuLtHvUicZsPFWPLjVY/s72-c/Google%20ACL%202023%20Toronto%20-%20xsmall.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6926843365781180400&lt;/id>;&lt;published>;2023-07-07T11:01:00.001-07:00&lt;/published>;&lt;updated>;2023-07-07T11:09:20.724-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modular visual question answering via code generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sanjay Subramanian, PhD student, UC Berkeley, and Arsha Nagrani, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://visualqa.org/&quot;>;Visual question answering&lt;/a>; (VQA) is a machine learning task that requires a model to answer a question about an image or &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;a set of images&lt;/a>;. Conventional VQA approaches need a large amount of labeled training data consisting of thousands of human-annotated question-answer pairs associated with images. In recent years, advances in large-scale pre-training have led to the development of VQA methods that perform well with &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;fewer than fifty training examples&lt;/a>; (few-shot) and &lt;a href=&quot;https://ai.googleblog.com/2022/07/rewriting-image-captions-for-visual.html&quot;>;without any human-annotated VQA training data&lt;/a>; (zero-shot). However, there is still a significant performance gap between these methods and state-of-the-art fully supervised VQA methods, such as &lt;a href=&quot;https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot;>;MaMMUT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;. In particular, few-shot methods struggle with spatial reasoning, counting, and multi-hop reasoning. Furthermore, few-shot methods have generally been limited to answering questions about single images. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve accuracy on VQA examples that involve complex reasoning, in “&lt;a href=&quot;https://arxiv.org/abs/2306.05392&quot;>;Modular Visual Question Answering via Code Generation&lt;/a>;,” to appear at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL 2023&lt;/a>;, we introduce CodeVQA, a framework that answers visual questions using program synthesis. Specifically, when given a question about an image or set of images, CodeVQA generates a Python program (code) with simple visual functions that allow it to process images, and executes this program to determine the answer. We demonstrate that in the few-shot setting, CodeVQA outperforms prior work by roughly 3% on the &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; dataset and 2% on the &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;CodeVQA&lt;/h2>; &lt;p>; The CodeVQA approach uses a code-writing large language model (LLM), such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PALM&lt;/a>;, to generate &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;Python&lt;/a>; programs (code). We guide the LLM to correctly use visual functions by crafting a prompt consisting of a description of these functions and fewer than fifteen “in-context” examples of visual questions paired with the associated Python code for them. To select these examples, we compute embeddings for the input question and of all of the questions for which we have annotated programs (a randomly chosen set of fifty). Then, we select questions that have the highest similarity to the input and use them as in-context examples. Given the prompt and question that we want to answer, the LLM generates a Python program representing that question. &lt;/p>; &lt;p>; We instantiate the CodeVQA framework using three visual functions: (1) &lt;code>;query&lt;/code>;, (2) &lt;code>;get_pos&lt;/code>;, and (3) &lt;code>;find_matching_image&lt;/code>;. &lt;/p>; &lt;ul>; &lt;li>;&lt;code>;Query&lt;/code>;, which answers a question about a single image, is implemented using the few-shot &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;>;Plug-and-Play VQA&lt;/a>; (PnP-VQA) method. PnP-VQA generates captions using &lt;a href=&quot;https://arxiv.org/abs/2201.12086&quot;>;BLIP&lt;/a>; — an image-captioning &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformer&lt;/a>; pre-trained on millions of image-caption pairs — and feeds these into a LLM that outputs the answers to the question. &lt;/li>;&lt;li>;&lt;code>;Get_pos&lt;/code>;, which is an object localizer that takes a description of an object as input and returns its position in the image, is implemented using &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GradCAM&lt;/a>;. Specifically, the description and the image are passed through the BLIP joint text-image encoder, which predicts an image-text matching score. GradCAM takes the gradient of this score with respect to the image features to find the region most relevant to the text. &lt;/li>;&lt;li>;&lt;code>;Find_matching_image&lt;/code>;, which is used in multi-image questions to find the image that best matches a given input phrase, is implemented by using BLIP text and image encoders to compute a text embedding for the phrase and an image embedding for each image. Then the dot products of the text embedding with each image embedding represent the relevance of each image to the phrase, and we pick the image that maximizes this relevance. &lt;/li>; &lt;/ul>; &lt;p>; The three functions can be implemented using models that require very little annotation (eg, text and image-text pairs collected from the web and a small number of VQA examples). Furthermore, the CodeVQA framework can be easily generalized beyond these functions to others that a user might implement (eg, object detection, image segmentation, or knowledge base retrieval). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s1024/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;501&quot; data-original-width=&quot;1024&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgctWAA2wguSRs-pYSpTcGYxbhewA6tuas1LgLJL6RhPchWefwaY0pEwotIJgfBaoAZYldtVqdYxmlNX6SQKFzWo_GsRRNe20eIImR8jfHw1cHZ_PW6EwbXFRre8B-qKeLyfqDOPg_CZz1aJow1RmNGeOLTqUX4SycBs-4ldMXqnnzWhlyou-T0xJtzyyp/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the CodeVQA method. First, a large language model generates a Python program (code), which invokes visual functions that represent the question. In this example, a simple VQA method (&lt;code>;query&lt;/code>;) is used to answer one part of the question, and an object localizer (&lt;code>;get_pos&lt;/code>;) is used to find the positions of the objects mentioned. Then the program produces an answer to the original question by combining the outputs of these functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; The CodeVQA framework correctly generates and executes Python programs not only for single-image questions, but also for multi-image questions. For example, if given two images, each showing two pandas, a question one might ask is, “Is it true that there are four pandas?” In this case, the LLM converts the counting question about the pair of images into a program in which an object count is obtained for each image (using the &lt;em>;query&lt;/em>; function). Then the counts for both images are added to compute a total count, which is then compared to the number in the original question to yield a yes or no answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s1080/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1080&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7e3U05l5rsLZkwVkSTvBCWzWPCgWgHNiv580c3UpuM2ehBYNCbIfIkFuOzM4J7a3N0yUWsHa7giVc5IXcRt4Frp3Dr8YSEnVhphr6DGr4V9K4QXfHSm4wLFpzUzQ_DuZg7KUycVYH2ltV-5GPOjpYxkqf1k6AjYQtuTnOKt2drgxTLQEYMDl3mQRqYUNJ/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate CodeVQA on three visual reasoning datasets: &lt;a href=&quot;https://cs.stanford.edu/people/dorarad/gqa/about.html&quot;>;GQA&lt;/a>; (single-image), &lt;a href=&quot;https://covr-dataset.github.io/&quot;>;COVR&lt;/a>; (multi-image), and &lt;a href=&quot;https://lil.nlp.cornell.edu/nlvr/&quot;>;NLVR2&lt;/a>; (multi-image). For GQA, we provide 12 in-context examples to each method, and for COVR and NLVR2, we provide six in-context examples to each method. The table below shows that CodeVQA improves consistently over the baseline few-shot VQA method on all three datasets. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;GQA&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;COVR&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;NLVR2&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.56 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.06 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;63.37 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;49.03 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;54.11 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64.04 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the GQA, COVR, and NLVR2 datasets, showing that CodeVQA consistently improves over few-shot PnP-VQA. The metric is exact-match accuracy, ie, the percentage of examples in which the predicted answer exactly matches the ground-truth answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that in GQA, CodeVQA&#39;s accuracy is roughly 30% higher than the baseline on spatial reasoning questions, 4% higher on “and” questions, and 3% higher on “or” questions. The third category includes multi-hop questions such as “Are there salt shakers or skateboards in the picture?”, for which the generated program is shown below. &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;img = open_image(&quot;Image13.jpg&quot;) salt_shakers_exist = query(img, &quot;Are there any salt shakers?&quot;) skateboards_exist = query(img, &quot;Are there any skateboards?&quot;) if salt_shakers_exist == &quot;yes&quot; or skateboards_exist == &quot;yes&quot;: answer = &quot;yes&quot; else: answer = &quot;no&quot; &lt;/font>;&lt;/pre>; &lt;br />; &lt;p>; In COVR, we find that CodeVQA&#39;s gain over the baseline is higher when the number of input images is larger, as shown in the table below. This trend indicates that breaking the problem down into single-image questions is beneficial. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Number of images&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;3&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5&lt;br />; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;Few-shot PnP-VQA&lt;/em>;&amp;nbsp; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;91.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;51.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;47.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;46.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;CodeVQA&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;75.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;48.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.2 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;53.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present CodeVQA, a framework for few-shot visual question answering that relies on code generation to perform multi-step visual reasoning. Exciting directions for future work include expanding the set of modules used and creating a similar framework for visual tasks beyond VQA. We note that care should be taken when considering whether to deploy a system such as CodeVQA, since vision-language models like the ones used in our visual functions &lt;a href=&quot;https://arxiv.org/abs/2108.02818&quot;>;have been shown to exhibit social biases&lt;/a>;. At the same time, compared to monolithic models, CodeVQA offers additional interpretability (through the Python program) and controllability (by modifying the prompts or visual functions), which are useful in production systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was a collaboration between &lt;a href=&quot;https://bair.berkeley.edu/baircommons.html&quot;>;UC Berkeley&#39;s Artificial Intelligence Research lab&lt;/a>; (BAIR) and Google Research, and was conducted by Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6926843365781180400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/modular-visual-question-answering-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6926843365781180400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/07/modular-visual-question-answering-via.html&quot; rel=&quot;alternate&quot; title=&quot;Modular visual question answering via code generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjitkbpvH8cFcz-jbaK4Z8RzeDVi2aryR7NDkV55pqlh73A6iAeFEhw7HREIWOD0z9YY5Id3lhDLAIRs8fIJM0MxSYMljx8d9glfnv8p1WnXJNrABjVYgqA9xmCaNWTTyYw4qgSB26WreN62wWr382-4cSEXHgXe4nnDokdtNm9flD4zhxw9yynus09ZFeD/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;