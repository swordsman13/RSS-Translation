<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-10-12T23:04:34.939-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Publications"></category><category term="Research"></category><category term="Machine Perception"></category><category term="TensorFlow"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Computational Photography"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="MOOC"></category><category term="AI for Social Good"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="optimization"></category><category term="AutoML"></category><category term="Hardware"></category><category term="ACL"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="Android"></category><category term="ICML"></category><category term="TPU"></category><category term="Awards"></category><category term="ML"></category><category term="Structured Data"></category><category term="EMNLP"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Responsible AI"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Collaboration"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1289&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-56830389427789775&lt;/id>;&lt;发布>;2023-10-12T13:56:00.002-07:00&lt;/发布>;&lt;更新>;2023-10- 12T14：54：51.773-07：00 &lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“化学”>;&lt;/类别>;&lt;类别方案=“http： //www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;量子计算&quot; >;&lt;/category>;&lt;title type=&quot;text&quot;>;在未来纠错量子计算机上开发物理模拟的工业用例&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;已发布作者：Nicholas Rubin，高级研究科学家，Ryan Babbush，量子 AI 团队量子算法负责人 &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigr1Q582VTmDaeIOT7X-hnXjEJ8QW237xvxICJqe-ZKg2zBAQn9gP foAbLSJXmG8IFQ5B0ysoh7O60 -U4mMKA4mJxB92Tm5MnY50n8B7dWpHwap3lk9_at6c4oEZ0lqjpeS-sqRZyKdyu1UjzJkbb2zRJp9nsZvkikxlK0eTZBjB5hJqvOtbaFPyWe7OfF/s1000/StoppingPower.jpg&quot; style=&quot;显示: 无;&quot; />; &lt;p>; 如果您关注量子计算领域，您就会听说过这样的说法：未来量子计算机将比传统计算机更高效地解决某些问题。它们有潜力改变许多行业，从制药到能源。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在很大程度上，这些主张都基于关于当问题规模接近无穷大时算法渐进缩放的争论，但这告诉我们很少关于量子计算机解决有限大小问题的实际性能。我们想要更具体：量子计算机到底比经典计算机更适合解决哪些问题，以及我们到底可以运行哪些量子算法来解决这些问题？一旦我们设计了一个算法，我们就可以超越基于渐进缩放的分析——我们可以确定在量子计算机上编译和运行该算法所需的实际资源，以及它与经典计算的比较。 &lt;/p>; &lt;p>; 在过去几年中，&lt;a href=&quot;https://quantumai.google/&quot;>;Google Quantum AI&lt;/a>; 与行业和学术合作伙伴合作，评估量子模拟的前景，以实现革新特定技术并对资源需求进行具体分析。 2022 年，我们开发了量子算法来分析一种名为&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093435/&quot;>;细胞色素 P450&lt;/a>; 的重要酶家族的化学成分。然后，在&lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040303&quot;>;我们今年秋天发布的论文&lt;/a>;中，我们演示了如何使用量子计算机来研究可持续发展用于锂离子电池的钴替代品。最近，正如我们在题为“&lt;a href=&quot;https://arxiv.org/abs/2308.12352v1&quot;>;惯性聚变目标设计的阻止能力的量子计算&lt;/a>;”的预印本中所报告的那样，我们已经发现了惯性约束聚变实验中材料特性建模的新应用，例如&lt;a href=&quot;https://lasers.llnl.gov/&quot;>;国家点火设施&lt;/a>; (NIF) 的实验&lt; a href=&quot;https://www.llnl.gov/&quot;>;劳伦斯利弗莫尔国家实验室&lt;/a>;，最近&lt;a href=&quot;https://www.nytimes.com/2022/12/13/science/ Nuclear-fusion-energy-breakthrough.html&quot;>;因核聚变的突破而成为头条新闻&lt;/a>;。 &lt;/p>; &lt;p>; 下面，我们描述了这三种与工业相关的量子计算机模拟应用。运行算法时需要一台纠错量子计算机，即 &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html?m=1&quot; >;还需要几年的时间&lt;/a>;，现在就进行这项工作将确保我们在构建这样的量子计算机时准备好高效的量子算法。我们的工作已经显着降低了编译和运行算法的成本，因为我们&lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.2.030305&quot;>;有&lt;/a>;&lt; a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.2.040332&quot;>;过去报道过&lt;/a>;。我们的工作对于展示量子计算的潜力至关重要，但它也为我们的硬件团队提供了未来运行有用的量子算法所需的量子位数量和时间的目标规范。 &lt;/p>; &lt;br />; &lt;h2>;应用1：CYP450机制&lt;/h2>; &lt;p>;制药行业经常被吹捧为使用量子计算机进行发现的成熟领域。但这种潜在应用的具体例子却很少。与制药公司 &lt;a href=&quot;https://www.boehringer-ingelheim.com/&quot;>;Boehringer Ingelheim&lt;/a>; 的合作伙伴合作，我们在初创公司的合作伙伴 &lt;a href=&quot;https://qsimulate.com /&quot;>;QSimulate&lt;/a>; 和 &lt;a href=&quot;https://www.columbia.edu/&quot;>;哥伦比亚大学&lt;/a>; 的学术同事，我们在 2022 年探索了一个示例&lt;em>;&lt;a href =&quot;https://www.pnas.org/&quot;>;PNAS&lt;/a>;&lt;/em>; 文章，“&lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.2203533119&quot;>;在当今的经典计算机和未来的量子计算机上可靠地评估细胞色素 P450 的电子结构&lt;/a>;”。 &lt;/p>; &lt;p>; 细胞色素 P450 是人体中天然存在的一种酶家族，可帮助我们代谢药物。它的工作非常出色：超过 70% 的药物代谢是由 P450 家族的酶进行的。这些酶通过氧化药物发挥作用，这一过程取决于电子之间复杂的相关性。相互作用的细节太复杂，科学家无法先验地知道酶对特定药物的效果如何。 &lt;/p>; &lt;p>; 在&lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.2203533119&quot;>;论文&lt;/a>;中，我们展示了量子计算机如何解决这个问题。 CYP450 代谢过程是一个复杂的反应链，整个过程中酶的电子结构发生许多中间变化。我们首先使用最先进的经典方法来确定在经典计算机上模拟此问题所需的资源。然后我们想象在 &lt;a href=&quot;https://blog.research.google/2023/02 /suppressing-quantum-errors-by-scaling.html&quot;>;表面代码纠错量子计算机&lt;/a>;。 &lt;/p>; &lt;p>; 借助量子计算机，我们可以以更高的精度和更少的资源来追踪不断变化的电子结构链。事实上，我们发现，正确解析该系统中的化学反应需要量子计算机提供更高的精度，因此量子计算机不仅会更好，而且是必要的。随着系统规模变大，即模拟中包含的量子能级越多，量子计算机就越能战胜经典计算机。最终，我们证明需要几百万个物理量子比特才能解决这个问题的&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_supremacy&quot;>;量子优势&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimUWf0soJPXwDCKwwl3b46rIeysW0V9fbV_5m-oKpfZpjscw8Xo4LyI0smpSLDFQ8LOIcEFOXotDKsXvOsRkXP9B juUhvY_ic36EG2izNuRPujdr_50wcv-GoAYR2JJh4AcmhJj6f9OfTFY_q0UyuK9hJP79QKkskP9iewyBx0FpK0-4FDjakrHLZZZZrt/s1999/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;869&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimUWf0soJPXwDCKwwl3b46rIeysW0V9fbV_5m-oKpfZpjscw8Xo4LyI0smpSLDFQ8LOIcEFOXotDKsXvOsRkXP9BjuUhvY_ic36EG2izNuRPujdr_50 wcv-GoAYR2JJh4AcmhJj6f9OfTFY_q0UyuK9hJP79QKkskP9iewyBx0FpK0-4FDjakrHLZZZZrt/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;&lt;b>;左：&lt;/b>; CYP 酶的电子轨道（红色和蓝色）示例。 CYP 系统建模需要超过 60 个这样的轨道。 &lt;b>;右：&lt;/b>;各种经典技术（蓝色）的实际运行时间（CPU）与量子算法（绿色）的假设运行时间（QPU）的比较。量子算法的较低斜率表明了优于经典方法的渐近标度。在大约 20-30 个轨道上，我们看到了量子算法比经典方法更有效的状态的交叉。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;应用 2：锂离子电池&lt;/h2>; &lt;p>;锂离子电池依赖于两种含锂材料之间的&lt;a href=&quot;https://en.wikipedia.org/wiki/Electrochemical_potial&quot;>;电化学势&lt;/a>;差异。目前用于锂离子电池阴极的一种材料是 LiCoO&lt;sub>;2&lt;/sub>;。不幸的是，从制造角度来看它有缺点。钴开采成本高昂，&lt;a href=&quot;https://earth.org/cobalt-mining-in-congo&quot;>;对环境具有破坏性&lt;/a>;，并且经常利用&lt;a href=&quot;https://www.cobalt-mining-in-congo&quot;>;钴矿。 npr.org/sections/goatsandsoda/2023/02/01/1152893248/red-cobalt-congo-drc-mining-siddharth-kara&quot;>;不安全或虐待性的劳动行为&lt;/a>;。因此，该领域的许多人对锂离子阴极的钴替代品感兴趣。 &lt;/p>; &lt;p>; 20世纪90年代，研究人员发现镍可以代替钴形成LiNiO&lt;sub>;2&lt;/sub>;（称为“氧化锂镍”或“LNO”）作为阴极。虽然人们发现纯 LNO 在生产中不稳定，但当今汽车行业使用的许多阴极材料都使用高含量的镍，因此与 LNO 类似。然而，尽管 LNO 已在工业中得到应用，但人们并没有了解 LNO 的所有化学性质，甚至其基态的性质仍然是一个争论的话题。 &lt;/p>; &lt;p>; 在我们最近的论文中，“&lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040303&quot;>;使用布洛赫轨道对材料进行容错量子模拟&lt;/ a>;，”我们与化学公司合作，&lt;a href=&quot;https://www.basf.com/us/en/who-we-are/change-for-climate.html&quot;>;巴斯夫&lt;/a>; 、分子建模初创公司 QSimulate 和澳大利亚麦考瑞大学的合作者开发执行技术对具有周期性、规则间隔原子结构的系统（例如 LNO）进行量子模拟。然后，我们应用这些技术来设计算法来研究 LNO 的几种不同候选结构的相对能量。使用经典计算机，量子波函数的高精度模拟被认为执行成本太高。在我们的工作中，我们发现量子计算机需要数千万个物理量子位来计算四种候选基态 LNO 结构中每一种的能量。这是第一台纠错量子计算机无法实现的，但我们预计这个数字会随着未来算法的改进而下降。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5aLU-bNmekPT3q2j2ePYkBdvfoZqs2bBp12TC1At6QK0YFD10laEJc2gJ7XA9HQwTKnfSP35o4zLDiSbnRAapq NWNRMOSWOAIQjoI2YVROMLM8rYpa6UGH1vNICKdvFGh6anjk5F10d1gc4_7_guWBwYbzs78hdfudj77ytUgqP_7Ep9KPzRwwRzCIGeb/s1364/image3.jpg&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1364&quot; height=&quot;358&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5aLU-bNmekPT3q2j2ePYkBdvfoZqs2bBp12TC1At6QK0YFD10laEJc2gJ7XA9HQwTKnfSP35o4zLDiSbnRAapqNWNRMOSWOAIQjoI2YVROMLM8rYpa6UG H1vNICKdvFGh6anjk5F10d1gc4_7_guWBwYbzs78hdfudj77ytUgqP_7Ep9KPzRwwRzCIGeb/w640-h358/image3.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LNO 的四个候选结构。在本文中，我们考虑了比较这些结构的能量以找到 LNO 基态所需的资源。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;应用 3：聚变反应堆动力学&lt;/h2>; &lt;p>;在我们的第三个也是最近的例子中，我们与&lt;a href=&quot;https://www.sandia.gov/&quot;>;桑迪亚国家实验室&lt;/a>;的理论家以及麦考瑞大学的合作者合作让我们假设的量子计算机承担模拟典型极端条件下带电粒子动力学的任务，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Inertial_confinement_fusion&quot;>;惯性约束聚变&lt;/a>; (ICF ）实验，例如国家点火装置的实验。在这些实验中，高强度激光被聚焦到金属腔 (&lt;a href=&quot;https://en.wikipedia.org/wiki/Hohlraum&quot;>;hohlraum&lt;/a>;) 中，该腔容纳由烧蚀器组成的目标胶囊周围的&lt;a href=&quot;https://en.wikipedia.org/wiki/Deuterium%E2%80%93tritium_fusion&quot;>;氘-氚燃料&lt;/a>;。当激光加热黑腔内部时，其壁会辐射出 X 射线，从而压缩胶囊，将内部的氘和氚加热到数千万开尔文。这使得燃料中的核子能够克服其&lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb_barrier&quot;>;相互静电斥力&lt;/a>;并开始融合成氦核，也称为&lt;a href= “https://en.wikipedia.org/wiki/Alpha_article&quot;>;阿尔法粒子&lt;/a>;。 &lt;/p>; &lt;p>; 这些实验的模拟计算要求很高，并且依赖于本身不确定的材料特性模型。即使使用类似于量子化学的方法来测试这些模型，计算成本也极其昂贵。在某些情况下，这样的测试计算消耗了>;1亿个CPU小时。模拟中最昂贵和最不准确的方面之一是持续聚变阶段之前的等离子体动力学（>;数十百万开尔文），此时胶囊和燃料的部分温度为更温和的 100k 开尔文。在这种“热致密物质”状态下，当持续聚变发生时，量子相关性在系统行为中比在“热致密物质”状态中发挥更大的作用。 &lt;/p>; &lt;p>; 在我们的新预印本“&lt;a href=&quot;https://arxiv.org/abs/2308.12352v1&quot;>;惯性聚变目标设计的阻止本领的量子计算&lt;/a>;”中，我们提出了一种量子算法，用于计算核聚变实验中热致密物质的所谓“阻止本领”。阻止本领是高能 α 粒子由于与周围等离子体的&lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb%27s_law&quot;>;库仑相互作用&lt;/a>;而减速的速率。了解系统的停止能力对于优化反应堆的效率至关重要。当阿尔法粒子被周围的等离子体减慢时，它会将能量转移到等离子体，从而加热等离子体。这种自加热过程是聚变反应维持燃烧等离子体的机制。该过程的详细建模将有助于为未来的反应堆设计提供信息。 &lt;/p>; &lt;p>; 我们估计计算制动力所需的量子算法需要介于 P450 应用程序和电池应用程序之间的资源。但由于这是第一个关于第一原理动力学（或有限温度下的任何应用）的案例研究，这样的估计只是一个起点，我们再次期望找到算法改进来降低未来的成本。尽管存在这种不确定性，它仍然肯定比经典替代方案更好，对于经典替代方案来说，这些模拟的唯一易于处理的方法是平均场方法&lt;/a>;。虽然这些方法在描述这些系统的物理特性时会产生未知的系统误差，但它们是目前执行此类模拟的唯一有意义的方法。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBtVt_flUA0xvpBirTdVvlwHewAPQs6z_7fZM4dj6AwdGlrUQg0w4izYIR-vXl_qMr_EUn91KGwXFdgJg bgPT66cFU9nepPbP4SSIhHQz8RJptKV0I0r98sjCCqiKZDUPgoVyEuO3CM2cRMcnA32dX2dXbbRAsHt5FKrVOgZpWOo5sHgNmiR0YT3DOfByG/s1586/image1.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;712&quot; data-original-width=&quot;1586&quot; height=&quot;287&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBtVt_flUA0xvpBirTdVvlwHewAPQs6z_7fZM4dj6AwdGlrUQg0w4izYIR-vXl_qMr_EUn91KGwXFdgJgbgPT66cFU9nepPbP4SSIhHQz 8RJptKV0I0r98sjCCqiKZDUPgoVyEuO3CM2cRMcnA32dX2dXbbRAsHt5FKrVOgZpWOo5sHgNmiR0YT3DOfByG/w640-h287/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;左：&lt;/b>; 射弹（红色）以初始速度 vproj 穿过介质（蓝色）。 &lt;b>;右：&lt;/b>;为了计算制动力，我们监测弹丸和介质之间的能量传递（蓝色实线）并确定其平均斜率（红色虚线）。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;h2>;讨论和结论&lt;/h2>; &lt;p>; 上述示例只是未来纠错量子计算机在模拟物理系统中的大量且不断增长的具体应用中的三个。这一系列研究有助于我们了解最能从量子计算的力量中受益的问题类别。特别是，最后一个示例与其他两个示例的不同之处在于它模拟动态系统。其他问题侧重于寻找量子系统的最低能量、静态基态，与此相反，量子动力学关注的是量子系统如何随时间变化。由于量子计算机本质上是动态的——量子位状态随着每次操作的执行而演变和变化——它们特别适合解决这类问题。我们最近与哥伦比亚大学、哈佛大学、桑迪亚国家实验室和澳大利亚麦考瑞大学的合作者一起在《自然通讯》上发表了一篇论文 &lt;a href=&quot;https://www.nature.com/articles/s41467-023-39024-0&quot;>;证明用于模拟电子动力学的量子算法甚至比近似的“平均场”经典计算更有效，同时提供更高的精度。 &lt;/p>; &lt;p>; 今天开发和改进算法使我们能够在最终实现纠错量子计算机时充分利用它们。就像经典计算案例一样，我们期望量子计算堆栈的各个级别都得到改进，以进一步降低资源需求。但这第一步有助于将夸张与适合量子计算加速的真实应用区分开来。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>;&lt;em>;我们要感谢我们的量子科学传播者 Katie McCormick 帮助撰写这篇博文。&lt;/em>;&lt;/p >;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/56830389427789775/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>; &lt;link href=&quot;http://blog.research.google/2023/10/developing-industrial-use-cases-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text /html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/56830389427789775&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href= &quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/56830389427789775&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2023/10/developing-industrial-use-cases-for.html&quot; rel=&quot;alternate&quot; title=&quot;开发未来纠错量子计算机物理模拟的工业用例&quot; type=&quot;text/html&quot;/>; &lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height= “16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>; &lt; /gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigr1Q582VTmDaeIOT7X-hnXjEJ8QW237xvxICJqe-ZKg2zBAQn9gPfoAbLsJXmG8IFQ5B0ysoh7O60 -U4mMKA4mJxB92Tm5MnY50n8B7dWpHwap3lk9_at6c4oEZ0lqjpeS-sqRZyKdyu1UjzJkbb2zRJp9nsZvkikxlK0eTZBjB5hJqvOtbaFPyWe7OfF/s72-c /StoppingPower.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry >;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-1075240247864880418&lt;/id>;&lt;发布>;2023-10-09T12:17:00.000-07:00&lt;/发布>;&lt;更新>;2023 -10-09T12:17:01.762-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“计算机视觉”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#” term=&quot;datasets&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SANPO ：场景理解、辅助功能、导航、寻路和避障数据集&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由高级软件工程师 Sagar M. Waghmare 和 Kimberly 发布Wilber，Google 研究部感知团队软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh10zxbymBGZgjXFHDrw-CdxlVL7nRi6yjaI3w3X_x5pjxn8UWA7NnymAqMXomfjSBXWVDQ4czo8nqINhxzIP LVx2Uv1l8RDQAbikuWWjIt9IwxSIuSlEtJ5AJwOJkdaKPwzUdu9BwkJJP1gDj2UJQpkJ15ELGjSKHMl9ce0_470SwRz5snz32lV-vSlMd2/s600/SANPOHero.gif&quot;样式=&quot;显示：无；” />; &lt;p>; 当大多数人在日常生活中导航时，他们使用眼睛水平的视角来处理来自环境的视觉输入。与机器人和自动驾驶汽车不同，人们没有任何“体外”传感器来帮助引导他们。相反，一个人的感官输入完全是“以自我为中心”，或者“来自自我”。这也适用于从类似人类的角度理解我们周围世界的新技术，例如，穿过未知建筑物的机器人、突出显示物体的 AR 眼镜或 &lt;a href=&quot;https://blog.google/outreach-initiatives /accessibility/project-guideline/&quot;>;帮助人们独立跑步的辅助技术&lt;/a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在计算机视觉中，场景理解是一个子领域，通过关注空间、功能和布局来研究可见对象如何与场景的 3D 结构和布局相关。对象与其环境之间的语义关系。例如，自动驾驶驾驶员必须了解道路、人行道和周围建筑物的 3D 结构，同时识别和识别街道标志和停车灯，通过安装在汽车顶部的特殊激光扫描仪而不是来自汽车顶部的特殊激光扫描仪获取 3D 数据，这项任务变得更加容易。驾驶员视角的 2D 图像。在公园里导航的机器人必须了解路径在哪里以及哪些障碍物可能会干扰，这可以通过周围环境的地图和 GPS 定位数据来简化。最后，帮助用户认路的 AR 眼镜需要了解用户在哪里以及他们在看什么。 &lt;/p>; &lt;p>; 计算机视觉社区通常研究自动驾驶等环境中的场景理解任务，其中除了以自我为中心的图像之外，还可以使用许多其他传感器（GPS、车轮定位、地图等）。然而，这个领域的大多数数据集并不只关注以自我为中心的数据，因此它们不太适用于以人为中心的导航任务。虽然有大量以自动驾驶为中心的场景理解数据集，但它们对以自我为中心的人类场景理解的泛化有限。全面的人类自我中心数据集将有助于构建相关应用程序的系统，并为场景理解社区提供具有挑战性的基准。&lt;br />; &lt;/p>; &lt;p>; 为此，我们提出了 &lt;a href=&quot;https:/ /arxiv.org/abs/2309.12172&quot;>;场景理解、辅助功能、导航、寻路、避障数据集&lt;/a>;，或&lt;a href=&quot;https://github.com/google-research-datasets/sanpo_dataset&quot;>; SANPO&lt;/a>;（日语中的“轻快漫步”一词），一个用于户外人类以自我为中心的场景理解的多属性视频数据集。该数据集由真实世界数据和合成数据组成，我们分别称为 SANPO-Real 和 SANPO-Synthetic。它支持各种各样的密集预测任务，对当前模型来说具有挑战性，并且包括带有深度图和视频全景蒙版的真实和合成数据，其中每个像素都被分配了一个语义类标签（对于某些语义类，每个像素也被分配了一个语义类标签）。分配了唯一标识场景中该对象的语义实例 ID）。真实数据集涵盖不同的环境，并具有来自两个立体摄像头的视频，支持多视图方法，包括以每秒 15 帧 (FPS) 的速度捕获的 11.4 小时，并带有密集注释。研究人员可以&lt;a href=&quot;https://github.com/google-research-datasets/sanpo_dataset&quot;>;此处&lt;/a>;下载并使用 SANPO。 &lt;/p>; &lt;h2>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5921eqIbEdm5k9lS-EqcW5VwLeSRdMhL1pU2jnQTk5NzdoO2FKPcQHbTd6bp3Ol6AmC-CnHm dAz2kc_M0C5Xr5_eDC3HoDzBhd9NCvQ9Tg1Lcd2kDXyC0iATO4c_I-FSBIgGb6dUhmqpokZ7qPl9qmvR0PXi5UWJXtngXUnfR3-ljiYcE9PqE8KRQM/ s640/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;360&quot; data-original-width=&quot;640&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5921eqIbEdm5k9lS-EqcW5VwLeSRdMhL1pU2jnQTk5NzdoO2FKPcQHbTd6bp3Ol6AmC-CnHmdAz2kc_M0C5Xr5_eDC3HoDzB hd9NCvQ9Tg1Lcd2kDXyC0iATO4c_I-FSBIgGb6dUhmqpokZ7qPl9qmvR0PXi5UWJXtngXUnfR3-ljiYcE9PqE8KRQM/s16000/image6.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用提供的注释（分割、深度和相机）构建的真实会话的 3D 场景职位）。顶部中心视频显示深度图，右上角显示 RGB 或语义注释。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;/h2>;&lt;h2 >;SANPO-Real&lt;/h2>; &lt;p>; SANPO-Real 是一个多视图视频数据集，包含使用两个立体摄像机录制的 701 个会话：头戴式 &lt;a href=&quot;https://www.stereolabs.com/zed-mini /&quot;>;ZED Mini&lt;/a>; 和胸部安装式 &lt;a href=&quot;https://www.stereolabs.com/zed-2i/&quot;>;ZED-2i&lt;/a>;。每个会话有 4 个 &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; 流，帧速率为 15 FPS。 597 个会话以 2208x1242 像素的分辨率录制，其余部分以 1920x1080 像素的分辨率录制。每个会话时长约为 30 秒，录制的视频均使用 &lt;a href=&quot;https://www.stereolabs.com/docs/&quot;>;Zed 软件&lt;/a>; 进行校正并以无损格式保存。每个会话都有高级属性注释、相机姿态轨迹、来自 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf&quot;>;CREStereo&lt;/a>; 的密集深度图，以及&lt;a href=&quot;https://www.stereolabs.com/docs/&quot;>;Zed SDK&lt;/a>; 提供的稀疏深度图。会话的子集具有每个实例的时间一致的&lt;a href=&quot;https://arxiv.org/abs/1801.00868&quot;>;全景分割&lt;/a>;注释。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLnM9NZMV5rPoS39qD3xw4bykNWCQLAC-M65laB2aFOgE_9sfCXGuC-iggztvkbwAIZXXWF075JOdWSSvFpg-chHQr Al-oeJqBgBevHRtWd_3AgjAmgD2_hOjP3tTBn8N_7AIwN-MBFs6fwDp_WmIuXqNcwFmPBVIiZPR02AHCSXxjTfAAp8Mcy5sh5s/s624 /image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;624&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLnM9NZMV5rPoS39qD3xw4bykNWCQLAC-M65laB2aFOgE_9sfCXGuC-iggztvkbwAIZXXWF075JOdWSSvFpg-chHQrAl-oeJqBgBevHRtWd_3AgjAmg D2_hOjP3tTBn8N_7AIwN-MBFs6fwDp_WmIuXqNcwFmPBVIiZPR02AHCSXxjTfAAp8Mcy5sh5s/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;用于收集真实世界数据的 SANPO 数据收集系统。&lt;strong >;右&lt;/strong>;：(i) 装有用于数据收集的 ZED 2i 和 ZED Mini 相机的背包（&lt;strong>;底部&lt;/strong>;），(ii) 背包内部显示 ZED 盒子和安装在其上的电池组3D 打印容器（&lt;strong>;中&lt;/strong>;），以及 (iii) 一个显示 ZED 摄像头实时反馈的 Android 应用程序（&lt;strong>;顶部&lt;/strong>;）。&lt;strong>;左：&lt;/ strong>;&amp;nbsp;胸戴式ZED-2i立体基线为12cm，焦距为2.1mm，头戴式ZED Mini立体基线为6.3cm，焦距为2.1mm。&lt;/em>;&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;时间一致的全景分割注释协议&lt;/h3>; &lt;p>; SANPO 包括三十种不同的类标签，包括各种表面（道路、人行道、路边）等）、栅栏（护栏、墙壁、大门）、障碍物（杆子、自行车架、树木）和生物（行人、骑手、动物）。为这些类收集高质量的注释是一个巨大的挑战。为了提供时间上一致的全景分割注释，我们使用级联注释协议将每个视频划分为 30 秒的子视频，并每隔 5 帧进行注释（每个子视频 90 帧）。在每个阶段，我们要求注释者一次围绕五个互斥的标签绘制边界。我们将相同的图像发送给不同的注释者，其阶段与收集蒙版所需的阶段一样多，直到分配所有标签，并且先前子集的注释被冻结并显示给注释者。我们使用 &lt;a href=&quot;https://arxiv.org/abs/2106.02638&quot;>;AOT&lt;/a>;，这是一种机器学习模型，它通过为注释者提供从之前的帧中获取的自动掩码来减少注释工作。注释过程。 AOT 还使用手动注释的前后帧来推断中间帧的分段注释。总体而言，这种方法减少了注释时间，提高了边界精度，并确保长达 30 秒的时间一致注释。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiz3jvevdmtzqxOFoJUrQhHDjoo-RZtZ78maxgZoBJCcPnvc3UsFNsOKaPljdY0K9Go7QBqUFZgwN8Jli2dcAff连字符连字符Qx4R42b2yVT-vfO5l2V0OiH99-iBFG4qHsE3hXkdFxu5AYxwE917JqZ1jfA190VtXqllOUVHPIswpJ9S7b2Qi2WUsCe_HsNEqwBg/s1081/image7.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;325&quot; data-original-width=&quot;1081&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiz3jvevdmtzqxOFoJUrQhHDjoo-RZtZ78maxgZoBJCcPnvc3UsFNsOKaPljdY0K9Go7QBqUFZgwN8Jli2dcAffhyphenhyphenQx4R42b2yVT-vfO5l2 V0OiH99-iBFG4qHsE3hXkdFxu5AYxwE917JqZ1jfA190VtXqllOUVHPIswpJ9S7b2Qi2WUsCe_HsNEqwBg/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;时间一致的全景分割注释。分段掩码的标题指示它是手动注释的还是 AOT 传播的。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;SANPO-Synthetic&lt;/h2>; &lt;p >; 由于硬件、算法和人为错误，现实世界的数据具有不完美的真实数据标签，而合成数据具有近乎完美的真实数据并且可以定制。我们与专门从事逼真合成数据生成的公司 &lt;a href=&quot;https://paralleldomain.com/&quot;>;Parallel Domain&lt;/a>; 合作，创建了 SANPO-Synthetic，这是一个高质量的合成数据集，以补充 SANPO-真实的。 Parallel Domain 擅长为机器学习应用程序创建手工合成环境和数据。得益于他们的工作，SANPO-Synthetic 将现实世界的拍摄条件与相机参数、位置和场景相匹配。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKZLoH89yql9CrcYDxR2j6Xy8DWqq0kbGAqElYTQM0yHjW3iWPIbP07vLKnNVo7tqgeVrKnccIbbB03AXVmN至56hNtLLF92T-0Zxln-q9n9Mc2Kia4tfHa-DjFQ9CkTurPK1loDxw8U_2HOAsm5tkicYdY5-ndR3wYToc5Aa8NwyxZYWwWNFWbpLwf1A/s480/image8 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKZLoH89yql9CrcYDxR2j6Xy8DWqq0kbGAqElYTQM0yHjW3iWPIbP07vLKnNVo7tqgeVrKnccIbbB03AXVmNto56hNtLLF92T-0Zxln-q9n 9Mc2Kia4tfHa-DjFQ9CkTurPK1loDxw8U_2HOAsm5tkicYdY5-ndR3wYToc5Aa8NwyxZYWwWNFWbpLwf1A/s16000/image8.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用提供的注释（分段、深度和里程计）构建的合成会话的 3D 场景。顶部中心视频显示深度图，右上角显示 RGB 或语义注释。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; SANPO-Synthetic 是一种高质量的视频数据集，手工制作以匹配现实世界场景。它包含使用虚拟化 Zed 摄像机记录的 1961 个会话，均匀分布在胸部安装位置和头部安装位置以及校准之间。这些视频是单眼的，仅从左镜头录制。这些会话的长度和 FPS（5、14.28 和 33.33）各不相同，以实现时间分辨率/长度权衡的混合，并以无损格式保存。所有会话都具有精确的相机姿势轨迹、密集像素精确深度图和时间一致的全景分割掩模。 &lt;/p>; &lt;p>; SANPO-Synthetic 数据具有像素完美的注释，即使对于小而遥远的实例也是如此。这有助于开发具有挑战性的数据集来模拟现实世界场景的复杂性。 SANPO-Synthetic 和 SANPO-Real 也是彼此的直接替代品，因此研究人员可以研究域转移任务或在训练过程中使用合成数据，而几乎不需要特定于域的假设。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibHOjQgGs8QvEs9YEuvqcrC1eu7ld4dtHpAgr2HUOnCHgDucRq1HpwbqDc4HF3YhCmq8MTwpKyoR-iTCRVvhElXXVSnDI CdDfNrkDiwCAWjU2gUWqm_ye2XomqSxpymie9QPR53YxA9WcTM5MTp96zjeE1dMao_pIVO2dMZknhUrZ8yPVxdw1PK5weXeQ/s960/image4.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEibHOjQgGs8QvEs9YEuvqcrC1eu7ld4dtHpAgr2HUOnCHgDucRq1HpwbqDc4HF3YhCmq8MTwpKyoR-iTCRVvhElXXVSnDICdDfNrkDiwCAWjU2gUWqm_ye2XomqSxpy mie9QPR53YxA9WcTM5MTp96zjeE1dMao_pIVO2dMZknhUrZ8yPVxdw1PK5weXeQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;真实场景和合成场景的均匀采样。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br / >; &lt;h2>;统计&lt;/h2>; &lt;h3>;语义类别&lt;/h3>; &lt;p>; 我们设计了 SANPO 分类法：i) 考虑到人类以自我为中心的导航，ii) 目标是相当容易注释，以及 iii ）尽可能接近现有的细分分类法。尽管在构建时考虑了人类以自我为中心的导航，但它可以轻松映射或扩展到其他人类以自我为中心的场景理解应用程序。 SANPO-Real 和 SANPO-Synthetic 都具有人们在以自我为中心的障碍物检测数据中所期望的各种对象，例如道路、建筑物、栅栏和树木。 SANPO-Synthetic 包括广泛分布的手工建模对象，而 SANPO-Real 则具有更多在图像中不常出现的“长尾”类，例如大门、公交车站或动物。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvu3MvAucerGc2NU9GpGvXTCQ0p9zmM9WnyrrjLPIlzG1saJopaIs7UsMB_BWSDAk2U0AH6j0gtRN4ZteYYlA8ooiFRSe khO1Lf_8guS9GHWwY1VC9JIzZgUFQ2vVpsuDq5Yxjh8LMZVrzfNAdsG1cyMsIjibBQ80uXPDpOV7Z1vRADdp60H6fi6ACgPw/s1999/image2.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1196&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgvu3MvAucerGc2NU9GpGvXTCQ0p9zmM9WnyrrjLPIlzG1saJopaIs7UsMB_BWSDAk2U0AH6j0gtRN4ZteYYlA8ooiFRSekhO1Lf_8guS9GHWwY1VC9JizZgUFQ2vVpsuDq 5Yxjh8LMZVrzfNAdsG1cyMsIjibBQ80uXPDpOV7Z1vRADdp60H6fi6ACgPw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;&lt;em style=&quot;text-align: left;&quot;>;图像在 SANPO 分类法中各类的分布。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt; h2>;实例掩码&lt;/h2>; &lt;p>; SANPO-Synthetic 和 SANPO-Real 的一部分也用全景实例掩码进行注释，它将每个像素分配给一个类和实例 ID。因为它通常是人工标记的，所以 SANPO-Real 具有大量帧，通常每帧少于 20 个实例。同样，SANPO-Synthetic 的虚拟环境可为场景中最独特的对象提供像素精确的分割。这意味着合成图像通常在每一帧中包含更多实例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF1gYL6YUhpZyIG4vWz8CkuPs3UH4nMJnRyiTqc1Z6cSrAoA5qnQehWBj15TOeYnWgzpQ5RXPTRMTsGZV-2 x0PKTkZKBuqV_y-K0TebqbXT0XtzRqpdPOS7XhCRY81tYkDRp6oSPnjzsomDpgtZFQIbIsJu8AhPxkY5vttiFNNH7wwcR3me-bVc5yPJ1Q/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF1gYL6YUhpZyIG4vWz8CkuPs3UH4nMJnRyiTqc1Z6cSrAoA5qnQehWBj15TOeYnWgzpQ5RXPTRMTsGZV-2x0PKTkZKBuqV_y-K0TebqbXT0 XtzRqpdPOS7XhCRY81tYkDRp6oSPnjzsomDpgtZFQIbIsJu8AhPxkY5vttiFNNH7wwcR3me-bVc5yPJ1Q/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;在考虑每帧实例计数时，合成数据每帧的实例数量通常比 SANPO 的标记部分多得多-真实。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;与其他数据集的比较&lt;/h2>; &lt;p>;我们在此将 SANPO 与其他重要视频数据集进行比较字段，包括 &lt;a href=&quot;https://www.cs.utexas.edu/~xiao/SCAND/SCAND.html&quot;>;SCAND&lt;/a>;、&lt;a href=&quot;https://cs.gmu.edu /~xiao/Research/MuSoHu/&quot;>;MuSoHu&lt;/a>;、&lt;a href=&quot;https://ego4d-data.org/&quot;>;Ego4D&lt;/a>;、&lt;a href=&quot;https://github. com/VIPSeg-Dataset/VIPSeg-Dataset&quot;>;VIPSeg&lt;/a>; 和 &lt;a href=&quot;https://waymo.com/open/&quot;>;Waymo Open&lt;/a>;。其中一些用于机器人导航 (SCAND) 或自动驾驶 (Waymo) 任务。在这些数据集中，只有 Waymo Open 和 SANPO 同时拥有全景分割和深度图，并且只有 SANPO 同时拥有真实数据和合成数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjv6lPLgaaNdFMyHnTXz2fAKueP_2nPi-W1VHGcoM-YA7bSJhJn6ImFD0uCRPn7rCaKBaQHorWCxnlkB5dBRCGk-IOM_e 5BRt3Ovj7MoLp264JDxU5H3vIEYr3NsU0YGXV9te41FeN0epoqh3LvsijQWyP4OP_NjoRWxpyOhqSyrXTcL_IU-k5NzJjiggVm/s848/SANPO比较.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;848&quot; height=&quot;468&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjv6lPLgaaNdFMyHnTXz2fAKueP_2nPi-W1VHGcoM-YA7bSJhJn6ImFD0uCRPn7rCaKBaQHorWCxnlkB5dBRCGk-IOM_e5BRt3Ovj7MoLp264J DxU5H3vIEYr3NsU0YGXV9te41FeN0epoqh3LvsijQWyP4OP_NjoRWxpyOhqSyrXTcL_IU-k5NzJjiggVm/w640-h468/SANPOComparison.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;与其他视频数据集的比较。对于立体声与单声道视频，标记为 ★ 的数据集为所有场景提供立体声视频，标记为 ☆ 的数据集为子集提供立体声视频。对于深度图，★ 表示密集深度，而 ☆ 表示稀疏深度，例如来自较低分辨率的激光雷达扫描仪。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2 >;结论和未来的工作&lt;/h2>; &lt;p>;我们提出了 SANPO，这是一个用于人类以自我为中心的场景理解的大规模且具有挑战性的视频数据集，其中包括具有密集预测注释的真实和合成样本。我们希望 SANPO 能够帮助研究人员为视障人士构建视觉导航系统，并促进视觉场景理解。更多详细信息请参阅&lt;a href=&quot;https://arxiv.org/abs/2309.12172&quot;>;预印本&lt;/a>;和&lt;a href=&quot;https://github.com/google-research-datasets /sanpo_dataset&quot;>;SANPO 数据集 GitHub 存储库&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该数据集是来自 Google 和我们的外部合作伙伴 Parallel Domain 内部各个团队的许多人辛勤工作的成果。 &lt;br />;&lt;br />; &lt;/em>;核心团队：&lt;em>;Mikhail Sirotenko、Dave Hawkey、Sagar Waghmare、Kimberly Wilber、Xuan Yang、Matthew Wilson&lt;br />;&lt;br />; &lt;/em>;并行域： &lt;em>; 斯图尔特·帕克 (Stuart Park)、艾伦·杜塞特 (Alan Doucet)、亚历克斯·瓦伦斯-拉努埃 (Alex Valence-Lanoue) 和拉尔斯·潘迪科夫。 &lt;br />;&lt;br />; 我们还要感谢以下团队成员：Hartwig Adam、Huisheng Wang、Lucian Ionita、Nitesh Bharadwaj、Suqi Liu、Stephanie Debats、Cattalyya Nuengsigkapian、Astuti Sharma、Alina Kuznetsova、Stefano Pellegrini、Yiwen Luo 、 Lily Pagan、Maxine Deines、Alex Siegman、Maura O&#39;Brien、Rachel Stigler、Bobby Tran、Supinder Tohra、Umesh Vashisht、Sudhindra Kopalle、Reet Bhatia。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot; http://blog.research.google/feeds/1075240247864880418/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog .research.google/2023/10/sanpo-scene-understanding-accessibility.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http: //www.blogger.com/feeds/8474926331452026626/posts/default/1075240247864880418&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds /8474926331452026626/posts/default/1075240247864880418&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/sanpo-scene-understanding- accessibility.html&quot; rel=&quot;alternate&quot; title=&quot;SANPO：场景理解、辅助功能、导航、寻路和避障数据集&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>; &lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas. google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:缩略图高度=“72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh10zxbymBGZgjXFHDrw-CdxlVL7nRi6yjaI3w3X_x5pjxn8UWA7NnymAqMXomfjSBXWVDQ4czo8nqINhxzIPLVx2Uv1l8RDQAbikuWW jIt9IwxSIuSlEtJ5AJwOJkdaKPwzUdu9BwkJJP1gDj2UJQpkJ15ELGjSKHMl9ce0_470SwRz5snz32lV-vSlMd2/s72-c/SANPOHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http ://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：博客-8474926331452026626.post-8807628089114947298&lt;/id>;&lt;已发布>;2023-10-04T10:26:00.003-07:00&lt;/已发布>;&lt;更新>;2023-10-04T10:26:49.868-07:00&lt;/已更新>; &lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;用于科学应用的可扩展球形 CNN&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Research、Athena 团队的研究科学家 Carlos Esteves 和 Ameesh Makadia&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw 6c1au-9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s398/image2.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 计算机视觉的典型深度学习模型，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 和&lt;a href=&quot; https://en.wikipedia.org/wiki/Vision_transformer&quot;>;视觉变换器&lt;/a>; (ViT)，假设平面（平坦）空间的处理信号。例如，数字图像表示为平面上的像素网格。然而，此类数据仅占我们在科学应用中遇到的数据的一小部分。从地球大气层采样的变量（例如温度和湿度）自然地呈现在球体上。某些类型的&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosmic_microwave_background&quot;>;​​宇宙学数据&lt;/a>;和全景照片也是球形信号，因此更好地进行处理。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 使用为平面图像设计的方法来处理球形信号存在问题，原因有几个。首先，存在采样问题，即无法在球体上定义平面 CNN 和 ViT 所需的均匀网格，且不会产生严重失真。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU4VfEu7zoN6hcXN5a0xaKnCpLgMub71nN059lUTuQgHiW8shAUKxGjcO3ZQddZ-vBPsykL9WQbv4mK9iAE e5iSha93qzlansJKQJdPzxZfxMkX1PU0YP9j9_nqEJiehqpMAW5p7UmSt6XtQSYzJvOnReXQJTIf6lZcYnHhYZpBfnSzbGs9xVajOP7wVs3/s1692/SphericalDistortions.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiU4VfEu7zoN6hcXN5a0xaKnCpLgMub71nN059lUTuQgHiW8shAUKxGjcO3ZQddZ-vBPsykL9WQbv4mK9iAEe5iSha93qzlansJKQJdPzxZfxMkX1PU 0YP9j9_nqEJiehqpMAW5p7UmSt6XtQSYzJvOnReXQJTIf6lZcYnHhYZpBfnSzbGs9xVajOP7wVs3/s16000/SphericalDistortions.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;将球体投影到平面上时，红色圆圈表示的面片在两极附近严重扭曲。这种采样问题损害了传统 CNN 和 ViT 在球形输入上的准确性。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 其次，球体通常因旋转而变得复杂，因此模型需要一种方法来解决这个问题。我们希望与 3D 旋转具有&lt;a href=&quot;https://en.wikipedia.org/wiki/Equivariant_map&quot;>;等变&lt;/a>;，以确保学习到的特征遵循输入的旋转。这可以更好地利用模型参数，并允许使用更少的数据进行训练。 3D 旋转等变在大多数输入没有首选方向的设置中也很有用，例如 3D 形状和分子。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhzyhCxjMHyAkR3LfadfFYdzY2TDks15-Z_pJ9n7UZQgYmf8HDYHX09cl_Qd4j3yOmo7Fr3T-rzZ_OTX5jky3 Vhu1R_N9IPqsCeV1LWAB90tPGsYnQykUDKxwte9dd4ONfYzxmbe1l0JtLoIsvcPKq4XjBAPBe1MGZSc2yFAFRMTBlL3Xp33EhAVPGhwaF/s512/image3.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;256&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhzyhCxjMHyAkR3LfadfFYdzY2TDks15-Z_pJ9n7UZQgYmf8HDYHX09cl_Qd4j3yOmo7Fr3T-rzZ_OTX5jky3Vhu1R_N9IPqsCeV1LWAB90tPGsY nQykUDKxwte9dd4ONfYzxmbe1l0JtLoIsvcPKq4XjBAPBe1MGZSc2yFAFRMTBlL3Xp33EhAVPGhwaF/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用全景相机进行无人机竞赛。这里，急转弯会导致球形图像发生较大的 3D 旋转。我们希望我们的模型对于这种旋转具有鲁棒性。来源：&lt;a href=&quot;https://www.youtube.com/watch?v=_J7qXbbXY80&quot;>;https://www.youtube.com/watch?v=_J7qXbbXY80&lt;/a>;（已获得许可;&lt;a href=&quot;https://creativecommons.org/licenses/by/3.0/legalcode&quot;>;CC BY&lt;/a>;）&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcdTjU_u4KqbG40bY_qPZRkt8mS_wPR2cotwKueGkMqRnz2GCkyFNfZb6UPktZ7hIkELEhEEdSlabTFlryxDNicP7Uvv7e 8N96u8WKEe79lWNpeY6eeAPPdCLzrbWEcRcNGPLlZwjgQ16petAInpKme8UPjg-j3wIDO3W9NwxtHkuF5AIi4VqZa6-z_YVw/s711/image1.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;358&quot; data-original-width=&quot;711&quot; height=&quot;322&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcdTjU_u4KqbG40bY_qPZRkt8mS_wPR2cotwKueGkMqRnz2GCkyFNfZb6UPktZ7hIkELEhEEdSlabTFlryxDNicP7Uvv7e8N96u8WKEe79lWNpeY6eeAPP dCLzrbWEcRcNGPLlZwjgQ16petAInpKme8UPjg-j3wIDO3W9NwxtHkuF5AIi4VqZa6-z_YVw/w640-h322/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;在大气中，经常会看到相似的图案出现在不同的位置和方向。我们希望我们的模型能够共享参数来识别这些模式。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;考虑到上述挑战，在“&lt; a href=&quot;https://arxiv.org/abs/2306.05420&quot;>;缩放球形 CNN&lt;/a>;”，发表于 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/ a>;，我们在 &lt;a href=&quot;https://jax.readthedocs.a>; 中引入了一个开源&lt;a href=&quot;https://github.com/google-research/spherical-cnn&quot;>;库&lt;/a>;。 io/en/latest/&quot;>;JAX&lt;/a>; 用于球面上的深度学习。我们演示了该库的应用程序如何在天气预报和分子特性预测基准（通常使用 Transformer 和图神经网络解决的任务）方面匹配或超越最先进的性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;球形 CNN 背景&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv .org/abs/1801.10130&quot;>;球形 CNN&lt;/a>; 利用球形解决了&lt;a href=&quot;https://arxiv.org/abs/1711.06721&quot;>;采样和旋转鲁棒性&lt;/a>;问题卷积和互相关运算，通常通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform#Compact_non-abelian_groups&quot;>;广义傅里叶变换&lt;/a>;计算。然而，对于平面，使用小滤波器的卷积速度更快，因为它可以在规则网格上执行，而无需使用傅里叶变换。迄今为止，球形输入的计算成本较高，限制了球形 CNN 在小型模型和数据集以及低分辨率数据集上的应用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;我们的贡献&lt;/h2>; &lt;p>; 我们已经实现了 &lt;a href=&quot; 中的球形卷积https://arxiv.org/abs/2006.10731&quot;>;&lt;a href=&quot;https://jax.readthedocs.io/en/latest/&quot;>;JAX&lt;/a>; 中的自旋加权球形 CNN&lt;/a>;注重速度，并使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_parallelism&quot;>;数据并行&lt;/a>;对大量 TPU 进行分布式训练。我们还引入了新的相位塌陷激活和光谱批量归一化层，以及可提高准确性和效率的新残差块，从而可以训练比以前大 100 倍的更准确模型。我们将这些新模型应用于分子特性回归和天气预报。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggm83f_OaxfBc4k1uMFMGudMpAWMDMZQkAn9pKBLcdzolHk_3f0emRMupSiu10dcCoxWWEf3KexI6FokwtdkwujNcygfMwc2 p-OvFCSUAWTmM6RE6sAcRCjwRSZxcCoq9gK_sdqyH_lZVLOxe4rL-gFBTtVdZjWjDsxcLPO23bYhOXaMXTUvZoKSs9AVao/s640/image7.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;431&quot; data-original-width=&quot;640&quot; height=&quot;430&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggm83f_OaxfBc4k1uMFMGudMpAWMDMZQkAn9pKBLcdzolHk_3f0emRMupSiu10dcCoxWWEf3KexI6FokwtdkwujNcygfMwc2p-OvFCSUAWTmM6RE6sAcRCjwRS ZxcCoq9gK_sdqyH_lZVLOxe4rL-gFBTtVdZjWjDsxcLPO23bYhOXaMXTUvZoKSs9AVao/w640-h430/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;我们将球形 CNN 的特征尺寸最多扩展了两个数量级和模型容量，与文献相比：&lt;a href=&quot;https://arxiv.org/abs/1801.10130&quot;>;Cohen&#39;18&lt;/a>;、&lt;a href=&quot;https://arxiv. org/abs/1711.06721&quot;>;伊斯特维斯&#39;18&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2006.10731&quot;>;伊斯特维斯&#39;20&lt;/a>; 和&lt;a href=&quot; https://arxiv.org/abs/2010.11661&quot;>;Cobb&#39;21&lt;/a>;。&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;>;VGG-19&lt;/a>;作为传统的 CNN 参考。我们最大的天气预报模型有 256 x 256 x 78 个输入和输出，在训练期间运行 96 个卷积层，最低内部分辨率为 128 x 128 x 256。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;br />; &lt;h2>;分子特性回归&lt;/h2>; &lt;p>;预测分子特性在&lt;a href=&quot;https://www.nature.com/articles/d41586-018- 05267-x&quot;>;药物发现&lt;/a>;，其目标是快速筛选大量分子，寻找具有所需特性的分子。类似的模型也可能与针对蛋白质之间相互作用的药物设计相关。当前计算或实验量子化学中的方法非常昂贵，这激励了机器学习的使用。 &lt;/p>; &lt;p>; 分子可以用一组原子及其在 3D 空间中的位置来表示；分子的旋转改变了位置，但不改变分子特性。由于球形 CNN 的旋转等方差性，这促进了其应用。然而，分子并没有被定义为球体上的信号，因此第一步是将它们映射到一组球函数。我们通过利用分子原子之间基于物理的相互作用来做到这一点。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBdDPSVCSfr2FQ4_Re4Pdhd0UQ4-1pFUROon3__vXr3-N2kOhWr7uN8K3QfukpYcDIIVj6hPCIAgYhltWpGHHtq5ywvJ _WGLMu5lSo0mbJYUZw8I0H0mw_CDb50D_dGQO__tPt-Jb81ooB6WLw-5Zx6C87jX4fKHgQaPxGy27kykUtrxpvlMuRhsV9Jr7i/s1600/image6 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1600&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBdDPSVCSfr2FQ4_Re4Pdhd0UQ4-1pFUROon3__vXr3-N2kOhWr7uN8K3QfukpYcDIIVj6hPCIAgYhltWpGHHtq5ywvJ_WGLMu5lSo0mbJYUZw8I0H0m w_CDb50D_dGQO__tPt-Jb81ooB6WLw-5Zx6C87jX4fKHgQaPxGy27kykUtrxpvlMuRhsV9Jr7i/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;每个原子都由一组球形信号表示，这些信号积累了与每种类型的其他原子的物理相互作用（如右侧的三个面板所示）。例如，氧原子（O；上图）有一个氧气通道（由左侧标记为“O”的球体表示）和氢气通道（右侧为“H”）。相对于两个氢原子而言，氧原子上累积的&lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb%27s_law&quot;>;库仑力&lt;/a>;由红色阴影表示球体底部标记为“H”的区域。由于氧原子本身不产生任何力，因此“O”球体是均匀的。我们为&lt;a href=&quot;https://en.wikipedia.org/wiki/Van_der_Waals_force&quot;>;范德华力&lt;/a>;提供了额外的通道。&lt;/em>;&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 球形 CNN 应用于每个原子的特征，随后将结果组合起来以产生属性预测。这使得大多数属性都具有最先进的性能，通常在 &lt;a href=&quot;https://www.nature.com/articles/sdata201422&quot;>;QM9&lt;/a>; 基准中进行评估：&lt;/p>; &lt;表align =“center”cellpadding =“0”cellspacing =“0”class =“tr-caption-container”style =“margin-left：auto; margin-right：auto;”>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVQebJURRBto17o1rYA7wz-Ze-vwHt0gtDHHbloA51I9__iaHu9KtRm8eI639aChAIqEE-rlmy4v8Kyg4TERuNP4sJfzOkXYwaJ9 CC1DU6-vQf0nH85hBXnuGWYhP6mRMGr_Sns_WonWqAkTtM8bbvnx3QbOdrGFEYF60_f9Q7F6uSzLbzVptd_EnnixNZ/s1409/image4.png&quot;样式= “左边距：自动；右边距：自动；”&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;786&quot; data-original-width=&quot;1409&quot; src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhVQebJURRBto17o1rYA7wz-Ze-vwHt0gtDHHbloA51I9__iaHu9KtRm8eI639aChAIqEE-rlmy4v8Kyg4TERuNP4sJfzOkXYwaJ9CC1DU6-vQf0nH85hBXnuGWYh P6mRMGr_Sns_WonWqAkTtM8bbvnx3QbOdrGFEYF60_f9Q7F6uSzLbzVptd_EnnixNZ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption &quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;在 QM9 的 12 个属性上与最新技术的错误比较（请参阅数据集&lt;a href= “http://www.nature.com/articles/sdata201422&quot;>;论文&lt;/a>;&amp;nbsp;了解详细信息）。我们展示&lt;a href=&quot;https://arxiv.org/abs/2202.02541&quot;>;TorchMD-Net&lt;/a>;&amp;nbsp;和&lt;a href=&quot;https://arxiv.org/abs/2102.03150&quot;>; PaiNN&lt;/a>;&amp;nbsp;结果，将 TorchMD-Net 错误标准化为 1.0（越低越好）。我们的模型（以绿色显示）在大多数目标中都优于基线。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;天气预报&lt;/h2>; &lt;p>;准确的气候预报是及时提供极端天气事件预警、实现有效水资源管理和指导明智的基础设施规划的宝贵工具。在一个日益&lt;a href=&quot;https://www.ncei.noaa.gov/access/billions/&quot;>;受到气候灾害威胁&lt;/a>;的世界中，迫切需要更快、更准确地提供预测比一般环流模型的时间跨度更长。预测模型对于预测应对气候变化努力的安全性和有效性也很重要，例如&lt;a href=&quot;https://www.ametsoc.org/index.cfm/ams/about-ams/ams-statements /statements-of-the-ams-in-force/climate-intervention/&quot;>;气候干预措施&lt;/a>;。当前最先进的技术使用基于流体动力学和热力学的昂贵&lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;>;数值模型&lt;/a>;，这些模型在经过一段时间后往往会发生漂移。几天。 &lt;/p>; &lt;p>; 鉴于这些挑战，机器学习研究人员迫切需要解决气候预测问题，因为数据驱动技术具有降低计算成本和提高长期准确性的潜力。球形 CNN 适合这项任务，因为大气数据本身就呈现在球体上。它们还可以有效地处理此类数据中常见的不同位置和方向的重复模式。 &lt;/p>; &lt;p>; 我们将我们的模型应用于多个天气预报基准，并且优于或匹配基于传统 CNN 的神经天气模型（具体来说，&lt;a href=&quot;https://arxiv.org/abs/2002.00469&quot;>;1&lt; /a>;、&lt;a href=&quot;https://arxiv.org/abs/2008.08626&quot;>;2&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2205.10972&quot;>;3&lt;/一个>;）。下面我们展示了&lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;测试设置&lt;/a>;中的结果，其中模型将许多大气变量作为输入，并提前六小时预测它们的值。然后，该模型迭代地应用于其自身的预测，以产生更长的预测。在训练期间，模型最多可提前三天进行预测，并可最多进行五天的评估。 &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler&lt;/a>; 为该任务提出了一种图神经网络，但我们证明球形 CNN 在相同设置下可以匹配 GNN 的精度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS-sTrl2nwk9Lys2hVTJznTbLnl3ii-VodPHRD5QkKWRl71Ri5IW1N3j9XqVvdUbSx-vp2UudZiPGtk3Qy5 hfHCoktwbyHVwbONnK7hLA7SSn7wnpRTTJ5FC3Z51tMVRAvAocexLw9AJUqyTOEZPYyINqzGmw2Obgy5LlhQVbLUtJgHLep_POyn6ANTqQz/s1300/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;353&quot; data-original-width=&quot;1300&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS-sTrl2nwk9Lys2hVTJznTbLnl3ii-VodPHRD5QkKWRl71Ri5IW1N3j9XqVvdUbSx-vp2UudZiPGtk3Qy5hfHCoktwbyHVwbONnK7hLA7SSn7w npRTTJ5FC3Z51tMVRAvAocexLw9AJUqyTOEZPYyINqzGmw2Obgy5LlhQVbLUtJgHLep_POyn6ANTqQz/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用球形 CNN 提前五天（120 小时）进行迭代天气预报。动画显示给定压力下的具体湿度预测及其误差。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot; 0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au- 9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s398/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; >;&lt;img border=&quot;0&quot; data-original-height=&quot;398&quot; data-original-width=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZl BUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP -Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;使用球形 CNN 进行风速和温度预测。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;其他资源&lt; /h2>; &lt;p>; 我们用于高效球形 CNN 的 JAX 库现已&lt;a href=&quot;https://github.com/google-research/spherical-cnn&quot;>;可用&lt;/a>;。我们已经展示了在分子属性回归和天气预报方面的应用，我们相信该库将有助于其他科学应用以及计算机视觉和 3D 视觉。 &lt;/p>; &lt;p>; 天气预报是 Google 的一个活跃研究领域，其目标是构建更准确、更强大的模型 - 例如 &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;Graphcast&lt;/ a>;，一个最近基于机器学习的中期预测模型 - 并构建能够促进整个研究社区进一步发展的工具，例如最近发布的 &lt;a href=&quot;https://blog.research.google/2023/08 /weatherbench-2-benchmark-for-next.html&quot;>;WeatherBench 2&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是与 Jean-Jacques 合作完成的Slotine，基于之前与 Kostas Daniilidis 和 Christine Allen-Blanchette 的合作。我们感谢 Stephan Hoyer、Stephan Rasp 和 Ignacio Lopez-Gomez 在数据处理和评估方面提供的帮助，感谢 Fei Sha、Vivian Yang、Anudhyan Boral、Leonardo Zepeda-Núñez 和 Avram Hershko 的建议和讨论。我们感谢 Michael Riley 和 Corinna Cortes 对这个项目的支持和鼓励。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8807628089114947298/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/scalable-spherical-cnns-for- science.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /8807628089114947298&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8807628089114947298&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/scalable-spherical-cnns-for-scientific.html&quot; rel=&quot;alternate&quot; title=&quot;可扩展用于科学应用的球形 CNN&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZa P-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s72-c/image2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7111364624265266582&lt;/id>;&lt;发布>;2023-10-02T00: 51:00.005-07:00&lt;/发布>;&lt;更新>;2023-10-07T07:25:19.127-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns# “ term=”会议”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“会议”>;&lt;/类别>;&lt;类别方案=“http://” www.blogger.com/atom/ns#&quot; term=&quot;ICCV&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 参加 ICCV 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;发布者：Shaina Mehta，Google 项目经理&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr 07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s320/ICCV%20hero.jpg “样式=“显示：无；” />; &lt;p>; Google 很荣幸成为 &lt;a href=&quot;&lt;a href=&quot;https://iccv2023.thecvf.com/iccv.2023.sponsors-93.php&quot;>;白金赞助商&lt;/a>; https://iccv2023.thecvf.com/&quot;>;国际计算机视觉会议&lt;/a>;（ICCV 2023），这是一项重要的年度会议，本周将于法国巴黎举行。作为计算机视觉研究领域的领导者，Google 在今年的会议上表现强劲，共收到 60 篇论文，并积极参与 27 个&lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.workshops-90 .php&quot;>;研讨会&lt;/a>;和&lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.tutorials-91.php&quot;>;教程&lt;/a>;。 Google 还很荣幸成为&lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;简历中的拉丁语&lt;/a>;研讨会的白金赞助商。我们期待分享我们广泛的计算机视觉研究成果，并扩大我们与更广泛的研究界的合作伙伴关系。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 参加 ICCV 2023？我们希望您能够参观 Google 展位，与积极追求计算机视觉最新创新的研究人员交谈，并查看一些预定的展位活动（例如下面列出的演示和问答环节）。访问 &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter 帐户，了解有关 ICCV 2023 上 Google 展位活动的更多信息。&lt;/p>; &lt;p>; 请看下面了解有关在 ICCV 2023 上展示的 Google 研究的更多信息（Google 隶属关系以&lt;strong>;粗体&lt;/strong>;显示）。 &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;董事会及组委会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 总主席：&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />; 财务主席：&lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>; &lt; br />; 劳资关系主席：&lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;br />; 宣传和社交媒体联合主席：&lt;strong>;&lt;em>;龚博庆&lt;/em>;&lt;/strong >; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;已接受论文&lt;/h2>; &lt;div style=&quot;margin -left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.14383.pdf&quot;>;具有轻量级 ToF 传感器的单目密集 SLAM 的多模态神经辐射场&lt;/a >; &lt;br />; &lt;em>;刘新阳&lt;/em>;、&lt;em>;李一进&lt;/em>;、&lt;em>;滕彦斌&lt;/em>;、&lt;em>;鲍虎军&lt;/em>;、&lt;em>;张国峰&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;崔兆鹏&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2309.05569.pdf&quot;>;ITI-GEN：包容性文本到图像生成&lt;/a>; &lt;br />; &lt;em>;Cheng Chang&lt;/em>;，&lt;em>;Xuanbai Chen&lt; /em>;、&lt;em>;柴思琪&lt;/em>;、&lt;em>;陈亨利吴&lt;/em>;、&lt;strong>;&lt;em>;Dmitry Lagun&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thabo Beeler &lt;/em>;&lt;/strong>;，&lt;em>;Fernando De la Torre&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16201.pdf&quot;>;ASIC：对齐稀疏的野外图像集合&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kamal Gupta&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;、&lt;em>;Abhinav Shrivastava&lt;/em>;、&lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;诺亚·斯内夫利&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿布舍克·卡尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.06833 .pdf&quot;>;VQ3D：在 ImageNet 上学习 3D 感知生成模型&lt;/a>; &lt;br />; &lt;em>;Kyle Sargent&lt;/em>;、&lt;em>;Jing Yu Koh&lt;/em>;、&lt;strong>;&lt;em>;张涵&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张惠文&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Charles Herrmann&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em >;Pratul Srinivasan&lt;/em>;&lt;/strong>;、&lt;em>;吴家军&lt;/em>;、&lt;strong>;&lt;em>;孙德庆&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2302.11154.pdf&quot;>;开放域视觉实体识别：识别数百万个维基百科实体&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;胡鹤翔&lt;/em>;&lt;/ strong>;,&lt;strong>;&lt;em>;鸾易&lt;/em>;&lt;/strong>;,&lt;em>;杨晨&lt;/em>;*,&lt;strong>;&lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;,&lt;strong>; >; &lt;em>;Mandar Joshi&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>;、&lt; strong>;&lt;em>;Ming-Wei Chang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15343.pdf&quot;>;语言图像预处理的 Sigmoid 损失-训练&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;翟晓华&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;巴兹尔·穆斯塔法&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;亚历山大·科列斯尼科夫&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;卢卡斯·拜尔&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05422 .pdf&quot;>;同时追踪所有地点&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;王倩倩&lt;/em>;&lt;/strong>;、&lt;em>;Yen-Yu Chang&lt;/em>;、&lt;em>;蔡若金&lt;/em>;、&lt;strong>;&lt;em>;李正琪&lt;/em>;&lt;/strong>;、&lt;em>;Bharath Hariharan&lt;/em>;、&lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>; ，&lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06706.pdf&quot;>;Zip-NeRF：反-基于别名网格的神经辐射场&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Ben Mildenhall&lt;/em>;&lt;/strong>; ,&lt;strong>;&lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Pratul P. Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;Peter Hedman&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07090.pdf&quot;>;Delta 去噪分数&lt;/a>; &lt;br />; &lt;em>;Amir Hertz&lt;/em>; *、&lt;strong>;&lt;em>;Kfir Aberman&lt;/em>;&lt;/strong>;、&lt;em>;丹尼尔·科恩-奥尔&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2303.13508.pdf&quot;>;DreamBooth3D：主题驱动的文本到 3D 生成&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Amit Raj&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;斯里尼瓦斯·卡扎&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;本·普尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·尼迈耶&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>; >;纳塔尼尔·鲁伊斯&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;本·米尔登霍尔&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;、&lt;strong>;&lt; em>;Kfir Aberman&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;迈克尔·鲁宾斯坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;乔纳森·巴伦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;李元珍&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;Varun Jampani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/ pdf/2306.09224.pdf&quot;>;百科全书式 VQA：有关细粒度类别详细属性的视觉问题&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em >;Jasper Uijlings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Lluis Castrejon&lt;/em>;&lt;/strong>;、&lt;em>;Arushi Goel&lt;/em>;*、&lt;em>;Felipe Cadar&lt;/em>;* 、&lt;strong>;&lt;em>;周浩&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;沙飞&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;安德烈·阿劳霍&lt;/em>;&lt;/strong>; >;，&lt;strong>;&lt;em>;Vittorio Ferrar&lt;/em>;i&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.05916.pdf&quot;>;GECCO：几何-条件点扩散模型&lt;/a>; &lt;br />; &lt;em>;Michał J. Tyszkiewicz&lt;/em>;、&lt;em>;Pascal Fua&lt;/em>;、&lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11489.pdf&quot;>;从不成对多视图之间的语义对齐中学习以进行自我中心视频识别&lt;/a>; &lt;br />; &lt;em>;Qitong王&lt;/em>;、&lt;strong>;&lt;em>;赵龙&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘婷&lt;/ em>;&lt;/strong>;, &lt;em>;Xi Peng&lt;/​​em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17806.pdf&quot;>;用于逆向渲染的神经微面场&lt; /a>; &lt;br />; &lt;em>;Alexander Mai&lt;/em>;、&lt;strong>;&lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;、&lt;em>;Falko Kuester&lt;/em>;、&lt;em>;Sara Fridovich- Keil&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09346.pdf&quot;>;Rosetta 神经元：挖掘模型动物园中的常见单元&lt;/a>; &lt;br / >; &lt;em>;Amil Dravid&lt;/em>;、&lt;em>;Yossi Gandelsman&lt;/em>;、&lt;em>;Alexei A. Efros&lt;/em>;、&lt;strong>;&lt;em>;Assaf Shocher&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12066.pdf&quot;>;教 CLIP 数到十&lt;/a>; &lt;br />; &lt;em>;Roni Paiss&lt;/em>;* 、&lt;strong>;&lt;em>;Ariel Ephrat&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Omer Tov&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Shiran Zada&lt;/em>;&lt;/strong>; >;、&lt;strong>; &lt;em>;Inbar Mosseri&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michal Irani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Tali Dekel&lt;/em>;&lt;/强>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12048.pdf&quot;>;Vox-E：文本引导的 3D 对象体素编辑&lt;/a>; &lt;br />; &lt; em>;Etai Sella&lt;/em>;、&lt;em>;Gal Fiebelman&lt;/em>;、&lt;strong>;&lt;em>;Peter Hedman&lt;/em>;&lt;/strong>;、&lt;em>;Hadar Averbuch-Elor&lt;/em>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12074.pdf&quot;>;CC3D：组合 3D 场景的布局条件生成&lt;/a>; &lt;br />; &lt;em>;Sherwin Bahmani&lt;/em >;、&lt;em>;Jeong Joon Park&lt;/em>;、&lt;em>;Despoina Paschalidou&lt;/em>;、&lt;em>;Xingguang Yan&lt;/em>;、&lt;em>;Gordon Wetzstein&lt;/em>;、&lt;em>;Leonidas Guibas&lt;/em>; em>;, &lt;strong>;&lt;em>;Andrea Tagliasacchi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11607.pdf&quot;>;深入研究运动-单目 3D 物体跟踪的感知匹配&lt;/a>; &lt;br />; &lt;em>;Kuan-Chih Huang&lt;/em>;、&lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;、&lt;strong>; &lt; em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01172.pdf&quot;>;3D 感知图像的生成多平面神经辐射一代&lt;/a>; &lt;br />; &lt;em>;Amandeep Kumar&lt;/em>;、&lt;em>;Ankan Kumar Bhunia&lt;/em>;、&lt;em>;Sanath Narayan&lt;/em>;、&lt;em>;Hisham Cholakkal&lt;/em>;、 &lt;em>;Rao Muhammad Anwer&lt;/em>;、&lt;em>;Salman Khan&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07313.pdf&quot;>;M2T：两次屏蔽变形金刚以加快解码速度&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Fabian Mentzer&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Eirikur Agustsson&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02859.pdf&quot;>;MULLER：用于视觉的多层拉普拉斯调整器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;屠正中&lt;/em>;&lt;/strong >;,&lt;strong>; &lt;em>;佩曼·米兰法&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;侯赛因·塔莱比&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2303.11305.pdf&quot;>;SVDiff：用于扩散微调的紧凑参数空间&lt;/a>; &lt;br />; &lt;em>;韩利公&lt;/em>;*，&lt;strong>;&lt;em>;李银晓&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;张涵&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Peyman Milanfar&lt;/em>;&lt;/strong>;、&lt;em>;迪米特里斯·梅塔克萨斯&lt;/em>; >;, &lt;strong>;&lt;em>;Feng Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.08996.pdf&quot;>;走向真实的人脸恢复迭代扩散模型及其他&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵阳&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;侯廷波&lt;/em>;&lt;/strong>;，&lt;strong>; &lt;em>;苏玉川&lt;/em>;&lt;/strong>;、&lt;strong>;贾旭辉&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;李彦东&lt;/em>;&lt;/strong>;、 &lt;strong>; &lt;em>;Matthias Grundmann&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08998.pdf&quot;>;利用视觉和视觉进行统一视觉关系检测语言模型&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;赵龙&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;龚伯庆&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;崔寅&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Florian Schroff&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em >;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;刘婷&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03757.pdf&quot;>;3D 运动放大：可视化时变辐射场中的细微运动&lt;/a>; &lt;br />; &lt;em>;Brandon Y . Feng&lt;/​​em>;、&lt;em>;Hadi Alzayer&lt;/em>;、&lt;strong>;&lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;William T. Freeman&lt;/em>;&lt;/em>; strong>;, &lt;em>;Jia-Bin Huang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06954.pdf&quot;>;全局特征是图像检索所需的全部和重新排名&lt;/a>; &lt;br />; &lt;em>;邵世豪&lt;/em>;、&lt;strong>;&lt;em>;陈凯峰&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Arjun Karpur&lt;/em>;&lt; /strong>;、&lt;em>;崔庆华&lt;/em>;、&lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;曹丙一&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.15827.pdf&quot;>;在基于提示的持续学习中引入语言指导&lt;/a>; &lt;br />; &lt;em>;Muhammad Gul Zain Ali Khan&lt; /em>;、&lt;em>;穆罕默德·费贾德·纳伊姆&lt;/em>;、&lt;em>;吕克·范古尔&lt;/em>;、&lt;em>;迪迪尔·史翠克&lt;/em>;、&lt;strong>;&lt;em>;费德里科·汤巴里&lt;/em>;&lt;/ strong>;, &lt;em>;Muhammad Zeshan Afzal&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01789.pdf&quot;>;用于图像去模糊的多尺度结构引导扩散&lt;/a >; &lt;br />; &lt;em>;任孟伟&lt;/em>;*、&lt;strong>;&lt;em>;毛里西奥·德尔布拉西奥&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;侯赛因·塔莱比&lt;/em>;&lt;/strong>;、 &lt;em>;吉多·格里格&lt;/em>;，&lt;strong>;&lt;em>;佩曼·米兰法&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.09711。 pdf&quot;>;挑战性条件下的鲁棒单目深度估计&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;、&lt;em>;Nils Morbitzer&lt;/em>;、&lt;em>;HyunJun Jung&lt;/em>;、&lt;em>; >;纳西尔·纳瓦布&lt;/em>;，&lt;strong>; &lt;em>;费德里科·汤巴里&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11751.pdf&quot; >;基于分数的扩散模型作为逆向成像的原则先验&lt;/a>; &lt;br />; &lt;em>;Berthy T. Feng&lt;/​​em>;*，&lt;strong>; &lt;em>;Jamie Smith&lt;/em>;&lt;/strong>;， &lt;strong>;&lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;张惠文&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Katherine L. Bouman&lt;/em>;&lt;/ strong>;,&lt;strong>;&lt;em>;威廉·T·弗里曼&lt;/em>;&lt;/strong>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.01858.pdf&quot;>;迈向环球图像嵌入：大规模数据集和通用图像表示的挑战&lt;/a>; &lt;br />; &lt;em>;Nikolaos-Antonios Ypsilantis&lt;/em>;，&lt;strong>;&lt;em>;Kaifeng Chen&lt;/em>;&lt;/strong>; ,&lt;strong>; &lt;em>;曹丙一&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;马里奥·利波夫斯基&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;佩林·多安-舍恩伯格&lt;/em>;&lt; /strong>;、&lt;strong>;&lt;em>;Grzegorz Makosa&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Boris Bluntschli&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Mojtaba Seyedhosseini&lt;/em>; &lt;/strong>;、&lt;em>;Ondrej Chum&lt;/em>;、&lt;strong>;&lt;em>;安德烈·阿劳霍&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2308.06383.pdf&quot;>;U-RED：部分点云的无监督 3D 形状检索和变形&lt;/a>; &lt;br />; &lt;em>;Yan Di&lt;/em>;、&lt;em>;Chenyangguang Zhu&lt;/em>;、 &lt;em>;张瑞达&lt;/em>;、&lt;strong>;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;、&lt;em>;苏永志&lt;/em>;、&lt;em>;Jason Rambach&lt;/em>;、&lt;em>; Didier Stricker&lt;/em>;、&lt;em>;纪向阳&lt;/em>;、&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv .org/pdf/2303.17606.pdf&quot;>;AvatarCraft：通过参数化形状和姿势控制将文本转换为神经人类头像&lt;/a>; &lt;br />; &lt;em>;Ruixiang Jiang&lt;/em>;，&lt;em>;Can Wang&lt;/em>; >;, &lt;em>;张静波&lt;/em>;, &lt;strong>;&lt;em>;柴梦雷&lt;/em>;&lt;/strong>;, &lt;em>;何明明&lt;/em>;, &lt;em>;陈东东&lt;/em>;, &lt; em>;Jing Liao&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14700.pdf&quot;>;通过改进的 AR 模型学习多功能 3D 形状生成&lt;/a>; &lt;br />; &lt;em>;罗思勉&lt;/em>;、&lt;em>;钱学林&lt;/em>;、&lt;em>;付彦伟&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt; em>;泰英&lt;/em>;、&lt;em>;张振宇&lt;/em>;、&lt;em>;王成杰&lt;/em>;、&lt;em>;薛向阳&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://arxiv.org/pdf/2308.11198.pdf&quot;>;稀疏视图中手-物体交互的新颖视图合成和姿势估计&lt;/a>; &lt;br />; &lt;em>;Wentian Qu&lt;/em>;，&lt;em>; >;崔兆鹏&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;em>;孟晨宇&lt;/em>;、&lt;em>;马翠霞&lt;/em>;、&lt;em>;邓晓明&lt;/em>;, &lt;em>;王红安&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05534.pdf&quot;>;PreSTU：场景文本预训练了解&lt;/a>; &lt;br />; &lt;em>;吉亨吉&lt;/em>;*、&lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;陈曦&lt;/em>;&lt; /strong>;,&lt;strong>;&lt;em>;胡鹤翔&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>;塞巴斯蒂安·古德曼&lt;/em>;&lt;/strong>;,&lt;em>;赵伟伦&lt;/em>;, &lt;strong>;&lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;可变形对象的隐式形状表示与密集对应的自监督学习&lt; /a>; &lt;br />; &lt;em>;张宝文&lt;/em>;、&lt;em>;李嘉禾&lt;/em>;、&lt;em>;邓晓明&lt;/em>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt; /strong>;, &lt;em>;马翠霞&lt;/em>;, &lt;em>;王红安&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.06948.pdf&quot;>;自我调节提示：不忘初心的基础模型适应&lt;/a>; &lt;br />; &lt;em>;Muhammad Uzair Khattak&lt;/em>;、&lt;em>;Syed Talal Wasi&lt;/em>;、&lt;em>;Muzammal Nasee&lt;/em>;、 &lt;em>;Salman Kha&lt;/em>;、&lt;strong>;&lt;em>;严明轩&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href= &quot;https://arxiv.org/pdf/2308.11015.pdf&quot;>;Spectral Graphomer：基于频谱图的变压器，使用多视图彩色图像进行自我中心双手重建&lt;/a>; &lt;br />; &lt;em>;Tze Ho Elden谢&lt;/em>;*、&lt;strong>;&lt;em>;Franziska Mueller&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;沉正阳&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;唐丹航&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;塔博·比勒&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;窦明松&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;张银达&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Sasa Petrovic&lt;/em>;&lt;/strong>;、&lt;em>;Hyung Jin Chang&lt;/em>;、&lt;strong>;&lt;em>;乔纳森·泰勒&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Bardia Doosti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12411.pdf&quot;>;综合多样化的人类3D 室内场景中的动作&lt;/a>; &lt;br />; &lt;em>;赵凯峰&lt;/em>;、&lt;em>;张彦&lt;/em>;、&lt;em>;王少飞&lt;/em>;、&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>;，&lt;em>;Siyu Tang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06419.pdf&quot;>;通过 3D 模型进行跟踪视频中未知物体的估计&lt;/a>; &lt;br />; &lt;em>;Denys Rozumnyi&lt;/em>;、&lt;em>;Jiri Matas&lt;/em>;、&lt;em>;Marc Pollefeys&lt;/em>;、&lt;strong>;&lt;em>;维托里奥·法拉利&lt;/em>;&lt;/strong>;、&lt;em>;马丁·R·奥斯瓦尔德&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11062.pdf&quot;>;UnLoc : 视频本地化任务的统一框架&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;沉彦&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;熊学涵&lt;/em>;&lt;/strong>;、 &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;王中浩&lt;/em>;&lt;/strong>; *、&lt;strong>;&lt;em>;葛维娜&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;大卫·罗斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;科迪莉亚·施密德&lt;/em>;&lt;/强>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06708.pdf&quot;>;动词在行动：提高视频语言模型中的动词理解&lt;/a>; &lt;br />; &lt; em>;Liliane Momeni&lt;/em>;、&lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>;、&lt;em>;Andrew Zisserman&lt;/em>; em>;、&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.06703v1.pdf&quot;>;VLSlice：交互式视觉和语言切片发现&lt;/a>; &lt;br />; &lt;em>;Eric Slyman&lt;/em>;、&lt;strong>;&lt;em>;Minsuk Kahng&lt;/em>;&lt;/strong>;、&lt;em>;Stefan Lee&lt;/em>; >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09012.pdf&quot;>;是的，我们可以：基于局部特征的视觉定位的约束近似最近邻&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;德罗尔·艾格&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德烈·阿劳霍&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;西蒙·林宁&lt;/em>;&lt;/强>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05922.pdf&quot;>;视听掩码自动编码器&lt;/a>; &lt;br />; &lt;em>;Mariana-Iuliana Georgescu&lt;/em >;*、&lt;strong>;&lt;em>;爱德华多·丰塞卡&lt;/em>;&lt;/strong>;、&lt;em>;Radu Tudor Ionescu&lt;/em>;、&lt;strong>;&lt;em>;马里奥·卢西奇&lt;/em>;&lt;/strong>;、&lt;strong >;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org /pdf/2307.11386.pdf&quot;>;CLR：用于持续学习的通道轻量级重编程&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge&lt;/em>;、&lt;em>;Yue Cheng Li&lt;/em>;、&lt;em>;Shuo倪&lt;/em>;、&lt;strong>;&lt;em>;赵家平&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Laurent Itti &lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05410.pdf&quot;>;LU-NeRF：通过同步本地未姿势 NeRF 进行场景和姿势估计&lt;/ a>; &lt;br />; &lt;em>;泽州&lt;/em>; &lt;em>;程&lt;/em>;*,&lt;strong>; &lt;em>;卡洛斯&lt;/em>; &lt;em>;埃斯特维斯&lt;/em>;&lt;/strong>;,&lt;strong>; >; &lt;em>;Varun&lt;/em>; &lt;em>;Jampani&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Abhishek&lt;/em>; &lt;em>;Kar&lt;/em>;&lt;/strong>;、&lt;em>;Subhransu &lt;/em>; &lt;em>;Maji&lt;/em>;、&lt;strong>;&lt;em>;Ameesh&lt;/em>; &lt;em>;Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2304.10075.pdf&quot;>;实时抗锯齿神经渲染的多尺度表示&lt;/a>; &lt;br />; &lt;em>;Dongting&lt;/em>; &lt;em>;Hu&lt;/em>;, &lt; em>;振凯&lt;/em>; &lt;em>;张&lt;/em>;、&lt;strong>;&lt;em>;廷波&lt;/em>; &lt;em>;侯&lt;/em>;&lt;/strong>;、&lt;em>;铜梁&lt;/em>; &lt;em >;刘&lt;/em>;、&lt;em>;欢&lt;/em>; &lt;em>;付&lt;/em>;、&lt;em>;明明&lt;/em>; &lt;em>;龚&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2304.10532.pdf&quot;>;Nerfbusters：从随意捕获的 NeRF 中移除幽灵文物&lt;/a>; &lt;br />; &lt;em>;Frederik Warburg&lt;/em>;，&lt;em>;Ethan Weber&lt; /em>;、&lt;em>;马修·坦西克&lt;/em>;、&lt;strong>;&lt;em>;亚历山大·霍林斯基&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;金泽安乔&lt;/em>;&lt;/strong>; &lt;/p >; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05407.pdf&quot;>;在没有先验知识的情况下分割已知物体和看不见的未知物体&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>; 、&lt;em>;阿尔瓦罗·马科斯-拉米罗&lt;/em>;、&lt;em>;迈克尔·施密特&lt;/em>;、&lt;em>;纳西尔·纳瓦布&lt;/em>;、&lt;em>;本杰明·布萨姆&lt;/em>;、&lt;strong>;&lt;em>;费德里科Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14340.pdf&quot;>;SparseFusion：融合多传感器 3D 对象的多模态稀疏表示检测&lt;/a>; &lt;br />; &lt;em>;谢一辰&lt;/em>;、&lt;em>;徐晨风&lt;/em>;、&lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>;、&lt;em>; >;Patrick Rim&lt;/em>;、&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;、&lt;em>;Kurt Keutzer&lt;/em>;、&lt;em>;Masayoshi Tomizuka&lt;/em>;、&lt;em>;魏展&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15446.pdf&quot;>;SwiftFormer：基于 Transformer 的实时移动视觉应用的高效附加注意力&lt;/a>; &lt;br />; &lt;em>;Abdelrahman Shaker&lt;/em>;、&lt;em>;Muhammad Maa&lt;/em>;、&lt;em>;Hanoona Rashee&lt;/em>;、&lt;em>;Salman Kha&lt;/em>;、&lt;strong>;&lt;em>; Ming-Hsuan Yan&lt;/em>;&lt;/strong>;、&lt;em>;Fahad Shahbaz Kha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12948.pdf&quot;>;敏捷建模：几分钟内从概念到分类器&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Otilia Stretcu&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Edward Vendrow&lt;/em>;&lt;/strong>; 、&lt;strong>; &lt;em>;畑健二&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;克里希那穆西·维斯瓦纳坦&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;维托里奥·法拉利、&lt;/em>;&lt;/ strong>; &lt;strong>;&lt;em>;Sasan Tavakkol&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;周文磊&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Aditya Avinash&lt;/em>;&lt;/ strong>;、&lt;strong>;&lt;em>;罗恩明&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;尼尔·戈登·奥尔德林&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;MohammadHossein Bateni&lt;/em>; &lt;/strong>;、&lt;strong>; &lt;em>;加布里埃尔·伯杰&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;安德鲁·邦纳&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;卢俊达&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;哈维尔·雷伊&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;朱利亚·德萨尔沃&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;兰杰克里希纳&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;阿里尔·福克斯曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09011。 pdf&quot;>;CAD-Estate：RGB 视频中的大型 CAD 模型注释&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kevis-Kokitsi Maninis&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;Stefan波波夫&lt;/em>;&lt;/strong>;、&lt;em>;马蒂亚斯·尼斯纳&lt;/em>;、&lt;strong>;&lt;em>;维托里奥·法拉利&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //arxiv.org/pdf/2306.01209.pdf&quot;>;恶劣天气下的人群计数&lt;/a>; &lt;br />; &lt;em>;黄志凯&lt;/em>;，&lt;em>;陈伟霆&lt;/em>;， &lt;em>;蒋元春&lt;/em>;、&lt;em>;郭施彦&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://arxiv.org/pdf/2304.06025.pdf&quot;>;DreamPose：稳定扩散的时尚视频合成&lt;/a>; &lt;br />; &lt;em>;Johanna Karras&lt;/em>;，&lt;strong>;&lt;em >;Aleksander Holynski&lt;/em>;&lt;/strong>;、&lt;em>;王庭春&lt;/em>;、&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https:/ /arxiv.org/pdf/2301.09637.pdf&quot;>;InfiniCity：无限规模城市综合&lt;/a>; &lt;br />; &lt;em>;Chieh Hubert Lin&lt;/em>;、&lt;em>;Hsin-Ying Lee&lt;/em>;、 &lt;em>;Willi Menapace&lt;/em>;、&lt;em>;Menglei Chai&lt;/em>;、&lt;em>;Aliaksandr Siarohin&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt; em>;Sergey Tulyakov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://faculty.ucmerced.edu/mhyang/papers/iccv2023_sampling.pdf&quot;>;采样：小说的场景自适应分层多平面图像表示查看单幅图像的合成&lt;/a>; &lt;br />; &lt;em>;周小宇&lt;/em>;、&lt;em>;林志伟&lt;/em>;、&lt;em>;单晓军&lt;/em>;、&lt;em>;王永涛&lt; /em>;, &lt;strong>;&lt;em>;孙德庆&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br / >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;教程&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href =&quot;http://sifeiliu.net/NDLM_ICCV23tutorial/&quot;>;利用噪声和无标签数据学习无法分类的大型模型&lt;/a>; &lt;br />; &lt;em>;Sifei Liu&lt;/em>;、&lt;em>;尹红旭&lt;/ em>;、&lt;em>;Shalini De Mello&lt;/em>;、&lt;em>;Pavlo Molchanov&lt;/em>;、&lt;em>;Jose M. Alvarez&lt;/em>;、&lt;em>;Jan Kautz&lt;/em>;、&lt;em>;小龙&lt;/em>; &lt;em>;王&lt;/em>;、&lt;em>;Anima Anandkumar&lt;/em>;、&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>;、&lt;em>;特雷弗·达雷尔&lt;/em>; >; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;研讨会&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot; >;人工智能中的LatinX&lt;/a>; &lt;br />; 白金赞助商&lt;br />; 小组成员：&lt;strong>;&lt;em>;Daniel Castro Chin&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Andre Araujo&lt;/em>; &lt;/strong>; &lt;br />; 特邀演讲嘉宾：&lt;strong>; &lt;em>;Irfan Essa&lt;/em>;&lt;/strong>; &lt;br />; 志愿者：&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong >;、&lt;strong>;&lt;em>;袁良哲&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Pedro Velez&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Vincent Etter&lt;/em>;&lt;/strong>; strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sg2rl.github.io/index.html&quot;>;场景图和图表示学习&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em >;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://web.northeastern.edu/smilelab/amfg2023/&quot;>;面部和手势分析与建模国际研讨会&lt; /a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Todd Zickler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dv-in-ecommerce.github.io /&quot;>;电子商务中的 3D 视觉和建模挑战&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https: //bigmac-vision.github.io/&quot;>;BigMAC：计算机视觉的大模型适应&lt;/a>; &lt;br />;组织者：&lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://iccv23-arow.github.io/&quot;>;现实世界中的对抗鲁棒性（AROW）&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;白宇桐&lt; /em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://geonet-challenge.github.io/ICCV2023/&quot;>;GeoNet：第一届跨地域鲁棒计算机视觉研讨会&lt;/a>; &lt; br />; 演讲者：&lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Tarun Kalluri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gkioxari.github.io/Tutorials/iccv2023/&quot;>;Quo Vadis，计算机视觉？ &lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;比尔·弗里曼&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/ view/vshh/home&quot;>;去 NeRF 还是不去 NeRF：人类大脑的视图合成挑战&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>; &lt;br / >; 组织者：&lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/nivt-iccv2023/&quot; >;视觉变形金刚的新想法&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />;组织者：&lt;strong>;&lt;em>;杨明轩&lt;/em>;&lt;/strong>; em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://lsfsl.net/limit23/&quot;>;非常有限的图像表示学习：自我、合成和公式监督的潜力&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Manel Baradad Jurjo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/rcv2023 /&quot;>;面向计算机视觉的资源高效深度学习&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;br />;组织者：&lt;strong>;&lt;em>;余家辉&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://cvaad-workshop.github.io/&quot;>;计算机视觉辅助建筑设计&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt; /p>; &lt;p>; &lt;a href=&quot;https://av4d.org/&quot;>;AV4D：空间声音的视觉学习&lt;/a>; &lt;br />;组织者：&lt;strong>;&lt;em>;David Harwath&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://wvlar.github.io/iccv23/&quot;>;视觉与语言算法推理&lt;/a>; &lt;br />; 演讲者：&lt;strong >;&lt;em>;François Chollet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;自动驾驶和机器人技术的神经场&lt;/a>; &lt;br />;演讲者： &lt;strong>;&lt;em>;乔恩·巴伦&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://campworkshop.org/&quot;>;构图和多模态感知国际挑战&lt;/a>; &lt; br />; 组织者：&lt;strong>;&lt;em>;Ranjay Krishna&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opensun3d.github.io/&quot;>;开放词汇 3D 场景理解 (OpenSUN3D)&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Francis Engelmann&lt;/em>;&lt;/强>;，&lt;强>;&lt;em>;约翰娜·沃尔德&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;费德里科·托姆巴里&lt;/em>;&lt;/strong>;，&lt;strong>;&lt;em>;列奥尼达斯·吉巴斯&lt;/em>;&lt; /strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mono3d-iccv-workshop&quot;>;单目 3D 感知前沿：几何基础模型&lt;/a>; &lt; br />; 演讲者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/perdream/home &quot;>;PerDream：通过多模式基础建模进行感知、决策和推理&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://cmp.felk.cvut.cz/sixd/workshop_2023/&quot;>;恢复 6D 物体姿势&lt;/a>; &lt;br />;演讲者：&lt;strong>;Fabian Manhardt&lt;/strong>;、&lt;strong>; Martin Sundermeyer&lt; /strong>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Martin Sundermeyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view /wicviccv2023/&quot;>;计算机视觉领域的女性 (WiCV)&lt;/a>; &lt;br />; 小组成员：&lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot; https://languagefor3dscenes.github.io/ICCV2023/&quot;>;3D 场景语言&lt;/a>; &lt;br />; 组织者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p >; &lt;a href=&quot;https://ai3dcc.github.io/&quot;>;人工智能用于 3D 内容创作&lt;/a>; &lt;br />; 演讲者：&lt;strong>;&lt;em>;Kai-Hung Chang&lt;/em>;&lt;/strong >; &lt;br />; 组织者：&lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cv4metaverse /&quot;>;Metaverse 的计算机视觉&lt;/a>; &lt;br />;演讲者：&lt;strong>;&lt;em>;Jon Barron&lt;/em>;&lt;/strong>;、&lt;strong>; &lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.datacomp.ai/workshop.html#first&quot;>;迈向下一代计算机视觉数据集&lt;/a>; &lt;br />; 演讲者：&lt;strong>; &lt;em>;Tom Duerig&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google 研究展位活动&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; 标题：即时调整：即时个性化图像到图像生成&lt;br />; 演讲者：&lt;strong>;&lt;em>;Xuhui Jia&lt;/em >;&lt;/strong>;, &lt;strong>;&lt;em>;Suraj Kothawade&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 4 日星期三中午 12:30 &lt;/p>; &lt;p>; 标题：Open Images V7 (&lt; a href=&quot;https://storage.googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf&quot;>;论文&lt;/a>;，&lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html html&quot;>;数据集&lt;/a>;，&lt;a href=&quot;https://blog.research.google/2022/10/open-images-v7-now-featuring-point.html&quot;>;博客文章&lt;/a>;） &lt;br />; 演讲者：&lt;strong>;&lt;em>;Rodrigo Benenson&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jasper Uijlings&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Jordi Pont-Tuset &lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 4 日星期三下午 3:30 &lt;/p>; &lt;p>; 标题：Pixel 8 Pro 上的新魔法橡皮擦&lt;br />; 演示者：&lt;strong>;&lt;em>;史蒂文·希克森&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;佩德罗·贝莱斯&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;阿尔伯特·肖&lt;/em>;&lt;/strong>;&lt;br />;周四，欧洲中部夏令时间 10 月 5 日上午 10:30 &lt;/p>; &lt;p>; 标题：前言：用于少量镜头超高分辨率人脸合成的数据驱动体积先验&lt;br />; 演讲者：&lt;strong>;&lt;em>;Marcel Bühler&lt; /em>;&lt;/strong>;、&lt;strong>;&lt;em>;Kripasindhu Sarkar&lt;/em>;&lt;/strong>; &lt;br />; 欧洲夏令时间 10 月 5 日星期四中午 12:30 &lt;/p>; &lt;p>; 标题：哎哟！合成和合成图像的视觉和语言基准 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 5 日星期四下午 1:00 &lt;/ p>; &lt;p>; 标题：Fact Check Explorer 中的图像搜索（&lt;a href=&quot;https://blog.google/products/news/new-features-coming-to-fact-check-explorer/&quot;>;博客文章&lt; /a>;) &lt;br />; 演讲者：&lt;strong>;&lt;em>;Yair Alon&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Avneesh Sud&lt;/em>;&lt;/strong>; &lt;br />; 十月星期四中欧夏令时间 5 日下午 3:30 &lt;/p>; &lt;p>; 标题：逆问题的即时调整潜在扩散模型 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Hyungjin Chung&lt;/em>;&lt;/strong>; &lt;br / >; 欧洲中部夏令时间 10 月 6 日星期五中午 12:30 &lt;/p>; &lt;p>; 标题：现实世界应用的神经隐式表示 &lt;br />; 演讲者：&lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;、&lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>; &lt;br />; 欧洲中部夏令时间 10 月 6 日星期五下午 3:30 &lt; /p>; &lt;/div>; &lt;!--脚注-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size:small;&quot;>; &lt;b>;*&lt;/b>;&amp;nbsp;在 Google 期间完成的工作&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7111364624265266582/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-at-iccv-2023.html #comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7111364624265266582&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7111364624265266582&quot; rel=&quot;self&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-at-iccv-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICCV 2023&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXl r07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s72 -c/ICCV%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3765645246777916540&lt;/id>;&lt;已发布>;2023-09-28T13:01:00.000-07:00&lt;/已发布>; &lt;更新>;2023-09-28T13:01:53.849-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算摄影&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns #&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DynIBaR：动态场景视频的时空视图合成&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline -author&quot;>;发布者：Google 研究中心研究科学家 Zengqi Li 和 Noah Snavely &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlV rxJlyoEYl7-tAer6v4MnIAw49TWLGWa8cFgl_c_2WUJqw1O3J8nVhU- VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s320/hero.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 手机相机是捕捉日常生活瞬间的强大工具。然而，使用单个相机捕​​捉动态场景从根本上是有限的。例如，如果我们想要调整摄像机运动或录制视频的时间（例如，在扫动摄像机以突出戏剧性时刻时冻结时间），我们通常需要带有同步摄像机装备的昂贵的好莱坞设置。在没有好莱坞预算的情况下，仅通过手机摄像头拍摄的视频是否有可能实现类似的效果？ &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2211.11082&quot;>;DynIBaR：基于神经动态图像的渲染&lt;/a”中>;”，&lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Awards&quot;>;最佳论文荣誉奖&lt;/a>;，位于 &lt;a href=&quot;https://cvpr2023.thecvf.com/ Conferences/2023&quot;>;CVPR 2023&lt;/a>;，我们描述了一种新方法，可以从复杂动态场景的单个视频生成逼真的自由视点渲染。基于神经动态图像的渲染 (DynIBaR) 可用于生成一系列视频效果，例如“&lt;a href=&quot;https://en.wikipedia.org/wiki/Bullet_time&quot;>;子弹时间&lt;/a>;”使用手机相机拍摄的单个视频的效果（时间暂停并且相机以正常速度围绕场景移动）、视频稳定、景深和慢动作。我们证明，DynIBaR 显着改进了复杂移动场景的视频渲染，为新型视频编辑应用打开了大门。我们还在 DynIBaR &lt;a href=&quot;https://dynibar.github.io/&quot;>;项目页面上发布了&lt;a href=&quot;https://github.com/google/dynibar&quot;>;代码&lt;/a>; &lt;/a>;，所以你可以自己尝试一下。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于复杂动态场景的野外视频，DynIBaR 可以冻结时间，同时允许摄像机继续在场景中自由移动。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt; /table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;背景&lt;/h2>; &lt;p>; 过去几年，计算机取得了巨大进步使用&lt;a href=&quot;https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot;>;神经辐射场&lt;/a>; (NeRF) 进行重建和渲染的视觉技术静态（非移动）3D 场景。然而，人们用移动设备拍摄的大多数视频都描绘了移动的物体，例如人、宠物和汽车。这些移动场景导致了更具挑战性的 4D（3D + 时间）场景重建问题，无法使用标准视图合成方法来解决。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;标准视图合成方法在应用于动态场景的视频时会输出模糊、不准确的渲染。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 其他最近处理视图的方法使用时空神经辐射场合成动态场景（即&lt;em>;&lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;动态 NeRF&lt;/a>;&lt;/em >;），但这些方法仍然表现出固有的局限性，阻碍了它们应用于随意捕捉的野外视频。特别是，他们很难从具有长时间持续时间、不受控制的摄像机路径和复杂物体运动的视频中呈现高质量的新颖视图。 &lt;/p>; &lt;p>; 关键的缺陷是它们将复杂的移动场景存储在单个数据结构中。特别是，它们以多层感知器（MLP）神经网络的权重对场景进行编码。 MLP 可以近似任何函数 - 在本例中，是映射 4D 时空点的函数（&lt;em>;x&lt;/em>;、&lt;em>;y&lt;/em>;、&lt;em>;z&lt;/em>;、&lt;em>; >;t&lt;/em>;）转换为我们可以在渲染场景图像时使用的 RGB 颜色和密度。然而，该 MLP 的容量（由其神经网络中的参数数量定义）必须根据视频长度和场景复杂性而增加，因此，在野外视频上训练此类模型在计算上可能很困难。结果，我们得到了模糊、不准确的渲染，就像 &lt;a href=&quot;https://free-view-video.github.io/&quot;>;DVS&lt;/a>; 和 &lt;a href=&quot;https:// 生成的渲染一样。 www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>;（如下所示）。 DynIBaR 通过采用不同的渲染范例来避免创建如此大的场景模型。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-0.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot; tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于复杂动态场景的视频，与之前的动态视图合成方法（&lt;strong>;顶行&lt;/strong>;）相比，DynIBaR（&lt;strong>;底行&lt;/strong>;）显着提高了渲染质量。先前的方法会产生模糊的渲染，因为它们需要将整个移动场景存储在 MLP 数据结构中。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;基于图像的渲染 (IBR)&lt;/h2>; &lt;p>; DynIBaR 背后的一个关键见解是，我们实际上不需要将所有场景内容存储在巨型 MLP 中的视频。相反，我们直接使用附近输入视频帧的像素数据来渲染新视图。 DynIBaR 构建于名为 &lt;a href= 的&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Image-based_modeling_and_rendering&quot;>;基于图像的渲染&lt;/a>;&lt;/em>; (IBR) 方法之上“https://ibrnet.github.io/&quot;>;IBRNet&lt;/a>; 专为静态场景的视图合成而设计。 IBR 方法认识到场景的新目标视图应该与附近的源图像非常相似，因此通过从附近的源帧动态选择和扭曲像素来合成目标，而不是提前重建整个场景。特别是，IBRNet 学习将附近的图像混合在一起，以在体积渲染框架内重新创建场景的新视图。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DynIBaR：将 IBR 扩展到复杂的动态视频&lt;/h2>; &lt;p>; 将 IBR 扩展到动态视频场景中，我们需要在渲染过程中考虑场景运动。因此，作为重建输入视频的一部分，我们求解每个 3D 点的运动，其中我们使用 MLP 编码的运动轨迹场来表示场景运动。与之前在 MLP 中存储整个场景外观和几何形状的动态 NeRF 方法不同，我们只存储运动（一种更平滑和稀疏的信号），并使用输入视频帧来确定渲染新视图所需的所有其他内容。 &lt;/p>; &lt;p>; 我们通过获取每个输入视频帧、使用体积渲染渲染光线以形成 2D 图像来优化给定视频的 DynIBaR（如 &lt;a href=&quot;https://ai.googleblog.com/2023 /06/reconstructing-indoor-spaces-with-nerf.html&quot;>;NeRF&lt;/a>;），并将渲染图像与输入帧进行比较。也就是说，我们的优化表示应该能够完美地重建输入视频。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW 6JJk1ghWO2oma_XWYD2FcKeylPDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/s660/image3.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;660&quot; height=&quot;465&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW6JJk1ghWO2oma_XWYD2FcKeyl PDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/w640-h465/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们说明了 DynIBaR 如何渲染动态场景的图像。为了简单起见，我们展示了一个 2D 世界，如从上面看到的。 (&lt;strong>;a&lt;/strong>;) 一组输入源视图（&lt;strong>;三角形&lt;/strong>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Viewing_frustum&quot;>;相机 frusta&lt;/a >;) 观察一个立方体在场景中移动（&lt;strong>;动画方块&lt;/strong>;）。每个摄像机都标有其时间戳（&lt;em>;t&lt;/em>;-2、&lt;em>;t&lt;/em>;-1 等）。 (&lt;strong>;b&lt;/strong>;) 为了渲染时间 &lt;em>;t&lt;/em>; 时相机的视图，DynIBaR 会通过每个像素（&lt;strong>;蓝线&lt;/strong>;）发射虚拟光线，并计算颜色以及沿该射线的样本点的不透明度。为了计算这些属性，DyniBaR 通过多视图几何将这些样本投影到其他视图中，但首先，我们必须补偿每个点的估计运动（&lt;strong>;红虚线&lt;/strong>;）。 (&lt;strong>;c&lt;/strong>;) 使用此估计运动，DynIBaR 将 3D 中的每个点移动到相关时间，然后将其投影到相应的源相机中，以对用于渲染的颜色进行采样。 DynIBaR 优化每个场景点的运动，作为学习如何合成场景新视图的一部分。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 但是，重建和派生新视图复杂、移动场景的视图是一个非常不适定的问题，因为有许多解决方案可以解释输入视频 - 例如，它可能会为每个时间步创建断开连接的 3D 表示。因此，仅优化 DynIBaR 来重建输入视频是不够的。为了获得高质量的结果，我们还引入了其他几种技术，包括一种称为跨时间渲染的方法。跨时间渲染是指使用 4D 表示在某一时刻的状态来渲染不同时刻的图像，这有助于 4D 表示随着时间的推移保持连贯。为了进一步提高渲染保真度，我们自动将场景分解为两个组件，静态组件和动态组件，分别通过时不变和时变场景表示进行建模。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;创建视频效果&lt;/h2>; &lt;p>; DynIBaR 支持各种视频效果。我们在下面展示了几个例子。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;视频稳定&lt;/h3>; &lt;p>; 我们使用摇晃的手持输入视频来比较 DynIBaR 的视频稳定性能优于现有 2D 视频稳定和动态 NeRF 方法，包括 &lt;a href=&quot;https://alex04072000.github.io/FuSta/&quot;>;FuSta&lt;/a>;、&lt;a href=&quot;https://dl.acm .org/doi/abs/10.1145/3363550&quot;>;DIFRINT&lt;/a>;、&lt;a href=&quot;https://hypernerf.github.io/&quot;>;HyperNeRF&lt;/a>; 和 &lt;a href=&quot;https:// /www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>;。我们证明 DynIBaR 可以产生更平滑的输出，具有更高的渲染保真度和更少的伪影（例如，闪烁或模糊的结果）。特别是，FuSta 会产生残留的相机抖动，DIFRINT 会在对象边界周围产生闪烁，而 HyperNeRF 和 NSFF 会产生模糊的结果。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-4.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h3>;同时视图合成和慢动作&lt;/h3>; &lt;p>; DynIBaR 可以同时在空间和时间上进行视图合成，产生流畅的 3D 电影效果。下面，我们演示了 DynIBaR 可以接受视频输入并生成使用新颖的相机路径渲染的平滑的 5 倍慢动作视频。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h3>;视频散景&lt;/h3>; &lt;p>; DynIBaR 还可以通过合成视频生成高质量的&lt;a href=&quot;https://en.wikipedia.org/wiki/Bokeh&quot;>;视频散景&lt;/a>;动态变化的&lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_of_field&quot;>;景深&lt;/a>;。给定全焦点输入视频，DynIBar 可以生成具有不同离焦区域的高质量输出视频，这些区域引起人们对移动（例如，奔跑的人和狗）和静态内容（例如，树木和建筑物）的注意场景中。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot;loop=&quot;&quot;muted=&quot;&quot;playsinline=&quot;&quot;width=&quot;100%&quot;>;&lt;source src=&quot;https://dynibar.github.io/static/视频/blog/blog-1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;结论&lt;/h2>; &lt;p>; DynIBaR 是我们从新摄像机路径渲染复杂移动场景的能力的一次飞跃。虽然它目前涉及每个视频的优化，但我们设想可以在野外视频上部署更快的版本，以便为使用移动设备的消费者视频编辑提供新的效果。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;DynIBaR 是以下研究人员合作的成果：谷歌研究中心和康奈尔大学。本文中介绍的工作的主要贡献者包括Zhengqi Li、Qianqian Wang、Forrester Cole、Richard Tucker 和Noah Snavely。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog. Research.google/feeds/3765645246777916540/comments/default&quot; rel=&quot;replies&quot; title=&quot;帖子评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023 /09/dynibar-space-time-view-synthesis-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http:// www.blogger.com/feeds/8474926331452026626/posts/default/3765645246777916540&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/3765645246777916540&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/dynibar-space-time-view- Synthesis-from.html&quot; rel=&quot;alternate&quot; title=&quot;DynIBaR：动态场景视频的时空视图合成&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri >;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google. com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:缩略图高度=“72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGWa8 cfgl_c_2WUJqw1O3J8nVhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns: media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com ,1999:blog-8474926331452026626.post-2311847718846675808&lt;/id>;&lt;发布>;2023-09-28T11:16:00.002-07:00&lt;/发布>;&lt;更新>;2023-09-28T11:22:44.163-07 :00 &lt;/更新>; &lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“深度学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/” atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type= &quot;text&quot;>;通过分布鲁棒优化重新加权梯度下降&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Ramnath Kumar，博士前研究员和 Arun Sai Suggala，研究科学家, Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR59ivF_uDCUUmbdijaAuiK 3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s320/hero%20RGD.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>;深度神经网络 (DNN) 已成为解决标准监督学习 (&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens- to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;使用 ViT 进行图像分类&lt;/a>;）到&lt;a href=&quot;https://arxiv.org/pdf/2201.11775.pdf&quot;>;元学习&lt;/a>;。学习 DNN 最常用的范例是&lt;em>; &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;经验风险最小化&lt;/a>; (ERM ）&lt;/em>;，旨在确定一个能够最小化平均&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss训练数据点的“损失”。多种算法，包括&lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;随机梯度下降&lt;/a>; (SGD)、&lt;a href=&quot;https://arxiv .org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>; 和 &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;>;Adagrad&lt;/a>;，有被提出来解决ERM。然而，ERM的一个缺点是它对所有样本进行平均加权，往往会忽略稀有和较困难的样本，而关注较容易和丰富的样本。这会导致在未见过的数据上表现不佳，尤其是在训练数据稀缺的情况下。&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了克服这一挑战，最近的工作开发了数据重新加权技术提高 ERM 绩效。然而，这些方法侧重于特定的学习任务（例如&lt;a href=&quot;https://arxiv.org/pdf/1708.02002.pdf&quot;>;分类&lt;/a>;）和/或需要学习&lt;a href=&quot;https ://arxiv.org/pdf/1902.07379.pdf&quot;>;额外的元模型&lt;/a>;，用于预测每个数据点的权重。额外模型的存在显着增加了训练的复杂性，并使它们在实践中变得笨拙。&lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2306.09222&quot;>;随机重新加权中通过分布稳健优化进行梯度下降&lt;/a>;”，我们引入了经典&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#&quot;>;SGD算法&lt;/a>;的一种变体，用于重新加权数据每个优化步骤中根据其难度进行评分。随机重加权梯度下降（RGD）是一种轻量级算法，具有简单的封闭式表达式，只需两行代码即可应用于解决任何学习任务。在学习过程的任何阶段，RGD 只是将数据点重新加权为其损失的指数。我们凭经验证明，RGD 重新加权算法提高了从监督学习到元学习等各种任务中众多学习算法的性能。值得注意的是，我们在 &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>; 和 &lt;a href=&quot;https://arxiv. org/abs/2206.08564&quot;>;表格分类&lt;/a>;。此外，RGD 算法还使用 &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>; 提高了 &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; 的性能GLUE&lt;/a>; 基准测试和&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;ImageNet-1K 上的 ViT&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分布式稳健优化&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv. org/pdf/1610.03425.pdf&quot;>;分布式鲁棒优化&lt;/a>; (DRO) 是一种假设可能发生“最坏情况”数据分布变化的方法，这可能会损害模型的性能。如果模型专注于识别少量虚假特征进行预测，这些“最坏情况”数据分布变化可能会导致样本错误分类，从而导致性能下降。 DRO 优化了“最坏情况”分布中样本的损失，使模型能够抵抗数据分布中的扰动（例如，从数据集中删除一小部分点、数据点的较小上/下加权等） 。在分类的背景下，这迫使模型减少对噪声特征的重视，而更多地重视有用和预测的特征。因此，使用 DRO 优化的模型往往在未见过的样本上具有&lt;a href=&quot;https://arxiv.org/pdf/1610.03425.pdf&quot;>;更好的泛化保证和更强的性能&lt;/a>;。 &lt;/p>; &lt;p>; 受这些结果的启发，我们开发了 RGD 算法作为解决 DRO 目标的技术。具体来说，我们专注于基于 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;Kullback–Leibler divergence&lt;/a>; 的 DRO，其中添加扰动来创建分布接近 KL 散度度量中的原始数据分布，使模型能够在所有可能的扰动下表现良好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJ BB5nJdefCX0idWac3WKaJgb9UfvljskRgcYPrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s1999/image7.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1236&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJBB5nJdefCX0idWac3WKaJgb9UfvljskRgcY PrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;图示 DRO 的图。与 &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;ERM&lt;/a>; 相比，ERM 学习一个最小化原始数据分布的预期损失的模型， DRO 学习的模型在原始数据分布的多个扰动版本上表现良好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;随机重新加权梯度下降&lt;/h2>; &lt;p>; 考虑样本的随机子集（称为小批量），其中每个数据点都有一个关联的&lt;a href= “https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;损失&lt;/a>; &lt;i>;L&lt;sub>;i&lt;/sub>;&lt; /i>;.像 SGD 这样的传统算法对小批量中的所有样本给予同等的重视，并通过沿着这些样本损失的平均梯度下降来更新模型的参数。通过 RGD，我们重新加权小批量中的每个样本，并更加重视模型识别为更困难的点。准确地说，我们使用损失作为代理来计算一个点的难度，并通过其损失的指数重新加权。最后，我们通过沿着样本梯度的加权平均值下降来更新模型参数。 &lt;/p>; &lt;p>; 出于稳定性考虑，在我们的实验中，我们在计算损失指数之前对损失进行裁剪和缩放。具体来说，我们将损失限制在某个阈值&lt;em>;T&lt;/em>;，并将其乘以与阈值成反比的标量。 RGD 的一个重要方面是它的简单性，因为它不依赖元模型来计算数据点的权重。此外，它可以用两行代码来实现，并与任何流行的优化器结合使用（例如&lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;SGD&lt;/a >;、&lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;亚当&lt;/a>;和&lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/ duchi11a.pdf&quot;>;Adagrad&lt;/a>;。&lt;/p>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellpacing=&quot;0&quot;class=&quot;tr-caption-container&quot;style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH- o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXxfdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobe DrmSSh1sb/s1973/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1541&quot; 数据-original-width=&quot;1973&quot; height=&quot;500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH-o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXx fdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobeDrmSSh1sb/w640-h500/image5。 png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;图解了背后的直观想法二元分类设置中的 RGD。特征 1 和特征 2 是模型可用于预测数据点标签的特征。 RGD 增加了模型错误分类的高损失数据点的权重。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们提出了将 RGD 与标准监督学习和领域适应的最先进技术进行比较的实证结果（请参阅 &lt;a href=&quot; https://arxiv.org/abs/2306.09222&quot;>;元学习结果论文&lt;/a>;）。在我们所有的实验中，我们使用保留的验证集来调整优化器的裁剪级别和学习率。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;监督学习&lt;/h3>; &lt;p>; 我们在多个监督学习任务上评估 RGD，包括语言、视觉和表格分类。对于语言分类任务，我们将 RGD 应用于在 &lt;a href=&quot;https://gluebenchmark 上训练的 &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; 模型。 com/leaderboard&quot;>;通用语言理解评估&lt;/a>; (&lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;GLUE&lt;/a>;) 基准，表明 RGD 比 BERT 基准高出 +1.94%标准偏差为 0.42%。为了评估 RGD 在视觉分类上的性能，我们将 RGD 应用于在 ImageNet-1K 数据集上训练的 ViT-S 模型，结果表明 RGD 的性能优于 ViT-S 基线 +1.01%，标准差为 0.23%。此外，我们还执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;假设检验&lt;/a>;，以确认这些结果具有统计显着性，p 值小于 0.05。&lt; /p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto ;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgp COgzhBRzNLyV0OkHlZskXpqOOmCV-xZyZ6NQpuC85jen-YxDfhF-usexsm -lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1334&quot; data-original-width=&quot;1999 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgpCOgzhBRzNLyV0OkHlZskXpqOOmCV -xZyZ6NQpuC85jen-YxDfhF-usexsm-lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s16000/image8.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RGD 使用 GLUE 和 Imagenet-1K 基准测试在语言和视觉分类方面的性能。请注意，MNLI、QQP、QNLI、SST-2、MRPC、RTE 和 COLA 是构成 GLUE 基准的不同数据集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 对于表格分类，我们使用&lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>;作为我们的基线，并考虑来自&lt;a href=&quot;https://archive.ics的各种二进制和多类数据集.uci.edu/&quot;>;加州大学欧文分校的机器学习存储库&lt;/a>;。我们表明，将 RGD 应用于 &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>; 框架，其在二元和多类表格分类上的性能分别提高了 1.51% 和 1.27% ，在该领域实现了最先进的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOO p_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK44LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s1999/image4.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOOp_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK4 4LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTY PLLLFGO0R6XzCITFBUVJFsMXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s1999/image6.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTYPRLLFGO0R6XzCITFBUVJFs MXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RGD 对各种表格数据集分类的性能。&lt;/td>;&lt;/ tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;领域泛化&lt;/h3>; &lt;p>; 评估RGD的泛化能力，我们使用标准的&lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>;基准，该基准通常用于研究模型的域外性能。我们应用RGD 到 &lt;a href=&quot;https://arxiv.org/abs/2210.01360&quot;>;FRR&lt;/a>;，这是一种改进域外基准的最新方法，并表明 RGD 与 FRR 的平均性能为 0.7%优于 FRR 基线。此外，我们通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;假设检验&lt;/a>;确认大多数基准测试结果（Office Home 除外）具有统计显着性p 值小于 0.05。&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLr L9Rrx3sqx8CIME00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4Ty3RRHX6IW4BcpQRQhbilp1- nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s1999/image3.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLrL9Rrx3sqx8CIME00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4T y3RRHX6IW4BcpQRQhbilp1-nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;RGD 在 DomainBed 分布偏移基准上的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;类别不平衡和公平性&lt;/h3>; &lt;p>; 为了证明使用 RGD 学习的模型在存在类别不平衡（数据集中某些类别代表性不足）的情况下仍表现良好，我们将 RGD 与 ERM 在 &lt;a href= 上的性能进行比较“https://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html&quot;>;长尾 CIFAR-10&lt;/a>;。我们报告，RGD 将基线 ERM 的准确性平均提高了 2.55%标准偏差为 0.23%。此外，我们还执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;假设检验&lt;/a>;并确认这些结果具有统计显着性，p 值小于 0.05。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pF KynE_4kcU1u3ERg_s35uKaRvc2PqvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1356&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pFKynE_4kcU1u3ERg_s35uKaRvc2P qvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;RGD 在类别不平衡领域的长尾 Cifar-10 基准上的性能。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;限制&lt;/h2>; &lt;p>; RGD 算法是使用流行的研究数据集开发的，这些数据集已经被精心设计以消除损坏（例如，噪声和不正确的标签）。因此，在训练数据有大量损坏的情况下，RGD 可能无法提供性能改进。处理此类场景的一种可能方法是将&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/375663.375668&quot;>;异常值去除技术&lt;/a>;应用于 RGD 算法。这种异常值去除技术应该能够从小批量中过滤掉异常值，并将剩余的点发送到我们的算法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; RGD 已被证明在各种任务上都有效，包括域外泛化、表格表示学习和类不平衡。它实现简单，只需更改两行代码即可无缝集成到现有算法中。总的来说，RGD 是一种很有前途的提高 DNN 性能的技术，并且可以帮助突破各个领域的界限。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这篇博文中描述的论文是由Ramnath Kumar、Arun Sai Suggala、Dheeraj Nagaraj 和 Kushal Majmundar。我们衷心感谢匿名审稿人 Prateek Jain、Pradeep Shenoy、Anshul Nasery、Lovish Madaan 以及 Google 印度研究院机器学习和优化团队的众多敬业成员，感谢他们对这项工作的宝贵反馈和贡献。&lt;/ em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2311847718846675808/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论“ type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html#comment-form&quot; rel=&quot;回复&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;edit&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt; link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html&quot; rel=&quot;alternate&quot; title=&quot;通过分布鲁棒优化重新加权梯度下降&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR5 9ivF_uDCUUmbdijaAuiK3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s72-c/ Hero%20RGD.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/条目>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-109886617977245321&lt;/id>;&lt;已发布>;2023-09-26T07:10:00.000-07:00&lt;/已发布>;&lt;已更新>; 2023-09-26T07:10:39.655-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=&quot;神经网络&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 研究人员着手绘制小鼠大脑图&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Research 研究科学家 Michał Januszewski&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV _ccWS2g1Qcjssq2HmmGCqbaL_ij-UroaO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s320/connectome%20600x580.gif&quot;样式=“显示：无；” />; &lt;p>; 人脑可能是现存计算最复杂的机器，由 &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_brain#:~:text=brainstem.%5B37%5D -,显微解剖学,-%5Bedit%5D&quot;>;数十亿个细胞组成的网络&lt;/a>;。研究人员目前还不了解其网络机制的故障如何导致精神疾病和其他疾病（例如痴呆症）的全貌。然而，新兴的&lt;a href=&quot;https://en.wikipedia.org/wiki/Connectomics&quot;>;连接组学&lt;/a>;领域旨在精确绘制大脑中每个细胞之间的连接，可以帮助解决这个问题。虽然地图只是为更简单的生物体创建的，但绘制更大大脑的技术进步可以使我们了解人脑的工作原理以及如何治疗脑部疾病。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 今天，我们&lt;a href=&quot;https://www.ninds.nih.gov/news-events/highlights-announcements/nih -brain-initiative-launches-projects-develop-innovative-technologies-map-brain-incredible-detail&quot;>;很高兴地宣布&lt;/a>; &lt;a href=&quot;https://research.google/teams/connectomics/ &quot;>;Google 研究中心的连接组学团队&lt;/a>;和我们的合作者正在启动一个&lt;a href=&quot;https://reporter.nih.gov/project-details/10665380&quot;>;耗资 3300 万美元的项目&lt;/a>;来拓展前沿未来五年的连接组学。由 &lt;a href=&quot;https://www.nih.gov/&quot;>;通过推进创新神经技术进行大脑研究&lt;/a>; (BRAIN) 计划的支持/&quot;>;美国国立卫生研究院&lt;/a>; (NIH) 和&lt;a href=&quot;https://lichtmanlab.fas.harvard.edu/&quot;>;哈佛大学&lt;/a>;的研究人员领导，我们将致力于与来自&lt;a href=&quot;https://alleninstitute.org/division/brain-science/&quot;>;艾伦研究所&lt;/a>;的多学科专家团队一起，&lt;a href=&quot;https://fietelab.mit.edu /&quot;>;麻省理工学院&lt;/a>;，&lt;a href=&quot;https://www2.mrc-lmb.cam.ac.uk/group-leaders/h-to-m/gregory-jefferis/&quot;>;剑桥大学&lt;/ a>;、&lt;a href=&quot;https://pni.princeton.edu/&quot;>;普林斯顿大学&lt;/a>;和&lt;a href=&quot;https://www.jhuapl.edu/&quot;>;约翰霍普金斯大学&lt;/a>; >;，顾问来自&lt;a href=&quot;https://www.janelia.org/lab/hess-lab&quot;>;HHMI 珍妮莉亚研究园区&lt;/a>;。我们的项目目标是应对神经科学领域的巨大挑战：绘制小鼠大脑的一小部分（2-3%）的图谱。我们将特别针对&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5690575/&quot;>;海马&lt;/a>;区域，该区域负责编码记忆、注意力和空间导航。该项目是 NIH 1.5 亿美元资助的 11 个项目之一&lt;a href=&quot;https://braininitiative.nih.gov/funding-opportunies/brain-initiative-connectivity-across-scales-brain-connects-compressive-0&quot;>;大脑倡议跨尺度连接（BRAIN CONNECTS）计划。谷歌研究中心正在为这项工作贡献计算和分析资源，并且不会从美国国立卫生研究院获得任何资助。我们的项目提出了一个关键问题：我们能否扩展和加速我们的技术，以绘制小鼠大脑的整个连接组？ &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;连接组学的现代时代&lt;/h2>; &lt;p>; 这项绘制小型连接组图的工作小鼠大脑的一部分建立在该领域十年的创新基础上，其中包括谷歌研究院连接组学团队发起的许多进展。我们希望完成类似于&lt;a href=&quot;https://www.genome.gov/ human-genome-project&quot;>;人类基因组计划&lt;/a>;早期的事情，当时科学家们花了数年时间对人类基因组的一小部分，因为他们改进了技术，使他们能够完成基因组的其余部分。 &lt;/p>; &lt;p>; 2021 年，我们和哈佛大学的合作者成功&lt;a href=&quot;https://blog.research.google/2021/06/a-browsable-petascale-reconstruction-of.html&quot;>;绘制了一个人类大脑的立方毫米&lt;/a>;，我们将其作为 &lt;a href=&quot;https://h01-release.storage.googleapis.com/landing.html&quot;>;H01&lt;/a>; 数据集发布，该数据集是研究人脑和扩展连接组学技术。但绘制整个人脑连接组需要收集和分析多达泽字节（十亿太字节）的数据，这超出了现有技术的当前能力。 &lt;/p>; &lt;p>; 分析鼠标连接组是第二好的事情。它足够小，在技术上是可行的，并且有可能提供与我们自己的想法相关的见解；神经科学家已经使用小鼠来研究人类大脑的功能和功能障碍。通过共同绘制 10-15 立方毫米的小鼠大脑图谱，我们希望开发出新的方法，使我们能够绘制小鼠大脑的整个其余部分以及此后的人类大脑图谱。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3wC7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_ GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/s960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “540”数据原始宽度=“960”高度=“360”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3w C7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/w640-h360/ image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;神经科学家一直在努力几十年来绘制越来越大、越来越复杂的连接体。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;生物学最大的数据集之一&lt;/h2>; &lt;p>; 在这个连接组学项目中，我们将绘制 &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978 -3-642-76447-9_1&quot;>;小鼠大脑的海马结构&lt;/a>;，它将短期记忆转化为长期记忆，并帮助小鼠在空间中导航。小鼠海马结构是我们试图通过这种方式了解的大脑中最大的区域。通过绘制小鼠大脑这一区域的图谱，我们将创建生物学中最大的数据集之一，其中包含约 25,000 TB（即 25 PB 的大脑数据）。作为参考，我们的银河系大约有 2500 亿颗恒星。如果这些恒星中的每一颗都是一个字节，则需要 100,000 个银河系才能匹配该项目在绘制小鼠大脑的一小部分区域时将收集的 25 PB 数据。 &lt;/p>; &lt;p>; 为了说明海马项目的规模，我们计算了存储已完成的绘制蛔虫和果蝇大脑的连接组项目的图像数据所需的 Pixel 手机数量（如下所示的像素堆栈），如下所示以及小鼠海马区和整个小鼠大脑项目，这些项目才刚刚开始。 &lt;/p>; &lt;p>; 然后，我们将每个像素堆栈的高度与熟悉的物体和地标进行比较。需要一堆 100 个像素（相当于一个四岁女孩那么高）来存储果蝇大脑的图像数据，这是迄今为止已完成的最大项目。相比之下，小鼠海马连接组工作将需要相当于超过 48,800 个像素的存储空间，达到帝国大厦的高度。下面的动画展示了小鼠海马体项目将如何超越之前的连接组项目的规模。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU 61NDijkDpPgnz2eqswz_J_alrpb2FColxo17BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s1600/image1.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU61NDijkDpPgnz2eqswz_J_alrpb2FColxo17 BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;我们正在与几位合作者合作，为小鼠大脑的海马区构建连接组（脑细胞之间的连接图）。该项目将创建有史以来最大的连接组数据集，超过之前绘制较小蛔虫和果蝇大脑的项目的规模。我们希望这项努力能够促进新方法的开发，使我们能够在以后绘制整个小鼠大脑的图谱。该动画展示了连接组学领域如何通过计算存储各种项目数据所需的 Pixel 手机数量来扩大规模。只需要两个像素（橄榄的高度）来存储蛔虫连接组数据，而需要一堆珠穆朗玛峰大小的像素来存储整个小鼠连接组的数据。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 了解小鼠海马结构的连接组有助于阐明我们大脑的工作方式。例如，我们可能会发现小鼠大脑和人类大脑中的这种电路之间的共同特征，这些特征解释了我们如何知道自己在哪里，我们的大脑如何将记忆与特定位置相关联，以及无法正确形成新空间的人会出现什么问题。回忆。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开放 PB 管道&lt;/h2>; &lt;p>; 在过去的十年中，我们的团队致力于开发用于管理大量连接组数据集并从中提取科学价值的工具。但小鼠大脑的神经元数量是果蝇大脑的 1000 倍，果蝇是一种生物体，&lt;a href=&quot;https://blog.research.google/2020/01/releasing- drosophila-hemibrain.html&quot;>;我们帮助为大脑的很大一部分构建了连接组&lt;/a>;。启动小鼠大脑连接组将挑战我们改进现有技术，使我们能够比以往更快地绘制更多数据。 &lt;/p>; &lt;p>; 我们将继续完善我们的&lt;a href=&quot;https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html&quot;>;洪水填充网络&lt;/a>;，它使用深度学习来追踪或“分段”每个神经元在由电子显微镜数据制成的三维大脑体积中的路径。我们还将扩展自我监督学习技术的功能，&lt;a href=&quot;https://blog.research.google/2022/11/multi-layered-mapping-of-brain-tissue.html&quot;>;SegCLR &lt;/a>;，它允许我们自动从分段体积中提取关键见解，例如识别细胞类型（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Pyramidal_cell#:~:text=Pyramidal %20cells%2C%20or%20pyramidal%20neurons，cortex%20and%20the%20corticospinal%20tract。&quot;>;锥体神经元&lt;/a>;，&lt;a href=&quot;https://en.wikipedia.org/wiki/Basket_cell&quot;>;篮子神经元&lt;/a>;等）和每个神经元的部分（例如轴突、树突等）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAn Uv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOdZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s720/image2.gif&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAnUv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOd ZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;洪水填充网络通过三维大脑空间追踪神经元。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们还将继续增强核心连接组学基础设施的可扩展性和性能，例如&lt;a href=&quot;https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html&quot;>;用于存储的TensorStore&lt;/a>;和&lt;a href=&quot;https://github .com/google/neuroglancer&quot;>;用于可视化的 Neuroglancer&lt;/a>;，以便使我们所有的计算管道和人工分析工作流程能够在这些新的数据规模上运行。我们渴望通过研究老鼠的大脑来了解我们自己的大脑。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;本博文中描述的鼠标连接组学项目将得到 NIH BRAIN Initiative 的部分支持，奖项编号为 1UM1NS132250。谷歌研究中心正在为小鼠连接组项目贡献计算和分析资源，并且不会获得美国国立卫生研究院的资助。许多人参与了使该项目成为可能的技术开发。我们感谢 Lichtman 实验室（哈佛大学）、HHMI Janelia 和 Denk 实验室（马克斯普朗克生物智能研究所）的长期学术合作者，并感谢 Google 连接组学团队的核心贡献。我们还感谢 John Guilyard 在这篇文章中创建了说明性动画，并感谢 Elise Kleeman 和 Erika Check Hayden 的支持。感谢 Lizzie Dorfman、Michael Brenner、Jay Yagnik 和 Jeff Dean 的支持、协调和领导。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog .research.google/feeds/109886617977245321/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2023/09/google-research-embarks-on-effort-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http:// /www.blogger.com/feeds/8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/ 8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/google-research-embarks-on -effort-to.html&quot; rel=&quot;alternate&quot; title=&quot;Google 研究开始绘制小鼠大脑图谱&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>; http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com /g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height= “72”网址=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV_ccWS2g1Qcjssq2HmmGCqbaL_ij-Uro aO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s72-c/connectome%20600x580.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http: //search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog- 8474926331452026626.POST-276208367666649407668 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2023-09-21T14：25：00.000-07：00：00：00 &lt;/00类别方案 =“http://www.blogger.com/atom/ns#”术语 =“ACL”>;&lt;/类别>;&lt;类别方案 =“http://www.blogger.com/atom/ns#”术语=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;逐步提炼：用更少的训练数据和更小的模型尺寸超越更大的语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Cheng-Yu Hsieh，学生研究员和云 AI 团队研究科学家 Chen-Yu Lee &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgb s7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s320/英雄.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 大型语言模型（LLM）启用了一种新的数据高效学习范式，其中它们可用于通过&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;零解决看不见的新任务- 射击或几次射击提示&lt;/a>;。然而，由于法学硕士规模庞大，在实际应用中部署法学硕士具有挑战性。例如，为单个 1750 亿个 LLM 提供服务需要使用&lt;a href=&quot;https://arxiv.org/abs/2201.12023&quot;>;专用基础设施&lt;/a>;至少 350GB GPU 内存，更不用说当今的状态了-最先进的 LLM 由超过&lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;5000 亿个参数&lt;/a>;组成。对于许多研究团队来说，这样的计算要求是无法满足的，特别是对于需要低延迟性能的应用程序。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了规避这些部署挑战，从业者通常选择部署较小的专用模型。这些较小的模型使用两种常见范例之一进行训练：&lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;微调&lt;/a>;或&lt;a href=&quot;https://arxiv.org /abs/1503.02531&quot;>;蒸馏&lt;/a>;。微调更新预先训练的较小模型（例如，&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/ abs/1910.10683&quot;>;T5&lt;/a>;) 使用下游手动注释数据。蒸馏法使用较大的法学硕士生成的标签来训练相同的较小模型。不幸的是，为了达到与法学硕士相当的性能，微调方法需要人工生成的标签，而获取这些标签既昂贵又繁琐，而蒸馏则需要大量未标记的数据，这些数据也很难收集。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;逐步蒸馏！用更少的训练数据和更小的模型大小超越更大的语言模型&lt;/a>;”，在 &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>; 上提出，我们着手解决这一问题-模型大小和训练数据收集成本之间的差距。我们引入了逐步蒸馏，这是一种新的简单机制，使我们能够使用比标准微调或蒸馏方法少得多的训练数据来训练更小的特定于任务的模型，这些方法的性能优于少数样本提示的法学硕士的性能。我们证明，逐步蒸馏机制使 770M 参数 T5 模型能够优于仅使用基准数据集中 80% 的示例的小样本提示的 540B PaLM 模型，这表明模型大小减少了 700 倍以上，而所需的资源却少得多。标准方法所需的培训数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQK qwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1570&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR 5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;虽然法学硕士提供了强大的零样本和少样本表现，但它们在实践中具有挑战性。另一方面，训练小型特定任务模型的传统方法需要大量的训练数据。逐步提炼提供了一种新的范例，可以减少部署的模型大小以及训练所需的数据数量。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;逐步提炼&lt;/h2>; &lt;p>; 逐步提炼的关键思想是提取信息来自法学硕士的&lt;em>;自然语言&lt;/em>; &lt;em>;基本原理（即&lt;/em>;中间推理步骤）&lt;em>; &lt;/em>;来自法学硕士，这反过来又可用于以更高效的数据方式训练小型模型方式。具体来说，自然语言原理解释了输入问题与其相应输出之间的联系。例如，当被问及“&lt;em>;杰西的房间长 11 英尺，宽 15 英尺。如果她已经有​​ 16 平方英尺的地毯，她还需要多少地毯才能覆盖整个地板？&lt;/em>;”，少数镜头&lt;a href=&quot;https://blog. Research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;思想链&lt;/a>; (CoT) 提示技术提供中间原理，例如“面积 = 长度” * 宽度。 Jesse 的房间有 11 * 15 平方英尺。&lt;/em>;”这更好地解释了从输入到最终答案“&lt;em>;(11 * 15 ) - 16&lt;/em>;”的联系。这些基本原理可以包含相关的任务知识，例如“面积=长度*宽度”，这些知识最初可能需要许多数据来供小模型学习。除了标准任务标签之外，我们还利用这些提取的基本原理作为额外的、更丰富的监督来训练小型模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1Cq KtMZG7gyRiw4YOQcUn1EUj386PgYZ1PP-Wq9vDS​​er0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI /s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1999&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EU j386PgYZ1PP-Wq9vDS​​er0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;逐步提炼概述：首先，我们利用 CoT 提示从 LLM 中提取基本原理。然后，我们使用生成的基本原理在多任务学习框架中训练小型特定于任务的模型，在该框架中，我们将任务前缀添加到输入示例中，并训练模型根据给定的任务前缀进行不同的输出。&lt;/td>;&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 逐步蒸馏由两个主要阶段组成。在第一阶段，我们利用小样本 CoT 提示从法学硕士中提取基本原理。具体来说，给定一个任务，我们在 LLM 输入提示中准备一些样本，其中每个示例由一个三元组组成，其中包含：（1）输入、（2）基本原理和（3）输出。根据提示，法学硕士能够模仿三元组演示，为任何新输入生成基本原理。例如，在&lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;常识性问答任务&lt;/a>;中，给定输入问题“Sammy想去人们所在的地方。他可能会去哪里？答案选择：(a) 人口稠密地区，(b) 赛道，(c) 沙漠，(d) 公寓，(e) 路障”，逐步提炼出问题的正确答案，“(a) 人口稠密”区域”，并结合提供问题与答案之间更好联系的基本原理，“答案必须是一个有很多人的地方。以上选择中，只有人口稠密的地区人才多。”通过在提示中提供与基本原理配对的 Co​​T 示例，&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;上下文学习能力&lt;/a>;允许法学硕士为未来未见的输入输出相应的基本原理。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc 4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1175&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwK x2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们使用少镜头 CoT 提示，其中包含示例理由（&lt;strong>;以绿色突出显示&lt;/strong>;）和标签（&lt;strong>;以蓝色突出显示&lt;/strong>;），以从法学硕士中获取新输入示例的基本原理。该示例来自一个常识性问答任务。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>;提取理由后，在第二阶段，我们将理由纳入训练中通过将训练过程视为多任务问题来构建小模型。具体来说，除了标准&lt;em>;&lt;a href=&quot;https://blog.research.google/2020/02/exploring-transfer -learning-with-t5.html?m=1&quot;>;标签预测任务&lt;/a>;&lt;/em>;。基本原理生成任务使模型能够学习生成预测的中间推理步骤，并指导模型更好地预测结果标签。我们将&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;任务前缀&lt;/a>;（即分别用于标签预测和基本原理生成的[标签]和[基本原理]）添加到输入示例中区分这两个任务的模型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验设置&lt;/h2>; &lt;p>; 在实验中，我们考虑 &lt;a href=&quot; https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; 模型作为 LLM。对于特定于任务的下游模型，我们使用 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 模型&lt;/a>;。对于 CoT 提示，我们使用&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;原始 CoT 提示&lt;/a>;（如果可用），并为新数据集策划我们自己的示例。我们在三个不同 NLP 任务的四个基准数据集上进行了实验：&lt;a href=&quot;https://arxiv.org/abs/1812.01193&quot;>;e-SNLI&lt;/a>; 和 &lt;a href=&quot;https://arxiv .org/abs/1910.14599&quot;>;ANLI&lt;/a>;用于&lt;a href=&quot;https://arxiv.org/abs/1508.05326&quot;>;自然语言推理&lt;/a>;； &lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;CQA&lt;/a>; 用于常识性问题回答；和&lt;a href=&quot;https://arxiv.org/abs/2103.07191&quot;>;SVAMP&lt;/a>;用于&lt;a href=&quot;https://aclanthology.org/N16-1136/&quot;>;算术数学应用题&lt;/一个>;。我们包括两组基线方法。 For comparison to &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompted LLMs&lt;/a>;, we compare to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompting&lt;/a>; with a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model. In the &lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;paper&lt;/a>;, we also compare standard task-specific model training to both &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;standard distillation&lt;/a>;. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Less training data&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>;, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1477&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Smaller deployed model size&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompted LLMs&lt;/a>;, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM&#39;s performance using standard fine-tuning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1384&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, ie, Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, eg, over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM&#39;s performance using the same model size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Distilling step-by-step outperforms few-shot LLMs with smaller models using less data&lt;/h3>; &lt;p>; Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM&#39;s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM&#39;s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1400&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM&#39;s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM&#39;s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Availability on Google Cloud Platform&lt;/h2>; &lt;p>; Distilling step-by-step is available for private preview on &lt;a href=&quot;https://cloud.google.com/vertex-ai&quot;>;Vertex AI&lt;/a>;. If you are interested in trying it out, please contact &lt;a href=&quot;mailto:vertex-llm-tuning-preview@google.com&quot;>;vertex-llm-tuning-preview@google.com&lt;/a>; with your Google Cloud Project number and a summary of your use case. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2762083676649407668/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html&quot; rel=&quot;alternate&quot; title=&quot;Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgbs7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7378908661087185403&lt;/id>;&lt;published>;2023-09-15T10:39:00.002-07:00&lt;/published>;&lt;updated>;2023-09-15T10:43:29.705-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MediaPipe FaceStylizer: On-device real-time few-shot face stylization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Haolin Jia, Software Engineer, and Qifei Wang, Senior Software Engineer, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s828/FaceStylizer-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, we have witnessed rising interest across consumers and researchers in integrated augmented reality (AR) experiences using real-time face feature generation and editing functions in mobile applications, including short videos, virtual reality, and gaming. As a result, there is a growing demand for lightweight, yet high-quality face generation and editing models, which are often based on &lt;a href=&quot;https://arxiv.org/pdf/1406.2661.pdf&quot;>;generative adversarial network&lt;/a>; (GAN) techniques. However, the majority of GAN models suffer from high computational complexity and the need for a large training dataset. In addition, it is also important to employ GAN models responsibly. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this post, we introduce &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;, an efficient design for few-shot face stylization that addresses the aforementioned model complexity and data efficiency challenges while being guided by Google&#39;s responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. The model consists of a face generator and a face encoder used as &lt;a href=&quot;https://arxiv.org/pdf/2101.05278.pdf&quot;>;GAN inversion&lt;/a>; to map the image into latent code for the generator. We introduce a mobile-friendly synthesis network for the face generator with an auxiliary head that converts features to &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; at each level of the generator to generate high quality images from coarse to fine granularities. We also carefully designed the &lt;a href=&quot;https://developers.google.com/machine-learning/gan/loss&quot;>;loss functions&lt;/a>; for the aforementioned auxiliary heads and combined them with the common GAN loss functions to distill the student generator from the teacher &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;>;StyleGAN&lt;/a>; model, resulting in a lightweight model that maintains high generation quality. The proposed solution is available in open source through &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;. Users can fine-tune the generator to learn a style from one or a few images using MediaPipe Model Maker, and deploy to on-device face stylization applications with the customized model using &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Few-shot on-device face stylization&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An end-to-end pipeline&lt;/h3>; &lt;p>; Our goal is to build a pipeline to support users to adapt the MediaPipe FaceStylizer to different styles by fine-tuning the model with a few examples. To enable such a face stylization pipeline, we built the pipeline with a GAN inversion encoder and efficient face generator model (see below). The encoder and generator pipeline can then be adapted to different styles via a few-shot learning process. The user first sends a single or a few similar samples of the style images to MediaPipe ModelMaker to fine-tune the model. The fine-tuning process freezes the encoder module and only fine-tunes the generator. The training process samples multiple latent codes close to the encoding output of the input style images as the input to the generator. The generator is then trained to reconstruct an image of a person&#39;s face in the style of the input style image by optimizing a joint adversarial loss function that also accounts for style and content. With such a fine-tuning process, the MediaPipe FaceStylizer can adapt to the customized style, which approximates the user&#39;s input. It can then be applied to stylize test images of real human faces. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Generator: BlazeStyleGAN&lt;/h3>; &lt;p>; The &lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;>;StyleGAN&lt;/a>; model family has been widely adopted for face generation and various face editing tasks. To support efficient on-device face generation, we based the design of our generator on StyleGAN. This generator, which we call &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.html&quot;>;BlazeStyleGAN&lt;/a>;, is similar to StyleGAN in that it also contains a mapping network and synthesis network. However, since the synthesis network of StyleGAN is the major contributor to the model&#39;s high computation complexity, we designed and employed a more efficient synthesis network. The improved efficiency and generation quality is achieved by: &lt;/p>; &lt;ol>; &lt;li>;Reducing the latent feature dimension in the synthesis network to a quarter of the resolution of the counterpart layers in the teacher StyleGAN, &lt;/li>; &lt;li>; Designing multiple auxiliary heads to transform the downscaled feature to the image domain to form a coarse-to-fine image pyramid to evaluate the perceptual quality of the reconstruction, and &lt;/li>; &lt;li>; Skipping all but the final auxiliary head at inference time. &lt;/li>; &lt;/ol>; &lt;p>; With the newly designed architecture, we train the BlazeStyleGAN model by distilling it from a teacher StyleGAN model. We use a multi-scale perceptual loss and adversarial loss in the distillation to transfer the high fidelity generation capability from the teacher model to the student BlazeStyleGAN model and also to mitigate the artifacts from the teacher model. &lt;/p>; &lt;p>; More details of the model architecture and training scheme can be found in &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.pdf&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;488&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual comparison between face samples generated by StyleGAN and BlazeStyleGAN. The images on the first row are generated by the teacher StyleGAN. The images on the second row are generated by the student BlazeStyleGAN. The face generated by BlazeStyleGAN has similar visual quality to the image generated by the teacher model. Some results demonstrate the student BlazeStyleGAN suppresses the artifacts from the teacher model in the distillation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the above figure, we demonstrate some sample results of our BlazeStyleGAN. By comparing with the face image generated by the teacher StyleGAN model (top row), the images generated by the student BlazeStyleGAN (bottom row) maintain high visual quality and further reduce artifacts produced by the teacher due to the loss function design in our distillation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An encoder for efficient GAN inversion&lt;/h3>; &lt;p>; To support image-to-image stylization, we also introduced an efficient GAN inversion as the encoder to map input images to the latent space of the generator. The encoder is defined by a &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;MobileNet V2&lt;/a>; backbone and trained with natural face images. The loss is defined as a combination of image perceptual quality loss, which measures the content difference, style similarity and embedding distance, as well as the &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization&quot;>;L1 loss&lt;/a>; between the input images and reconstructed images. &lt;/p>; &lt;br />; &lt;h2>;On-device performance&lt;/h2>; &lt;p>; We documented model complexities in terms of parameter numbers and computing &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPs&lt;/a>; in the following table. Compared to the teacher StyleGAN (33.2M parameters), BlazeStyleGAN (generator) significantly reduces the model complexity, with only 2.01M parameters and 1.28G FLOPs for output resolution 256x256. Compared to StyleGAN-1024 (generating image size of 1024x1024), the BlazeStyleGAN-1024 can reduce both model size and computation complexity by 95% with no notable quality difference and can even suppress the artifacts from the teacher StyleGAN model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Image Size&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;#Params (M)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;FLOPs (G)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;StyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;33.17 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.3 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.07 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;512 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.05 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.57 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.28 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Encoder&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.44 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.60 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model complexity measured by parameter numbers and FLOPs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We benchmarked the inference time of the MediaPipe FaceStylizer on various high-end mobile devices and demonstrated the results in the table below. From the results, both BlazeStyleGAN-256 and BlazeStyleGAN-512 achieved real-time performance on all GPU devices. It can run in less than 10 ms runtime on a high-end phone&#39;s GPU. BlazeStyleGAN-256 can also achieve real-time performance on the iOS devices&#39; CPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;BlazeStyleGAN-256 (ms)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Encoder-256 (ms)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 11&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.14 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.48 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 12&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.25 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 13 Pro&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.22 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.41 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Pixel 6&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.24 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.23 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Samsung Galaxy S10&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;17.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Samsung Galaxy S20&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.95 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.20 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency benchmark of the BlazeStyleGAN, face encoder, and the end-to-end pipeline on various mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Fairness evaluation&lt;/h2>; &lt;p>; The model has been trained with a high diversity dataset of human faces. The model is expected to be fair to different human faces. The fairness evaluation demonstrates the model performs good and balanced in terms of human gender, skin-tone, and ages. &lt;/p>; &lt;br />; &lt;h2>;Face stylization visualization&lt;/h2>; &lt;p>; Some face stylization results are demonstrated in the following figure. The images in the top row (in orange boxes) represent the style images used to fine-tune the model. The images in the left column (in the green boxes) are the natural face images used for testing. The 2x4 matrix of images represents the output of the MediaPipe FaceStylizer which is blending outputs between the natural faces on the left-most column and the corresponding face styles on the top row. The results demonstrate that our solution can achieve high-quality face stylization for several popular styles. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIGBFYBgQiL-bpMHWRKDl8WF2rO-aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s940/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;940&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIGBFYBgQiL-bpMHWRKDl8WF2rO-aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Sample results of our MediaPipe FaceStylizer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MediaPipe Solutions&lt;/h2>; &lt;p>; The MediaPipe FaceStylizer is going to be released to public users in &lt;a href=&quot;https://developers.google.com/mediapipe/solutions&quot;>;MediaPipe Solutions&lt;/a>;. Users can leverage &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Model Maker&lt;/a>; to train a customized face stylization model using their own style images. After training, the exported bundle of &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TFLite&lt;/a>; model files can be deployed to applications across platforms (Android, iOS, Web, Python, etc.) using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Tasks FaceStylizer API&lt;/a>; in just a few lines of code. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is made possible through a collaboration spanning several teams across Google. We&#39;d like to acknowledge contributions from Omer Tov, Yang Zhao, Andrey Vakunov, Fei Deng, Ariel Ephrat, Inbar Mosseri, Lu Wang, Chuo-Ling Chang, Tingbo Hou, and Matthias Grundmann.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7378908661087185403/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html&quot; rel=&quot;alternate&quot; title=&quot;MediaPipe FaceStylizer: On-device real-time few-shot face stylization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s72-c/FaceStylizer-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-118669777610020383&lt;/id>;&lt;published>;2023-09-14T12:39:00.035-07:00&lt;/published>;&lt;updated>;2023-09-14T18:23:00.311-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device content distillation with graph neural networks&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Gabriel Barcik and Duc-Hieu Tran, Research Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s971/ScreenGNN.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In today&#39;s digital age, smartphones and desktop web browsers serve as the primary tools for accessing news and information. However, the proliferation of website clutter — encompassing complex layouts, navigation elements, and extraneous links — significantly impairs both the reading experience and article navigation. This issue is particularly acute for individuals with accessibility requirements. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve the user experience and make reading more accessible, &lt;a href=&quot;https://www.android.com/google-features-on-android/december-2022/#accessibility&quot;>;Android&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/education/chromebook-updates-2023/&quot;>;Chrome&lt;/a>; users may leverage the Reading Mode feature, which enhances accessibility by processing webpages to allow customizable contrast, adjustable text size, more legible fonts, and to enable text-to-speech utilities. Additionally, Android&#39;s Reading Mode is equipped to distill content from apps. Expanding Reading Mode to encompass a wide array of content and improving its performance, while still operating locally on the user&#39;s device without transmitting data externally, poses a unique challenge. &lt;/p>; &lt;p>; To broaden Reading Mode capabilities without compromising privacy, we have developed a novel on-device content distillation model. Unlike early attempts using &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>; — a heuristic approach limited to news articles — our model excels in both quality and versatility across various types of content. We ensure that article content doesn&#39;t leave the confines of the local environment. Our on-device content distillation model smoothly transforms long-form content into a simple and customizable layout for a more pleasant reading journey while also outperforming the leading alternative approaches. Here we explore details of this research highlighting our approach, methodology, and results. &lt;/p>; &lt;br />; &lt;h2>;Graph neural networks&lt;/h2>; &lt;p>; Instead of relying on complicated heuristics that are difficult to maintain and scale to a variety of article layouts, we approach this task as a fully &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;supervised learning&lt;/a>; problem. This data-driven approach allows the model to generalize better across different layouts, without the constraints and fragility of heuristics. Previous work for optimizing the reading experience relied on HTML or parsing, filtering, and modeling of a &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction&quot;>;document object model&lt;/a>; (DOM), a programming interface automatically generated by the user&#39;s web browser from site HTML that represents the structure of a document and allows it to be manipulated. &lt;/p>; &lt;p>; The new Reading Mode model relies on &lt;a href=&quot;https://developer.chrome.com/blog/full-accessibility-tree/#what-is-the-accessibility-tree&quot;>;accessibility trees&lt;/a>;, which provide a streamlined and more accessible representation of the DOM. Accessibility trees are automatically generated from the DOM tree and are utilized by assistive technologies to allow people with disabilities to interact with web content. These are available on Chrome Web browser and on Android through &lt;a href=&quot;https://developer.android.com/reference/android/view/accessibility/AccessibilityNodeInfo&quot;>;AccessibilityNodeInfo&lt;/a>; objects, which are provided for both WebView and native application content. &lt;/p>; &lt;p>; We started by manually collecting and annotating accessibility trees. The Android dataset used for this project comprises on the order of 10k labeled examples, while the Chrome dataset contains approximately 100k labeled examples. We developed a novel tool that uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;graph neural networks&lt;/a>; (GNNs) to distill essential content from the accessibility trees using a multi-class supervised learning approach. The datasets consist of long-form articles sampled from the web and labeled with classes such as headline, paragraph, images, publication date, etc. &lt;/p>; &lt;p>; GNNs are a natural choice for dealing with tree-like data structures, because unlike traditional models that often demand detailed, hand-crafted features to understand the layout and links within such trees, GNNs learn these connections naturally. To illustrate this, consider the analogy of a family tree. In such a tree, each node represents a family member and the connections denote familial relationships. If one were to predict certain traits using conventional models, features like the &quot;number of immediate family members with a trait&quot; might be needed. However, with GNNs, such manual feature crafting becomes redundant. By directly feeding the tree structure into the model, GNNs utilize a message-passing mechanism where each node communicates with its neighbors. Over time, information gets shared and accumulated across the network, enabling the model to naturally discern intricate relationships. &lt;/p>; &lt;p>; Returning to the context of accessibility trees, this means that GNNs can efficiently distill content by understanding and leveraging the inherent structure and relationships within the tree. This capability allows them to identify and possibly omit non-essential sections based on the information flow within the tree, ensuring more accurate content distillation. &lt;/p>; &lt;p>; Our architecture heavily follows the &lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;encode-process-decode&lt;/a>; paradigm using a &lt;a href=&quot;https://arxiv.org/abs/1704.01212&quot;>;message-passing neural network&lt;/a>; to classify text nodes. The overall design is illustrated in the figure below. The tree representation of the article is the input to the model. We compute lightweight features based on bounding box information, text information, and accessibility roles. The GNN then propagates each node&#39;s latent representation through the edges of the tree using a message-passing neural network. This propagation process allows nearby nodes, containers, and text elements to share contextual information with each other, enhancing the model&#39;s understanding of the page&#39;s structure and content. Each node then updates its current state based on the message received, providing a more informed basis for classifying the nodes. After a fixed number of message-passing steps, the now contextualized latent representations of the nodes are decoded into essential or non-essential classes. This approach enables the model to leverage both the inherent relationships in the tree and the hand-crafted features representing each node, thereby enriching the final classification. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHwbSlNBQiYUY7Q9ZAg-cJbR8SpU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHwbSlNBQiYUY7Q9ZAg-cJbR8SpU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual demonstration of the algorithm in action, processing an article on a mobile device. A graph neural network (GNN) is used to distill essential content from an article. 1. A tree representation of the article is extracted from the application. 2. Lightweight features are computed for each node, represented as vectors. 3. A message-passing neural network propagates information through the edges of the tree and updates each node representation. 4. Leaf nodes containing text content are classified as essential or non-essential content. 5. A decluttered version of the application is composed based on the GNN output.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We deliberately restrict the feature set used by the model to increase its broad generalization across languages and speed up inference latency on user devices. This was a unique challenge, as we needed to create an on-device lightweight model that could preserve privacy. &lt;/p>; &lt;p>; Our final lightweight Android model has 64k parameters and is 334kB in size with a median latency of 800ms, while the Chrome model has 241k parameters, is 928kB in size, and has a 378ms median latency. By employing such on-device processing, we ensure that user data never leaves the device, reinforcing our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;responsible approach&lt;/a>; and commitment to user privacy. The features used in the model can be grouped into intermediate node features, leaf-node text features, and element position features. We performed &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;>;feature engineering&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_selection&quot;>;feature selection&lt;/a>; to optimize the set of features for model performance and model size. The final model was transformed into &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; format to deploy as an on-device model on Android or Chrome. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; We trained the GNN for about 50 epochs in a single GPU. The performance of the Android model on webpages and native application test sets is presented below: &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWoDA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s1339/AndroidQuality.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWoDA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s16000/AndroidQuality.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the content distillation metrics in Android for webpages and native apps. We report &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision, recall&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>; for three classes: non-essential content, headline, and main body text, including macro average and weighted average by number of instances in each class. Node metrics assess the classification performance at the granularity of the accessibility tree node, which is analogous to a paragraph level. In contrast, word metrics evaluate classification at an individual word level, meaning each word within a node gets the same classification.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;13&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Android Quality &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;Webpages&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;Native Apps&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td style=&quot;font-size: normal;&quot;>;&lt;strong>;&lt;em>;Node Metrics&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall &lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;non-essential&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9842 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9846 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9844 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9744 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9350 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9543 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;headline&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9187 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9784 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9476 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9183 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8568 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8865 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;main-text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9223 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9172 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9197 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8443 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9424 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8907 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;macro-average&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9417 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9600 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9506 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9124 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9114 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9105 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;weighted average&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9392 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9353 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9363 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td>;&lt;strong>;&lt;em>;Word Metrics&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall &lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;headline + main-text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9510&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9683&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9595&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9473&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9507&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9490&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the content distillation metrics in Android for webpages and native apps. We report &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision, recall&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>; for three classes: non-essential content, headline, and main body text, including macro average and weighted average by number of instances in each class. Node metrics assess the classification performance at the granularity of the accessibility tree node, which is analogous to a paragraph level. In contrast, word metrics evaluate classification at an individual word level, meaning each word within a node gets the same classification.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;p>; In assessing the results&#39; quality on commonly visited webpage articles, an F1-score exceeding 0.9 for main-text (essentially paragraphs) corresponds to 88% of these articles being processed without missing any paragraphs. Furthermore, in over 95% of cases, the distillation proves to be valuable for readers. Put simply, the vast majority of readers will perceive the distilled content as both pertinent and precise, with errors or omissions being an infrequent occurrence. &lt;/p>; &lt;p>; The comparison of Chrome content distillation with other models such as &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>; or &lt;a href=&quot;https://github.com/mozilla/readability&quot;>;Mozilla Readability&lt;/a>; on a set of English language pages is presented in the table below. We reuse the metrics from machine translation to compare the quality of these models. The reference text is from the groundtruth main content and the text from the models as hypothesis text. The results show the excellent performance of our models in comparison to other DOM-based approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9StyysA89xcqSLyj7QWYi-Hc4eye-3c3WNlA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s1339/ChromeQuality.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9StyysA89xcqSLyj7QWYi-Hc4eye-3c3WNlA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s16000/ChromeQuality.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the comparison between DOM-Distiller, Mozilla Readability and the new Chrome model. We report text-based metrics, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;, &lt;a href=&quot;https://aclanthology.org/W15-3049.pdf&quot;>;CHRF&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;, by comparing the main body text distilled from each model to a ground-truth text manually labeled by raters using our annotation policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;7&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Chrome Model Comparison on Webpages &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: center;&quot;>;&lt;strong>;&lt;em>;Metric / Model&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;DOM Distiller&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Mozilla Readability&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Our Chrome model&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;BLEU&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;79.16 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.59 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;CHRF&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE1&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.10 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.62 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.13 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE2&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;82.66 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.81 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE3&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;80.21 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.45 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.60 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;83.58 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.02 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL-SUM&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;83.46 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.03 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the comparison between DOM-Distiller, Mozilla Readability and the new Chrome model. We report text-based metrics, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;, &lt;a href=&quot;https://aclanthology.org/W15-3049.pdf&quot;>;CHRF&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;, by comparing the main body text distilled from each model to a ground-truth text manually labeled by raters using our annotation policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;p>; The F1-score of the Chrome content distillation model for headline and main text content on the test sets of different widely spoken languages demonstrates that the Chrome model, in particular, is able to support a wide range of languages. &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4-OC4_JFrieWniKArBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxeem_J7UTM/s1339/ChromeLanguages.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4-OC4_JFrieWniKArBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxeem_J7UTM/s16000/ChromeLanguages.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents per language of F1-scores of the Chrome model for the headline and main text classes. The language codes correspond to the following languages: German, English, Spanish, French, Italian, Persian, Japanese, Korean, Portuguese, Vietnamese, simplified Chinese and traditional Chinese.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;27&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Chrome Model on Different Languages &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: left;&quot;>;&lt;strong>;&lt;em>;F1-score&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;de&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;en&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;es&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fr&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;it&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fa&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ja&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ko&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;pt&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;vi&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hans&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hant&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;average&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;headline&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.89 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.96 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;main text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.87 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.88 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents per language of F1-scores of the Chrome model for the headline and main text classes. The language codes correspond to the following languages: German, English, Spanish, French, Italian, Persian, Japanese, Korean, Portuguese, Vietnamese, simplified Chinese and traditional Chinese.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The digital age demands both streamlined content presentation and an unwavering commitment to user privacy. Our research highlights the effectiveness of Reading Mode in platforms like Android and Chrome, offering an innovative, data-driven approach to content parsing through Graph Neural Networks. Crucially, our lightweight on-device model ensures that content distillation occurs without compromising user data, with all processes executed locally. This not only enhances the reading experience but also reinforces our dedication to user privacy. As we navigate the evolving landscape of digital content consumption, our findings underscore the paramount importance of prioritizing the user in both experience and security. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project is the result of joint work with Manuel Tragut, Mihai Popa, Abodunrinwa Toki, Abhanshu Sharma, Matt Sharifi, David Petrou and Blaise Aguera y Arcas. We sincerely thank our collaborators Gang Li and Yang Li. We are very grateful to Tom Small for assisting us in preparing the post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/118669777610020383/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html&quot; rel=&quot;alternate&quot; title=&quot;On-device content distillation with graph neural networks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s72-c/ScreenGNN.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6191878611070084392&lt;/id>;&lt;published>;2023-09-12T14:22:00.000-07:00&lt;/published>;&lt;updated>;2023-09-12T14:22:55.668-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;World scale inverse reinforcement learning in Google Maps&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Matt Barnes, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Routing in Google Maps remains one of our most helpful and frequently used features. Determining the best route from A to B requires making complex trade-offs between factors including the &lt;a href=&quot;https://www.deepmind.com/blog/traffic-prediction-with-advanced-graph-neural-networks&quot;>;estimated time of arrival&lt;/a>; (ETA), tolls, directness, surface conditions (eg, paved, unpaved roads), and user preferences, which vary across transportation mode and local geography. Often, the most natural visibility we have into travelers&#39; preferences is by analyzing real-world travel patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Learning preferences from observed sequential decision making behavior is a classic application of &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning#Inverse_reinforcement_learning&quot;>;inverse reinforcement learning&lt;/a>; (IRL). Given a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP) — a formalization of the road network — and a set of demonstration trajectories (the traveled routes), the goal of IRL is to recover the users&#39; latent reward function. Although &lt;a href=&quot;https://link.springer.com/article/10.1007/s10462-021-10108-x&quot;>;past research&lt;/a>; has created increasingly general IRL solutions, these have not been successfully scaled to world-sized MDPs. Scaling IRL algorithms is challenging because they typically require solving an RL subroutine &lt;em>;at every update step&lt;/em>;. At first glance, even attempting to fit a world-scale MDP into memory to compute a single gradient step appears infeasible due to the large number of road segments and limited high bandwidth memory. When applying IRL to routing, one needs to consider all reasonable routes between each demonstration&#39;s origin and destination. This implies that any attempt to break the world-scale MDP into smaller components cannot consider components smaller than a metropolitan area. &lt;/p>; &lt;p>; To this end, in &quot;&lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;Massively Scalable Inverse Reinforcement Learning in Google Maps&lt;/a>;&quot;, we share the result of a multi-year collaboration among Google Research, Maps, and Google DeepMind to surpass this IRL scalability limitation. We revisit classic algorithms in this space, and introduce advances in graph compression and parallelization, along with a new IRL algorithm called Receding Horizon Inverse Planning (RHIP) that provides fine-grained control over performance trade-offs. The final RHIP policy achieves a 16–24% relative improvement in global route match rate, ie, the percentage of de-identified traveled routes that exactly match the suggested route in Google Maps. To the best of our knowledge, this represents the largest instance of IRL in a real world setting to date. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Google Maps improvements in route match rate relative to the existing baseline, when using the RHIP inverse reinforcement learning policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The benefits of IRL&lt;/h2>; &lt;p>; A subtle but crucial detail about the routing problem is that it is &lt;em>;goal conditioned&lt;/em>;, meaning that every destination state induces a slightly different MDP (specifically, the destination is a terminal, zero-reward state). IRL approaches are well suited for these types of problems because the learned reward function transfers across MDPs, and only the destination state is modified. This is in contrast to approaches that directly learn a policy, which typically require an extra factor of &lt;em>;S&lt;/em>; parameters, where &lt;em>;S&lt;/em>; is the number of MDP states. &lt;/p>; &lt;p>; Once the reward function is learned via IRL, we take advantage of a powerful inference-time trick. First, we evaluate the entire graph&#39;s rewards once in an &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/static-vs-dynamic-inference/video-lecture&quot;>;offline batch setting&lt;/a>;. This computation is performed entirely on servers without access to individual trips, and operates only over batches of road segments in the graph. Then, we save the results to an in-memory database and use a fast online &lt;a href=&quot;https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm&quot;>;graph search algorithm&lt;/a>; to find the highest reward path for routing requests between any origin and destination. This circumvents the need to perform online inference of a deeply parameterized model or policy, and vastly improves serving costs and latency. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/s791/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;406&quot; data-original-width=&quot;791&quot; height=&quot;328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/w640-h328/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reward model deployment using batch inference and fast online planners.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Receding Horizon Inverse Planning&lt;/h2>; &lt;p>; To scale IRL to the world MDP, we compress the graph and shard the global MDP using a sparse &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;Mixture of Experts&lt;/a>; (MoE) based on geographic regions. We then apply classic IRL algorithms to solve the local MDPs, estimate the loss, and send gradients back to the MoE. The worldwide reward graph is computed by decompressing the final MoE reward model. To provide more control over performance characteristics, we introduce a new generalized IRL algorithm called Receding Horizon Inverse Planning (RHIP). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s1617/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;526&quot; data-original-width=&quot;1617&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;IRL reward model training using MoE parallelization, graph compression, and RHIP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; RHIP is inspired by people&#39;s tendency to perform extensive local planning (&quot;What am I doing for the next hour?&quot;) and approximate long-term planning (&quot;What will my life look like in 5 years?&quot;). To take advantage of this insight, RHIP uses robust yet expensive stochastic policies in the local region surrounding the demonstration path, and switches to cheaper deterministic planners beyond some horizon. Adjusting the horizon &lt;em>;H&lt;/em>; allows controlling computational costs, and often allows the discovery of the performance sweet spot. Interestingly, RHIP generalizes many classic IRL algorithms and provides the novel insight that they can be viewed along a stochastic vs. deterministic spectrum (specifically, for &lt;em>;H&lt;/em>;=∞ it reduces to &lt;a href=&quot;https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf&quot;>;MaxEnt&lt;/a>;, for &lt;em>;H&lt;/em>;=1 it reduces to &lt;a href=&quot;https://www.ijcai.org/Proceedings/07/Papers/416.pdf&quot;>;BIRL&lt;/a>;, and for &lt;em>;H&lt;/em>;=0 it reduces to &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1143844.1143936&quot;>;MMP&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANax2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/s1039/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1039&quot; height=&quot;260&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANax2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/w640-h260/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given a demonstration from s&lt;sub>;o&lt;/sub>; to s&lt;sub>;d&lt;/sub>;, (1) RHIP follows a robust yet expensive stochastic policy in the local region surrounding the demonstration (&lt;strong>;blue region&lt;/strong>;). (2) Beyond some horizon H, RHIP switches to following a cheaper deterministic planner (&lt;strong>;red lines&lt;/strong>;). Adjusting the horizon enables fine-grained control over performance and computational costs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Routing wins&lt;/h2>; &lt;p>; The RHIP policy provides a 15.9% and 24.1% lift in global route match rate for driving and two-wheelers (eg, scooters, motorcycles, mopeds) relative to the well-tuned Maps baseline, respectively. We&#39;re especially excited about the benefits to more sustainable transportation modes, where factors beyond journey time play a substantial role. By tuning RHIP&#39;s horizon &lt;em>;H&lt;/em>;, we&#39;re able to achieve a policy that is both more accurate than all other IRL policies and 70% faster than MaxEnt. &lt;/p>; &lt;p>; Our 360M parameter reward model provides intuitive wins for Google Maps users in live &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B experiments&lt;/a>;. Examining road segments with a large absolute difference between the learned rewards and the baseline rewards can help improve certain Google Maps routes. For example: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s1245/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1245&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Nottingham, UK. The preferred route (&lt;strong>;blue&lt;/strong>;) was previously marked as private property due to the presence of a large gate, which indicated to our systems that the road may be closed at times and would not be ideal for drivers. As a result, Google Maps routed drivers through a longer, alternate detour instead (&lt;strong>;red&lt;/strong>;). However, because real-world driving patterns showed that users regularly take the preferred route without an issue (as the gate is almost never closed), IRL now learns to route drivers along the preferred route by placing a large positive reward on this road segment.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Increasing performance via increased scale – both in terms of dataset size and model complexity – has proven to be a persistent trend in machine learning. Similar gains for inverse reinforcement learning problems have historically remained elusive, largely due to the challenges with handling practically sized MDPs. By introducing scalability advancements to classic IRL algorithms, we&#39;re now able to train reward models on problems with hundreds of millions of states, demonstration trajectories, and model parameters, respectively. To the best of our knowledge, this is the largest instance of IRL in a real-world setting to date. See the &lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;paper&lt;/a>; to learn more about this work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Matthew Abueg, Oliver Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, Shawn O&#39;Banion, Ryan Epp, Renaud Hartert, Rui Song, Thomas Sharp, Rémi Robert, Zoltan Szego, Beth Luan, Brit Larabee and Agnieszka Madurska.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We&#39;d also like to extend our thanks to Arno Eigenwillig, Jacob Moorman, Jonathan Spencer, Remi Munos, Michael Bloesch and Arun Ahuja for valuable discussions and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6191878611070084392/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html&quot; rel=&quot;alternate&quot; title=&quot;World scale inverse reinforcement learning in Google Maps&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-237108111388574068&lt;/id>;&lt;published>;2023-09-08T15:59:00.002-07:00&lt;/published>;&lt;updated>;2023-09-11T10:00:14.121-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private median and more&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Edith Cohen and Uri Stemmer, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s1554/dpmedian.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential privacy&lt;/a>; (DP) is a rigorous mathematical definition of privacy. DP algorithms are randomized to protect user data by ensuring that the probability of any particular output is nearly unchanged when a data point is added or removed. Therefore, the output of a DP algorithm does not disclose the presence of any one data point. There has been significant progress in both foundational research and adoption of differential privacy with contributions such as the &lt;a href=&quot;https://privacysandbox.com/&quot;>;Privacy Sandbox&lt;/a>; and &lt;a href=&quot;https://opensource.googleblog.com/2020/06/expanding-our-differential-privacy.html&quot;>;Google Open Source Library&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML and data analytics algorithms can often be described as performing multiple basic computation steps on the same dataset. When each such step is differentially private, so is the output, but with multiple steps the overall privacy guarantee &lt;em>;deteriorates,&lt;/em>; a phenomenon known as the &lt;em>;cost of composition&lt;/em>;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Composability&quot;>;Composition theorems&lt;/a>; bound the increase in privacy loss with the number &lt;em>;k&lt;/em>; of computations: In the general case, the privacy loss increases with the square root of &lt;em>;k&lt;/em>;. This means that we need much stricter privacy guarantees for each step in order to meet our overall privacy guarantee goal. But in that case, we lose utility. One way to improve the privacy vs. utility trade-off is to identify when the use cases admit a tighter privacy analysis than what follows from composition theorems. &lt;/p>; &lt;p>; Good candidates for such improvement are when each step is applied to a disjoint part (slice) of the dataset. When the slices are selected in a data-independent way, each point affects only one of the &lt;em>;k&lt;/em>; outputs and the privacy guarantees do not deteriorate with &lt;em>;k&lt;/em>;. However, there are applications in which we need to select the slices adaptively (that is, in a way that depends on the output of prior steps). In these cases, a change of a single data point may cascade — changing multiple slices and thus increasing composition cost. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;Õptimal Differentially Private Learning of Thresholds and Quasi-Concave Optimization&lt;/a>;”, presented at &lt;a href=&quot;http://acm-stoc.org/stoc2023/&quot;>;STOC 2023&lt;/a>;, we describe a new paradigm that allows for slices to be selected adaptively and yet avoids composition cost. We show that DP algorithms for multiple fundamental aggregation and learning tasks can be expressed in this Reorder-Slice-Compute (RSC) paradigm, gaining significant improvements in utility. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The Reorder-Slice-Compute (RSC) paradigm&lt;/h2>; &lt;p>; An algorithm &lt;em>;A&lt;/em>; falls in the RSC paradigm if it can be expressed in the following general form (see visualization below). The input is a sensitive set &lt;em>;D&lt;/em>; of data points. The algorithm then performs a sequence of &lt;em>;k&lt;/em>; steps as follows: &lt;/p>; &lt;ol>; &lt;li>;Select an ordering over data points, a slice size &lt;em>;m&lt;/em>;, and a DP algorithm &lt;em>;M&lt;/em>;. The selection may depend on the output of &lt;em>;A&lt;/em>; in prior steps (and hence is adaptive). &lt;/li>;&lt;li>;Slice out the (approximately) top &lt;em>;m&lt;/em>; data points according to the order from the dataset &lt;em>;D&lt;/em>;, apply &lt;em>;M&lt;/em>; to the slice, and output the result. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s666/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;478&quot; data-original-width=&quot;666&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visualization of three Reorder-Slice-Compute (RSC) steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If we analyze the overall privacy loss of an RSC algorithm using DP composition theorems, the privacy guarantee suffers from the expected composition cost, ie, it deteriorates with the square root of the number of steps &lt;em>;k&lt;/em>;. To eliminate this composition cost, we provide a novel analysis that removes the dependence on &lt;em>;k&lt;/em>; altogether: the overall privacy guarantee is close to that of a single step! The idea behind our tighter analysis is a novel technique that limits the potential cascade of affected steps when a single data point is modified (details &lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;in the paper&lt;/a>;). &lt;/p>; &lt;p>; Tighter privacy analysis means better utility. The effectiveness of DP algorithms is often stated in terms of the smallest input size (number of data points) that suffices in order to release a correct result that meets the privacy requirements. We describe several problems with algorithms that can be expressed in the RSC paradigm and for which our tighter analysis improved utility. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private interval point&lt;/h2>; &lt;p>; We start with the following basic aggregation task. The input is a dataset &lt;em>;D&lt;/em>; of &lt;em>;n&lt;/em>; points from an ordered domain &lt;em>;X&lt;/em>; (think of the domain as the natural numbers between &lt;em>;1&lt;/em>; and&lt;em>; |X|&lt;/em>;). The goal is to return a point &lt;em>;y&lt;/em>; in &lt;em>;X&lt;/em>; that is in the &lt;em>;interval&lt;/em>; of &lt;em>;D&lt;/em>;, that is between the minimum and the maximum points in &lt;em>;D&lt;/em>;. &lt;/p>; &lt;p>; The solution to the interval point problem is trivial without the privacy requirement: simply return any point in the dataset &lt;em>;D&lt;/em>;. But this solution is not privacy-preserving as it discloses the presence of a particular datapoint in the input. We can also see that if there is only one point in the dataset, a privacy-preserving solution is not possible, as it must return that point. We can therefore ask the following fundamental question: What is the smallest input size &lt;em>;N&lt;/em>; for which we can solve the private interval point problem? &lt;/p>; &lt;p>; It is known that &lt;em>;N&lt;/em>; must increase with the domain size &lt;em>;|X|&lt;/em>; and that this dependence is at least the &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;iterated log function&lt;/a>;&lt;em>; &lt;/em>;log&lt;em>;* |X|&lt;/em>; [&lt;a href=&quot;https://ieeexplore.ieee.org/document/7354419&quot;>;1&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3313276.3316312&quot;>;2&lt;/a>;]. On the other hand, the &lt;a href=&quot;https://proceedings.mlr.press/v125/kaplan20a.html&quot;>;best prior&lt;/a>; DP algorithm required the input size to be at least (log* &lt;em>;|X|&lt;/em>;)&lt;sup>;1.5&lt;/sup>;. To close this gap, we designed an RSC algorithm that requires only an order of log*&lt;em>; |X|&lt;/em>; points. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;iterated log function&lt;/a>; is extremely slow growing: It is the number of times we need to take a logarithm of a value before we reach a value that is equal to or smaller than &lt;em>;1&lt;/em>;. How did this function naturally come out in the analysis? Each step of the RSC algorithm remapped the domain to a logarithm of its prior size. Therefore there were log* &lt;em>;|X|&lt;/em>; steps in total. The tighter RSC analysis eliminated a square root of the number of steps from the required input size. &lt;/p>; &lt;p>; Even though the interval point task seems very basic, it captures the essence of the difficulty of private solutions for common aggregation tasks. We next describe two of these tasks and express the required input size to these tasks in terms of &lt;em>;N&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private approximate median&lt;/h2>; &lt;p>; One of these common aggregation tasks is &lt;em>;approximate median&lt;/em>;: The input is a dataset &lt;em>;D&lt;/em>; of &lt;em>;n&lt;/em>; points from an ordered domain &lt;em>;X&lt;/em>;. The goal is to return a point &lt;em>;y&lt;/em>; that is between the ⅓ and ⅔ quantiles of &lt;em>;D&lt;/em>;. That is, at least a third of the points in &lt;em>;D&lt;/em>; are smaller or equal to &lt;em>;y&lt;/em>; and at least a third of the points are larger or equal to &lt;em>;y&lt;/em>;. Note that returning an exact median is not possible with differential privacy, since it discloses the presence of a datapoint. Hence we consider the relaxed requirement of an approximate median (shown below). &lt;/p>; &lt;p>; We can compute an approximate median by finding an interval point: We slice out the &lt;em>;N&lt;/em>; smallest points and the &lt;em>;N&lt;/em>; largest points and then compute an interval point of the remaining points. The latter must be an approximate median. This works when the dataset size is at least &lt;em>;3N&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;545&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of a data D over domain X, the set of interval points, and the set of approximate medians.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private learning of axis-aligned rectangles&lt;/h2>; &lt;p>; For the next task, the input is a set of &lt;em>;n&lt;/em>; labeled data points, where each point &lt;em>;x = (x&lt;sub>;1&lt;/sub>;,....,x&lt;sub>;d&lt;/sub>;)&lt;/em>; is a &lt;em>;d&lt;/em>;-dimensional vector over a domain &lt;em>;X&lt;/em>;. Displayed below, the goal is to learn values &lt;em>;a&lt;sub>;i &lt;/sub>;, b&lt;sub>;i&lt;/sub>;&lt;/em>; for the axes &lt;em>;i=1,...,d&lt;/em>; that define a &lt;em>;d&lt;/em>;-dimensional rectangle, so that for each example &lt;em>;x&lt;/em>; &lt;/p>; &lt;ul>; &lt;li>;If &lt;em>;x&lt;/em>; is positively labeled (shown as red plus signs below) then it lies within the rectangle, that is, for all axes &lt;em>;i&lt;/em>;, &lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>; is in the interval &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>;, and &lt;/li>;&lt;li>;If &lt;em>;x&lt;/em>; is negatively labeled (shown as blue minus signs below) then it lies outside the rectangle, that is, for at least one axis &lt;em>;i&lt;/em>;, &lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>; is outside the interval &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>;. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/s1656/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1160&quot; data-original-width=&quot;1656&quot; height=&quot;280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/w400-h280/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A set of 2-dimensional labeled points and a respective rectangle.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Any DP solution for this problem must be approximate in that the learned rectangle must be allowed to mislabel some data points, with some positively labeled points outside the rectangle or negatively labeled points inside it. This is because an exact solution could be very sensitive to the presence of a particular data point and would not be private. The goal is a DP solution that keeps this necessary number of mislabeled points small. &lt;/p>; &lt;p>; We first consider the one-dimensional case (&lt;em>;d = 1)&lt;/em>;. We are looking for an interval &lt;em>;[a,b]&lt;/em>; that covers all positive points and none of the negative points. We show that we can do this with at most &lt;em>;2N&lt;/em>; mislabeled points. We focus on the positively labeled points. In the first RSC step we slice out the &lt;em>;N&lt;/em>; smallest points and compute a private interval point as &lt;em>;a&lt;/em>;. We then slice out the &lt;em>;N&lt;/em>; largest points and compute a private interval point as &lt;em>;b&lt;/em>;. The solution&lt;em>; [a,b] &lt;/em>;correctly labels all negatively labeled points and mislabels at most &lt;em>;2N&lt;/em>; of the positively labeled points. Thus, at most ~&lt;em>;2N&lt;/em>; points are mislabeled in total. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;441&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration for &lt;em>;d = 1&lt;/em>;, we slice out &lt;em>;N&lt;/em>; left positive points and compute an interval point &lt;em>;a&lt;/em>;, slice out &lt;em>;N&lt;/em>; right positive points and compute an interval point &lt;em>;b&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With &lt;em>;d &amp;gt; 1&lt;/em>;, we iterate over the axes&lt;em>; i = 1,....,d&lt;/em>; and apply the above for the &lt;em>;i&lt;sup>;th &lt;/sup>;&lt;/em>;coordinates of input points to obtain the values &lt;em>;a&lt;sub>;i &lt;/sub>;, b&lt;sub>;i&lt;/sub>; &lt;/em>;. In each iteration, we perform two RSC steps and slice out &lt;em>;2N&lt;/em>; positively labeled points. In total, we slice out &lt;em>;2dN&lt;/em>; points and all remaining points were correctly labeled. That is, all negatively-labeled points are outside the final &lt;em>;d&lt;/em>;-dimensional rectangle and all positively-labeled points, except perhaps ~&lt;em>;2dN&lt;/em>;, lie inside the rectangle. Note that this algorithm uses the full flexibility of RSC in that the points are ordered differently by each axis. Since we perform &lt;em>;d&lt;/em>; steps, the RSC analysis shaves off a factor of square root of &lt;em>;d&lt;/em>; from the number of mislabeled points. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training ML models with adaptive selection of training examples&lt;/h2>; &lt;p>; The training efficiency or performance of ML models can sometimes be improved by selecting training examples in a way that depends on the current state of the model, eg, self-paced &lt;a href=&quot;https://www.frontiersin.org/research-topics/36658/curriculum-learning-and-related-applications&quot;>;curriculum learning&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Active_learning_(machine_learning)&quot;>;active learning&lt;/a>;. &lt;/p>; &lt;p>; The most common method for private training of ML models is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>;, where noise is added to the gradient update from each minibatch of training examples. Privacy analysis with DP-SGD typically assumes that training examples are &lt;em>;randomly&lt;/em>; partitioned into minibatches. But if we impose a data-dependent selection order on training examples, and further modify the selection criteria &lt;em>;k&lt;/em>; times during training, then analysis through DP composition results in deterioration of the privacy guarantees of a magnitude equal to the square root of &lt;em>;k&lt;/em>;. &lt;/p>; &lt;p>; Fortunately, example selection with DP-SGD can be naturally expressed in the RSC paradigm: each selection criteria reorders the training examples and each minibatch is a slice (for which we compute a noisy gradient). With RSC analysis, there is no privacy deterioration with &lt;em>;k&lt;/em>;, which brings DP-SGD training with example selection into the practical domain. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The RSC paradigm was introduced in order to tackle an open problem that is primarily of theoretical significance, but turns out to be a versatile tool with the potential to enhance data efficiency in production environments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The &lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;work&lt;/a>; described here was done jointly with &lt;a href=&quot;https://people.eecs.berkeley.edu/~xinlyu/&quot;>;Xin Lyu&lt;/a>;, Jelani Nelson, and Tamas Sarlos.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/237108111388574068/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/differentially-private-median-and-more.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/differentially-private-median-and-more.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private median and more&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s72-c/dpmedian.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5967099570177693866&lt;/id>;&lt;published>;2023-09-07T15:03:00.000-07:00&lt;/published>;&lt;updated>;2023-09-07T15:03:10.510-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A novel computational fluid dynamics framework for turbulent flow research&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shantanu Shahane, Software Engineer, and Matthias Ihme, Research Scientist, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Turbulence is ubiquitous in environmental and engineering fluid flows, and is encountered routinely in everyday life. A better understanding of these turbulent processes could provide valuable insights across a variety of research areas — improving the prediction of cloud formation by atmospheric transport and the spreading of wildfires by turbulent energy exchange, understanding sedimentation of deposits in rivers, and improving the efficiency of combustion in aircraft engines to reduce emissions, to name a few. However, despite its importance, our current understanding and our ability to reliably predict such flows remains limited. This is mainly attributed to the highly chaotic nature and the enormous spatial and temporal scales these fluid flows occupy, ranging from energetic, large-scale movements on the order of several meters on the high-end, where energy is injected into the fluid flow, all the way down to micrometers (μm) on the low-end, where the turbulence is dissipated into heat by viscous friction. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A powerful tool to understand these turbulent flows is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Direct_numerical_simulation&quot;>;direct numerical simulation&lt;/a>; (DNS), which provides a detailed representation of the unsteady three-dimensional flow-field without making any approximations or simplifications. More specifically, this approach utilizes a discrete grid with small enough grid spacing to capture the underlying continuous equations that govern the dynamics of the system (in this case, variable-density &lt;a href=&quot;https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations&quot;>;Navier-Stokes equations&lt;/a>;, which govern all fluid flow dynamics). When the grid spacing is small enough, the discrete grid points are enough to represent the true (continuous) equations without the loss of accuracy. While this is attractive, such simulations require tremendous computational resources in order to capture the correct fluid-flow behaviors across such a wide range of spatial scales. &lt;/p>; &lt;p>; The actual span in spatial resolution to which direct numerical calculations must be applied depends on the task and is determined by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Reynolds_number&quot;>;Reynolds number&lt;/a>;, which compares inertial to viscous forces. Typically, the Reynolds number can range between 10&lt;sup>;2&lt;/sup>; up to 10&lt;sup>;7 &lt;/sup>;(even larger for atmospheric or interstellar problems). In 3D, the grid size for the resolution required scales roughly with the Reynolds number to the power of 4.5! Because of this strong scaling dependency, simulating such flows is generally limited to flow regimes with moderate Reynolds numbers, and typically requires access to &lt;a href=&quot;https://www.top500.org/lists/top500/2023/06/&quot;>;high-performance computing systems&lt;/a>; with millions of CPU/GPU cores. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;A TensorFlow simulation framework for scientific computing of fluid flows on tensor processing units&lt;/a>;”, we introduce a new simulation framework that enables the computation of fluid flows with TPUs. By leveraging latest advances on &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>; software and TPU-hardware architecture, this software tool allows detailed large-scale simulations of turbulent flows at unprecedented scale, pushing the boundaries of scientific discovery and turbulence analysis. We demonstrate that this framework scales efficiently to accommodate the scale of the problem or, alternatively, improved run times, which is remarkable since most large-scale distributed computation frameworks exhibit reduced efficiency with scaling. The software is available as an open-source project on &lt;a href=&quot;https://github.com/google-research/swirl-lm&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Large-scale scientific computation with accelerators&lt;/h2>; &lt;p>; The software solves variable-density Navier-Stokes equations on TPU architectures using the TensorFlow framework. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;>;single-instruction, multiple-data&lt;/a>; (SIMD) approach is adopted for parallelization of the TPU solver implementation. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_difference_method&quot;>;finite difference&lt;/a>; operators on a &lt;a href=&quot;https://www.cfd-online.com/Wiki/Collocated_grid&quot;>;colocated&lt;/a>; structured mesh are cast as filters of the convolution function of TensorFlow, leveraging TPU&#39;s matrix multiply unit (MXU). The framework takes advantage of the low-latency high-bandwidth inter-chips interconnect (ICI) between the TPU accelerators. In addition, by leveraging the single-precision floating-point computations and highly optimized executable through the accelerated linear algebra (XLA) compiler, it&#39;s possible to perform large-scale simulations with excellent scaling on TPU hardware architectures. &lt;/p>; &lt;p>; This research effort demonstrates that the &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph&lt;/a>;-based TensorFlow in combination with new types of ML special purpose hardware, can be used as a programming paradigm to solve partial differential equations representing multiphysics flows. The latter is achieved by augmenting the Navier-Stokes equations with physical models to account for chemical reactions, heat-transfer, and density changes to enable, for example, simulations of &lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;cloud formation&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;wildfires&lt;/a>;. &lt;/p>; &lt;p>; It&#39;s worth noting that this framework is the first open-source &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_fluid_dynamics&quot;>;computational fluid dynamics&lt;/a>; (CFD) framework for high-performance, large-scale simulations to fully leverage the cloud accelerators that have become common (and become a commodity) with the advancement of machine learning (ML) in recent years. While our work focuses on using TPU accelerators, the code can be easily adjusted for other accelerators, such as GPU clusters. &lt;/p>; &lt;p>; This framework demonstrates a way to greatly reduce the cost and turn-around time associated with running large-scale scientific CFD simulations and enables even greater iteration speed in fields, such as climate and weather research. Since the framework is implemented using TensorFlow, an ML language, it also enables the ready integration with ML methods and allows the exploration of ML approaches on CFD problems. With the general accessibility of TPU and GPU hardware, this approach lowers the barrier for researchers to contribute to our understanding of large-scale turbulent systems. &lt;/p>; &lt;br />; &lt;h2>;Framework validation and homogeneous isotropic turbulence&lt;/h2>; &lt;p>; Beyond demonstrating the performance and the scaling capabilities, it is also critical to validate the correctness of this framework to ensure that when it is used for CFD problems, we get reasonable results. For this purpose, researchers typically use idealized benchmark problems during CFD solver development, many of which we adopted in our work (more details in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;paper&lt;/a>;). &lt;/p>; &lt;p>; One such benchmark for turbulence analysis is &lt;a href=&quot;https://en.wikipedia.org/wiki/Homogeneous_isotropic_turbulence&quot;>;homogeneous isotropic turbulence&lt;/a>;&lt;em>; &lt;/em>;(HIT), which is a canonical and well studied flow in which the statistical properties, such as kinetic energy, are invariant under translations and rotations of the coordinate axes. By pushing the resolution to the limits of the current state of the art, we were able to perform direct numerical simulations with more than eight billion degrees of freedom — equivalent to a three-dimensional mesh with 2,048 grid points along each of the three directions. We used 512 TPU-v4 cores, distributing the computation of the grid points along the x, y, and z axes to a distribution of [2,2,128] cores, respectively, optimized for the performance on TPU. The wall clock time per timestep was around 425 milliseconds and the flow was simulated for a total of 400,000 timesteps. 50 TB data, which includes the velocity and density fields, is stored for 400 timesteps (every 1,000th step). To our knowledge, this is one of the largest turbulent flow simulations of its kind conducted to date. &lt;/p>; &lt;p>; Due to the complex, chaotic nature of the turbulent flow field, which extends across several magnitudes of resolution, simulating the system in high resolution is necessary. Because we employ a fine-resolution grid with eight billion points, we are able to accurately resolve the field. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;894&quot; data-original-width=&quot;990&quot; height=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/w400-h361/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Contours of &lt;em>;x&lt;/em>;-component of velocity along the &lt;em>;z&lt;/em>; midplane. The high resolution of the simulation is critical to accurately represent the turbulent field.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The turbulent kinetic energy and dissipation rates are two statistical quantities commonly used to analyze a turbulent flow. The temporal decay of these properties in a turbulent field without additional energy injection is due to viscous dissipation and the decay asymptotes follow the expected analytical &lt;a href=&quot;https://en.wikipedia.org/wiki/Energy_cascade&quot;>;power law&lt;/a>;. This is in agreement with the theoretical asymptotes and observations reported in the literature and thus, validates our framework. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s562/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Solid line: Temporal evolution of turbulent kinetic energy (&lt;em>;k&lt;/em>;). Dashed line: Analytical power laws for decaying homogeneous isotropic turbulence (&lt;em>;n&lt;/em>;=1.3) (&lt;em>;Ⲧ&lt;sub>;l&lt;/sub>;&lt;/em>;: &lt;a href=&quot;https://physics.stackexchange.com/questions/79184/what-are-the-length-and-time-scales-in-turbulence&quot;>;eddy turnover time&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s565/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;565&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Solid line: Temporal evolution of dissipation rate (&lt;em>;ε&lt;/em>;). Dashed line: Analytical power laws for decaying homogeneous isotropic turbulence (n=1.3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The energy spectrum of a turbulent flow represents the energy content across &lt;a href=&quot;https://en.wikipedia.org/wiki/Wavenumber&quot;>;wavenumber&lt;/a>;, where the wavenumber &lt;i>;k&lt;/i>; is proportional to the inverse wavelength &lt;i>;λ&lt;/i>; (ie, &lt;i>;k&lt;/i>; ∝ 1/&lt;i>;λ&lt;/i>;). Generally, the spectrum can be qualitatively divided into three ranges: source range, inertial range and viscous dissipative range (from left to right on the wavenumber axis, below). The lowest wavenumbers in the source range correspond to the largest turbulent eddies, which have the most energy content. These large eddies transfer energy to turbulence in the intermediate wavenumbers (inertial range), which is statistically isotropic (ie, essentially uniform in all directions). The smallest eddies, corresponding to the largest wavenumbers, are dissipated into thermal energy by the viscosity of the fluid. By virtue of the fine grid having 2,048 points in each of the three spatial directions, we are able to resolve the flow field up to the length scale at which viscous dissipation takes place. This direct numerical simulation approach is the most accurate as it does not require any closure model to approximate the energy cascade below the grid size. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s569/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;569&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spectrum of turbulent kinetic energy at different time instances. The spectrum is normalized by the instantaneous integral length (&lt;em>;l&lt;/em>;) and the turbulent kinetic energy (&lt;em>;k&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A new era for turbulent flows research&lt;/h2>; &lt;p>; More recently, we extended this framework to predict &lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;wildfires&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;atmospheric flows&lt;/a>;, which is relevant for climate-risk assessment. Apart from enabling high-fidelity simulations of complex turbulent flows, this simulation framework also provides capabilities for &lt;a href=&quot;https://www.osti.gov/servlets/purl/1478744&quot;>;scientific machine learning&lt;/a>; (SciML) — for example, downsampling from a fine to a coarse grid (&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_order_reduction&quot;>;model reduction&lt;/a>;) or building models that run at lower resolution while still capturing the correct dynamic behaviors. It could also provide avenues for further scientific discovery, such as building ML-based models to better parameterize microphysics of turbulent flows, including physical relationships between temperature, pressure, vapor fraction, etc., and could improve upon various control tasks, eg, to reduce the energy consumption of buildings or find more efficient propeller shapes. While attractive, a main bottleneck in SciML has been the availability of data for training. To explore this, we have been working with groups at Stanford and Kaggle to make the data from our high-resolution HIT simulation available through a community-hosted web-platform, &lt;a href=&quot;https://blastnet.github.io&quot;>;BLASTNet&lt;/a>;, to provide broad access to high-fidelity data to the research community via a network-of-datasets approach. We hope that the availability of these emerging high-fidelity simulation tools in conjunction with community-driven datasets will lead to significant advances in various areas of fluid mechanics. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Qing Wang, Yi-Fan Chen, and John Anderson for consulting and advice, Tyler Russell and Carla Bromberg for program management. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5967099570177693866/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html&quot; rel=&quot;alternate&quot; title=&quot;A novel computational fluid dynamics framework for turbulent flow research&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s72-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8763480087701216145&lt;/id>;&lt;published>;2023-09-06T12:47:00.001-07:00&lt;/published>;&lt;updated>;2023-09-12T16:01:52.707-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;TSMixer: An all-MLP architecture for time series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Si-An Chen, Student Researcher, Cloud AI Team, and Chun-Liang Li, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;span style=&quot;font-size: small;&quot;>;&lt;i>;&lt;b>;Update — 2023/09/12:&lt;/b>; This post has been updated as the work described is now published in &lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;&lt;em>;Transactions on Machine Learning Research&lt;/em>;&lt;/a>;.&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time series&lt;/a>; forecasting is critical to various real-world applications, from &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;demand forecasting&lt;/a>; to &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/29606177/&quot;>;pandemic spread prediction&lt;/a>;. In &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;multivariate time series forecasting&lt;/a>; (forecasting multiple variants at the same time), one can split existing methods into two categories: univariate models and multivariate models. Univariate models focus on inter-series interactions or temporal patterns that encompass trends and seasonal patterns on a time series with a single variable. Examples of such trends and seasonal patterns might be the way mortgage rates increase due to inflation, and how traffic peaks during rush hour. In addition to inter-series patterns, multivariate models process intra-series features, known as cross-variate information, which is especially useful when one series is an advanced indicator of another series. For example, a rise in body weight may cause an increase in blood pressure, and increasing the price of a product may lead to a decrease in sales. Multivariate models have &lt;a href=&quot;https://research.google/pubs/pub49697/&quot;>;recently become popular solutions&lt;/a>; for multivariate forecasting as practitioners believe their capability of handling cross-variate information may lead to better performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In recent years, deep learning Transformer-based architectures have become a popular choice for multivariate forecasting models due to their superior performance on sequence tasks. However, advanced multivariate models &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;perform surprisingly worse&lt;/a>; than simple univariate linear models on commonly-used &lt;a href=&quot;https://github.com/thuml/Autoformer&quot;>;long-term forecasting benchmarks&lt;/a>;, such as &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;Electricity Transformer Temperature&lt;/a>; (ETT), &lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption&quot;>;Electricity&lt;/a>;, &lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;Traffic&lt;/a>;, and &lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;Weather&lt;/a>;. These results raise two questions: &lt;/p>; &lt;ul>; &lt;li>;Does cross-variate information benefit time series forecasting? &lt;/li>;&lt;li>;When cross-variate information is not beneficial, can multivariate models still perform as well as univariate models? &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06053&quot;>;TSMixer: An All-MLP Architecture for Time Series Forecasting&lt;/a>;”, published in the &lt;em>;&lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;Transactions on Machine Learning Research&lt;/a>;&lt;/em>; (TMLR), we analyze the advantages of univariate linear models and reveal their effectiveness. Insights from this analysis lead us to develop Time-Series Mixer (TSMixer), an advanced multivariate model that leverages linear model characteristics and performs well on long-term forecasting benchmarks. To the best of our knowledge, TSMixer is the first multivariate model that performs as well as state-of-the-art univariate models on long-term forecasting benchmarks, where we show that cross-variate information is less beneficial. To demonstrate the importance of cross-variate information, we evaluate a more challenging real-world application, &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;M5&lt;/a>;. Finally, empirical results show that TSMixer outperforms state-of-the-art models, such as &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;Fedformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TSMixer architecture&lt;/h2>; &lt;p>; A key difference between linear models and Transformers is how they capture temporal patterns. On one hand, linear models apply fixed and time-step-dependent weights to capture static temporal patterns, and are unable to process cross-variate information. On the other hand, Transformers use &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention mechanisms&lt;/a>; that apply dynamic and data-dependent weights at each time step, capturing dynamic temporal patterns and enabling them to process cross-variate information. &lt;/p>; &lt;p>; In our analysis, we show that under common assumptions of temporal patterns, linear models have naïve solutions to perfectly recover the time series or place bounds on the error, which means they are great solutions for learning static temporal patterns of univariate time series more effectively. In contrast, it is non-trivial to find similar solutions for attention mechanisms, as the weights applied to each time step are dynamic. Consequently, we develop a new architecture by replacing Transformer attention layers with linear layers. The resulting TSMixer model, which is similar to the computer vision &lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;>;MLP-Mixer&lt;/a>; method, alternates between applications of the multi-layer perceptron in different directions, which we call &lt;em>;time-mixing&lt;/em>; and &lt;em>;feature-mixing,&lt;/em>; respectively. The TSMixer architecture efficiently captures both temporal patterns and cross-variate information, as shown in the figure below. The residual designs ensure that TSMixer retains the capacity of temporal linear models while still being able to exploit cross-variate information. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer block and TSMixer block architectures. TSMixer replaces the multi-head attention layer with time-mixing, a linear model applied on the time dimension.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/s1646/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1646&quot; height=&quot;195&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/w400-h195/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison between data-dependent (attention mechanisms) and time-step-dependent (linear models). This is an example of forecasting the next time step by learning the weights of the previous three time steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on long-term forecasting benchmarks&lt;/h2>; &lt;p>; We evaluate TSMixer using seven popular &lt;a href=&quot;https://github.com/thuml/Autoformer&quot;>;long-term forecasting datasets&lt;/a>; (&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm1&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm2&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh1&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh2&lt;/a>;, &lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption&quot;>;Electricity&lt;/a>;, &lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;Traffic&lt;/a>;, and &lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;Weather&lt;/a>;), where &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;recent research&lt;/a>; has shown that univariate linear models outperform advanced multivariate models with large margins. We compare TSMixer with state-of-the-art multivariate models (&lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;FEDformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.07436&quot;>;Informer&lt;/a>;), and univariate models, including &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;linear models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;. The figure below shows the average improvement of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;mean squared error&lt;/a>; (MSE) by TSMixer compared with others. The average is calculated across datasets and multiple forecasting horizons. We demonstrate that TSMixer significantly outperforms other multivariate models and performs on par with state-of-the-art univariate models. These results show that multivariate models are capable of performing as well as univariate models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The average MSE improvement of TSMixer compared with other baselines. The red bars show multivariate methods and the blue bars show univariate methods. TSMixer achieves significant improvement over other multivariate models and achieves comparable results to univariate models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Ablation study&lt;/h2>; &lt;p>; We performed an ablation study to compare TSMixer with TMix-Only, a TSMixer variant that consists of time mixing layers only. The results show that TMix-Only performs almost the same as TSMixer, which means the additional feature mixing layers do not improve the performance and confirms that cross-variate information is less beneficial on popular benchmarks. The results validate the superior univariate model performance shown in &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;previous research&lt;/a>;. However, existing long-term forecasting benchmarks are not well representative of the need for cross-variate information in some real-world applications where time series may be intermittent or sparse, hence temporal patterns may not be sufficient for forecasting. Therefore, it may be inappropriate to evaluate multivariate forecasting models solely on these benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on M5: Effectiveness of cross-variate information&lt;/h2>; &lt;p>; To further demonstrate the benefit of multivariate models, we evaluate TSMixer on the challenging M5 benchmark, a large-scale retail dataset containing crucial cross-variate interactions. M5 contains the information of 30,490 products collected over 5 years. Each product description includes time series data, like daily sales, sell price, promotional event information, and static (non-time-series) features, such as store location and product category. The goal is to forecast the daily sales of each product for the next 28 days, evaluated using the &lt;a href=&quot;https://mofc.unic.ac.cy/m5-competition/&quot;>;weighted root mean square scaled error&lt;/a>; (WRMSSE) from the M5 competition. The complicated nature of retail makes it more challenging to forecast solely using univariate models that focus on temporal patterns, so multivariate models with cross-variate information and even auxiliary features are more essential. &lt;/p>; &lt;p>; First, we compare TSMixer to other methods only considering the historical data, such as daily sales and historical sell prices. The results show that multivariate models outperforms univariate models significantly, indicating the usefulness of cross-variate information. And among all compared methods, TSMixer effectively leverages the cross-variate information and achieves the best performance. &lt;/p>; &lt;p>; Additionally, to leverage more information, such as static features (eg, store location, product category) and future time series (eg, a promotional event scheduled in coming days) provided in M5, we propose a principle design to extend TSMixer. The extended TSMixer aligns different types of features into the same length, and then applies multiple mixing layers to the concatenated features to make predictions. The extended TSMixer architecture outperforms models popular in industrial applications, including &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;, showcasing its strong potential for real-world impact. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/s1950/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1950&quot; data-original-width=&quot;1760&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/w578-h640/image2.png&quot; width=&quot;578&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The architecture of the extended TSMixer. In the first stage (align stage), it aligns the different types of features into the same length before concatenating them. In the second stage (mixing stage) it applies multiple mixing layers conditioned with static features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The WRMSSE on M5. The first three methods (&lt;strong>;blue&lt;/strong>;) are univariate models. The middle three methods (&lt;strong>;orange&lt;/strong>;) are multivariate models that consider only historical features. The last three methods (&lt;strong>;red&lt;/strong>;) are multivariate models that consider historical, future, and static features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present TSMixer, an advanced multivariate model that leverages linear model characteristics and performs as well as state-of-the-art univariate models on long-term forecasting benchmarks. TSMixer creates new possibilities for the development of time series forecasting architectures by providing insights into the importance of cross-variate and auxiliary information in real-world scenarios. The empirical results highlight the need to consider more realistic benchmarks for multivariate forecasting models in future research. We hope that this work will inspire further exploration in the field of time series forecasting, and lead to the development of more powerful and effective models that can be applied to real-world applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8763480087701216145/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html&quot; rel=&quot;alternate&quot; title=&quot;TSMixer: An all-MLP architecture for time series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6223088894812086013&lt;/id>;&lt;published>;2023-08-31T10:14:00.000-07:00&lt;/published>;&lt;updated>;2023-08-31T10:14:27.378-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;WeatherBench 2: A benchmark for the next generation of data-driven weather models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Stephan Rasp, Research Scientist, and Carla Bromberg, Program Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s800/weatherbenchgif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In 1950, weather forecasting started its digital revolution when researchers used the first programmable, general-purpose computer &lt;a href=&quot;https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20(%2F%CB%88%C9%9Bni,of%20them%20in%20one%20computer.&quot;>;ENIAC&lt;/a>; to solve mathematical equations describing how weather evolves. In the more than 70 years since, continuous advancements in computing power and improvements to the model formulations have led to steady gains in weather forecast skill: a 7-day forecast today is about as accurate as a 5-day forecast in 2000 and a 3-day forecast in 1980. While improving forecast accuracy at the pace of approximately one day per decade may not seem like a big deal, every day improved is important in far reaching use cases, such as for logistics planning, disaster management, agriculture and energy production. This &lt;a href=&quot;https://www.nature.com/articles/nature14956&quot;>;“quiet” revolution&lt;/a>; has been tremendously valuable to society, saving lives and providing economic value across many sectors. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Now we are seeing the start of yet another revolution in weather forecasting, this time fueled by advances in machine learning (ML). Rather than hard-coding approximations of the physical equations, the idea is to have algorithms learn how weather evolves from looking at large volumes of past weather data. Early attempts at doing so go back to &lt;a href=&quot;https://gmd.copernicus.org/articles/11/3999/2018/&quot;>;2018&lt;/a>; but the pace picked up considerably in the last two years when several large ML models demonstrated weather forecasting skill comparable to the best physics-based models. Google&#39;s MetNet [&lt;a href=&quot;https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;2&lt;/a>;], for instance, demonstrated state-of-the-art capabilities for forecasting regional weather one day ahead. For global prediction, Google DeepMind created &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>;, a graph neural network to make 10 day predictions at a horizontal resolution of 25 km, competitive with the best physics-based models in many skill metrics. &lt;/p>; &lt;p>; Apart from potentially providing more accurate forecasts, one key advantage of such ML methods is that, once trained, they can create forecasts in a matter of minutes on inexpensive hardware. In contrast, traditional weather forecasts require large super-computers that run for hours every day. Clearly, ML represents a tremendous opportunity for the weather forecasting community. This has also been recognized by leading weather forecasting centers, such as the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&#39;&lt;/a>; (ECMWF) &lt;a href=&quot;https://www.ecmwf.int/en/elibrary/81207-machine-learning-ecmwf-roadmap-next-10-years&quot;>;machine learning roadmap&lt;/a>; or the &lt;a href=&quot;https://www.noaa.gov/&quot;>;National Oceanic and Atmospheric Administration&#39;s&lt;/a>; (NOAA) &lt;a href=&quot;https://sciencecouncil.noaa.gov/wp-content/uploads/2023/04/2020-AI-Strategy.pdf&quot;>;artificial intelligence strategy&lt;/a>;. &lt;/p>; &lt;p>; To ensure that ML models are trusted and optimized for the right goal, forecast evaluation is crucial. Evaluating weather forecasts isn&#39;t straightforward, however, because weather is an incredibly multi-faceted problem. Different end-users are interested in different properties of forecasts, for example, renewable energy producers care about wind speeds and solar radiation, while crisis response teams are concerned about the track of a potential cyclone or an impending heat wave. In other words, there is no single metric to determine what a “good” weather forecast is, and the evaluation has to reflect the multi-faceted nature of weather and its downstream applications. Furthermore, differences in the exact evaluation setup — eg, which resolution and ground truth data is used — can make it difficult to compare models. Having a way to compare novel and established methods in a fair and reproducible manner is crucial to measure progress in the field. &lt;/p>; &lt;p>; To this end, we are announcing &lt;a href=&quot;https://arxiv.org/abs/2308.15560&quot;>;WeatherBench 2&lt;/a>; (WB2), a benchmark for the next generation of data-driven, global weather models. WB2 is an update to the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002203&quot;>;original benchmark&lt;/a>; published in 2020, which was based on initial, lower-resolution ML models. The goal of WB2 is to accelerate the progress of data-driven weather models by providing a trusted, reproducible framework for evaluating and comparing different methodologies. The &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;official website&lt;/a>; contains scores from several state-of-the-art models (at the time of writing, these are &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler (2022)&lt;/a>;, an early graph neural network, Google DeepMind&#39;s &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>; and Huawei&#39;s &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06185-3&quot;>;Pangu-Weather&lt;/a>;, a transformer-based ML model). In addition, forecasts from ECMWF&#39;s high-resolution and ensemble forecasting systems are included, which represent some of the best traditional weather forecasting models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s1960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;372&quot; data-original-width=&quot;1960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Making evaluation easier&lt;/h2>; &lt;p>; The key component of WB2 is an &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;open-source evaluation framework&lt;/a>; that allows users to evaluate their forecasts in the same manner as other baselines. Weather forecast data at high-resolutions can be quite large, making even evaluation a computational challenge. For this reason, we built our evaluation code on &lt;a href=&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>;, which allows users to split computations into smaller chunks and evaluate them in a distributed fashion, for example using &lt;a href=&quot;https://cloud.google.com/dataflow&quot;>;DataFlow&lt;/a>; on Google Cloud. The code comes with a &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/evaluation.html&quot;>;quick-start guide&lt;/a>; to help people get up to speed. &lt;/p>; &lt;p>; Additionally, we &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/data-guide.html&quot;>;provide&lt;/a>; most of the ground-truth and baseline data on Google Cloud Storage in cloud-optimized &lt;a href=&quot;https://zarr.dev/&quot;>;Zarr&lt;/a>; format at different resolutions, for example, a comprehensive copy of the &lt;a href=&quot;https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803&quot;>;ERA5&lt;/a>; dataset used to train most ML models. This is part of a larger Google effort to provide &lt;a href=&quot;https://github.com/google-research/arco-era5&quot;>;analysis-ready, cloud-optimized weather and climate datasets&lt;/a>; to the research community and &lt;a href=&quot;https://github.com/google-research/arco-era5#roadmap&quot;>;beyond&lt;/a>;. Since downloading these data from the respective archives and converting them can be time-consuming and compute-intensive, we hope that this should considerably lower the entry barrier for the community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Assessing forecast skill&lt;/h2>; &lt;p>; Together with our collaborators from &lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;, we defined a set of headline scores that best capture the quality of global weather forecasts. As the figure below shows, several of the ML-based forecasts have lower errors than the &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts&quot;>;state-of-the-art physical models&lt;/a>; on deterministic metrics. This holds for a range of variables and regions, and underlines the competitiveness and promise of ML-based approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;This scorecard shows the skill of different models compared to ECMWF&#39;s &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/changes-ecmwf-model&quot;>;Integrated Forecasting System&lt;/a>; (IFS), one of the best physics-based weather forecasts, for several variables. IFS forecasts are evaluated against IFS analysis. All other models are evaluated against ERA5. The order of ML models reflects publication date.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Toward reliable probabilistic forecasts&lt;/h2>; &lt;p>; However, a single forecast often isn&#39;t enough. Weather is inherently chaotic because of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;butterfly effect&lt;/a>;. For this reason, operational weather centers now run ~50 slightly perturbed realizations of their model, called an ensemble, to estimate the forecast probability distribution across various scenarios. This is important, for example, if one wants to know the likelihood of extreme weather. &lt;/p>; &lt;p>; Creating reliable probabilistic forecasts will be one of the next key challenges for global ML models. Regional ML models, such as Google&#39;s &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet&lt;/a>; already estimate probabilities. To anticipate this next generation of global models, WB2 already provides probabilistic metrics and baselines, among them &lt;a href=&quot;https://www.ecmwf.int/en/about/media-centre/focus/2017/fact-sheet-ensemble-weather-forecasting&quot;>;ECMWF&#39;s IFS ensemble&lt;/a>;, to accelerate research in this direction. &lt;/p>; &lt;p>; As mentioned above, weather forecasting has many aspects, and while the headline metrics try to capture the most important aspects of forecast skill, they are by no means sufficient. One example is forecast realism. Currently, many ML forecast models tend to “hedge their bets” in the face of the intrinsic uncertainty of the atmosphere. In other words, they tend to predict smoothed out fields that give lower average error but do not represent a realistic, physically consistent state of the atmosphere. An example of this can be seen in the animation below. The two data-driven models, Pangu-Weather and GraphCast (bottom), predict the large-scale evolution of the atmosphere remarkably well. However, they also have less small-scale structure compared to the ground truth or the physical forecasting model IFS HRES (top). In WB2 we include a range of these case studies and also a spectral metric that quantifies such blurring. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1020&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Forecasts of a front passing through the continental United States initialized on January 3, 2020. Maps show temperature at a pressure level of 850 &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;>;hPa&lt;/a>; (roughly equivalent to an altitude of 1.5km) and &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;>;geopotential&lt;/a>; at a pressure level of 500 hPa (roughly 5.5 km) in contours. ERA5 is the corresponding ground-truth analysis, IFS HRES is ECMWF&#39;s physics-based forecasting model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; WeatherBench 2 will continue to evolve alongside ML model development. The &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;official website&lt;/a>; will be updated with the latest state-of-the-art models. (To submit a model, please follow &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/submit.html&quot;>;these instructions&lt;/a>;). We also invite the community to provide feedback and suggestions for improvements through issues and pull requests on the &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;WB2 GitHub page&lt;/a>;. &lt;/p>; &lt;p>; Designing evaluation well and targeting the right metrics is crucial in order to make sure ML weather models benefit society as quickly as possible. WeatherBench 2 as it is now is just the starting point. We plan to extend it in the future to address key issues for the future of ML-based weather forecasting. Specifically, we would like to add station observations and better precipitation datasets. Furthermore, we will explore the inclusion of nowcasting and subseasonal-to-seasonal predictions to the benchmark. &lt;/p>; &lt;p>; We hope that WeatherBench 2 can aid researchers and end-users as weather forecasting continues to evolve. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;WeatherBench 2 is the result of collaboration across many different teams at Google and external collaborators at ECMWF. From ECMWF, we would like to thank Matthew Chantry, Zied Ben Bouallegue and Peter Dueben. From Google, we would like to thank the core contributors to the project: Stephan Rasp, Stephan Hoyer, Peter Battaglia, Alex Merose, Ian Langmore, Tyler Russell, Alvaro Sanchez, Antonio Lobato, Laurence Chiu, Rob Carver, Vivian Yang, Shreya Agrawal, Thomas Turnbull, Jason Hickey, Carla Bromberg, Jared Sisk, Luke Barrington, Aaron Bell, and Fei Sha. We also would like to thank Kunal Shah, Rahul Mahrsee, Aniket Rawat, and Satish Kumar. Thanks to John Anderson for sponsoring WeatherBench 2. Furthermore, we would like to thank Kaifeng Bi from the Pangu-Weather team and Ryan Keisler for their help in adding their models to WeatherBench 2.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6223088894812086013/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot; rel=&quot;alternate&quot; title=&quot;WeatherBench 2: A benchmark for the next generation of data-driven weather models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s72-c/weatherbenchgif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1342410539466537463&lt;/id>;&lt;published>;2023-08-30T12:34:00.002-07:00&lt;/published>;&lt;updated>;2023-08-30T12:39:33.664-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Modeling and improving text stability in live captions&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Vikas Bahirwani, Research Scientist, and Susan Xu, Software Engineer, Google Augmented Reality&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s2300/stabilizedcaptions.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Automatic speech recognition (ASR) technology has made conversations more accessible with live captions in remote conferencing software, mobile applications, and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3379337.3415817&quot;>;head-worn displays&lt;/a>;. However, to maintain real-time responsiveness, live caption systems often display interim predictions that are updated as new utterances are received. This can cause &lt;em>;text instability (&lt;/em>;a “flicker” where previously displayed text is updated, shown in the captions on the left in the video below), which can impair users&#39; reading experience due to distraction, fatigue, and difficulty following the conversation. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://research.google/pubs/pub52450/&quot;>;Modeling and Improving Text Stability in Live Captions&lt;/a>;”, presented at &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/98883&quot;>;ACM CHI 2023&lt;/a>;, we formalize this problem of text stability through a few key contributions. First, we quantify the text instability by employing a vision-based flicker metric that uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Luminance&quot;>;luminance&lt;/a>; contrast and &lt;a href=&quot;https://en.wikipedia.org/wiki/Discrete_Fourier_transform&quot;>;discrete Fourier transform&lt;/a>;. Second, we also introduce a stability algorithm to stabilize the rendering of live captions via tokenized alignment, semantic merging, and smooth animation. Finally, we conducted a user study (N=123) to understand viewers&#39; experience with live captioning&lt;strong>;. &lt;/strong>;Our statistical analysis demonstrates a strong correlation between our proposed flicker metric and viewers&#39; experience. Furthermore, it shows that our proposed stabilization techniques significantly improves viewers&#39; experience (eg, the captions on the right in the video above). &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/-hiAnT6QkmU?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;-hiAnT6QkmU&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Raw ASR captions vs. stabilized captions&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Metric&lt;/h2>; &lt;p>; Inspired by &lt;a href=&quot;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/5203/0000/Toward-perceptual-metrics-for-video-watermark-evaluation/10.1117/12.512550.full&quot;>;previous work&lt;/a>;, we propose a flicker-based metric to quantify text stability and objectively evaluate the performance of live captioning systems. Specifically, our goal is to quantify the flicker in a grayscale live caption video. We achieve this by comparing the difference in luminance between individual frames (frames in the figures below) that constitute the video. Large visual changes in luminance are obvious (eg, addition of the word “bright” in the figure on the bottom), but subtle changes (eg, update from “... this gold. Nice..” to “... this. Gold is nice”) may be difficult to discern for readers. However, converting the change in luminance to its constituting frequencies exposes both the obvious and subtle changes. &lt;/p>; &lt;p>; Thus, for each pair of contiguous frames, we convert the difference in luminance into its constituting frequencies using discrete Fourier transform. We then sum over each of the low and high frequencies to quantify the flicker in this pair. Finally, we average over all of the frame-pairs to get a per-video flicker. &lt;/p>; &lt;p>; For instance, we can see below that two identical frames (top) yield a flicker of 0, while two non-identical frames (bottom) yield a non-zero flicker. It is worth noting that higher values of the metric indicate high flicker in the video and thus, a worse user experience than lower values of the metric. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s1399/noflicker.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;268&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEippymQyO7Sdsa2EsCa2cYpD6Gtv036e5i-oqFN7tranIse6oFDGhr49hKdw1-e_cuPAMzMRnygFCh78sCy1vztBfLLlGqieWGTJT4qRV_ZeUkUvQ3aYHkIAzoAeCAJs7SIo6J2iPxv5enbjzSLgwEH1n9Bt0YIp0sVPmZI9oujvLR7pjrhzDbmPdtjKUdD/s16000/noflicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the flicker metric between two identical frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s1399/flicker.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;263&quot; data-original-width=&quot;1399&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5JPwDITfUswOjN3CVmunFfYNuBS4rQrdvs7SC2KEfu5YpsAqTl0eJCYnT22615Ho1ouiC3QUFbCqNeHPRxE1XIotPM7DZE-LA99lIKznPDXqyJvAw2SRBGlEvrw1Qyo7-ux11-7hZsDLMAA0m_1vfeYTS3GSapAAFBBQmPZHxDlzgTUzsRx811kWPJhrW/s16000/flicker.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the flicker between two non-identical frames.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Stability algorithm &lt;/h2>; &lt;p>; To improve the stability of live captions, we propose an algorithm that takes as input already rendered sequence of tokens (eg, “Previous” in the figure below) and the new sequence of ASR predictions, and outputs an updated stabilized text (eg, “Updated text (with stabilization)” below). It considers both the natural language understanding (NLU) aspect as well as the ergonomic aspect (display, layout, etc.) of the user experience in deciding when and how to produce a stable updated text. Specifically, our algorithm performs tokenized alignment, semantic merging, and smooth animation to achieve this goal. In what follows, a token is defined as a word or punctuation produced by ASR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s1995/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;1995&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyGZb1acXj87Bz13Qj_Gzw47F4gfB5-ZZ-5qYNLBUkdjmNAyIjp3tOHH4hwUIXrpKCg1G_zoZ2DvlWOd45w4_GiltlNjsU3dz82vn9qhoTJR1R_1vQB6rtZDOF9kFsJaUDHaJ7QWnKQzziGKsnfO1TK0HxWHFtI4mqkoDGkBSklE9p4_jt0FdUC9WDt_lg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show (a) the previously already rendered text, (b) the baseline layout of updated text without our merging algorithm, and (c) the updated text as generated by our stabilization algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />;&lt;p>;&lt;br />;&lt;/p>;&lt;p>;&lt;br />;&lt;/p>; &lt;p>; Our algorithm address the challenge of producing stabilized updated text by first identifying three classes of changes (highlighted in red, green, and blue below): &lt;/p>; &lt;ol type=&quot;A&quot;>; &lt;li>;Red: Addition of tokens to the end of previously rendered captions (eg, &quot;How about&#39;&#39;). &lt;/li>; &lt;li>;Green: Addition / deletion of tokens, in the middle of already rendered captions. &lt;ul>; &lt;li>;B1: Addition of tokens (eg, &quot;I&#39;&#39; and &quot;friends&#39;&#39;). These may or may not affect the overall comprehension of the captions, but may lead to layout change. Such layout changes are not desired in live captions as they cause significant jitter and poorer user experience. Here “I” does not add to the comprehension but “friends” does. Thus, it is important to balance updates with stability specially for B1 type tokens. &lt;/li>; &lt;li>;B2: Removal of tokens, eg, &quot;in&#39;&#39; is removed in the updated sentence. &lt;/li>; &lt;/ul>; &lt;/li>; &lt;li>;Blue: Re-captioning of tokens: This includes token edits that may or may not have an impact on the overall comprehension of the captions. &lt;/li>;&lt;ul>; &lt;li>;C1: Proper nouns like &quot;disney land&#39;&#39; are updated to &quot;Disneyland&#39;&#39;. &lt;/li>; &lt;li>;C2: Grammatical shorthands like &quot;it&#39;s&#39;&#39; are updated to &quot;It was&#39;&#39;. &lt;/li>; &lt;/ul>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;354&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUjTAyVIRFYHGc0wEYXbx4wXxWiW-r13LXb27HDDPUMkdgUpwZ-V-0XddMTCVNJJbiv9j5Hr5Gf7pnd7_PPe548BnxGhX7YA0caHOOhcjWVhSlAg4RDIQI9Kcg2RoSXCcsiI-04qKbsWiIS0ECKup-AnFxQGBZUg64uygZFFxQi4G1hoPayLhC8Db2aLqq/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Classes of changes between previously displayed and updated text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Alignment, merging, and smoothing &lt;/h2>; &lt;p>; To maximize text stability, our goal is to align the old sequence with the new sequence using updates that make minimal changes to the existing layout while ensuring accurate and meaningful captions. To achieve this, we leverage a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#:~:text=The%20Needleman%E2%80%93Wunsch%20algorithm%20is%20still%20widely%20used%20for%20optimal,alignments%20having%20the%20highest%20score.&quot;>;Needleman-Wunsch algorithm&lt;/a>; with dynamic programming to merge the two sequences depending on the class of tokens as defined above: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Case A tokens:&lt;/b>; We directly add case A tokens, and line breaks as needed to fit the updated captions. &lt;/li>;&lt;li>;&lt;b>;Case B tokens:&lt;/b>; Our preliminary studies showed that users preferred stability over accuracy for previously displayed captions. Thus, we only update case B tokens if the updates do not break an existing line layout. &lt;/li>;&lt;li>;&lt;b>;Case C tokens:&lt;/b>; We compare the semantic similarity of case C tokens by transforming original and updated sentences into sentence embeddings, measuring their dot-product, and updating them only if they are semantically different (similarity &amp;lt; 0.85) and the update will not cause new line breaks. &lt;/li>; &lt;/ul>; &lt;p>; Finally, we leverage animations to reduce visual jitter. We implement smooth scrolling and fading of newly added tokens to further stabilize the overall layout of the live captions. &lt;/p>; &lt;h2>;User evaluation&lt;/h2>; &lt;p>; We conducted a user study with 123 participants to (1) examine the correlation of our proposed flicker metric with viewers&#39; experience of the live captions, and (2) assess the effectiveness of our stabilization techniques. &lt;/p>; &lt;p>; We manually selected 20 videos in YouTube to obtain a broad coverage of topics including video conferences, documentaries, academic talks, tutorials, news, comedy, and more. For each video, we selected a 30-second clip with at least 90% speech. &lt;/p>; &lt;p>; We prepared four types of renderings of live captions to compare: &lt;/p>; &lt;ol>; &lt;li>;Raw ASR: raw speech-to-text results from a speech-to-text API. &lt;/li>;&lt;li>;Raw ASR + thresholding: only display interim speech-to-text result if its confidence score is higher than 0.85. &lt;/li>;&lt;li>;Stabilized captions: captions using our algorithm described above with alignment and merging. &lt;/li>;&lt;li>;Stabilized and smooth captions: stabilized captions with smooth animation (scrolling + fading) to assess whether softened display experience helps improve the user experience. &lt;/li>; &lt;/ol>; &lt;p>; We collected user ratings by asking the participants to watch the recorded live captions and rate their assessments of comfort, distraction, ease of reading, ease of following the video, fatigue, and whether the captions impaired their experience. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Correlation between flicker metric and user experience&lt;/h3>; &lt;p>; We calculated &lt;a href=&quot;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&quot;>;Spearman&#39;s coefficient&lt;/a>; between the flicker metric and each of the behavioral measurements (values range from -1 to 1, where negative values indicate a negative relationship between the two variables, positive values indicate a positive relationship, and zero indicates no relationship). Shown below, our study demonstrates statistically significant (𝑝 &amp;lt; 0.001) correlations between our flicker metric and users&#39; ratings. The absolute values of the coefficient are around 0.3, indicating a moderate relationship. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Behavioral Measurement&lt;/b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;&lt;b>;Correlation to Flickering Metric*&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Comfort &lt;/td>; &lt;td align=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Distraction &lt;/td>; &lt;td align=&quot;center&quot;>; 0.33 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Easy to read &lt;/td>; &lt;td align=&quot;center&quot;>; -0.31 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Easy to follow videos &lt;/td>; &lt;td align=&quot;center&quot;>; -0.29 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Fatigue &lt;/td>; &lt;td align=&quot;center&quot;>; 0.36 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Impaired Experience &lt;/td>; &lt;td align=&quot;center&quot;>; 0.31 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spearman correlation tests of our proposed flickering metric. *&lt;em>;p&lt;/em>; &amp;lt; 0.001.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Stabilization of live captions&lt;/h3>; &lt;p>; Our proposed technique (stabilized smooth captions) received consistently better ratings, significant as measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;>;Mann-Whitney U test&lt;/a>; (&lt;em>;p&lt;/em>; &amp;lt; 0.01 in the figure below), in five out of six aforementioned survey statements. That is, users considered the stabilized captions with smoothing to be more comfortable and easier to read, while feeling less distraction, fatigue, and impairment to their experience than other types of rendering. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s960/graph.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;387&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjaijzjVqNukm5E5IKwW-PgfMepP1F0OmuA7fLlbLa8h5qQ6O_y7TJNI4DcPnCqXbP_YT2GdRO2YoX__jJD7yYjqjaNsJ-5K9TsJBMyKDEmy8kS92ZATfAXD1UJZNMndKUXH4w2MArZ4OsklVnnJiJb6iwnFzSyPNaX3LcADBXHIsVKaDHWPpopiYe9AiUC/s16000/graph.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User ratings from 1 (Strongly Disagree) – 7 (Strongly Agree) on survey statements. (**: p&amp;lt;0.01, ***: p&amp;lt;0.001; ****: p&amp;lt;0.0001; ns: non-significant)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future direction&lt;/h2>; &lt;p>; Text instability in live captioning significantly impairs users&#39; reading experience. This work proposes a vision-based metric to model caption stability that statistically significantly correlates with users&#39; experience, and an algorithm to stabilize the rendering of live captions. Our proposed solution can be potentially integrated into existing ASR systems to enhance the usability of live captions for a variety of users, including those with translation needs or those with hearing accessibility needs. &lt;/p>; &lt;p>; Our work represents a substantial step towards measuring and improving text stability. This can be evolved to include language-based metrics that focus on the consistency of the words and phrases used in live captions over time. These metrics may provide a reflection of user discomfort as it relates to language comprehension and understanding in real-world scenarios. We are also interested in conducting &lt;a href=&quot;https://en.wikipedia.org/wiki/Eye_tracking&quot;>;eye-tracking&lt;/a>; studies (eg, videos shown below) to track viewers&#39; gaze patterns, such as eye fixation and saccades, allowing us to better understand the types of errors that are most distracting and how to improve text stability for those. &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowfullscreen=&quot;&quot; youtube-src-id=&quot;1E2jGUscWyc&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/1E2jGUscWyc?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of tracking a viewer&#39;s gaze when reading raw ASR captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe class=&quot;BLOG_video_class&quot; allowfullscreen=&quot;&quot; youtube-src-id=&quot;YfotwPIznL8&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/YfotwPIznL8?rel=0&amp;amp;&quot; frameborder=&quot;0&quot;>;&lt;/iframe>;&lt;/div>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of tracking a viewer&#39;s gaze when reading stabilized and smoothed captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 120%;&quot;>; &lt;br />; &lt;/div>; &lt;p>; By improving text stability in live captions, we can create more effective communication tools and improve how people connect in everyday conversations in familiar or, through translation, unfamiliar languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Key contributors include Xingyu “Bruce” Liu, Jun Zhang, Leonardo Ferrer, Susan Xu, Vikas Bahirwani, Boris Smus, Alex Olwal, and Ruofei Du. We wish to extend our thanks to our colleagues who provided assistance, including Nishtha Bhatia, Max Spear, and Darcy Philippon. We would also like to thank Lin Li, Evan Parker, and CHI 2023 reviewers.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1342410539466537463/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1342410539466537463&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/modeling-and-improving-text-stability.html&quot; rel=&quot;alternate&quot; title=&quot;Modeling and improving text stability in live captions&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmZKQpatrRUfrTX1UjaVW9wrHoVajjHupeWwkV45-muvTV7F1It2G37lV7OzA8aS_AKcxxNaX9AsJGfWAH5Hf5vedsp0L51VLZE-kxgUevXur_npeMsJT1GXIX_ArfCvcupT4Y8U5-8Gbzb0oiIptaxr8zd4fkk4ICy-mNmOTWXQPX7GU3cpnXoMfH9tnz/s72-c/stabilizedcaptions.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8352967842060136321&lt;/id>;&lt;published>;2023-08-29T12:57:00.001-07:00&lt;/published>;&lt;updated>;2023-08-30T17:14:40.708-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SayTap: Language to quadrupedal locomotion&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yujin Tang and Wenhao Yu, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s320/SayTap%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Simple and effective interaction between human and quadrupedal robots paves the way towards creating intelligent and capable helper robots, forging a future where technology enhances our lives in ways beyond our imagination. Key to such human-robot interaction systems is enabling quadrupedal robots to respond to natural language instructions. Recent developments in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) have demonstrated the potential to perform &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;high-level planning&lt;/a>;. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques, especially for inherently unstable legged robots, necessitating high-frequency control signals. Consequently, most &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing&lt;/a>; &lt;a href=&quot;https://code-as-policies.github.io/&quot;>;work&lt;/a>; presumes the provision of high-level APIs for LLMs to dictate robot behavior, inherently limiting the system&#39;s expressive capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://saytap.github.io&quot;>;SayTap: Language to Quadrupedal Locomotion&lt;/a>;”, to be presented at &lt;a href=&quot;https://www.corl2023.org/&quot;>;CoRL 2023&lt;/a>;, we propose an approach that uses foot contact patterns (which refer to the sequence and manner in which a four-legged agent places its feet on the ground while moving) as an interface to bridge human commands in natural language and a locomotion controller that outputs low-level commands. This results in an interactive quadrupedal robot system that allows users to flexibly craft diverse locomotion behaviors (eg, a user can ask the robot to walk, run, jump or make other movements using simple language). We contribute an LLM prompt design, a reward function, and a method to expose the SayTap controller to the feasible distribution of contact patterns. We demonstrate that SayTap is a controller capable of achieving diverse locomotion patterns that can be transferred to real robot hardware. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;SayTap method&lt;/h2>; &lt;p>; The SayTap approach uses a contact pattern template, which is a 4 X &lt;i>;T&lt;/i>; matrix of 0s and 1s, with 0s representing an agent&#39;s feet in the air and 1s for feet on the ground. From top to bottom, each row in the matrix gives the foot contact patterns of the front left (FL), front right (FR), rear left (RL) and rear right (RR) feet. SayTap&#39;s control frequency is 50 Hz, so each 0 or 1 lasts 0.02 seconds. In this work, a desired foot contact pattern is defined by a cyclic sliding window of size L&lt;sub>;w&lt;/sub>; and of shape 4 X L&lt;sub>;w&lt;/sub>;. The sliding window extracts from the contact pattern template four foot ground contact flags, which indicate if a foot is on the ground or in the air between &lt;i>;t&lt;/i>; + 1 and &lt;i>;t&lt;/i>; + L&lt;sub>;w&lt;/sub>;. The figure below provides an overview of the SayTap method. &lt;/p>; &lt;p>; SayTap introduces these desired foot contact patterns as a new interface between natural language user commands and the locomotion controller. The locomotion controller is used to complete the main task (eg, following specified velocities) and to place the robot&#39;s feet on the ground at the specified time, such that the realized foot contact patterns are as close to the desired contact patterns as possible. To achieve this, the locomotion controller takes the desired foot contact pattern at each time step as its input in addition to the robot&#39;s proprioceptive sensory data (eg, joint positions and velocities) and task-related inputs (eg, user-specified velocity commands). We use &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;>;deep reinforcement learning&lt;/a>; to train the locomotion controller and represent it as a deep neural network. During controller training, a random generator samples the desired foot contact patterns, the policy is then optimized to output low-level robot actions to achieve the desired foot contact pattern. Then at test time a LLM translates user commands into foot contact patterns. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/s935/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;935&quot; height=&quot;297&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLmdLcl2EWiO0yd0SRS1Sh5pCcaUFXZgU3owwRCNq0W7pDG3_fdHW9Z2aLrGlJ9CTOIkfI-G8jVZrYE083_U84CCOq2pGuewb4szwCvoRc8VxgClJyi0Cqv6wmf6xxzHz99tUpU80cGRzBOYWxrwyrVGLfRxcYZzcb28hItXd9xwBz7qfRGKR8H5UcOhl5/w640-h297/image13.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap approach overview.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/saytap_blog.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap uses foot contact patterns (eg, 0 and 1 sequences for each foot in the inset, where 0s are foot in the air and 1s are foot on the ground) as an interface that bridges natural language user commands and low-level control commands. With a reinforcement learning-based locomotion controller that is trained to realize the desired contact patterns, SayTap allows a quadrupedal robot to take both simple and direct instructions (eg, “Trot forward slowly.”) as well as vague user commands (eg, “Good news, we are going to a picnic this weekend!”) and react accordingly.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We demonstrate that the LLM is capable of accurately mapping user commands into foot contact pattern templates in specified formats when given properly designed prompts, even in cases when the commands are unstructured or vague. In training, we use a random pattern generator to produce contact pattern templates that are of various pattern lengths &lt;i>;T&lt;/i>;, foot-ground contact ratios within a cycle based on a given gait type &lt;i>;G&lt;/i>;, so that the locomotion controller gets to learn on a wide distribution of movements leading to better generalization. See the &lt;a href=&quot;https://arxiv.org/abs/2306.07580&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; With a simple prompt that contains only three in-context examples of commonly seen foot contact patterns, an LLM can translate various human commands accurately into contact patterns and even generalize to those that do not explicitly specify how the robot should react. &lt;/p>; &lt;p>; SayTap prompts are concise and consist of four components: (1) general instruction that describes the tasks the LLM should accomplish; (2) gait definition that reminds the LLM of basic knowledge about quadrupedal gaits and how they can be related to emotions; (3) output format definition; and (4) examples that give the LLM chances to learn in-context. We also specify five velocities that allow a robot to move forward or backward, fast or slow, or remain still. &lt;/p>; &lt;span style=&quot;font-size: small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;&lt;font color=&quot;#0000ff&quot;>; General instruction block&lt;/font>; You are a dog foot contact pattern expert. Your job is to give a velocity and a foot contact pattern based on the input. You will always give the output in the correct format no matter what the input is. &lt;font color=&quot;#0000ff&quot;>;Gait definition block&lt;/font>; The following are description about gaits: 1. Trotting is a gait where two diagonally opposite legs strike the ground at the same time. 2. Pacing is a gait where the two legs on the left/right side of the body strike the ground at the same time. 3. Bounding is a gait where the two front/rear legs strike the ground at the same time. It has a longer suspension phase where all feet are off the ground, for example, for at least 25% of the cycle length. This gait also gives a happy feeling. &lt;font color=&quot;#0000ff&quot;>;Output format definition block&lt;/font>; The following are rules for describing the velocity and foot contact patterns: 1. You should first output the velocity, then the foot contact pattern. 2. There are five velocities to choose from: [-1.0, -0.5, 0.0, 0.5, 1.0]. 3. A pattern has 4 lines, each of which represents the foot contact pattern of a leg. 4. Each line has a label. &quot;FL&quot; is front left leg, &quot;FR&quot; is front right leg, &quot;RL&quot; is rear left leg, and &quot;RR&quot; is rear right leg. 5. In each line, &quot;0&quot; represents foot in the air, &quot;1&quot; represents foot on the ground. &lt;font color=&quot;#0000ff&quot;>;Example block&lt;/font>; Input: Trot slowly Output: 0.5 FL: 11111111111111111000000000 FR: 00000000011111111111111111 RL: 00000000011111111111111111 RR: 11111111111111111000000000 Input: Bound in place Output: 0.0 FL: 11111111111100000000000000 FR: 11111111111100000000000000 RL: 00000011111111111100000000 RR: 00000011111111111100000000 Input: Pace backward fast Output: -1.0 FL: 11111111100001111111110000 FR: 00001111111110000111111111 RL: 11111111100001111111110000 RR: 00001111111110000111111111 Input: &lt;/pre>;&lt;/span>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SayTap prompt to the LLM. Texts in blue are used for illustration and are not input to LLM.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Following simple and direct commands&lt;/h3>; &lt;p>; We demonstrate in the videos below that the SayTap system can successfully perform tasks where the commands are direct and clear. Although some commands are not covered by the three in-context examples, we are able to guide the LLM to express its internal knowledge from the pre-training phase via the “Gait definition block” (see the second block in our prompt above) in the prompt. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/basic_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Following unstructured or vague commands&lt;/h3>; &lt;p>; But what is more interesting is SayTap&#39;s ability to process unstructured and vague instructions. With only a little hint in the prompt to connect certain gaits with general impressions of emotions, the robot bounds up and down when hearing exciting messages, like “We are going to a picnic!” Furthermore, it also presents the scenes accurately (eg, moving quickly with its feet barely touching the ground when told the ground is very hot). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/saytap/saytap.github.io/raw/main/assets/mp4/extended_test5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We present SayTap, an interactive system for quadrupedal robots that allows users to flexibly craft diverse locomotion behaviors. SayTap introduces desired foot contact patterns as a new interface between natural language and the low-level controller. This new interface is straightforward and flexible, moreover, it allows a robot to follow both direct instructions and commands that do not explicitly state how the robot should react. &lt;/p>; &lt;p>; One interesting direction for future work is to test if commands that imply a specific feeling will allow the LLM to output a desired gait. In the gait definition block shown in the results section above, we provide a sentence that connects a happy mood with bounding gaits. We believe that providing more information can augment the LLM&#39;s interpretations (eg, implied feelings). In our evaluation, the connection between a happy feeling and a bounding gait led the robot to act vividly when following vague human commands. Another interesting direction for future work is to introduce multi-modal inputs, such as videos and audio. Foot contact patterns translated from those signals will, in theory, still work with our pipeline and will unlock many more interesting use cases. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust and Tatsuya Harada conducted this research. This work was conceived and performed while the team was in Google Research and will be continued at Google DeepMind. The authors would like to thank Tingnan Zhang, Linda Luu, Kuang-Huei Lee, Vincent Vanhoucke and Douglas Eck for their valuable discussions and technical support in the experiments.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8352967842060136321/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8352967842060136321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot; rel=&quot;alternate&quot; title=&quot;SayTap: Language to quadrupedal locomotion&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWzjJBM7DUieoZyN9KN5N5ta17-mXVFjbz-eOS4dJPZM9W0yC9OHL4f-ng2BMjF3v62w-X5cMFN1bzv5PTEJTG5KdOrmhySUpQqM01aevHn1JyP9VJxYSvio46dX78aBg3tDn_rXKs7I1e_nXntWcEeGePLGRRbvGz8TK-FZnvm-tRfXtxOHWHaGqcHaMI/s72-c/SayTap%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-781660063956467509&lt;/id>;&lt;published>;2023-08-28T09:59:00.003-07:00&lt;/published>;&lt;updated>;2023-08-29T01:37:19.871-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s1100/RO-ViT-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection task&lt;/a>; (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniques such as &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-text pre-training&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;knowledge distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.09106&quot;>;pseudo labeling&lt;/a>;, and frozen models, often employing &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) backbones, have been proposed. With the growing popularity of &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;>;vision transformers&lt;/a>; (ViTs), it is important to explore their potential for building proficient open-vocabulary detectors. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The existing approaches assume the availability of pre-trained &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;vision-language models&lt;/a>; (VLMs) and focus on fine-tuning or &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;distillation&lt;/a>; from these models to address the disparity between image-level pre-training and object-level fine-tuning. However, as VLMs are primarily designed for image-level tasks like classification and retrieval, they do not fully leverage the concept of objects or regions during the pre-training phase. Thus, it could be beneficial for open-vocabulary detection if we build locality information into the image-text pre-training. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.07011&quot;>;RO-ViT: Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>;”, presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce a simple method to pre-train vision transformers in a region-aware manner to improve open-vocabulary detection. In vision transformers, positional embeddings are added to image patches to encode information about the spatial position of each patch within the image. Standard pre-training typically uses full-image positional embeddings, which does not generalize well to detection tasks. Thus, we propose a new positional embedding scheme, called “cropped positional embedding”, that better aligns with the use of region crops in detection fine-tuning. In addition, we replace the &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function#Neural_networks&quot;>;softmax cross entropy loss&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;>;focal loss&lt;/a>; in contrastive image-text learning, allowing us to learn from more challenging and informative examples. Finally, we leverage recent advances in novel object proposals to enhance open-vocabulary detection fine-tuning, which is motivated by the observation that existing methods often miss novel objects during the proposal stage due to overfitting to foreground categories. We are also releasing the code &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm/rovit&quot;>;here&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Region-aware image-text pre-training&lt;/h2>; &lt;p>; Existing VLMs are trained to match an image as a whole to a text description. However, we observe there is a mismatch between the way the positional embeddings are used in the existing contrastive pre-training approaches and open-vocabulary detection. The positional embeddings are important to transformers as they provide the information of where each element in the set comes from. This information is often useful for downstream recognition and localization tasks. Pre-training approaches typically apply full-image positional embeddings during training, and use the same positional embeddings for downstream tasks, eg, zero-shot recognition. However, the recognition occurs at region-level for open-vocabulary detection fine-tuning, which requires the full-image positional embeddings to generalize to regions that they never see during the pre-training. &lt;/p>; &lt;p>; To address this, we propose &lt;em>;cropped positional embeddings&lt;/em>; (CPE). With CPE, we upsample positional embeddings from the image size typical for pre-training, eg, 224x224 pixels, to that typical for detection tasks, eg, 1024x1024 pixels. Then we randomly crop and resize a region, and use it as the image-level positional embeddings during pre-training. The position, scale, and aspect ratio of the crop is randomly sampled. Intuitively, this causes the model to view an image not as a full image in itself, but as a region crop from some larger unknown image. This better matches the downstream use case of detection where recognition occurs at region- rather than image-level. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s952/RO-ViT-img6.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;952&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjh720RAiQLlaXc5jlv5hPp7qnNXPXyEV8bUc6GNKJd9dckzmgvusKgIggqPP8NXvvbUb55TzSDP-gAdhl4gIq0CxXZqTJq4heQruWCeaQsM5uY2LKiO91-n7fPkUtnW2cYg3YP4iKC520pLXWNNG-iDQk80xsadhW-qTytYg44DWYu379-BaDczwm_vGoE/s16000/RO-ViT-img6.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;For the pre-training, we propose&amp;nbsp;&lt;em>;cropped positional embedding&lt;/em>;&amp;nbsp;(CPE) which randomly crops and resizes a region of positional embeddings instead of using the whole-image positional embedding (PE). In addition, we use focal loss instead of the common softmax cross entropy loss for contrastive learning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also find it beneficial to learn from hard examples with a focal loss. Focal loss enables finer control over how hard examples are weighted than what the softmax cross entropy loss can provide. We adopt the focal loss and replace it with the softmax cross entropy loss in both image-to-text and text-to-image losses. Both CPE and focal loss introduce no extra parameters and minimal computation costs. &lt;/p>; &lt;br />; &lt;h2>;Open-vocabulary detector fine-tuning&lt;/h2>; &lt;p>; An open-vocabulary detector is trained with the detection labels of &#39;base&#39; categories, but needs to detect the union of &#39;base&#39; and &#39;novel&#39; (unlabeled) categories at test time. Despite the backbone features pre-trained from the vast open-vocabulary data, the added detector layers (neck and heads) are newly trained with the downstream detection dataset. Existing approaches often miss novel/unlabeled objects in the object proposal stage because the proposals tend to classify them as background. To remedy this, we leverage recent advances in a novel object proposal method and adopt the localization quality-based objectness (ie, &lt;a href=&quot;https://arxiv.org/abs/2108.06753&quot;>;centerness&lt;/a>; score) instead of object-or-not binary classification score, which is combined with the detection score. During training, we compute the detection scores for each detected region as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; between the region&#39;s embedding (computed via &lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks&quot;>;RoI-Align&lt;/a>; operation) and the text embeddings of the base categories. At test time, we append the text embeddings of novel categories, and the detection score is now computed with the union of the base and novel categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s682/RO-ViT-img4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;682&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpulnBZgXTo37R7jq14m8vdkKd0xQIeSAqBF5mleY1EAMd1Uu1IY-Sf8cMhTlp-iIR56umGVirxfwtpNFeEpaCJw7MCZE0IsOhdkt8ny6ipDfFi4BxNOgYdmrP4Vsw874IF5US7uDBrOSwP7J2yYemDmJqp4q8E4TXnLoT0r0xZpAu2dI-YBskOGZKPvNo/s16000/RO-ViT-img4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The pre-trained ViT backbone is transferred to the downstream open-vocabulary detection by replacing the global average pooling with detector heads. The&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks#:~:text=new%20method%20called-,ROIAlign,-%2C%20which%20can%20represent&quot; style=&quot;text-align: left;&quot;>;RoI-Align&lt;/a>;&amp;nbsp;embeddings are matched with the cached category embeddings to obtain the VLM score, which is combined with the detection score into the open-vocabulary detection score.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate RO-ViT on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, our best model achieves 33.6 box&amp;nbsp;&lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>;&amp;nbsp;on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;AP&lt;sub>;r&lt;/sub>;&lt;/a>;) and 32.1 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask AP&lt;sub>;r&lt;/sub>;&lt;/a>;, which outperforms&amp;nbsp;the best existing ViT-based approach OWL-ViT by 8.0 AP&lt;sub>;r&lt;/sub>;&amp;nbsp;and the best CNN-based approach ViLD-Ens by 5.8 mask AP&lt;sub>;r&lt;/sub>;. It also exceeds the performance of many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s1200/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJwMkd6yjqUGQQwd3AvfjVXHMO1Fqm1nJ10dCRe1YwArbbOihweKhq0ITBAhdDliZvaRxlYel-0Y3ovMWmY_Mq-BEQPNDs5PwmvwugHGGwTp7jQHrll2CF-MIRibhJ9u4CTihtnhb_HS4Gn01prYhJgAkV7YYFCeuEec_N9EIv7X3vM_STQv9w52zmcAcM/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT outperforms both the state-of-the-art (SOTA)&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2205.06230&quot; style=&quot;text-align: left;&quot;>;ViT-based&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot; style=&quot;text-align: left;&quot;>;CNN-based&lt;/a>;&amp;nbsp;methods on LVIS open-vocabulary detection benchmark. We show mask AP on rare categories (AP&lt;sub>;r&lt;/sub>;) , except for SOTA ViT-based (OwL-ViT) where we show box AP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Apart from evaluating region-level representation through open-vocabulary detection, we evaluate the image-level representation of RO-ViT in image-text retrieval through the &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1505.04870&quot;>;Flickr30K&lt;/a>; benchmarks. Our model with 303M ViT outperforms the state-of-the-art CoCa model with 1B ViT on MS COCO, and is on par on Flickr30K. This shows that our pre-training method not only improves the region-level representation but also the global image-level representation for retrieval. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s1692/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvv3eObc3ZSKGIygKA7leAfFIrVUx4EXmvx3O17d4TA1Gf1qLAOPRBQ0euAWH6mZAkmTmZBXVXy6s2NqESWDlbGpeRKVrwp3_wXzf8KGqaY50rK3fLcWtP-rfi1gDRiWkhuVDKVr1QqOGumfyJV-GrK5yI7XH0xFlrWXZISsQzAYpBK9wTdP0JX4b36s0o/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show zero-shot image-text retrieval on MS COCO and Flickr30K benchmarks, and compare with dual-encoder methods. We report recall@1 (top-1 recall) on image-to-text (I2T) and text-to-image (T2I) retrieval tasks. RO-ViT outperforms the state-of-the-art&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot; style=&quot;text-align: left;&quot;>;CoCa&lt;/a>;&amp;nbsp;with the same backbone.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s1476/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;362&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhh_nm9hEfD99tDp8Jtzx7DVwdsylNNAdBFGx-JMmj8EJGf1JeNd3BboFEPmf0lxbHCizm_vTGqlCYlal0PZYV2rJiFfI7jUZdtKIYBg3Dr9uUJA_UvRhQ9M9HBzT-03RPv3ZhGm7rj86AxOYqQH1KWYHkUqHyMPU_lx9wGBWX-0nYS_ICu9hRlGYPCBxjk/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RO-ViT open-vocabulary detection on LVIS. We only show the novel categories for clarity. RO-ViT detects many novel categories that it has never seen during detection training: “fishbowl”, “sombrero”, “persimmon”, “gargoyle”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualization of positional embeddings&lt;/h2>; &lt;p>; We visualize and compare the learned positional embeddings of RO-ViT with the baseline. Each tile is the cosine similarity between positional embeddings of one patch and all other patches. For example, the tile in the top-left corner (marked in red) visualizes the similarity between the positional embedding of the location (row=1, column=1) and those positional embeddings of all other locations in 2D. The brightness of the patch indicates how close the learned positional embeddings of different locations are. RO-ViT forms more distinct clusters at different patch locations showing symmetrical global patterns around the center patch. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANdQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s1686/RO-ViT-img1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1686&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqfeYO7FKWB_ZyoOpvwArOkfRn77jYWBJ7X1_wVdjZQRVdeXJKDTtaGwBpR6XbvK7L6_OgcDWEwESYAbXMevRwStwANdQkmi6f2w62aXNhkEu3daexyXV5F9wNYvZubp1hQE9oh3P602suQr4KOKcRT1f5Jr8yluYSe2QfA9h6uLSZEWB4hiwu24z6kttD/s16000/RO-ViT-img1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Each tile shows the cosine similarity between the positional embedding of the patch (at the indicated row-column position) and the positional embeddings of all other patches. ViT-B/16 backbone is used.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present RO-ViT, a contrastive image-text pre-training framework to bridge the gap between image-level pre-training and open-vocabulary detection fine-tuning. Our methods are simple, scalable, and easy to apply to any contrastive backbones with minimal computation overhead and no increase in parameters. RO-ViT achieves the state-of-the-art on LVIS open-vocabulary detection benchmark and on the image-text retrieval benchmarks, showing the learned representation is not only beneficial at region-level but also highly effective at the image-level. We hope this study can help the research on open-vocabulary detection from the perspective of image-text pre-training which can benefit both region-level and image-level tasks. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Dahun Kim, Anelia Angelova, and Weicheng Kuo conducted this work and are now at Google DeepMind. We would like to thank our colleagues at Google Research for their advice and helpful discussions. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/781660063956467509/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/781660063956467509&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wOjJcc9-JUy0J6NEo8aRgBIeiHRY6YdneL3pBlAF4GszMf6MctGLuZG5ZClFHqMGK9j_RpgF-M2AvcScwa98FwLHtEt1rC7HCiSPhnNpG0podsHDn8uKlh9fVuIj5xYGUFytZWHkE4pANrDnXLknL-7_FTTEYVtL2MVR-DMwREMdxi3TeGZKw1OcLiPI/s72-c/RO-ViT-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3964459643854458124&lt;/id>;&lt;published>;2023-08-25T10:38:00.003-07:00&lt;/published>;&lt;updated>;2023-08-25T10:46:59.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Perception Fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Susanna Ricco and Utsav Prabhu, co-leads, Perception Fairness Team, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI research&lt;/a>; is built on a foundation of collaboration — between teams with diverse backgrounds and expertise, between researchers and product developers, and ultimately with the community at large. The Perception Fairness team drives progress by combining deep subject-matter expertise in both computer vision and machine learning (ML) fairness with direct connections to the researchers building the perception systems that power products across Google and beyond. Together, we are working to intentionally design our systems to be inclusive from the ground up, guided by &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/s948/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;214&quot; data-original-width=&quot;948&quot; height=&quot;144&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfN1jwbJve1vMavRMJkmG6JXejOEdb4irf3KcYGnf-_UdciQRy_gXI0D6x9afEZLjCZwb9C0VfyXbvhuckpTonH8ghjgAJ7yDZsc0OlEOznCZlNg_OQxYacKcwDJSMkWPm8Xc_EROheeAsCYFVyYsigqA7Ydfnl9k5rB6erVkYpL7MqBpdeqIJm9H5sani/w640-h144/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Perception Fairness research spans the design, development, and deployment of advanced multimodal models including the latest foundation and generative models powering Google&#39;s products.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our team&#39;s mission is to advance the frontiers of fairness and inclusion in multimodal ML systems, especially related to &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;foundation&lt;/a>; models and &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>;. This encompasses core technology components including classification, localization, captioning, retrieval, visual question answering, text-to-image or text-to-video generation, and generative image and video editing. We believe that fairness and inclusion can and should be top-line performance goals for these applications. Our research is focused on unlocking novel analyses and mitigations that enable us to proactively design for these objectives throughout the development cycle. We answer core questions, such as: How can we use ML to responsibly and faithfully model human perception of demographic, cultural, and social identities in order to promote fairness and inclusion? What kinds of system biases (eg, underperforming on images of people with certain skin tones) can we measure and how can we use these metrics to design better algorithms? How can we build more inclusive algorithms and systems and react quickly when failures occur? &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Measuring representation of people in media&lt;/h2>; &lt;p>; ML systems that can edit, curate or create images or videos can affect anyone exposed to their outputs, shaping or reinforcing the beliefs of viewers around the world. Research to reduce representational harms, such as reinforcing stereotypes or denigrating or erasing groups of people, requires a deep understanding of both the content and the &lt;a href=&quot;https://ai.googleblog.com/2023/07/using-societal-context-knowledge-to.html&quot;>;societal context&lt;/a>;. It hinges on how different observers perceive themselves, their communities, or how others are represented. There&#39;s considerable debate in the field regarding which social categories should be studied with computational tools and how to do so responsibly. Our research focuses on working toward scalable solutions that are informed by sociology and social psychology, are aligned with human perception, embrace the subjective nature of the problem, and enable nuanced measurement and mitigation. One example is our research on &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;differences in human perception and annotation of skin tone in images&lt;/a>; using the &lt;a href=&quot;skintone.google&quot;>;Monk Skin Tone scale&lt;/a>;. &lt;/p>; &lt;p>; Our tools are also used to study representation in large-scale content collections. Through our Media Understanding for Social Exploration (MUSE) project, we&#39;ve partnered with academic researchers, nonprofit organizations, and major consumer brands to understand patterns in mainstream media and advertising content. We first published this work in 2017, with a co-authored study analyzing &lt;a href=&quot;https://about.google/intl/ALL_au/main/gender-equality-films/&quot;>;gender equity in Hollywood movies&lt;/a>;. Since then, we&#39;ve increased the scale and depth of our analyses. In 2019, we released findings based on &lt;a href=&quot;https://www.thinkwithgoogle.com/feature/diversity-inclusion/&quot;>;over 2.7 million YouTube advertisements&lt;/a>;. In the &lt;a href=&quot;https://blog.google/technology/ai/using-ai-to-study-12-years-of-representation-in-tv/&quot;>;latest study&lt;/a>;, we examine representation across intersections of perceived gender presentation, perceived age, and skin tone in over twelve years of popular US television shows. These studies provide insights for content creators and advertisers and further inform our own research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/s800/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEik91aneLGhfJIcDFuL3gNFmsmobl70nJlIJzSW5rbkIhlJ2eTzLnUKQpInQKK4YqzaKzmdgF6BUisBMqRtHdVDHp8QpPtS8ONz02Nrgn4If7YOukbw0txEfy1IpP2kBAXyKik4_P4-z6OEss8v7p4p7uVsBTJfppODRfOHbVwWSO3cRbJo1Uhcg5XJKePG/w640-h360/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration (not actual data) of computational signals that can be analyzed at scale to reveal representational patterns in media collections. [Video Collection / Getty Images]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Moving forward, we&#39;re expanding the ML fairness concepts on which we focus and the domains in which they are responsibly applied. Looking beyond photorealistic images of people, we are working to develop tools that model the representation of communities and cultures in illustrations, abstract depictions of humanoid characters, and even images with no people in them at all. Finally, we need to reason about not just who is depicted, but how they are portrayed — what narrative is communicated through the surrounding image content, the accompanying text, and the broader cultural context. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Analyzing bias properties of perceptual systems&lt;/h2>; &lt;p>; Building advanced ML systems is complex, with multiple stakeholders informing various criteria that decide product behavior. Overall quality has historically been defined and measured using summary statistics (like overall accuracy) over a test dataset as a proxy for user experience. But not all users experience products in the same way. &lt;br />; &lt;/p>; &lt;p>; Perception Fairness enables practical measurement of nuanced system behavior beyond summary statistics, and makes these metrics core to the system quality that directly informs product behaviors and launch decisions. This is often much harder than it seems. Distilling complex bias issues (eg, disparities in performance across intersectional subgroups or instances of stereotype reinforcement) to a small number of metrics without losing important nuance is extremely challenging. Another challenge is balancing the interplay between fairness metrics and other product metrics (eg, user satisfaction, accuracy, latency), which are often phrased as conflicting despite being compatible. It is common for researchers to describe their work as optimizing an &quot;accuracy-fairness&quot; tradeoff when in reality widespread user satisfaction is aligned with meeting fairness and inclusion objectives. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s640/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJ9E6V3m19znCwF7YOqmwzykyZzHKs-SPwCwoEovEkPtYqJssYhLpTqBwSJWtoUUXhIRdtg-qUO63FnTr4iBu2yPn_wK23Z8PdYkeEmycLRJ0hsNFlikIpXmBoY9QWI1K-gFN4guIgLqFQcRatK1fo-6iDHBTTxN2efQraT32zYvfJ3Me0lfvToMq8oRdW/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We built and released the &lt;a href=&quot;https://storage.googleapis.com/openimages/web/extended.html#miap&quot;>;MIAP dataset&lt;/a>; as part of &lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;Open Images&lt;/a>;, leveraging our research on perception of socially relevant concepts and detection of biased behavior in complex systems to create a resource that furthers ML fairness research in computer vision. Original photo credits — left: &lt;a href=&quot;https://www.flickr.com/photos/boston_public_library/8242010414/&quot;>;Boston Public Library&lt;/a>;; middle: &lt;a href=&quot;https://www.flickr.com/photos/jenrobinson/20183915655/&quot;>;jen robinson&lt;/a>;; right: &lt;a href=&quot;https://www.flickr.com/photos/mrgarin/2484859086/&quot;>;Garin Fons&lt;/a>;; all used with permission under the &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;>;CC- BY 2.0 license&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To these ends, our team focuses on two broad research directions. First, democratizing access to well-understood and widely-applicable fairness analysis tooling, engaging partner organizations in adopting them into product workflows, and informing leadership across the company in interpreting results. This work includes developing broad benchmarks, &lt;a href=&quot;https://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html&quot;>;curating widely-useful high-quality test datasets&lt;/a>; and tooling centered around techniques such as sliced analysis and counterfactual testing — often building on the core representation signals work described earlier. Second, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3461702.3462557&quot;>;advancing novel approaches&lt;/a>; towards fairness analytics — including partnering with &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;product efforts&lt;/a>; that may result in breakthrough findings or &lt;a href=&quot;https://services.google.com/fh/files/blogs/bsr-google-cr-api-hria-executive-summary.pdf&quot;>;inform launch strategy&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Advancing AI responsibly&lt;/h2>; &lt;p>; Our work does not stop with analyzing model behavior. Rather, we use this as a jumping-off point for identifying algorithmic improvements in collaboration with other researchers and engineers on product teams. Over the past year we&#39;ve launched upgraded components that power Search and &lt;a href=&quot;https://blog.google/products/photos/google-photos-memories-view/&quot;>;Memories&lt;/a>; features in Google Photos, leading to more consistent performance and drastically improving robustness through added layers that keep mistakes from cascading through the system. We are working on improving ranking algorithms in Google Images to diversify representation. We updated algorithms that may reinforce historical stereotypes, using additional signals responsibly, such that it&#39;s more likely for &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;everyone to see themselves reflected in Search results and find what they&#39;re looking for&lt;/a>;. &lt;/p>; &lt;p>; This work naturally carries over to the world of generative AI, where &lt;a href=&quot;https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text/&quot;>;models can create collections of images or videos seeded from image and text prompts&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;can answer questions about images and videos&lt;/a>;. We&#39;re excited about the potential of these technologies to &lt;a href=&quot;https://blog.google/products/shopping/ai-virtual-try-on-google-shopping/&quot;>;deliver new experiences to users&lt;/a>; and as tools to further our own research. To enable this, we&#39;re collaborating across the research and responsible AI communities to develop guardrails that mitigate failure modes. We&#39;re leveraging our tools for understanding representation to power scalable benchmarks that can be combined with human feedback, and investing in research from pre-training through deployment to steer the models to generate higher quality, more inclusive, and more controllable output. We want these models to inspire people, producing diverse outputs, translating concepts without relying on tropes or stereotypes, and providing consistent behaviors and responses across counterfactual variations of prompts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Opportunities and ongoing work&lt;/h2>; &lt;p>; Despite over a decade of focused work, the field of perception fairness technologies still seems like a nascent and fast-growing space, rife with opportunities for breakthrough techniques. We continue to see opportunities to contribute technical advances backed by interdisciplinary scholarship. The gap between what we can measure in images versus the underlying aspects of human identity and expression is large — closing this gap will require increasingly complex media analytics solutions. Data metrics that indicate true representation, situated in the appropriate context and heeding a diversity of viewpoints, remains an open challenge for us. Can we reach a point where we can reliably identify depictions of nuanced stereotypes, continually update them to reflect an ever-changing society, and discern situations in which they could be offensive? Algorithmic advances driven by human feedback point a promising path forward. &lt;/p>; &lt;p>; Recent focus on AI safety and ethics in the context of modern large model development has spurred new ways of thinking about measuring systemic biases. We are exploring multiple avenues to use these models — along with recent developments in concept-based explainability methods, causal inference methods, and cutting-edge UX research — to quantify and minimize undesired biased behaviors. We look forward to tackling the challenges ahead and developing technology that is built for everybody. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank every member of the Perception Fairness team, and all of our collaborators.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3964459643854458124/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3964459643854458124&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Perception Fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvTD0aWY1G5CHtuxdtUEk965xABcjRdBuilg_78JFVxqDTqLqgtyNAypbqx0PoBPsduY1Jf3MOHdsoPXm3T8gSCzvtQwyeNcwG5fxlX3-CSzNAHIjJe8K0EfkL34wX_S8NjwdtD81fX9FRGHck2KBM1GtQrP1-inoVXdrU1ZkgBMe_ZVdXPNTRyW5m92te/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6350776943545232437&lt;/id>;&lt;published>;2023-08-24T15:10:00.000-07:00&lt;/published>;&lt;updated>;2023-08-24T15:10:57.268-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;How to compare a noisy quantum processor to a classical computer&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sergio Boixo and Vadim Smelyanskiy, Principal Scientists, Google Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; A full-scale error-corrected quantum computer will be able to solve some problems that are impossible for classical computers, but building such a device is a huge endeavor. We are proud of the &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestones&lt;/a>; that we have achieved toward a fully error-corrected quantum computer, but that large-scale computer is still some number of years away. Meanwhile, we are using our current noisy quantum processors as flexible platforms for quantum experiments. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In contrast to an error-corrected quantum &lt;em>;computer&lt;/em>;, experiments in noisy quantum &lt;em>;processors&lt;/em>; are currently limited to a few thousand quantum operations or gates, before noise degrades the quantum state. In 2019 we implemented a specific computational task called &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1666-5&quot;>;random circuit sampling&lt;/a>; on our quantum processor &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;and showed&lt;/a>; for the first time that it outperformed state-of-the-art classical supercomputing. &lt;/p>; &lt;p>; Although they have not yet reached beyond-classical capabilities, we have also used our processors to observe novel physical phenomena, such as &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04257-w&quot;>;time crystals&lt;/a>; and &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;Majorana edge modes&lt;/a>;, and have made new experimental discoveries, such as robust &lt;a href=&quot;https://ai.googleblog.com/2022/12/formation-of-robust-bound-states-of.html&quot;>;bound states&lt;/a>; of interacting photons and the &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abq5769&quot;>;noise-resilience&lt;/a>; of Majorana edge modes of Floquet evolutions. &lt;/p>; &lt;p>; We expect that even in this intermediate, noisy regime, we will find applications for the quantum processors in which useful quantum experiments can be performed much faster than can be calculated on classical supercomputers — we call these &quot;computational applications&quot; of the quantum processors. No one has yet demonstrated such a beyond-classical computational application. So as we aim to achieve this milestone, the question is: What is the best way to compare a quantum experiment run on such a quantum processor to the computational cost of a classical application? &lt;/p>; &lt;p>; We already know how to compare an error-corrected quantum algorithm to a classical algorithm. In that case, the field of &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity&quot;>;computational complexity&lt;/a>; tells us that we can compare their respective computational costs — that is, the number of operations required to accomplish the task. But with our current experimental quantum processors, the situation is not so well defined. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments&lt;/a>;”, we provide a framework for measuring the computational cost of a quantum experiment, introducing the experiment&#39;s “effective quantum volume”, which is the number of quantum operations or gates that contribute to a measurement outcome. We apply this framework to evaluate the computational cost of three recent experiments: our &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;random circuit sampling&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;experiment&lt;/a>;, our&lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>; experiment measuring quantities known as “out of time order correlators” (OTOCs)&lt;/a>;, and a &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;recent experiment on a Floquet evolution&lt;/a>; related to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Ising_model&quot;>;Ising model&lt;/a>;. We are particularly excited about OTOCs because they provide a direct way to experimentally measure the effective quantum volume of a circuit (a sequence of quantum gates or operations), which is itself a computationally difficult task for a classical computer to estimate precisely. OTOCs are also important in &lt;a href=&quot;https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance&quot;>;nuclear magnetic resonance&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Electron_paramagnetic_resonance&quot;>;electron spin resonance spectroscopy&lt;/a>;. Therefore, we believe that OTOC experiments are a promising candidate for a first-ever computational application of quantum processors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/s1280/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;594&quot; data-original-width=&quot;1280&quot; height=&quot;297&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4EN3ksTWx9Rau9WMZRrnsuYn6h68Gsj8F1jUve1pevj3fzc-FaFlYWlYeNSOnNbbfAl_2ndEbYIsyfiwyafv9Kgd6VsOOMzDaexufiJs_8oyo1G37h2lr4Y1Y28zD7lZ3OIB_EL_LsY-ZR7XwHsmTnDiKoIzL3-bEhfYxJNmWlRY_Ms0XVfvh5G3jlZrn/w640-h297/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Plot of computational cost and impact of some recent quantum experiments. While some (eg, &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;QC-QMC 2022&lt;/a>;) have had high impact and others (eg, &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04351-z&quot;>;RCS 2023&lt;/a>;) have had high computational cost, none have yet been both useful and hard enough to be considered a “computational application.” We hypothesize that our future OTOC experiment could be the first to pass this threshold. Other experiments plotted are referenced in the text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Random circuit sampling: Evaluating the computational cost of a noisy circuit&lt;/h2>; &lt;p>; When it comes to running a quantum circuit on a noisy quantum processor, there are two competing considerations. On one hand, we aim to do something that is difficult to achieve classically. The computational cost — the number of operations required to accomplish the task on a classical computer — depends on the quantum circuit&#39;s &lt;em>;effective quantum volume&lt;/em>;: the larger the volume, the higher the computational cost, and the more a quantum processor can outperform a classical one. &lt;/p>; &lt;p>; But on the other hand, on a noisy processor, each quantum gate can introduce an error to the calculation. The more operations, the higher the error, and the lower the fidelity of the quantum circuit in measuring a quantity of interest. Under this consideration, we might prefer simpler circuits with a smaller effective volume, but these are easily simulated by classical computers. The balance of these competing considerations, which we want to maximize, is called the &quot;computational resource&quot;, shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/s973/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;653&quot; data-original-width=&quot;973&quot; height=&quot;430&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRNGqSxHGAUo5SLktuvIeZLR0k4cav-msWvm8cu1SFbXjcqKe1D9_XMzH7XYdIXdMwaGXC4UHuzhAfV7EJKaD8Z3RPTU1wOEPS4TwK55cMxJmg19mQD7ZCtuu_8MDMW2mrtSPDJove8k96tquIEErm8_O5KIvZCT2Goh15cuCSMIOnsPoPQ008pLWdNRsX/w640-h430/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Graph of the tradeoff between quantum volume and noise in a quantum circuit, captured in a quantity called the “computational resource.” For a noisy quantum circuit, this will initially increase with the computational cost, but eventually, noise will overrun the circuit and cause it to decrease. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; We can see how these competing considerations play out in a simple &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;“hello world” program&lt;/a>; for quantum processors, known as random circuit sampling (RCS), which was the first demonstration of a quantum processor outperforming a classical computer. Any error in any gate is likely to make this experiment fail. Inevitably, this is a hard experiment to achieve with significant fidelity, and thus it also serves as a benchmark of system fidelity. But it also corresponds to the highest known computational cost achievable by a quantum processor. We recently reported the &lt;a href=&quot;https://arxiv.org/abs/2304.11119&quot;>;most powerful RCS&lt;/a>; experiment performed to date, with a low measured experimental fidelity of 1.7x10&lt;sup>;-3&lt;/sup>;, and a high theoretical computational cost of ~10&lt;sup>;23&lt;/sup>;. These quantum circuits had 700 two-qubit gates. We estimate that this experiment would take ~47 years to simulate in the world&#39;s largest supercomputer. While this checks one of the two boxes needed for a computational application — it outperforms a classical supercomputer — it is not a particularly useful application &lt;em>;per se&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;OTOCs and Floquet evolution: The effective quantum volume of a local observable&lt;/h2>; &lt;p>; There are many open questions in quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Many-body_problem&quot;>;many-body physics&lt;/a>; that are classically intractable, so running some of these experiments on our quantum processor has great potential. We typically think of these experiments a bit differently than we do the RCS experiment. Rather than measuring the quantum state of all qubits at the end of the experiment, we are usually concerned with more specific, local physical observables. Because not every operation in the circuit necessarily impacts the observable, a local observable&#39;s effective quantum volume might be smaller than that of the full circuit needed to run the experiment. &lt;/p>; &lt;p>; We can understand this by applying the concept of a light cone from &lt;a href=&quot;https://en.wikipedia.org/wiki/Special_relativity&quot;>;relativity&lt;/a>;, which determines which events in space-time can be causally connected: some events cannot possibly influence one another because information takes time to propagate between them. We say that two such events are outside their respective light cones. In a quantum experiment, we replace the light cone with something called a “butterfly cone,” where the growth of the cone is determined by the butterfly speed — the speed with which information spreads throughout the system. (This speed is characterized by measuring OTOCs, discussed later.) The effective quantum volume of a local observable is essentially the volume of the butterfly cone, including only the quantum operations that are causally connected to the observable. So, the faster information spreads in a system, the larger the effective volume and therefore the harder it is to simulate classically. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/s1392/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1392&quot; data-original-width=&quot;1092&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjevGHNHtp7l4QiiXQTZJa-W3Sh2Xr_8FuTWhMC27HJanlYgqBS24MPby4l_mYmTb5Tla8VwzoURvkU8KSWSOA_7IIBtozY-WpDN34wUfig-rtH1NC7vKqRO4Q7ffFFxn5aizuEMiwSzNTYV62dQ3LZCiNNQ3P5qy_ProATnnwJ9hAT-QPrptKluwIenis7/w502-h640/image2.png&quot; width=&quot;502&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A depiction of the effective volume V&lt;sub>;eff&lt;/sub>; of the gates contributing to the local observable B. A related quantity called the effective area A&lt;sub>;eff&lt;/sub>; is represented by the cross-section of the plane and the cone. The perimeter of the base corresponds to the front of information travel that moves with the butterfly velocity v&lt;sub>;B&lt;/sub>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; We apply this framework to a recent &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06096-3&quot;>;experiment&lt;/a>; implementing a so-called Floquet Ising model, a physical model related to the time crystal and Majorana experiments. From the data of this experiment, one can directly estimate an effective fidelity of 0.37 for the largest circuits. With the measured gate error rate of ~1%, this gives an estimated effective volume of ~100. This is much smaller than the light cone, which included two thousand gates on 127 qubits. So, the butterfly velocity of this experiment is quite small. Indeed, we argue that the effective volume covers only ~28 qubits, not 127, using numerical simulations that obtain a larger precision than the experiment. This small effective volume has also been &lt;a href=&quot;https://arxiv.org/abs/2306.17839&quot;>;corroborated&lt;/a>; with the OTOC technique. Although this was a deep circuit, the estimated computational cost is 5x10&lt;sup>;11&lt;/sup>;, almost one trillion times less than the recent RCS experiment. Correspondingly, this experiment can be &lt;a href=&quot;https://arxiv.org/abs/2306.15970&quot;>;simulated&lt;/a>; in less than a second per data point on a single A100 GPU. So, while this is certainly a useful application, it does not fulfill the second requirement of a computational application: substantially outperforming a classical simulation. &lt;/p>; &lt;p>; Information scrambling experiments with OTOCs are a promising avenue for a computational application. OTOCs can tell us important physical information about a system, such as the butterfly velocity, which is critical for precisely measuring the effective quantum volume of a circuit. OTOC experiments with fast entangling gates offer a potential path for a first beyond-classical demonstration of a computational application with a quantum processor. Indeed, in our &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abg5029&quot;>;experiment&lt;/a>; from 2021 we achieved an effective fidelity of F&lt;sub>;eff &lt;/sub>;~ 0.06 with an experimental signal-to-noise ratio of ~1, corresponding to an effective volume of ~250 gates and a computational cost of 2x10&lt;sup>;12&lt;/sup>;. &lt;/p>; &lt;p>; While these early OTOC experiments are not sufficiently complex to outperform classical simulations, there is a deep physical reason why OTOC experiments are good candidates for the first demonstration of a computational application. Most of the interesting quantum phenomena accessible to near-term quantum processors that are hard to simulate classically correspond to a quantum circuit exploring many, many quantum energy levels. Such evolutions are typically chaotic and standard time-order correlators (TOC) decay very quickly to a purely random average in this regime. There is no experimental signal left. This does not happen for &lt;a href=&quot;https://arxiv.org/abs/2101.08870&quot;>;OTOC measurements&lt;/a>;, which allows us to grow complexity at will, only limited by the error per gate. We anticipate that a reduction of the error rate by half would double the computational cost, pushing this experiment to the beyond-classical regime. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Using the effective quantum volume framework we have developed, we have determined the computational cost of our RCS and OTOC experiments, as well as a recent Floquet evolution experiment. While none of these meet the requirements yet for a computational application, we expect that with improved error rates, an OTOC experiment will be the first beyond-classical, useful application of a quantum processor. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6350776943545232437/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6350776943545232437&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html&quot; rel=&quot;alternate&quot; title=&quot;How to compare a noisy quantum processor to a classical computer&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1m1RqJqSczNGfbF05c8xU86oE8qjQSUVctpJVz3H_FvmW-ebQI9kxslYLp_CwAGoN4ve4RIndX2qsEcLuEDPnWZFZ4uDrqzyPVw-Kl10Aol6C1UQM4b2YXMwJ7BwXuc42U2EV9nPUQ-UkRfEoVmvqM9yHsMINGFibYWWvhVj2yDHJMTymEs8RQoszIYvu/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3589347607411181063&lt;/id>;&lt;published>;2023-08-24T12:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-24T12:33:37.720-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Teaching language models to reason algorithmically&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hattie Zhou, Graduate Student at MILA, Hanie Sedghi, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs), such as &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have shown impressive progress in recent years, which have been driven by &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;scaling up models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;>;training data sizes&lt;/a>;. Nonetheless, a long standing debate has been whether LLMs can reason symbolically (ie, manipulating symbols based on logical rules). For example, LLMs are able to perform simple arithmetic operations when numbers are small, but struggle to perform with large numbers. This suggests that LLMs have not learned the underlying rules needed to perform these arithmetic operations. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While neural networks have powerful &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;>;pattern matching capabilities&lt;/a>;, they are prone to overfitting to spurious statistical patterns in the data. This does not hinder good performance when the training data is large and diverse and the evaluation is in-distribution. However, for tasks that require rule-based reasoning (such as addition), LLMs struggle with out-of-distribution generalization as spurious correlations in the training data are often much easier to exploit than the true rule-based solution. As a result, despite significant progress in a variety of natural language processing tasks, performance on simple arithmetic tasks like addition has remained a challenge. Even with modest improvement of &lt;a href=&quot;https://openai.com/research/gpt-4&quot;>;GPT-4&lt;/a>; on the &lt;a href=&quot;https://arxiv.org/abs/2103.03874&quot;>;MATH&lt;/a>; dataset, &lt;a href=&quot;https://arxiv.org/abs/2303.12712&quot;>;errors are still largely due to arithmetic and calculation mistakes&lt;/a>;. Thus, an important question is whether LLMs are capable of algorithmic reasoning, which involves solving a task by applying a set of abstract rules that define the algorithm. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;Teaching Algorithmic Reasoning via In-Context Learning&lt;/a>;”, we describe an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; to enable algorithmic reasoning capabilities in LLMs. In-context learning refers to a model&#39;s ability to perform a task after seeing a few examples of it within the context of the model. The task is specified to the model using a prompt, without the need for weight updates. We also present a novel algorithmic prompting technique that enables general purpose language models to achieve strong &lt;a href=&quot;https://arxiv.org/abs/2207.04901&quot;>;generalization&lt;/a>; on arithmetic problems that are more difficult than those seen in the prompt. Finally, we demonstrate that a model can reliably execute algorithms on out-of-distribution examples with an appropriate choice of prompting strategy. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/s1200/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200&quot; height=&quot;89&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEin8actGxFN2C2mvGMhV_fSzN4Cta1Yd84uvVO5fRO2RcMjzrPY1t-V0mJ2u0x1s0zpCW3bKLX-_3kF4twqNyQMMTrmlTeOvfVdwS6iMYabwgE_RVPe_PFKM6-JBdGS1B8WyMmiVLRalfhD-mN4c-CE4ZXQNhL-Q8pP3q-uy_Xt6i7VkZCGiKqA97AGFnlB/w640-h89/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By providing algorithmic prompts, we can teach a model the rules of arithmetic via in-context learning. In this example, the LLM (word predictor) outputs the correct answer when prompted with an easy addition question (eg, 267+197), but fails when asked a similar addition question with longer digits. However, when the more difficult question is appended with an algorithmic prompt for addition (blue box with white +&lt;strong>; &lt;/strong>;shown below the word predictor), the model is able to answer correctly. Moreover, the model is capable of simulating the multiplication algorithm (&lt;strong>;X&lt;/strong>;) by composing a series of addition calculations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Teaching an algorithm as a skill&lt;/h2>; &lt;p>; In order to teach a model an algorithm as a skill, we develop algorithmic prompting, which builds upon other rationale-augmented approaches (eg, &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>;). Algorithmic prompting extracts algorithmic reasoning abilities from LLMs, and has two notable distinctions compared to other prompting approaches: (1) it solves tasks by outputting the steps needed for an algorithmic solution, and (2) it explains each algorithmic step with sufficient detail so there is no room for misinterpretation by the LLM. &lt;/p>; &lt;p>; To gain intuition for algorithmic prompting, let&#39;s consider the task of two-number addition. In a scratchpad-style prompt, we process each digit from right to left and keep track of the carry value (ie, we add a 1 to the next digit if the current digit is greater than 9) at each step. However, the rule of carry is ambiguous after seeing only a few examples of carry values. We find that including explicit equations to describe the rule of carry helps the model focus on the relevant details and interpret the prompt more accurately. We use this insight to develop an algorithmic prompt for two-number addition, where we provide explicit equations for each step of computation and describe various indexing operations in non-ambiguous formats. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/s1584/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1250&quot; data-original-width=&quot;1584&quot; height=&quot;505&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhTx1CTb79qa87h3q9BoJK8uSMy4UwAFZW6BBKL5qvtui-uZnNXh87gqi7rJHxVilfynkyKzuA6Il6QKef-UuC260vMcydrNuFCf60hwF2I02ey9hAMo50gc7ESc_zbaj1-sUr-nviGOb2I-7k0qNSLEqfyJuOY5mLTkDzy14YMJ32U5mkQT2Y85drKXIVr/w640-h505/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of various prompt strategies for addition.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using only three prompt examples of addition with answer length up to five digits, we evaluate performance on additions of up to 19 digits. Accuracy is measured over 2,000 total examples sampled uniformly over the length of the answer. As shown below, the use of algorithmic prompts maintains high accuracy for questions significantly longer than what&#39;s seen in the prompt, which demonstrates that the model is indeed solving the task by executing an input-agnostic algorithm. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s1600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;548&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAqquZ03AeyJeoVQbxpVmz2_jQuPNs3RevVc3XWIJ2ilPiA2JTBBnX84z1cwd0_YBq9wDNzu03SDZmUt3vQqffZjl3520HbWTF7lHR3ggiztdynfDh-O_D8YgIZfONlMlBldcfUpfuWGqxT7_MEuncQUmYyoQw3sTWXtbESzh2PkOyNsTRzdyatAOaNIua/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy on addition questions of increasing length for different prompting methods. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Leveraging algorithmic skills as tool use&lt;/h2>; &lt;p>; To evaluate if the model can leverage algorithmic reasoning in a broader reasoning process, we evaluate performance using grade school math word problems (&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;GSM8k&lt;/a>;). We specifically attempt to replace addition calculations from GSM8k with an algorithmic solution. &lt;/p>; &lt;p>; Motivated by context length limitations and possible interference between different algorithms, we explore a strategy where differently-prompted models interact with one another to solve complex tasks. In the context of GSM8k, we have one model that specializes in informal mathematical reasoning using &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain-of-thought prompting&lt;/a>;, and a second model that specializes in addition using &lt;a href=&quot;https://arxiv.org/abs/2211.09066&quot;>;algorithmic prompting&lt;/a>;. The informal mathematical reasoning model is prompted to output specialized tokens in order to call on the addition-prompted model to perform the arithmetic steps. We extract the queries between tokens, send them to the addition-model and return the answer to the first model, after which the first model continues its output. We evaluate our approach using a difficult problem from the GSM8k (GSM8k-Hard), where we randomly select 50 addition-only questions and increase the numerical values in the questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/s1268/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;330&quot; data-original-width=&quot;1268&quot; height=&quot;167&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtjcHJT3_qPVpO5VhPSH0PVILowl-e7yauZXuNuLo_1AXupvC66MM2PGTszpx5_TpFsLxikySH0nGuzfNQcuOcPZBfkWF2Ly7Z358pLdKWhyblfikvvxf1SaX1MmPnW9Y4Qxgiy71Xq4tIV27YtnrSY2EZ9aBS4KoC3lCRRrZDKAZgzYBJaM0n9Qz2hbi7/w640-h167/An%20example%20from%20the%20GSM8k-Hard%20dataset.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example from the GSM8k-Hard dataset. The chain-of-thought prompt is augmented with brackets to indicate when an algorithmic call should be performed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We find that using separate contexts and models with specialized prompts is an effective way to tackle GSM8k-Hard. Below, we observe that the performance of the model with algorithmic call for addition is 2.3x the chain-of-thought baseline. Finally, this strategy presents an example of solving complex tasks by facilitating interactions between LLMs specialized to different skills via in-context learning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/s716/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;192&quot; data-original-width=&quot;716&quot; height=&quot;108&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEgloMAxK3oPVxq6_zsDlZbJPtdaOc06-3x-AHLxNBnNqPvsgi535Pud4DJ9NlW1Jj_9QSi1lkV0pyOoBWUb4C7vxIOTQtRNYpLHim6CL-DrH0Q4KV6E_7b9GRcTeZhRBV_VcZyZeQLOXjjxEdef6Ahf_ea73jOn1Lj20_C8w8WlV_vmZAw8Ul4e3yt4u/w400-h108/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Chain-of-thought (CoT) performance on GSM8k-Hard with or without algorithmic call.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an approach that leverages &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; and a novel algorithmic prompting technique to unlock algorithmic reasoning abilities in LLMs. Our results suggest that it may be possible to transform longer context into better reasoning performance by providing more detailed explanations. Thus, these findings point to the ability of using or otherwise simulating long contexts and generating more informative rationales as promising research directions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Behnam Neyshabur, Azade Nova, Hugo Larochelle and Aaron Courville for their valuable contributions to the paper and great feedback on the blog. We thank Tom Small for creating the animations in this post. This work was done during Hattie Zhou&#39;s internship at Google Research.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3589347607411181063/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3589347607411181063&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot; rel=&quot;alternate&quot; title=&quot;Teaching language models to reason algorithmically&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAcRJnal11QtGWoPisWMdALAc6RjoHACiQOfXBIBDnG5Vx_bZ2nS9KJKFfPrq_n_pDArbmBOVQG7UIr8cNo96aFqEVWUGN-2e0aXVIylHIfr4ZMKXkGRI_BsuhVm-xrpWSJTWJ_Cg8h5Vmfqr79R8E4cSazK6d2UHEOxCG49qM0uRW7uL5RwwWgpxPy0ci/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6641308922483151364&lt;/id>;&lt;published>;2023-08-22T11:47:00.004-07:00&lt;/published>;&lt;updated>;2023-08-24T10:34:33.888-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Language to rewards for robotic skill synthesis&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Wenhao Yu and Fei Xia, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s320/Language%20to%20reward.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Empowering end-users to interactively teach robots to perform novel tasks is a crucial capability for their successful integration into real-world applications. For example, a user may want to teach a robot dog to perform a new trick, or teach a manipulator robot how to organize a lunch box based on user preferences. The recent advancements in &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;large language models&lt;/a>; (LLMs) pre-trained on extensive internet data have shown a promising path towards achieving this goal. Indeed, researchers have explored diverse ways of leveraging LLMs for robotics, from &lt;a href=&quot;https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html&quot;>;step-by-step planning&lt;/a>; and &lt;a href=&quot;https://interactive-language.github.io/&quot;>;goal-oriented dialogue&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2022/11/robots-that-write-their-own-code.html&quot;>;robot-code-writing agents&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While these methods impart new modes of compositional generalization, they focus on using language to link together new behaviors from an &lt;a href=&quot;https://sites.research.google/palm-saycan&quot;>;existing library of control primitives&lt;/a>; that are either manually engineered or learned &lt;em>;a priori&lt;/em>;. Despite having internal knowledge about robot motions, LLMs struggle to directly output low-level robot commands due to the limited availability of relevant training data. As a result, the expression of these methods are bottlenecked by the breadth of the available primitives, the design of which often requires extensive expert knowledge or massive data collection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.08647&quot;>;Language to Rewards for Robotic Skill Synthesis&lt;/a>;”, we propose an approach to enable users to teach robots novel actions through natural language input. To do so, we leverage reward functions as an interface that bridges the gap between language and low-level robot actions. We posit that reward functions provide an ideal interface for such tasks given their richness in semantics, modularity, and interpretability. They also provide a direct connection to low-level policies through black-box optimization or reinforcement learning (RL). We developed a language-to-reward system that leverages LLMs to translate natural language user instructions into reward-specifying code and then applies &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; to find optimal low-level robot actions that maximize the generated reward function. We demonstrate our language-to-reward system on a variety of robotic control tasks in simulation using a quadruped robot and a dexterous manipulator robot. We further validate our method on a physical robot manipulator. &lt;/p>; &lt;p>; The language-to-reward system consists of two core components: (1) a Reward Translator, and (2) a Motion Controller&lt;em>;. &lt;/em>;The&lt;em>; &lt;/em>;Reward Translator maps natural language instruction from users to reward functions represented as &lt;a href=&quot;https://en.wikipedia.org/wiki/Python_(programming_language)&quot;>;python code&lt;/a>;. The Motion Controller optimizes the given reward function using &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;receding horizon optimization&lt;/a>; to find the optimal low-level robot actions, such as the amount of torque that should be applied to each robot motor. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s720/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;405&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz218pDru9Wyk0pj7T-jiPvQJj9XC3ivxJuiPpH4phIIei9x4U7VaE1KRHV0M48fQSYIoyaxpzo2Yyz7y-nO65Udb8AKFowN3JPungwPuu5ORwpUqG3B4hdTwHdkSttTtgNazORo9H0p64i7CIs4iRTyHCdCVXOxB_8AXOcjgGdNxKA-LP2nGFWjr0WkZh/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLMs cannot directly generate low-level robotic actions due to lack of data in pre-training dataset. We propose to use reward functions to bridge the gap between language and low-level robot actions, and enable novel complex robot motions from natural language instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Reward Translator: Translating user instructions to reward functions&lt;/h2>; &lt;p>; The Reward Translator module was built with the goal of mapping natural language user instructions to reward functions. Reward tuning is highly domain-specific and requires expert knowledge, so it was not surprising to us when we found that LLMs trained on generic language datasets are unable to directly generate a reward function for a specific hardware. To address this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;in-context learning&lt;/a>; ability of LLMs. Furthermore, we split the Reward Translator into two sub-modules: &lt;em>;Motion Descriptor&lt;/em>; and &lt;em>;Reward Coder&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion Descriptor&lt;/h3>; &lt;p>; First, we design a Motion Descriptor that interprets input from a user and expands it into a natural language description of the desired robot motion following a predefined template. This Motion Descriptor turns potentially ambiguous or vague user instructions into more specific and descriptive robot motions, making the reward coding task more stable. Moreover, users interact with the system through the motion description field, so this also provides a more interpretable interface for users compared to directly showing the reward function. &lt;/p>; &lt;p>; To create the Motion Descriptor, we use an LLM to translate the user input into a detailed description of the desired robot motion. We design &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_motion_descriptor.txt&quot;>;prompts&lt;/a>; that guide the LLMs to output the motion description with the right amount of details and format. By translating a vague user instruction into a more detailed description, we are able to more reliably generate the reward function with our system. This idea can also be potentially applied more generally beyond robotics tasks, and is relevant to &lt;a href=&quot;https://innermonologue.github.io/&quot;>;Inner-Monologue&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought prompting&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reward Coder &lt;/h3>; &lt;p>; In the second stage, we use the same LLM from Motion Descriptor for Reward Coder, which translates generated motion description into the reward function. Reward functions are represented using python code to benefit from the LLMs&#39; knowledge of reward, coding, and code structure. &lt;/p>; &lt;p>; Ideally, we would like to use an LLM to directly generate a reward function &lt;i>;R&lt;/i>; (&lt;i>;s&lt;/i>;, &lt;i>;t&lt;/i>;) that maps the robot state &lt;i>;s&lt;/i>; and time &lt;i>;t&lt;/i>; into a scalar reward value. However, generating the correct reward function from scratch is still a challenging problem for LLMs and correcting the errors requires the user to understand the generated code to provide the right feedback. As such, we pre-define a set of reward terms that are commonly used for the robot of interest and allow LLMs to composite different reward terms to formulate the final reward function. To achieve this, we design a &lt;a href=&quot;https://language-to-reward.github.io/assets/prompts/quadruped_reward_coder.txt&quot;>;prompt&lt;/a>; that specifies the reward terms and guide the LLM to generate the correct reward function for the task.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s1293/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;1293&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhor0tAvZBd0I28_73ICjmzbRyYaJj6UEht-H7MEMUSG9Bssg4CQY4vt6KpqD27_VosI3z4Rh0Xj8GXUod3lpXyR4N9x69w7Y4ykeH23Au7JXGD7fM417UcoWDVBBt8ZJ6_BnlaxdS9X0l9YtdAMo6xBKqBN5yuSv6La3CLYk614lxqOJBcszqqED7bf7hL/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The internal structure of the Reward Translator, which is tasked to map user inputs to reward functions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Motion Controller: Translating reward functions to robot actions&lt;/h2>; &lt;p>; The Motion Controller takes the reward function generated by the Reward Translator and synthesizes a controller that maps robot observation to low-level robot actions. To do this, we formulate the controller synthesis problem as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP), which can be solved using different strategies, including RL, &lt;a href=&quot;https://en.wikipedia.org/wiki/Trajectory_optimization&quot;>;offline trajectory optimization&lt;/a>;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Model_predictive_control&quot;>;model predictive control&lt;/a>; (MPC). Specifically, we use an open-source implementation based on the &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;MuJoCo MPC&lt;/a>; (MJPC). &lt;/p>; &lt;p>; MJPC has demonstrated the interactive creation of diverse behaviors, such as legged locomotion, grasping, and finger-gaiting, while supporting multiple planning algorithms, such as &lt;a href=&quot;https://homes.cs.washington.edu/~todorov/papers/TodorovACC05.pdf&quot;>;iterative linear–quadratic–Gaussian&lt;/a>; (iLQG) and &lt;a href=&quot;https://arxiv.org/abs/2212.00541&quot;>;predictive sampling&lt;/a>;. More importantly, the frequent re-planning in MJPC empowers its robustness to uncertainties in the system and enables an interactive motion synthesis and correction system when combined with LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples &lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Robot dog&lt;/h3>; &lt;p>; In the first example, we apply the language-to-reward system to a simulated quadruped robot and teach it to perform various skills. For each skill, the user will provide a concise instruction to the system, which will then synthesize the robot motion by using reward functions as an intermediate interface. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_quadruped_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Dexterous manipulator&lt;/h3>; &lt;p>; We then apply the language-to-reward system to a dexterous manipulator robot to perform a variety of manipulation tasks. The dexterous manipulator has 27 degrees of freedom, which is very challenging to control. Many of these tasks require manipulation skills beyond grasping, making it difficult for pre-designed primitives to work. We also include an example where the user can interactively instruct the robot to place an apple inside a drawer. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/sim/all_manipulation_videos.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Validation on real robots&lt;/h2>; &lt;p>; We also validate the language-to-reward method using a real-world manipulation robot to perform tasks such as picking up objects and opening a drawer. To perform the optimization in Motion Controller, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/ARTag&quot;>;AprilTag&lt;/a>;, a fiducial marker system, and &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;F-VLM&lt;/a>;, an open-vocabulary object detection tool, to identify the position of the table and objects being manipulated. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;yes&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://language-to-reward.github.io/videos/real/l2r_demo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we describe a new paradigm for interfacing an LLM with a robot through reward functions, powered by a low-level model predictive control tool, MuJoCo MPC. Using reward functions as the interface enables LLMs to work in a semantic-rich space that plays to the strengths of LLMs, while ensuring the expressiveness of the resulting controller. To further improve the performance of the system, we propose to use a structured motion description template to better extract internal knowledge about robot motions from LLMs. We demonstrate our proposed system on two simulated robot platforms and one real robot for both locomotion and manipulation tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our co-authors Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, and Yuval Tassa for their help and support in various aspects of the project. We would also like to acknowledge Ken Caluwaerts, Kristian Hartikainen, Steven Bohez, Carolina Parada, Marc Toussaint, and the teams at Google DeepMind for their feedback and contributions.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6641308922483151364/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6641308922483151364&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot; rel=&quot;alternate&quot; title=&quot;Language to rewards for robotic skill synthesis&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacqpwjLAyYeUGIRPNXYoXb6Ph_d3WB9nQqOHqKZLMY2YvK42G5-LILRyP5YWW7oPVxSyKGO8d-RjWO-k4XYF9elHZ_G_NkAUFRRNFDvLJh1s3_1iTNeyC4B9CZUztROlYv6kYA7RYxNZnFwQrFdHJdHGZyzMF09L5igS_He1A31m4ZNvTU9V0upGzaRiw/s72-c/Language%20to%20reward.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6990000750711940626&lt;/id>;&lt;published>;2023-08-21T00:33:00.002-07:00&lt;/published>;&lt;updated>;2023-08-21T00:35:57.579-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Interspeech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at Interspeech 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s1075/Interspeech2023-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the 24th Annual &lt;a href=&quot;https://interspeech2023.org/&quot;>;Conference of the International Speech Communication Association&lt;/a>; (INTERSPEECH 2023) is being held in Dublin, Ireland, representing one of the world&#39;s most extensive conferences on research and technology of spoken language understanding and processing. Experts in speech-related research fields gather to take part in oral presentations and poster sessions and to build collaborations across the globe. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to be a &lt;a href=&quot;https://interspeech2023.org/sponsorship-exhibition/sponsors/&quot;>;Platinum Sponsor&lt;/a>; of INTERSPEECH 2023, where we will be showcasing more than 20 research publications and supporting a number of workshops and special sessions. We welcome in-person attendees to drop by the Google Research booth to meet our researchers and participate in Q&amp;amp;As and demonstrations of some of our latest speech technologies, which help to improve accessibility and provide convenience in communication for billions of users. In addition, online attendees are encouraged to visit our &lt;a href=&quot;http://topia.io/google-research&quot;>;virtual booth in Topia&lt;/a>; where you can get up-to-date information on research and opportunities at Google. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). You can also learn more about the Google research being presented at INTERSPEECH 2023 below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; ISCA Board, Technical Committee Chair: &lt;strong>;&lt;em>;Bhuvana Ramabhadran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Area Chairs include: &lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Analysis of Speech and Audio Signals: &lt;strong>;&lt;em>;Richard Rose&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Speech Synthesis and Spoken Language Generation: &lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>;&lt;br />; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Special Areas: &lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Satellite events&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/competition2023.html&quot;>;VoxCeleb Speaker Recognition Challenge 2023&lt;/a>; (VoxSRC-23)&lt;br />; Organizers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ssw2023.org/&quot;>;ISCA Speech Synthesis Workshop&lt;/a>; (SSW12)&lt;br />; Speakers include: &lt;strong>;&lt;em>;Rob Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Keynote talk – ISCA Medalist&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/narayanan23_interspeech.pdf&quot;>;Bridging Speech Science and Technology — Now and Into the Future&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Shrikanth Narayanan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Survey Talk&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Speech Compression in the AI Era&lt;br />; Speaker: &lt;strong>;&lt;em>;Jan Skoglund&lt;/em>;&lt;br />;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Special session papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rose23_interspeech.pdf&quot;>;Cascaded Encoders for Fine-Tuning ASR Models on Overlapped Speech&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Richard Rose&lt;/b>;, &lt;b>;Oscar Chang&lt;/b>;, &lt;b>;Olivier Siohan&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/erdogan23_interspeech.pdf&quot;>;TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Hakan Erdogan&lt;/b>;, &lt;b>;Scott Wisdom&lt;/b>;, Xuankai Chang*, &lt;b>;Zalán Borsos&lt;/b>;, &lt;b>;Marco Tagliasacchi&lt;/b>;, &lt;b>;Neil Zeghidour&lt;/b>;, &lt;b>;John R. Hershey&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/liang23d_interspeech.pdf&quot;>;DeePMOS: Deep Posterior Mean-Opinion-Score of Speech&lt;/a>;&lt;br />; &lt;em>;Xinyu Liang, Fredrik Cumlin,&lt;strong>; Christian Schüldt&lt;/strong>;, Saikat Chatterjee&lt;strong>;&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/baskar23_interspeech.pdf&quot;>;O-1: Self-Training with Oracle and 1-Best Hypothesis&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Murali Karthick Baskar&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Bhuvana Ramabhadran&lt;/b>;, &lt;b>;Kartik Audhkhasi&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/huo23b_interspeech.pdf&quot;>;Re-investigating the Efficient Transfer Learning of Speech Foundation Model Using Feature Fusion Methods&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zhouyuan Huo&lt;/b>;, &lt;b>;Khe Chai Sim&lt;/b>;, &lt;b>;Dongseong Hwang&lt;/b>;, &lt;b>;Tsendsuren Munkhdalai&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;,&lt;b>; Pedro Moreno&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/camp23_interspeech.pdf&quot;>;MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Joshua Camp&lt;/b>;, &lt;b>;Tom Kenter&lt;/b>;, &lt;b>;Lev Finkelstein&lt;/b>;, &lt;b>;Rob Clark&lt;/b>;&lt;/em>;&lt;br />; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/gong23c_interspeech.pdf&quot;>;LanSER: Language-Model Supported Speech Emotion Recognition&lt;/a>;&lt;br />; &lt;em>;Taesik Gong,&lt;strong>; Josh Belanich&lt;/strong>;, &lt;strong>;Krishna Somandepalli&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Brian Eoff&lt;/strong>;, &lt;strong>;Brendan Jou&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/li23fa_interspeech.pdf&quot;>;Modular Domain Adaptation for Conformer-Based Streaming ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Qiujia Li&lt;/b>;, &lt;b>;Bo Li&lt;/b>;, &lt;b>;Dongseong Hwang&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, &lt;b>;Pedro M. Mengibar&lt;/b>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/panchapagesan23_interspeech.pdf&quot;>;On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Sankaran Panchapagesan&lt;/b>;, &lt;b>;Turaj Zakizadeh Shabestary&lt;/b>;, &lt;b>;Arun Narayanan&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/eisenstein23_interspeech.pdf&quot;>;MD3: The Multi-dialect Dataset of Dialogues&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Jacob Eisenstein&lt;/b>;, &lt;b>;Vinodkumar Prabhakaran&lt;/b>;, &lt;b>;Clara Rivera&lt;/b>;, Dorottya Demszky, Devyani Sharma&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/wu23e_interspeech.pdf&quot;>;Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Zelin Wu&lt;/b>;, &lt;b>;Tsendsuren Munkhdalai&lt;/b>;, &lt;b>;Pat Rondon&lt;/b>;, &lt;b>;Golan Pundak&lt;/b>;, &lt;b>;Khe Chai Sim&lt;/b>;, &lt;b>;Christopher Li&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/blau23_interspeech.pdf&quot;>;Using Text Injection to Improve Recognition of Personal Identifiers in Speech&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yochai Blau&lt;/b>;, &lt;b>;Rohan Agrawal&lt;/b>;, &lt;b>;Lior Madmony&lt;/b>;, &lt;b>;Gary Wang&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Zhehuai Chen&lt;/b>;, &lt;b>;Zorik Gekhman&lt;/b>;, &lt;b>;Genady Beryozkin&lt;/b>;, &lt;b>;Parisa Haghani&lt;/b>;, &lt;b>;Bhuvana Ramabhadran&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/chen23j_interspeech.pdf&quot;>;How to Estimate Model Transferability of Pre-trained Speech Models?&lt;/a>;&lt;br />; &lt;em>;Zih-Ching Chen, Chao-Han Huck Yang*, &lt;strong>;Bo Li&lt;/strong>;, &lt;strong>;Yu Zhang&lt;/strong>;, &lt;strong>;Nanxin Chen&lt;/strong>;, &lt;strong>;Shuo-yiin Chang&lt;/strong>;, &lt;strong>;Rohit Prabhavalkar, &lt;/strong>;Hung-yi Lee, &lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/peyser23_interspeech.pdf&quot;>;Improving Joint Speech-Text Representations Without Alignment&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Cal Peyser&lt;/b>;, &lt;b>;Zhong Meng&lt;/b>;, &lt;b>;Ke Hu&lt;/b>;, &lt;b>;Rohit Prabhavalkar&lt;/b>;, &lt;b>;Andrew Rosenberg&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, Michael Picheny, Kyunghyun Cho &lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/bijwadia23_interspeech.pdf&quot;>;Text Injection for Capitalization and Turn-Taking Prediction in Speech Models&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shaan Bijwadia&lt;/b>;, &lt;b>;Shuo-yiin Chang&lt;/b>;, &lt;b>;Weiran Wang&lt;/b>;, &lt;b>;Zhong Meng&lt;/b>;, &lt;b>;Hao Zhang&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23_interspeech.pdf&quot;>;Streaming Parrotron for On-Device Speech-to-Speech Conversion&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Fadi Biadsy&lt;/b>;, &lt;b>;Xia Zhang&lt;/b>;, &lt;b>;Liyang Jiang&lt;/b>;, &lt;b>;Phoenix Meadowlark&lt;/b>;, &lt;b>;Shivani Agrawal&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/huang23b_interspeech.pdf&quot;>;Semantic Segmentation with Bidirectional Language Models Improves Long-Form ASR&lt;/a>;&lt;br />; &lt;strong>;W. Ronny Huang&lt;/strong>;, &lt;strong>;Hao Zhang&lt;/strong>;, &lt;strong>;Shankar Kumar&lt;/strong>;, &lt;strong>;Shuo-yiin Chang&lt;/strong>;, &lt;strong>;Tara N. Sainath&lt;br />;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/taguchi23_interspeech.pdf&quot;>;Universal Automatic Phonetic Transcription into the International Phonetic Alphabet&lt;/a>;&lt;br />; &lt;em>;Chihiro Taguchi, Yusuke Sakai,&lt;strong>; Parisa Haghani&lt;/strong>;, David Chiang&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/hu23c_interspeech.pdf&quot;>;Mixture-of-Expert Conformer for Streaming Multilingual ASR&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Ke Hu&lt;/b>;, &lt;b>;Bo Li&lt;/b>;, &lt;b>;Tara N. Sainath&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;, &lt;b>;Francoise Beaufays&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23c_interspeech.pdf&quot;>;Real Time Spectrogram Inversion on Mobile Phone&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Marco Tagliasacchi&lt;/b>;, &lt;b>;Yunpeng Li&lt;/b>;, &lt;b>;Liyang Jiang&lt;/b>;, &lt;b>;Xia Zhang&lt;/b>;, &lt;b>;Fadi Biadsy&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/rybakov23b_interspeech.pdf&quot;>;2-Bit Conformer Quantization for Automatic Speech Recognition&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Oleg Rybakov&lt;/b>;, &lt;b>;Phoenix Meadowlark&lt;/b>;, &lt;b>;Shaojin Ding&lt;/b>;, &lt;b>;David Qiu&lt;/b>;, &lt;b>;Jian Li&lt;/b>;, &lt;b>;David Rim&lt;/b>;, &lt;b>;Yanzhang He&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/koizumi23_interspeech.pdf&quot;>;LibriTTS-R: A Restored Multi-speaker Text-to-Speech Corpus&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yuma Koizumi&lt;/b>;, &lt;b>;Heiga Zen&lt;/b>;, &lt;b>;Shigeki Karita&lt;/b>;, &lt;b>;Yifan Ding&lt;/b>;, Kohei Yatabe, &lt;b>;Nobuyuki Morioka&lt;/b>;, &lt;b>;Michiel Bacchiani&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;, &lt;b>;Wei Han&lt;/b>;, &lt;b>;Ankur Bapna&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/yu23_interspeech.pdf&quot;>;PronScribe: Highly Accurate Multimodal Phonemic Transcription from Speech and Text&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Yang Yu&lt;/b>;, Matthew Perez*, &lt;b>;Ankur Bapna&lt;/b>;, &lt;b>;Fadi Haik&lt;/b>;, &lt;b>;Siamak Tazari&lt;/b>;, &lt;b>;Yu Zhang&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2023/vashishth23_interspeech.pdf&quot;>;Label Aware Speech Representation Learning for Language Identification&lt;/a>;&lt;br />; &lt;em>;&lt;b>;Shikhar Vashishth&lt;/b>;, &lt;b>;Shikhar Bharadwaj&lt;/b>;, &lt;b>;Sriram Ganapathy&lt;/b>;, &lt;b>;Ankur Bapna&lt;/b>;, &lt;b>;Min Ma&lt;/b>;, &lt;b>;Wei Han&lt;/b>;, &lt;b>;Vera Axelrod&lt;/b>;, &lt;b>;Partha Talukdar&lt;/b>;&lt;br />;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6990000750711940626/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google-at-interspeech-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6990000750711940626&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/google-at-interspeech-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at Interspeech 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ9tgKAAGviknbBlcGujLQ0tdvvu9tyvwCGble7iZ8jXSHMhYPEUI0rRiX0PwzFSK0Kx-vO6ByrqK20v_qhxE3xE6W0P4tGPQdFGx3OeZJVLOR-_Erh0-0qIE9YWLHuJiKjB5nXOpiPmTxg7Y4sT_m2WXn-uqPvZ7r9iySvCqfgo756D25jyw66KtFOEbi/s72-c/Interspeech2023-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-362086873015740792&lt;/id>;&lt;published>;2023-08-18T11:28:00.004-07:00&lt;/published>;&lt;updated>;2023-08-18T11:30:51.167-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Autonomous visual information seeking with large language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s2500/AVIS.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There has been great progress towards adapting large language models (LLMs) to accommodate multimodal inputs for tasks including &lt;a href=&quot;https://huggingface.co/docs/transformers/main/tasks/image_captioning#:~:text=Image%20captioning%20is%20the%20task,by%20describing%20images%20to%20them.&quot;>;image captioning&lt;/a>;, &lt;a href=&quot;https://huggingface.co/tasks/visual-question-answering&quot;>;visual question answering (VQA)&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open vocabulary recognition&lt;/a>;. Despite such achievements, current state-of-the-art visual language models (VLMs) perform inadequately on visual information seeking datasets, such as &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>;, where external knowledge is required to answer the questions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;528&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifyNe-vfM669M0AsGp7RYUoWVVyt5-AQrDA34CTde4zp5CgFzaqQqiNmnxcNz3AbvKpoBXoBywipTwYlkZkjzjIilpgLPaJvSpmMLaApLNmkGqH1GHgvzmHZ2w2S5Ku-hCubbW62wZIwuhKTn2R9S5OlrBkyF2ylkU8APoVAqaagGiRc5l3u05g6qag9mU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of visual information seeking queries where external knowledge is required to answer the question. Images are taken from the OK-VQA dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs//2306.08129&quot;>;AVIS: Autonomous Visual Information Seeking with Large Language Models&lt;/a>;”, we introduce a novel method that achieves state-of-the-art results on visual information seeking tasks. Our method integrates LLMs with three types of tools: (i) computer vision tools for extracting visual information from images, (ii) a web search tool for retrieving open world knowledge and facts, and (iii) an image search tool to glean relevant information from metadata associated with visually similar images. AVIS employs an LLM-powered planner to choose tools and queries at each step. It also uses an LLM-powered reasoner to analyze tool outputs and extract key information. A working memory component retains information throughout the process. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSFcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s1999/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1758&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzymhonGfhvGcxIN4AuIe75wxYUANfqlEoKDBzy_fUjZoXGcc_QuHtXRanG537LLxVMCkI1xyYYze_00sGePnb_ZgX6LznnUfDchvvbdRyoV3iFnyn7oy8bB81TCbh_D-I3unIwgxswoSFcHe0vno1WgxKhZV2LcU0JY46kxPBEQxMUZwzNpkfCl4zf5gp/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of AVIS&#39;s generated workflow for answering a challenging visual information seeking question. The input image is taken from the Infoseek dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Comparison to previous work&lt;/h2>; &lt;p>; Recent studies (eg, &lt;a href=&quot;https://arxiv.org/abs/2304.09842&quot;>;Chameleon&lt;/a>;, &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>; and &lt;a href=&quot;https://multimodal-react.github.io/&quot;>;MM-ReAct&lt;/a>;) explored adding tools to LLMs for multimodal inputs. These systems follow a two-stage process: planning (breaking down questions into structured programs or instructions) and execution (using tools to gather information). Despite success in basic tasks, this approach often falters in complex real-world scenarios. &lt;/p>; &lt;p>; There has also been a surge of interest in applying LLMs as autonomous agents (eg, &lt;a href=&quot;https://openai.com/research/webgpt&quot;>;WebGPT&lt;/a>; and &lt;a href=&quot;https://react-lm.github.io/&quot;>;ReAct&lt;/a>;). These agents interact with their environment, adapt based on real-time feedback, and achieve goals. However, these methods do not restrict the tools that can be invoked at each stage, leading to an immense search space. Consequently, even the most advanced LLMs today can fall into infinite loops or propagate errors. AVIS tackles this via guided LLM use, influenced by human decisions from a user study. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Informing LLM decision making with a user study&lt;/h2>; &lt;p>; Many of the visual questions in datasets such as &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; pose a challenge even for humans, often requiring the assistance of various tools and APIs. An example question from the OK-VQA dataset is shown below. We conducted a user study to understand human decision-making when using external tools. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;889&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgWg3x2MDAHuxlTNhBLcEh4d7EdxOgD8h1OmiRiSEe84Y-CANG9ner5DEuDLSCUXBtRahz-aGuMSSW_ApXazC_21Wi_ye8xP8eaifynjwh0hD5U-0i-cqWxb9m_iPttSzIcumJJ315EtHvHpTHYg7LCX-N2OP6WosgobEsPyJTK6du2cG2U6DXsMPP5QmXU/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We conducted a user study to understand human decision-making when using external tools. Image is taken from the OK-VQA dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The users were equipped with an identical set of tools as our method, including &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PALI&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Search_engine&quot;>;web search&lt;/a>;. They received input images, questions, detected object crops, and buttons linked to image search results. These buttons offered diverse information about the detected object crops, such as knowledge graph entities, similar image captions, related product titles, and identical image captions. &lt;/p>; &lt;p>; We record user actions and outputs and use it as a guide for our system in two key ways. First, we construct a transition graph (shown below) by analyzing the sequence of decisions made by users. This graph defines distinct states and restricts the available set of actions at each state. For example, at the start state, the system can take only one of these three actions: PALI caption, PALI VQA, or object detection. Second, we use the examples of human decision-making to guide our planner and reasoner with relevant contextual instances to enhance the performance and effectiveness of our system. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/s1146/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width=&quot;845&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVdH0MNzXGI8oDnO3sVzeUKfimHNp4E0_f9XDyJcqfVjDoaJXwX-FGI9RQj8r0vShmoY2gzuarv0p0uJ27-Icb-heq0KorHY_eoe5LPgVZnr1SVf6_oJL3JjLv4fhulayer_TvOtAv_yKYZgeUdSgwXLZxInR3xxh_xiKcFCaPRf9wtDdReWSi7Ts154tZ/w472-h640/image7.png&quot; width=&quot;472&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS transition graph.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;General framework&lt;/h2>; &lt;p>; Our approach employs a dynamic decision-making strategy designed to respond to visual information-seeking queries. Our system has three primary components. First, we have a &lt;em>;planner&lt;/em>; to determine the subsequent action, including the appropriate API call and the query it needs to process. Second, we have a &lt;em>;working memory&lt;/em>; that retains information about the results obtained from API executions. Last, we have a &lt;em>;reasoner&lt;/em>;, whose role is to process the outputs from the API calls. It determines whether the obtained information is sufficient to produce the final response, or if additional data retrieval is required. &lt;/p>; &lt;p>; The planner undertakes a series of steps each time a decision is required regarding which tool to employ and what query to send to it. Based on the present state, the planner provides a range of potential subsequent actions. The potential action space may be so large that it makes the search space intractable. To address this issue, the planner refers to the transition graph to eliminate irrelevant actions. The planner also excludes the actions that have already been taken before and are stored in the working memory. &lt;/p>; &lt;p>; Next, the planner collects a set of relevant in-context examples that are assembled from the decisions previously made by humans during the user study. With these examples and the working memory that holds data collected from past tool interactions, the planner formulates a prompt. The prompt is then sent to the LLM, which returns a structured answer, determining the next tool to be activated and the query to be dispatched to it. This design allows the planner to be invoked multiple times throughout the process, thereby facilitating dynamic decision-making that gradually leads to answering the input query. &lt;/p>; &lt;p>; We employ a reasoner to analyze the output of the tool execution, extract the useful information and decide into which category the tool output falls: informative, uninformative, or final answer. Our method utilizes the LLM with appropriate prompting and in-context examples to perform the reasoning. If the reasoner concludes that it&#39;s ready to provide an answer, it will output the final response, thus concluding the task. If it determines that the tool output is uninformative, it will revert back to the planner to select another action based on the current state. If it finds the tool output to be useful, it will modify the state and transfer control back to the planner to make a new decision at the new state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1469&quot; data-original-width=&quot;1999&quot; height=&quot;470&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM4kxtSw8Uvmttk6GfBnm_6S6jUapWg5wWMqA5oXIDSJQjdWsaLfSaRx-bALthbxSg-0_LMg7p4ayC1hpjQfWpzfO55oDSurSEEmLn63_2pbMKiRVrKReXZZ5tNooNvOqL_IncQx3GU1dZDUmSs7orJDdHEj-f5y9nUMFJbYCuuFBPP0XVaW3OltOgQySd/w640-h470/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS employs a dynamic decision-making strategy to respond to visual information-seeking queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate AVIS on &lt;a href=&quot;https://arxiv.org/abs/2302.11713&quot;>;Infoseek&lt;/a>; and &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; datasets. As shown below, even robust visual-language models, such as &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, fail to yield high accuracy when fine-tuned on Infoseek. Our approach (AVIS), without fine-tuning, achieves 50.7% accuracy on the unseen entity split of this dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEge21VZK2Ze6y9yDi2x-2FSwI3SGBGokA1d2AgJAM5ySwBzb4aRWCS3WFQBGqB-FGqtzOwKftuvzW-THb2IDHuQRCTs2EikipLFKX-B2TMvYVt2rlglHjqYkpwHXfbNqNMRCgisQQN2rXaU19m7TTmR2SuVKjwQ-Srqgzdgpfe8x18RxHIQM_9aCw8gkSnA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVIS visual question answering results on Infoseek dataset. AVIS achieves higher accuracy in comparison to previous baselines based on &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2202.03052&quot;>;OFA&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results on the OK-VQA dataset are shown below. AVIS with few-shot in-context examples achieves an accuracy of 60.2%, higher than most of the previous works. AVIS achieves lower but comparable accuracy in comparison to the PALI model fine-tuned on OK-VQA. This difference, compared to Infoseek where AVIS outperforms fine-tuned PALI, is due to the fact that most question-answer examples in OK-VQA rely on common sense knowledge rather than on fine-grained knowledge. Therefore, PaLI is able to encode such generic knowledge in the model parameters and doesn&#39;t require external knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKY5cRi9EQY3z7aLkOaXXpGt-6L6UhvO4YCzm0Dwb-O1jcYB-PkNyIlzjPK3qPnujWs6o7naaY8HoKSI1d5cpoT08bC4c5NRy9OKI4Pn8a6LxeajAygpjfJ6VWebvNW66MxUYaCr38l429iw4UD6q0I0nq6dKqBFEzFcRVF4kF84wj_pts_NiegSAWFC97/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. AVIS achieves higher accuracy in comparison to previous works that use few-shot or zero-shot learning, including &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; and &lt;a href=&quot;https://viper.cs.columbia.edu/&quot;>;ViperGPT&lt;/a>;. AVIS also achieves higher accuracy than most of the previous works that are fine-tuned on OK-VQA dataset, including &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;REVEAL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2206.01201&quot;>;ReVIVE&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>;, and achieves results that are close to the fine-tuned &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>; model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a novel approach that equips LLMs with the ability to use a variety of tools for answering knowledge-intensive visual questions. Our methodology, anchored in human decision-making data collected from a user study, employs a structured framework that uses an LLM-powered planner to dynamically decide on tool selection and query formation. An LLM-powered reasoner is tasked with processing and extracting key information from the output of the selected tool. Our method iteratively employs the planner and reasoner to leverage different tools until all necessary information required to answer the visual question is amassed. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A. Ross, Cordelia Schmid and Alireza Fathi.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/362086873015740792/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/362086873015740792&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/autonomous-visual-information-seeking.html&quot; rel=&quot;alternate&quot; title=&quot;Autonomous visual information seeking with large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje4SF07XPWF1tjYompjrnyrqMXDjqkeotbgVq0mMaGL6fuTPtw45P0TewFTemIVW8KBVCDdWtMS89gLqNpbDNjwWRg8WlvzzkhBGBOWmM1SUFzF5vkoFiiaIylBb2jZELcM4HDYqYoAmK4eYzrvfCHgAASKIZY1kVGcL9ORQXF4Qdfo32mA8Z4bh8smHNA/s72-c/AVIS.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4779594044050650231&lt;/id>;&lt;published>;2023-08-17T11:08:00.000-07:00&lt;/published>;&lt;updated>;2023-08-17T11:08:24.346-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Neural network pruning with combinatorial optimization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hussein Hazimeh, Research Scientist, Athena Team, and Riade Benbaki, Graduate Student at MIT&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s320/CHITA%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Modern neural networks have achieved impressive performance across a variety of applications, such as &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;language, mathematical reasoning&lt;/a>;, and &lt;a href=&quot;https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html&quot;>;vision&lt;/a>;. However, these networks often use large architectures that require lots of computational resources. This can make it impractical to serve such models to users, especially in resource-constrained environments like wearables and smartphones. A &lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;widely used approach&lt;/a>; to mitigate the inference costs of pre-trained networks is to prune them by removing some of their weights, in a way that doesn&#39;t significantly affect utility. In standard neural networks, each weight defines a connection between two neurons. So after weights are pruned, the input will propagate through a smaller set of connections and thus requires less computational resources. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s1600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFiItw-bPSnBAKOMxSd8GM2bt11LSJ_41g12HmyGQGGAgCGKv_NBrYEf_0rEbAq7yMRzvvFurATkEPNapY39gxzE52FOnAvjG2HwJ4_h5A1F71ks1NeBmpdEWLOOxGaTRsltcl8kGl8L7ytBJxcma5vWEgD-o4IoXjcogVw6NkqvJgNZHsXrfb5k8dNkg3/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Original network vs. a pruned network.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Pruning methods can be applied at different stages of the network&#39;s training process: post, during, or before training (ie, immediately after weight initialization). In this post, we focus on the post-training setting: given a pre-trained network, how can we determine which weights should be pruned? One popular method is &lt;a href=&quot;https://jmlr.org/papers/volume22/21-0366/21-0366.pdf&quot;>;magnitude pruning&lt;/a>;, which removes weights with the smallest magnitude. While efficient, this method doesn&#39;t directly consider the effect of removing weights on the network&#39;s performance. Another popular paradigm is &lt;a href=&quot;https://jmlr.org/papers/v22/21-0366.html&quot;>;optimization-based pruning&lt;/a>;, which removes weights based on how much their removal impacts the loss function. Although conceptually appealing, most existing optimization-based approaches seem to face a serious tradeoff between performance and computational requirements. Methods that make crude approximations (eg, assuming a diagonal &lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;>;Hessian matrix&lt;/a>;) can scale well, but have relatively low performance. On the other hand, while methods that make fewer approximations tend to perform better, they appear to be much less scalable. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2302.14623&quot;>;Fast as CHITA: Neural Network Pruning with Combinatorial Optimization&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/&quot;>;ICML 2023&lt;/a>;, we describe how we developed an optimization-based approach for pruning pre-trained neural networks at scale. CHITA (which stands for “Combinatorial Hessian-free Iterative Thresholding Algorithm”) outperforms existing pruning methods in terms of scalability and performance tradeoffs, and it does so by leveraging advances from several fields, including &lt;a href=&quot;https://en.wikipedia.org/wiki/High-dimensional_statistics&quot;>;high-dimensional statistics&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization&lt;/a>;, and neural network pruning. For example, CHITA can be 20x to 1000x faster than state-of-the-art methods for pruning &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; and improves accuracy by over 10% in many settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview of contributions&lt;/h2>; &lt;p>; CHITA has two notable technical improvements over popular methods: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Efficient use of second-order information&lt;/strong>;: Pruning methods that use second-order information (ie, relating to &lt;a href=&quot;https://en.wikipedia.org/wiki/Second_derivative&quot;>;second derivatives&lt;/a>;) achieve the state of the art in many settings. In the literature, this information is typically used by computing the Hessian matrix or its inverse, an operation that is very difficult to scale because the Hessian size is quadratic with respect to the number of weights. Through careful reformulation, CHITA uses second-order information without having to compute or store the Hessian matrix explicitly, thus allowing for more scalability. &lt;/li>;&lt;li>;&lt;strong>;Combinatorial optimization&lt;/strong>;: Popular optimization-based methods use a simple optimization technique that prunes weights in isolation, ie, when deciding to prune a certain weight they don&#39;t take into account whether other weights have been pruned. This could lead to pruning important weights because weights deemed unimportant in isolation may become important when other weights are pruned. CHITA avoids this issue by using a more advanced, combinatorial optimization algorithm that takes into account how pruning one weight impacts others. &lt;/li>; &lt;/ul>; &lt;p>; In the sections below, we discuss CHITA&#39;s pruning formulation and algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A computation-friendly pruning formulation&lt;/h2>; &lt;p>; There are many possible pruning candidates, which are obtained by retaining only a subset of the weights from the original network. Let &lt;em>;k&lt;/em>; be a user-specified parameter that denotes the number of weights to retain. Pruning can be naturally formulated as a &lt;a href=&quot;https://arxiv.org/abs/1803.01454&quot;>;best-subset selection&lt;/a>; (BSS) problem: among all possible pruning candidates (ie, subsets of weights) with only &lt;em>;k&lt;/em>; weights retained, the candidate that has the smallest loss is selected. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s1600/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIuxL23IilgYpOEWtnP9B4zbiPnuV5NUML47JP0q1idyLLmZUqRlHrxx77iFIinFWUXMekNhKSltLlZvzBSTaqsYmbithvXGlvggyaAZrtb4mg9oiYMWArjvf_lj7T9IbY1Ae4-wijzOZzTazsxWImdGRgLSyAJEc5WQWHvylSwcHQJWX8gXfEk70l8iEs/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pruning as a BSS problem: among all possible pruning candidates with the same total number of weights, the best candidate is defined as the one with the least loss. This illustration shows four candidates, but this number is generally much larger.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Solving the pruning BSS problem on the original loss function is generally computationally intractable. Thus, similar to previous work, such as &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf&quot;>;OBD&lt;/a>; and &lt;a href=&quot;https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf&quot;>;OBS&lt;/a>;, we approximate the loss with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Quadratic_form&quot;>;quadratic function&lt;/a>; by using a second-order &lt;a href=&quot;https://en.wikipedia.org/wiki/Taylor_series&quot;>;Taylor series&lt;/a>;, where the Hessian is estimated with the empirical &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;>;Fisher information matrix&lt;/a>;. While gradients can be typically computed efficiently, computing and storing the Hessian matrix is prohibitively expensive due to its sheer size. In the literature, it is common to deal with this challenge by making restrictive assumptions on the Hessian (eg, diagonal matrix) and also on the algorithm (eg, pruning weights in isolation). &lt;/p>; &lt;p>; CHITA uses an efficient reformulation of the pruning problem (BSS using the quadratic loss) that avoids explicitly computing the Hessian matrix, while still using all the information from this matrix. This is made possible by exploiting the low-&lt;a href=&quot;https://en.wikipedia.org/wiki/Rank_(linear_algebra)&quot;>;rank&lt;/a>; structure of the empirical Fisher information matrix. This reformulation can be viewed as a sparse &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;>;linear regression&lt;/a>; problem, where each regression coefficient corresponds to a certain weight in the neural network. After obtaining a solution to this regression problem, coefficients set to zero will correspond to weights that should be pruned. Our regression data matrix is (&lt;em>;n&lt;/em>; x &lt;em>;p&lt;/em>;), where &lt;em>;n&lt;/em>; is the batch (sub-sample) size and &lt;em>;p&lt;/em>; is the number of weights in the original network. Typically &lt;em>;n&lt;/em>; &amp;lt;&amp;lt; &lt;em>;p&lt;/em>;, so storing and operating with this data matrix is much more scalable than common pruning approaches that operate with the (&lt;em>;p&lt;/em>; x &lt;em>;p&lt;/em>;) Hessian. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s1601/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;901&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiea6tTRbJe9ZKEwR09cBnzZC_erK1TkeNRJgEBkhowDETOsJQbDGHZqgL20pn-QEy05hMV2ac4oD-2TRQjUWvptKLdKvBISi8f3o0nJjxhwKnuaMpTeuxUfysihGiifxtPLT3KFrvm5FRaektFiLodj5hZY1E9DOFD0SjSJqlyRT6jPmaKMGWdKe2uyJx/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;CHITA reformulates the quadratic loss approximation, which requires an expensive Hessian matrix, as a linear regression (LR) problem. The LR&#39;s data matrix is linear in &lt;em>;p&lt;/em>;, which makes the reformulation more scalable than the original quadratic approximation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scalable optimization algorithms&lt;/h2>; &lt;p>; CHITA reduces pruning to a linear regression problem under the following sparsity constraint: at most &lt;em>;k&lt;/em>; regression coefficients can be nonzero. To obtain a solution to this problem, we consider a modification of the well-known &lt;a href=&quot;https://arxiv.org/pdf/0805.0510.pdf&quot;>;iterative hard thresholding&lt;/a>; (IHT) algorithm. IHT performs &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;>;gradient descent&lt;/a>; where after each update the following post-processing step is performed: all regression coefficients outside the Top-&lt;em>;k&lt;/em>; (ie, the &lt;em>;k&lt;/em>; coefficients with the largest magnitude) are set to zero. IHT typically delivers a good solution to the problem, and it does so iteratively exploring different pruning candidates and jointly optimizing over the weights. &lt;/p>; &lt;p>; Due to the scale of the problem, standard IHT with constant &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;>;learning rate&lt;/a>; can suffer from very slow convergence. For faster convergence, we developed a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Line_search&quot;>;line-search&lt;/a>; method that exploits the problem structure to find a suitable learning rate, ie, one that leads to a sufficiently large decrease in the loss. We also employed several computational schemes to improve CHITA&#39;s efficiency and the quality of the second-order approximation, leading to an improved version that we call CHITA++. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We compare CHITA&#39;s run time and accuracy with several state-of-the-art pruning methods using different architectures, including &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;MobileNet&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Run time&lt;/strong>;: CHITA is much more scalable than comparable methods that perform joint optimization (as opposed to pruning weights in isolation). For example, CHITA&#39;s speed-up can reach over 1000x when pruning ResNet. &lt;/p>; &lt;p>; &lt;strong>;Post-pruning accuracy&lt;/strong>;:&lt;strong>; &lt;/strong>;Below, we compare the performance of CHITA and CHITA++ with magnitude pruning (MP), &lt;a href=&quot;https://arxiv.org/abs/2004.14340&quot;>;Woodfisher&lt;/a>; (WF), and &lt;a href=&quot;https://arxiv.org/abs/2203.04466&quot;>;Combinatorial Brain Surgeon&lt;/a>; (CBS), for pruning 70% of the model weights. Overall, we see good improvements from CHITA and CHITA++. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixl7vQywUa9pM7EyjeLu2v2I9Y6WnfAHj0Jb2x_laXhRky7Ku0Vxh-ZqqsjiZmpCEQYvwja080c1aGYRjd9FxFV6DySlkFi0SvwrJCpc5g3gGjiRF_lfcKLWFGwImX-_x3r_CHrj_qsAukEFtUjIfwn33Nbu7r5_1yRjkjYBtyF6Pz-KwvpQVIJKDEnkg_/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Post-pruning accuracy of various methods on ResNet20. Results are reported for pruning 70% of the model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1PIRd58fkkpTJcLBF0QcoT-TY1cqpA-5H0i1FNsfxa0OmqWkYScucFlaKSWI5UqMQ_99EjBjSURs16o44_KZsrhOubO-tHDbF6xwnfgYnYI_AbKVhELS1nUWAq6XZHyWaUcWpqwyeJco-Cp-w2OUUDsPUNMBhGCTkSBFjV6ODFY60K-VCFnGENDN4LtfA/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Post-pruning accuracy of various methods on MobileNet. Results are reported for pruning 70% of the model weights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Next, we report results for pruning a larger network: ResNet50 (on this network, some of the methods listed in the ResNet20 figure couldn&#39;t scale). Here we compare with magnitude pruning and &lt;a href=&quot;https://arxiv.org/abs/2107.03356&quot;>;M-FAC&lt;/a>;. The figure below shows that CHITA achieves better test accuracy for a wide range of sparsity levels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/s1050/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;784&quot; data-original-width=&quot;1050&quot; height=&quot;478&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0Gbdfth1GxVUurwwg8aPxzNkQfwQkV2ZDUAnAJF9SZHPG7anjBQ0NQPidqhzTZYU1_QYtJ54fRzmmTrscJwlSEU5Mf4eHGe4ubJ0xJuNtjr6JMfO72BBBox904eo7yzqhAPguPN7LwKx-_lgB0hVHfFhIYIdp39WolNXhGefB-jKeLoq3jBVTHrhJ2CUU/w640-h478/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Test accuracy of pruned networks, obtained using different methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion, limitations, and future work&lt;/h2>; &lt;p>; We presented CHITA, an optimization-based approach for pruning pre-trained neural networks. CHITA offers scalability and competitive performance by efficiently using second-order information and drawing on ideas from combinatorial optimization and high-dimensional statistics. &lt;/p>; &lt;p>; CHITA is designed for &lt;em>;unstructured pruning&lt;/em>; in which any weight can be removed. In theory, unstructured pruning can significantly reduce computational requirements. However, realizing these reductions in practice requires special software (and possibly hardware) that support sparse computations. In contrast, &lt;em>;structured pruning&lt;/em>;, which removes whole structures like neurons, may offer improvements that are easier to attain on general-purpose software and hardware. It would be interesting to extend CHITA to structured pruning. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is part of a research collaboration between Google and MIT. Thanks to Rahul Mazumder, Natalia Ponomareva, Wenyu Chen, Xiang Meng, Zhe Zhao, and Sergei Vassilvitskii for their help in preparing this post and the paper. Also thanks to John Guilyard for creating the graphics in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4779594044050650231/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4779594044050650231&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/neural-network-pruning-with.html&quot; rel=&quot;alternate&quot; title=&quot;Neural network pruning with combinatorial optimization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyU4esZL0DIoU6gbv90zR7Fw-r8Jm9DhLix7eBHMwp50c_3l1pP0myByQ4fSPidsrfhMrOxS2hQxLJuQ4d5DVJP3n5hAocfJeAWQDNjcvrU679bnFYcww0qcNWNzr3SEEcOQqG8owJmNxIWIrqJq_6ReXBJ9PUK-tW1ou0j73P3grgASIrfudrTyjyHu5K/s72-c/CHITA%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;