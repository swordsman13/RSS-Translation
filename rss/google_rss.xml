<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-04-20T16:26:48.042-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="datasets"></category><category term="Education"></category><category term="Neural Networks"></category><category term="Health"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="Computer Science"></category><category term="Quantum AI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Self-Supervised Learning"></category><category term="accessibility"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="NeurIPS"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Responsible AI"></category><category term="ACL"></category><category term="Audio"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ML Fairness"></category><category term="video"></category><category term="Awards"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Supervised Learning"></category><category term="User Experience"></category><category term="Collaboration"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="TTS"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Environment"></category><category term="Google Accelerated Science"></category><category term="Speech Recognition"></category><category term="DeepMind"></category><category term="Google Translate"></category><category term="Large Language Models"></category><category term="Video Analysis"></category><category term="statistics"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="UI"></category><category term="Vision Research"></category><category term="Acoustic Modeling"></category><category term="Interspeech"></category><category term="Systems"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Differential Privacy"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="crowd-sourcing"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Art"></category><category term="Biology"></category><category term="Climate"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Graphs"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="Weather"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1352&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1569605132526995799&lt;/id>;&lt;发布>;2024-03-29T11:03:00.000-07:00&lt;/发布>;&lt;更新>;2024-03- 29T11:03:10.261-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Climate&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;天气&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;生成人工智能来量化天气预报的不确定性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Lizao (Larry) Li, Google 研究部软件工程师和研究科学家 Rob Carver&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdl ATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif&quot;样式=&quot;显示：无；” />; &lt;p>; 准确的天气预报可以对人们的生活产生直接影响，从帮助做出日常决策（例如为一天的活动携带什么物品）到通知紧急行动（例如，在恶劣的天气条件下保护人们）。随着气候变化，准确及时的天气预报的重要性只会越来越大。认识到这一点，谷歌一直在投资天气和气候研究，以帮助确保未来的预报技术能够满足对可靠天气信息的需求。我们最近的一些创新包括 &lt;a href=&quot;https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot;>;MetNet-3&lt;/a>;、 Google 对未来长达 24 小时的高分辨率预测，以及 &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global- Weather-forecasting/&quot;>;GraphCast&lt;/a>;，一种可以预测最多未来 10 天天气的天气模型。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 天气本质上是随机的。为了量化不确定性，传统方法依靠基于物理的模拟来生成预测集合。然而，生成大型集合以便准确识别和表征罕见和极端天气事件的计算成本很高。考虑到这一点，我们很高兴地宣布我们旨在加速天气预报进展的最新创新，&lt;a href=&quot;https://www.science.org/doi/10.1126/sciadv.adk4489 &quot;>;可扩展的集成包络扩散采样器&lt;/a>; (SEEDS)，最近发表于&lt;em>;&lt;a href=&quot;https://www.science.org/journal/sciadv&quot;>;科学进展&lt;/a>;&lt;/em >;。 SEEDS 是一种生成式 AI 模型，可以有效地大规模生成天气预报集合，而成本仅为传统基于物理的预报模型的一小部分。这项技术为天气和气候科学开辟了新的机遇，它代表了概率扩散模型在天气和气候预测中的首批应用之一，概率扩散模型是媒体生成最新进展背后的一种生成人工智能技术。 &lt;/p>; &lt;br />; &lt;h2>;概率预测的必要性：蝴蝶效应&lt;/h2>; &lt;p>; 1972 年 12 月，&lt;a href=&quot;https://www.aaas.org/&quot;>;美国科学促进会&lt;/a>;在华盛顿召开会议，麻省理工学院气象学教授&lt;a href=&quot;https://en.wikipedia.org/wiki/Edward_Norton_Lorenz&quot;>;Ed Lorenz&lt;/a>;做了题为“ ，“巴西蝴蝶翅膀的扇动是否会在德克萨斯州引发龙卷风？”这促成了“&lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;蝴蝶效应&lt;/a>;”一词。他在 1963 年发表的具有里程碑意义的论文的基础上进行了研究，在该论文中，他研究了“超远程天气预报”的可行性，并描述了当与数值天气预报模型及时集成时，初始条件的误差如何呈指数级增长。这种指数误差增长（称为混沌）会导致确定性可预测性限制，从而限制了决策中个人预测的使用，因为它们无法量化天气条件固有的不确定性。在预测飓风、热浪或洪水等极端天气事件时，这一点尤其成问题。 &lt;/p>; &lt;p>; 认识到确定性预报的局限性，世界各地的气象机构都发布了概率预报。此类预测基于确定性预测的集合，每个预测都是通过在初始条件中包含合成噪声和物理过程中的随机性来生成的。利用天气模型中快速的误差增长率，集合中的预测有目的地不同：初始不确定性被调整以生成尽可能不同的运行，并且天气模型中的随机过程在模型运行期间引入了额外的差异。通过对集合中的所有预测进行平均来减轻误差的增长，并且预测集合中的可变性量化了天气条件的不确定性。 &lt;/p>; &lt;p>; 虽然有效，但生成这些概率预测的计算成本很高。它们需要在大型超级计算机上多次运行高度复杂的数值天气模型。因此，许多业务天气预报只能为每个预报周期生成约 10-50 个集合成员。对于担心罕见但影响大的天气事件的可能性的用户来说，这是一个问题，这些天气事件通常需要更大的集合来评估几天之后的情况。例如，需要一个由 10,000 名成员组成的集合来预测发生概率为 1% 的事件的可能性，相对误差小于 10%。量化此类极端事件的概率可能有用，例如对于应急管理准备或能源贸易商。 &lt;/p>; &lt;br />; &lt;h2>;SEEDS：人工智能驱动的进步&lt;/h2>; &lt;p>; 在上述&lt;a href=&quot;https://www.science.org/doi/10.1126/sciadv.adk4489&quot;中>;论文&lt;/a>;，我们提出了可扩展集合包络扩散采样器（SEEDS），这是一种用于天气预报集合生成的生成人工智能技术。 SEEDS 基于&lt;a href=&quot;https://blog.research.google/2021/07/high-fidelity-image- Generation-using.html&quot;>;去噪扩散概率&lt;/a>;模型，这是一种状态最先进的生成式人工智能方法部分由谷歌研究院首创。 &lt;/p>; &lt;p>; SEEDS 可以根据运行数值天气预报系统的一到两次预报生成大型集合。生成的集合不仅可以产生可信的类似真实天气的预测，而且在技能指标（例如排名直方图）方面也可以匹配或超过基于物理的集合&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;均方根误差&lt;/a>; (RMSE) 和 &lt;a href= “https://www.tandfonline.com/doi/abs/10.1198/016214506000001437&quot;>;连续排名概率得分&lt;/a>; (CRPS)。特别是，生成的集合为预测分布的尾部分配了更准确的可能性，例如 ±2σ 和 ±3σ 天气事件。最重要的是，与超级计算机进行预测所需的计算时间相比，该模型的计算成本可以忽略不计。它在 Google Cloud TPUv3-32 实例上每 3 分钟具有 256 个集成成员（分辨率为 2°）的吞吐量，并且可以通过部署更多加速器轻松扩展到更高的吞吐量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1 BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;1000&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3 PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;SEEDS 生成更多数量级的样本来填充天气模式的分布。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;生成合理的天气预报&lt;/h2>; &lt;p>;众所周知，生成式人工智能可以生成非常详细的图像和视频。此属性对于生成与可能的天气模式一致的集合预报特别有用，这最终为下游应用带来最大的附加值。正如 Lorenz 指出的那样，“他们制作的[天气预报]地图应该看起来像真实的天气地图。”下图将 SEEDS 的预测与美国正在运行的天气预报系统的预测进行了对比 (&lt;a href=&quot;https:// www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/gefs.php&quot;>;全球集合预报系统&lt;/a>;，GEFS）&lt;a href=&quot;https://en.wikipedia 期间的特定日期.org/wiki/2022_European_heatwaves&quot;>;2022 年欧洲热浪&lt;/a>;。我们还将结果与高斯模型的预测进行了比较，该模型预测每个位置每个大气场的单变量平均值和标准差，这是一种常见且计算高效的方法但不太复杂的数据驱动方法。这种高斯模型旨在表征逐点后处理的输出，它忽略相关性并将每个网格点视为独立的随机变量。相比之下，真实的天气图将具有详细的&lt;em>;。 &lt;/p>; &lt;p>; 由于 SEEDS 直接模拟大气状态的联合分布，因此它可以真实地捕获对流层中部位势与平均海平面压力之间的空间协方差和相关性。密切相关，天气预报员通常使用它们来评估和验证预报。平均海平面压力的梯度是驱动地表风的因素，而对流层中部位势的梯度则产生高层风，从而改变大规模的天气模式。 &lt;/p>; &lt;p>; 下图所示的 SEEDS 生成的样本（Ca-Ch 帧）显示了葡萄牙西部的一个位势槽，其空间结构类似于美国实际预测或基于观测的再分析中发现的空间结构。尽管高斯模型可以充分预测边缘单变量分布，但它无法捕获跨领域或空间相关性。这阻碍了对这些异常现象对来自北非的热空气入侵可能产生的影响的评估，而北非的热空气入侵可能会加剧欧洲的热浪。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVu jZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s1999/image2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1675&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8u FWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;2022 年 7 月 14 日 0:00 UTC 欧洲各地的邮票地图。等值线代表平均海平面压力（虚线标记低于 1010 hPa 的等压线），而热图则描绘 500 hPa 压力水平下的位势高度。 (A)&lt;a href=&quot;https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5&quot;>;ERA5&lt;/a>;重新分析，真实观察的代理。 (Ba-Bb) 2 位成员将美国 7 天的运营预测用作我们模型的种子。 (Ca-Ch) 8 个来自 SEEDS 的样品。 (Da-Dh) 8 个非种子成员来自 7 天的美国业务集合预报。 (Ea-Ed) 来自逐点高斯模型的 4 个样本，该模型由整个美国作战集合的均值和方差参数化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;更准确地报道极端事件&lt;/h2>; &lt;p>; 下面我们展示了极端高温期间里斯本附近2米处的温度和总水汽柱的联合分布活动时间：2022年7月14日，当地时间1:00。我们使用 2022 年 7 月 7 日发布的 7 天预测。对于每个图，我们使用 SEEDS 生成 16,384 个成员的集合。 ERA5 观测到的天气事件用星号表示。还显示了操作系综，其中正方形表示用于为生成的系综提供种子的预测，三角形表示其余系综成员。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k 6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;941&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqk KeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;SEEDS 提供了对 2022 年 7 月 14 日欧洲极端高温事件（以棕色星表示）的更好统计覆盖。每幅图显示了葡萄牙里斯本附近网格点上总柱积分水蒸气 (TCVW) 与温度的关系，这些样本来自我们的模型生成的 16,384 个样本（显示为绿点），以取自 2 个种子（蓝色方块）为条件美国 7 天业务集合预报（由稀疏的棕色三角形表示）。有效预报时间为当地时间1:00。实体轮廓级别对应于 SEEDS 内核密度的等比例，最外层包围质量的 95%，每个级别之间包围 11.875%。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; br />; &lt;p>; 根据美国作战小组的说法，在 7 天前观测到的事件发生的可能性非常小，以至于其 31 名成员都没有预测到近地表温度会像观测到的那样温暖。事实上，根据高斯核密度估计计算出的事件概率低于 1%，这意味着成员少于 100 人的集合不太可能包含像此事件一样极端的预测。相比之下，SEEDS 集合能够从两个播种预测中进行推断，提供可能的天气状态的范围，并更好地统计事件的覆盖范围。这样既可以量化事件发生的概率，又可以对事件发生的天气状况进行采样。具体来说，我们高度可扩展的生成方法可以创建非常大的集合，通过为任何用户定义的诊断提供超过给定阈值的天气状态样本来表征非常罕见的事件。 &lt;/p>; &lt;br />; &lt;h2>;结论和未来展望&lt;/h2>; &lt;p>; SEEDS 利用生成式人工智能的力量来生成与美国正在运行的预报系统相当的集合预报，但速度更快。本文报告的结果只需要来自操作系统的 2 个播种预测，该系统在当前版本中生成 31 个预测。这导致了一种混合预报系统，其中使用基于物理的模型计算的一些天气轨迹来播种扩散模型，该模型可以更有效地生成额外的预报。这种方法提供了当前操作天气预报范例的替代方案，其中统计模拟器节省的计算资源可以分配用于提高基于物理的模型的分辨率或更频繁地发布预报。 &lt;/p>; &lt;p>; 我们相信，SEEDS 只是人工智能在未来几年加速数值天气预报业务进展的众多方式之一。我们希望这一生成式人工智能在天气预报模拟和后处理方面的实用性演示将促进其在气候风险评估等研究领域的应用，在这些领域中，生成大量气候预测集合对于准确量化未来的不确定性至关重要气候。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;所有 SEEDS 作者 Lizao Li、Rob Carver、Ignacio Lopez-Gomez、Fei Sha 和 John Anderson 共同撰写了这篇博文， Carla Bromberg 担任项目负责人。我们还要感谢动画设计者 Tom Small。我们 Google Research 的同事为 SEEDS 工作提供了宝贵的建议。其中，我们感谢 Leonardo Zepeda-Núñez、Zhong Yi Wan、Stephan Rasp、Stephan Hoyer 和 Tapio Schneider 的投入和有益的讨论。我们感谢泰勒·拉塞尔 (Tyler Russell) 提供的额外技术项目管理，以及亚历克斯·梅罗斯 (Alex Merose) 的数据协调和支持。我们还感谢 Cenk Gazen、Shreya Agrawal 和 Jason Hickey 在 SEEDS 工作的早期阶段进行的讨论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1569605132526995799/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html&quot; rel=&quot;alternate&quot; title=&quot;生成人工智能来量化天气预报的不确定性&quot; type=&quot;text/html&quot;/ >;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>; &lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATD KSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-1799535679952845079&lt;/id>;&lt;发布>;2024-03-28T13:53:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-29T12： 00:03.604-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;神经网络&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;statistics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AutoBNN：组合概率时间序列预测贝叶斯神经网络&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Urs Köster&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4trPid -OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s320/AutoBNN.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;时间序列&lt;/a>;问题无处不在，从预测天气和交通模式到了解经济趋势。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_inference&quot;>;贝叶斯&lt;/a>;方法从对数据模式（先验概率）的假设开始，收集证据（例如，新的时间序列数据），并不断更新该假设以形成后验概率分布。传统的贝叶斯方法，例如高斯过程（GP）和&lt;a href=&quot;https://blog.tensorflow.org/2019/03/ Structure-time-series-modeling-in.html&quot;>;结构时间序列&lt;/a>;广泛用于对时间序列数据进行建模，例如常用的&lt;a href=&quot;https://gml.noaa.gov/ccgg /trends/&quot;>;冒纳罗亚二氧化碳&lt;/a>;数据集。然而，他们通常依赖领域专家来精心选择合适的模型组件，并且计算成本可能很高。神经网络等替代方案缺乏可解释性，因此很难理解它们如何生成预测，并且不能产生可靠的置信区间。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入 &lt;a href=&quot;https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn&quot; >;AutoBNN&lt;/a>;，一个用 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; 编写的新开源包。 AutoBNN 自动发现可解释的时间序列预测模型，提供高质量的不确定性估计，并有效扩展以用于大型数据集。我们描述了 AutoBNN 如何将传统概率方法的可解释性与神经网络的可扩展性和灵活性结合起来。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;AutoBNN&lt;/h2>; &lt;p>; AutoBNN 基于 &lt;a href=&quot;https:/ /proceedings.mlr.press/v28/duvenaud13.html&quot;>;行&lt;/a>; &lt;a href=&quot;https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550&quot;>;&lt;/a>; &lt;a href= &quot;https://proceedings.mlr.press/v202/saad23a.html&quot;>;过去十年的研究&lt;/a>;通过使用具有学习能力的 GP 进行时间序列建模，提高了预测准确性&lt;a href=&quot;https:// www.cs.toronto.edu/~duvenaud/cookbook/&quot;>;内核&lt;/a>;结构。 GP 的核函数对有关正在建模的函数的假设进行编码，例如趋势、周期性或噪声的存在。对于学习的 GP 核，核函数是组合定义的：它是一个基本核（例如线性核、二次核、周期性核、周期性核） >;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function&quot;>;Matérn&lt;/a>;&lt;/code>; 或 &lt;code>;ExponentiatedQuadratic&lt;/code>;）或组合两个或使用&lt;code>;加法&lt;/code>;、&lt;code>;乘法&lt;/code>;或&lt;code>;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/170等运算符的多个内核函数.pdf&quot;>;ChangePoint&lt;/a>;&lt;/code>;。这种组合内核结构有两个相关的目的。首先，这很简单，对于数据专家（但不一定是 GP 专家）的用户可以为其时间序列构建合理的先验。其次，诸如&lt;a href=&quot;https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf&quot;>;顺序蒙特卡罗&lt;/a>;之类的技术可用于小型结构上的离散搜索，并可以输出可解释的结果。&lt;/p>; &lt;p>; AutoBNN 改进了这些想法，用&lt;a href=&quot;https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/&quot;>;贝叶斯神经网络&lt; /a>; (BNN)，同时保留组合内核结构。 BNN 是一种具有权重概率分布的神经网络，而不是一组固定的权重。这会导致输出分布，捕获预测中的不确定性。与 GP 相比，BNN 具有以下优势：首先，训练大型 GP 的计算成本很高，并且传统的训练算法按时间序列中数据点数量的立方进行缩放。相反，对于固定宽度，训练 BNN 通常与数据点的数量呈近似线性关系。其次，与 GP 训练操作相比，BNN 更适合 GPU 和 &lt;a href=&quot;https://cloud.google.com/tpu?hl=en&quot;>;TPU&lt;/a>; 硬件加速。第三，组合 BNN 可以轻松地与&lt;a href=&quot;https://arxiv.org/abs/2007.06823&quot;>;传统深度 BNN&lt;/a>; 结合，后者具有特征发现的能力。人们可以想象一种“混合”架构，其中用户指定 &lt;code>;Add&lt;/code>;(&lt;code>;Linear&lt;/code>;, &lt;code>;Periodic&lt;/code>;, &lt;code>;Deep&lt; 的顶级结构/code>;），而深度 BNN 则用来学习潜在高维协变量信息的贡献。 &lt;/p>; &lt;p>; 那么如何将具有组合内核的 GP 转换为 BNN？单层神经网络通常会随着神经元数量（或“宽度”）收敛到 GP &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-1-4612-0745-0_2 ”趋向无穷大&lt;/a>;。最近，研究人员&lt;a href=&quot;https://openreview.net/forum?id=gRwh5HkdaTm&quot;>;发现了&lt;/a>;另一个方向的对应关系——许多受欢迎的全科医生&lt;a href=&quot;https://www .cs.toronto.edu/~duvenaud/cookbook/&quot;>;内核&lt;/a>;（例如 &lt;code>;Matern&lt;/code>;、&lt;code>;ExponentiatedQuadratic&lt;/code>;、&lt;code>;Polynomial&lt;/code>; 或 &lt; code>;Periodic&lt;/code>;）可以通过适当选择激活函数和权重分布作为无限宽度的 BNN 获得。此外，即使宽度远小于无穷大，这些 BNN 仍然接近相应的 GP。例如，下图显示了 &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_matrix#:~:text=In%20probability%20theory%20and%20statistics,of%20a%20given 中的差异%20random%20vector&quot;>;观测值对之间的协方差&lt;/a>;，以及真实 GP 及其对应的&lt;a href=&quot;https://en.wikipedia.org/wiki/Kriging&quot;>;回归&lt;/a>;结果width-10 神经网络版本。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_q F-eGv2C1QkU9oDCAS09FfoCne81yEAqC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s1350/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;598&quot; data-original-width=&quot;1350&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_qF-eGv2C1QkU9oDCAS09FfoCne81yEA qC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;真实 GP 内核（顶行）与其宽度 10 神经网络之间的 &lt;a href=&quot;https://en.wikipedia.org/wiki/Gram_matrix&quot;>;Gram 矩阵&lt;/a>; 的比较网络近似值（底行）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; 样式=“左边距：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoidYqlAK2J1n4y71Qn- WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux 3AXzUENmJ0NA8Ch9dyICT15/s1328/image4.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;586&quot; data-original-width=&quot;1328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhoidYqlaK2J1n4y71Qn-WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC 2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux3AXzUENmJ0NA8Ch9dyICT15/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;真实 GP 内核（顶行）与其宽度为 10 的神经网络近似值（底行）之间的回归结果比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;最后，通过&lt;code>;加法&lt;/code>;和&lt;code>;乘法&lt;/code>;的&lt;a href=&quot;https://arxiv.org/abs/1905.06076&quot;>;BNN类似物&lt;/a>;完成翻译GP 上的算子，并且通过将组件 BNN 的输出相加来直接产生 BNN 加法，BNN 乘法是通过乘以 BNN 隐藏层的激活然后应用共享密集层来实现的。因此，我们只能将具有相同隐藏宽度的 BNN 相乘。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用 AutoBNN&lt;/h2>; &lt;p>; AutoBNN &lt;a href=&quot;https://github .com/tensorflow/probability/tree/main/spinoffs/autobnn&quot;>;软件包&lt;/a>;在&lt;a href=&quot;https://www.tensorflow.org/probability&quot;>;Tensorflow Probability&lt;/a>;中提供。它在 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; 中实现并使用 &lt;a href=&quot;https://github.com/google/flax&quot;>;flax。 linen&lt;/a>; 神经网络库。它实现了迄今为止讨论的所有基本内核和运算符（&lt;code>;Linear&lt;/code>;、&lt;code>;Quadratic&lt;/code>;、&lt;code>;Matern&lt;/code>;、&lt;code>;ExponentiatedQuadratic&lt;/code>;、&lt; code>;Periodic&lt;/code>;、&lt;code>;Addition&lt;/code>;、&lt;code>;Multiplication&lt;/code>;）加上一个新内核和三个新运算符：&lt;/p>; &lt;ul>; &lt;li>;a &lt;code>;OneLayer &lt;/code>; 内核，单个隐藏层 &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;ReLU&lt;/a>; BNN，&lt;/li>;&lt;li>;a &lt;code >;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/170.pdf&quot;>;ChangePoint&lt;/a>;&lt;/code>; 运算符，允许在两个内核之间平滑切换，&lt;/li>;&lt;li>;一个 &lt;code>;LearnableChangePoint&lt;/code>; 运算符，与 &lt;code>;ChangePoint&lt;/code>; 相同，只是位置和斜率是给定先验分布并且可以从数据中学习，以及 &lt;/li>;&lt;li>;a &lt;code >;WeightedSum&lt;/code>; 运算符。 &lt;/li>; &lt;/ul>; &lt;p>; &lt;code>;WeightedSum&lt;/code>; 将两个或多个 BNN 与可学习的混合权重结合起来，其中可学习的权重遵循 &lt;a href=&quot;https://en.wikipedia.org/ wiki/Dirichlet_distribution&quot;>;狄利克雷先验&lt;/a>;。默认情况下，使用浓度为 1.0 的平坦狄利克雷分布。 &lt;/p>; &lt;p>; &lt;code>;WeightedSums&lt;/code>; 允许结构发现的“软”版本，即一次训练许多可能模型的线性组合。与离散结构的结构发现相比，例如在 AutoGP 中，这允许我们使用标准梯度方法来学习结构，而不是使用昂贵的离散优化。 WeightedSum 允许我们并行评估它们，而不是串联评估潜在的组合结构。 &lt;/p>; &lt;p>; 为了轻松实现探索，AutoBNN 定义了&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py&quot;>;模型数量包含顶级或内部 &lt;code>;WeightedSums&lt;/code>; 的结构&lt;/a>;。这些模型的名称可以用作任何 &lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/estimators.py&quot;>;估计器&lt;中的第一个参数/a>; 构造函数，并包含诸如 &lt;code>;&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py#L133&quot;>;sum_of_stumps&lt;/a 之类的内容>;&lt;/code>;（所有基本内核的 &lt;code>;WeightedSum&lt;/code>;）和 &lt;code>;sum_of_shallow&lt;/code>;（将基本内核与所有运算符的所有可能组合相加）。&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td 样式=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXz CeSCZBFDHIc1qYfK53EwEdngf1KykzCfpPiIg3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s1389/image2.png&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;255&quot; data-original-width=&quot;1389&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXzCesCZBFDHIc1qYfK53EwEdngf1KykzCfpPiI g3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;sum_of_stumps 模型的插图。顶行中的条形显示每个基本内核贡献的量，底行显示基本内核所代表的函数。生成的加权和显示在上。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 下图演示了从&lt;a href=&quot;https://forecasters.org/resources/time-series-data/m3-competition/&quot;>;M3&lt;/a>; 数据集。六个基本结构是 &lt;code>;ExponentiatedQuadratic&lt;/code>; （这是相同的）作为径向基函数内核，或简称为 &lt;a href=&quot;https://en.wikipedia.org/wiki/Radial_basis_function_kernel&quot;>;RBF&lt;/a>;）、&lt;code>;Matern&lt;/code>;、&lt;code>;Linear &lt;/code>;、&lt;code>;Quadratic&lt;/code>;、&lt;code>;OneLayer&lt;/code>; 和 &lt;code>;Periodic&lt;/code>; 内核。该图显示了 32 个粒子集合中它们的权重的 MAP 估计值。所有高似然粒子对周期性分量给予较大权重，对线性分量、二次分量和单层分量给予较低权重>;，以及 &lt;code>;RBF&lt;/code>; 或 &lt;code>;Matern&lt;/code>; 的较大权重。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H 2-JZOmxxRJSm9ExG_PUr6U7iFl8nyp4lEaNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s868/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;868&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H2-JZOmxxRJSm9ExG_PUR6U7iFl8nyp4lE aNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;32 个粒子的基本核权重的 &lt;a href=&quot;https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php&quot;>;MAP&lt;/a>; 估计的平行坐标图。 &lt;code>;sum_of_stumps&lt;/code>; 模型在 M3 数据集的 N374 系列上进行训练（插入蓝色）。较暗的线对应于可能性较高的粒子。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;通过使用&lt;code>;WeightedSums&lt;/code>;作为其他算子的输入，可以表示丰富的组合结构，同时保持模型紧凑和可学习权重的数量较少。例如，我们包含 &lt;code>;sum_of_products&lt;/code>; 模型（如下图所示），它首先创建两个 &lt;code>;WeightedSums&lt;/code>; 的成对乘积，然后计算两个乘积的总和。通过将一些权重设置为零，我们可以创建许多不同的离散结构。该模型中可能的结构总数为 2&lt;sup>;16&lt;/sup>;，因为有 16 个可以打开或关闭的基本内核。所有这些结构都是通过仅训练这一模型来隐式探索的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQ tvzMR_548jt4h2lbTzfnp7ZUeYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s1754/AutoBNN%20illustration.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1754&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQtvzMR_548jt4h2lbTzfnp7ZU eYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s16000/AutoBNN%20illustration.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;“sum_of_products”模型的图示。四个 WeightedSum 中的每一个都具有与“sum_of_stumps”模型相同的结构。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 然而，我们发现某些内核组合（例如， &lt;code>;Periodic&lt;/code>; 与 &lt;code>;Matern&lt;/code>; 或 &lt;code>;ExponentiatedQuadratic&lt;/code>; 的乘积）会导致许多数据集过度拟合。为了防止这种情况，我们定义了 &lt;code>;sum_of_safe_shallow&lt;/code>; 等模型类，在使用 &lt;code>;WeightedSums&lt;/code>; 执行结构发现时排除此类乘积。 &lt;/p>; &lt;p>; 对于训练，AutoBNN 提供了 AutoBnnMapEstimator 和 AutoBnnMCMCEstimator 来分别执行 MAP 和 MCMC 推理。任一估计器都可以与六个&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/likelihoods.py&quot;>;似然函数&lt;/a>;中的任何一个组合，包括四种基于连续数据的具有不同噪声特征的正态分布，两种基于计数数据的负二项式分布。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31Pbx TQF29yH1cTJRdI​​-XkXmnZMR_imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s1076/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;280&quot; data-original-width=&quot;1076&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31PbxTQF29yH1cTJRdI​​-XkXmnZMR _imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 &lt;a href=&quot;https://gml.noaa.gov/ccgg/trends/&quot;>;Mauna Loa CO2&lt;/a 上运行 AutoBNN 的结果>; 我们的示例 &lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb&quot;>;colab&lt;/a>; 中的数据集。该模型捕获数据中的趋势和季节性成分。外推未来，均值预测略微低估了实际趋势，而 95% 置信区间逐渐增大。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 拟合如图所示的模型上面，所需要的只是以下 10 行代码，使用 &lt;a href=&quot;https://scikit-learn.org/stable/&quot;>;scikit-learn&lt;/a>; 启发的估计器接口：&lt;/p>; &lt;pre class=&quot;prettyprint&quot;>;导入 autobnn 作为 ab model = ab.operators.Add( bnns=(ab.kernels.PeriodicBNN(width=50), ab.kernels.LinearBNN(width=50), ab.kernels.MaternBNN (width=50))) estimator = ab.estimators.AutoBnnMapEstimator( model, &#39;normal_likelihood_logistic_noise&#39;, jax.random.PRNGKey(42), period=[12]) estimator.fit(my_training_data_xs, my_training_data_ys) 低、中、高 = estimator.predict_quantiles(my_training_data_xs) &lt;/pre>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; &lt;a href =&quot;https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn&quot;>;AutoBNN&lt;/a>; 为构建复杂的时间序列预测模型提供了强大而灵活的框架。通过将 BNN 和 GP 的优势与组合内核相结合，AutoBNN 为理解和预测复杂数据打开了一个充满可能性的世界。我们邀请社区尝试&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb&quot; target=&quot;_blank&quot;>;colab&lt;/a>;，并且利用该库进行创新并解决现实世界的挑战。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;AutoBNN 由 Colin Carroll、Thomas Colthurst 编写，乌尔斯·科斯特和斯里尼瓦斯·瓦苏代万。我们要感谢 Kevin Murphy、Brian Patton 和 Feras Saad 的建议和反馈。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research .google/feeds/1799535679952845079/comments/default&quot; rel=&quot;replies&quot; title=&quot;帖子评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/ 03/autobnn-probabilistic-time-series.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com /feeds/8474926331452026626/posts/default/1799535679952845079&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 1799535679952845079&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/autobnn-probabilistic-time-series.html&quot; rel=&quot; ternate&quot; title=&quot;AutoBNN：使用组合贝叶斯神经网络进行概率时间序列预测&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4 trPid-OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s72-c/AutoBNN.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7061041222399769838&lt;/ id>;&lt;发布>;2024-03-20T13:54:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-20T13:54:06.249-07:00&lt;/更新>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;健康&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt; /category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;肺癌筛查的计算机辅助诊断&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Atilla Kiraly 和产品经理 Rory Pilgrim &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD7R8_tNqtH6D8X_KiMt ZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s320/PULMA%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 肺癌是全球癌症相关死亡的主要原因 &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/cancer#:~:text= 2020 年报告的%20most%20common%20causes%20of，直肠%20（916%20000%20deaths）%3B&quot;>;180 万人死亡&lt;/a>;。晚期诊断会大大降低生存机会。 &lt;a href=&quot;https://www.cdc.gov/cancer/lung/basic_info/screening.htm&quot;>;肺癌筛查&lt;/a>;通过&lt;a href=&quot;https://www.cancer.gov/about -cancer/diagnosis-staging/ct-scans-fact-sheet#:~:text=indicate%20real%20problems.-,Lung%20cancer,-Low%2Ddose%20CT&quot;>;计算机断层扫描&lt;/a>; (CT),它提供了详细的肺部 3D 图像，已被证明可以通过更早地检测潜在的癌症迹象，将高危人群的死亡率降低至少 20%。在美国，筛查涉及每年一次扫描，一些国家或病例建议或多或少地进行扫描。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/lung-cancer-screening&quot;>;美国预防服务工作组&lt;/a>;最近将肺癌筛查建议扩大了&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;大约80%&lt;/a>;，预计将增加筛查妇女以及少数种族和族裔群体的参与机会。然而，假阳性（即错误地报告无癌症患者的潜在癌症）可能会引起焦虑，并导致患者接受不必要的手术，同时增加医疗保健系统的成本。此外，根据医疗基础设施和放射科医生的可用性，筛查大量个体的效率可能具有挑战性。 &lt;/p>; &lt;p>; 在 Google，我们之前开发了&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;用于肺癌检测的机器学习 (ML) 模型&lt;/ a>;，并评估了它们自动检测和分类显示潜在癌症迹象的区域的能力。事实证明，在检测可能的癌症方面，其性能可与专家相媲美。虽然他们取得了很高的绩效，但要充分发挥他们的潜力，在现实环境中有效地传达发现是必要的。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://pubs.rsna.org/doi/10.1148/ryai.230079&quot;>;肺癌筛查中的辅助人工智能：一项回顾性跨国研究美国和日本&lt;/a>;”，发表于&lt;em>;&lt;a href=&quot;https://pubs.rsna.org/journal/ai&quot;>;放射学人工智能&lt;/a>;&lt;/em>;，我们研究了机器学习如何建模可以有效地将发现传达给放射科医生。我们还引入了一个以用户为中心的通用界面，以帮助放射科医生利用此类模型进行肺癌筛查。该系统以 CT 成像作为输入，并使用四个类别（无怀疑、可能良性、可疑、高度可疑）以及相应的感兴趣区域输出癌症怀疑评级。我们通过美国和日本的随机读者研究，使用当地癌症评分系统 (&lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/ RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot;>;Lung-RADS V1.1&lt;/a>; 和 &lt;a href=&quot;https://www.jscts.org/pdf/guideline/gls3rdfig_english130621.pdf&quot;>;仙台分数&lt;/a>;）和模仿现实设置的图像查看器。我们发现，在两项读者研究中，读者特异性随着模型的帮助而增加。为了加快利用机器学习模型进行类似研究的进展，我们提供&lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>;开源代码&lt;/a>;处理 CT 图像并生成与放射科医生使用的&lt;a href=&quot;https://en.wikipedia.org/wiki/Picture_archiving_and_communication_system&quot;>;图片存档和通信系统&lt;/a>; (PACS) 兼容的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开发一个接口来传达模型结果&lt;/h2>; &lt;p>; 将 ML 模型集成到放射科医生工作流程中涉及了解他们任务的细微差别和目标，以便为他们提供有意义的支持。在肺癌筛查方面，医院遵循定期更新的各个国家/地区特定指南。例如，在美国，Lung-RADs V1.1 指定 &lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot; >;字母数字评分&lt;/a>;表示肺癌风险和后续建议&lt;em>;。 &lt;/em>;在评估患者时，放射科医生将 CT 加载到工作站中以读取病例、查找肺部结节或病变，并应用既定指南来确定后续决策。 &lt;/p>; &lt;p>; 我们的第一步是通过额外的训练来改进&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前开发的机器学习模型&lt;/a>;数据和架构改进，包括&lt;a href=&quot;https://research.google/pubs/attention-is-all-you-need/&quot;>;自我注意力&lt;/a>;。然后，我们没有针对特定的指南，而是尝试了一种独立于指南或其特定版本来传达人工智能结果的补充方式。具体来说，系统输出提供怀疑评级和定位（感兴趣区域），供用户结合自己的具体指南进行考虑。该界面可生成与 CT 研究直接相关的输出图像，无需更改用户的工作站。放射科医生只需要查看一小组附加图像。他们的系统或与系统的交互没有其他变化。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug- HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s857/image1.png&quot; style=&quot;mar&quot; gin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original -width=&quot;857&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug-HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6 FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s16000/image1.png&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统输出示例。放射科医生的评估结果在发现可疑病变的 CT 体积位置上进行可视化。总体怀疑显示在 CT 图像的顶部。圆圈突出显示可疑病变，而方形则显示从不同角度（称为矢状视图）对同一病变的渲染。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 辅助肺癌筛查系统包括13 个模型，并具有类似于&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前的工作&lt;/a中使用的端到端系统的高级架构>;。这些模型相互协调，首先对肺部进行分割，获得总体评估，定位三个可疑区域，然后使用该信息为每个区域分配可疑评级。该系统使用 &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;>;Google Kubernetes Engine&lt;/a>; (GKE) 部署在 Google Cloud 上，该引擎提取映像、运行机器学习模型并提供了结果。这样可以实现可扩展性，并直接连接到&lt;a href=&quot;https://cloud.google.com/healthcare-api/docs/concepts/dicom&quot;>;DICOM 存储&lt;/a>;中存储图像的服务器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfk b9mGxEmL191sfpAclKD085x-u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s646/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;394&quot; data-original-width=&quot;646&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9mGxEmL191sfpAclKD085x- u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统的 Google Cloud 部署概述以及提供图像和计算结果的各个组件的定向调用流程。使用 Google Cloud 服务将图像提供给查看者和系统。该系统在 Google Kubernetes Engine 上运行，该引擎提取图像、处理图像并将其写回到 DICOM 存储中。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;读者研究&lt;/h2>; &lt;p>;为了评估系统在改善临床表现方面的效用，我们进行了两项读者研究（即设计的实验）使用预先存在的、去识别化的 CT 扫描来评估临床表现（比较有或没有技术帮助的专家表现）与 12 名放射科医生。我们向 6 位美国放射科医生和 6 位日本放射科医生介绍了 627 个具有挑战性的病例。在实验设置中，读者被分为两组，每个案例阅读两次，有或没有模型的帮助。读者被要求应用他们在临床实践中通常使用的评分指南，并报告他们对每个病例​​对癌症的总体怀疑。然后，我们比较了读者的反馈结果，以衡量模型对他们的工作流程和决策的影响。分数和怀疑水平是根据个人的实际癌症结果来判断的，以衡量敏感性、特异性和&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and -auc#:~:text=AUC%20stands%20for%20%22Area%20under,across%20all%20possible%20classification%20thresholds。&quot;>;ROC 曲线下面积&lt;/a>; (AUC) 值。这些是在有帮助和没有帮助的情况下进行比较的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1y LUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPmwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s794/image3.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;297&quot; data-original-width=&quot;794&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFP mwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;多案例多读者研究涉及每个读者对每个案例进行两次审查，一次有机器学习系统协助，一次没有机器学习系统协助。在此可视化中，读者首先在没有帮助的情况下查看 A 组（&lt;strong>;蓝色&lt;/strong>;），然后在冲洗期后在有帮助的情况下查看（&lt;strong>;橙色&lt;/strong>;）。第二个读者组遵循相反的路径，首先在帮助下阅读同一组案例 A 组。读者被随机分配到这些组中，以消除排序的影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;使用相同界面进行这些研究的能力突出了其对完全不同的癌症评分的普遍性系统，以及模型和辅助能力对不同患者群体的推广。我们的研究结果表明，当放射科医生在临床评估中使用该系统时，他们在没有可操作的肺癌发现的情况下正确识别肺部图像的能力（即&lt;em>;特异性&lt;/em>;）比之前提高了绝对 5-7%当他们不使用辅助系统时。这可能意味着每筛查 15-20 名患者，就有可能避免不必要的后续程序，从而减轻他们的焦虑和医疗保健系统的负担。反过来，这可以帮助提高肺癌筛查计划的可持续性，特别是随着&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;更多人有资格接受筛查&lt;/a >;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8 ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoBKt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s1999/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoB Kt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;在美国和日本的读者研究中，通过机器学习模型的帮助，读者的特异性得到了提高。特异性值是根据可操作的发现（发现可疑的东西）与没有可操作的发现的读者评分得出的，并与个体的真实癌症结果进行比较。在模型协助下，读者标记出较少的癌症阴性个体进行后续访问。对癌症阳性个体的敏感性保持不变。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过合作将其转化为现实世界的影响 &lt;/h2>; &lt;p>; 该系统结果表明，有可能减少随访次数、减少焦虑，并降低肺癌筛查的总体成本。为了将这项研究转化为现实世界的临床影响，我们正在与：&lt;a href=&quot;https://deephealth.com/&quot;>;DeepHealth&lt;/a>;，一家领先的人工智能驱动的健康信息学提供商；与印度领先的放射学服务提供商 &lt;a href=&quot;https://apolloradiologyintl.com/&quot;>;Apollo Radiology International&lt;/a>; 合作，探索将该系统纳入未来产品的途径。此外，我们还希望通过 &lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>; 帮助其他研究人员研究如何最好地将 ML 模型结果集成到临床工作流程中开源代码&lt;/a>;用于读者研究并结合本博客中描述的见解。我们希望这将有助于加速医学影像研究人员对其人工智能模型进行读者研究，并促进该领域的转化研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目的主要贡献者包括 Corbin Cunningham、Zaid Nabulsi、Ryan Najafi、Jie Yang、Charles Lau、Joseph R. Ledsam、叶文兴、Diego Ardila、Scott M. McKinney、Rory Pilgrim、Hiroaki Saito、Yasuteru Shimamura、Mozziyar Etemadi、Yun Liu、David Melnick、Sunny Jansen、Nadia Harhen 、David P. Nadich、Mikhail Fomitchev、Ziyad Helali、Shabir Adeel、Greg S. Corrado、Lily Peng、Daniel Tse、Shravya Shetty、Shruthi Prabhakara、Neeral Beladia 和 Krish Eswaran。感谢 Arnav Agharwal 和 Andrew Sellergren 的开源支持，以及 Vivek Natarajan 和 Michael D. Howell 的反馈。还要衷心感谢放射科医生在整个研究过程中通过图像解释和注释工作实现了这项工作，以及 Jonny Wong 和 Carli Sampson 协调读者研究。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt; /content>;&lt;link href=&quot;http://blog.research.google/feeds/7061041222399769838/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/03/computer-aided-diagnosis-for-lung.html&quot; rel=&quot;alternate&quot; title=&quot;肺癌筛查的计算机辅助诊断&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI &lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http: //schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author >;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD 7R8_tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s72-c/PULMA%20hero.jpg&quot;宽度=&quot; 72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-4615278636568583418&lt;/id>;&lt;发布>;2024-03-20T09:06:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-20T09:06:06.7 53 -07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“环境”>;&lt;/类别>;&lt;类别方案=“http://www.blogger” .com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;利用人工智能扩大全球对可靠洪水预报的访问&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt; span class=&quot;byline-author&quot;>;发布者：Yossi Matias，工程副总裁研究和 Gray Nearing，研究科学家，Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX-dCHpcYt nSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s320/洪水%20预测% 20hero%20image.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 洪水是&lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;最常见的自然灾害&lt;/ a>;，并造成全球每年大约&lt;a href=&quot;https://www.swissre.com/risk-knowledge/mitigating-climate-risk/floods.html&quot;>;500亿美元&lt;/a>;的经济损失。 &lt;a href=&quot;https://library.wmo.int/records/item/57630-2021-state-of-climate-services-water?offset=1#:~:text=WMO%2DNo.,1278&amp;amp; text=More%20than%202%20billion%20people,for%20the%20past%2020%20years.&quot;>;自 2000 年以来，与洪水相关的灾害发生率增加了一倍多&lt;/a>;&lt;a href=&quot;https: //www.nature.com/articles/s41598-020-70816-2&quot;>;由于气候变化&lt;/a>;。近 &lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;15 亿人&lt;/a>;，占 19%世界人口面临着严重洪水事件的巨大风险。升级预警系统，让这些人群能够获得准确、及时的信息&lt;a href=&quot;https://elibrary.worldbank.org/doi/abs/10.1596/1813-9450-6058&quot;>;每年可以挽救数千人的生命&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在可靠的洪水预报对全球人们生活的潜在影响的推动下，我们于 2017 年开始了洪水预报工作。通过此 &lt;a href=&quot;https ://blog.google/technology/ai/google-ai-global-flood-forecasting/&quot;>;多年历程&lt;/a>;，多年来，我们在推进研究的同时建立了实时运营系统洪水预报系统，&lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;在 Google 搜索、地图、Android 通知和通过&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;。不过，为了&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;在全球范围内扩展&lt;/a>;，尤其是在某些地方在无法获得准确的当地数据的地方，需要取得更多的研究进展。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;未测量流域的极端洪水全球预测&lt;/a>;”中，发表在&lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;自然&lt;/a>;&lt;/em>;中，我们展示了机器学习 (ML) 技术如何显着改善全球范围&lt;a href= “https://sites.research.google/floodforecasting/&quot;>;洪水预报&lt;/a>;相对于洪水相关数据稀缺的国家当前最先进的技术。借助这些基于人工智能的技术，我们将当前可用的全球临近预报的可靠性平均从零天延长到五天，并将非洲和亚洲地区的预报改进为与欧洲当前可用的预报类似。模型的评估是与欧洲中期天气预报中心 (&lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;) 合作进行的。 &lt;/p>; &lt;p>; 这些技术还使&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;能够提前 7 天提供实时河流预报，&lt;a href =&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;覆盖&lt;/a>; 80 多个国家的河流河段。人们、社区、政府和国际组织可以利用这些信息来采取预期行动，帮助保护弱势群体。 &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot;frameborder=&quot;0&quot; height=&quot;360 “ src=&quot;https://www.youtube.com/embed/ET04pDj-RvM?si=WJJXEtwJqtyMRuC_?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;[ET04pDj-RvM]&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;Google 的洪水预报 &lt;/h2>; &lt;p>; 为 FloodHub 工具提供支持的机器学习模型是与多个合作伙伴（包括学术界、政府、国际组织和机构）合作多年研究的成果。非政府组织。 &lt;/p>; &lt;p>; 2018 年，我们&lt;a href=&quot;https://blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;启动了试点&lt; /a>; 印度恒河-雅鲁藏布江流域的早期预警系统，&lt;a href=&quot;https://arxiv.org/abs/1901.09583&quot;>;假设&lt;/a>;认为机器学习可以帮助解决具有挑战性的问题可靠的大规模洪水预报。第二年，该试点进一步&lt;a href=&quot;https://blog.google/technology/ai/tracking-our-progress-on-flood-forecasting/&quot;>;扩大&lt;/a>;&lt;a href=&quot;https: //ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;通过结合淹没模型、实时水位测量、创建高程图和水文建模。 &lt;/p>; &lt;p>; 在与学者的&lt;a href=&quot;https://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html&quot;>;合作&lt;/a>;中，特别是，我们与&lt;a href=&quot;https://www.jku.at/en/institute-for-machine-learning/&quot;>;JKU 机器学习研究所&lt;/a>;一起探索了基于机器学习的水文模型，表明基于&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;LSTM&lt;/a>;的模型可以&lt;a href=&quot;https://hess. copernicus.org/articles/23/5089/2019/&quot;>;比传统的概念性和基于物理的&lt;a href=&quot;https://en.wikipedia.org/wiki/Hydrological_model&quot;>;水文学产生更准确的模拟&lt;/a>;模型&lt;/a>;。这项研究带来了&lt;a href=&quot;https://blog.research.google/2020/09/the-technology-behind-our-recent.html&quot;>;洪水预报改进&lt;/a>;，实现了&lt;a href= &quot;https://blog.google/technology/ai/flood-forecasts-india-bangladesh/&quot;>;扩大&lt;/a>;我们的预测覆盖范围，将印度和孟加拉国全部覆盖。我们还与耶鲁大学的研究人员合作，测试可增加&lt;a href=&quot;https://egc.yale.edu/about/perspectives/pande-and-coauthors-using-technology-save-lives-during-印度季风季节&quot;>;洪水警报的范围和影响&lt;/a>;。 &lt;/p>; &lt;p>; 我们的水文模型通过处理降水和物理流域信息等公开的天气数据来预测河流洪水。此类模型必须根据来自各个河流的&lt;a href=&quot;https://en.wikipedia.org/wiki/Stream_gauge&quot;>;水流测量站&lt;/a>;的长数据记录进行校准。全球河流流域（流域）拥有流量计的比例较低，流量计价格昂贵，但却是提供相关数据所必需的，并且为水文模拟和预报提供流量计具有挑战性&lt;a href=&quot;https://www.tandfonline.com/doi /full/10.1080/02626667.2013.803183&quot;>;缺乏此基础设施的流域的预测&lt;/a>;。 &lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;国内生产总值&lt;/a>; (GDP) 下降与&lt;a href=&quot;https:// www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;洪水风险的脆弱性&lt;/a>;，并且一个国家的国民生产总值与公开数据量之间存在负相关关系。机器学习通过允许&lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;在所有可用河流数据上训练单个模型&lt;/a>;来帮助解决这个问题适用于&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;没有可用数据&lt;/a>;的未计量盆地。通过这种方式，模型可以在全球范围内进行训练，并且可以对任何河流位置进行预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZr bCKnROTNn_hmOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s1051/Streamflow%20data%20from%20the %20Global%20Runoff%20Data%20Center.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width= “1051”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCKnROTNn_h mOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s16000/Streamflow%20data%20from%20the%20Global%20Runoff%20Data%20Center.jpg&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;公开可用的流量之间存在逆（对数）相关性一个国家的数据和国民生产总值。来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;的水流数据。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的学术合作促成了机器学习研究，该研究开发了&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;估计河流预报的不确定性&lt;/ a>; 并展示了 ML 河流预测模型&lt;a href=&quot;https://hess.copernicus.org/articles/25/2685/2021/hess-25-2685-2021-relations.html&quot;>;如何从多个数据中合成信息来源&lt;/a>;。他们证明，这些模型可以&lt;a href=&quot;https://hess.copernicus.org/articles/26/3377/2022/hess-26-3377-2022.html&quot;>;可靠地模拟极端事件&lt;/a>;，甚至当这些事件不是训练数据的一部分时。为了&lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;为开放科学做出贡献&lt;/a>;，我们将在 2023 年开放-在 &lt;em>;&lt;a href=&quot;https://www.nature.com/articles/s41597-023-01975-w&quot;>;Nature Scientific Data&lt;/a>;&lt;/ 中获取了社区驱动的大样本水文学数据集嗯>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;河流预报模型&lt;/h2>; &lt;p>; 国家和国际机构使用的大多数水文模型洪水预报和河流建模是状态空间模型，仅取决于日常输入（例如降水、温度等）和系统的当前状态（例如土壤湿度、积雪等）。 LSTM 是状态空间模型的一种变体，通过定义表示单个时间步长的神经网络来工作，其中处理输入数据（例如当前天气状况）以生成该时间步长的更新状态信息和输出值（流） 。 LSTM 按顺序应用来进行时间序列预测，从这个意义上说，其行为类似于科学家通常概念化水​​文系统的方式。根据经验，我们发现 &lt;a href=&quot;https://hess.copernicus.org/articles/23/5089/2019/&quot;>;LSTM 在河流预测任务上表现良好&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xx rB87mupNTPpioKT0wRgSSc1FwYDmfCUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s960/image1.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB87mupNTPpioKT0wRgSSc1FwYDmf CUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;LSTM 的示意图，LSTM 是一种按时间顺序运行的神经网络。可以在&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;此处&lt;/a>;找到易于理解的入门读物。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的河流预报模型使用两个连续应用的 LSTM：（1）“后报”LSTM 摄取截至当前时间（或者更确切地说，预报发布时间）的历史天气数据（动态后报特征） ），（2）“预测”LSTM 从后播 LSTM 中提取状态以及预测的天气数据（动态预测特征）来做出未来的预测。将一年的历史天气数据输入到后报 LSTM 中，将 7 天的预报天气数据输入到预报 LSTM 中。静态特征包括流域的地理和地球物理特征，这些特征被输入到后报和预测 LSTM 中，并允许模型学习各种类型流域的不同水文行为和响应。 &lt;/p>; &lt;p>; 预测 LSTM 的输出被输入到使用 &lt;a href=&quot;https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf 的“头”层&quot;>;混合密度网络&lt;/a>;产生概率预测（即，水流概率分布的预测参数）。具体来说，该模型在每次预测时都会预测重尾概率密度函数的混合参数，称为“非对称拉普拉斯分布”时间步。结果是一个混合密度函数，称为&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf&quot;>;非对称拉普拉斯的可数混合&lt;/a>;（ CMAL）分布，表示特定时间特定河流中体积流量的概率预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ 3dm8TRicy_wkTVtM4Xio_mhQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s960/LSTM-based%20river %20forecast%20model.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm8TRicy_wkTVtM4Xio_m hQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s16000/LSTM-based%20river%20forecast%20model.jpeg&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;基于 LSTM 的河流预报模型架构。按顺序应用两个 LSTM，一个摄取历史天气数据，一个摄取预测天气数据。模型输出是每个预测时间步长的水流概率分布参数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;输入和训练数据&lt;/h2>; &lt;p>; 该模型使用三种类型的公开数据输入，大部分来自政府来源：&lt;/p>; &lt;ol>; &lt;li>;&lt; em>;代表地理和地球物理变量的静态流域属性：&lt;/em>;来自 &lt;a href=&quot;https://www.Hydrosheds.org/Hydroatlas&quot;>;HydroATLAS 项目&lt;/a>;，包括长期气候指数等数据（降水量、温度、积雪比例）、土地覆盖和人为属性（例如，作为人类发展指标的夜间灯光指数）。 &lt;/li>;&lt;li>;&lt;em>;历史气象时间序列数据&lt;/em>;：用于在预测发布之前一年启动模型。数据来自&lt;a href=&quot;https://gpm.nasa.gov/data/imerg&quot;>;NASA IMERG&lt;/a>;，&lt;a href=&quot;https://psl.noaa.gov/data/gridded/ data.cpc.globalprecip.html&quot;>;NOAA CPC 全球统一日降水量分析&lt;/a>;，以及 &lt;a href=&quot;https://cds.climate.copernicus.eu/cdsapp#!/dataset/ reanalysis-era5-land?tab=overview&quot;>;ECMWF ERA5-land 再分析&lt;/a>;。变量包括每日总降水量、气温、太阳和热辐射、降雪和表面压力。 &lt;/li>;&lt;li>;&lt;em>;7 天预测范围内的预测气象时间序列&lt;/em>;：用作预测 LSTM 的输入。这些数据与上面列出的气象变量相同，来自 &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/datasets/set-i&quot;>;ECMWF HRES 大气模型&lt;/a>;。 &lt;/li>; &lt;/ol>; &lt;p>; 训练数据是来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;的每日流量值&lt; /a>; 1980 - 2023 年期间。使用来自 5,680 个不同流域水流测量仪（如下所示）的数据训练单个水流预测模型，以改进&lt;a href=&quot;https://eartharxiv.org/repository/view/6363 /&quot;>;准确度&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET 634isx6xzUswrYfDwp8oueoWJ7c3hf0os-RISANrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s1417/gauge_locations_map(1 ).jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;689&quot; data-original-width=&quot;1417&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET634isx6xzUswrYfDwp8oueo WJ7c3hf0os-RIsaNrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s16000/gauge_locations_map(1).jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;5,680 个流量计的位置，为 &lt;a href=&quot;https://www.bafg.de/ 的河流预报模型提供训练数据GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;改进当前最先进的技术&lt;/h2>; &lt;p>;我们将我们的河流预测模型与&lt;a href=&quot;https://www .globalfloods.eu/&quot;>;GloFAS 版本 4&lt;/a>;，当前最先进的全球洪水预报系统。这些实验表明，机器学习可以更早地针对规模更大、影响力更大的事件提供准确的警告。 &lt;/p>; &lt;p>; 下图显示了预测河流位置不同严重程度事件时&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1分数&lt;/a>;的分布世界各地，精确度为正负 1 天。 F1 分数是精确度和召回率的平均值，事件严重性通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Return_period#:~:text=A%20return%20period%2C%20also%20known 来衡量,river%20discharge%20flows%20to%20发生。&quot;>;返回期&lt;/a>;。例如，2 年重现期事件是预计平均每两年超过一次的水流量。我们的模型在长达 4 天或 5 天的交付时间内实现了可靠性评分，平均而言，与 GloFAS 即时预报（0 天交付时间）的可靠性相似或更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe4 6W8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s3908/分布%20of%20F1%20分数%20over %202-year%20.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1844&quot; data-original-width=&quot;3908 “ src =” Q28MTS72ZQJHLJXDGMJSA6VCHLB9ZH3EO8XETWSWSQJ8LNRAEJCAEZFPKGPKGPFPKGPFJRPIL4XMHRPHSDO1WX7BZMVSMVSMVSMVSLFMVSLDFMZOWMZOWMFMFARB5 a>; &lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/F- Score&quot;>;F1 得分&lt;/a>; 2014-2023 年期间全球 2,092 个流域的 2 年重现期事件，由 GloFAS（&lt;strong>;蓝色&lt;/strong>;）和我们的模型（&lt;strong>;橙色&lt;/strong>; >;) 在不同的交货时间。平均而言，我们的模型在统计上与 2 年（显示）以及 1 年、5 年和 10 年事件（未显示）提前 5 天的 GloFAS 即时预报（0 天提前时间）一样准确.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外（未显示），我们的模型在更大和更罕见的极端事件中实现了准确性，在 5 年重现期事件中的精确度和召回率得分在 1 年重现期事件中，与 GloFAS 的精度相似或更好。有关更多信息，请参阅&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;展望未来&lt;/h2>; &lt;p>;洪水预报计划是我们&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;适应和恢复能力工作&lt;/a>;并体现了 Google 的承诺&lt;a href=&quot;https:// /research.google/teams/climate-and-sustainability/&quot;>;应对气候变化&lt;/a>;，同时帮助全球社区增强抵御能力。我们相信人工智能和机器学习将继续在帮助推动气候行动科学研究方面发挥关键作用。 &lt;/p>; &lt;p>; 我们积极&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/4-flood-forecasting-collaboration-case-studies-show-how-ai-can-help-与有需要的社区/&quot;>;与多个国际援助组织（例如人道主义数据中心和红十字会）合作&lt;/a>;，提供可行的洪水预报。此外，与&lt;a href=&quot;https://wmo.int/&quot;>;世界气象组织&lt;/a>; (WMO) 持续合作&lt;a href=&quot;https://blog.google/outreach-initiatives /sustainability/early-warning-system-wmo-google/&quot;>;支持气候灾害预警系统&lt;/a>;，我们正在进行一项研究，以帮助了解人工智能如何帮助解决国家洪水预报机构面临的现实挑战。 &lt;/p>; &lt;p>; 虽然这里介绍的工作表明洪水预报向前迈出了重要一步，但未来的工作还需要进一步将洪水预报覆盖范围扩大到全球更多地点以及其他类型的洪水相关事件和灾害，包括山洪和灾害。城市洪水。我们期待与学术界和专家界、地方政府和业界的合作伙伴继续合作，以实现这些目标。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4615278636568583418/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html&quot; rel=&quot;alternate&quot; title=&quot;利用人工智能扩大全球获得可靠洪水预报的机会&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX- dCHpcYtnSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s72-c/洪水%20forecasting%20hero%2 0image.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-520087429457973735&lt;/id>;&lt;已发布>;2024-03-19T13:15:00.000- 07:00&lt;/发布>;&lt;更新>;2024-03-19T13:15:33.664-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= HCI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;ScreenAI：用于 UI 和视觉情境语言理解的视觉语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Srinivas Sunkara 和Gilles Baechler，Google 研究部软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4 uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s320/ScreenAI%20-%20hero。 jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 屏幕用户界面 (UI) 和信息图表（例如图表、图表和表格）在人类交流和人机交互中发挥着重要作用，因为它们促进了丰富的交互式用户体验。 UI 和信息图表共享相似的设计原则和视觉语言（例如图标和布局），这提供了构建可以理解、推理并与这些界面交互的单一模型的机会。然而，由于其复杂性和不同的呈现格式，信息图表和 UI 提出了独特的建模挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入“&lt;a href=&quot;https://arxiv.org/abs/2402.04615&quot;>;ScreenAI：一种视觉语言UI 和信息图表理解模型&lt;/a>;”。 ScreenAI 通过来自 &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI 架构&lt;/a>;的灵活修补策略进行了改进”pix2结构&lt;/a>;。我们在独特的数据集和任务组合上训练 ScreenAI，其中包括一项新颖的屏幕注释任务，该任务要求模型识别屏幕上的 UI 元素信息（即类型、位置和描述）。这些文本注释为大型语言模型 (LLM) 提供了屏幕描述，使它们能够自动大规模生成问答 (QA)、UI 导航和摘要训练数据集。仅用 5B 参数，ScreenAI 就可以在基于 UI 和信息图表的任务上实现最先进的结果 (&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;WebSRC&lt;/a>;和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;），以及 &lt;a href=&quot;https://github.com/vis-nlp 上一流的性能/ChartQA&quot;>;图表 QA&lt;/a>;、&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>; 和 &lt; a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。我们还发布了三个新数据集：&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#screen-annotation-dataset-details&quot;>;屏幕注释&lt;/ a>; 评估模型的布局理解能力，以及&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa/tree/main?tab=readme-ov-file#short_answers- directory&quot;>;ScreenQA Short&lt;/a>; 和&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#complexqa&quot; target=&quot;_blank&quot;>;复杂 ScreenQA&lt; /a>; 对其 QA 能力进行更全面的评估。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ScreenAI&lt;/h2>; &lt;p>; ScreenAI 的架构基于 &lt;a href=&quot;https:// /arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>;，由多模态编码器块和自回归解码器组成。 PaLI 编码器使用创建图像嵌入的视觉转换器 (ViT) 和将图像和文本嵌入连接起来的多模态编码器作为输入。这种灵活的架构使 ScreenAI 能够解决可以重新转换为文本+图像到文本问题的视觉任务。 &lt;/p>; &lt;p>; 在 PaLI 架构之上，我们采用了 pix2struct 中引入的灵活修补策略。不使用固定网格图案，而是选择网格尺寸以保留输入图像的原始纵横比。这使得 ScreenAI 能够在各种长宽比的图像上正常工作。 &lt;/p>; &lt;p>; ScreenAI 模型分两个阶段进行训练：预训练阶段和微调阶段。首先，应用自监督学习自动生成数据标签，然后用于训练 ViT 和语言模型。 ViT 在微调阶段被冻结，其中使用的大多数数据都是由人类评估者手动标记的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQX Zls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s1600/image6.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;583&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8 FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;ScreenAI模型架构。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;数据生成&lt;/h2>; &lt;p>; 为了创建 ScreenAI 的预训练数据集，我们首先编译来自各种设备（包括台式机、移动设备和平板电脑）的大量屏幕截图。这是通过使用&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot; target=&quot;_blank&quot;>;可公开访问的网页&lt;/a>;并遵循用于&lt;a href=&quot;的编程探索方法来实现的https://dl.acm.org/doi/10.1145/3126594.3126651&quot; target=&quot;_blank&quot;>;RICO 数据集&lt;/a>;，适用于移动应用。然后，我们应用基于 &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; target=&quot;_blank&quot;>;DETR&lt;/a>; 模型的布局注释器，该模型可识别并标记各种 UI 元素（例如，图像、象形图、按钮、文本）及其空间关系。使用能够区分 77 种不同图标类型的&lt;a href=&quot;https://arxiv.org/abs/2210.02663&quot; target=&quot;_blank&quot;>;图标分类器&lt;/a>;对象形图进行进一步分析。这种详细的分类对于解释通过图标传达的微妙信息至关重要。对于分类器未覆盖的图标以及信息图表和图像，我们使用 PaLI 图像字幕模型来生成提供上下文信息的描述性字幕。我们还应用&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot; target=&quot;_blank&quot;>;光学字符识别&lt;/a>; (OCR) 引擎来提取和注释屏幕上的文本内容。我们将 OCR 文本与之前的注释相结合，创建每个屏幕的详细描述。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp 1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s1747/image2.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1055&quot; data-original-width=&quot;1747&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJf BMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;带有生成注释的移动应用屏幕截图，其中包括 UI 元素及其描述，例如，&lt;code>;TEXT&lt;/code>; 元素还包含来自 OCR 的文本内容，&lt;code>;IMAGE&lt;/code>; 元素包含图像标题，&lt;code>;LIST_ITEMs&lt;/code>; 包含其所有子元素。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;基于LLM的数据生成&lt;/h3>; &lt;p>;我们使用&lt;a href=&quot;https://blog.google/technology/ai/google增强预训练数据的多样性-palm-2-ai-large-language-model/&quot;>;PaLM 2&lt;/a>; 通过两步过程生成输入-输出对。首先，使用上述技术生成屏幕注释，然后我们围绕此模式制作提示，以便法学硕士创建合成数据。这个过程需要及时的工程和迭代细化才能找到有效的提示。我们通过针对质量阈值的人工验证来评估生成的数据的质量。 &lt;/p>; &lt;br />; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px;white-space: pre-wrap;&quot;>;&lt;font color=&quot;#008000&quot;>;你只讲 JSON。不要编写非 JSON 的文本。您将看到以下移动屏幕截图，并用文字进行了描述。您能否针对屏幕截图的内容生成 5 个问题以及相应的简短答案？答案应尽可能简短，仅包含必要的信息。您的答案应结构如下：问题：[ {{问题：问题，答案：答案}}，...] {THE SCREEN SCHEMA} &lt;/font>;&lt;/pre>; &lt;br />; &lt;tablealign= &quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;QA 数据生成提示示例。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 通过结合法学硕士的自然语言能力通过结构化模式，我们模拟各种用户交互和场景，以生成合成的、真实的任务。特别是，我们生成三类任务：&lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;问答&lt;/strong>;：模型被要求回答有关屏幕截图内容的问题，例如，“什么时候餐厅开门了吗？” &lt;/li>;&lt;li>;&lt;strong>;屏幕导航&lt;/strong>;：要求模型将自然语言话语转换为屏幕上的可执行操作，例如“单击搜索按钮”。 &lt;/li>;&lt;li>;&lt;strong>;屏幕摘要&lt;/strong>;：要求模型用一两句话总结屏幕内容。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto；margin-right：auto；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR4 4ceNwDpkvaSLa4R3v4C-hEsnHDEC-JUUx31zZmDHDDwhWAMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7 /s1398/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1398&quot; data-original-width=&quot;1272&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHd Ec-JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用现有 ScreenAI 模型和 LLM 生成用于 QA、摘要和导航任务的数据的工作流程框图。每个任务都使用自定义提示来强调所需的方面，例如与计数相关的问题、涉及推理的问题等。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;tablealign=&quot;center&quot;cellpadding =&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;img height=&quot;540&quot; src=&quot;https://lh7-us.googleusercontent.com/LmUtXBMXK-zy_rMShHQ_Hk4vQeXu2Kpx8zfzjhE3uAREczbkbGTEjZ7OMTbqtB37lD4rF31xJsoWdVXNAXLbbM1Uc_01WZWmOfBg9RwyA UEToPpa1W38Pt117Zj5LrNfnxXqjXoAJDZd-zcAIgU4QSoBaAKsIrSi8_POI14F5hguN1NJL9a2RsrKg6WHz7w&quot; style=&quot;margin-left: auto; margin-right: auto; : 0 像素;&quot; width=&quot;705&quot; />;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM 生成的数据。屏幕 QA、导航和摘要示例。对于导航，操作边界框在屏幕截图上显示为红色。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;实验与结果&lt;/h2>; &lt;p>; 如前所述，ScreenAI 的训练分两个阶段：预训练和微调。预训练数据标签是使用自我监督学习获得的，微调数据标签来自人类评估者。 &lt;/p>; &lt;p>; 我们使用公共 QA、摘要和导航数据集以及与 UI 相关的各种任务来微调 ScreenAI。对于 QA，我们在多模式和文档理解领域使用完善的基准，例如 &lt;a href=&quot;https://github.com/vis-nlp/ChartQA&quot;>;ChartQA&lt;/a>;、&lt;a href=&quot;https ://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1&quot;>;DocVQA&lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch =17&amp;com=tasks&quot;>;多页 DocVQA&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>;、&lt;a href=&quot;https://ocr -vqa.github.io/&quot;>;OCR VQA&lt;/a>;、&lt;a href=&quot;https://x-lance.github.io/WebSRC/&quot;>;Web SRC&lt;/a>; 和 &lt;a href=&quot;https ://github.com/google-research-datasets/screen_qa&quot;>;ScreenQA&lt;/a>;。对于导航，使用的数据集包括&lt;a href=&quot;https://github.com/google-research-datasets/uibert/tree/main&quot;>;引用表达式&lt;/a>;、&lt;a href=&quot;https://github.com/uibert/tree/main&quot;>;引用表达式&lt;/a>; com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2209.15099&quot;>;马克杯&lt;/a>;和&lt;a href=&quot;https://github.com /google-research/google-research/tree/master/android_in_the_wild&quot;>;野外 Android&lt;/a>;。最后，我们使用 &lt;a href=&quot;https://github.com/google-research-datasets/screen2words&quot;>;Screen2Words&lt;/a>; 进行屏幕摘要，并使用 &lt;a href=&quot;https://paperswithcode.com/paper/ widget-captioning-generate-natural-language/review/&quot;>;小部件标题&lt;/a>;，用于描述特定的 UI 元素。除了微调数据集之外，我们还使用三个新颖的基准来评估微调的 ScreenAI 模型：&lt;/p>; &lt;ol>; &lt;li>;屏幕注释：启用评估模型布局注释和空间理解功能。 &lt;/li>;&lt;li>;ScreenQA Short：ScreenQA 的变体，其真实答案已被缩短，仅包含与其他 QA 任务更好地结合的相关信息。 &lt;/li>;&lt;li>;复杂 ScreenQA：用更困难的问题（计数、算术、比较和不可回答的问题）补充 ScreenQA Short，并包含各种宽高比的屏幕。 &lt;/li>; &lt;/ol>; &lt;p>; 经过微调的 ScreenAI 模型在各种 UI 和基于信息图表的任务上实现了最先进的结果 (&lt;a href=&quot;https://x-lance.github.com&quot;)。 io/WebSRC/&quot;>;WebSRC&lt;/a>; 和 &lt;a href=&quot;https://github.com/aburns4/MoTIF&quot;>;MoTIF&lt;/a>;）以及 &lt;a href=&quot;https 上一流的性能://github.com/vis-nlp/ChartQA&quot;>;图表质量检查&lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=1 &quot;>;DocVQA&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicVQA&lt;/a>; 与相似大小的模型进行比较。 ScreenAI 在 Screen2Words 和 OCR-VQA 上实现了具有竞争力的性能。此外，我们还报告了新引入的基准数据集的结果，作为进一步研究的基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOswns-GWJRdSCj3UmyxytOZxfo M64psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s1183/image2.png “ 样式=“左边距：自动;右边距：自动;”>; &lt;img border =“0” data-original-height =“1137” data-original-width =“1183” src =“https：// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFb NpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;将 ScreenAI 的模型性能与相似尺寸的最先进 (SOTA) 模型进行比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 接下来，我们检查 ScreenAI 的扩展能力，并观察到在所有任务中，增加模型大小可以提高性能，并且在最大尺寸时改进尚未饱和。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQ GwKGyO6YkAW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys86 9f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模型性能随着尺寸的增大而提高，即使在最大 5B 参数尺寸下性能也没有饱和。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们引入了 ScreenAI 模型以及统一的表示使我们能够利用所有这些领域的数据开发自我监督的学习任务。我们还说明了使用法学硕士生成数据的影响，并研究通过修改训练混合物来提高模型在特定方面的性能。我们应用所有这些技术来构建多任务训练模型，这些模型在许多公共基准上与最先进的方法相媲美。然而，我们也注意到，我们的方法仍然落后于大型模型，需要进一步的研究来弥补这一差距。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目是与 Maria 共同工作的结果Wang、Fedir Zubach、Hassan Mansoor、Vincent Etter、Victor Carbune、Jason Lin、Jindong Chen 和 Abhanshu Sharma。我们感谢 Fangyu Liu、Xi Chen、Efi Kokiopoulou、Jesse Berent、Gabriel Barcik、Lukas Zilka、Oriana Riva、Gang Li、Yang Li、Radu Soricut 和 Tania Bedrax-Weiss 的富有洞察力的反馈和讨论，以及 Rahul Aralikatte、Hao Cheng 和 Daniel Kim 在数据准备方面的支持。我们还感谢 Jay Yagnik、Blaise Aguera y Arcas、Ewa Dominowska、David Petrou 和 Matt Sharifi 的领导、远见和支持。我们非常感谢 Tom Small 帮助我们创建本文中的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/520087429457973735/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/screenai-visual-language-model- for-ui.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/520087429457973735&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/520087429457973735&quot; rel=&quot;self “ type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html&quot; rel=&quot;alternate&quot; title=&quot;ScreenAI：用于 UI 和视觉情境语言理解的视觉语言模型&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_i FjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s72-c/ScreenAI%20-%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search .yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post -4328167517765145678&lt;/id>;&lt;发布>;2024-03-19T08:00:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-19T08:00:00.150-07:00&lt;/更新>;&lt;类别方案= “http://www.blogger.com/atom/ns#” term=&quot;crowd-source&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term= “数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“多样性”>;&lt;/类别>;&lt;类别方案=“http://www.blogger” .com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SCIN：代表性皮肤病图像的新资源&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class =&quot;byline-author&quot;>;发布者：Google Research 研究科学家 Pooja Rao&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7 OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-spBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUBJ/s1600/ SCINHero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 健康数据集在研究和医学教育中发挥着至关重要的作用，但创建代表现实世界的数据集可能具有挑战性。例如，皮肤病的外观和严重程度各不相同，并且不同肤色的表现也不同。然而，现有的皮肤科图像数据集通常缺乏对日常状况（如皮疹、过敏和感染）的表征，并且偏向较浅的肤色。此外，种族和民族信息经常缺失，阻碍了我们评估差异或制定解决方案的能力。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为了解决这些限制，我们发布了&lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>;皮肤状况图像网络 (SIN) 数据集&lt;/a>;与 &lt;a href=&quot;https://med.stanford.edu/&quot;>;Stanford Medicine&lt;/a>; 的医生合作。我们设计 SCIN 是为了反映人们在线搜索的广泛问题，补充临床数据集中常见的疾病类型。它包含各种肤色和身体部位的图像，有助于确保未来的人工智能工具对所有人有效工作。我们已将 &lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>;SCIN 数据集&lt;/a>;作为开放访问资源免费提供给研究人员、教育工作者和开发人员，并且已采取谨慎的措施来保护贡献者的隐私。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2n vguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s1327/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1118&quot; data-original-width=&quot;1327&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlN pI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;来自 SCIN 数据集的图像和元数据示例集。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;数据集组成&lt;/h2>; &lt;p>; SCIN 数据集目前包含超过 10,000 张皮肤、指甲或头发状况的图像，这些图像由经历这些状况的个人直接贡献。根据机构审查委员会批准的一项研究，所有贡献都是在美国个人知情同意的情况下自愿做出的。为了为皮肤科医生的回顾性标签提供背景信息，贡献者被要求拍摄特写和稍远距离的图像。他们可以选择自我报告人口统计信息和&lt;a href=&quot;https://en.wikipedia.org/wiki/Fitzpatrick_scale&quot;>;晒黑倾向&lt;/a>;（自我报告的菲茨帕特里克皮肤类型，即 sFST） ，并描述与其关注相关的质地、持续时间和症状。 &lt;/p>; &lt;p>; 一到三名皮肤科医生用最多五个皮肤病学条件标记每个贡献，以及每个标签的置信度得分。 SCIN 数据集包含这些单独的标签，以及从它们导出的聚合和加权的鉴别诊断，可用于模型测试或训练。这些标签是回顾性分配的，并不等同于临床诊断，但它们使我们能够将 SCIN 数据集中皮肤病状况的分布与现有数据集进行比较。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_ b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s1851/image2.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;775&quot; data-original-width=&quot;1851&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFO Zy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;SCIN 数据集主要包含过敏、炎症和感染性疾病，而来自临床来源的数据集则侧重于良性和恶性 &lt;a href=&quot;https://en.wikipedia.org/ wiki/Neoplasm&quot;>;肿瘤&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;虽然许多现有的皮肤病学数据集侧重于恶性和良性肿瘤，旨在协助皮肤癌诊断，SCIN 数据集主要由常见的过敏、炎症和感染性疾病组成。 SCIN 数据集中的大多数图像都显示出早期阶段的问题——超过一半是在照片拍摄前不到一周出现的，30% 是在照片拍摄前不到一天出现的。此时间范围内的情况在卫生系统中很少见，因此在现有皮肤病学数据集中代表性不足。 &lt;/p>; &lt;p>; 我们还获得了皮肤科医生对 Fitzpatrick 皮肤类型的估计（估计 FST 或 eFST）以及外行贴标者对 &lt;a href=&quot;https://en.wikipedia.org/wiki/Monk_Skin_Tone_Scale&quot;>;和尚肤色的估计&lt;/a>; (eMST) 用于图像。这使得可以将皮肤状况和皮肤类型分布与现有皮肤病学数据集中的皮肤状况和皮肤类型分布进行比较。尽管我们没有选择性地针对任何皮肤类型或肤色，但与临床来源的类似数据集相比，SCIN 数据集具有平衡的 Fitzpatrick 皮肤类型分布（更多类型 3、4、5 和 6）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-c OeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s1851 /image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;620&quot; data-original-width=&quot;分段阅读_第 1851 章2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;与现有未丰富的皮肤病学数据集相比，SCIN 数据集中自我报告和皮肤科医生估计的 Fitzpatrick 皮肤类型分布 &lt; a href=&quot;https://github.com/mattgroh/fitzpatrick17k&quot;>;(Fitzpatrick17k&lt;/a>;, &lt;a href=&quot;https://www.fc.up.pt/addi/ph2%20database.html&quot;>; PH²&lt;/a>;、&lt;a href=&quot;https://www.it.pt/AutomaticPage?id=3459&quot;>;SKINL2&lt;/a>; 和&lt;a href=&quot;https://www.ncbi.nlm。 nih.gov/pmc/articles/PMC7479321/&quot;>;PAD-UFES-20&lt;/a>;)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; &lt;a href=&quot;https: //en.wikipedia.org/wiki/Fitzpatrick_scale&quot;>;菲茨帕特里克皮肤类型&lt;/a>;量表最初是作为照片打字量表开发的，用于测量皮肤类型对紫外线辐射的反应，广泛应用于皮肤病学研究。 Monk 肤色等级是一种较新的 10 色度等级，用于测量肤色而不是皮肤光型，捕捉较深肤色之间更细微的差异。虽然这两种量表都不是为了使用图像进行回顾性估计，但包含这些标签的目的是为了使未来能够对皮肤病学中的皮肤类型和色调表示进行研究。例如，SCIN 数据集为美国人口中这些皮肤类型和色调的分布提供了初始基准。 &lt;/p>; &lt;p>; SCIN 数据集女性和年轻人的代表性很高，这可能反映了多种因素的综合影响。这些可能包括皮肤状况发生率的差异、在线寻求健康信息的倾向以及不同人群对研究做出贡献的意愿的差异。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;众包方法&lt;/h2>; &lt;p>; 为了创建 SCIN 数据集，我们使用了一种新颖的众包方法，我们在与 &lt;a href=&quot;https://med.stanford.edu 的研究人员共同撰写的&lt;a href=&quot;https://arxiv.org/abs/2402.18545&quot;>;研究论文&lt;/a>;中对此进行了描述/&quot;>;斯坦福大学医学&lt;/a>;。这种方法使个人能够在医疗保健研究中发挥积极作用。它使我们能够在人们健康问题的早期阶段（可能在他们寻求正式护理之前）接触到他们。至关重要的是，这种方法使用网络搜索结果页面上的广告（许多人健康之旅的起点）来与参与者建立联系。 &lt;/p>; &lt;p>; 我们的结果表明，众包可以产生具有较低垃圾邮件率的高质量数据集。超过 97.5% 的贡献是皮肤状况的真实图像。在执行进一步的过滤步骤以排除超出 SCIN 数据集范围的图像并删除重复项后，我们能够发布在 8 个月的研究期间收到的近 90% 的贡献。大多数图像都很清晰且曝光良好。大约一半的贡献包括自我报告的人口统计数据，80% 包含与皮肤状况相关的自我报告信息，例如质地、持续时间或其他症状。我们发现皮肤科医生回顾性鉴别诊断的能力更多地取决于自我报告信息的可用性，而不是图像质量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDC jQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81caAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s1999/image4.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1610&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6 fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;皮肤科医生对其标签的信心（范围从 1 到 5）取决于自我报告的人口统计和症状信息的可用性。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 虽然永远无法保证完美的图像去识别，但在创建 SCIN 数据集时，保护贡献图像的个人的隐私是首要任务。通过知情同意，贡献者意识到潜在的重新识别风险，并建议避免上传具有识别特征的图像。提交后的隐私保护措施包括手动编辑或裁剪以排除潜在的识别区域、反向图像搜索以排除公开可用的副本以及元数据删除或聚合。 SCIN &lt;a href=&quot;https://github.com/google-research-datasets/scin?tab=License-1-ov-file#readme&quot;>;数据使用许可&lt;/a>;禁止尝试重新识别贡献者。 &lt;/p>; &lt;p>; 我们希望 SCIN 数据集能够为那些致力于推进包容性皮肤病学研究、教育和人工智能工具开发的人们提供有用的资源。通过展示传统数据集创建方法的替代方法，SCIN 为自我报告数据或回顾性标记可行的领域中更具代表性的数据集铺平了道路。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢所有合著者 Abbi Ward , Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirisokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel (斯坦福大学医学院) ）、Steven Lin（斯坦福医学）、Justin Ko（斯坦福医学）、Alan Karthikesalingam 和 Christopher Semturs。我们还感谢 Yetunde Ibitoye、Sami Lachgar、Lisa Lehmann、Javier Perez、Margaret Ann Smith（斯坦福大学医学院）、Rachelle Sico、Amit Talreja、Annisah Um&#39;rani 和 Wayne Westerlind 对这项工作的重要贡献。最后，我们感谢 Heather Cole-Lewis、Naama Hammel、Ivor Horn、Michael Howell、Yun Liu 和 Eric Teasley 对研究设计和手稿的富有洞察力的评论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4328167517765145678/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/scin-new-resource-for-representative.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4328167517765145678&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4328167517765145678&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/scin-new-resource-for-representative.html&quot; rel=&quot;alternate&quot; title=&quot;SCIN：代表性皮肤病图像的新资源&quot; type=&quot;text/html&quot; />;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image高度=“16” rel=“http://schemas.google.com/g/2005#thumbnail” src=“https://img1.blogblog.com/img/b16-rounded.gif” 宽度=“16” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3Cd KparGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s72-c/SCINHero .png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;条目>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-757976001746578714&lt;/id>;&lt;发布>;2024-03-18T11:41:00.000-07:00&lt;/发布>;&lt;更新>;2024-03 -18T12：01：42.865-07：00 &lt;/更新>; &lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“计算机视觉”>;&lt;/类别>;&lt;类别方案=“ http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MELON：从姿势未知的图像重建 3D 对象&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院高级软件工程师 Mark Matthews 和研究科学家 Dmitry Lagun&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gV aY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s320/MELON%20HERO.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 一个人先前的经验和对世界的理解通常使他们能够轻松地推断出一个物体的整体外观，即使只看它的一些 2D 图片。然而，计算机在仅给定少量图像的情况下以 3D 形式重建物体形状的能力多年来仍然是一个困难的算法问题。这项基本的计算机视觉任务的应用范围从电子商务 3D 模型的创建到自动车辆导航。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 问题的关键部分是如何确定拍摄图像的确切位置，即&lt;em>;姿势推断&lt;/em>;。如果相机姿势已知，则可以使用一系列成功的技术 - 例如&lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;神经辐射场&lt;/a>; (NeRF) 或&lt;a href=&quot;https: //repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&quot;>;3D 高斯分布&lt;/a>; — 可以重建 3D 对象。但如果这些姿势不可用，那么我们将面临一个困难的“先有鸡还是先有蛋”的问题，如果我们知道 3D 对象，我们就可以确定姿势，但在我们知道相机姿势之前，我们无法重建 3D 对象。伪对称性使问题变得更加困难——即，从不同角度观察时，许多物体看起来很相似。例如，像椅子这样的方形物体每旋转 90° 看起来就会很相似。通过从不同角度将物体渲染在转盘上并绘制其光度&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-similarity&quot;>;自相似性&lt;/a，可以揭示物体的伪对称性>; 地图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRm kpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s1764/image5.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;923&quot; data-original-width=&quot;1764&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwV uxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL 2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;玩具卡车模型的自相似图。 &lt;strong>;左：&lt;/strong>;模型在转盘上以各种&lt;a href=&quot;https://en.wikipedia.org/wiki/Azimuth&quot;>;方位角&lt;/a>;、θ 进行渲染。 &lt;strong>;右：&lt;/strong>; θ 渲染的平均 &lt;a href=&quot;https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm&quot;>;L2&lt;/a>; RGB 相似度θ*。伪相似性由红色虚线表示。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 上图仅可视化一维旋转。当引入更多自由度时，它会变得更加复杂（并且难以可视化）。伪对称性使问题变得不适定，简单的方法常常收敛于局部最小值。在实践中，这种方法可能会将后视图误认为是对象的前视图，因为它们具有相似的轮廓。以前的技术（例如 &lt;a href=&quot;https://chenhsuanlin.bitbucket.io/bundle- adjustment-NeRF/&quot;>;BARF&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/abs/2205.15768 &quot;>;SAMURAI&lt;/a>;）通过依赖接近全局最小值的初始姿态估计来回避这个问题。但如果这些都不可用，我们该如何解决这个问题呢？ &lt;/p>; &lt;p>; 方法，例如 &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_GNeRF_GAN-Based_Neural_Radiance_Field_Without_Posed_Camera_ICCV_2021_paper.pdf&quot;>;GNeRF&lt;/a>; 和 &lt;a href=&quot; https://dl.acm.org/doi/10.1145/3503161.3548078&quot;>;VMRF&lt;/a>;利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;>;生成对抗网络&lt;/a>; （GAN）来克服这个问题。这些技术能够人为地“放大”有限数量的训练视图，从而帮助重建。然而，GAN 技术通常具有复杂且有时不稳定的训练过程，使得在实践中难以实现稳健且可靠的收敛。一系列其他成功的方法，例如 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.html&quot;>;SparsePose&lt;/a>; 或 &lt;a href=&quot;https: //rust-paper.github.io/&quot;>;RUST&lt;/a>;，可以从有限数量的视图中推断姿势，但需要对大量姿势图像数据集进行预训练，而这些图像并不总是可用的，并且可能会受到影响推断不同类型图像的姿势时的“域间隙”问题。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;MELON: NeRF with Unposeed Images in SO(3)&lt;/a>;”中，重点关注&lt;a href= “https://3dvconf.github.io/2024/&quot;>;3DV 2024&lt;/a>;，我们提出了一种技术，可以在以 3D 方式重建对象的同时完全从头开始确定以对象为中心的相机姿势。 &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON&lt;/a>;（NeRF 的模等效潜在优化）是第一个无需初始相机姿态估计和复杂训练即可实现此目的的技术对标记数据进行方案或预训练。 MELON 是一种相对简单的技术，可以轻松集成到现有的 NeRF 方法中。我们证明，MELON 可以从未摆出的图像中以最先进的精度重建 NeRF，同时只需要 4-6 个物体的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MELON&lt;/h2>; &lt;p>; 我们利用两项关键技术来帮助这种不适定的收敛问题。第一个是一个非常轻量级的、动态训练的&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 编码器，可以从训练图像中回归相机姿势。我们将缩小的训练图像传递给四层 CNN，以推断相机姿态。该 CNN 从噪声中初始化，无需预训练。它的容量非常小，以至于它迫使相似的图像呈现相似的姿势，提供隐式正则化，极大地帮助收敛。 &lt;/p>; &lt;p>; 第二种技术是模损失，同时考虑对象的伪对称性。我们从每个训练图像的一组固定视点渲染对象，仅通过最适合训练图像的视图反向传播损失。这有效地考虑了每个图像的多个视图的合理性。在实践中，我们发现大多数情况下只需要 &lt;em>;N&lt;/em>;=2 个视图（从另一侧查看对象），但有时使用 &lt;em>;N&lt;/em>;=4 可以获得更好的结果方形物体。 &lt;/p>; &lt;p>; 这两种技术被集成到标准 NeRF 训练中，只不过不是固定相机姿势，而是由 CNN 推断姿势并通过模损失复制。光度梯度通过最合适的相机反向传播到 CNN。我们观察到相机通常会快速收敛到全局最佳姿势（参见下面的动画）。经过神经场训练后，MELON 可以使用标准 NeRF 渲染方法合成新颖的视图。 &lt;/p>; &lt;p>; 我们通过使用 &lt;a href=&quot;https://github.com/bmild/nerf&quot;>;NeRF-Synthetic&lt;/a>; 数据集来简化问题，该数据集是 NeRF 研究的流行基准，在姿势推断文献。这个合成数据集的摄像头距离精确固定，并且方向一致，要求我们仅推断出相机。这与位于地球中心的物体相同，相机始终指向它，并沿着表面移动。然后我们只需要纬度和经度（2 个自由度）来指定相机姿态。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRApBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7l ApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s1315/image4.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;395&quot; data-original-width=&quot;1315&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzT ayiw3DMijPHS0ovkLVTcQGpp2_gayA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekg bFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;MELON 使用动态训练的轻量级 CNN 编码器来预测每个图像的姿势。预测的姿势通过模损失来复制，它仅惩罚距地面真实颜色的最小 L2 距离。在评估时，神经场可用于生成新颖的视图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们计算两个关键指标来评估 MELON 在 NeRF 合成数据集上的性能。地面实况和推断姿势之间的方向误差可以量化为我们在所有训练图像上平均的单个角度误差，即姿势误差。然后，我们通过测量&lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;峰值信噪比&lt;/a>;来测试 MELON 从新视图渲染对象的准确性（PSNR）反对提出测试意见。我们看到，MELON 在训练的前 1,000 步内快速收敛到大多数相机的近似姿势，并在 50k 步后实现了 27.5 dB 的有竞争力的 PSNR。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02y b3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s960/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69 __A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz 8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;优化过程中玩具卡车模型上的 MELON 收敛。 &lt;strong>;左&lt;/strong>;：NeRF 的渲染。 &lt;strong>;右&lt;/strong>;：预测（蓝色&lt;em>;x&lt;/em>;）和地面实况（红点）摄像机的极坐标图。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MELON 在 NeRF 合成数据集中的其他场景中取得了类似的结果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRK ToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s1999/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7ck-1DZuHaZRDq4Y59_C riULb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqy WIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;100k 训练步骤后 NeRF-Synthetic 场景上的 ground-truth (GT) 和 MELON 的重建质量比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;嘈杂的图像&lt;/h3>; &lt;p>; MELON 在执行&lt;a href=&quot;https://en.wikipedia 时也能很好地工作。 org/wiki/View_synthesis&quot;>;新颖的视图合成&lt;/a>;来自极其嘈杂的、未摆姿势的图像。我们向训练图像中添加不同数量的&lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise&quot;>;高斯白噪声&lt;/a>;。例如，下面 &lt;em>;σ&lt;/em>;=1.0 中的物体是无法辨认的，但 MELON 可以确定该物体的姿势并生成该物体的新颖视图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTR VkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s1182/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1182&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3 CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh _hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;从嘈杂的未摆姿势的 128×128 图像中合成新颖的视图。顶部：训练视图中存在的噪声级别示例。底部：根据嘈杂的训练视图和平均角度姿势误差重建模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;考虑到诸如 &lt;a href= 之类的技术，这也许不应该太令人惊讶“https://bmild.github.io/rawnerf/&quot;>;RawNeRF&lt;/a>; 展示了 NeRF 在已知相机姿势的情况下出色的去噪能力。事实上，MELON 对于未知相机姿势的噪声图像如此有效，这是出乎意料的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出了 MELON，一种可以确定以对象为中心的相机的技术姿势重建 3D 对象，无需近似姿势初始化、复杂的 GAN 训练方案或对标记数据进行预训练。 MELON 是一种相对简单的技术，可以轻松集成到现有的 NeRF 方法中。虽然我们只在合成图像上演示了 MELON，但我们正在调整我们的技术以适应现实世界的条件。请参阅&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;论文&lt;/a>;和&lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON 网站&lt;/a>; >; 了解更多。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢我们的论文共同作者Axel Levy、Matan Sela 和 Gordon Wetzstein，以及 Florian Schroff 和 Hartwig Adam 在构建这项技术方面不断提供帮助。我们还感谢 Matthew Brown、Ricardo Martin-Brualla 和 Frederic Poitevin 对论文草稿提供的有益反馈。我们还感谢使用 SLAC 共享科学数据设施 (SDF) 的计算资源。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/757976001746578714 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing- 3d-objects-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626 /posts/default/757976001746578714&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel= “self” type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html&quot; rel=&quot;alternate&quot; title=&quot;MELON：从姿势未知的图像重建 3D 对象&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/ 12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https: //img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXP az_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_ gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s72-c/MELON%20HERO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search. yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post- 977556648557231190&lt;/id>;&lt;发布>;2024-03-15T11:22:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-15T11:22:13.760-07:00&lt;/更新>;&lt;类别方案= http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI &quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;HEAL：机器学习性能健康公平评估框架&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者迈克·谢克曼 (Mike Schaekermann)，谷歌研究院研究科学家；艾弗·霍恩 (Ivor Horn)，首席健康股权官Google 核心总监&lt;/span>; &lt;img=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn 2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s1600/HEAL-Hero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 健康公平是世界范围内的一个主要社会问题，其差异有多种原因。这些来源包括获得医疗保健的限制、临床治疗的差异，甚至诊断技术的根本差异。例如，在皮肤科中，少数族裔、社会经济地位较低的人或医疗保健机会有限的个人等人群的皮肤癌结果更差。虽然机器学习 (ML) 和人工智能 (AI) 的最新进展有望帮助改善医疗保健，但从研究到临床的转变必须伴随着仔细了解它们是否以及如何影响健康公平。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;em>;健康公平&lt;/em>;由公共卫生组织定义为每个人尽可能保持健康的机会公平。重要的是，公平可能不同于&lt;em>;平等&lt;/em>;。例如，在改善健康方面存在更大障碍的人可能需要更多或不同的努力才能体验这个公平的机会。同样，公平并不是医疗保健人工智能文献中定义的&lt;em>;公平&lt;/em>;。尽管人工智能公平性通常力求人工智能技术在不同患者群体中具有平等的性能，但这并不以优先考虑现有健康差异的性能为中心。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmF n4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s1999 /image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1609&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4l oEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6X QVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s16000/image2.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;健康公平考虑因素。如果干预措施（例如基于机器学习的工具，以深蓝色表示）有助于减少现有的健康结果差异（以浅蓝色表示），则可以促进健康公平。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; 在“&lt;a href=&quot;https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00058-0/fulltext&quot;>;机器学习性能的健康公平评估 (HEAL) ：框架和皮肤病学人工智能模型案例研究&lt;/a>;”，发表于&lt;a href=&quot;https://www.thelancet.com/journals/eclinm/home&quot;>;&lt;i>;The Lancet eClinicalMedicine&lt;/i>;&lt; /a>;，我们提出了一种定量评估基于机器学习的医疗技术是否公平运行的方法。换句话说，机器学习模型对于那些在模型要解决的情况下健康状况最差的人来说是否表现良好？这一目标基于以下原则：健康公平应优先考虑并衡量模型在不同健康结果方面的表现，这可能是由于包括结构性不平等在内的许多因素（例如人口、社会、文化、政治、经济、环境和地理）。 &lt;/p>; &lt;br />; &lt;h2>;健康公平框架 (HEAL)&lt;/h2>; &lt;p>; HEAL 框架提出了一个 4 步流程来估计基于 ML 的健康技术公平执行的可能性：&lt;/p>; p>; &lt;ol>; &lt;li>; 确定与健康不平等相关的因素并定义工具性能指标，&lt;/li>; &lt;li>; 确定并量化先前存在的健康差异，&lt;/li>; &lt;li>; 衡量工具的性能每个亚群体，&lt;/li>; &lt;li>; 衡量该工具根据健康差异优先考虑绩效的可能性。 &lt;/li>; &lt;/ol>; &lt;p>; 最后一步的输出称为 HEAL 指标，它量化 ML 模型的性能与健康差异的反相关程度。换句话说，该模型对于健康状况较差的人群是否表现更好？ &lt;/p>; &lt;p>; 这个 4 步流程旨在提供改进信息，使 ML 模型性能更加公平，并定期进行迭代和重新评估。例如，步骤（2）中健康结果数据的可用性可以告知步骤（1）中人口因素和括号的选择，并且该框架可以再次应用新的数据集、模型和人群。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4 sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s1999/image1.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1352&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniW KIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14 QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;机器学习性能健康公平评估框架 (HEAL)。我们的指导原则是避免加剧健康不平等，这些步骤帮助我们识别差异并评估不公平模型&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;通过这项工作，我们朝着鼓励对人工智能技术的健康公平考虑因素进行明确评估迈出了一步，并且鼓励在模型开发过程中优先考虑减少面临结构性不平等的亚人群的健康不平等，因为结构性不平等可能会导致不同的结果。我们应该注意到，目前的框架并未对因果关系进行建模，因此无法量化新技术对减少健康结果差异的实际影响。然而，HEAL 指标可能有助于识别改进的机会，其中当前的表现并未优先考虑先前存在的健康差异。 &lt;/p>; &lt;br />; &lt;h2>;皮肤病学模型案例研究&lt;/h2>; &lt;p>; 作为一个说明性案例研究，我们将该框架应用于皮肤病学模型，该模型利用类似于中描述的卷积神经网络&lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;之前的工作&lt;/a>;。此示例皮肤病学模型经过训练，可使用包含 29,000 个病例的开发数据集对 288 种皮肤状况进行分类。模型的输入包括三张皮肤问题的照片以及人口统计信息和简短的结构化病史。输出包含可能匹配的皮肤状况的排名列表。 &lt;/p>; &lt;p>; 使用 HEAL 框架，我们通过评估该模型是否根据预先存在的健康结果优先考虑性能来评估该模型。该模型旨在根据皮肤问题的照片和患者元数据预测可能的皮肤病（从数百个列表中）。模型的评估是使用前 3 个一致性指标来完成的，该指标量化了前 3 个输出条件与皮肤科医生小组建议的最可能条件相匹配的频率。 HEAL 指标是通过此前 3 名协议与健康结果排名的反相关来计算的。 &lt;/p>; &lt;p>; 我们使用了包含 5,420 个远程皮肤病病例的数据集（丰富了年龄、性别和种族/民族的多样性）来回顾性评估模型的 HEAL 指标。该数据集包含来自美国初级保健提供者和澳大利亚皮肤癌诊所的 20 岁或以上患者的“存储并转发”病例。基于对文献的回顾，我们决定探索种族/民族、性别和年龄作为不平等的潜在因素，并使用抽样技术来确保我们的评估数据集充分代表所有种族/民族、性别和年龄组。为了量化每个亚组预先存在的健康结果，我们依赖于 &lt;a href=&quot;https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates/global- health-estimates-leading-causes-of-dalys&quot;>;公共&lt;/a>; &lt;a href=&quot;https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30925-9/fulltext &quot;>;世界卫生组织认可的数据库&lt;/a>;，例如&lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4427&quot;>;损失寿命&lt;/a>; (YLL) 和&lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158&quot;>;伤残调整生命年&lt;/a >;（伤残调整生命年；生命损失年数加上残疾年数）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6r OqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s1511 /Table1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;第1511章QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s16000/Table1.png&quot;/>;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;跨种族/族裔亚群的所有皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 YLL）、模型性能（ top-3 协议），以及健康结果和工具性能的排名。&lt;br />;（*越高越好；衡量模型相对于表中的轴公平执行的可能性。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwn zza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHt FmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s1518/Table2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right:自动;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;316&quot; data-original-width=&quot;1518&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza- QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1ga BCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZG jxSkNt9JQK/s16000/Table2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;针对不同性别的所有皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 DALY）、模型性能（前 3 名一致性）以及健康结果和工具性能的排名。 （* 如上所述。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table &lt;p>; 我们的分析估计，该模型有 80.5% 的可能性在种族/族裔亚组中公平地表现，92.1% 的可能性在不同种族/族裔亚组中公平地表现。性别。 &lt;/p>; &lt;p>; 然而，虽然该模型在癌症疾病的各个年龄组中可能表现相当，但我们发现它在非癌症疾病的各个年龄组中都有改进的空间。例如，70 岁以上的人与非癌症皮肤状况相关的健康状况最差，但该模型并未优先考虑该亚组的表现。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl 6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s1508/Table3 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1508&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPn LkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s16000/Table3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;跨年龄组的所有癌症和非癌症皮肤病的 HEAL 指标，包括健康结果（每 100,000 人的 DALY）、模型性能（顶部-3 协议），以及健康结果和工具性能的排名。 （* 同上。）&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;将事物放在上下文中&lt;/h2>; &lt;p>;对于整体评估，不能采用 HEAL 指标隔离中。相反，该指标应与许多其他因素结合起来，从计算效率和数据隐私到道德价值观，以及可能影响结果的方面（例如，选择偏差或不同人口群体的评估数据的代表性差异）。 &lt;/p>; &lt;p>; 作为一个对抗性的例子，可以通过故意降低最有利的子群体的模型性能来人为地改进 HEAL 指标，直到该子群体的性能比所有其他子群体更差。出于说明目的，给定亚群 A 和 B，其中 A 的健康结果比 B 差，请考虑在两个模型之间进行选择：模型 1 (M1) 对亚群 A 的表现比对亚群 B 的表现好 5%。模型 2 (M2) 的表现好 5%子群体 A 比 B 更差。M1 的 HEAL 指标会更高，因为它优先考虑结果更差的子群体的表现。然而，M1 对于亚群 A 和 B 的绝对性能可能分别仅为 75% 和 70%，而 M2 对于亚群 A 和 B 的绝对性能分别为 75% 和 80%。选择 M1 而不是 M2 会导致所有子群体的整体表现更差，因为有些子群体的状况更糟，而没有子群体的状况更好。 &lt;/p>; &lt;p>; 因此，HEAL 指标应与&lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_efficiency&quot;>;帕累托条件&lt;/a>;一起使用（本文将进一步讨论） ，这限制了模型的变化，使得每个亚群的结果与现状相比要么保持不变，要么有所改善，并且任何亚群的表现都不会恶化。 &lt;/p>; &lt;p>; HEAL 框架目前的形式评估了基于机器学习的模型相对于特定亚群预先存在的健康差异优先考虑亚群表现的可能性。这与了解机器学习是否会减少现实中不同亚人群结果差异的目标不同。具体来说，对结果的改善进行建模需要对使用任何给定模型之前和之后发生的护理旅程中的步骤进行因果理解。需要未来的研究来解决这一差距。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; HEAL 框架可以定量评估健康人工智能技术在健康差异方面优先考虑性能的可能性。该案例研究展示了如何将该框架应用于皮肤病学领域，表明模型性能很可能优先考虑性别和种族/民族之间的健康差异，但也揭示了改善跨年龄非癌症疾病的潜力。该案例研究还说明了应用该框架所有建议方面的能力的局限性（例如，绘制社会背景、数据的可用性），从而凸显了基于机器学习的工具的健康公平考虑的复杂性。 &lt;/p>; &lt;p>; 这项工作是为应对人工智能和健康公平的巨大挑战而提出的方法，不仅可以在模型开发过程中提供有用的评估框架，而且可以在预实施和现实世界监测阶段提供有用的评估框架，例如，以健康公平仪表板的形式。我们认为 HEAL 框架的优势在于其未来在各种人工智能工具和用例中的应用以及在此过程中的完善。最后，我们承认，了解人工智能技术对健康公平影响的成功方法需要的不仅仅是一组指标。这需要代表受模型影响最大的群体的社区商定的一系列目标。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的研究是 Google 多个团队的共同工作。我们感谢所有合著者：Terry Spitz、Malcolm Pyles、Heather Cole-Lewis、Ellery Wulczyn、Stephen R. Pfohl、Donald Martin, Jr.、Ronnachai Jaroensri、Geoff Keeling、Yuan Liu、Stephanie Farquhar、Qinghan Xu、 Jenna Lester、Cían Hughes、Patricia Strachan、Fraser Tan、Peggy Bui、Craig H. Mermel、Lily H. Peng、Yossi Matias、Greg S. Corrado、Dale R. Webster、Sunny Virmani、Christopher Semturs、Yun Liu 和 Po-陈宣.卡梅隆.我们还感谢 Lauren Winer、Sami Lachgar、Ting-An Lin、Aaron Loh、Morgan Du、Jenny Rizk、Renee Wong、Ashley Carrick、Preeti Singh、Annisah Um&#39;rani、Jessica Schrouff、Alexander Brown 和 Anna Iurchenko 对我们的支持此项目。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/977556648557231190/comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;edit&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href =&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html&quot; rel=&quot;alternate&quot; title=&quot;HEAL：机器学习性能健康公平评估框架&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkEC IxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSaf2h0jETJ1PemIR5I6o9pIIW/s72 -c/HEAL-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total >;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7868032799856333119&lt;/id>;&lt;已发布>;2024-03-14T12:38:00.000-07:00&lt;/已发布>; &lt;更新>;2024-03-14T12:38:11.597-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;大语言模型&quot;>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“机器学习”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/” ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;title type=&quot; text&quot;>;Cappy：用小记分器超越并提升大型多任务语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：软件工程师 Yun Zhu 和 Lijuan Liu , Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0E jUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s320/Cappy%20hero.jpg&quot;样式= “显示：无；” />; &lt;p>; 大型语言模型 (LLM) 的进步催生了一种新范式，将各种自然语言处理 (NLP) 任务统一在指令跟踪框架内。这种范式以最近的多任务法学硕士为例，例如 &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;、&lt;a href=&quot;https://arxiv.org/ abs/2210.11416&quot;>;FLAN&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML&lt;/a>;。首先，收集多任务数据，每个任务都遵循特定于任务的模板，其中每个标记的示例都转换为指令（例如，&lt;em>;“&lt;/em>;将概念放在一起形成一个句子：滑雪，山，滑雪者&lt;em>;”&lt;/em>;）与相应的响应配对（例如，&lt;em>;“&lt;/em>;滑雪者滑雪下山&lt;em>;”&lt;/em>;）这些指令-响应对用于训练 LLM，产生一个条件生成模型，该模型将指令作为输入并生成响应。此外，多任务 LLM 表现出卓越的任务泛化能力，因为它们可以通过理解和解决全新指令来解决看不见的任务。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt- ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6u whawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s640/Cappy% 20instruction-following.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;177&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOT NGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s16000/Cappy%20instruction-following.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;多任务LLM（例如FLAN）的指令跟踪预训练演示。该范式下的预训练任务提高了未见任务的性能。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 由于仅使用指令理解和解决各种任务的复杂性，多任务 LLM 的大小通常从数十亿个参数到数千亿个参数（例如，&lt;a href=&quot;https ://arxiv.org/abs/2210.11416&quot;>;FLAN-11B&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0-11B&lt;/a>; 和 &lt;a href= “https://arxiv.org/abs/2212.12017&quot;>;OPT-IML-175B&lt;/a>;）。因此，操作如此庞大的模型带来了巨大的挑战，因为它们需要相当大的计算能力，并对 GPU 和 TPU 的内存容量提出了很高的要求，使得它们的训练和推理成本高昂且效率低下。需要大量存储来为每个下游任务维护唯一的 LLM 副本。此外，最强大的多任务LLM（例如FLAN-PaLM-540B）是闭源的，这使得它们不可能被改编。然而，在实际应用中，利用单个多任务LLM以零样本的方式管理所有可以想象的任务仍然很困难，特别是在处理复杂任务、个性化任务以及无法使用指令简洁定义的任务时。另一方面，如果不结合丰富的先验知识，下游训练数据的大小通常不足以很好地训练模型。因此，长期以来人们一直希望使法学硕士能够接受下游监督，同时绕过存储、内存和访问问题。 &lt;/p>; &lt;p>; 某些&lt;em>;参数高效调整&lt;/em>;策略，包括&lt;a href=&quot;https://aclanthology.org/2021.acl-long.353.pdf&quot;>;提示调整&lt;/em>; a>; 和 &lt;a href=&quot;https://openreview.net/pdf?id=nZeVKeeFYf9&quot;>;适配器&lt;/a>; 大大减少了存储需求，但它们在调整过程中仍然通过 LLM 参数执行反向传播，从而保持高记忆力要求。此外，一些&lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/2301.00234.pdf&quot;>;上下文学习&lt;/a>;&lt;/em>;技术通过集成有限数量的监督示例来规避参数调整进入指令。然而，这些技术受到模型最大输入长度的限制，仅允许少量样本来指导任务解决。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2311.06720&quot;>;Cappy：使用小型记分器超越并提升大型多任务 LM&lt;/a>;”中，发表于&lt; a href=&quot;https://nips.cc/virtual/2023/index.html&quot;>;NeurIPS 2023&lt;/a>;，我们提出了一种新方法，可以提高多任务 LLM 的性能和效率。我们引入了一种轻量级预训练记分器 Cappy，它基于 &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; 的持续预训练，仅包含 3.6 亿个参数。 Cappy 将指令和候选响应作为输入，并产生 0 到 1 之间的分数，指示响应相对于指令的估计正确性。 Cappy 可以独立执行分类任务，也可以作为法学硕士的辅助组件，提高其性能。此外，Cappy 可以有效地实现下游监督，无需任何微调，从而避免了通过 LLM 参数进行反向传播的需要，并减少了内存需求。最后，Cappy 的适配不需要访问 LLM 参数，因为它与闭源多任务 LLM 兼容，例如那些只能通过 WebAPI 访问的参数。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-m fiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s1999/Cappy %20overview.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;975&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1 knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s16000/Cappy%20overview.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy 将指令和响应对作为输入，并输出 0 到 1 之间的分数，表示对响应正确性的估计遵守指示。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;预训练&lt;/h2>; &lt;p>; 我们从相同的数据集集合开始，其中包括使用的来自 &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; 的 39 个不同数据集训练&lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;。该集合包含广泛的任务类型，例如问答、情感分析和摘要。每个数据集都与一个或多个模板相关联，这些模板将每个实例从原始数据集转换为与其真实响应配对的指令。 &lt;/p>; &lt;p>; Cappy 的回归建模要求每个预训练数据实例包含一个指令-响应对以及响应的正确性注释，因此我们生成一个具有范围从 0 到 1 的正确性注释的数据集。在生成任务中的实例中，我们利用现有的多任务 LLM 通过采样生成多个响应，以给定的指令为条件。随后，我们利用响应与实例的真实响应之间的相似性，为指令和每个响应形成的对分配一个注释。具体来说，我们采用 &lt;a href=&quot;https://aclanthology.org/W04-1013/&quot;>;Rouge-L&lt;/a>;，这是一种用于衡量整体多任务性能的常用指标，已证明与人类评估，将这种相似性计算为弱监督的一种形式。 &lt;/p>; &lt;p>; 结果，我们获得了包含 1.6 亿个实例的有效回归数据集，并配有正确性分数注释。最终的 Cappy 模型是在 &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; 模型之上使用回归数据集进行连续预训练的结果。 Cappy 的预训练是在 Google 的 &lt;a href=&quot;https://arxiv.org/abs/2304.01433&quot;>;TPU-v4&lt;/a>; 上进行的，使用 &lt;a href=&quot;https://arxiv.org/ pdf/2310.16355.pdf&quot;>;RedCoast&lt;/a>;，一个用于自动化分布式训练的轻量级工具包。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLy ZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s1999/Cappy%20data%20增强.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;438&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z0 3aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdI K_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s16000/Cappy%20data%20augmentation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用多任务LLM进行数据增强，为Cappy的预训练和微调构建弱监督回归数据集。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;应用Cappy&lt;/h2​​>; &lt;p>; Cappy解决实际任务在候选人选择机制内。更具体地说，给定指令和一组候选响应，Cappy 会为每个候选响应生成一个分数。这是通过在每个单独的响应旁边输入指令，然后指定得分最高的响应作为其预测来实现的。在分类任务中，所有候选响应本质上都是预定义的。例如，对于情感分类任务的指令（例如，“根据此评论，用户会推荐该产品吗？：‘即使对于非游戏玩家来说也令人惊叹。’”），候选回答是“是”或“不”。在这种情况下，Cappy 独立运作。另一方面，在生成任务中，候选答案不是预先定义的，需要现有的多任务 LLM 来生成候选答案。在这种情况下，Cappy 充当多任务 LLM 的辅助组件，增强其解码。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;使用 Cappy 调整多任务 LLM &lt;/h3>; &lt;p>; 当有可用的下游训练数据时，Cappy 能够有效且高效地适应下游任务上的多任务 LLM。具体来说，我们对 Cappy 进行微调，将下游任务信息集成到 LLM 预测中。此过程涉及创建特定于下游训练数据的单独回归数据集，并使用与构建预训练数据相同的数据注释过程。因此，经过微调的 Cappy 与多任务 LLM 协作，提高了 LLM 在下游任务上的性能。 &lt;/p>; &lt;p>; 与其他 LLM 调整策略相比，使用 Cappy 调整 LLM 显着降低了对设备内存的高需求，因为它避免了下游任务通过 LLM 参数进行反向传播的需要。此外，Cappy 适配不依赖于对 LLM 参数的访问，使其与闭源多任务 LLM 兼容，例如只能通过 WebAPI 访问的 LLM。与通过将训练示例附加到指令前缀来规避模型调整的上下文学习方法相比，Cappy 不受 LLM 最大输入长度的限制。因此，Cappy 可以合并无限数量的下游训练示例。 Cappy 还可以与其他适应方法一起应用，例如微调和上下文学习，进一步提高其整体性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyn cRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s1999/Cappy%20downstream%20适应.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSX fP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s16000/Cappy%20downstream%20adaptation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy 和依赖于 LLM 参数的方法（例如微调和提示调整）之间的下游适应性比较。 Cappy 的应用程序增强了多任务 LLM。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们评估了 Cappy 在来自 &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; 的十一项语言理解分类任务中的表现。我们证明，具有 360M 参数的 Cappy 性能优于 OPT-175B 和 OPT-IML-30B，并且与现有最好的多任务 LLM（T0-11B 和 OPT-IML-175B）的准确性相匹配。这些发现凸显了 Cappy 的能力和参数效率，这可以归功于其基于评分的预训练策略，该策略通过区分高质量和低质量响应来整合对比信息。相反，以前的多任务法学硕士完全依赖于仅利用真实答案的&lt;a href=&quot;https://en.wikipedia.org/wiki/Teacher_forcing&quot;>;教师强制训练&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9 ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s1999/Cappy%20精度.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_ 5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s16000/Cappy%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;总体准确度是 PromptSource 十一项测试任务的平均值。 “RM”是指&lt;a href=&quot;https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2&quot;>;预训练的 RLHF 奖励模型&lt;/a>;。 Cappy 与现有多任务 LLM 中最好的匹配。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 我们还研究了多任务 LLM 与 Cappy 对复杂任务的适应性 &lt;a href =&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;，一组手动策划的任务，被认为超出了许多法学硕士的能力范围。我们专注于所有 45 代 BIG-Bench 任务，特别是那些不提供预先确定的答案选择的任务。我们在每个测试集上使用 Rouge-L 分数（代表模型生成与相应的基本事实之间的总体相似性）来评估性能，报告 45 次测试的平均分数。在这个实验中，FLAN-T5 的所有变体都作为 LLM 的骨干，并且基础 FLAN-T5 模型被冻结。如下所示的这些结果表明，Cappy 大幅提高了 FLAN-T5 模型的性能，始终优于通过使用 LLM 本身的自我评分进行样本选择而实现的最有效基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2f hhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s1999/Cappy%20平均%20Rouge-L%20score.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1625&quot; data-original-width=&quot;1999 “ src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e _WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s16000/Cappy%20averaging%20Rouge-L%20score.png&quot;/>;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;BIG-Bench 中 45 项复杂任务的 Rouge-L 平均得分。 X轴代表不同尺寸的FLAN-T5型号。每条虚线代表一种适用于 FLAN-T5 的方法。自我评分是指利用LLM的交叉熵来选择答案。 Cappy大幅提升了FLAN-T5模型的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br / >; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们介绍 Cappy，这是一种提高多任务 LLM 性能和效率的新颖方法。在我们的实验中，我们使用 Cappy 将单个法学硕士应用于多个领域。未来，Cappy 作为预训练模型，有可能以除单一法学硕士以外的其他创造性方式使用。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢Bowen Tan、Jindong Chen、Lei Meng 、Abhanshu Sharma 和 Ewa Dominowska 的宝贵反馈。我们还要感谢 Eric Xing 和zhiting Hu 的建议。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7868032799856333119/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html&quot; rel=&quot;alternate&quot; title=&quot;Cappy：用小记分器超越并提升大型多任务语言模型&quot; 类型=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNf IzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up -Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s72-c/Cappy%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:总计>;0&lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-6090481872694489715&lt;/id>;&lt;发布>;2024-03-12T14:15:00.000 -07:00&lt;/已发布>;&lt;更新>;2024-03-19T09:12:05.568-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”term= “生成人工智能”&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category schema=&quot;http://www. blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;像图一样说话：大型语言模型的编码图&lt;/stitle>;&lt;content type=&quot;html &quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究院研究科学家 Bahare Fatemi 和 Bryan Perozzi&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4 DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s1600/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 想象一下您周围的所有事物 - 您的朋友、厨房里的工具，甚至自行车的零件。它们都以不同的方式连接起来。在计算机科学中，术语&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;图&lt;/a>;&lt;/em>;用于描述对象之间的连接。图由节点（对象本身）和边（两个节点之间的连接，指示它们之间的关系）组成。现在图表无处不在。互联网本身就是一个由链接在一起的网站组成的巨大图表。甚至搜索引擎使用的知识也是以类似图表的方式组织的。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>;此外，考虑一下人工智能的显着进步——例如可以在几秒钟内写出故事的聊天机器人，甚至可以解释医疗报告的软件。这一令人兴奋的进步很大程度上要归功于大型语言模型（LLM）。新的法学硕士技术正在不断开发用于不同的用途。 &lt;/p>; &lt;p>; 由于图无处不在，而且 LLM 技术正在兴起，因此在“&lt;a href=&quot;https://openreview.net/forum?id=IuXR1CCrSi&quot;>;像图一样交谈：对大型图进行编码”中语言模型&lt;/a>;”，在 &lt;a href=&quot;https://iclr.cc/&quot;>;ICLR 2024&lt;/a>; 上提出，我们提出了一种方法来教授强大的法学硕士如何更好地利用图形信息进行推理。图表是组织信息的有用方式，但法学硕士大多接受常规文本的培训。目的是测试不同的技术，看看哪种技术最有效并获得实用的见解。将图表翻译成法学硕士可以理解的文本是一项非常复杂的任务。困难源于具有多个节点的图结构固有的复杂性以及连接它们的复杂边缘网络。我们的工作研究如何获取图表并将其转换为法学硕士可以理解的格式。我们还设计了一个名为&lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>;的基准来研究不同的方法不同的图形推理问题，并展示如何以使得法学硕士能够解决图形问题的方式来表达图形相关问题。我们表明，LLM 在图推理任务上的表现在三个基本层面上有所不同：1）图编码方法，2）图任务本身的性质，3）有趣的是，所考虑的图的结构。这些发现为我们提供了如何最好地表示法学硕士图表的线索。选择正确的方法可以使法学硕士在图形任务方面的成绩提高高达 60%！ &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf 8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s1600/image7.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa 4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;如图所示，使用两种不同的方法将图形编码为文本，并将文本和有关图形的问题提供给法学硕士。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图形文本&lt;/h2>; &lt;p>; 能够系统地找出翻译图形的最佳方式为了文本，我们首先设计一个名为 &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>; 的基准测试。将 GraphQA 视为一项考试，旨在评估在特定于图的问题上强大的法学硕士。我们希望了解法学硕士能够如何很好地理解和解决涉及不同设置中的图形的问题。为了为法学硕士创建全面且真实的考试，我们不只使用一种类型的图表，而是使用多种图表的混合，以确保连接数量的广度。这主要是因为不同的图类型使解决此类问题变得更容易或更困难。通过这种方式，GraphQA 可以帮助揭露法学硕士对图表的看法的偏见，并且整个考试更接近法学硕士在现实世界中可能遇到的现实设置。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PK i9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s1368/image6.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;291&quot; data-original-width=&quot;1368&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLh JxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;我们使用 LLM 进行图形推理的框架概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; GraphQA 专注于与图形相关的简单任务，例如检查边缘是否存在，计算节点或边的数量，查找连接到特定节点的节点，并检查图中的循环。这些任务可能看起来很基本，但它们需要理解节点和边之间的关系。通过涵盖从识别模式到创建新连接等不同类型的挑战，GraphQA 可以帮助模型学习如何有效地分析图形。这些基本任务对于更复杂的图推理至关重要，例如寻找节点之间的最短路径、检测社区或识别有影响力的节点。此外，GraphQA 还包括使用各种算法生成随机图，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/ a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki /Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabasi-Albert 模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>; >;，以及更简单的图结构，如路径、完整图和星图，为训练提供了多样化的数据集。 &lt;/p>; &lt;p>; 在处理图表时，我们还需要找到方法来提出法学硕士可以理解的与图表相关的问题。 &lt;em>;提示启发式&lt;/em>;是实现此目的的不同策略。让我们分解一下常见的：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;零射击&lt;/em>;：简单地描述任务（“这张图中有循环吗？”）并告诉法学硕士去为了它。没有提供示例。 &lt;/li>;&lt;li>;&lt;em>;Few-shot&lt;/em>;：这就像在真正的交易之前给法学硕士进行一次小型练习测试。我们提供了一些示例图形问题及其正确答案。 &lt;/li>;&lt;li>;&lt;em>;思想链&lt;/em>;：在这里，我们通过示例向法学硕士展示如何逐步分解问题。目标是教它在面对新图表时生成自己的“思维过程”。 &lt;/li>;&lt;li>;&lt;em>;零CoT&lt;/em>;：与CoT类似，但我们没有给LLM提供训练示例，而是给LLM一个简单的提示，比如“让我们一步步思考”，以触发其自己解决问题的崩溃。 &lt;/li>;&lt;li>;&lt;em>;BAG（构建图形）&lt;/em>;：这是专门用于图形任务的。我们在描述中添加了短语“让我们构建一个图...”，帮助法学硕士专注于图结构。 &lt;/li>; &lt;/ul>; &lt;p>; 我们探索了将图表转换为法学硕士可以使用的文本的不同方法。我们的关键问题是： &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;节点编码&lt;/em>;：我们如何表示各个节点？测试的选项包括简单的&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer&quot;>;整数&lt;/a>;、常用名称（人物、字符）和字母。 &lt;/li>;&lt;li>;&lt;em>;边缘编码&lt;/em>;：我们如何描述节点之间的关系？方法涉及括号符号、“是朋友”等短语以及箭头等符号表示。 &lt;/li>; &lt;/ul>; &lt;p>; 各种节点和边编码被系统地组合起来。这导致了如下图所示的函数： &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; 右边距：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh -Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZchHDY-oS1ts/s85 5/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-宽度=“855”高度=“302”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUP vW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/w640-h302/image1.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于通过文本对图形进行编码的图形编码函数示例。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;分析与结果&lt;/h2>; &lt;p>;我们进行了三项关键实验：一是测试 LLM 如何处理图形任务，二是了解 LLM 的大小和不同图形形状如何影响性能。我们在 GraphQA 上运行所有实验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;LLM 如何处理图形任务&lt;/h3>; &lt;p>; 在这个实验中，我们测试了 pre 的效果如何。经过训练的法学硕士可以解决图形问题，例如识别连接、循环和节点度。以下是我们了解到的内容： &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;LLM 的挣扎：&lt;/em>;在大多数基本任务上，LLM 的表现并不比随机猜测好多少。 &lt;/li>;&lt;li>;&lt;em>;编码非常重要&lt;/em>;：我们如何将图形表示为文本对 LLM 性能有很大影响。一般来说，“事件”编码对于大多数任务都表现出色。 &lt;/li>; &lt;/ul>; &lt;p>; 我们的结果总结在下面的图表中。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssv z8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s1864/image8 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BO ALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据各种图形编码器函数在不同图形任务上的准确性进行比较。该图的主要结论是图形编码函数非常重要。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;越大（通常）越好&lt;/h3>; &lt;p>; 在这个实验中，我们想看看 LLM 的大小（就参数数量而言）是否会影响他们处理图形问题的能力。为此，我们在 &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>; 的 XXS、XS、S 和 L 尺寸上测试了相同的图形任务。以下是我们的发现摘要：&lt;/p>; &lt;ul>; &lt;li>;一般来说，较大的模型在图形推理任务上表现更好。额外的参数似乎给了他们学习更复杂模式的空间。奇怪的是，对于“边存在”任务（找出图中的两个节点是否相连）来说，大小并不那么重要。 &lt;/li>;&lt;li>;即使是最大的法学硕士也无法在循环检查问题（找出图表是否包含循环）上始终击败简单的基线解决方案。这表明法学硕士在某些图形任务方面仍有改进的空间。 &lt;/li>; &lt;/ul>; &lt;tablealign=“center”cellpadding=“0”cellspacing=“0”class=“tr-caption-container”style=“margin-left：auto；margin-right：auto；” >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2s Wc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/s1227 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;959&quot; data-original-width=&quot;1227&quot; height=&quot; 500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17Add foORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/w640-h500/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;模型容量对 PaLM 2-XXS、XS、S 和 L 的图形推理任务的影响。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;不同的图形形状是否会让法学硕士感到困惑&lt;/h3 >; &lt;p>; 我们想知道图的“形状”（节点如何连接）是否会影响法学硕士解决问题的能力。将下图视为图形形状的不同示例。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOafKwr7_w9dHTUD1xtnI6IMAswp 0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s1400/image4.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU 5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;使用 GraphQA 的不同图形生成器生成的图形示例。 ER、BA、SBM 和 SFN 指的是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;巴拉巴西-阿尔伯特&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;分别.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们发现图结构对LLM性能有很大影响。例如，在询问循环是否存在的任务中，法学硕士在紧密互连的图（循环在那里很常见）上表现出色，但在路径图（循环永远不会发生）上表现不佳。有趣的是，提供一些混合的例子有助于它适应。例如，对于循环检查，我们在提示中添加了一些包含循环的示例和一些不包含循环的示例作为少样本示例。其他任务也出现类似的模式。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJ JsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s1864/image5.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKETHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7LAy m4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;比较不同图形任务上的不同图形生成器。这里的主要观察是图结构对法学硕士的表现有重大影响。 ER、BA、SBM 和 SFN 指的是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a >;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;巴拉巴西-阿尔伯特&lt;/a>;、&lt;a href=&quot;https://en .wikipedia.org/wiki/Stochastic_block_model&quot;>;随机块模型&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;无标度网络&lt;/a>;分别.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;简而言之，我们深入研究了如何最好地将图形表示为文本，以便法学硕士能够理解它们。我们发现三个主要因素会产生影响：&lt;/p>; &lt;ul>; &lt;li>;&lt;em>;如何将图表转换为文本&lt;/em>;：我们如何将图表表示为文本会显着影响法学硕士的表现。一般来说，事件编码对于大多数任务都表现出色。&lt;/li>;&lt;li>;&lt;em>;任务类型&lt;/em>;：某些类型的图形问题对于法学硕士来说往往更难，即使从图形到图形的良好翻译也是如此。文本。 &lt;/li>;&lt;li>;&lt;em>;图结构&lt;/em>;：令人惊讶的是，我们进行推理的图的“形状”（连接密集、稀疏等）会影响法学硕士的表现。 &lt;/li>; &lt;/ul>; &lt;p>; 这项研究揭示了如何为法学硕士准备图表的关键见解。正确的编码技术可以显着提高法学硕士在图问题上的准确性（提高约 5% 到 60% 以上）。我们的新基准 GraphQA 将有助于推动该领域的进一步研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们谨向我们的合作伙伴表示感谢——作者 Jonathan Halcrow，感谢他对本书做出的宝贵贡献。我们衷心感谢 Anton Tsitsulin、Dustin Zelle、Silvio Lattanzi、Vahab Mirrokni 以及 Google Research 的整个图挖掘团队，他们富有洞察力的评论、彻底的校对和建设性的反馈极大地提高了我们的工作质量。我们还要特别感谢 Tom Small 创建了本文中使用的动画。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6090481872694489715 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like- graph-encoding-graphs-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds /8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html&quot; rel =&quot;alternate&quot; title=&quot;像图一样说话：为大型语言模型编码图&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger .com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj 6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s72-c/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot;宽度=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id >;标签：blogger.com，1999：blog-8474926331452026626.post-470840348983280912&lt;/id>;&lt;已发布>;2024-03-11T12:08:00.000-07:00&lt;/已发布>;&lt;更新>;2024-03-11T12:13 ：03.824-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Chain-of-table：在推理链中不断演化表格以实现表格理解&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：云 AI 团队学生研究员王子龙和研究科学家李振宇&lt;/span>; &lt;img src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUTIhHxVvsBjbPxvZLcN cD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s832/Chain-of-Table.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人们每天都使用表格以结构化、易于访问的格式组织和解释复杂的信息。由于此类表格无处不在，表格数据的推理长期以来一直是自然语言处理 (NLP) 的中心主题。该领域的研究人员致力于利用语言模型来帮助用户回答问题、验证陈述以及基于表格分析数据。然而，语言模型是在大量纯文本上进行训练的，因此表格数据固有的结构化性质可能很难让语言模型完全理解和利用。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 最近，&lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;>;大型语言模型&lt;/a>;（法学硕士）通过生成可靠的推理链，在各种自然语言理解（NLU）任务中取得了出色的表现，如作品所示就像&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;思想链&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2205.10625&quot;>;最少到-大多数&lt;/a>;。然而，法学硕士对表格数据进行推理的最合适方式仍然是一个悬而未决的问题。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2401.04398&quot;>;Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding&lt;/a>;”中，我们提出一个解决表格理解任务的框架，我们训练法学硕士逐步概述他们的推理，迭代更新给定的表格以反映思维过程的每个部分，类似于人们如何解决基于表格的问题。这使得LLM能够将表格转化为更简单、更易于管理的段，以便能够深入理解和分析表格的每个部分。这种方法取得了重大改进，并在 &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/1909.02164&quot;>;TabFact&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>; 基准。下图显示了所提出的表链和其他方法的高级概述。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0Y wJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s1478/image2.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1478&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75b It7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;给定一个复杂的表格，其中骑车人的国籍和姓名位于同一单元格中，（a）通用的多步骤推理无法提供正确的答案（b）程序辅助推理生成并执行程序（例如、SQL 查询）来提供答案，但无法准确解决问题。相比之下，(c) Chain-of-Table 对一系列操作进行迭代采样，有效地将复杂的表转换为专门针对问题定制的版本。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chain-of-Table&lt;/h2>; &lt;p>; 在 Chain-of-Table 中，我们指导法学硕士使用&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;情境学习&lt;/a>; 迭代生成操作并更新表以表示其基于表格数据的推理链。这使得法学硕士能够根据之前操作的结果动态规划下一个操作。该表的这种不断演变形成了一条链，它为给定问题的推理过程提供了更加结构化和清晰的表示，并使法学硕士能够做出更准确和可靠的预测。 &lt;/p>; &lt;p>; 例如，当被问到“哪位演员获得最多的全国有色人种协进会形象奖？”表链框架促使法学硕士生成反映表格推理过程的表格运算。它首先标识相关列。然后，它根据共享内容聚合行。最后，它对汇总结果进行重新排序，以生成明确回答所提出问题的最终表格。 &lt;/p>; &lt;p>; 这些操作会转换表格以与提出的问题保持一致。为了在大型表上平衡性能和计算费用，我们根据表格行的子集构建操作链。同时，逐步操作通过显示表格操作的中间结果来揭示底层推理过程，促进增强可解释性和理解性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR 08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s1999/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;983&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T -wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Chain-of-Table 中表格推理过程的图示。这个迭代过程涉及动态规划操作链并将中间结果准确地存储在转换后的表中。这些中间表格作为表格思维过程，可以指导法学硕士更可靠地找到正确答案。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Chain-of-表由三个主要阶段组成。在第一阶段，它指示法学硕士通过上下文学习动态规划下一步操作。具体来说，提示涉及三个组成部分，如下图所示：&lt;/p>; &lt;ol>; &lt;li>;问题&lt;em>;Q&lt;/em>;：“哪个国家的自行车运动员进入前三名的人数最多？” &lt;/li>;&lt;li>;操作历史&lt;em>;链&lt;/em>;：&lt;code>;f_add_col(Country)&lt;/code>;和&lt;code>;f_select_row(1, 2, 3)&lt;/code>;。 &lt;/li>;&lt;li>;最新中间表&lt;em>;T&lt;/em>;：转换后的中间表。 &lt;/li>; &lt;/ol>; &lt;p>; 通过在提示中提供三元组&lt;em>;(T, Q, chain)&lt;/em>;，LLM可以观察之前的表格推理过程，并从操作中选择下一个操作pool逐步完成推理链。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_s ltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s1958/image1.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1233&quot; data-original-width=&quot;1958&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>; thable链如何从操作池中选择下一个操作并为操作生成参数。（a）台链样品从操作池中的下一个操作。 （b）将所选操作作为输入并生成其参数。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;p>; &lt;br />; &lt;p>; &lt;br />; &lt;p>;确定，在第二阶段，我们需要生成参数。如上所述，表链在提示中考虑了三个组件，如图所示：（1）问题，（2）选定的操作及其所需的参数，以及（3）最新的中间表。 &lt;/p>; &lt;p>;例如，选择操作&lt;code>; f_group_by &lt;/code>;时，它需要一个标题名称作为参数。 &lt;/p>; &lt;p>; LLM在表中选择合适的标头。配备了所选操作和生成的参数，可执行该操作，并为以下推理构建新的中间表。 &lt;/p>; &lt;p>; Chain-of-Table 迭代前两个阶段来计划下一个操作并生成所需的参数。在此过程中，我们创建一个操作链作为表格推理步骤的代理。这些操作生成中间表，向法学硕士呈现每个步骤的结果。因此，输出表包含有关表格推理的中间阶段的全面信息。在最后阶段，我们使用此输出表来制定最终查询，并提示法学硕士以及问题以获得最终答案。 &lt;/p>; &lt;br />; &lt;h2>;实验设置&lt;/h2>; &lt;p>;我们使用&lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2-S&lt;/a>;&amp;nbsp ；以及&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>; 作为 LLM 的骨干和在三个公共表理解基准上进行实验：&lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/1909.02164 &quot;>;TabFact&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>;。 WikiTQ 和 FeTaQA 是基于表格的问答的数据集。 TabFact 是一个基于表的事实验证基准。在这篇博文中，我们将重点关注 WikiTQ 和 TabFact 上的结果。我们将 Chain-of-Table 与通用推理方法（例如，End-to-End QA、Few-Shot QA 和 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-思想&lt;/a>;）和程序辅助方法（例如，&lt;a href=&quot;https://arxiv.org/abs/2204.00498&quot;>;文本到SQL&lt;/a>;、&lt;a href=&quot;https: //arxiv.org/abs/2210.02875&quot;>;活页夹&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;日期&lt;/a>;）。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;答案更准确&lt;/h3>; &lt;p>;与通用推理方法和程序辅助推理相比方法中，Chain-of-Table 在 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 和&lt;a href=&quot;https://openai 上实现了更好的性能.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;。这归因于动态采样操作和信息丰富的中间表。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3mu AAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s1018/ChainOfTableUnderstanding.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;755&quot; data-original-width=&quot;1018&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4j QkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn 9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s16000/ChainOfTableUnderstanding.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;了解 WikiTQ 和 TabFact 上使用 PaLM 2 和 GPT 3.5 与各种模型的比较结果。&lt;/span>;&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;在较难的问题上具有更好的稳健性&lt;/h3>; &lt;p>; 在 Chain-of 中-表格，操作链越长，说明题及其对应表格的难度和复杂度越高。我们根据 Chain-of-Table 中的操作长度对测试样本进行分类。我们将 Chain-of-Table 与 Chain-of-Thought 和 Dater 进行比较，作为代表性的通用和程序辅助推理方法。我们使用 &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; 在 &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>; 上的结果来说明这一点维基百科&lt;/a>;。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrS ZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIAEsHJohvlrSna7/s1548/CoTOpChainLength.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;624&quot; data-original-width=&quot;1548&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2i UzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744 FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIAEsHJohvlrSna7/s16000/CoTOpChainLength.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;对于需要不同长度操作链的问题，思想链、日期器和 WikiTQ 上建议的表链的性能。我们提出的原子操作显着提高了通用和程序辅助推理对应物的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 值得注意的是，Chain-of-Table 始终超越了两个基线跨所有操作链长度的方法，与 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; 相比，显着提升高达 11.6%，高达 7.9%与&lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;日期&lt;/a>;相比。此外，与其他基线方法相比，表链的性能随着操作数量的增加而缓慢下降，当操作数量从 4 增加到 5 时，仅表现出最小的下降。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;更大的表格具有更好的鲁棒性&lt;/h3>; &lt;p>;我们对表格进行分类&lt;a href= &quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>; 根据代币数量分为三组：小型（&lt;2000 个代币）、中型（2000 到 4000 个代币）和大型（>;4000 个代币） 。然后我们将 Chain-of-Table 与 &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Datar&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot; 进行比较>;Binder&lt;/a>;，两个最新且最强的基线。 &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsai xakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1008&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8z srRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4 VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Binder、Dater 和建议的 Chain-of-Table 在小型（&lt;2000 个代币）、中型上的性能来自 WikiTQ 的（2000 到 4000 个令牌）和大型（>; 4000 个令牌）表。我们观察到，性能随着输入表的增大而降低，而表链则优雅地减小，与竞争方法相比取得了显着的改进。 （如上，下划线文字表示性能第二好；粗体表示性能最好。）&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Binder性能、 Dater 以及来自 WikiTQ 的小型（&lt;2000 个令牌）、中型（2000 到 4000 个令牌）和大型（>;4000 个令牌）表上提议的 Chain-of-Table。我们观察到，性能随着输入表的增大而降低，而表链则优雅地减小，与竞争方法相比取得了显着的改进。 （如上所述，下划线文本表示第二佳性能；粗体表示最佳性能。） &lt;/p>; &lt;p>; 正如预期的那样，性能随着输入表的增大而下降，因为模型需要通过较长的上下文进行推理。尽管如此，所提出的表链的性能会优雅地下降，在处理大型表时，比第二佳竞争方法显着提高了 10% 以上。这证明了推理链在处理长表格输入方面的有效性。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们提出的表链方法通过利用表格结构来表达基于表格的推理的中间步骤，从而增强了 LLM 的推理能力。它指示法学硕士根据输入表及其相关问题动态规划操作链。这种不断发展的表格设计为促进法学硕士对表格的理解提供了新的思路。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由王子龙、张浩、李春亮、朱利安·马丁·艾森施洛斯、文森特·佩罗、王子峰、莱斯利·米库里奇进行, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister.感谢 Chih-Kuan Yeh 和 Sergey Ioffe 提供的宝贵反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/470840348983280912/comments/default&quot; rel =&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables- in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /470840348983280912&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/470840348983280912&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html&quot; rel=&quot;alternate&quot; title= “Chain-of-table：在推理链中不断演化表格以实现表格理解” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv 8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNc D_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s72-c/Chain-of-Table.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376&lt;/id>;&lt;已发布>;2024-03-08T11:33:00.000-08:00&lt;/已发布>;&lt;更新>;2024-03-13T09:18:01.747-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;健康&quot;>;&lt;/category>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“图像分类”>;&lt;/类别>;&lt;title type =“text”>;皮肤病学和病理学的健康特定嵌入工具&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google Health 临床研究科学家 Dave Steiner 和 Google Research 产品经理 Rory Pilgrim&lt;/span>; &lt;img src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58 CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiA eG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s16000/Path%20+%20Derm%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 全球范围内缺乏跨专业的医学影像专家解读，包括&lt;a href=&quot;https://www.rsna.org/news/2022/may/Global-Radiograph-Shortage&quot;>;放射学&lt;/ a>;、&lt;a href=&quot;https://www.aad.org/dw/monthly/2021/december/feature-running-dry&quot;>;皮肤科&lt;/a>;和&lt;a href=&quot;https://proscia. com/infographic-the-state-of-the-pathology-workforce-2022/&quot;>;病理学&lt;/a>;。机器学习 (ML) 技术可以通过支持工​​具帮助减轻这一负担，使医生能够更准确、更高效地解读这些图像。然而，此类机器学习工具的开发和实施通常受到高质量数据、机器学习专业知识和计算资源的可用性的限制。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;催化ML用于医学成像的一种方法是通过使用深度学习（DL）来捕获医学图像中信息的特定领域模型作为压缩的数值向量（称为嵌入）。这些嵌入代表了对图像中重要特征的一种预先学习的理解。与&lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_Dimensionity&quot;>;处理高维数据&lt;相比，识别嵌入中的模式可以减少训练高性能模型所需的数据量、专业知识和计算量/a>;，如图像，直接。事实上，这些嵌入可用于执行专业领域内的各种下游任务（请参见下面的动画图）。这种利用预先学习的理解来解决相关任务的框架类似于经验丰富的吉他手通过耳朵快速学习新歌曲的框架。因为吉他手已经建立了技能和理解的基础，所以他们可以快速掌握新歌曲的模式和节奏。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xB x3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPSH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s1600/路径%20+% 20Derm%20train%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQ f9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THph EkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s16000/Path%20+%20Derm%20train%20LP.gif&quot;/>;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation 用于将（图像，标签）对的小数据集转换为（嵌入，标签）对。然后，这些对可用于使用线性探针（即轻量级线性分类器）来训练特定于任务的分类器（如图所示），或使用嵌入作为输入的其他类型的模型。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-右：自动;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag- ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUpplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s1600/Path%20+%20Derm%20-%20e valuate%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= “500”数据原始宽度=“1600”=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0e Cl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqbHdlxQyyz/s16000/Path%20+%20Derm%20-%20评估%20LP.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;一旦线性探针被训练，它可用于从新图像上对嵌入的预测。这些预测可以与地面真实信息进行比较，以评估线性探针的性能。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;为了使这种类型的嵌入模型可用并进一步推动随着医学成像领域 ML 工具的开发，我们很高兴发布两个用于研究用途的特定领域工具：&lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation &quot;>;Derm Foundation&lt;/a>; 和 &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/path-foundation&quot;>;Path Foundation&lt;/a>;。在此之前，我们已经使用 &lt;a href=&quot;https://blog.research.google/2022/07/simplified-transfer-learning-for-chest.html&quot;>;CXR 基金会&lt; /a>; 用于胸部 X 光片的嵌入工具，代表了我们跨多种医疗专业模式不断扩展的研究产品的一部分。这些嵌入工具将图像作为输入并生成分别专门用于皮肤病学和数字病理学图像领域的数值向量（嵌入）。通过使用各自的嵌入工具运行胸部 X 射线、皮肤病学或病理学图像的数据集，研究人员可以获得自己图像的嵌入，并使用这些嵌入为其应用快速开发新模型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;路径基础&lt;/h2>; &lt;p>; 在“&lt;a href=&quot;https://arxiv”中.org/abs/2310.13259&quot;>;组织病理学自监督模型的特定领域优化和多样化评估&lt;/a>;”，我们表明病理图像的自监督学习（SSL）模型优于传统的预训练方法，并且能够下游任务分类器的有效训练。这项工作的重点是&lt;a href=&quot;https://en.wikipedia.org/wiki/H%26E_stain&quot;>;苏木精和伊红&lt;/a>; (H&amp;E) 染色载玻片，它是诊断病理学中的主要组织染色剂，能够病理学家在显微镜下观察细胞特征。使用 SSL 模型的输出训练的线性分类器的性能与之前在更多数量级的标记数据上训练的深度学习模型的性能相匹配。 &lt;/p>; &lt;p>; 由于数字病理图像和“自然图像”照片之间存在显着差异，这项工作涉及模型训练过程中的多项病理特定优化。一个关键要素是病理学中的&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/&quot;>;全幻灯片图像&lt;/a>; (WSI) 的跨度可以是 100,000 像素（比典型智能手机照片大数千倍）并由专家在多个放大倍数（缩放级别）下进行分析。因此，WSI 通常会被分解为更小的块或补丁，以供计算机视觉和深度学习应用程序使用。生成的图像信息密集，细胞或组织结构分布在整个帧中，而不是具有不同的语义对象或前景与背景变化，从而为稳健的 SSL 和特征提取带来了独特的挑战。此外，物理（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Microtome&quot;>;切割&lt;/a>;）和化学（例如，&lt;a href=&quot;https://en.wikipedia. org/wiki/Fixation_(histology)&quot;>;固定&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Staining&quot;>;染色&lt;/a>;）用于制备样品的过程会影响图像外观显着。 &lt;/p>; &lt;p>; 考虑到这些重要方面，特定于病理学的 SSL 优化包括帮助模型学习&lt;a href=&quot;https://arxiv.org/abs/2206.12694&quot;>;污点不可知特征&lt;/a>; ，将模型推广到多个放大倍数的补丁，&lt;a href=&quot;https://blog.research.google/2020/02/generating-diverse-synthetic-medical.html&quot;>;增强&lt;/a>;数据以模拟扫描和图像后处理以及自定义数据平衡，以改善 SSL 训练的输入异构性。使用涉及 17 种不同组织类型和 12 种不同任务的广泛基准任务对这些方法进行了广泛评估。 &lt;/p>; &lt;p>; 利用视觉转换器 (&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;>;ViT-S/16&lt;/a>;) 架构，Path Foundation 被选为上述优化和评估过程中表现最佳的模型（如下图所示）。因此，该模型在性能和模型大小之间提供了重要的平衡，以便在大型病理 WSI 的许多单独图像块上生成嵌入时能够进行有价值且可扩展的使用。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJ b79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s1999/Path%20+%20Derm% 20SSL.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1097&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joB GIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxW gYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s16000/Path%20+%20Derm%20SSL.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;针对 Path Foundation 进行特定于病理学优化的 SSL 训练。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;特定领域图像表示的值也可以在下图中看到，它显示了 Path Foundation 的线性探测性能改进（通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;>; 测量） AUROC&lt;/a>;）与传统的自然图像预训练（&lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;ImageNet-21k&lt;/a>;）相比。这包括对诸如&lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2665774&quot;>;淋巴结转移性乳腺癌检测&lt;/a>;、&lt;a href=&quot;https:// jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;前列腺癌分级&lt;/a>;和&lt;a href=&quot;https://www.nature.com/articles/s41523-022-00478-y&quot;>;乳腺癌评分&lt;/a>;等。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9 D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s1999/路径％20+％20derm％20 Embeddings.png“样式=”保证金左：自动右：自动; 1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9 ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s16000/Path%20+%20Derm%20embeddings.png&quot; />;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据组织病理学中多个评估任务的线性探测评估，Path Foundation 嵌入显着优于传统 ImageNet 嵌入.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;真皮粉底&lt;/ h2>; &lt;p>; &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; 是一种嵌入工具，源自我们的研究应用深度学习&lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;解读皮肤病图像&lt;/a>;，其中包括我们最近的工作：添加&lt;a href=&quot;https://arxiv.org/abs/2402.15566&quot;>;改进以更好地泛化到新数据集&lt;/a>;。由于其针对皮肤病学的预训练，它对皮肤状况图像中存在的特征有潜在的了解，可用于快速开发模型来对皮肤状况进行分类。 API 底层的模型是经过两个阶段训练的 &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>;BiT ResNet-101x3&lt;/a>;。第一个预训练阶段使用对比学习，类似于 &lt;a href=&quot;https://arxiv.org/abs/2010.00747&quot;>;ConVIRT&lt;/a>;，在大量图像-文本对上进行训练&lt;a href =&quot;https://blog.research.google/2017/07/revisiting-unreasonable-effectness.html&quot;>;来自互联网&lt;/a>;。在第二阶段，然后使用临床数据集（例如来自远程皮肤病学服务的数据集）对该预训练模型的图像组件进行微调，以进行病情分类。 &lt;/p>; &lt;p>; 与组织病理学图像不同，皮肤病学图像更类似于用于训练当今许多计算机视觉模型的现实世界图像。然而，对于专门的皮肤病学任务，创建高质量模型可能仍然需要大型数据集。借助 Derm Foundation，研究人员可以使用自己的较小数据集来检索特定领域的嵌入，并使用这些数据来构建较小的模型（例如线性分类器或其他小型非线性模型），使他们能够验证他们的研究或产品想法。为了评估这种方法，我们使用远程皮肤病学数据在下游任务上训练模型。模型训练涉及不同的数据集大小（12.5%、25%、50%、100%），以将基于嵌入的线性分类器与微调进行比较。 &lt;/p>; &lt;p>; 考虑的建模变体是： &lt;/p>; &lt;ul>; &lt;li>;来自 &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>; 的冻结嵌入的线性分类器BiT-M&lt;/a>;（标准预训练图像模型）&lt;/li>;&lt;li>;BiT-M 的微调版本，具有用于下游任务的额外密集层&lt;/li>;&lt;li>;线性分类器来自 Derm Foundation API 的冻结嵌入 &lt;/li>;&lt;li>;Derm Foundation API 底层模型的微调版本，带有用于下游任务的额外层 &lt;/li>; &lt;/ul>; &lt;p>; 我们发现模型建立在 Derm Foundation 嵌入之上的用于皮肤病学相关任务的质量比仅基于嵌入构建或通过 BiT-M 微调的质量要高得多。人们发现，对于较小的训练数据集大小，这一优势最为明显。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7e IqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s1240/Path%20+%20Derm%20task% 20accuracy.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;842&quot; data-original-width=&quot;1240&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB 4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn 7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s16000/Path%20+%20Derm%20task%20accuracy.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;这些结果表明 Derm Foundation 也可以作为加速皮肤相关建模任务的有用起点。我们的目标是让其他研究人员能够以模型学到的皮肤病学的基本特征和表征为基础。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;但是，这种分析存在局限性。我们仍在探索这些嵌入在任务类型、患者群体和图像设置中的推广效果如何。使用 Derm Foundation 构建的下游模型仍然需要仔细评估，以了解其在预期环境中的预期性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Access Path 和 Derm Foundation&lt;/h2>; &lt;p>; 我们设想 Derm Foundation 和 Path Foundation嵌入工具将支持一系列用例，包括诊断任务模型的高效开发、质量保证和分析前工作流程改进、图像索引和管理以及生物标志物发现和验证。我们正在向研究界发布这两种工具，以便他们能够探索嵌入对于自己的皮肤病学和病理学数据的实用性。 &lt;/p>; &lt;p>; 要获得访问权限，请使用以下 Google 表单签署每个工具的服务条款。 &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSe5icNBzU_lO2CwjLLIOwbqIcWnJC-m4Sl7MgvI9Lng3QT6Zg/viewform?resourcekey=0-dahJtiVe2CqYkNEdWPcXgw&quot;>;Derm 基金会访问表&lt;/ a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/1auyo2VkzlzuiAXavZy1AWUyQHAqO7T3BLK-7ofKUvug/edit?resourcekey=0-Z9pRxjDI-kaDEUIiNfMAWQ#question=1168037695&amp;field=173852432&quot;>;路径基础访问表单&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>; 获得每个工具的访问权限后，您可以使用 API 从存储在 Google Cloud 中的皮肤病学图像或数字病理学图像中检索嵌入。批准的用户只是好奇地看到该模型和嵌入在行动中的嵌入可以使用提供的示例COLAB笔记本电脑使用公共数据进行培训模型进行分类&lt;a href =“ https://github.com/google-health/image-image-image-image-image-research/- blob/master/derm-foundation/derm_foundation_demo.ipynb&quot;>;六种常见皮肤病&lt;/a>;或识别 &lt;a href=&quot;https://github.com/Google-Health/imaging-research/blob/master/ 中的肿瘤path-foundation/linear-classifier-demo.ipynb&quot;>;组织病理学补丁&lt;/a>;。我们期待看到这些工具可以解锁的用例范围。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢许多提供帮助的合作者使这项工作成为可能的成员包括 Yun Liu、Can Kirmizi、Fereshteh Mahvar、Bram Sterling、Arman Tajback、Kenneth Philbrik、Arnav Agharwal、Aurora Cheung、Andrew Sellergren、Boris Babenko、Basil Mustafa、Jan Freyberg、Terry Spitz、Yuan Liu、Pinal Bavishi，阿尤什·杰恩、阿米特·塔尔雷贾、拉吉夫·里克耶、艾比·沃德、杰里米·赖、法鲁克·艾哈迈德、苏普里亚·维贾伊、蒂亚姆·贾罗恩斯里、杰西卡·卢、Saurabh Vyawahare、萨洛尼·阿加瓦尔、埃勒里·乌尔钦、乔纳森·克劳斯、法亚兹·贾米尔、汤姆·斯莫尔、安妮莎·乌姆拉尼、 Lauren Winer、Sami Lachgar、Yossi Matias、Greg Corrado 和 Dale Webster。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1106624361649572376/comments/default &quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools- for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /1106624361649572376&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1106624361649572376&quot; rel=&quot;self&quot; 类型=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools-for.html&quot; rel=&quot;alternate&quot; title=&quot;健康-皮肤病学和病理学专用嵌入工具&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9 Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s72-c/Path%20+%20Derm%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/ mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-1765359719068432739&lt;/id >;&lt;发布>;2024-03-07T10:15:00.000-08:00&lt;/发布>;&lt;更新>;2024-03-07T10:19:31.177-08:00&lt;/更新>;&lt;category schema=&quot;http:// www.blogger.com/atom/ns#&quot; term=&quot;大型语言模型&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器智能&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;社交学习：协作学习大语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究实习生 Amirkeivan Mohtashami 和软件工程师 Florian Hartmann&lt;/span>; &lt;img src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp 8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 大型语言模型 (LLM) 显着提高了解决使用自然语言指定的任务的技术水平，通常达到接近人类的性能。随着这些模型越来越多地支持辅助代理，他们有效地相互学习可能是有益的，就像人们在社交环境中所做的那样，这将使基于 LLM 的代理能够提高彼此的表现。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 讨论人类、班杜拉和沃尔特斯的学习过程&lt;a href=&quot;https://books.google.ch/books/about/Social_Learning_Theory .html?id=IXvuAAAAMAAJ&amp;amp;redir_esc=y&quot;>;在 1977 年描述了&lt;/a>;&lt;em>;社会学习&lt;/em>;的概念，概述了人们使用的观察学习的不同模型。向他人学习的一种常见方法是通过描述如何参与特定行为的&lt;em>;口头指导&lt;/em>;（例如，来自老师）。或者，学习可以通过模仿行为的活生生例子的&lt;em>;实时模型&lt;/em>;来进行。 &lt;/p>; &lt;p>; 鉴于法学硕士模仿人类交流的成功，在我们的论文“&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;社交学习：利用大型语言模型进行协作学习&lt;/”中a>;”，我们调查法学硕士是否能够利用社交学习相互学习。为此，我们概述了一个社会学习框架，其中法学硕士使用自然语言以隐私意识方式相互分享知识。我们评估了我们的框架在各种数据集上的有效性，并提出了在这种情况下衡量隐私的定量方法。与之前的协作学习方法相比，例如常见的&lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;联合学习&lt;/a>;方法通常依赖于关于梯度，在我们的框架中，智能体纯粹使用自然语言互相教学。 &lt;/p>; &lt;br />; &lt;h2>;法学硕士的社交学习&lt;/h2>; &lt;p>;为了将社交学习扩展到语言模型，我们考虑这样的场景：法学硕士学生应该学习解决来自多个教师实体的任务，这些教师实体已经知道那个任务。在我们的论文中，我们评估了学生在各种任务上的表现，例如短短信中的&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2034691.2034742&quot;>;垃圾邮件检测&lt;/a>;（ SMS），解决&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;小学数学问题&lt;/a>;，以及&lt;a href=&quot;https://arxiv.org/abs/1905.10044&quot;>;根据给定的文本回答问题&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1 jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s900/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;381&quot; data-original-width=&quot;900&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5t DBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;社会学习过程的可视化：教师模型向学生模型提供说明或少量示例，而无需共享其私有数据。&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 语言模型在仅给出少数示例的情况下就显示出执行任务的非凡能力 - 这个过程称为 &lt;a href=&quot;https://arxiv.org/abs/2005.14165 ”少样本学习&lt;/a>;。考虑到这一点，我们提供了人工标记的任务示例，使教师模型能够将其教授给学生。当由于隐私问题等原因而无法直接与学生共享这些示例时，就会出现社交学习的主要用例之一。 &lt;/p>; &lt;p>; 为了说明这一点，让我们看一个垃圾邮件检测任务的假设示例。教师模型位于设备上，一些用户自愿将他们收到的传入消息标记为“垃圾邮件”或“非垃圾邮件”。这是有用的数据，可以帮助训练学生模型区分垃圾邮件和非垃圾邮件，但与其他用户共享个人消息是侵犯隐私的行为，应该避免。为了防止这种情况，社交学习过程可以将知识从教师模型转移到学生，这样学生就可以了解垃圾邮件的样子，而无需共享用户的个人短信。 &lt;/p>; &lt;p>; 我们通过与我们上面讨论的已建立的人类社会学习理论进行类比来研究这种社会学习方法的有效性。在这些实验中，我们使用 &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-S&lt;/a>; 模型老师和学生。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y 3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1117&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8yt HsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;社会学习的系统观点：在培训时，多名教师教授学生。在推理时，学生正在使用从老师那里学到的知识。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;综合示例&lt;/h3>; &lt;p>;作为对应物针对传统社会学习描述的实时教学模型，我们提出了一种学习方法，教师为任务生成新的综合示例并与学生分享。这样做的动机是，人们可以创建一个与原始示例完全不同但同样具有教育意义的新示例。事实上，我们观察到我们生成的示例与真实示例有很大不同，可以保护隐私，同时仍然能够实现与使用原始示例实现的性能相当的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkP eKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s1456/image5.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPF hsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;对于多项任务，生成的 8 个示例的性能与原始数据一样好（请参阅我们的&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们通过任务套件上的综合示例来评估学习的效果。特别是当示例数量足够多时，例如，n = 16，我们观察到对于大多数任务来说，共享原始数据和通过社交学习使用合成数据进行教学之间没有统计上的显着差异，这表明隐私改进不一定会出现以模型质量为代价。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHR8sU3WkHsPKVlsQm VnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s1456/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHR8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEiror XE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;生成 16 个示例（而不是仅 8 个示例）进一步缩小了相对于原始示例的性能差距。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;p>; 一个例外是垃圾邮件检测，使用合成数据进行教学的准确性较低。这可能是因为当前模型的训练过程使它们偏向于仅生成非垃圾邮件示例。在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;中，我们还研究了用于选择要使用的良好示例子集的聚合方法。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;综合指令&lt;/h3>; &lt;p>;鉴于语言模型在以下指令中的成功，言语通过让教师为任务生成指令，教学模型也可以自然地适应语言模型。我们的实验表明，提供这样的生成指令可以有效地提高零样本提示的性能，达到与原始示例的少样本提示相当的准确性。然而，我们确实发现教师模型可能无法在某些任务上提供良好的指导，例如由于输出的复杂格式要求。 &lt;/p>; &lt;p>;对于&lt;a href=&quot;https://arxiv.org/abs/1606.06031&quot;>;兰巴达&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>; GSM8k&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;随机插入&lt;/a>;，提供合成示例比提供生成的指令表现更好，而在其他任务中生成的指令获得更高的准确度。这一观察结果表明，教学模式的选择取决于手头的任务，就像最有效的教学方法因任务而异一样。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJe jJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s1451/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1451&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEH y8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;根据任务的不同，生成指令比生成新示例效果更好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;记忆私人例子&lt;/h2>; &lt;p>;我们希望社会学习中的教师在不透露原始数据细节的情况下教授学生。为了量化这个过程泄露信息的可能性，我们使用了&lt;a href=&quot;https://research.google/pubs/the-secret-sharer-evaluating-and-testing-unintending-memorization-in-neural-networks/ &quot;>;Secret Sharer&lt;/a>;，一种流行的方法，用于量化模型记忆其训练数据的程度，并将其适应社交学习环境。我们选择此方法是因为它之前&lt;a href=&quot;https://blog.research.google/2023/03/distributed- Differential-privacy-for.html&quot;>;用于&lt;/a>;评估联邦学习中的记忆情况。 &lt;/p>; &lt;p>; 为了将秘密分享者方法应用于社交学习，我们设计了“金丝雀”数据点，以便我们可以具体测量训练过程记住了多少它们。这些数据点包含在教师用来生成新示例的数据集中。社会学习过程完成后，我们可以衡量学生对老师使用的秘密数据点的信心，与那些甚至没有与老师分享的类似数据点相比。 &lt;/p>; &lt;p>; 在我们的分析中（在&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;论文&lt;/a>;中详细讨论），我们使用包含名称和代码的金丝雀示例。我们的结果表明，学生对老师使用的金丝雀的信心仅稍高一些。相反，当直接与学生共享原始数据点时，对包含的金丝雀的置信度远高于对保留集的置信度。这支持了这样的结论：教师确实使用其数据进行教学，而不是简单地复制它。 &lt;/p>; &lt;br />; &lt;h2>;结论和后续步骤&lt;/h2>; &lt;p>;我们引入了一个社会学习框架，该框架允许能够访问私有数据的语言模型通过文本通信传输知识，同时维护该数据的隐私。数据。在此框架中，我们将共享示例和共享指令确定为基本模型，并在多个任务上对其进行评估。此外，我们将秘密共享者指标调整到我们的框架中，提出了衡量数据泄漏的指标。 &lt;/p>; &lt;p>;作为下一步，我们正在寻找改善教学过程的方法，例如，添加反馈循环和迭代。此外，我们希望研究将社交学习用于文本以外的方式。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们谨向共同作者 Matt Sharifi、Sian Gooding、Lukas Zilka 和 Blaise Aguera y Arcas 表示感谢。在纸上。此外，我们还要感谢 Victor Cărbune、Zachary Garrett、Tautvydas Misiunas、Sofia Neata 和 John Platt 的反馈，这些反馈极大地改进了本文。我们还要感谢 Tom Small 创作了这个动画人物。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1765359719068432739/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html #comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;self&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html&quot; rel=&quot;alternate&quot; title=&quot;社交学习：协作学习大语言模型” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@ blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEicN2GYOP9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyy pTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-8393293208018757284&lt;/id>;&lt;发布>;2024-03-06T10：26：00.000-08： 00&lt;/published>;&lt;updated>;2024-03-06T14:44:03.387-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot; >;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“数据集”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/” atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Croissant：用于 ML 就绪数据集的元数据格式&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot; byline-author&quot;>;发布者：Google 研究部软件工程师 Omar Benjelloun 和 Google Core ML 软件工程师兼 MLCommons 协会主席 Peter Mattson&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ IMG/B/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyA AN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s1600/CroissantHero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 机器学习 (ML) 从业者希望重用现有数据集来训练 ML 模型，通常会花费大量时间来理解数据、理解其组织或弄清楚要使用哪个子集作为特征。事实上，机器学习领域的进展长期以来一直受到一个根本障碍的阻碍：数据表示的多样性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML 数据集涵盖广泛的内容类型，从文本和结构化数据到图像、音频和视频。即使在涵盖相同类型内容的数据集中，每个数据集也具有独特的文件和数据格式&lt;em>;临时&lt;/em>;排列。这一挑战降低了整个机器学习开发过程（从查找数据到训练模型）的生产力。它还阻碍了急需的数据集工具的开发。 &lt;/p>; &lt;p>; 数据集有通用元数据格式，例如 &lt;a href=&quot;http://schema.org/Dataset&quot;>;schema.org&lt;/a>; 和 &lt;a href=&quot;https:// www.w3.org/TR/vocab-dcat-3/&quot;>;DCAT&lt;/a>;。然而，这些格式是为数据发现而设计的，而不是为了满足机器学习数据的特定需求，例如从结构化和非结构化源中提取和组合数据的能力，以包含能够实现的元数据.google/responsibility/responsible-ai-practices/&quot;>;负责任地使用数据&lt;/a>;，或描述 ML 使用特征，例如定义训练、测试和验证集。 &lt;/p>; &lt;p>; 今天，我们推出 &lt;a href=&quot;https://mlcommons.org/croissant&quot;>;Croissant&lt;/a>;，这是一种适用于 ML 就绪数据集的新元数据格式。羊角面包是由工业和学术界的社区合作开发的，这是&lt;a href=&quot;https://mlcommons.org/&quot;>; mlCommons &lt;/a>;努力的一部分。 Croissant 格式不会改变实际数据的表示方式（例如图像或文本文件格式）——它提供了描述和组织数据的标准方法。 Croissant 建立在 &lt;a href=&quot;https://schema.org/&quot;>;schema.org&lt;/a>; 之上，这是在网络上发布结构化数据的事实标准，已被超过 4000 万个数据集使用。 Croissant 通过 ML 相关元数据、数据资源、数据组织和默认 ML 语义的综合层对其进行了增强。 &lt;/p>; &lt;p>; 此外，我们还宣布主要工具和存储库的支持：今天，三个广泛使用的 ML 数据集集合 - &lt;a href=&quot;http://www.kaggle.com/datasets&quot;>;Kaggle&lt; /a>;、&lt;a href=&quot;https://huggingface.co/datasets?other=croissant&amp;amp;sort=trending&quot;>;拥抱脸&lt;/a>; 和 &lt;a href=&quot;https://openml.org/search ?type=data&quot;>;OpenML&lt;/a>; — 将开始支持其托管数据集的 Croissant 格式； &lt;a href=&quot;http://g.co/datasetsearch&quot;>;数据集搜索&lt;/a>;工具可让用户在网络上搜索 Croissant 数据集；以及流行的机器学习框架，包括 &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;、&lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;、和 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;，可以使用 &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; 轻松加载 Croissant 数据集TensorFlow 数据集&lt;/a>; (TFDS) 包。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;羊角面包&lt;/h2>; &lt;p>;此1.0羊角面包的释放包括一个完整的&lt;a href =“ https” https ：//mlcommons.org/croissant/1.0“>;规范&lt;/a>;格式，一组&lt;a href=&quot;https://github.com/github.com/mlcommons/croissant/croissant/croissant/croissant/tree/main/main/main/main/datasets&quot;>;示例数据集&lt;/a>;，开源&lt;a href=&quot;https://github.com/mlcommons/croissant/croissant/tree/main/main/main/python/mlcroissant&quot;>; Python Library &lt;/a>;开源&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>; Visual Editor &lt;/a>;以直观的方式加载，检查和创建羊角面包数据集描述。 &lt;/p>; &lt;p>;支持负责任的AI（RAI）是一开始就羊角面包的关键目标。我们还发布了&lt;a href=&quot;https://mlcommons.org/croissant/rai/1.0&quot;>;杂交式rai votabulary &lt;/a>;延伸的&lt;a href=&quot;https://mlcommons.org/croissant/rai/1.0&quot;>;延伸诸如数据生命周期管理，数据标签，参与式数据，ML安全性和公平评估，解释性和合规性等案例。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;为什么ML数据共享格式？&lt;/h2>; &lt;p>;大多数ML工作实际上是数据工作。培训数据是决定模型行为的“代码”。数据集可以从用于训练大型语言模型（LLM）的文本集合到用于训练汽车避免碰撞系统的驾驶场景（注释视频）的集合。但是，开发ML模型的步骤通常遵循相同的以数据为中心的过程：（1）查找或收集数据，（2）清洁和完善数据，（3）在数据上训练模型，（4）测试有关更多数据的模型，（5）发现该模型不起作用，（6）分析数据以找出原因，（7）重复直到实现可行的模型。由于缺乏通用格式，许多步骤都更加困难。对于资源有限的研究和早期企业家努力，这种“数据开发负担”尤其沉重。 &lt;/p>; &lt;p>;像羊角面包这样的格式的目标是使整个过程更容易。例如，可以通过搜索引擎和数据集存储库来利用元数据，以使找到正确的数据集变得更加容易。数据资源和组织信息使开发清洁，完善和分析数据的工具变得更加容易。这些信息和默认的ML语义使ML框架可以使用数据使用最少的代码来训练和测试模型。这些改进共同大大减轻了数据开发负担。 &lt;/p>; &lt;p>;此外，数据集作者还关心其数据集的可发现性和易用性。采用羊角面包可以提高其数据集的价值，同时只需最少的努力，这要归功于ML数据平台的可用创建工具和支持。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;今天羊角面包可以做什么？&lt;/h2>; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“边距 - 左：自动; margin-right：auto;”>; &lt;tbody>; &lt;try>; &lt;tr>; &lt;tr>; &lt;td style =“ text-align：center; center; center;>;>; &lt;a href =” tgbllyh-1dnh8duz7-tusDuig5v2piqJMQ5DW9MiseEsBVnMSie8JrrxoehxfctGQI0AhieoyFuhyWdfSyrmbtSyrmbt8Bhum/s908/s908/image1.png image1.png“ ImageAnChor =” ImageAnChor =“ ImageAnChor =” ImageAnChor =“ 1” ：auto; auto-right：auto;“>; &lt;img border =“ 0” data-Forginal-height =“ 540” data-eriginal-width =“ 908” src =“ /b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style =“ Text-Align：Center;”>;羊角面包生态系统：用户可以搜索羊角面包数据集，从主要存储库中下载它们，然后轻松地将其加载到他们喜欢的ML Frameworks中。他们可以使用羊角面包编辑器。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;今天，用户可以在：&lt;/p>; &lt;ul>; &lt;&lt;/p>; &lt;/pdbody>; &lt;p>;上创建，检查和修改羊角面包元数据。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>; li>; Google &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;>;数据集搜索&lt;/a>;，提供了一个羊角面过滤器。 &lt;/li>; &lt;li>; &lt;a href=&quot;https://huggingface.co/datasets？other = croissant＆amp; sort = trending&quot;>; huggingface &lt;/a>; //kaggle.com/datasets&quot;>; kaggle &lt;/a>; &lt;/li>; &lt;li>; &lt;a href=&quot;https://openml.org/search?type=data&quot;>; opentml &lt;/a>; &lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>;使用羊角面包数据集，可以通过&lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>; tensorflow轻松获取数据。数据集&lt;/a>;用于在流行的ML框架中使用，例如&lt;a href=&quot;https://www.tensorflow.org/&quot;>; tensorflow &lt;/a>;，&lt;a href=&quot;https://pytorch.org/&quot;>; pytorch &lt;/a>;和&lt;a href=&quot;https://github.com/google/jax&quot;>; jax &lt;/a>;。 &lt;/li>; &lt;li>;使用&lt;a href=&quot;https://huggingface.co/spaces/mlcommons/croissant-editor&quot;>; Croissant Editor ui &lt;/a>;（&lt;a href =“ https） ：//github.com/mlcommons/croissant/tree/main/editor“>; github &lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;p>;要发布羊角面包数据集，用户可以：&lt;/p>; &lt;ul>; &lt;li>;使用&lt;a href =“ https://huggingface.co/spaces/mlcommons/croissant -Editor“>;牛角编辑UI &lt;/a>;（&lt;a href=&quot;https://github.com/mlcommons/croissant/croissant/croissant/tree/main/editor&quot;>; github &lt;/a>;）生成大部分杂色元数据通过分析用户提供的数据并填充重要的元数据字段（例如RAI属性）来自动自动。 &lt;/li>; &lt;li>;作为其数据集网页的一部分发布羊角面信息，以使其可发现和重复使用。 &lt;/li>; &lt;li>;在支持羊角面包的一个存储库中发布其数据，例如Kaggle，Huggingface和OpenML，并自动生成羊角面包元数据。 &lt;/li>; &lt;/ul>; &lt;div style =“线路高：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;未来方向&lt;/h2>; &lt;p>;我们对羊角面包的帮助ML感到兴奋从业者，但是使这种格式真正有用需要社区的支持。我们鼓励数据集创建者考虑提供羊角面包元数据。我们鼓励托管数据集的平台提供羊角面包文件，以便在数据集网页中下载并嵌入羊角面包元数据，以便可以通过数据集搜索引擎发现它们。帮助用户使用ML数据集的工具，例如标签或数据分析工具，也应考虑支持羊角面包数据集。我们可以一起减轻数据开发负担，并实现ML研发的更丰富的生态系统。 &lt;/p>; &lt;p>;我们鼓励社区参加&lt;a href=&quot;http://mlcommons.org/croissant&quot;>;加入我们&lt;/a>;为这项工作做出了贡献。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>; benspectledgments &lt;/h2>; &lt;p>; &lt;p>; &lt;em>; Croissant由&lt;a a href =“ https开发：//datasetsearch.research.google.com/“>; dataset search &lt;/a>;，&lt;a href=&quot;https://www.kaggle.com/&quot;>; kaggle &lt;/a>;和&lt;a href =“ https：https：https： //www.tensorflow.org/datasets&quot;>; tensorflow数据集&lt;/a>; Google的团队，作为&lt;a href=&quot;http://mlcommons.org&quot;>; mlcommons &lt;/a>;社区工作组的一部分。包括来自这些组织的贡献者：拜耳，Ctuning Foundation，Dans-knaw，Dotphoton，Harvard，Husgaft，Hugging Face，Kings College London，List，Meta，NASA，NASA，NASA，NASA，NASA，北卡罗来纳州立大学，开放数据研究所，加泰罗尼亚大学开放大学，Sage Bionetworks和Sage Bionetworks和Sage Bionetworks和tu eindhoven。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/8393293208018757284/comments/comments/comments/comments/default/default/default” =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/03/croissant-metadata-format-format-for-ml-ml-ml-ready.html#comment-comment-form” rel =“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/839329329329329320801875757284应用程序/atom+xml“/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626/posts/posts/posts/default/8393293293293208018757284 link href =“ http://blog.research.google/2024/03/croissant-metadata-format-format-for-ml-ml-ready.html” rel =“替代” title =“ croissant：obsoissant：ml-Ready DataSets的元数据格式“ type =” text/html”/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777775266161 &lt;/uri>; &lt;/email>; &lt;gd：Image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” 。 -T1O5PSACF-EKVHIEIWHMR7RGFT7O3A2RK94GWE8WBOO3DULXRQT1XZ9X4I2AMKJXCUTUKBBSIA8XVYAANAN_LJJJJJJJJJYMABXRXRYON_ uau2p4srnp/s72-c/croissanthero.png“ width =“ 72” xmlns：媒体=“ http：//search.yahoo.com/mrss/ THR：THR>; 0 &lt;/thr：Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：Blogger.com，1999：Blog-8474926331452026626.POST-POST-2754526782497247497 &lt;/id>; ：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-03-05T08：40：45.490-08：00 &lt;/updated>; &lt;category scheme =&#39; term =“会议”>; &lt;/category>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” term =“ conferences”>; &lt;/caterory>; &lt;category>; &lt;category scheme =“ http：// http：// wwwww .blogger.com/atom/ns＃“ term =“ physics”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” >; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term =“量子计算”>; &lt;/category>; &lt;/cattory>; &lt;title type =“ text”>; google at aps 2024 &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由凯特·韦伯（Kate Weber）和香农·莱昂（Shannon Leon）发表，Google Research，Quantum AI Team &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img /b/r29vz2xl/avvxsejy2hfq3rn4qrujcsmupiau4ueiicq219mdvfu4fnj9kf5pbmui0x4kf5pbmui us ds5fywxigjex4nxmpoib2je1z2qxdlnzlkfm075wstfjd777777 xvns2t9hckwzylf/s1600/s1600/lockup_googperleresearch_fullcolor_fullcolor_hero.jper.jpg&#39; />; &lt;p>;今天&lt;a href=&quot;https://www.aps.org/meetings/meeting/meeting.cfm?name=mar24&quot;>; 2024 3月会议&lt;/a>; /www.aps.org/&quot;>;美国物理社会&lt;/a>;（APS）在明尼苏达州明尼阿波利斯（Minneapolis）开幕。 APS 2024跨物理和相关领域的主题大会汇集了研究人员，学生和行业专业人员分享他们的发现并建立合作伙伴关系，以实现与物理相关科学和技术的基本进步。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;今年，Google在APS上拥有强大的影响力，由Google &lt;a href =“ https://quantumai.google/”托管的展位。 >; Quantum ai &lt;/a>;团队，在整个会议期间进行了50多次谈判，并参加会议组织活动，特殊会议和活动。亲自参加APS 2024？快来访问Google的量子AI展位，以了解有关我们为解决该领域最有趣的挑战所做的激动人心的工作的更多信息。 &lt;！ - 访问&lt;a href =“ https://twitter.com/googleai”>; @googleai &lt;/a>; x（twitter）帐户以了解有关Google Booth Activity（例如，demos and q＆amp; a sessions） 。-->; &lt;/p>; &lt;p>;您可以了解有关我们在会议上介绍的最新尖端工作以及下面的展位活动的更多信息（&lt;strong>; Bold &lt;/strong>;中列出的Googles）。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; >;会议椅包括：&lt;strong>; Aaron Szasz &lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;展位活动&lt; /h2>; &lt;div style =“边距左：20px;”>; &lt;p>; &lt;em>;此时间表可能会更改。请访问Google Quantum AI展位以获取更多信息。&lt;/em>; &lt;/p>; &lt;p>;崩溃：一种可视化QEC电路的原型交互式工具&lt;br />;主持人：&lt;strong>; Matt McEwen &lt;/strong>; &lt;br/ >;星期二，3月5日| CST上午11:00 &lt; /p>; &lt;p>; Qualtran：一个开源库，用于有效的资源估计容宽算法&lt;br />;主持人：&lt;strong>; tanuj khattar &lt; /strong>; &lt;br />; | 2:30 pm CST &lt; /p>; &lt;p>; Qualtran：一个开源库，用于有效资源估计容宽算法的资源&lt;br />;主持人：&lt;strong>; tanuj khattar &lt; /strong>; &lt;br />; | 11:00 AM CST &lt; /p>; &lt;p>; $ 5M Xprize /Google Quantum AI竞赛，以加速量子应用Q＆amp; a &lt;br />;主持人：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />; | 11:00 AM CST &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;谈话&lt;/h2>; &lt;h3>;星期一&lt;/h3>; >; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/a45.1&quot;>;从少数少数人身上认识高度enterge单量测量&lt;/a>; &lt;br />;主持人：&lt;strong>; hsin-yuan huang &lt; /strong>; &lt;br />;作者：&lt;strong>; hsin-yuan huang &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; A45：机器学习量子物理学的新边界&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.ops.org/meeting/mar24/session/session/a51.2&quot;>;用超导码头的模拟量子模拟&lt;/a>; &lt;br />;主持人：&lt;strong>; trond andersen &lt;/strong>; &lt;br />;作者：&lt;strong>; trond i andersen &lt;/strong>;，&lt;strong>; xiao mi &lt;/strong >;，&lt;strong>; Amir H Karamlou &lt;/strong>;，&lt;strong>; Nikita Astrakhantsev &lt;/strong>;，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; Julia Berndtsson &lt;/strong>; strong>;，&lt;strong>; dmitry abanin &lt;/strong>;，&lt;strong>; lev b ioffe &lt;/strong>;，&lt;strong>; yu chen &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; pedram roushan &lt; /strong>; &lt;br />; &lt;em>;会话A51：嘈杂的量子硬件上的应用程序i &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/sessision /B50.6&quot;>;在表面代码电路上下文中的电路错误&lt;/a>; &lt;br />;主持人：&lt;strong>; dripto m debroy &lt;/strong>; &lt;br />;作者：&lt;strong>; dripto m debroy &lt;/strong &lt;/strong &lt;/strong &lt;/strong >;，&lt;strong>; Jonathan A Gross &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>;，&lt;strong>; Zhang jiang &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session b50：用qCVV技术表征噪声&lt;/em em em em em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/b51.6&quot;>;量子计算惯性融合目标设计的停止功率量量经典算法的限制&lt;/a>; &lt;br />;主持人：Andrew D. Baczewski &lt;br />;作者：&lt;strong>; Nicholas C. Rubin &lt; /strong>;，Dominic W. Berry，Alina Kononov，&lt;strong>; fionn D. Malone &lt;/strong>;，&lt;strong>; Tanuj Khattar &lt;/strong>;，Alec White，&lt;strong>; Joonho Lee &lt;/strong>;，&lt;strong>; Hartmut Neven &lt;/strong>;，&lt;strong>; Ryan Babbush &lt;/strong>;，Andrew D. BACZEWSKI &lt;br />; &lt;em>; session B51：量子应用的异构设计&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2308.12355.pdf&quot;>;链接到纸张&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/b51.7&quot;>;以及经典算法的极限&lt;/a>; &lt;br />;主持人：&lt;strong>; nicholas C. rubin &lt;/strong>; &lt;br />;作者：&lt;strong>; nicholas C. Rubin &lt;/strong>;，Dominic W. Berry，Dominic W. Berry， Alina Kononov，&lt;strong>; Fionn D. Malone &lt;/strong>;，&lt;strong>; Tanuj Khattar &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Joonho Lee &lt;/strong>;，&lt;strong>; hartmut neven neven &lt;/strong>;，&lt;strong &lt;strong >; Ryan Babbush &lt;/strong>;，Andrew D. Baczewski &lt;br />; &lt;em>; session B51：量子应用的异质设计&lt;/em>; &lt;br />; &lt;br />; 2308.12352.pdf“>;链接到纸张&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/b56.4&quot;>; nisq to Fart bolance &lt;/a>; &lt;br />;主持人：&lt;strong>; sabrina s hong &lt; /strong>; &lt;br />;作者：&lt;strong>; sabrina s hong &lt; /strong>; &lt;br />; &lt;br />;从NISQ到FART耐受&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/b31.9&quot;>; /a>; &lt;br />;主持人：&lt;strong>; Ramis Movassagh &lt; /strong>; &lt;br />;作者：Alireza Seif，Yu-Xin Wang，&lt;strong>; Ramis Movassagh &lt; /strong>;，Aashish A. clerk &lt;br />; &lt;em>;会议B31：多体系统的测量引起的关键性&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2310.18305.pdf&quot;>;链接到纸张/p>; &lt;p>; &lt;a href=&quot;https://meetings.ops.org/meeting/mar24/session/b52.9&quot;>;有效的量子量，保真度和计算成本的噪音量子处理实验&lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt; BR/>;主持人：&lt;strong>; Salvatore Mandra &lt;/strong>; &lt;br />;作者：&lt;strong>; Kostyantyn Kechedzhi &lt;/strong>;，&lt;strong>; Sergei v isakov v isakov &lt;/strong>;，&lt;strong>; salvatore mandra &lt;/strong &lt;/strong>; ，&lt;strong>;本杰明·维拉隆加（Benjamin Villalonga）&lt;/strong>;，&lt;strong>; x。 mi &lt;/strong>;，&lt;strong>; sergio boixo &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session b52：量子算法和复杂性&lt;/em>; &lt;br />; =“ https://arxiv.org/pdf/2306.15970.pdf”>;链接到纸张&lt;/a>; &lt;/a>; &lt;/p>; &lt;p>; &lt;a href =“ Session /D60.4“>;使用机器学习相互作用电位和原子位置的协方差&lt;/a>; &lt;br />;演示者：MGCINI K PHUTHI &lt;br />;作者：Mgcini K Phuthi，Yang Huang，Michael Widom，Michael Widom ，&lt;strong>; ekin d cubuk &lt;/strong>;，Venkat Viswanathan &lt;br />; &lt;em>;会话D60：分子和材料的机器学习：化学空间和动力学&lt;/em>; &lt;/em>; &lt;/p>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/div>; &lt;h3>;星期二&lt;/h3>; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;p>; &lt;a href =“ https：https： //meetings.aps.org/meeting/mar24/session/f50.4&quot;>; in-situ脉冲信封特征技术（Inspect）&lt;/a>; &lt;br />; &lt;br />;主持人：&lt;strong>; Zhang jiang &lt;/strong>; &lt;/strong>; &lt;br />;作者：&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>; Jonathan a Gross &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session f50：高级随机基准和门校准态BR/>;主持人：&lt;strong>; Jonathan A Gross &lt;/strong>; &lt;br />;作者：&lt;strong>; Jonathan a Gross &lt;/strong>;，&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>;éliegenois，dripto m debroy &lt;/strong>;，ze-pei cian*，&lt;strong>; wojciech mruczkiewicz &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session f50：高级随机基准测试和门校准&lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.org/meeting/mar24/session/eee01.2”>;带有二次模型的回归统计物理&lt;/a>; &lt;br />;主持人：Blake Bordelon &lt;br />; Bordelon，Cengiz Pehlevan，&lt;strong>; Yasaman Bahri &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session EE01：V：统计和非线性物理学II​​ &lt;/em>; &lt;/em>; &lt;/em>; &lt;/p &lt;/p &lt;p>; &lt;a href = a href =“ https：/ /Meetings.aps.org/meeting/mar24/session/g51.2&quot;>;改进的状态准备电子结构的先定制模拟&lt;/a>; &lt;br />;主持人：&lt;strong>; William J Huggins &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/ Br/>;作者：&lt;strong>; William J Huggins &lt;/strong>;，&lt;strong>; Oskar Leimkuhler &lt;/strong>;，&lt;strong>; Torin f stetina &lt;/strong>;，&lt;strong>; birgitta whaley &lt;/strong>; &lt;br />; &lt;em>; session G51：汉密尔顿模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/g30.2&quot;>;控制大型超导量子处理器&lt;/a>; &lt;br />;主持人：&lt;strong>; Paul V. Klimov &lt;/strong>; &lt;br />;作者：&lt;strong>; Paul V. Klimov &lt;/strong>;，&lt;strong>; Andreas Bengtsson &lt;/strong>;，&lt; Strong>; Chris Quintana &lt;/strong>;，&lt;strong>; Alexandre Bourassa &lt;/strong>;，&lt;strong>; Sabrina Hong &lt;/strong>;，&lt;strong>; Andrew Dunsworth &lt;/strong>;，&lt;strong>; Kevin J. Satzinger &lt;/strong>; ，&lt;strong>; William P. Livingston &lt;/strong>;，&lt;strong>; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Murphy Y. Niu &lt;/strong>;，&lt;strong>; Trond I. Andersen I. Yaxing Zhang &lt;/strong>;，&lt;strong>; desmond Chik &lt;/strong>;，&lt;strong>; Zijun Chen &lt;/strong>;，&lt;strong>; Charles Neill &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; catherine Erickson &lt;/strong>; Alejandro Grajales dau &lt;/strong>;，&lt;strong>; Anthony Megrant &lt;/strong>;，&lt;strong>; Pedram Roushan &lt;/strong>;，&lt;strong>; Alexander N. Korotkov &lt;/strong>;，&lt;strong>; Julian Kelly &lt;/strong>;，&lt;strong>; &lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; yu chen &lt;/strong>;，&lt;strong>; hartmut neven neven &lt;/strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session g30：量子计算的商业应用&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.02321.pdf&quot;>;链接到纸张&lt;/a>; &lt;/a>; &lt;/p>; &lt;p>; &lt;a href =“ /meeting/mar24/session/g50.5&quot;>; gaussian玻色子采样：确定量子优势&lt;/a>; &lt;br />;主持人：Peter D Drummond &lt;br />;作者：Peter d Drummond，Alex Dlummond，Alex Dellios，Alex Dellios，Ned Goodman，Ned Goodman，Margaret D里德（Reid），&lt;strong>; ben villalonga &lt;/strong>; &lt;br />; &lt;em>; session g50：量子表征，验证和验证ii &lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https：// https：// 。 Yiqing Zhou，Yichen Xu，Chao Wan，Jin Zhou，&lt;strong>; Yuri d Lensky &lt;/strong>;，Jesse Hoke，&lt;strong>; Pedram Roushan &lt;/strong>;，Kilian Q Weinberger，Eun-Ah Kim kim &lt;br />; >;会议G50：量子表征，验证和验证II &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/k48.10&quot;>;平衡超导电路中的耦合&lt;/a>; &lt;br />;主持人：&lt;strong>; Daniel T Sank &lt;/strong>; &lt;br />;作者：&lt;strong>; Daniel T Sank &lt;/strong>;，&lt;strong>; Sergei v Isakov &lt;/strong >;，&lt;strong>; Mostafa Khezri &lt;/strong>;，&lt;strong>; Juan Atalaya &lt;/strong>; &lt;br />; &lt;em>; session K48：强烈驱动的超导系统&lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;p>; &lt;p>; &lt;a href = a href = “ https://meetings.ops.org/meeting/mar24/session/k49.12&quot;>; resource使用qᴜᴀʟᴛʀᴀɴ&lt;/a>; &lt;br />;主持人：&lt;strong>; tanuj khattar &lt;/strong>; &lt;/strong>; &lt;/strong>; &lt;/>; br/>;作者：&lt;strong>; tanuj khattar &lt;/strong>;，&lt;b>; Matthew Harrigan &lt;/b>;，&lt;b>; Fionn D. Malone &lt;/b>;，&lt;b>; nour yosri &lt;/b>;，&lt;b>; Nicholas C. Rubin &lt;/b>; &lt;br />; &lt;em>;会话K49：近期量子计算机上的算法和实现&lt;/em>; &lt;/em>; &lt;/p>; &lt;/div>; &lt;/div>; &lt;div style =&#39;line-Height：40％ ;“>; &lt;br />; &lt;/div>; &lt;h3>;星期三&lt;/h3>; &lt;div style =” Margin-Left：20px;“>; &lt;p>; &lt;a href =” https://meetings.aps.aps.org/会议/mar24/session/m24.1“>;用超导量子台发现新颖的量子动态&lt;/a>; &lt;br />; &lt;br />;主持人：&lt;strong>; pedram roushan &lt;/strong>; &lt;br />;作者：&lt;strong>; pedram roushan &lt;/强>; &lt;br />; &lt;em>;会话M24：跨平台的模拟量子模拟&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.orp.org/meeting/mar24/mar24/session/session/m27 .7“>;三阴性乳腺癌中的解剖肿瘤异质性：动态细胞细胞和细胞矩阵相互作用的关键作用，Celeste Nelson，Molly Brennan，&lt;strong>; Mohak Patel &lt; /strong>;，Christian Franck，Sophia Martinez，Joe Tien，Lena Gamboa，Thomas Valentin，Amanda Khoo，Amanda Khoo，Evelyn K Williams &lt;br />;细胞和组织II &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/n48.2&quot;>;朝着实施受保护的电荷-Parity Qubits &lt; /a>; &lt;br />;主持人：Abigail Shearrow &lt;br />;作者：Abigail Shearrow，Matthew Snyder，Bradley G Cole，Kenneth R Dodge，Yebin Liu，Andrey Klots，&lt;strong>; Plourde，Robert McDermott &lt;br />; &lt;em>; session N48：非常规超导码头&lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ .3“>;电子电容隧道连接处的受保护电荷 - 偏向码头&lt;/a>; &lt;br />;主持人：Bradley G Cole &lt;br />;作者：Bradley G Cole，Kenneth R Dodge，Yebin Liu，Yebin Liu，Abigail Shearrow，Matthew Snyder Snyder Snyder Snyder Snyder Snyder Snyder Snyder Snyder Snyder ，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; lev b ioffe &lt;/strong>;，Robert McDermott，Blt Plourde &lt;br />; &lt;br />; &lt;em>; session n48：非常规的超导速度Qubits &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/p &lt;p &lt;p >; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/n51.7&quot;>;克服量子错误校正中的泄漏&lt;/a>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;/strong>; &lt;br />;作者：&lt;strong>; Kevin C. Miao &lt;/strong>;，&lt;strong>; Matt McEwen &lt;/strong>;，&lt;strong>; Juan Atalaya &lt;/strong>;，&lt;strong>; dvir kafri &lt;/strong &lt;/strong >;，&lt;strong>; Leonid P. Pryadko &lt;/strong>;，&lt;strong>; Andreas Bengtsson &lt;/strong>;，&lt;strong>; Alex Opremcak &lt;/strong>;，&lt;strong>; Kevin J. Satzinger J. Satzinger J. Satzinger &lt;/strong>;，&lt;strong>; Zijun Chen &lt;/strong>;，&lt;strong>; Paul V. Klimov &lt;/strong>;，&lt;strong>; Chris Quintana &lt;/strong>;，&lt;strong>; Rajeev Acharya &lt;/strong>;，&lt;strong>; Kyle Anderson &lt;/strong &lt;/strong &lt;/strong >; Markus Ansmann &lt;/strong>;，&lt;strong>; Frank Arute &lt;/strong>;，&lt;strong>; Kunal Arya &lt;/strong>;，&lt;strong>; Abraham Asfaw &lt;/strong>;，&lt;strong>; Joseph C. Bardin &lt;/strong>;， &lt;strong>; Alexandre Bourassa &lt;/strong>;，&lt;strong>; Jenna Bovaird &lt;/strong>;，&lt;strong>; Leon Brill &lt;/strong>;，&lt;strong>; Bob B. B. B. Buckley &lt;/strong>;，&lt;strong>; David A. Buell A. Buell &lt; /strong>;，&lt;strong>; Tim Burger &lt;/strong>;，&lt;strong>; Brian Burkett &lt;/strong>;，&lt;strong>; Nicholas Bushnell &lt;/strong>;，&lt;strong>; Juan Campero &lt;/strong>;，&lt;strong>; Ben Chiaro &lt; /strong>;，&lt;strong>; Roberto Collins &lt;/strong>;，&lt;strong>; Paul Conner &lt;/strong>;，&lt;strong>; Alexander L. Crook &lt;/strong>;，&lt;strong>; Ben Curtin &lt;/strong &lt;/strong>;，&lt;strong>; dripto M. Debroy &lt;/strong>;，&lt;strong>; Sean DeMura &lt;/strong>;，&lt;strong>; Andrew Dunsworth &lt;/strong>;，&lt;strong>; Catherine Erickson &lt;/strong>;，&lt;strong>; reza fatemi &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong >; Vinicius S. ferreira &lt;/strong>;，&lt;strong>; Leslie Flores burgos &lt;/strong>;，&lt;strong>; ebrahim forati &lt;/strong>;，&lt;strong>; Austin G. Fowler &lt;/strong>;，&lt;strong>; Brooks Foxen &lt;/ Strong>;，&lt;strong>; Gonzalo Garcia &lt;/strong>;，&lt;strong>; William Giang &lt;/strong>;，&lt;strong>; Craig Gidney &lt;/strong>;，&lt;strong>; Marissa Giustina &lt;/strong>;，&lt;strong>; raja gosula &lt;/strong>; Strong>;，&lt;strong>; Alejandro Grajales dau &lt;/strong>;，&lt;strong>; Jonathan A. Gross &lt;/strong>;，&lt;strong>; Michael C. Hamilton &lt;/strong>;，&lt;strong>; Sean D. Harrington &lt;/strong>;， &lt;strong>; Paula Heu &lt;/strong>;，&lt;strong>; Jeremy Hilton &lt;/strong>;，&lt;strong>; Markus R. Hoffmann &lt;/strong>;，&lt;strong>; Sabrina Hong &lt;/strong>;，&lt;strong>; Trent Huang &lt;/strong &lt;/strong >;，&lt;strong>; Ashley Huff &lt;/strong>;，&lt;strong>; Justin Iveland &lt;/strong>;，&lt;strong>; Evan Jeffrey &lt;/strong>;，&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>; Cody Jones &lt;/strong &lt;/strong >;，&lt;strong>; Julian Kelly &lt;/strong>;，&lt;strong>; Seon Kim &lt;/strong>;，&lt;strong>; Fedor Kostritsa &lt;/strong>;，&lt;strong>; John Mark Kreikebaum &lt;/strong>;，&lt;strong>; David Landhuis &lt;/ strong>;，&lt;strong>; pavel laptev &lt;/strong>;，&lt;strong>;百合法&lt;/strong>;，&lt;strong>; kenny Lee &lt;/strong>;，&lt;strong>; Brian J. Lester &lt;/strong>;，&lt;strong>; Alexander t 。 Anthony Megrant &lt;/strong>;，&lt;strong>; Xiao Mi &lt;/strong>;，&lt;strong>; Shirin Montazeri &lt;/strong>;，&lt;strong>; Alexis Morvan &lt;/strong>;，&lt;strong>; ofer Naaman &lt;/strong>;，&lt;strong>; Matthew Neeley &lt;/strong>;，&lt;strong>; Charles Neill &lt;/strong>;，&lt;strong>; ani nersisyan &lt;/strong>;，&lt;strong>; Michael Newman &lt;/strong>;，&lt;strong>; >; Anthony Nguyen &lt;/strong>;，&lt;strong>; Murray nguyen &lt;/strong>;，&lt;strong>; Rebecca Potter &lt;/strong>;，&lt;strong>; Charles Rocque &lt;/strong>;，&lt;strong>; Pedram Roushan &lt;/strong>;，&lt;strong &lt;strong >; Kannan Sankaragomathi &lt;/strong>;，&lt;strong>; Christopher Schuster &lt;/strong>;，&lt;strong>; Michael J. Shearn &lt;/strong>;，&lt;strong>; Aaron Shorter &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; &lt;strong>; Vladimir Shvarts &lt;/strong>;，&lt;strong>; Jindra Skruzny &lt;/strong>;，&lt;strong>; w。 Clarke Smith &lt;/strong>;，&lt;strong>; George Sterling &lt;/strong>;，&lt;strong>; Marco Szalay &lt;/strong>;，&lt;strong>; Douglas Thor &lt;/strong>;，&lt;strong>; Alfredo Torres &lt;/strong>;，&lt;strong>; Theodore White &lt;/strong>;，&lt;strong>; Bryan WK Woo &lt;/strong>;，&lt;strong>; Z。 Jamie Yao &lt;/strong>;，&lt;strong>; ping yeh &lt;/strong>;，&lt;strong>; juhwan yoo &lt;/strong>;，&lt;strong>; grayson Young &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>; Ningfeng Zhu &lt;/strong>;，&lt;strong>; nicholas Zobrist &lt;/strong>;，&lt;strong>; hartmut neven &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，&lt;strong>; andre petukhov &lt;/strong>;，&lt;strong>;，&lt;strong>; Alexander N. Korotkov &lt;/strong>;，&lt;strong>; Daniel Sank &lt;/strong>;，&lt;strong>; Yu Chen &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session n51：量子错误校正代码性能和实现i &lt;/em>; &lt;br />; &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;链接到纸张：//meetings.aps.org/meeting/mar24/session/n51.11“>;对表面代码的性能进行建模，以非均匀误差分布：第1部分&lt;/a>; &lt;br />;主持人：&lt;strong>; yuri：&lt;strong>; yuri D Lensky &lt;/strong>; &lt;br />;作者：&lt;strong>; Yuri d Lensky &lt;/strong>;，&lt;strong>; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Kostyantyn kechedzhi &lt;/strong>;，&lt;strong>; &lt;strong>; igor aleiner &lt;/强>; &lt;br />; &lt;em>;会话N51：量子错误校正代码性能和实现I &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/ Session/N51.12“>;对表面代码的性能进行非均匀错误分布进行建模：第2部分&lt;/a>; &lt;br />; &lt;br />;主持人：&lt;strong>; Volodymyr Sivak &lt;/strong>; &lt;br />;作者：&lt;strong >; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Michael Newman &lt;/strong>;，&lt;strong>; Cody Jones &lt;/strong>;，&lt;strong>; Henry Schurkus &lt;/strong>;，&lt;strong>; dvir kafri &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong >; Yuri d Lensky &lt;/strong>;，&lt;strong>; Paul Klimov &lt;/strong>;，&lt;strong>; Kostyantyn Kechedzhi &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>; &lt;br/>;校正代码性能和实施I &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/q51.7&quot;>;高度优化的张量网络网络收缩&lt;/a>; &lt;br />;主持人的模拟&lt;/a>; &lt;strong>;本杰明·维拉隆加（Benjamin Villalonga）&lt; /strong>; &lt;br />;作者：&lt;strong>; benjamin villalonga &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session Q51：量子古典算法的共同进化各个级别的动手开源软件&lt;/a>; &lt;br />;主持人：&lt;strong>; Abraham Asfaw &lt; /strong>; &lt;br />;作者：&lt;strong>; Abraham Asfaw &lt; /strong>; &lt;br />; &lt;br />; >;会议Q61：在所有级别II &lt;/em>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;星期四&lt; /h3>; &lt;div style =“ margin-left：20px;”>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/s51.1&quot;>;颜色代码的源解码器&lt;/a>; &lt;br />;主持人：&lt;strong>; craig gidney &lt;/strong>; &lt;br />;作者：&lt;strong>; craig gidney &lt;/strong>;，&lt;strong>; cody jones &lt;/strong>; &lt;br />; &lt;em>;会话S51：量子错误校正代码性能和实现II &lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/pdf/2312.08813.pdf&quot;>;链接到纸张&lt; /a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/s18.2&quot;>;用大语言模型进行Hartree-Fock多体物理学计算/a>; &lt;br />;主持人：&lt;strong>; eun-ah kim &lt;/strong>; &lt;br />;作者：&lt;strong>; eun-ah kim kim &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strand pan，&lt;strong>; nayantara mudur &lt;/strong>; ，威廉·塔兰托（William Taranto），&lt;strong>; subhashini venugopalan &lt;/strong>;，&lt;strong>; yasaman bahri &lt;/strong>;，&lt;strong>; Michael P Brenner &lt;/strong>; &lt;br />;物理学学习i &lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/session/s51.5&quot;>;新的方法将资源降低水平的新方法代码&lt;/a>; &lt;br />;主持人：&lt;strong>;迈克尔·纽曼彼得·布鲁克斯&lt;/strong>;，&lt;strong>; cody Jones &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session s51：量子错误校正代码性能和实现II &lt;/em>; &lt;br />; &lt;br />; &lt;a a href =“ https：/ /arxiv.org/pdf/2312.04522.pdf&quot;>; link to Paper &lt;/a>; &lt;/p>; &lt;p>; &lt;p>; &lt;a href =“ https://meetings.aps.orp.org/meeting/mar24/mar24/session/session/session/s49.10 “>;将量子计算机应用于药物设计的挑战和机会, Leticia Gonzalez, Elica Kyoseva, Nikolaj Moll, Markus Oppel, Robert M. Parrish, &lt;strong>;Nicholas C. Rubin&lt;/strong>;, Michael Streif, Christofer S. Tautermann, Horst Weiss, Nathan Wiebe, Clemens Utschig-Utschig &lt;br />; &lt;em>;会话S49：近期应用程序的量子算法进步a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/t45.1&quot;>;从Google的新应用程序中的超级Quadratic量子优势中调度&lt;/ a>; &lt;br />;主持人：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />;作者：&lt;strong>; ryan babbush &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; session t45：量子算法的最新进展&lt; /em em em em >; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/t48.11&quot;>; qubit作为反射计&lt;/a>; presenter：&lt;br />; &lt;br />; &lt;br />; &lt;strong >; yaxing Zhang &lt;/strong>; &lt;br />;作者：&lt;strong>; yaxing Zhang &lt;/strong>;，&lt;strong>; benjamin chiaro &lt;/strong>; &lt;br />; &lt;br />; &lt;em>; session t48：超导制造，包装，包装等；验证&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.ops.org/meeting/mar24/mar24/session/session/w14.3&quot;>;随机测量诱导的随机-MATRIX在非局限性的相位过渡floquet量子电路&lt;/a>; &lt;br />;主持人：Aleksei Khindanov &lt;br />;作者：Aleksei Khindanov，&lt;strong>; lara faoro &lt;/strong>;，&lt;strong>; lev ioffe &lt;/strong>;，&lt;strong>; /strong>; &lt;br />; &lt;em>; session W14：测量引起的相变&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.ops.org/meeting/mar24/session/ W58.5“>;有限密度的连续限量多体基接地状态&lt;/a>; &lt;br />;主持人：Subhayan Sahu &lt;br />;作者：Subhayan Sahu，&lt;strong>;GuifréVidalvidal>; &lt; /strong>; &lt; /strong>; &lt;br / >; &lt;em>;会话W58：流体动力学和相关学科中的极端计算科学发现II &lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.orp.org/meeting/mar24/ session/w50.8“>;在海森伯格自旋链中无限温度下磁化的动力学&lt;/a>; &lt;br />;主持人：&lt;strong>; Eliott Rosenberg &lt;/strong>; &lt;br />;作者：&lt;strong>; Eliott Rosenberg &lt;/ Strong>;，&lt;strong>; Trond Andersen &lt;/strong>;，Rhine Samajdar，&lt;strong>; Andre Petukhov &lt;/strong>;，Jesse Hoke*，&lt;strong>; Dmitry Abanin &lt;/strong>;，&lt;strong>; Andreas Bengtsson &lt;/strong>;， &lt;strong>; Ilya drozdov &lt;/strong>;，&lt;strong>;凯瑟琳·埃里克森（Catherine Erickson）&lt;/strong>;，&lt;strong>; Paul Klimov &lt;/strong>;，&lt;strong>; xiao mi &lt;/strong>;，&lt;strong>; Alexis Morvan &lt;/strong>;， &lt;strong>; Matthew Neeley &lt;/strong>;，&lt;strong>; Charles Neill &lt;/strong>;，&lt;strong>; Rajeev Acharya &lt;/strong>;，&lt;strong>; Richard Allen &lt;/strong>;，&lt;strong>; Kyle Anderson &lt;/strong>;， &lt;strong>; Markus Ansmann &lt;/strong>;，&lt;strong>; Frank Arute &lt;/strong>;，&lt;strong>; Kunal Arya &lt;/strong>;，&lt;strong>; Abraham Asfaw &lt;/strong>;，&lt;strong>; Juan Atalaya &lt;/strong>;， &lt;strong>; Joseph Bardin &lt;/strong>;，&lt;strong>; a。 Bilmes &lt;/strong>;，&lt;strong>; Gina Bortoli &lt;/strong>;，&lt;strong>; Alexandre Bourassa &lt;/strong>;，&lt;strong>; Jenna Bovaird &lt;/strong>;，&lt;strong>; Leon Brill &lt;/strong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Michael Broughton &lt;/strong>;，&lt;strong>; Bob B. Buckley &lt;/strong>;，&lt;strong>; David Buell &lt;/strong>;，&lt;strong>; Tim Burger &lt;/strong>;，&lt;strong>; Brian Burkett &lt;/strong>;，&lt;strong >; Nicholas Bushnell &lt;/strong>;，&lt;strong>; Juan Campero &lt;/strong>;，&lt;strong>; Hung-Shen Chang &lt;/strong>;，&lt;strong>; Zijun Chen &lt;/strong>;，&lt;strong>; benjamin chiaro &lt;/strong &lt;/strong &lt;/strong>;， &lt;strong>; Desmond Chik &lt;/strong>;，&lt;strong>; Josh Cogan &lt;/strong>;，&lt;strong>; Roberto Collins &lt;/strong>;，&lt;strong>; Paul Conner &lt;/strong>;，&lt;strong>; William Courtney &lt;/strong>;， &lt;strong>; Alexander Crook &lt;/strong>;，&lt;strong>; Ben Curtin &lt;/strong>;，&lt;strong>; Dripto Debroy &lt;/strong>;，&lt;strong>; Alexander del Toro Barba &lt;/strong>;，&lt;strong>; Sean Demura &lt;/strong >;，&lt;strong>; Agustin di Paolo &lt;/strong>;，&lt;strong>; Andrew Dunsworth &lt;/strong>;，&lt;strong>; Clint Earle &lt;/strong>;，&lt;strong>; e。 farhi &lt;/strong>;，&lt;strong>; reza fatemi &lt;/strong>;，&lt;strong>; vinicius ferreira &lt;/strong>;，&lt;strong>; leslie flores &lt;/strong>;，&lt;strong>; ebrahim forati &lt;/strong>;，&lt;strong>; austin Fowler &lt;/strong>;，&lt;strong>; brooks foxen &lt;/strong>;，&lt;strong>; gonzalo garcia &lt;/strong>;，&lt;strong>;éliegenois &lt;/strong>;，&lt;strong>; william giang &lt;/strong>;，&lt;strong>; craig Gidney &lt;/strong>;，&lt;strong>; dar Gilboa &lt;/strong>;，&lt;strong>; Marissa Giustina &lt;/strong>;，&lt;strong>; raja gosula &lt;/strong>;，&lt;strong>; alejandro grajales dau &lt;/strong>;，&lt;strong>; Jonathan Gross &lt;/strong>;，&lt;strong>; Steve Habegger &lt;/strong>;，&lt;strong>; Michael Hamilton &lt;/strong>;，&lt;strong>; Monica Hansen &lt;/strong>;，&lt;strong>; Matthew Harrigan &lt;/strong>;，&lt;strong>; Sean Harrington &lt;/strong>;，&lt;strong>; Paula Heu &lt;/strong>;，&lt;strong>; Gordon Hill &lt;/strong>;，&lt;strong>; Markus Hoffmann &lt;/strong>;，&lt;strong>; Sabrina Hong &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; Trent Huang &lt;/strong>;，&lt;strong>; Ashley Huff &lt;/strong>;，&lt;strong>; William Huggins &lt;/strong>;，&lt;strong>; lev ioffe &lt;/strong>;，&lt;strong>; Sergei Isakov &lt;/strong>;，&lt;strong>; Justin Iveland &lt;/strong>;，&lt;strong>; Evan Jeffrey &lt;/strong>;，&lt;strong>; Zhang Jiang &lt;/strong>;，&lt;strong>; Cody Jones &lt;/strong>;，&lt;strong>; Pavol Juhas &lt;/strong>;，&lt;strong>; D . kafri &lt;/strong>;，&lt;strong>; tanuj khattar &lt;/strong>;，&lt;strong>; mostafa khezri &lt;/strong>;，&lt;strong>;máriaKieferová&lt;/strong>;，&lt;strong>; seon kim &lt;/strong>;，&lt;strong>; Alexei Kitaev &lt;/strong>;，&lt;strong>; Andrey Klots &lt;/strong>;，&lt;strong>; Alexander Korotkov &lt;/strong>;，&lt;strong>; Fedor Kostritsa &lt;/strong>;，&lt;strong>; John Mark Kreikebaum &lt;/strong>;，&lt;strong>; David Landhuis &lt;/strong>;，&lt;strong>; Pavel Laptev &lt;/strong>;，&lt;strong>; Kim Ming Lau &lt;/strong>;，&lt;strong>; Lily Laws &lt;/strong>;，&lt;strong>; Joonho Lee &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong &lt;strong >; Kenneth Lee &lt;/strong>;，&lt;strong>; Yuri Lensky &lt;/strong>;，&lt;strong>; Brian Lester &lt;/strong>;，&lt;strong>; Alexander Lill &lt;/strong>;，&lt;strong>; Wayne Liu &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;strong &lt;strong >; William P. Livingston &lt;/strong>;，&lt;strong>; a。 locharla &lt;/strong>;，&lt;strong>; SalvatoreMandrà&lt;/strong>;，&lt;strong>; Orion Martin &lt;/strong>;，&lt;strong>; Steven Martin &lt;/strong>;，&lt;strong>; Jarrod McClean &lt;/strong>;，&lt;strong>; Matthew McEwen &lt;/strong>;，&lt;strong>; Seneca Meeks &lt;/strong>;，&lt;strong>; Kevin Miao &lt;/strong>;，&lt;strong>; Amanda Mieszala &lt;/strong>;，&lt;strong>; Shirin Montazeri &lt;/strong>;，&lt;strong>; Ramis Movassagh &lt;/strong>;，&lt;strong>; wojciech mruczkiewicz &lt;/strong>;，&lt;strong>; ani nersisyan &lt;/strong>;，&lt;strong>; Michael Newman &lt;/strong &lt;/strong>;，&lt;strong>; Anthony Nguyen &lt;/strong>;，&lt;strong>; Murray Nguyen &lt;/strong>;，&lt;strong>; m。 Niu &lt;/strong>;，&lt;strong>; Thomas O&#39;Brien &lt;/strong>;，&lt;strong>; Seun Omonije &lt;/strong>;，&lt;strong>; Alex Opremcak &lt;/strong>;，&lt;strong>; rebecca Potter &lt;/strong>;，&lt;strong >; Leonid Pryadko &lt;/strong>;，&lt;strong>; Chris Quintana &lt;/strong>;，&lt;strong>; David Rhodes &lt;/strong>;，&lt;strong>; Charles Rocque &lt;/strong>;，&lt;strong>; n。鲁宾&lt;/strong>;，&lt;strong>; negar saei &lt;/strong>;，&lt;strong>;丹尼尔·桑克&lt;/strong>;，&lt;strong>; kannan sankaragomathi &lt;/strong>;，&lt;strong>; kevin kevin satzinger &lt;/strong &lt;/strong &lt;/strong>;，&lt;strong>; henry Schurkus &lt;/strong>;，&lt;strong>; Christopher Schuster &lt;/strong>;，&lt;strong>; Michael Shearn &lt;/strong>;，&lt;strong>; Aaron Shorter &lt;/strong>;，&lt;strong>; Shvarts &lt;/strong>;，&lt;strong>; Volodymyr Sivak &lt;/strong>;，&lt;strong>; Jindra Skruzny &lt;/strong>;，&lt;strong>; Clarke Smith &lt;/strong>;，&lt;strong>; Rolando Somma &lt;/strong &lt;/strong>; Sterling &lt;/strong>;，&lt;strong>; Doug菌株&lt;/strong>;，&lt;strong>; Marco Szalay &lt;/strong>;，&lt;strong>; Douglas Thor &lt;/strong>;，&lt;strong>; Alfredo Torres &lt;/strong>;，&lt;strong>; guifre vidal &lt;/strong>;，&lt;strong>;本杰明·维拉隆加（Benjamin villalonga）&lt;/strong>;，&lt;strong>;凯瑟琳·沃尔格拉夫·海德威勒（Catherine Vollgraff Heidweeller）&lt;/strong>;，&lt;strong>; theodore white &lt;/strong>;，&lt;strong>; bryan woo &lt;/strong>;，&lt;strong>; Cheng Xing &lt;/strong>;，&lt;strong>; Jamie Yao &lt;/strong>;，&lt;strong>; ping yeh &lt;/strong>;，&lt;strong>; juhwan yoo &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong &lt;/strong>; Adam Zalcman &lt;/strong>;，&lt;strong>; yaxing Zhang &lt;/strong>;，&lt;strong>; ningfeng Zhu &lt;/strong>;，&lt;strong>; Nicholas Zobrist &lt;/strong>;，&lt;strong>; Ryan Babbush &lt;/strong>;，&lt;strong>; Dave Bacon &lt;/strong>;，&lt;strong>; Sergio Boixo &lt;/strong>;，&lt;strong>; Jeremy Hilton &lt;/strong>;，&lt;strong>; Erik Lucero &lt;/strong>;，&lt;strong>; Anthony Megrant &lt;/strong>;，&lt;strong>; Julian Kelly &lt;/strong>;，&lt;strong>; Yu Chen &lt;/strong>;，&lt;strong>; vadim smelyanskiy &lt;/strong>;，Vedika Khemani，Sarang Gopalakrishnan，&lt;strong>; toma toma toma tomatomažprosen&lt;/ strong>;，&lt;strong>; pedram roushan &lt;/strong>; &lt;br />; &lt;em>; session W50：多体物理学的量子模拟&lt;/em>; &lt;br />; &lt;br />; &lt;a href =“ https://arxiv.org/ pdf/2306.09333.pdf“>;链接到纸&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.orp.org/meeting/mar24/mar24/sessive/w50.13&quot;>;量子计算机上的方法&lt;/a>; &lt;br />;主持人：Kianna Wan &lt;br />;作者：Kianna Wan，Dominic W Berry，&lt;strong>; Ryan Babbush &lt; /strong>; &lt;br />;多体物理学的仿真&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/div>; &lt;h3>;星期五&lt;/h3>; &lt;div style =“ Margin-Left：20px;”>; &lt;p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/session/session/y43.1&quot;>;量子计算行业和保护国家安全：什么工具会起作用吗？&lt;/a>; &lt;br />;主持人：&lt;strong>; kate Weber &lt; /strong>; &lt;br />;作者：&lt;strong>; Kate Weber &lt; /strong>; &lt;br />; &lt;br />; &lt;br />; &lt;em>; y43：行业，创新和国家安全：找到正确的余额&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/meeting/mar24/mar24/session/session/y46.3&quot;>;新颖的充电效果在Fluxonium Qubit &lt;/a>; &lt;br />;主持人：&lt;strong>; Agustin di paolo &lt;/strong>; &lt;br />;作者：&lt;strong>; agustin di paolo &lt;/strong>;，Kyle Serniak，Andrew J Kerman，Andrew J Kerman，&lt;strong，&lt;strong &lt;strong >; William D Oliver &lt;/strong>; &lt;br />; &lt;em>;会话y46：基于Fluxonium的超导sigibits &lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ /mar24/session/z46.3&quot;>;超导电路中参数相互作用的microwave工程&lt;/a>; &lt;br />;主持人：&lt;strong>; ofer naaman &lt;/strong>; &lt;br />;作者：&lt;strong>;强>; &lt;br />; &lt;em>;会话Z46：宽带参数放大器和循环器&lt;/em>; &lt;/p>; &lt;p>; &lt;a href =“ https://meetings.aps.ops.org/meeting/mar24/mar24/session/session/z62 .3“>;使用内核多项式方法的大磁单元细胞的线性自旋波理论&lt;/a>; &lt;br />;主持人：Harry Lane &lt;br />;作者：Harry Lane，Hao Zhang，David A Dahlbom，Sam Quinn，&lt; Strong>; Rolando d Somma &lt;/strong>;，Martin P Murigal，Cristian D Batista，Kipton Barros &lt;br />; &lt;em>; secess Z62：合作现象，理论&lt;/em>; &lt;/em>; &lt;/em>; &lt;/p>; &lt;/div>; &lt;！ - 脚注 - >; &lt;hr width =“ 80％” />; &lt;p>; &lt;span class =“苹果式式启动” style =“ font-size：x-small;”>; &lt;sup>; &lt;b>;*&lt; />; b>; &lt;/sup>;在Google &lt;/span>; &lt;/p>; &lt;/penter>; &lt;link href =“ http://blog.research.google/feeds/2754526782497247497/comments/comments/comments/default/default =” “ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/google-aps-aps-aps-2024.html#comment-form “ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/default/275452678249724972472477777777.7545267824972497&#39;R =“” “ type =” application/atom+xml“/>; &lt;link href =” http://www.blogger.com/feeds/8474926333145202626/posts/posts/posts/default/27545452678267824972497247247247497 real = “/>; &lt;link href =” http://blog.research.google/2024/03/google-at-aps-2024.html“ rel =“替代” title =“ google at aps 2024” type =“ type =” type =“文本/ html“/>; &lt;aunder>; &lt;名称>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777526611 &lt;/uri>; &lt;emage>; &lt;Email>; ：Image Height =“ 16” RER =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded.gif.gif.gif” width width =“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg “ width =” 72“ XMLNS：媒体=” http://search.yahoo.com/mrss/“>; &lt;/媒体：thumbnail>; &lt;thr：thr>; &lt;thr：thr>; 0 &lt;/thr>; 0 &lt;/thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999年：Blog-8474926331452026626.POST-16952642777638670894 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2024-02-222T12：05：00.000-08：00.000-08：00：00：00：00：00：00 &lt;/00 &lt;/00 ：07：07：08.500-08：00 &lt;/updated>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ Machine Intelligence”>; &lt;/category>; &lt;category>; &lt;category>; &lt;类别方案=“ http：http：http：http：http：http： //www.blogger.com/atom/ns#“ term =“ Machine Conception”>; &lt;/cattory>; &lt;title type =“ text”>; videoprism：视频理解的基础视觉编码&lt;/stitle>; &lt;content>; &lt;content type type =&#39; HTML“>; &lt;span class =” Byline-aTHOTOR“>;由Long Zhao，高级研究科学家Long Zhao和Google Spotch Software Engresser的Ting Liu发表&lt;/span>; &lt;img src =” com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;网络上提供了一个惊人的视频，涵盖了人们共享的日常时刻到历史时刻到科学观察的各种内容，每个观察都包含世界上独特的记录。正确的工具可以帮助研究人员分析这些视频，改变我们如何了解周围的世界。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;视频提供的动态视觉内容比静态图像更丰富，捕获移动，变化和实体之间的动态关系。分析这种复杂性，以及公开可公开的视频数据的巨大多样性，需要超越传统图像理解的模型。因此，许多在视频理解上表现最好的方法仍然依赖于针对特定任务量身定制的专业模型。最近，使用视频基础模型（VIFM）在该领域取得了令人兴奋的进步：//arxiv.org/abs/2212.03191“>; internvideo &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2212.04979&quot;>; videocococa //arxiv.org/abs/2303.16058&quot;>; umt &lt;/a>;。但是，构建处理视频数据多样性的VIFM仍然是一个挑战。 &lt;/p>; &lt;p>;目的是建立一个用于通用视频理解的单个模型，我们介绍了“ &lt;a href=&quot;https://arxiv.org/abs/2402.13217&quot;>; videoprism：videoprism：一种基础视觉编码器：视频理解&lt;/a>;”。 Videoprism是一种VIFM，旨在处理各种视频理解任务，包括分类，本地化，检索，字幕和问题答案（QA）。我们在训练数据和建模策略中都提出了创新。我们在大量和多样化的数据集上预先培训视频：3600万个高质量的视频文本对和5.82亿个带有嘈杂或机器生成的并行文本的视频剪辑。我们的培训预培训方法是为这种混合数据设计的，可以从视频文本对和视频本身学习。摄影非常容易适应新的视频理解挑战，并使用单个冷冻模型来实现最先进的性能。 &lt;/p>; &lt;p>; &lt;/p>; &lt;video autoplay =“” loop =“”“ muted =”“ playsinline =”“ width =“ 100％”>; &lt;source src =“ https://github.com/github.com/garyzhao /videoprism-blog/raw/main/teaser.mp4“ type =“ video/mp4”>; &lt;/source>; &lt;/source>; &lt;/video>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr =” tr -caption-container“ style =”保证金左：自动右：auto;“>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center; center;通用视频编码器，可以通过从单个冷冻模型中产生视频表示形式来实现广泛的视频理解任务，包括分类，本地化，检索，字幕和问题答案。&lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ &lt;/ td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h2>;预训练数据&lt;/h2>; &lt;p>;强大的VIFM需要大量的视频来训练，类似于其他视频基础模型（FMS），例如大型语言模型（LLMS）。理想情况下，我们希望预培训数据成为世界上所有视频的代表性样本。尽管这些视频中的大多数自然没有完美的字幕或描述，但即使是不完美的文本也可以提供有关视频语义内容的有用信息。 &lt;/p>; &lt;p>;为了给我们的模型提供最佳的起点，我们将大量的预训练语料库组成，该语料库由几个公共和私人数据集组成，包括&lt;a href =“ https://rowanzellers.com/merlot/ “>; yt-temoral-180m &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2307.06942&quot;>; internvid &lt;/a>;，&lt;a href =“ /2204.00679&quot;>; videocc &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2007.14937&quot;>; wts-70m &lt;/a>;等。 ，再加上另外5.82亿个片段，具有不同水平的嘈杂文字（例如自动生成的成绩单）。据我们所知，这是同类视频培训语料库最大，最多样化的视频培训语料库。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxssegrhfnm1r-xb3ztydwwc0m7zochlpiqulpiqury quly quly quly qullandtyttytytym Qulllandtyttyttyttyttyttytyty VEE1NU3GNRKJR7PE-YFAIVRC1AL-BXZECSOSO0AOJXFZSDHFV45OZOOBEYAA93IINECGDNURYH4HLC3W7QR2PXPX0PPX0FY6-4QFMTKMTKBORA_PFMTKBORA_PFHSPPPHSPPP7NRR1OW0 NRR1NRR1199999999999999999999999999999. 。 src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>; &lt;td class =“ tr-caption”样式=“ text-align：center;”>;视频文本预训练数据上的统计信息。 ＆nbsp; &lt;a href=&quot;https://arxiv.org/abs/2104.14806&quot;>;剪辑相似性得分&lt;/a>;＆nbsp;（越高，越高，越高）证明了我们的预训练的多样化字幕质量数据，这是用于收获文本的各种方法的副产品。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;h2>;两阶段训练&lt;/h2>; &lt;p>;视频抗流模型体系结构源于标准&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;视觉变压器&lt;/a>;（VIT），其分解设计，该设计依次编码&lt;href之后的空间和时间信息=“ https://arxiv.org/abs/2103.15691”>; vivit &lt;/a>;。我们的培训方法利用了上述嘈杂文本的高质量视频文本数据和视频数据。首先，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/self-supervise_learning#contrastive_self-supervise_learning&quot;>;对比度学习&lt;/a>;同时最大程度地提高负面视频文本对之间的距离），以教我们的模型与自己的文本说明（包括不完美的文本描述）匹配视频。这为将语义语言内容与视觉内容匹配的基础奠定了基础。 &lt;/p>; &lt;p>;在视频文本对比培训后，我们利用没有文本说明的视频收集。在这里，我们建立在&lt;a href=&quot;https://arxiv.org/abs/2212.04500&quot;>;掩盖视频建模框架&lt;/a>;以进行一些改进的视频中预测蒙面的补丁。我们训练该模型，以预测第一阶段模型的视频级全局嵌入和令牌嵌入，以有效利用该阶段中获得的知识。然后，我们将预测的令牌随机洗牌以防止模型学习快捷方式。 &lt;/p>; &lt;p>; Videoprism设置的独特之处在于，我们使用两个互补的预训练信号：文本说明和视频中的视觉内容。文本描述通常集中在外观上，而视频内容提供了有关运动和视觉动态的信息。这使视频杂志能够在需要了解外观和运动的任务中表现出色。 &lt;/p>; &lt;br />; &lt;h2>;结果&lt;/h2>; &lt;p>;我们对视频理解任务的四个广泛类别的视频症进行了广泛的评估，包括视频分类和本地化，视频文本检索，视频字幕，问题回答问题和科学视频理解。 Videoprism在33个视频理解基准中的30个中实现了最先进的性能，这一切都以最小的单个冷冻模型适应。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999 /Image20.png“ ImageAnchor =“ 1”样式=“边距 - 左：自动右：自动;”>; &lt;img border =“ 0” data-Original-height =“ 1999”数据 - 1959&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png&quot; width=&quot;628 “/>;” td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;分类和本地化&lt;/h3>; &lt;p>;我们在现有的大规模视频理解基准（&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>; vievoglue &lt;/a>;）上评估视频概论。我们发现（1）录像带的表现优于所有其他最先进的FM，并且（2）没有其他单个模型一致地排在第二位。这告诉我们，视频杂志已经学会了有效地将各种视频信号包装成一个编码器 - 从不同的粒度语义到外观和运动提示 - 并且在各种视频源中都可以很好地运行。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：中心;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvz2xl/avvxsehnnyg_lnnynnynnynnynnynnynlnlfwdisjelqfwlkjleb1quzor4 xqubf_bknmym to lmccw3ek9qt6nn4lrvff45wu8j2ylcqi4hpe-rfowzmguv8ii6nq8hilemnrs1lmwcuohtvngs04dsxc7yvztamc7yvztamcu0srvumumuhhnnn4unuhhn4u94uikepnikepnikepnike/s1166 g8166.prrywow.prrywow.prrywowwo&#39;&#39;181818frywowwo&#39;1181818frywo&#39;11818frywo&#39;11816.1818frywo&#39; =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 742” data-Original-width =“ 1816” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ Text-Align：Center;”>; Videoprism优于最先进的方法（包括&lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>; clip &lt;/a>;，&lt;a href =&#39; https://arxiv.org/abs/2104.11178&quot;>; vatt &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>; internvideo &lt;/a>; ：//arxiv.org/abs/2303.16058“>; umt &lt;/a>;）在&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;视频理解基准&lt;/a>;。在此图中，我们显示了与以前的最佳模型相比的绝对得分差异，以突出视频的相对改进。在&lt;a href=&quot;http://vuchallenge.org/charades.html&quot;>; charades &lt;/a>;，&lt;a href=&quot;http://activitive-net.org/&quot;>; activitynet &lt;/a>;，&lt;a href =“ https://research.google.com/ava/”>; ava &lt;/a>;和&lt;a href=&quot;https://research.google.com/ava/&quot;>; ava-k &lt;/a>; ，我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/evaluation_measures_(Information_retrieval)#mean_average_precision&quot;>;平均平均精度&lt;/a>;（MAP）作为评估度量标准。在其他数据集上，我们报告Top-1精度。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;与LLMS &lt;/h3>; &lt;p>;结合使用，我们进一步探索了将视频与LLMS结合在一起，以解锁其处理各种视频语言任务的能力。特别是，当与文本编码器配对时（以下&lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;>; lit &lt;/a>;）或语言解码器（例如&lt;a href =” https：/https：/ /arxiv.org/abs/2305.10403&quot;>; palm-2 &lt;/a>;），视频杂志可用于视频 - 文本检索，视频字幕和视频QA任务。我们比较了一组广泛而具有挑战性的视觉语言基准测试的组合模型。 Videoprism在大多数基准测试中设定了新的艺术状态。从视觉结果中，我们发现Videoprism能够理解视频中的复杂动作和外观（例如，模型可以在下面的视觉示例中识别窗口上旋转对象的不同颜色）。这些结果表明，录像带与语言模型非常兼容。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png “ style =”保证金左：自动右右：自动;”>; &lt;img border =“ 0” data-original-height =“ 932” data-Original-width =“ 1028”高度=“ 580” SRC = “ https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsejd7v86xym18_i3s0aemjiiyxajeiyxajeiyxajeiyxajeiyxajeiyxajeiyicq5kkkreicq5vkk3qnwtr96hkvssobsosobso qurin0 qurin0 qrin0f24Qrin0f23jprintimtt23jprintimtimtt3jprin 9UCV42RZXZ7CRKR21NUCR0ZWALKSUX9FXIBJWVMLQGB19Y5J8J8J8VT_ROKY4DB1SKUKKKKKKKBBC9FACCCCCCC6TC-XLUMHK5P65_UR与最新方法相比arxiv.org/abs/2212.04979&quot;>; videococa &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>; umt &lt;/a>; and umt &lt;/a>;和&lt;a a href =“ https：// arxiv。多个视频 - 文本检索（顶部）和视频字幕和视频QA（底部）基准测试的org/abs/2204.14198“>; flamingo &lt;/a>;）。我们还显示了与先前的最佳模型相比，绝对得分差异，以突出视频爆发的相对改进。我们在&lt;a href =“ https://www.microsoft.com/en-us/research/publication/msr-vtt-a-a-large-video-video-description-description-dataset-for-bridgor-bridging-video-video-video-video-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-vide-video- and-language/“>; masrvtt &lt;/a>;，&lt;a href=&quot;https://eric--xw.github.io/vatex-website/index.html&quot;>; vatex &lt;/a>;，and &lt;a href =” https://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;>; activitivitivitynet &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>; a href =“ https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-video-description-datasetr-dataset-for-brid-bridgor-bridging-video-andlanguage/ cap &lt;/a>;，&lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>; vatex-cap &lt;/a>;和&lt;a href =“ youcook2.eecs.umich.edu/&quot;>; youucook2 &lt;/a>;，&lt;a href=&quot;https://github.com/xudejing/video-question-andwering&quot;>; msrvttt-qa &lt;/a>; and &lt;a href=&quot;https://github.com/xudejing/video-question-swering&quot;>; msvd-qa &lt;/a>;和&lt;a href =“ https://arxiv.org/abs/cmp-lg /9406033“>; wups索引&lt;/a>;在&lt;a href=&quot;https://doc-doc.github.io/docs/nextqa.html&quot;>; next-qa &lt;/a>;。&lt;/td>; &lt;/td>; &lt;/td>; &lt;/tr >; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;video autoplay =“” loop =“”“ muted =”“ playSinline =”“ width =“ 100％”>; &lt;source src =“ https://github.com /garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4“ type =“ video/mp4”>; &lt;/source>; &lt;/source>; &lt;/video>; &lt;video autoplay =“ loop =”“ loop =”“ =“ 100％”>; &lt;源src =“ https://github.com/garyzhao/videoprism-blog/raw/main/main/spin_roller_skating.mp4” type =“ video/mp4”视频autoplay =“” loop =“” muted =“” playsInline =“” width =“ 100％”>; &lt;source src =“ https://github.com/garyzhao/garyzhao/garyoprism-blog/raw/raw/raw/main/making_ice_ice_ice_cream_cream_ski_ski_lifting.mp4 “ type =“ video/mp4”>; &lt;/source>; &lt;/video>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：汽车;边缘权利：auto;“>; &lt;tbody>; &lt;try>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;我们使用Videoprism与文本编码器一起显示定性结果第一行），并适用于视频质量质量图（第二行和第三行）的语言解码器。 /tbody>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;科学应用跨领域的科学家，包括伦理学，行为神经科学和生态学等领域。 caltech.edu/records/zrznw-w7386&quot;>; fly vs. fly &lt;/a>;，&lt;a href=&quot;https://data.calta.caltech.edu/records/s0vdx-0k302&quot;>; calms21 &lt;/a>; href =“ https://shirleymaxx.github.io/chimpact/”>; chimpact &lt;/a>;和&lt;a href=&quot;https://dirtmaxim.github.io/kabr/kabr//qubr/&quot;>; kabr &lt;/a>;。摄影不仅表现出色，而且实际上超过了专门为这些任务设计的模型。这表明诸如Videoprism之类的工具具有改变科学家在不同领域分析视频数据的潜力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/r29vz2xl/avvxsei3v-c36gwup8ckacvqfvqfvqfvvvvvvvvvvvvvvvvvvvvcco999999999999999999999999999999999999999999999999999. vjejxsqdb4zbgbkyazv-_i9qe5u7kus_z8qwlvqfzxx0jfelsdpfgj9v4qqhumwx_ekypm-vg7pdymxn0kj1 -s98izjl3u8cpvqoohyasuwxivt7m4_/s1200/image5.png“ style =” margin-left：auto; auto; &quot; height=&quot;397&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png&quot; width =“ 640”/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; videoprism优于各种科学基准的域专家。我们展示了绝对得分的差异，以突出视频的相对改进。我们报告了所有数据集的平均平均精度（MAP），但KABR除了使用类平均top-1精度的KABR。&lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;br />; &lt;h2>;结论&lt; /h2>; &lt;p>;使用视频，我们介绍了一个功能强大且通用的视频编码器，为通用视频理解设置了新的标准。通过我们的广泛评估，我们强调建立大量和多样化的预培训数据集以及创新的建模技术。视频爆发不仅始终超过强大的基线，而且它可以使其概括地位的独特能力可以很好地解决一系列现实世界应用程序。由于其潜在的广泛使用，我们致力于在我们的&lt;a href=&quot;http://ai.google/principles&quot;>; AI原则的指导下继续在该领域继续进行进一步负责任的研究&lt;/a>;。我们希望Videoprism在AI和视频分析的交集中为未来的突破铺平了道路，从而有助于实现跨科学发现，教育和医疗保健等领域的VIFM的潜力。 &lt;/p>; &lt;br />; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;此博客文章是代表所有录像带作者发表的：Long Zhao，Nitesh B. Gundavarapu，Liangzhe Yuan，Hao Zhou，Hao Zhou，Shen，Shen Yan，Jennifer J. Sun，Luke Friedman，Rui Qian，Tobias Weyand，Yue Zhao，Rachel Hornung，Florian Schroff，Ming-Hsuan Yang，David A. Ross，Huisheng Wang，Hartwig Adam，Mikhail Sirotenko，Ting Liuu和Boqing Gong Gong Gong Gong Gong Gong Gong Gong Gong Gong Gog Gog Gong Gong Gong Gong Gong Gong Gong Gong Gong 。我们衷心感谢David Hendon的产品管理工作，Alex Siegman，Ramya Ganeshan和Victor Gomes的计划和资源管理工作。 We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill马克，阿尔沙·纳格拉尼，卡罗琳·潘托法鲁，苏珊特·普拉卡什，科迪莉亚·施密特，布莱恩·塞伯德，布莱恩·塞耶伯德，莫伊塔巴·塞耶德斯塞尼，阿曼达·萨德勒，里夫·A·萨鲁斯，雷切尔·斯蒂格勒，瑞秋·斯蒂格勒，雷切尔·斯蒂格勒，保罗·沃伊格（Paul voigtlaender）支持和反馈极大地为这项工作做出了贡献。我们感谢Jay Yagnik，Rahul Sukthankar和Tomas Izo对这个项目的热情支持。最后，我们感谢Tom Small，Jennifer J. Sun，Hao Zhou，Nitesh B. Gundavarapu，Luke Friedman和Mikhail Sirotenko在制作此博客文章方面提供了巨大帮助。&lt;/em>; &lt;/em>; &lt;/p>; &lt;p>; &lt;/p &lt;/p >; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/1695264277638670894/comments/comments/default/default” &lt;link href =“ http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html#comment-form” “/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/default/1695264264277638670894“ ：//www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894“ 2024/02/videoprism-foundational-visual-encoder.html“ rel =“替代” title =“ videoprism：视频理解的基本视觉编码器”类型=“ text/html”/>; &lt;after>; &lt;unce>; /name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161&lt;/uri>; /schemas.google.com/g/2005#thumbnail“ src =” https://img1.blogblog.com/img/b16-nounded.gif.gif“ &lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif&quot; width=&quot;72&quot; xmlns:media =“ http://search.yahoo.com/mrss/”>; &lt;/媒体：thumbnail>; &lt;thr：thr>; thr>; thr>; 0 &lt;/thr：total>; &lt;/entry>; &lt;/entry>; &lt;enter>; &lt;entry>; &lt;id>; &lt;id>; tag：blogger.com， 1999：Blog-84749263145202626.POST-434323550909091741 &lt;/id>; &lt;/id>; &lt;/id>; &lt;出版>; 2024-02-212：15：00.000-08：00.000-08：00 &lt;/00 &lt;/00 &lt;/00 &lt;/00 /更新>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ dindinial privacy”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http://www.blogger.com/atom /ns＃“ term =“ gboard”>; &lt;/category>; &lt;类别scheme =“ http://www.blogger.com/atom/ns#” term =“ on-device Learning”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“负责人”>; &lt;/category>; &lt;title type =“ text”>;私人培训的私人培训进展>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由研究科学家Zheng Xu和软件工程师Yanxiang Zhang发布，Google &lt;/span>; &lt;/span>; &lt;img src =“ https：// bloggerger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>;语言模型（LMS）受过训练可以预测下一个单词给定输入文本的文本是许多应用程序的关键技术[&lt;a href =“ https://blog..google/technology/technology/ai/google-palm-palm-palm-palm-palm-palm-palm-palm-palm-palm-pal-palm-2----- ai-large-language-model/“>; 1 &lt;/a>;，&lt;a href=&quot;https://blog.google/technology/technology/google-google-gemini-ai/&quot;>; 2 &lt;/a>;]。在&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.android.inputmethod.latin＆amp; amp;hl= en_us＆gl＆gl=us&quot;>; gboard &lt;/a>;用于通过支持&lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;下一个单词预测&lt;/a>;（nwp），&lt;a href =“ https：// https：// https：// https：// https：// support.google.com/gboard/answer/7068415&quot;>; smart compose &lt;/a>;，&lt;a href=&quot;https://support.google.com/gboard/gboard/answer/nanswer/7068415&quot;>;智能完成&lt;/a>; a href =“ https://support.google.com/gboard/answer/7068415”>;建议要键入&lt;/a>; &lt;span style =“ text-decoration：useverline;”>;，&lt;/span>; and &lt;a href=&quot;https://support.google.com/gboard/gboard/answer/7068415&quot;>;验证一个>;。在用户设备而不是企业服务器上部署模型具有较低的延迟和模型使用范围更好的隐私。直接从用户数据培训设备模型的同时，有效地改善了诸如NWP和&lt;a href =的应用程序的实用性性能， >;智能文本选择&lt;/a>;，保护用户数据对模型培训的隐私很重要。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ； -Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/image45.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= &quot;1600&quot; data-original-width=&quot;1996&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif&quot; />;&lt; /a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; gboard特征由设备语言模型提供动力。&lt;/td>; &lt;/td>; &lt;/ TR>; &lt;/tbody>; &lt;/table>; &lt;p>;在此博客中，我们讨论了多年的研究如何为Gboard LMS的私人培训提供动力，因为概念证明的开发&lt;a href =&#39;https：// blog.research.google/2017/04/federated-learning-collaborative.html&quot;>; federated学习&lt;/a>;（fl）（fl）（fl）和正式&lt;a a href =“ https：//blog.rog.rog.rog.reasearch.google/2022/02/2022/02 /federated-learning-with-formal.html&quot;>; differential隐私&lt;/a>;（dp）在2022年保证。&lt;a href =“ https://blog.research.google/2017/04/federated-learning-collaborative。 html“>; fl &lt;/a>;使手机可以在设备上保留所有培训数据的同时协作学习模型，以及&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/differential_privacy&quot;>; dp &lt;/a >;提供了可量化的数据匿名化度量。正式地，DP通常以（&lt;em>;ε&lt;/em>;，Δ&lt;/em>;）为特征，其值较小，代表更强的保证。机器学习（ML）模型被认为具有&lt;a href=&quot;https://blog.reasearch.google/2023/05/making-making-making-models-differentially-private.html&quot;>;合理的DP保证强的DP保证ε= 1 &lt;/a>;当&lt;em>;Δ&lt;/em>;很小时。 &lt;/p>; &lt;p>;截至今天，Gboard中的所有NWP神经网络LMS都经过FL培训，并具有正式的DP保证，并且未来对用户数据培训的Gboard LMS都需要DP。这30多个Gboard On Device LMS以7种以上的语言和15个国家/地区启动，并满足（&lt;em>;ɛ&lt;/em>;，&lt;em>;Δ&lt;/em>;） - 小型&lt;em>;Δ&lt;&lt;em>;Δ&lt;&lt;em>;Δ&lt;&lt;em>;Δ&lt;/em>;） /em>; of 10 &lt;sup>; -10 &lt;/sup>;和0.994至13.69之间。据我们所知，这是在Google或任何地方的生产中最大的已知部署，也是第一次强大的DP保证&lt;em>;ɛ&lt;/em>;＆lt;直接在用户数据训练的模型中宣布了1个。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; ：//arxiv.org/abs/2306.14793“>; Gboard中的私人联盟学习&lt;/a>;”，我们讨论了如何与众不同&lt;a href =“ https://queue.acm.org/detail.cfm？id = 3501293” >;隐私原则&lt;/a>;当前反映在生产模型中，包括：&lt;/p>; &lt;ul>; &lt;li>; &lt;em>;透明度和用户控制&lt;/em>;：我们提供披露所使用的数据，什么目的用于在各种渠道中使用，以及Gboard用户如何轻松&lt;a href=&quot;https://support.google.com/gboard/gboard/answer/12373137&quot;>;配置&lt;/a>;楷模。 &lt;/li>; &lt;li>; &lt;em>;数据最小化&lt;/em>;：fl立即聚集了重点的更新，以改善特定模型。 &lt;a href=&quot;https://eprint.iacr.org/2017/281.pdf&quot;>;安全的聚合&lt;/a>;（secagg）是一种加密方法，可以进一步保证只能访问近世更新的汇总结果。 &lt;/li>; &lt;li>; &lt;em>;数据匿名&lt;/em>;：DP由服务器应用，以防止模型记住单个用户的培训数据中的唯一信息。 &lt;/li>; &lt;li>; &lt;em>;可审核性和可验证性&lt;/em>;：我们以开源代码公开了关键算法方法和隐私会计（&lt;a href =” https://github.com/tensorflow/ Federated/blob/main/tensorflow_federated/python/gentregators/dionalial_privacy.py“>; tff聚合器&lt;/a>;，&lt;a href =” /tree_aggregation_query.py &quot;>; tfp dpquery &lt;/a>;，&lt;a href=&quot;https://github.com/google-google-research/federated/federated/blob/blob/master/dp_ftrl/dp_ftrl/blogptrl/blogpost_supplemental_privacy_privacy_acccotting.privacy_acccounting.ipcourt.ipcourt.ipcourt.ip courceTing.ip courceTing &lt;//dpicting &lt;/dpicting &lt;/a>; dp>;和&lt;a href=&quot;https://github.com/google/federated-compute&quot;>; fl System &lt;/a>;）。 &lt;/li>; &lt;/ul>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;近年来，佛罗里达州的简短历史&lt;/h3>; &lt;p>;培训&lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>; Gboard gboard on-evice on-evice lms &lt;/a>;的默认方法&lt;/a>;在2020年，一种&lt;a href=&quot;https://arxiv.org/abs/1710.06963&quot;>;剪辑和添加噪声&lt;/a>;到模型更新中的DP机制用于&lt;href =“ https：// arxiv。 org/abs/2009.10031“>;防止记忆&lt;/a>;用于训练西班牙西班牙LM，该LM满足有限的DP保证（&lt;a href =“ https://blog.reasearch.google/2023/05/05/making-making-making-making-making-ml---making-ml----模型 - 二级联。html“>; tier 3 &lt;/a>; &lt;/a>;“ &lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;如何dp-fy ml” &lt;/a>;指南中所述。在2022年，借助&lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>; dp-follow-the-the-regularized-leader（dp-ftrl）算法&lt;/a>;，西班牙LM变成第一个直接对使用&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-with-formal.html &quot;>;正式DP保证（ε=ε= 8.9，δ= 10 &lt;sup>; -10 &lt;/sup>;） -  dp &lt;/a>;（相当于报告的&lt;em>; &lt;a href =“ https://blog.creachearch.google/2022/02/federated--学习 -  with-formal.html“>;ρ= 0.81 &lt;/a>; &lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1605.02065&quot;>;零concentrated-differential-differential-privacy &lt;/a>;） ，因此满足&lt;a href=&quot;https://blog.research.google/2023/05/making-making-ml-models-differentially-private.html&quot;>;合理的隐私保证保证：//blog.research.google/2023/05/making-making-ml-models-differenly-private.html“>; tier 2 &lt;/a>;）。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;默认情况下，在“ &lt;a a href =”中，默认情况下，在联邦学习&lt;/h2>; &lt;p>;中划分&lt;/h2>; &lt;p>; https://arxiv.org/abs/2305.18465&quot;>; federated学习具有差异隐私的Gboard语言模型&lt;/a>;”，我们宣布，Gboard中的所有NWP神经网络LMS均具有DP保证，以及Gboard LMS LMS LMS LMS LMS LMS的所有未来启动经过用户数据培训需要DP保证。通过应用以下实践：&lt;/p>; &lt;ul>; &lt;li>;在FL中启用DP：&lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;>;多语言&lt;/a >; &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>; c4 &lt;/a>;数据集。 &lt;/li>; &lt;li>;通过公共数据集上的仿真实验，找到一个允许高实用程序的DP噪声与信号比率。增加对一轮模型更新贡献的客户数量可改善隐私，同时保持噪声比以确定良好的效用，直到达到DP目标的点，或者系统允许的最大值和人口大小。 &lt;/li>; &lt;li>;配置参数以限制每个客户可以根据计算预算和估计的人群&lt;a href =&#39;https://arxiv.org/abs/1902.0101046，限制每个客户可以贡献的频率（例如，每隔几天一次） “>; FL系统&lt;/a>;。 &lt;/li>; &lt;li>;运行&lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>; dp-ftrl &lt;/a>;通过&lt;a href选择的每设备更新量的限制训练。 =“ https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e5e2b3a90e103368b6b6”>;自适应剪接&lt;/a>;或根据经验进行固定。 &lt;/li>; &lt;/ul>; &lt;p>;可以通过采用&lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-differential-privacy-for.html&quot;>;进步来进一步应用Secagg在改善量表和灵敏度的计算和通信方面&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N /S1600/image3.gif“ style =”保证金 - 左：自动右右：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 1000” data-original-width =“ 1600” =” EV-_2MULO5ZLAWNPCEMDGDNOVWP4M8T5QCZMPTUINPOTRY1WMXMTSASPSASPNPMLVOKQKQKLOKQKLONWYFMJF0TXBHMC-DKPI-DKPI-O7T4FBN8N8-N8-NN8-N-N-N-N-N-N-N-N-N-N-N/S16000/S16000/S16000/IMPAIM3.GIF3.GIF3.GIF3.GIF” tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;具有不同隐私和（secagg）的联合学习>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;报告DP保证&lt;/h3>; &lt;p>;在下面的barplot中可视化启动的Gboard NWP LMS的DP保证。 &lt;em>; x &lt;/em>;轴显示了通过语言锁定标记的LMS，并在相应的种群上进行了培训；当&lt;em>; y &lt;/em>;轴显示&lt;em>;ε&lt;/em>;值时，&lt;em>;Δ&lt;/em>;固定在&lt;&lt;&lt;em>;Δ a href =“ https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf”>;（ε，δ）-dpp &lt;/a>;（较低）。这些模型的效用要么明显优于生产中以前的非神经模型，要么与A/B测试期间用户相互作用指标测量的先前没有DP的LMS相当。例如，通过采用最佳实践，西班牙西班牙模型的DP保证从&lt;em>; &lt;a href =“ https://blog.research.google/2022/02/federated-learning-formal .html“>;ε= 8.9 &lt;/a>; &lt;/em>; to &lt;em>;ε&lt;/em>; = 5.37。 Secagg还用于培训西班牙西班牙模式和美国英语模型。 DP保证的更多详细信息在&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;附录&lt;/a>;之后2023/05/make-ml-models-divertially-private.html“>;指南概述&lt;/a>; &lt;/a>;在“ &lt;a href=&quot;https://arxiv.org/abs/abs/2303.00654&quot;>; /a>;”。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;对更强的dp保证&lt;/h2>; &lt;p>; &lt;em>;ε&lt;/em>; 〜〜 10次​​发射的LMS的10 dp保证已被视为&lt;a href=&quot;https://blog.research.google/2023/05/making-making-mod-models-models-diferentially-private.html&quot;>;合理的ML模型&lt;/a>;实际上，尽管Gboard中DP FL的旅程继续在保护数据隐私的同时改善用户打字体验。我们很高兴地宣布，首次对巴西葡萄牙的生产LMS和拉丁美洲的西班牙人进行了培训和推出，并以DP保证为&lt;em>;ε&lt;/em>;≤1，满足&lt;a a href =&#39;&#39; https://blog.research.google/2023/05/making-making-ml-models-differenly-private.html&quot;>; tier 1强隐私保证&lt;/a>;。具体而言，（&lt;em>;ε&lt;/em>; = 0.994，&lt;em>;Δ&lt;/em>; = 10 &lt;sup>; -10 &lt;/sup>;） - 通过运行高级&lt;a href =“ HTTPS”来实现DP保证：//arxiv.org/abs/2306.08153“>;矩阵分解dp-ftrl &lt;/a>;（MF-DP-FTRL）算法，其中12,000多个设备参与每个培训的服务器模型更新的每个训练回合，大于&lt;a href = &lt;a href = &lt;a href = “ https://arxiv.org/abs/2305.18465&quot;>; 6500+设备设置&lt;/a>;，以及一项精心配置的政策，将每个客户限制为最多在14天内的2000年培训中最多参与两次巴西大量葡萄牙用户人口。使用类似的环境，ES-US西班牙LM接受了在拉丁美洲的多个国家的大量人群中进行培训（&lt;em>;ε&lt;/em>; = 0.994，&lt;em>;Δ&lt;/em>; = 10 &lt;sup>; -10 &lt;/sup>;） -  DP。 &lt;em>;ε&lt;/em>;≤1ES-US模型可显着改善许多国家的效用，并在哥伦比亚，厄瓜多尔，危地马拉，墨西哥和委内瑞拉推出。对于西班牙较小的人口，ES-ES LM的DP保证从&lt;em>; &lt;em>; &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;ε= 5.37 &lt;/a>; &lt;/a>; &lt;/em>;用&lt;a href = &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>; dp-ftrl &lt;/a>;用&lt;a href =“ https：// arxiv） .org/abs/2306.08153“>; mf-dp-ftrl &lt;/a>; &lt;/a>;而没有增加每回合的设备数量。更多技术细节在&lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/federated/blob/master/master/master/master/mf_dpftrl_matrices/privacy_acccounting.ipynb&quot;>; for colabab &lt;/a>;会计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleuserercontent.com/img/r29vz2xl/avvz2xl/avvxsegp1ynoabd8iroisqdx-ohq-a8pudh2v1 fr7btr7btr.7btr.7btrcr76-tuue86-tuue86-tuue86-tuue.2 4NYBM_VN6BPRMFD_AHNHYKGJTLD7OCKAL6MHDRXCSA-M6RF3VM7KZQ5HXFDPBW9HK7BBSQU8EV4B.QU8EV4B5QAN3HW4B1BB1YXIKJNOKJNOKFHRKEFHRKEF0HOKFHRKEF0HAKFHRKEF0HXGT9DBLU3BLU3YK/SIM1999999999999999999999999999999999999999年11999999999999999. 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption”样式=“ text-align：center;”>; dp保证Gboard NWP LMS（紫色条代表ε= 8.9的第一个ES-ES启动；青色棒代表了经过&lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>; MF-DP-FTRL &lt;/a>;的模型的隐私改进。 &lt;a href=&quot;https://blog.research.google/2023/05/making-making-mod-models-differentyly-private.html&quot;>; tiers &lt;/a>;来自“ &lt;a href =” https：// arxiv .org/abs/2303.00654“>;如何DP-FY ML &lt;/a>;”指南； En-us*和es-es*还接受了secagg的培训）。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/div>; &lt;h2>;讨论和下一步&lt;/h2>; &lt;p>;我们的经验表明，通过系统算法在客户参与方面可以在实践中实现DP，并且在人口的人口时，隐私和实用性都可以很强大型&lt;em>;和&lt;/em>;汇总了大量设备的贡献。 &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;使用公共数据&lt;/a>;，&lt;a href =“ &lt;a href =” https：// arxiv。 org/abs/2306.08153“>;新的MF-DP-FTRL算法&lt;/a>;，&lt;a href=&quot;https://github.com/google./google/differential-privacy&quot;>; anding Countering councting &lt;/a>;。使用这些技术，可能的DP保证是可能的，但仍然具有挑战性。积极研究经验隐私审计[&lt;a href=&quot;https://arxiv.org/abs/2302.03098&quot;>; 1 &lt;/a>;，&lt;a href=&quot;https://arxiv.org/arxiv.org/abs/2305.08846&quot;>; 2 2 &lt;/a>;]建议DP模型可能比最严重的DP保证所暗示的更私密。当我们不断推动算法的前沿时，应该优先考虑隐私 - 陈旧计算的哪个维度？ &lt;/p>; &lt;p>;我们正在积极研究ML的所有隐私方面，包括将DP-FTRL扩展到&lt;a href =“ https://blog.research.google/2023/03/distributed-differential-differential-privacy-for .html“>;分布式dp &lt;/a>;并改善&lt;a href=&quot;https://arxiv.org/abs/2306.14793&quot;>;可审核性和可核能&lt;/a>;。 &lt;a href=&quot;https://en.wikipedia.org/wiki/trusted_execution_environment&quot;>;值得信赖的执行环境&lt;/a>;为实质上增加模型大小而具有可验证的隐私权开辟了机会。最近的&lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;大型LMS中的突破&lt;/a>;（llms）激励我们&lt;a href =“ &lt;a href =” https：/// arxiv.org/abs/2305.12132&quot;>; rethink &lt;/a>;使用&lt;a href=&quot;https://arxiv.org/abs/2212.06470&quot;>;公共培训中的信息和LLMS之间的更多互动信息，设备LMS和Gboard生产。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;作者要感谢彼得·凯鲁兹（Peter Kairouz），布伦丹McMahan和Daniel Ramage在博客文章本身，Shaofeng Li和Tom Small上提供了早期反馈，以帮助动画人物，以及Google的团队，这些团队帮助了算法设计，基础设施实施和生产维护。下面的合作者直接为提出的结果做出了贡献：&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;研究与算法开发：Galen Andrew，Stanislav Chiknavaryan，Christopher A. Choquetter A. Choquette-Choo，Arun Ganesh，Arun Ganesh，Peter Kairouz，Ryan McKenna ，H。BrendanMcMahan，Jesse Rosenstock，Timon Van Overveldt，Keith Rush，Shuang Song，Thomas Steinke，Abhradeep Guha Thakurta，Om Thakkar和Yuanbo Zhang。 and leadership support: Mingqing Chen, Stefan Dierauf, Billy Dou, Hubert Eichner, Zachary Garrett, Jeremy Gillula, Jianpeng Hou, Hui Li, Xu Liu, Wenzhi Mao, Brett McLarnon, Mengchen Pei, Daniel Ramage, Swaroop Ramaswamy, Haicheng Sun, Andreas Terzis，Yun Wang，Shanshan Wu，Yu Xiao和Shumin Zhai。 rel =“回复” title =“ post注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/02/advances-indvances-indvances-in-private-private-training-fraining-for- 。 4343235509909091741“ rel =“ edit” type =“ application/atom+xml”/>; &lt;link href =“ http:/ http://www.blogger.com/feeds/8474926263314520262626/ “应用程序/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/02/advances-indvances-invances-in-praining-training-training-for.html”生产室内语言模型的私人培训“ type =” text/html“/>; &lt;aund>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/1209862651477775266161 &lt;/uri &lt;/uri &lt;/uri >; &lt;Email>; noreply@blogger.com &lt;/email>; &lt;gd：image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https：// img1。 blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:缩略图>; &lt;thr：thr>; 0 &lt;/thr：thr>; &lt;/entry>; &lt;/entry>; &lt;entry>; &lt;id>; tag：blogger.com，1999：blog-8474926331452026626.post-560593333333332999262611025 14T10：32：00.000-08：00 &lt;/publisted>; &lt;更新>; 2024-02-14T10：32：25.557-08：00 &lt;/updated>; &lt;category scheme =“ http://wwww.blogger.com/atom/ ns＃“ term =”深度学习“>; &lt;/category>; &lt;category scheme =” http://www.blogger.com/atom/ns#“ term =“ pertrised Learning”>; &lt;/category>; &lt;title>; &lt;title type type =&#39;文本“>;在概念上学习培训数据的重要性，Google研究&lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img/r29vz2xl/avvxsegueskww4yd6cftplarnv7owmsljyeemmsljyeemmsljyemmsljyemsljymslimjibsibmjibmjbmjbmjbmjbmj4qoiii qoiii ibsmj4qoiii ibsmj94qoiii z94 qoiiii ibmj94qoiii z94 qoiii z94qoiii z94 qoiii z94qoiii ibmj94qoiii z94q Xe5brvclmqqffrlbutoya5phieelq1azrhsiagacz-ov_jxamsfrde0ejotyrqpoxpoxpoxpox3xv5makvjfkp9xecx4t2colbiz8r2r2r2r25y5y5y5kritfg/s1600/note note distrans distrand distern note streate />; &lt;p>;我们周围世界的不断变化的本质对AI模型的发展构成了重大挑战。通常，对模型进行纵向数据培训，希望所使用的培训数据能准确地代表该模型将来可能收到的输入。更一般而言，默认假设所有培训数据都同样相关，通常会在实践中破裂。例如，下图显示了来自&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; clear &lt;/a>;非机构学习基准的图像年跨度（我们称为&lt;em>;慢速概念漂移&lt;/em>;），对对象分类模型提出了挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw -_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height= &quot;662&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png&quot; />;&lt; /a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;来自清除基准测试的示例图像。 （改编自Lin等人&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;。&lt;/a>;）&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/ttoby>; &lt;/table>; &lt;br>; &lt;br>; &lt;br>; &lt;br>; &lt;br>; &lt; p>;替代方法，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/online_machine_learning&quot;>;在线&lt;/a>;和&lt;a href =“ https：///wiki.continualai.org/the-wiki.org/the--the------------------持续的wiki/介绍到 - 访问学习”>;连续学习&lt;/a>;，反复更新一个模型，其中少量最近数据以保持其最新。这隐含优先考虑最近的数据，因为从过去的数据中学习的学习是通过后续更新逐渐消除的。但是，在现实世界中，不同类型的信息以不同的速度失去相关性，因此有两个关键问题：1）通过设计，它们专注于最新数据，并丢失了来自较旧数据的任何信号被删除。 2）随着时间的推移，数据实例衰减的贡献均匀&lt;em>;均匀&lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>; &lt;em>;不论数据内容。 &lt;/p>; &lt;p>;在我们最近的工作中，“ &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>; instance-conditional decay of非遗产学习&lt;/a>;”，我们建议为每个实例分配培训期间的重要性得分，以最大程度地提高未来数据模型性能。为此，我们采用了一种辅助模型，该模型使用培训实例及其年龄来产生这些分数。该模型与主要模型共同学习。我们解决了上述挑战，并在用于非组织学习的一系列基准数据集上取得了重大收益。例如，在&lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;最近的大规模基准&lt;/a>;用于非组织的学习（在10年内〜39m照片），我们出现在15％的相对准确性通过学习的培训数据重新加权。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;概念漂移的挑战&lt;/h2>; &lt;p>; &lt;/h2>; &lt;p>;获得对慢速概念的定量见解漂流，我们在&lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;最近的照片分类任务&lt;/a>;上构建了分类器&lt;/a>;，该任务大约是从社交媒体网站上获得的大约3900万张照片。我们比较了离线培训，该培训在所有训练数据中以随机顺序进行了多次迭代，并持续培训，这些培训在每个月的数据中以顺序（时间）顺序进行了多次迭代。我们在训练期间和随后的两个模型都冻结的时期均测量了模型的准确性，即，未对新数据进行进一步更新（如下所示）。在训练期结束时（左图，X轴= 0），两种方法都看到了相同数量的数据，但显示了较大的性能差距。这是由于&lt;a href=&quot;https://www.sciencedirect.com/science/article/article/arbs/pii/S0079742108605368&quot;>;在训练序列中以不受控制的方式减少。另一方面，忘记具有其优势 - 在测试期间（右图显示），持续训练的模型的降级速度远低于离线模型，因为它依赖于较旧的数据。两种模型在测试期间的衰减都在确认数据随着时间的推移确实在不断发展，并且两个模型的相关性越来越小。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png “ ImageAnchor =“ 1”样式=“边距 - 左：自动右：自动;”>; &lt;img border =“ 0” data-forginal-height =“ 616” data-Original-width =“ 1554” src = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>;在照片分类任务上比较离线和经过训练的模型。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/try>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;br>; &lt;br>; &lt; div style =“线路高：40％;”>; &lt;br>; &lt;br>; &lt;/div>; &lt;h2>;时间敏感培训数据&lt;/h2>; &lt;p>;我们设计一种方法，结合了离线学习的好处（灵活性有效重复所有可用数据）和持续学习（淡化旧数据的能力）以解决缓慢的概念漂移。我们以离线学习为基础，然后仔细控制过去数据的影响和优化目标，均旨在减少未来的模型衰减。 &lt;/p>; &lt;p>;假设我们希望训练模型，&lt;em>; m &lt;/em>;，&lt;em>; &lt;/em>;给定一些随时间收集的训练数据。我们建议还训练一个辅助模型，该模型根据其内容和年龄为每个点分配重量。该重量在&lt;em>; m &lt;/em>;的训练目标中从该数据点缩放了贡献。权重的目的是在未来数据上提高&lt;em>; m &lt;/em>;的性能。 &lt;/p>; &lt;p>; in &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;我们的工作&lt;/a>;，我们描述了如何可以&lt;em>; meta-learned，&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/ em>; ie，以帮助学习模型&lt;em>; m &lt;/em>;本身的方式与&lt;em>; m &lt;/em>;一起学习。助手模型的关键设计选择是我们以阶级的方式分开了与年龄相关的贡献。具体而言，我们通过结合来自多个不同固定时间表的衰减时尺度的贡献来设置重量，并将给定实例的大约“分配”到其最合适的时间表上。我们在实验中发现，这种形式的助手模型的表现优于我们考虑的许多其他替代方案，从不受约束的关节函数到单个衰减的时间尺度（指数或线性），由于其简单性和表现性的结合。可以在&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>; paper &lt;/a>;中找到完整的详细信息。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;实例重量评分&lt;/h2>; &lt;p>;下面的顶部图表明，我们学到的助手模型确实上升了 - 在&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;&quot;>;清晰的对象识别挑战挑战&lt;/a>;中&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; &lt;a href=&quot;https://arxiv.org/abs/a>;中，重量更现代的对象&lt;/a>;;外观较旧的物体相应地加权。在仔细检查（下图下图，基于梯度的&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;特征重要性&lt;/a>;评估），我们看到助手模型集中在内部的主要对象上图像，而不是可能与实例年龄相关的背景特征。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 499” data-Original-width =“ 1999” src =“ src =” https ：//blogger.googleusercontent.com/img/b/r29vz2xl/avvxseggqimnpfiw7s3jet9qoxqom1KT8VIAHIHIHIHAPUSLJCAXYBAXYBLJCAXYBLJCCAXYBLJCCAXYLJCCHENN7LZHNN7LN7J6QTIZ9-QKKWBJBJBJBJBERSQLBERSQLBERMENMENS 6evzoo6Hygr5OxImp5vo9zuncf1q3bpvau94Hm9d71xwogrqm9c8lj6ixrb69w_jjneqw5jgcg_u6zw2j/s16000/s16000/s16000/image1.png -caption“ style =” text-align：center;“>; &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; clear &lt;/a>;基准（cameral＆amp;计算机类别）分别由我们的助手模型分别分配了最高和最低权重。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;table align =“ center” cellpadding =“ 0” CellSpacing =“ 0 “ class =“ tr-caption-container”样式=“保证金左：自动：自动：”>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center; center;”>; &lt;a href = “ https://blogger.googleusercontent.com/img/r29vz2xl/avvxsehikcafyxnrhjukwv3kjofmjk_v9wsplzfmyaya tzccccodbn-tzccodbncncncncncncncnuolozogf9njygqp_twzczczczczczczcsbmmmsbrmmsbmmmbsbrmmsbrmmsbmmbsbrmmbnsbldhsmbrnhsmbrnhmnhm a g.am n.pam g.am a兰比克5AF6OTQ_OWVK-XOWGKADCIFYZ8MHE-GOVEZ56QSSSIPKXDDUNMCA0ORNF_KGW8PH0UZCI8UBCQDBHS0J4NQ5HB5HB5J7E/S1999/S1999/S1999/s1999/image.png5.png“ ImageAnChor” -right：auto;“>; &lt;img border =” 0“ data-eriginal-height =“ 339” data-eriginal-width =“ 1999” src =“ https：//blogger.googger.googleusercontent.com/img/img/img/img/b/r29vz2xxl /AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-对齐：中心;“>;“>;特征对助手模型”在示例图像上的特征重要性分析，来自&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; clear &lt;/a>;基准&lt;/td>; &lt;/td>; &lt;/td>; &lt;/tr >; &lt;/tbody>; &lt;/table>; &lt;br>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;/h2>; &lt;div style =“ line-height： 40％;“>; &lt;br>; &lt;/div>; &lt;h3>;在大规模数据上获取&lt;/h3>; &lt;p>;我们首先研究大型&lt;a href =“ https://arxiv.org/abs/ 2108.09020“>;照片分类任务&lt;/a>;（PCAT）在&lt;a href=&quot;https://arxiv.org/abs/1503.01817&quot;>; yfcc100m数据集培训和接下来的五年作为测试数据。我们的方法（如下所示）在无夹式基线（黑色）以及许多其他健壮的学习技术上大大改善了。有趣的是，我们的方法故意在遥远的过去（将来不太可能重复发生培训数据）中进行准确的折衷，以换取测试期间的明显改进。同样，根据需要，我们的方法在测试期间的降解少于其他基线。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png&quot; imageanchor=&quot;1&quot; style =“边距 - 左：自动;边缘权利：自动;”>; &lt;img border =“ 0” data-foriginal-height =“ 600” data-eriginal-width =“ 800” src =“ https：// 。 XJD20XP5OG5YOWR2SMYVQNDHHASUNT5IY_RM_SJANFASM4JT1PF_TCHYPHENHYPHENK8Y0MNI2JJI2JJI1ODWCSIH_7VAC7B/S16000/S16000/IMPAIM3.PNG 3.png &lt;/>; &lt;/a>;对齐：中心;“>;对PCAT数据集上的方法和相关基线的比较。&lt;/td>; &lt;/tr>; &lt;/try>; &lt;/tbody>; &lt;/table>; &lt;br>; &lt;br>; &lt;div style =“ line-height：40％;” >; &lt;br>; &lt;/div>; &lt;h3>;广泛的适用性&lt;/h3>; &lt;p>;我们在广泛的非组织学习挑战挑战数据集中验证了我们的发现（请参阅&lt;a href = &lt;a href =“ https：// arxiv .org/abs/2108.09020“>; 1 &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>; 2 &lt;/a>;，&lt;a href =“ /abs/2211.14238&quot;>; 3 &lt;/a>;，&lt;a href=&quot;https://proceedings.mlr.press/v206/awasthi23b/awasthi23b/awasthi23b.pdf&quot;>; 4 &lt;/a>; 4 &lt;/a>;有关详细信息） （照片，卫星图像，社交媒体文本，医疗记录，传感器读数，表格数据）和尺寸（从10k到39m的实例不等）。与每个数据集的最近发布的基准方法相比，我们报告了测试期间的显着收益（如下所示）。请注意，每个数据集的先前最著名方法可能有所不同。这些结果展示了我们方法的广泛适用性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s765/image7.png&quot; ImageAnChor =“ 1”样式=“边距左：自动;缘右右：auto;”>; &lt;img border =“ 0” data-Original-height =“ 552” data-Original-width =“ 765” src =“ src =” https://blogger.googleusercontent.com/img/b/r29vz2xl/avvxsehw95hiflfz4einddwi0-yxonjybmlt2yhp_ekzm8v5e1whpxetxet thpxet to B44_QSEEE5U4K_Z70Y_9SSSSSSSSSSS3IF8Y5XKMXKQYI5VZATAFWC7NV5MGVNW_YL8HA6N7-GUPGGCJI2QTGKKTCNQNQNQNQNQNQNQN2ON2ON1RUBT- “ TR-CAPTION”样式=“ Text-Align：Center;”>;我们方法在研究自然概念漂移的各种任务上的性能增益。我们报告的收益是每个数据集的先前最著名方法。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br>; &lt;br>; &lt;div style =“ line-height：40％;”>; &lt;br >; &lt;/div>; &lt;h3>;持续学习的扩展&lt;/h3>; &lt;p>;最后，我们考虑了我们的工作的有趣扩展。上面的工作描述了如何使用受持续学习启发的想法来扩展离线学习以处理概念漂移。但是，有时离线学习是不可行的 - 例如，如果可用的培训数据数量太大而无法维护或处理。我们通过在&lt;/em>;的上下文中应用时间重新加权&lt;em>;来调整我们的方法，以直接的方式进行持续学习。该提案仍然保留了持续学习的某些局限性，例如，模型更新仅在大多数数据上执行，并且所有优化决策（包括我们的重量重量）仅在该数据上做出。然而，我们的方法始终击败定期的持续学习以及照片分类基准上的其他各种持续学习算法（见下文）。由于我们的方法与这里许多基线相比的思想是互补的，因此与它们结合使用时，我们预计会有更大的收益。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png&quot; imageanchor =“ 1” style =“边距左：auto; margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 600” data-Original-width =“ 800” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>;与最新基线相比，我们的方法的结果适用于持续学习。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br>; &lt;br>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们通过结合先前方法的优势 - 离线学习，以解决数据漂移的挑战它有效地重用数据，并继续学习，重点是最新数据。我们希望我们的工作有助于改善实践中概念漂移的模型鲁棒性，并在解决缓慢的概念漂移的无处不在问题方面产生越来越多的兴趣和新想法。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;我们感谢迈克·莫泽尔（Mike Mozer）在早期的早期进行了许多有趣的讨论这项工作的阶段，以及在开发过程中非常有用的建议和反馈。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/56059333333329926261025/comments/默认值“ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/learning-importance-of-raining-ogning-importance-of-training -data.html＃comment-form“ rel =” reply =“ title =“ 0 comment” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feeds/847492626331452026262626/posts/ default/5605933033299261025&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;self&quot; type =“ application/application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/02/learning-importance-of-training-data.hta.html” rel =“替代”在概念漂移下学习培训数据的重要性“ type =” text/html“/>; &lt;uname>; &lt;名称>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri &lt;/uri &lt;/uri >; &lt;Email>; noreply@blogger.com &lt;/email>; &lt;gd：image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https：// img1。 blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ https：//blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr：thr>; 0 &lt;/thr：total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999年：Blog-8474926331452026626.POST-5933333365460125094774 &lt;/id>; 11：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-02-13T14：11：49.258-08：00 &lt;/updated>; &lt;category scheme =&#39; “ term =“差异隐私”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“ pancys ai”>; &lt;/category>; &lt;category>; &lt;category scheme =“ http：http：http：http：http：http：http：http：http：http： //www.blogger.com/atom/ns#“ term =“安全与隐私”>; &lt;/category>; &lt;title type =“ text”>; dp-auditorium：一个灵活的库库，用于审核差异隐私&lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;由MónicaRiberoDíaz发表，研究科学家，Google Research &lt;/span>; &lt;img src =“ https://blogger.googleusercontent.com/img/b/b/ R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/differential_privacy&quot;>;差异隐私&lt;/a>;（DP）是限制任何个人用户信息的随机机制的属性处理和分析数据。 DP提供了一种强大的解决方案，以解决对数据保护的日益关注，启用技术&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-learning-with-formal.html&quot;>;跨越&lt;/a>; &lt;a href=&quot;https://www.apple.com/privacy/docs/differential_privacy_overview.pdf&quot;>; Industries &lt;/a>;和政府应用程序（例如程序 - 策略/年十年级/十年/2020/计划管理/process/procutsion-discluse-avidence/dindialial-privacy.html“>;美国人口普查&lt;/a>;），而不会损害个人用户身份。随着其采用率的增加，重要的是要确定具有错误实施的机制的潜在风险。研究人员最近发现了私人机制的数学证据及其实施的错误。例如，&lt;a href=&quot;https://arxiv.org/pdf/1603.01699.pdf&quot;>;研究人员比较&lt;/a>;六个稀疏矢量技术（SVT）变化，发现六个实际上只遇到了六个的私密性保证。即使数学证明是正确的，实现该机制的代码也容易受到人为错误的影响。 &lt;/p>; &lt;a name =&#39;more&#39;>; &lt;/a>; &lt;p>;但是，实际和有效的DP审核主要是由于机制的固有随机性以及测试保证的概率性质。此外，存在一系列保证类型，（例如，&lt;a href=&quot;https://dl.acm.org/doi/10.1007/11681878_14&quot;>; pure dp &lt;/a>;，&lt;a href =“ /link.springer.com/chapter/10.1007/11761679_29&quot;>; approximate dp &lt;/a>;，&lt;a href=&quot;https://arxiv.org/arxiv.org/abs/1702.07476&quot;>; =“ https://arxiv.org/pdf/1603.01887.pdf”>;集中的DP &lt;/a>;），这种多样性有助于制定审计问题的复杂性。此外，考虑到提出的机制的数量，调试数学证明和代码库是一项棘手的任务。虽然在特定的机制假设下存在&lt;em>; Ad hoc &lt;/em>;测试技术，但很少有努力为测试DP机制开发可扩展的工具。 &lt;/p>; &lt;p>;到此为止，在“ &lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>; dp-auditorium：用于审核差异隐私的大型库&lt;/a>;”，我们引入&lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/main/main/python/dp_auditorium&quot;>;开源库库&lt;/a>;用于审核DP保证，只有Black-Box访问机制（即，不知道该机制的内部属性）。 DP-Auditorium在Python中实现，并提供了一个灵活的接口，允许贡献不断提高其测试功能。我们还引入了新的测试算法，这些算法对RényiDP，Pure DP和近似DP的功能空间进行了差异优化。我们证明，DP原告可以有效地识别DP保证违规行为，并建议哪些测试最适合在各种隐私保证下检测特定的错误。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; dp保证&lt;/h2>; &lt;p>; dp机制的输出是从一个绘制的样本概率分布（&lt;em>; m &lt;/em>;（&lt;em>; d &lt;/em>;））可满足数学属性，以确保用户数据的隐私。因此，DP保证与概率分布对之间的属性密切相关。如果由数据集&lt;em>; d &lt;/em>;确定的概率分布和一个相邻的数据集&lt;em>; d&#39;&lt;/em>;，则机制是私有的是&lt;em>; &lt;a href=&quot;https://en.wikipedia.org/wiki/computational_indistrinishability&quot;>;在给定的分歧度量下&lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/em>;。 &lt;/p>; &lt;p>;例如，经典&lt;a href=&quot;https://software.imdea.org/~federico/pubs/2013.icalp.pdf&quot;>;近似DP &lt;/a>;定义指出机制如果&lt;a href=&quot;https://arxiv.org/pdf/1508.00335.pdf&quot;>; hockey-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick-stick- &lt;/a>;在&lt;em>; m &lt;/em>;之间>;（&lt;em>; d&#39;&lt;/em>;），最多是&lt;em>;Δ&lt;/em>;。纯dp是近似dp的特殊实例，其中&lt;em>;δ= 0 &lt;/em>;。最后，一种机制被认为&lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;rényidp &lt;/a>;带有参数（&lt;em>; 𝛼 &lt;/em>;，&lt;em>; &lt;em>;ε）&lt;/em em）&lt;/em em） >;如果&lt;a href=&quot;https://en.wikipedia.org/wiki/r%C3%A9NYI_ENTROPY&quot;>;rényidivergence &lt;/a>; forder &lt;em>; 𝛼 &lt;/em>;，最多是&lt;em>; &lt;em>; &lt;em>; ε&lt;/em>;（其中&lt;em>;ε&lt;/em>;是一个小的正值）。在这三个定义中，&lt;em>;ε&lt;/em>;不是可互换的，而是直观地传达了相同的概念。 &lt;em>;ε&lt;/em>;的较大值意味着两个分布之间或更少的隐私之间的差异，因为两个分布更容易区分。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>; dp-auditorium &lt;/h2>; &lt;p>; dp-auditorium包括两个主要组件：属性测试仪和数据集查找器。物业测试人员从在特定数据集评估的机制中获取样本作为输入，旨在确定所提供数据集中违反隐私保证的行为。数据集查找器建议隐私保证可能失败的数据集。通过将这两个组件组合在一起，DP原告可以实现（1）对不同机制和隐私定义的自动测试以及（2）检测隐私机制中的错误。我们实施了各种私人和非私人机制，包括计算记录平均值和更复杂机制的简单机制，例如不同的SVT和&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/stochastic_gradastic_gradient_descent &quot;>;渐变下降&lt;/a>;机制变体。 &lt;/p>; &lt;p>; &lt;strong>;属性测试仪&lt;/strong>;确定是否存在拒绝两个概率分布之间给定差异的假设，即&lt;em>; p &lt;/em>;和&lt;em>; q &lt;/em>; ，由预先指定的预算限制，该预算由DP保证进行了测试。他们从&lt;em>; p &lt;/em>; q，&lt;/em>;中从样品中计算出一个下限，如果下限值超过预期差异，则拒绝该属性。如果结果确实有限，则无法提供保证。为了测试一系列隐私保证，DP-Auditorium介绍了三个新颖的测试人员：（1）Hockeystickpropertytester，（2）Rényipropertytester和（3）Mmdpropertytester。与其他方法不同，这些测试人员不依赖于测试分布的显式直方图近似。他们依靠曲棍球刺激性差异，rényiDivergence和&lt;a href=&quot;https://jmlr.csail.mit.mit.edu/papers/v13/gretton12a.html&quot;>;最大平均差异（（ MMD）可以通过优化功能空间来估计差异。作为基准，我们实施了&lt;a href=&quot;https://arxiv.org/abs/1806.06427&quot;>; Histogropropertytester &lt;/a>;，通常使用的近似DP测试仪。虽然我们的三个测试人员遵循类似的方法，但对于简洁起见，我们在这篇文章中专注于曲棍球史密斯果属。 &lt;/p>; &lt;p>;给定两个相邻数据集，&lt;em>; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;，hockeystickpropertytester找到了一个下限，&lt;i>; &lt;span style =“ bottom：9px;左：9px； d）&lt;/em>;和&lt;em>; m &lt;/em>;（&lt;em>; d&#39;&lt;/em>;），具有很高的可能性。曲棍球 - 粘性分歧强制执行两个分布&lt;em>; m &lt;/em>;（&lt;em>; d）&lt;/em>;和&lt;em>; m &lt;/em>;（&lt;em>; d&#39;&lt;/em>;）大约DP保证。因此，如果隐私保证声称曲棍球 - 粘性差异最多是&lt;em>;Δ&lt;/em>;，而&lt;i>; &lt;span style =”底部：9px; left：9px; 9px; 9px; position; tosis：相对; transfrom：scale（scale from：scale（ 4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp; >; &lt;em>;Δ&lt;/em>;，然后具有很高的差异，差异高于&lt;em>; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;上所承诺的差异，并且该机制无法满足给定的近似DP保证。下限&lt;i>; &lt;span style =“底部：9px;左：9px;位置：相对; Transfrom：scale（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp;被计算为曲棍球 - 粘性散射的变分配方的经验和可拖动的对应物（请参阅&lt;a href=&quot;https://arxiv.org/pdf/2307.05608.pdf&quot;>; &lt;a 。 &lt;i>; &lt;>; &lt;span style =“底部：9px;左：9px;位置：相对; Transfrom：scale（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>;＆nbsp;随着从机制中得出的样品数量增加，但是随着变化公式的简化而减小。我们平衡了这些因素，以确保&lt;i>; &lt;span style =“底部：9px;左：9px;位置：相对;转换：比例（4,0.5）;“>;^&lt;/span>;Δ&lt;/i>; &amp;nbsp;既准确又易于计算。 &lt;/p>; &lt;p>; &lt;strong>;数据集查找器&lt;/strong>;使用&lt;a href=&quot;https://arxiv.org/pdf/2207.13676.pdf&quot;>; black-box优化&lt;/a>;查找数据集&lt;em >; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;最大化&lt;i>; &lt;span style =“底部：9px; left：9px; position; tocation;相对; transfrom：scale（4,0.5）;“>;^>;^ &lt;/span>;Δ&lt;/i>;，差异值的下限&lt;em>;Δ&lt;/em>;。请注意，黑框优化技术是专门为设置设计的，在这些设置中，在目标函数可能是不切实际甚至不可能的情况下。这些优化技术在探索阶段和剥削阶段之间振荡，以估计目标函数的形状，并预测物镜可以具有最佳值的领域。相比之下，完整的探索算法，例如&lt;a href=&quot;https://en.wikipedia.org/wiki/hyperparameter_optimization#grid_search&quot;>;网格搜索方法&lt;/a>; em>; d &lt;/em>;和&lt;em>; d&#39;&lt;/em>;。 DP-Auditorium通过开放源的黑框优化库实现了不同的数据集查找器&lt;a href=&quot;https://github.com/google/google/vizier/vizier&quot;>; Vizier &lt;/a>;。 &lt;/p>; &lt;p>;在新机制上运行现有组件仅需要将机制定义为python函数，该函数采用了一系列数据&lt;em>; d &lt;/em>;和所需数量的样品&lt;em>; n &lt;/em >;通过在&lt;em>; d &lt;/em>;上计算的机制输出。此外，我们还为测试人员和数据集查找器提供灵活的包装器，使从业者可以实施自己的测试和数据集搜索算法。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>;具有不同输出空间的九种非私有机制。对于每个属性测试仪，我们使用&lt;em>;ε&lt;/em>;的不同值在固定数据集上重复测试，并报告每个测试仪标识隐私错误的次数。尽管没有测试仪始终胜过其他测试人员，但我们确定了以前技术（Histogroperpertytester）所遗漏的错误。请注意，组合植物的组织不适用于SVT机制。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：中心;”>; &lt;a href =“ anj4vp4swxoznpz4388x-iwisjqazxndagm8f4-hl5ghlags3aiuqhns-gnjfa_atjfa_at9lmamvitlrdep5ojhpra6yostjray6yost6yost66lz8zsif8zsif8zsif8wiwiwiwiw6u4pkn7/s7pkn7/pake255 “ style =”保证金左：自动右：自动;”>; &lt;img border =“ 0” data-original-height =“ 409” data-original-width =“ 785” blogger.googleusercontent.com/img/b/r29vz2xl/avvxsehllyauj1cew8xcqqunnmvggkz2bd5uhlzudlx3xvdn_tw4zbw4zbbdw4zbdw4 zbd55tci6zvvvvvvvvvvvvvvvppzjquzjjquznjquznjquznjquznj ozg azg izg azg iozzndpzizndpzizndpzizndpzizndpzizndpzizndpzizndpzizndpz -hl5ghlags3aiuqhns-gnjfa_at9lmamvitlrdep5ojhpra6oldjry6yost66lz8zsif8wiwiw6uhkfa4pkn7/s16000/s16000/s16000/image22.png字幕“ style =”文本align：中心;“>;每个属性测试仪的次数违反了经过测试的非私有机制的隐私行为。 NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Laplace_Mechanism&quot;>;Laplace&lt;/a>; and &lt;a href=&quot;https://en.wikipedia. org/wiki/addistil_noise_differential_privacy_mechanisms＃gaussian_mechanism“>;高斯&lt;/a>;计算平均值的机制&lt;a href=&quot;https://github.com/tensorflow/privacy/blob/blob/master/tensorflow_privacy/privacy/privacy/privacy/pptimizers/dp_optimizer_keras.keras.keras.keras.py&quot;>; DP梯度渐变discent algorithM &lt;/gorithm &lt;/a>;（DP-GD）私人数据上的损失功能。为了保留隐私，DP-GD采用剪裁机制来绑定&lt;a href=&quot;https://mathworld.wolfram.com/l2-norm.html&quot;>; l2-norm &lt;/a>;梯度的价值&lt;/a>; em>; g &lt;/em>;，然后增加高斯噪声。该实现错误地假设添加的噪声具有&lt;em>; g &lt;/em>;的比例，而实际上，量表为&lt;em>; sg &lt;/em>;，其中&lt;em>; s &lt;/em>;是一个正面的标量。这种差异导致近似的DP保证，仅适用于大于或等于1的值，我们评估了属性测试人员在检测此错误中的有效性Hockeystickpropertytester和RényiPropertytester在识别侵犯隐私，表现优于MMDPropertytester和Histogroperpertytester方面表现出色。值得注意的是，这些测试人员即使在&lt;em>; s &lt;/em>;的值高达0.6的值中也检测到错误。值得一提的是，&lt;em>; s &lt;/em>; = 0.5对应于a &lt;a href =” ib.py＃l445c1 -l446c1“>;常见错误&lt;/a>;在考虑隐私预算&lt;em>;ε&lt;/em>;的文献中涉及丢失两个因子。 DP原告成功捕获了此错误，如下所示。有关更多详细信息，请参见第5.6节&lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;在这里&lt;/a>;。 &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;“>; &lt;tbody>; &lt;trody>; &lt;try>; &lt;tr>; &lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg&quot; imageanchor=&quot; 1“样式=”保证金左：自动右：自动;”>; &lt;img border =“ 0” data-forminal-height =“ 332” data-Original-width =“ 836” /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>;估计的差异和测试阈值&lt;em>; s &lt;/em>;的不同值&lt;em>; s &lt;/em>;使用Histogroperpertytester（&lt;strong>;左>; &lt;/strong>;）和Hockeystickpropertytester（&lt;strong>;右&lt;/strong>;）。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” 0“” class =“ tr-caption-container”样式=“边距 - 左：auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s828/image20.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto;边缘权利：自动;“>; &lt;img border =“ 0” data-original-height =“ 333” data-eriginal-width =“ 828” src =“ https://blogger.googleusercontent.com/img/img/b/ R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s16000/image20.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;“>;用rényipropertytester（&lt;strong>; left &lt;/strong>;）和mmdpropertytester（&lt;strong>;右>; &lt;/strong>;右>; &lt;/strong>;） ）&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;p>;要测试数据集查找器，我们在找到违反隐私的情况下探索了探索的数据集数量。在少于10个数据集查找器的调用中，发现了与网格搜索相比，查找数据集的效率更高。纸&lt;/a>;。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; dp是数据保护最强大的框架之一。但是，正确实施DP机制可能具有挑战性，并且容易使用传统的单元测试方法来轻易检测到的错误。统一的测试框架可以帮助审计师，监管机构和学者确保私人机制确实是私人的。 &lt;/p>; &lt;p>; dp-auditorium是通过功能空间优化DP测试DP的一种新方法。我们的结果表明，这种基于功能的估计始终优于先前的黑盒访问测试仪。最后，我们证明这些基于函数的估计器与直方图估计相比，可以更好地发现隐私错误的发现率。由&lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/main/python/dp_auditorium&quot;>;开放源&lt;/a>; DP-Auditorium，我们旨在为端到 - 最终测试新的差异私有算法。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;确认&lt;/h2>; &lt;p>; &lt;em>;此处描述的工作是与AndrésMuñoz共同完成的麦地那，威廉·孔和乌马尔·赛义德。我们感谢Chris Dibak和Vadym Doroshenko为我们的图书馆提供了有益的工程支持和界面建议。&lt;/em>; &lt;/p>; &lt;/penter>; &lt;link href =“ http://blog.research.google/feeds/59333333333333333333333333333333333333333333333333333333333333333460125094774/comments /默认值“ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/dp-auditorium-flexible- Library-for.html＃comment-form“ rel =” reply =“ title =” 0注释“ type” type =“ text/html”/>; &lt;link href =“ http://www.blogger.com/feed.com/feeds/8474926331452026262626262626262626/posts /default/5933365460125094774&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;self “ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/02/dp-auditorium-flexible-library-library-for.html“ rel =” re =“替代” title = “ DP-ADITORIUM：一个灵活的库库” &lt;/uri>; &lt;email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https：/https：/ /img1.blogblog.com/img/b16-rounded.gif“ width =“ 16”>; &lt;/gd：image>; &lt;/rution>; &lt;媒体：thumbnail Height =“ 72” url =“ https：//blogger.googleusercorcercontent .com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss /&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3264694155710647310&lt;/id>; &lt;published>;2024-02-06T11:17:00.000-08:00&lt;/published>;&lt;updated>;2024-02-06T11:17:53.968-08:00&lt;/updated>;&lt;category scheme=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;Graph Mining&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category >;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns #&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Graph neural networks in TensorFlow&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx -I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s1600/TFGNN%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks 。 Discrete mathematics and computer science have a long history of formalizing such networks as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graphs&lt;/a>;&lt;/em>;, consisting of &lt;em>;nodes&lt;/em>; connected by &lt;em>;edges&lt;/em>; in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;Graph neural networks&lt;/a>;, or GNNs for short, have emerged as a powerful technique to leverage both the graph&#39;s connectivity (as in the older algorithms &lt;a href=&quot;http://perozzi.net/projects/deepwalk/&quot;>;DeepWalk&lt;/a>; and &lt;a href=&quot;https://snap.stanford.edu/node2vec/&quot;>;Node2Vec&lt;/a>;) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What&#39;s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph&#39;s &lt;em>;discrete&lt;/em>;, &lt;em>;relational&lt;/em>; information in a &lt;em>;continuous&lt;/em>; way so that it can be included naturally in another deep learning system. &lt;/p>; &lt;p>; We are excited to announce the release of &lt;a href=&quot;https://github.com/tensorflow/gnn&quot;>;TensorFlow GNN 1.0&lt;/a>; (TF-GNN), a production-tested library for building GNNs at large scales. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs, where types of objects and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types, and TF-GNN&#39;s heterogeneous focus makes it natural to represent them. &lt;/p>; &lt;p>; Inside TensorFlow, such graphs are represented by objects of type &lt;code>;tfgnn.GraphTensor&lt;/code>;. This is a composite tensor type (a collection of tensors in one Python class) accepted as a &lt;a href=&quot;https://en.wikipedia.org/wiki/First-class_citizen&quot;>;first-class citizen&lt;/a>; in &lt;code>;tf.data.Dataset&lt;/code>;, &lt;code>;tf.function&lt;/code>;, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level &lt;a href=&quot;https://www.tensorflow.org/guide/keras&quot;>;Keras API&lt;/a>;, or directly using the &lt;code>;tfgnn.GraphTensor&lt;/code>; primitive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;GNNs: Making predictions for an object in context&lt;/h2>; &lt;p>; For illustration, let&#39;s look at one typical application of TF-GNN: predicting a property of a certain type of node in a graph defined by cross-referencing tables of a huge database. For example, a citation database of Computer Science (CS) arXiv papers with one-to-many cites and many-to-one cited relationships where we would like to predict the subject area of each paper. &lt;/p>; &lt;p>; Like most neural networks, a GNN is trained on a dataset of many labeled examples (~millions), but each training step consists only of a much smaller batch of training examples (say, hundreds). To scale to millions, the GNN gets trained on a stream of reasonably small subgraphs from the underlying graph. Each subgraph contains enough of the original data to compute the GNN result for the labeled node at its center and train the model. This process — typically referred to as subgraph sampling — is extremely consequential for GNN training. Most existing tooling accomplishes sampling in a batch way, producing static subgraphs for training. TF-GNN provides tooling to improve on this by sampling dynamically and interactively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Pictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; TF-GNN 1.0 debuts a flexible Python API to configure dynamic or batch subgraph sampling at all relevant scales: interactively in a Colab notebook (like &lt;a href=&quot;https://colab.research.google.com/github /tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;this one&lt;/a>;), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by &lt;a href =&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md&quot;>;in-memory&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md&quot;>;beam-based&lt;/a>; sampling, respectively. &lt;/p>; &lt;p>; On those same sampled subgraphs, the GNN&#39;s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node&#39;s neighborhood. One classical approach is &lt;a href=&quot;https://research.google/pubs/neural-message-passing-for-quantum-chemistry/&quot;>;message-passing neural networks&lt;/a>;. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After &lt;em>;n&lt;/em>; rounds, the hidden state of the root node reflects the aggregate information from all nodes within &lt;em>;n&lt;/em>; edges (pictured below for &lt;em>;n&lt;/em>; = 2) 。 The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;511&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The training setup is completed by placing an output layer on top of the GNN&#39;s hidden state for the labeled nodes, computing the &lt;em>;loss &lt;/em>;(to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training. &lt;/p>; &lt;p>; Beyond supervised training (ie, minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (ie, without labels). This lets us compute a &lt;em>;continuous&lt;/em>; representation (or &lt;em>;embedding&lt;/em>;) of the &lt;em>;discrete&lt;/em>; graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building GNN architectures&lt;/h2>; &lt;p>; The TF-GNN library supports building and training GNNs at various levels of abstraction. &lt;/p>; &lt;p>; At the highest level, users can take any of the predefined models bundled with the library that are expressed in Keras layers. Besides a small collection of models from the research literature, TF-GNN comes with a highly configurable model template that provides a curated selection of modeling choices that we have found to provide strong baselines on many of our in-house problems. The templates implement GNN layers; users need only to initialize the Keras layers. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;865&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s16000/TFGNN%20code1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; At the lowest level, users can write a GNN model from scratch in terms of primitives for passing data around the graph, such as broadcasting data from a node to all its outgoing edges or pooling data into a node from all its incoming edges (eg, computing the sum of incoming messages). TF-GNN&#39;s graph data model treats nodes, edges and whole input graphs equally when it comes to features or hidden states, making it straightforward to express not only node-centric models like the MPNN discussed above but also more general forms of &lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;GraphNets&lt;/a>;. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md&quot;>;user guide&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models&quot;>;model collection&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training orchestration&lt;/h2>; &lt;p>; While advanced users are free to do custom model training, the &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md&quot;>;TF-GNN Runner&lt;/a>; also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s1400/TFGNN%20code2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;508&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s16000/TFGNN%20code2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The Runner provides ready-to-use solutions for ML pains like distributed training and &lt;code>;tfgnn.GraphTensor&lt;/code>; padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;392&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s16000/TFGNN%20code3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Additionally, the TF-GNN Runner also includes an implementation of &lt;a href=&quot;https://www.tensorflow.org/tutorials/interpretability/integrated_gradients&quot;>;integrated gradients&lt;/a>; for use in model attribution. Integrated gradients output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In short, we hope TF-GNN will be useful to advance the application of GNNs in TensorFlow at scale and fuel further innovation in the field. If you&#39;re curious to find out more, please try our &lt;a href=&quot;https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;Colab demo&lt;/a>; with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/overview.md&quot;>;user guides and Colabs&lt;/a>;, or take a look at our &lt;a href=&quot;https://arxiv.org/abs/2207.03522&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The TF-GNN release 1.0 was developed by a collaboration between Google Research: Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle, Google Core ML: Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe, and Google DeepMind:&lt;strong>; &lt;/strong>;Alvaro Sanchez-Gonzalez and Lisa Wang.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3264694155710647310/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html&quot; rel=&quot;alternate&quot; title=&quot;Graph neural networks in TensorFlow&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6956767920612914706&lt;/id>;&lt;published>;2024-02-02T11:07:00.000-08:00&lt;/published>;&lt;updated>;2024-02-07T16:05:00.722-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A decoder-only foundation model for time-series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Yichen Zhou, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time-series&lt;/a>; forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural科学。 In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&quot;>;improving demand forecasting accuracy&lt;/a>; can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (eg, DL models performed well in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0169207021001874&quot;>;M5 competition&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;translation&lt;/a>;, &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/&quot;>;retrieval-augmented generation&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligent_code_completion&quot;>;code completion&lt;/a>;. These models are trained on massive amounts of &lt;em>;textual &lt;/em>;data derived from a variety of sources like &lt;a href=&quot;https://commoncrawl.org/&quot;>;common crawl&lt;/a>; and open-source code that allows them to identify patterns in languages. This makes them very powerful &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-shot_learning&quot;>;zero-shot&lt;/a>; tools; for instance, &lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;when paired with retrieval&lt;/a>;, they can answer questions about and summarize current events 。 &lt;/p>; &lt;p>; Despite DL-based forecasters largely &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;outperforming&lt;/a>; traditional methods and progress being made in &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;reducing training and inference costs&lt;/a>;, they face challenges: most DL architectures require &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;long and involved training and validation cycles&lt;/a>; before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like &lt;a href=&quot;https://en.wikipedia.org/wiki/Customer_demand_planning&quot;>;retail demand planning&lt;/a>;. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/pdf/2310.10688.pdf&quot;>;A decoder-only foundation model for time-series forecasting&lt;/a>;”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>;Google Cloud Vertex AI&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A decoder-only foundation model for time-series forecasting&lt;/h2>; &lt;p>; LLMs are usually trained in a &lt;a href=&quot;https://arxiv.org/pdf/1801.10198.pdf&quot;>;decoder-only&lt;/a>; fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformer&lt;/a>; layers that produce an output corresponding to each input token (it cannot attend to future tokens) 。 Finally, the output corresponding to the &lt;em>;i&lt;/em>;-th token summarizes all the information from previous tokens and predicts the (&lt;em>;i&lt;/em>;+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”. &lt;/p>; &lt;p>; A foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining数据集。 Similar to LLMs, we use stacked transformer layers (self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward&lt;/a>; layers) as the main building blocks for the TimesFM model 。 In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;long-horizon forecasting work&lt;/a>;. The task then is to forecast the (&lt;em>;i&lt;/em>;+1)-th patch of time-points given the &lt;em>;i&lt;/em>;-th output at the end of the stacked transformer layers. &lt;/p>; &lt;p>; However, there are several key differences from language models. Firstly, we need a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with &lt;a href=&quot;https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/&quot;>;positional encodings&lt;/a>; (体育）。 For that, we use a residual block similar to our prior work in &lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;long-horizon forecasting&lt;/a>;. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, ie, the output patch length can be larger than the input patch length. &lt;/p>; &lt;p>; Consider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0 “ class =“ tr-caption-container”样式=“保证金左：自动：自动：”>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center; center;”>; &lt;a href = &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border= &quot;0&quot; data-original-height=&quot;674&quot; data-original-width=&quot;1084&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3. jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TimesFM architecture.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Pretraining data&lt;/h2>; &lt;p>; Just like LLMs get better with more tokens, TimesFM requires a large volume of legitimate time series data to learn and improve. We have spent a great amount of time creating and assessing our training datasets, and the following is what we have found works best: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Synthetic data helps with the basics.&lt;/strong>; Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting. &lt;/p>;&lt;/div>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Real-world data adds real-world flavor.&lt;/strong>; We comb through available public time series datasets, and selectively put together a large corpus of 100 billion time-points. Among these datasets there are &lt;a href=&quot;https://trends.google.com/trends/&quot;>;Google Trends&lt;/a>; and &lt;a href=&quot;https://meta.wikimedia.org/wiki/Research:Page_view&quot;>;Wikipedia Pageviews&lt;/a>;, which track what people are interested in, and that nicely mirrors trends and patterns in many other real-world time series. This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training. &lt;/p>;&lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Zero-shot evaluation results&lt;/h2>; &lt;p>; We evaluate TimesFM zero-shot on data not seen during training using popular time-series benchmarks. We observe that TimesFM performs better than most statistical methods like &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot;>;ARIMA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_smoothing&quot;>;ETS&lt;/a>; and can match or outperform powerful DL models like &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; that have been &lt;em>;explicitly trained&lt;/em>; on the target time-series. &lt;/p>; &lt;p>; We used the &lt;a href=&quot;https://huggingface.co/datasets/monash_tsf&quot;>;Monash Forecasting Archive&lt;/a>; to evaluate TimesFM&#39;s out-of-the-box performance. This archive contains tens of thousands of time-series from various domains like traffic, weather, and demand forecasting covering frequencies ranging from few minutes to yearly data. Following existing literature, we inspect the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE) &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;appropriately scaled&lt;/a>; so that it can be averaged across the datasets. We see that zero-shot (ZS) TimesFM is better than most supervised approaches, including recent deep learning models. We also compare TimesFM to &lt;a href=&quot;https://platform.openai.com/docs/models/gpt-3-5&quot;>;GPT-3.5&lt;/a>; for forecasting using a specific prompting technique proposed by &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime(ZS)&lt;/a>;. We demonstrate that TimesFM performs better than llmtime(ZS) despite being orders of magnitude smaller. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;876&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Scaled MAE (the lower the better) of TimesFM(ZS) against other supervised and zero-shot approaches on Monash datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Most of the Monash datasets are short or medium horizon, ie, the prediction length is not too long. We also test TimesFM on popular benchmarks for long horizon forecasting against a recent state-of-the-art baseline &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; (and other long-horizon forecasting baselines). In the next figure, we plot the MAE on &lt;a href=&quot;https://paperswithcode.com/dataset/ett&quot;>;ETT&lt;/a>; datasets for the task of predicting 96 and 192 time-points into the future. The metric has been calculated on the last test window of each dataset (as done by the &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime&lt;/a>; paper). We see that TimesFM not only surpasses the performance of llmtime(ZS) but also matches that of the supervised PatchTST model explicitly trained on the respective datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;433&quot; data-original-width=&quot;735&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Last window MAE (the lower the better) of TimesFM(ZS) against llmtime(ZS) and long-horizon forecasting baselines on ETT datasets.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We train a decoder- only foundation model for time-series forecasting using a large pretraining corpus of 100B real world time-points, the majority of which was search interest time-series data derived from Google Trends and pageviews from Wikipedia. We show that even a relatively small 200M parameter pretrained model that uses our TimesFM architecture displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov and Yichen Zhou.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6956767920612914706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html&quot; rel=&quot;alternate&quot; title=&quot;A decoder-only foundation model for time-series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502&lt;/id>;&lt;published>;2024-02-02T09:49:00.000-08:00&lt;/published>;&lt;updated>;2024-02-02T09:49:36.211-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML Fairness&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Intervening on early readouts for mitigating spurious features and simplicity bias&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Machine learning models in the real world are often trained on limited data that may contain unintended &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_(statistics)&quot;>;statistical biases&lt;/a >;。 For example, in the &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CELEBA&lt;/a>; celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as &lt;a href=&quot;https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging&quot;>;medical diagnosis&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Surprisingly, recent work has also discovered an inherent tendency of deep networks to &lt;em>;amplify such statistical biases&lt;/em>;, through the so-called &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf&quot;>;simplicity bias&lt;/a>; of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. &lt;/p>; &lt;p>; With the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying &lt;em>;early readouts&lt;/em>; and &lt;em>;feature forgetting&lt;/em>; 。 First, in “&lt;a href=&quot;https://arxiv.org/abs/2310.18590&quot;>;Using Early Readouts to Mediate Featural Bias in Distillation&lt;/a>;”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;>;model distillation&lt;/a>;, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;Overcoming Simplicity Bias in Deep Networks using a Feature Sieve&lt;/a>;”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model&#39;s ability to generalize to unseen domains compared to previous approaches. Our &lt;a href=&quot;https://ai.google/responsibility/principles&quot;>;AI Principles&lt;/a>; and our &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;1080&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation comparing hypothetical responses from two models trained with and without the feature sieve.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Early readouts for debiasing distillation&lt;/h2>; &lt;p>; We first illustrate the diagnostic value of &lt;em>;early readouts&lt;/em >; and their application in debiased distillation, ie, making sure that the student model inherits the teacher model&#39;s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the &lt;a href=&quot;https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e&quot;>;cross-entropy loss&lt;/a>; between student outputs and the ground-truth labels) and teacher matching (minimizing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL divergence&lt;/a>; loss between student and teacher outputs for any given input). &lt;/p>; &lt;p>; Suppose one trains a linear decoder, ie, a small auxiliary neural network named as &lt;em>;Aux,&lt;/em>; on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model&#39;s dependence on potentially spurious features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;796&quot; data-original-width=&quot;1128&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrating the usage of early readouts (ie, output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used this signal to modulate the contribution of the teacher in the distillation loss on a per-instance basis, and found significant improvements in the trained student model as a result. &lt;/p>; &lt;p>; We evaluated our approach on standard benchmark datasets known to contain spurious correlations (&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;>;Waterbirds&lt;/a>;, &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CelebA&lt;/a>;, &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/civil_comments&quot;>;CivilComments&lt;/a>;, &lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;>;MNLI&lt;/a>;). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color 。 Thus, a measure of model performance is its &lt;em>;worst group accuracy&lt;/em>;, ie, the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our &lt;a href=&quot;https://arxiv.org/pdf/2310.18590.pdf&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1270&quot; height=&quot;511&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overcoming simplicity bias with a feature sieve&lt;/h2>; &lt;p>; In a second, closely related project, we intervene directly on the information provided by early readouts, to improve &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;feature learning&lt;/a>; and &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/generalization/video-lecture&quot;>;generalization&lt;/a>;. The workflow alternates between &lt;em>;identifying &lt;/em>;problematic features and &lt;em>;erasing identified features&lt;/em>; from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;1098&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Training workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We describe the identification and erasure steps in more detail: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Identifying simple features&lt;/b>;: We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. &lt;/li>;&lt;li>;&lt;b>;Applying the feature sieve&lt;/b>;: We aim to erase the identified features in the early layers of the neural network with the use of a novel &lt;em>;forgetting loss&lt;/em>;,&lt;em>; L&lt;sub>;f &lt;/sub>;&lt;/em>;, which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged. &lt;/li>; &lt;/ul>; &lt;p>; We can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter search&lt;/a>; to maximize validation accuracy, a standard measure of generalization. Since we include “no-forgetting” (ie, the baseline model) in the search space, we expect to find settings that are at least as good as the baseline. &lt;/p>; &lt;p>; Below we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (&lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;) and animal categorization (&lt;a href=&quot;https://arxiv.org/pdf/1906.02899v3.pdf&quot;>;NICO&lt;/a>;). Feature importance was estimated using post-hoc gradient-based importance scoring (&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GRAD-CAM&lt;/a>;), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;850&quot; data-original-width=&quot;1616&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Feature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this ability to learn better, generalizable features, we show substantial gains over a range of relevant baselines on real-world spurious feature benchmark datasets: &lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2104.06885.pdf&quot;>;CelebA Hair&lt;/a>;, &lt;a href=&quot;https://nico.thumedialab.com/&quot;>;NICO&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/imagenet_a&quot;>;ImagenetA&lt;/a>;, by margins up to 11% (see figure below). More details are available in &lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1082&quot; data-original-width=&quot;844&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png&quot; width=&quot;501&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We hope that our work on early readouts and their use in feature sieving for generalization will both spur the development of a new class of adversarial feature learning approaches and help improve the generalization capability and robustness of deep learning systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;The work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at &lt;a href=&quot;https://www.iitb.ac.in/&quot;>;IIT Bombay&lt;/a>;. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2254287928040727502/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html&quot; rel=&quot;alternate&quot; title=&quot;Intervening on early readouts for mitigating spurious features and simplicity bias&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media :thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984&lt;/id>;&lt;published>;2024-01 -31T13:59:00.000-08:00&lt;/published>;&lt;updated>;2024-01-31T13:59:36.056-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom /ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme= &quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MobileDiffusion: Rapid text-to-image generation on-device&lt; /stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML&lt;/span>; &lt;img src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (eg, &lt;a href=&quot;https://stability.ai/news/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>;, &lt;a href=&quot;https://openai.com/research/dall-e&quot;>;DALL·E&lt;/a>;, and &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;). While recent advancements in inference solutions on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;Android&lt;/a>; via MediaPipe and &lt;a href=&quot;https://github.com/apple/ml-stable-diffusion&quot;>;iOS&lt;/a>; via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices&lt;/a>;”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style =&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif&quot; style =“左边距：自动； margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rapid text-to-image generation on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires &lt;a href=&quot;https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot;>;iterative denoising&lt;/a>; to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature. &lt;/p>; &lt;p>; The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.00927&quot;>;DPM&lt;/a>;) or distillation techniques (eg, &lt;a href=&quot;https://arxiv.org/abs/2202.00512&quot;>;progressive distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;>;consistency distillation&lt;/a>;), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.&quot;>;Adversarial Diffusion Distillation&lt;/a>;, even reduce to a single necessary step. &lt;/p>; &lt;p>; However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (eg, &lt;a href=&quot;https://snap-research.github.io/SnapFusion/&quot;>;SnapFusion&lt;/a>;). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MobileDiffusion&lt;/h2>; &lt;p>; Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model&#39;s architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion&#39;s &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;UNet architecture&lt;/a>;. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion. &lt;/p>; &lt;p>; The design of MobileDiffusion follows that of &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;latent diffusion models&lt;/a>;. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP-ViT/L14&lt;/a>;, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Diffusion UNet&lt;/h3>; &lt;p>; As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (eg, data, optimizer) to study the effects of different architectures. &lt;/p>; &lt;p>; In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of &lt;a href=&quot;https://arxiv.org/abs/2301.11093&quot;>;UViT&lt;/a>; architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV /s915/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;915&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Convolution blocks, in particular &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;separable convolution&lt;/a>;. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance. &lt;/p>; &lt;p>; In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of &lt;a href=&quot;https://arxiv.org/pdf/2110.12894.pdf&quot;>;FLOPs&lt;/a>; (floating-point operations) and number of parameters. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Comparison of some diffusion UNets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h3>;Image decoder&lt;/h3>; &lt;p>; In addition to the UNet, we also optimized the image decoder. We trained a &lt;a href=&quot;https://arxiv.org/abs/2012.03715&quot;>;variational autoencoder&lt;/a>; (VAE) to encode an &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size. To further enhance efficiency, we design a lightweight decoder architecture by pruning the original&#39;s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our &lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;789&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Decoder&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;#Params (M)&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;PSNR↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;SSIM↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;LPIPS↓&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;SD&lt;/b>; &lt;/td>; &lt;td>;49.5 &lt;/td>; &lt;td>;26.7 &lt;/td>; &lt;td>;0.76 &lt;/td>; &lt;td>;0.037 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours&lt;/b>; &lt;/td>; &lt;td>;39.3 &lt;/td>; &lt;td>;30.0 &lt;/td>; &lt;td>;0.83 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours-Lite&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;9.8 &lt;/td>; &lt;td>;30.2 &lt;/td>; &lt;td>;0.84 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_similarity&quot;>;structural similarity index measure&lt;/a>; (SSIM), and &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;>;Learned Perceptual Image Patch Similarity&lt;/a>; (LPIPS).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;One-step sampling&lt;/h3>; &lt;p>; In addition to optimizing the model architecture, we adopt a &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN hybrid&lt;/a>; to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (eg, &lt;a href=&quot;https://arxiv.org/abs/2301.09515&quot;>;StyleGAN-T&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.05511&quot;>;GigaGAN&lt;/a>;) confront similar complexities, resulting in highly intricate and expensive training. &lt;/p>; &lt;p>; To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training. &lt;/p>; &lt;p>; The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ -UILKw/s960/image7.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;960 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of DiffusionGAN fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling 。 With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1728&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images generated by our MobileDiffusion&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1184&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency measurements (&lt;b>;s&lt;/b>;) on mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;responsible AI practices&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.&lt; /em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5966553114967673984/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html&quot; rel=&quot;alternate&quot; title=&quot;MobileDiffusion: Rapid text-to-image generation on-device&quot; type=&quot;text/ html“/>; &lt;aunder>; &lt;名称>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777526611 &lt;/uri>; &lt;emage>; &lt;Email>; ：Image Height =“ 16” RER =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/b16-rounded.gif.gif.gif” width width =“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c /InstantTIGO%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5144906729109253495&lt;/id>;&lt;published>;2024-01-26T11:56:00.000-08:00&lt;/published>;&lt;updated >;2024-01-26T11:56:23.553-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Mixed-input matrix multiplication performance optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Manish Gupta , Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;display: none ;” />; &lt;p>; AI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs). LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver &lt;a href=&quot;https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e&quot;>;tens of exaflops&lt;/a>; of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The bulk of an LLM&#39;s memory and compute are consumed by &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;>;weights&lt;/a>; in &lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;matrix multiplication&lt;/a>; operations. Using narrower &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Primitive_data_type&quot;>;data types&lt;/a>;&lt;/em>; reduces memory consumption. For example, storing weights in the 8-bit &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_(computer_science)&quot;>;integer&lt;/a>; (ie, U8 or S8) data type reduces the memory footprint by 4× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;>;single-precision&lt;/a>; (F32) and 2× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Half-precision_floating-point_format&quot;>;half-precision&lt;/a>; (F16) or &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format&quot;>;bfloat16&lt;/a>; (BF16). Furthermore, &lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;previous work has&lt;/a>; shown that LLM models running matrix multiplications with &lt;em>;weights&lt;/em>; in S8 and &lt;em>;input&lt;/em>; in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as &lt;em>;weight-only quantization&lt;/em>; and requires efficient implementation of matrix multiplication with &lt;em>;mixed-inputs&lt;/em>;, eg, half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations. &lt;/p>; &lt;p>; To that end, in this blog we focus on mapping mixed-input matrix multiplication onto the &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/&quot;>;NVIDIA Ampere architecture&lt;/a>;. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; repository. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Memory footprint for an 175B parameter LLM model with various data types formats.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The matrix-multiply-accumulate operation&lt;/h2>; &lt;p>; Modern AI hardware accelerators such as &lt;a href= &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google&#39;s TPU&lt;/a>; and &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA&#39;s GPU&lt;/a>; multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the &lt;em>;matrix-multiply-accumulate&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;) operation. For the rest of the blog the reference to &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation are fixed in hardware 。 This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. &lt;/p>; &lt;p>; The Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation is defined by specifying two input matrices (eg, &lt;em>;A&lt;/em>; &amp;amp; &lt;em>;B&lt;/em>;, shown below) to produce a result matrix, &lt;em>;C&lt;/em>;. The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation natively supports mixed-precision. &lt;em>;&lt;a href=&quot;https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/&quot;>;Mixed-precision Tensor Cores&lt;/a>;&lt;/em>; allow mixing input (&lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) data type with the result (&lt;em>;C&lt;/em>;) data type. In contrast, &lt;em>;mixed-input &lt;/em>;matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Tensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Challenges of mixed -input matrix multiplication&lt;/h2>; &lt;p>; To simplify the discussion, we restrict to a specific example of mixed-input matrix multiplication: F16 for user input and U8 for the model weights (written as F16 * U8). The techniques described here work for various combinations of mixed-input data types. &lt;/p>; &lt;p>; A GPU programmer can access a &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;hierarchy of memory&lt;/a>;, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a &lt;em>;warp&lt;/em>;. The supported data type &lt;em>;and&lt;/em>; layout within a warp are fixed for an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Data type conversion &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the &lt;em>;mixed-input&lt;/em>; matrix multiplication to hardware-supported &lt;em>;mixed-precision&lt;/em>; Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Layout conformance &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix &lt;em>;B&lt;/em>; of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called &lt;em>;layout conformance&lt;/em>; and needs to be achieved in the software. &lt;/p>; &lt;p>; The figure below shows an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation consuming matrix &lt;em>;A&lt;/em>; and matrix &lt;em>;B&lt;/em>; from registers to produce matrix &lt;em>;C&lt;/em>; in registers, distributed across one warp. The thread &lt;em>;T0&lt;/em>; is highlighted and zoomed in to show the weight matrix &lt;em>;B&lt;/em>; goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). (Original figure source &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100&lt;/a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Software strategies addressing challenges&lt;/h2>; &lt;p>; A typical data type conversion involves a sequence of operations on 32-bit registers, shown below. Each rectangular block represents a register and the adjoining text are the operations. The entire sequence shows the conversion from 4xU8 to 2x(2xF16). The sequence involves roughly 10 operations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; from 4xU8 to 2x(2xF16) in 32-bit registers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There are many ways of achieving layout conformance. Two of the existing solutions are: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Narrower bitwidth shared memory loads&lt;/em>;: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in &lt;em>;two&lt;/em>; 32-bit registers, with each register containing 2xF16 values (shown above for the matrix &lt;em>;B&lt;/em>;&#39;s thread &lt;em>;T0&lt;/em>;). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth. &lt;/li>;&lt;li>;&lt;em>;Pre-processing in global memory&lt;/em>;: An &lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;alternative strategy&lt;/a>; involves rearranging the data within the global memory (one level above the shared memory in &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;memory hierarchy&lt;/a>;), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; adopts this method to effectively address layout conformance challenges. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimized software strategies&lt;/h2>; &lt;p>; To further optimize and reduce the overhead of data type conversion and layout conformance, we have implemented &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a>;&lt;/code>; and &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;&lt;/code>;, respectively. &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. &lt;/p>; &lt;p>; The conversion sequence for &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>; is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. &lt;code>;FastNumericArrayConvertor&lt;/code>; uses the &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt&quot;>;permute byte&lt;/a>;&lt;/code>; to rearrange bytes of 4xU8 into two registers. Additionally, &lt;code>;FastNumericArrayConvertor&lt;/code>; does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in &lt;em>;two&lt;/em>; 32-bit registers containing 2x(2xF16) values. The &lt;code>;FastNumericArrayConvertor&lt;/code>; for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; utilizes &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;permute bytes&lt;/a>;&lt;/code>; and packed arithmetic, reducing the number of instructions in the data type conversion.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations 。 &lt;/p>; &lt;p>; NVIDIA Ampere architecture provides a load matrix instruction (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;). The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the &lt;em>;shape&lt;/em>; and &lt;em>;layout&lt;/em>; that &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>; consume. The use of &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; &lt;em>;reduces&lt;/em>; the number of load instructions and &lt;em>;increases&lt;/em>; the memory bandwidth utilization. Since the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; instruction moves U8 data to registers, the layout after the load conforms with U8*U8 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, and not with F16*F16 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation 。 We implemented &lt;code>;FragmentShuffler&lt;/code>; to rearrange the data within registers using shuffle (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; operations to achieve the layout conformance. &lt;/p>;&lt;p>; The most significant contribution of this work is to achieve layout conformance through register shuffles, avoiding offline pre-processing in global memory or narrower bitwidth shared memory loads. Furthermore, we provide implementations for &lt;code>;FastNumericArrayConvertor&lt;/code>; covering data type conversion from &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8-to-BF16&lt;/a>;, and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8-to-BF16&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance results&lt;/h2>; &lt;p>; We measured the performance of eight mixed-input variants of &lt;em>;our method&lt;/em>; (shown below in blue and red; varying the data types of matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) and two &lt;em>;mixed-precision&lt;/em>; data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Mixed-input matrix multiplication performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape &lt;code>;m=3456, n=4096, k=2048.&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p >; &lt;em>;We would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia. We would like to thank our NVIDIA partners Rawn Henry, Pradeep Ramani, Vijay Thakkar, Haicheng Wu, Andrew Kerr, Matthew Nicely, and Vartika Singh.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:/ /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;Mixed-input matrix multiplication performance optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;published >;2024-01-23T14:27:00.000-08:00&lt;/published>;&lt;updated>;2024-01-23T14:27:09.785-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer: Scaling transformers for graph-structured data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;Graphs&lt;/a>;, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A common approach to learning on graphs are &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;graph neural networks&lt;/a>; (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2&quot;>;message-passing&lt;/a>; framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors. &lt;/p>; &lt;p>; Recently, &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer models&lt;/a>; have emerged as a popular alternative to message-passing GNNs. These models build on the success of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer architectures&lt;/a>; in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism&lt;em>; &lt;/em>;that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see &lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot;>;the first open problem here&lt;/a>;). &lt;/p>; &lt;p>; A natural remedy is to use a &lt;em>;sparse&lt;/em>; interaction graph with fewer edges. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;Many sparse and efficient transformers have been proposed&lt;/a>; to eliminate the quadratic bottleneck for sequences, however, they do not generally extend to graphs in a principled manner. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;spectral graph theory&lt;/a>;, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Expander graphs&lt;/h2>; &lt;p>; A key idea at the heart of Exphormer is the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph&quot;>;expander graphs&lt;/a>;, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, ie, a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes. &lt;/p>; &lt;p>; A common class of expander graphs are &lt;em>;d&lt;/em>;-regular expanders, in which there are &lt;em>;d&lt;/em>; edges from every node (ie, every node has degree &lt;em>;d&lt;/em>;). The quality of an expander graph is measured by its &lt;em>;spectral gap&lt;/em>;, an algebraic property of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;>;adjacency matrix&lt;/a>; (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Ramanujan_graph&quot;>;Ramanujan graphs&lt;/a>; — they achieve a gap of &lt;em>;d&lt;/em>; - 2*√(&lt;em>;d&lt;/em>;-1), which is essentially the best possible among &lt;em>;d&lt;/em>;-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of &lt;em>;d&lt;/em>;. We use a &lt;a href=&quot;https://arxiv.org/abs/cs/0405020&quot;>;randomized expander construction of Friedman&lt;/a>;, which produces near-Ramanujan graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;800&quot; height=&quot;320&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968&quot;>;&lt;span face=&quot;Arial, sans- serif&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Expander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.&lt;/span>;&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse &lt;em>;d&lt;/em>;-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that &lt;em>;d&lt;/em>; is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.&lt;/p>; &lt;br />; &lt;h2>;Exphormer: Constructing a sparse interaction graph&lt;/h2>; &lt;p>; Exphormer combines expander edges with the input graph and virtual nodes. More specifically, the sparse attention mechanism of Exphormer builds an interaction graph consisting of three types of edges: &lt;/p>; &lt;ul>; &lt;li>;Edges from the input graph (&lt;em>;local attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from a constant-degree expander graph (&lt;em>;expander attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from every node to a small set of virtual nodes (&lt;em>;global attention&lt;/em>;) &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3&quot;>;&lt;span face=&quot;Arial, sans-serif&quot; style=&quot;font-size: 10pt;字体样式：斜体；字体变体替代：正常；字体变体东亚：正常；字体变体数字：正常；字体变体位置：正常；垂直对齐：基线； white-space-collapse: preserve;&quot;>;Exphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.&lt;/span>;&lt;/ span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Each component serves a specific purpose: the edges from the input graph retain the inductive bias from the input graph structure (which typically gets lost in a fully-connected attention module). Meanwhile, expander edges allow good global connectivity and random walk mixing properties (which spectrally approximate the complete graph with far fewer edges). Finally, virtual nodes serve as global “memory sinks” that can directly communicate with every node. While this results in additional edges from each virtual node equal to the number of nodes in the input graph, the resulting graph is still sparse. The degree of the expander graph and the number of virtual nodes are hyperparameters to tune for improving the quality指标。 &lt;/p>; &lt;p>; Furthermore, since we use an expander graph of constant degree and a small constant number of virtual nodes for the global attention, the resulting sparse attention mechanism is linear in the size of the original input graph, ie, it models a number of direct interactions on the order of the total number of nodes and edges. &lt;/p>; &lt;p>; We additionally show that Exphormer is as expressive as the dense transformer and obeys universal approximation properties. In particular, when the sparse attention graph of Exphormer is augmented with self loops (edges connecting a node to itself), it can universally approximate continuous functions [&lt;a href=&quot;https://arxiv.org/abs/1912.10077&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>;2&lt;/a>;]. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Relation to sparse Transformers for sequences&lt;/h3>; &lt;p>; It is interesting to compare Exphormer to sparse attention methods for sequences. Perhaps the architecture most conceptually similar to our approach is &lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>;BigBird&lt;/a>;, which builds an interaction graph by combining different components. BigBird also uses virtual nodes, but, unlike Exphormer, it uses window attention and random attention from an &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>; random graph model for the remaining components. &lt;/p>; &lt;p>; Window attention in BigBird looks at the tokens surrounding a token in a sequence — the local neighborhood attention in Exphormer can be viewed as a generalization of window attention to graphs. &lt;/p>; &lt;p>; The Erdős-Rényi graph on &lt;em>;n&lt;/em>; nodes, &lt;em>;G(n, p)&lt;/em>;, which connects every pair of nodes independently with probability &lt;em>;p&lt;/em>;, also functions as an expander graph for suitably high &lt;em>;p&lt;/em>;. However, a superlinear number of edges (Ω(&lt;em>;n&lt;/em>; log &lt;em>;n&lt;/em>;)) is needed to ensure that an Erdős-Rényi graph is connected, let alone a good expander. On the other hand, the expanders used in Exphormer have only a &lt;em>;linear&lt;/em>; number of edges. &lt;/p>; &lt;br />; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; Earlier works have shown the use of full graph Transformer-based models on datasets with graphs of size up to 5,000 nodes. To evaluate the performance of Exphormer, we build upon the celebrated &lt;a href=&quot;https://github.com/rampasek/GraphGPS&quot;>;GraphGPS framework&lt;/a>; [&lt;a href=&quot;https://arxiv.org/abs/2205.12454&quot;>;3&lt;/a>;], which combines both message passing and graph transformers and achieves state-of-the-art performance on a number of datasets. We show that replacing dense attention with Exphormer for the graph attention component in the GraphGPS framework allows one to achieve models with comparable or better performance, often with fewer trainable parameters. &lt;/p>; &lt;p>; Furthermore, Exphormer notably allows graph transformer architectures to scale well beyond the usual graph size limits mentioned above. Exphormer can scale up to datasets of 10,000+ node graphs, such as the &lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>;Coauthor dataset&lt;/a>;, and even beyond to larger graphs such as the well-known &lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>;ogbn-arxiv dataset&lt;/a>;, a citation network, which consists of 170K nodes and 1.1 million edges. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance .png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original- height=&quot;190&quot; data-original-width=&quot;1522&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot; https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper&#39;s publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td align=&quot;left&quot;>;&lt;strong>;Model&amp;nbsp;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PascalVOC-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;COCO-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Func&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;AP &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Struct&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MAE &lt;/font>;&lt;strong>;↓&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PCQM-Contact&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MRR &lt;/font>;&lt;strong>;↑&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>;&lt;td colspan=&quot;6&quot;>;&lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />;&lt;/div>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;Standard GraphGPS&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.375 ± 0.011&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.341 ± 0.004&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;&lt;strong>;0.654 ± 0.004&lt;/strong>; &amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.250 ± 0.001&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.334 ± 0.001 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Exphormer (ours)&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.398 ± 0.004&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.346 ± 0.001 &amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;em>;&amp;nbsp;0.653 ± 0.004&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong >;&lt;em>;&amp;nbsp;0.248 ± 0.001&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.364 ± 0.002&lt;/em>;&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>;-->; &lt;p>; Finally, we observe that Exphormer, which creates an overlay graph of small diameter via expanders, exhibits the ability to effectively learn long-range dependencies 。 The &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>;&amp;nbsp;is a suite of five graph learning datasets designed to measure the ability of models to capture long-range interactions 。 Results show that Exphormer-based models outperform standard GraphGPS models (which were previously state-of-the-art on four out of five datasets at the time of publication). &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Graph transformers have emerged as an important architecture for ML that adapts the highly successful sequence-based transformers used in NLP to graph-structured data. Scalability has, however, proven to be a major challenge in enabling the use of graph transformers on datasets with large graphs. In this post, we have presented Exphormer, a sparse attention framework that uses expander graphs to improve scalability of graph transformers. Exphormer is shown to have important theoretical properties and exhibit strong empirical performance, particularly on datasets where it is crucial to learn long range dependencies. For more information, we point the reader to a short presentation &lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;video&lt;/a>; from ICML 2023. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our research collaborators Hamed Shirzad and Danica J. Sutherland from The University of British Columbia as well as Ali Kemal Sinop from Google Research. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1418736582601940076/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html&quot; rel=&quot;alternate&quot; title=&quot;Exphormer: Scaling transformers for graph-structured data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;