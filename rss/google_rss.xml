<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com,1999:blog-8474926331452026626</id><updated> 2023-06-11T05:57:25.149-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Publications"></category><category term="Research"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="conference"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="NLP"></category><category term="CVPR"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="Computer Science"></category><category term="On-device Learning"></category><category term="MOOC"></category><category term="HCI"></category><category term="Security and Privacy"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="Self-Supervised Learning"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="NeurIPS"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="DeepMind"></category><category term="Earth Engine"></category><category term="Google Maps"></category><category term="K-12"></category><category term="Responsible AI"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Acoustic Modeling"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Semantic Models"></category><category term="Systems"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Augmented Reality"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Recommender Systems"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Social Networks"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="RAI-HCT Highlights"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="Differential Privacy"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新消息。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1240&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2015281937814138473&lt;/id>;&lt;published>;2023-06-09T12:09:00.000-07:00&lt;/published>;&lt;updated>;2023-06- 09T12:09:34.603-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;图像处理&quot;>;&lt;/category>;&lt;title type=&quot;文本&quot;>;Imagen Editor 和 EditBench：推进和评估文本引导图像修复&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google Research 研究工程师 Su Wang 和 Ceslee Montgormery 发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_Y ZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s320/Imagen%20Editor%20&amp;amp;%20EditBench% 20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 在过去的几年中，&lt;a href=&quot;https://en.wikipedia.org/wiki/Text-to-image_model&quot;>;文本到图像生成&lt;/a>;的研究呈爆炸式增长的突破（特别是 &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;、&lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; , &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; 等）自然渗透到相关主题中。特别是，文本引导图像编辑 (TGIE) 是一项实际任务，涉及编辑生成和拍摄的视觉效果，而不是完全重做它们。当重新创建视觉效果非常耗时或不可行时（例如，调整度假照片中的对象或完善从头生成的可爱小狗的细粒度细节），快速、自动化和可控的编辑是一种方便的解决方案。此外，TGIE 代表了改进基础模型本身训练的重要机会。多模态模型需要不同的数据才能正确训练，而 TGIE 编辑可以生成和重组高质量和可扩展的合成数据，这可能是最重要的，可以提供优化训练数据沿任何给定轴分布的方法。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2212.06909&quot;>;Imagen Editor 和 EditBench：推进和评估文本引导Image Inpainting&lt;/a>;”，将在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; 上展示，我们介绍 &lt;a href=&quot;https://imagen.research .google/editor/&quot;>;Imagen Editor&lt;/a>;，用于掩蔽&lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;修复&lt; /a>; — 即，当用户在叠加层或“遮罩”（通常在绘图类型界面中生成）旁边提供文本说明时，指示他们想要修改的图像区域。我们还介绍了 &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;EditBench&lt;/a>;，这是一种衡量图像编辑模型质量的方法。 EditBench 超越了&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;通常&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=bKBhQhPeKaF&quot;>;使用&lt;/ a>; &lt;a href=&quot;https://arxiv.org/abs/1910.13321&quot;>;粗粒度&lt;/a>;“这个图像是否匹配这个文本”方法，并深入到各种类型的属性、对象和场景以便更细粒度地了解模型性能。特别是，它在不忽视图像质量的情况下，非常强调图文对齐的忠实度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s1261/image2。 png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https:/ /1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;给定图像、用户定义的蒙版和文本提示，Imagen Editor 对指定区域进行本地化编辑。该模型有意义地结合了用户的意图并执行逼真的编辑。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Imagen Editor&lt;/h2>; &lt;p>; Imagen Editor 是一个&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html&quot;>;扩散-基于模型&lt;/a>; 在 &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>; 上进行微调以进行编辑。它的目标是改进语言输入的表示、细粒度控制和高保真输出。 Imagen Editor 从用户那里获取三个输入：1) 要编辑的图像，2) 用于指定编辑区域的二进制掩码，以及 3) 文本提示——所有三个输入都会引导输出样本。 &lt;/p>; &lt;p>; Imagen Editor 依靠三种核心技术进行高质量的文本引导图像修复。首先，不同于之前的修复模型（例如，&lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1801.07892&quot; >;Context Attention&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;>;Gated Convolution&lt;/a>;）应用随机框和笔划掩码，Imagen Editor 采用对象检测器掩码策略一个&lt;a href=&quot;https://arxiv.org/abs/1801.04381​​&quot;>;对象检测器模块&lt;/a>;，在训练期间生成对象掩码。对象蒙版基于检测到的对象而不是随机补丁，并允许在编辑文本提示和蒙版区域之间进行更有原则的对齐。从经验上讲，该方法有助于模型避免当蒙版区域很小或仅部分覆盖对象时忽略文本提示的普遍问题（例如，&lt;a href=&quot;https://arxiv.org/abs/2204.14217&quot;>; CogView2&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH 6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/s648/image9.png&quot; 样式=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;648&quot; height=&quot;328&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv _kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/w640-h328/image9.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;随机蒙版（&lt;strong>;左&lt;/strong>;）经常捕获背景或与对象边界相交，定义仅从图像上下文就可以合理修复的区域。对象遮罩（&lt;strong>;右&lt;/strong>;）更难仅从图像上下文修复，鼓励模型在训练期间更多地依赖文本输入。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 接下来，在训练和推理期间，Imagen Editor 通过调节全分辨率（本作品中为 1024×1024）、输入图像和掩码的逐通道连接（类似于 &lt;a href= &quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887996&quot;>;SR3&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;调色板&lt;/a >; 和 &lt;a href=&quot;https://arxiv.org/abs/2112.10741&quot;>;滑行&lt;/a>;）。对于基础扩散 64×64 模型和 64×64→256×256 超分辨率模型，我们应用参数化下采样卷积（例如，带步长的卷积），我们根据经验发现这对高保真度至关重要。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6 QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s646/image4.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;365&quot; data-original-width=&quot;646&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1Rg XTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Imagen 针对图像编辑进行了微调。所有扩散模型，即基础模型和超分辨率 (SR) 模型，都以高分辨率 1024×1024 图像和蒙版输入为条件。为此，引入了新的卷积图像编码器。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 最后，在推理时我们应用 &lt;a href=&quot;https://arxiv .org/abs/2207.12598&quot;>;无分类器指导&lt;/a>; (CFG) 使样本偏向特定条件，在本例中为文本提示。 CFG 在文本条件和非条件模型预测之间进行插值，以确保生成的图像与文本引导图像修复的输入文本提示之间的强对齐。我们遵循 &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;Imagen Video&lt;/a>; 并使用高引导权重和引导振荡（在引导权重值范围内振荡的引导计划）。在基础模型（stage-1 64x diffusion）中，确保与文本的强对齐最为关键，我们使用在 1 到 30 之间振荡的指导权重计划。我们观察到高指导权重与振荡指导相结合会产生最佳效果样本保真度和文本图像对齐之间的权衡。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EditBench&lt;/h2>; &lt;p>; 用于文本引导图像修复评估的EditBench数据集包含240张图像，有 120 个生成图像和 120 个自然图像。生成的图像由 &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; 合成，自然图像由 &lt;a href=&quot;https://arxiv.org/abs/1602.07332 &quot;>;Visual Genome&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/1811.00982&quot;>;Open Images&lt;/a>; 数据集。 EditBench 捕获各种语言、图像类型和文本提示特异性级别（即简单、丰富和完整的字幕）。每个示例都包含 (1) 蒙版输入图像，(2) 输入文本提示，以及 (3) 用作自动度量参考的高质量输出图像。为了深入了解不同模型的相对优势和劣势，EditBench 提示旨在测试三个类别的细粒度细节：(1) 属性（例如，材料、颜色、形状、尺寸、数量）； (2) 对象类型（例如，常见、稀有、文本渲染）； (3) 场景（例如，室内、室外、写实或绘画）。为了解不同的提示规范如何影响模型性能，我们提供了三种文本提示类型：单一属性 (Mask Simple) 或蒙版对象的多属性描述 (Mask Rich) – 或整个图像描述 (Full Image) . Mask Rich 特别探索了模型处理复杂属性绑定和包含的能力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau 9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s531/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;531&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4 Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;完整图像用作成功修复的参考。遮罩以自由形式、非暗示形状覆盖目标对象。我们评估 Mask Simple、Mask Rich 和 Full Image 提示，与传统的文本到图像模型一致。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 由于固有的弱点在现有的自动评估指标（&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;CLIPScore&lt;/a>; 和 &lt;a href=&quot;https://datasets-benchmarks-proceedings.neurips.cc/paper /2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf&quot;>;CLIP-R-Precision&lt;/a>;) 对于 TGIE，我们将人工评估作为 EditBench 的黄金标准。在下面的部分中，我们将演示如何将 EditBench 应用于模型评估。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;评估&lt;/h2>; &lt;p>; 我们评估 Imagen Editor 模型——使用对象遮罩 (IM)和随机掩蔽 (IM-RM) — 与可比模型相比，&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;稳定扩散&lt;/a>; (SD) 和 &lt;a href=&quot;https:/ /cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; (DL2)。 Imagen Editor 在所有 EditBench 评估类别中都以可观的优势优于这些模型。 &lt;/p>; &lt;p>; 对于完整图像提示，&lt;em>;单图像人工评估&lt;/em>; 提供二元答案以确认图像是否与说明匹配。对于 Mask Simple 提示，单图像人工评估确认对象和属性是否正确呈现并正确绑定（例如，对于红猫，红色桌子上的白猫将是不正确的绑定）。 &lt;em>;并排人工评估&lt;/em>;仅使用 Mask Rich 提示对 IM 与其他三个模型（IM-RM、DL2 和 SD）中的每一个进行并排比较，并指出哪个图像与标题更匹配以实现文本图像对齐，以及哪个图像最逼真。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1V YKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/s800/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;703&quot; data-original-width=&quot;800&quot; height=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzX lRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/w640-h562/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;人工评价。全图提示引出注释者对文本图像对齐的整体印象； Mask Simple 和 Mask Rich 检查是否正确包含特定属性、对象和属性绑定。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 对于单图像人类评估，IM全面获得最高评分（比第二高的表现模型高 10-13%）。对于其余的，性能顺序是IM-RM＞； DL2>; SD（有 3-6% 的差异）除了 Mask Simple，其中 IM-RM 落后 4-8%。由于 Full 和 Mask Rich 涉及相对更多的语义内容，我们推测 IM-RM 和 IM 受益于更高性能的 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning -with-t5.html&quot;>;T5 XXL&lt;/a>; 文本编码器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJ yEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s1117/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1117&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQt IBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;通过提示类型对 EditBench 上文本引导图像修复的单图像人工评估。对于 Mask Simple 和 Mask Rich 提示，如果编辑的图像准确地包括提示中指定的每个属性和对象，包括正确的属性绑定，则文本-图像对齐是正确的。请注意，由于不同的评估设计，Full vs. Mask-only 提示，结果比较不直接。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; EditBench 专注于精细-粒度注释，因此我们评估对象和属性类型的模型。对于对象类型，IM 在所有类别中均处于领先地位，在常见、稀有和文本渲染方面的表现比排名第二的模型高 10-11%。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Q h-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s1086/image8.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1086&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE 0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;在 EditBench Mask Simple 上按对象类型进行单图像人工评估。作为一个队列，模型在对象渲染方面比文本渲染更好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 对于属性类型，IM 的评级要高得多（13- 16%）比第二高的性能模型高，除了在数量上，DL2 仅落后 1%。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-18 5GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s1086/image10.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;607&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6 evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;EditBench Mask Simple 上的单图像人工评估（按属性类型）。对象屏蔽提高了对提示属性的全面遵守（IM 与 IM-RM）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 并排比较与其他一对一模型相比，IM 在文本对齐方面领先，并且有很大的余量，与 SD、DL2 和 IM-RM 相比，注释者更喜欢它。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s636/image6.png&quot; 样式= &quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;319&quot; data-original-width=&quot;636&quot; src=&quot;https://1.bp ... -align: center;&quot;>; 对图像真实感和图像真实性的并排人类评估EditBench Mask Rich 提示上的文本图像对齐。对于文本图像对齐，Imagen Editor 在所有比较中都是首选。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 最后，我们说明了具有代表性的并排比较所有型号。有关更多示例，请参阅&lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kS u6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s766/image3.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;766&quot; data-original-width=&quot;638&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7Q nLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “text-align: center;”>;Mask Simple 与 Mask Rich 提示的示例模型输出。与使用随机屏蔽训练的相同模型相比，对象屏蔽提高了 Imagen Editor 对提示的细粒度遵守。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们展示了 Imagen Editor 和 EditBench，在文本引导图像修复及其评估方面取得了重大进展。 Imagen Editor 是从 Imagen 微调的文本引导图像修复。 EditBench 是文本引导图像修复的综合系统基准，可跨多个维度评估性能：属性、对象和场景。请注意，出于对负责任 AI 的担忧，我们不会向公众发布 Imagen Editor。另一方面，EditBench 已&lt;a href=&quot;https://imagen.research.google/editor/&quot;>;完整发布&lt;/a>;，以造福于研究社区。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 Gunjan Baid、Nicole Brichtova、Sara Mahdavi 、Kathy Meier-Hellstern、Zarana Parekh、Anusha Ramesh、Tris Warkentin、Austin Waters 和 Vijay Vasudevan 的慷慨支持。我们感谢 Igor Karpov、Isabel Kraus-Liang、Raghava Ram Pamidigantam、Mahesh Maddinala 和所有匿名人工注释者的协调以完成人工评估任务。我们感谢 Huiwen Chang、Austin Tarango 和 Douglas Eck 提供论文反馈。感谢 Erica Moreira 和 Victor Gomes 在资源协调方面的帮助。最后，感谢 DALL-E 2 的作者允许我们将他们的模型输出用于研究目的。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com /feeds/2015281937814138473/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/ imagen-editor-and-editbench-advancing.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com /feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 2015281937814138473&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot; rel =&quot;alternate&quot; title=&quot;Imagen Editor 和 EditBench：推进和评估文本引导图像修复&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www .blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#缩略图&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb 5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s72-c/Imagen%20Editor%20&amp;%20EditBench%20hero.jpg&quot; width=&quot; 72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-779195312945173416&lt;/id>;&lt;published>;2023-06-07T11:07:00.001-07:00&lt;/published>;&lt;updated>;2023-06-07T11:07:21.3 99 -07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;使用 SQuId 评估多种语言的语音合成&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Thibault Sellam 发表, 谷歌研究科学家&lt;/span>; 0OIQLAl29XK-MAh3XpOP05CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s320/SQuId%20hero.jpg&quot; style=&quot;显示：无;&quot; />; &lt;p>; 之前，我们介绍了&lt;a href=&quot;https://blog.google/technology/ai/ways-ai-is-scaling-helpful/&quot;>;1,000 种语言计划&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; 以制作语音和语言技术为目标可供全球数十亿用户使用。这一承诺的一部分涉及开发高质量的语音合成技术，这些技术建立在诸如 &lt;a href=&quot;https://ai.googleblog.com/2022/04/vdtts-visually-driven-text-to-speech. html&quot;>;VDTTS&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;，对于那些会说多种不同的语言。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7 -f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdbjCvVENZEDd6CweDk0fKiNJjGUd7cjgtfT9ddHQbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy2 1q520QkcSuA/s234/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;159&quot; data-original-width= “234” src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7-f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdBjCvVENZEDd6CweD k0fKiNJjGUd7cjgtfT9ddHQqbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy21q520QkcSuA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;br />; &lt;p>; 开发一个新模型后，必须评估它生成的语音是否准确和自然：内容必须与任务相关，发音正确，语气适当，并且不应有声学伪影，例如裂纹或信号相关噪声。这种评估是多语言语音系统发展的主要瓶颈。 &lt;/p>; &lt;p>; 评估语音合成模型质量的最流行方法是人工评估：文本转语音 (TTS) 工程师从最新模型中生成几千条语音，将它们发送给人工评估，然后几天后收到结果。此评估阶段通常涉及&lt;em>;听力测试&lt;/em>;，在此期间，数十名注释者一个接一个地聆听话语，以确定它们听起来有多自然。虽然人类在检测一段文本听起来是否自然方面仍然不败，但这个过程可能不切实际——尤其是在研究项目的早期阶段，当工程师需要快速反馈来测试和重新制定他们的方法时。人工评估是昂贵、耗时的，并且可能受到感兴趣语言的评估者可用性的限制。 &lt;/p>; &lt;p>; 另一个阻碍进步的障碍是不同的项目和机构通常使用不同的评级、平台和协议，这使得同类比较变得不可能。在这方面，语音合成技术落后于文本生成，研究人员长期以来一直使用 &lt;a href=&quot;https://aclanthology.org/P02-1040/&quot;>;BLEU&lt;/a>; 等自动指标来补充人类评估，或者，最近，&lt;a href=&quot;https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html&quot;>;BLEURT&lt;/a>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;SQuId：测量多种语言中的语音自然度&lt;/a>;”中，将在 &lt;a href=&quot; https://2023.ieeeicassp.org/&quot;>;ICASSP 2023&lt;/a>;，我们介绍了 SQuId（语音质量识别），一个 600M 参数的回归模型，描述了一段语音在多大程度上听起来很自然。 SQuId 基于 &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>;（一种由 Google 开发的预训练语音文本模型），根据超过一百万个质量评分进行微调跨越 42 种语言并在 65 种语言中进行了测试。我们演示了如何使用 SQuId 来补充人类评级以评估多种语言。这是迄今为止此类已发表的最大成果。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;用 SQuId 评估 TTS&lt;/h2>; &lt;p>; SQuId 背后的主要假设是训练回归基于先前收集的评级的模型可以为我们提供一种评估 TTS 模型质量的低成本方法。因此，该模型可以成为 TTS 研究人员评估工具箱的宝贵补充，提供近乎即时但不太准确的人工评估替代方案。 &lt;/p>; &lt;p>; SQuId 将话语作为输入和一个可选的语言环境标记（即语言的本地化变体，例如“巴西葡萄牙语”或“英国英语”）。它返回 1 到 5 之间的分数，表示波形听起来的自然程度，较高的值表示更自然的波形。 &lt;/p>; &lt;p>; 在内部，该模型包括三个组件：(1) 编码器，(2) 池化/回归层，以及 (3) 全连接层。首先，&lt;em>;编码器&lt;/em>;将频谱图作为输入并将其嵌入到一个较小的二维矩阵中，该矩阵包含 3,200 个大小为 1,024 的向量，其中每个向量编码一个时间步长。池化/回归层聚合向量，附加语言环境标签，并将结果馈送到返回分数的全连接层。最后，我们应用特定于应用程序的后处理来重新缩放或标准化分数，使其在 [1, 5] 范围内，这对于自然度人类评级很常见。我们用回归损失端到端地训练整个模型。 &lt;/p>; &lt;p>; 编码器是迄今为止最大和最重要的模型部分。我们使用了 &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>;，一个预先存在的 600M 参数 &lt;a href=&quot;http://www.interspeech2020.org/index. php?m=content&amp;amp;c=index&amp;amp;a=show&amp;amp;catid=418&amp;amp;id=1331&quot;>;Conformer&lt;/a>; 针对语音（51 种语言）和文本（101 种语言）进行预训练。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h 4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s475/image3.png&quot; 样式=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;226&quot; data-original-width=&quot;475&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf 82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;SQuId 模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 为了训练和评估模型，我们创建了 SQuId 语料库：包含 190 万个数据的集合对 66 种语言的话语进行评分，为 2,000 多个研究和产品 TTS 项目收集。 SQuId 语料库涵盖了多种系统，包括连接模型和神经模型，适用于广泛的用例，例如行车路线和虚拟助手。人工检查显示 SQuId 暴露于大量 TTS 错误，例如声学伪影（例如，噼啪声和爆裂声）、不正确的韵律（例如，英语中没有升调的问题）、文本规范化错误（例如，用语言表达“7” /7”作为“七除以七”而不是“七月七日”），或发音错误（例如，将“tough”表达为“toe”）。&lt;/p>; &lt;p>;训练多语言系统时出现的常见问题是训练数据可能不是对所有感兴趣的语言都是统一可用的。SQuId 也不例外。下图说明了每个语言环境的语料库大小。我们看到分布主要以美国英语为主。&lt;/p >; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQu dsSVzOyqbt1cWaLKzpadU- dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s3362/locales_blog.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;3362&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQudsSVzOyqbt1cWaLKzpadU-dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF 0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s16000/locales_blog.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;SQuId 数据集中的区域分布。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 当存在这样的变化时，我们如何才能为所有语言提供良好的性能？灵感来自 &lt;a href=&quot;https ://ai.googleblog.com/2019/10/exploring-massively-multilingual.html&quot;>;之前关于机器翻译的工作&lt;/a>;&lt;strong>;，&lt;/strong>;以及&lt;a href=&quot;https:/ /ieeexplore.ieee.org/document/6424230&quot;>;过去的工作&lt;/a>; 来自 &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02 /DNN-MultiLingual-ICASSP2013.pdf&quot;>;语音文献&lt;/a>;，我们决定为所有语言训练一个模型，而不是为每种语言使用单独的模型。假设是，如果模型足够大，则可能会发生&lt;em>;跨区域转移&lt;/em>;：由于对其他区域进行联合训练，模型在每个区域的准确度都会提高。正如我们的实验所示，跨区域设置被证明是性能的强大驱动力。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验结果&lt;/h2>; &lt;p>; 为了了解 SQuId 的整体性能，我们将其与自定义的进行比较Big-SSL-MOS 模型（在&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;论文&lt;/a>;中描述），一个受 &lt;a href=&quot;https://arxiv .org/abs/2110.02635&quot;>;MOS-SSL&lt;/a>;，最先进的 TTS 评估系统。 Big-SSL-MOS 基于 &lt;a href=&quot;https://arxiv.org/abs/2108.06209&quot;>;w2v-BERT&lt;/a>; 并在 &lt;a href=&quot;https://arxiv.org /pdf/2203.11389.pdf&quot;>;VoiceMOS&#39;22 Challenge&lt;/a>; 数据集，评估时最受欢迎的数据集。我们试验了该模型的几种变体，发现 SQuId 的准确度提高了 50.0%。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNf kx7z6odaf2hEJGSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/s1131 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;550&quot; data-original-width=&quot;1131&quot; height=&quot; 311&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNfkx7z6odaf2hEJ GSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/w640-h311/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId 与最先进的基线对比。我们使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&quot;>;Kendall Tau&lt;/a>; 衡量与人类评级的一致性，其中较高的值表示较高的准确性。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; 为了解跨区域传输的影响，我们进行了一系列消融研究。我们改变了训练集中引入的语言环境数量，并测量了对 SQuId 准确性的影响。英语在数据集中已经过多，添加语言环境的影响可以忽略不计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXS sNmDnnM5Q51fZ7kEjPCp2Ph667_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/s949/image5.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;881&quot; data-original-width=&quot;949&quot; height=&quot;371&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXSsNmDnnM5Q51fZ7kEjPCp2Ph6 67_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/w400-h371/image5.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId 在美国英语上的表现，在微调期间使用 1、8 和 42 语言环境。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;p>; 然而，跨区域传输对于大多数其他区域更有效：&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;样式 =“左边距：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbjhO- kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwibw691lLHscb277kB9RtJXd7MEL2n StMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/s1320/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;549&quot; data-original-width=&quot;1320&quot; height=&quot;266&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEhbjhO-kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwib w691lLHscb277kB9RtJXd7MEL2nStMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/w640-h266/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;SQuId 在四个选定语言环境（韩语、法语、泰语和泰米尔语）上的性能，在微调期间使用 1、8 和 42 个语言环境。对于每个语言环境，我们还提供训练集大小。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 为了将迁移推向极限，我们在训练期间保留了 24 个语言环境，并将它们专门用于测试。因此，我们测量到SQuId 可以在多大程度上处理它以前从未见过的语言。下图表明，尽管效果不统一，但跨区域传输有效。&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing= &quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizF9U8Wxz- zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGxarp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZV p0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/s823/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;823&quot; height=&quot;264&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEizF9U8Wxz-zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGx arp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZVp0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/w640-h264/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId 在四个“零样本”区域设置上的性能；在微调期间使用 1、8 和 42 个区域设置。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; 跨语言环境何时运作，如何运作？我们在&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;论文&lt;/ a>;，并表明尽管语言相似性发挥了作用（例如，巴西葡萄牙语培训有助于欧洲葡萄牙语），但它远不是唯一重要的因素。&lt;/p>; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>; 我们介绍了 SQuId，这是一个 600M 参数回归模型，它利用 SQuId 数据集和跨区域学习来评估语音质量并描述听起来多么自然。我们证明 SQuId 可以在多种语言的评估中补充人类评估者。未来的工作包括提高准确性、扩大所涵盖的语言范围以及处理新的错误类型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这篇文章的作者现在是谷歌的一部分深度思维。非常感谢本文的所有作者：Ankur Bapna、Joshua Camp、Diana Mackinnon、Ankur P. Parikh 和 Jason Riesa。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai. googleblog.com/feeds/779195312945173416/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023 /06/evaluating-speech-synthesis-in-many.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www. blogger.com/feeds/8474926331452026626/posts/default/779195312945173416&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/779195312945173416&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/evaluating-speech-synthesis-in-many. html&quot; rel=&quot;alternate&quot; title=&quot;使用 SQuId 评估多种语言的语音合成&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger .com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheQlvvXBg5s0pU2XQwb7gxDhd_sNEHcE2EifW94vv-gYgpKF8V8BG5hKqw91qF5IzbZFnp2vZr1Am0OIQLAl29XK-MAh3XpOP0 5CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s72-c/SQuId%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1995595447829405143&lt;/ id>;&lt;published>;2023-06-06T09:58:00.000-07:00&lt;/published>;&lt;updated>;2023-06-06T09:58:41.452-07:00&lt;/updated>;&lt;category scheme=&quot;http:/ /www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt; /category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;视觉字幕：使用大型语言模型以动态视觉增强视频会议&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：谷歌增强现实研究科学家 Ruofei Du 和高级研究科学家 Alex Olwal&lt; /span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYNNO0NMhCPTDsGDkVzs2FJv ITsOknz8GcO4hJ7Eds57YVXdFtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s745/VisualCaptions.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; 视频会议的最新进展通过实时字幕和噪声消除等功能显着改善了远程视频通信。然而，在各种情况下，动态视觉增强有助于更好地传达复杂和细微的信息。例如，在讨论在日本餐厅点什么时，您的朋友可以分享视觉效果，帮助您更有信心点“寿喜烧”。或者在谈论您最近去旧金山的家庭旅行时，您可能想展示一张您个人相册中的照片。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://research.google/pubs/pub52074/&quot;>;视觉字幕：通过 On-the 增强口头交流-fly Visuals&lt;/a>;”，在 &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/95817&quot;>;ACM CHI 2023&lt;/a>; 上展示，我们介绍了一个系统使用口头提示通过实时视觉增强同步视频通信。我们微调了一个大型语言模型，以使用我们的&lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;数据集&lt;/a>;在开放式词汇对话中主动建议相关视觉效果为此目的策划。我们将 Visual Captions 开源为 &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; 项目的一部分，该项目旨在通过实时转录快速构建增强通信的原型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA 1HeOomCg2vQk61Flc7P58lAPXiiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s640/image11 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;277&quot; data-original-width=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA1HeOomCg2vQk61Flc7P58lAPX iiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;视觉字幕通过实时视觉效果促进口头交流。该系统甚至可以抵抗实时语音到文本转录中经常出现的典型错误。例如，在上下文之外，转录模型将“码头”一词误解为“对”，但 Visual Captions 仍然推荐圣莫尼卡码头的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; br />; &lt;h2>;通过动态视觉增强口头交流的设计空间&lt;/h2>; &lt;p>; 我们邀请了 10 名内部参与者，每个参与者都有不同的技术和非技术背景，包括软件工程师、研究人员、用户体验设计师、视觉艺术家、学生等讨论他们对潜在的实时视觉增强服务的特殊需求和愿望。在两个会议中，我们介绍了设想系统的低保真原型，然后是现有文本到图像系统的视频演示。这些讨论形成了一个具有八个维度的设计空间，用于实时对话的视觉增强，在下面标记为 D1 到 D8。 &lt;/p>; &lt;p>; 视觉增强可以与对话同步或异步（D1：时间），可用于表达和理解语音内容（D2：主题），并且可以使用各种不同的视觉来应用内容、视觉类型和视觉来源（D3：视觉）。这种视觉增强可能会有所不同，具体取决于会议的规模（D4：规模）以及会议是在同一地点还是远程设置（D5：空间）。这些因素也会影响视觉效果是应该私下展示、参与者之间共享还是对所有人公开（D6：隐私）。参与者还确定了他们希望在进行对话时与系统交互的不同方式（D7：启动）。例如，人们提出了不同级别的“主动性”，这表示用户希望模型采取主动性的程度。最后，参与者设想了不同的交互方法，例如，使用语音或手势进行输入。 （D8：互动）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hp tVhSDF4gwBpPY3Ed6m6cMGzzT358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s1999/image6 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1547&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hptVhSDF4gwBpPY3Ed6m6cMGzzT 358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用动态视觉增强口头交流的设计空间。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 由这个初始通知反馈，我们设计视觉字幕专注于生成语义相关的&lt;em>;视觉内容&lt;/em>;、&lt;em>;类型&lt;/em>;和&lt;em>;源&lt;/em>;的&lt;em>;同步&lt;/em>;视觉效果.虽然这些最初的探索性会议的参与者参与的是一对一的远程对话，但视觉字幕在野外的部署通常是一对多的（例如，一个人向观众做演示）和多对多的-许多场景（例如，会议中多人之间的讨论）。 &lt;/p>; &lt;p>; 因为最能补充对话的视觉效果在很大程度上取决于讨论的上下文，所以我们需要一个专门用于此目的的训练集。因此，我们收集了 1595 个四元组的数据集，包括&lt;em>;语言 (1)&lt;/em>;、&lt;em>;视觉内容 (2)&lt;/em>;、&lt;em>;类型 (3)&lt;/em>; 和 &lt;em >;source (4)&lt;/em>; 跨越各种语境，包括日常对话、讲座和旅游指南。例如，“我很想看到它！”对应视觉内容“笑脸”、视觉类型“emoji”、视觉来源“大众搜索”。 “她有没有告诉你我们去墨西哥旅行的事？”对应视觉内容“墨西哥旅行一张照片”、视觉类型“照片”、视觉来源“个人相册”。我们为研究社区公开发布了这个&lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K 数据集&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;视觉意图预测模型&lt;/h2>; &lt;p>; 为了预测哪些视觉可以补充对话，我们使用 VC1.5K 数据集训练了一个基于大型语言模型的视觉意图预测模型.对于训练，我们将每个视觉意图解析为“&lt;code style=&quot;font-size: small;&quot;>;&lt;Visual Type>; of &lt;Visual Content>; from &lt;Visual Source>;&lt;/code>;”的格式。 &lt;/p>; &lt;pre class=&quot;prettyprint&quot; style=&quot;font-size: small; margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;{&quot;prompt&quot;: &quot;&lt;;前两句>; →&quot;, &quot;completion&quot;: &quot;&lt;Visual Type 1>; of &quot;&lt;Visual Type 1>;;来自&quot;&lt;Visual Source 1>;; &lt;Visual Type 2>; of &quot;&lt;Visual Type 2>;;来自 &quot;&amp;lt;Visual Source 2&amp;gt;; ... \𝑛&quot;} &lt;/pre>; &lt;p>; 使用这种格式，该系统可以处理开放式词汇对话并根据上下文预测视觉内容、视觉源和视觉类型。有趣的是，我们发现它优于基于关键字的方法，后者无法处理开放式词汇示例，例如“你的艾米阿姨将在本周六拜访”，并且无法建议相关的视觉类型或视觉来源。&lt;/p>; &lt;table align= &quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;文本对齐：居中；&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWW CNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevobH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s2552/vc-examples.png&quot; style=&quot;margin-left: auto; margin-右：自动；”>;&lt;img border=&quot;0&quot; data-original-height=&quot;1900&quot; data-original-width=&quot;2552&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWWCNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevo视觉示例我们的模型的意图预测。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们使用 VC1.5K 数据集中的 1276 (80%) 个示例来微调大型语言模型和其余的319 (20%) 个例子作为测试数据。我们使用令牌准确度指标衡量微调模型的性能，即模型正确预测的批次中令牌的百分比。在训练期间，我们的模型达到了 97% 的训练标记准确率和 87% 的验证标记准确率。 &lt;/p>; &lt;br />; &lt;h2>;性能&lt;/h2>; &lt;p>; 为了评估经过训练的视觉字幕模型的实用性，我们邀请了 89 名参与者执行 846 项任务。他们被要求对六个定性陈述提供“1 - 强烈不同意”到“7 - 强烈同意”的反馈。大多数参与者更喜欢在对话中看到视觉效果（Q1，83% ≥ 5 – 有点同意）。此外，他们认为显示的视觉效果有用且信息丰富（Q2，82% ≥ 5-有点同意），高质量（Q3，82% ≥ 5-有点同意），并且与原始演讲相关（Q4，84% ≥ 5 – 有点同意）。参与者还发现预测的视觉类型（Q5，87% ≥ 5-有点同意）和视觉来源（Q6，86% ≥ 5-有点同意）在给定相应对话的上下文的情况下是准确的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9 OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsUVfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s1828/image2.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1828&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsU VfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;研究参与者对视觉预测模型的技术评估结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 有了这个经过微调的视觉意图预测模型，我们在 &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; 平台上开发了 Visual Captions，它可以直接在视频会议平台的摄像头流上添加新的交互式小部件，例如&lt;a href=&quot;https://apps.google.com/meet/&quot;>;Google Meet&lt;/a>;。如下图系统工作流程所示，Visual Captions 会自动捕获用户的语音，检索最后的句子，每 100 毫秒将其输入视觉意图预测模型，检索相关视觉效果，然后实时建议视觉效果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSL y_XLW3_l6NAtJJ-tkrxOYrL_kuW2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s1756/image7.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;662&quot; data-original-width=&quot;1756&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSLy_XLW3_l6NAtJJ-tkrxOYrL_ku W2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Visual Captions 的系统工作流程。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Visual Captions 在建议视觉效果时提供三个级别的主动性：&lt; /p>; &lt;ul>; &lt;li>;&lt;em>;自动显示&lt;/em>;（高主动性）：系统自动搜索并向所有会议参与者公开显示视觉效果。无需用户交互。 &lt;/li>;&lt;li>;&lt;em>;自动建议&lt;/em>;（中等主动性）：建议的视觉效果显示在私有滚动视图中。然后用户单击视觉对象以公开显示它。在这种模式下，系统主动推荐视觉效果，但用户决定何时显示什么。 &lt;/li>;&lt;li>;&lt;em>;On-demand-suggest &lt;/em>;（低主动性）：系统只会在用户按下空格键时建议视觉效果。 &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;定量和定性评估：用户研究&lt;/h2>; &lt;p>; 我们在受控实验室研究中评估了视觉字幕 (&lt;em>;n &lt;/em>;= 26 ) 和野外部署研究 (&lt;em>;n&lt;/em>; = 10)。参与者发现，实时视觉效果有助于解释不熟悉的概念、解决语言歧义并使对话更具吸引力，从而促进现场对话。参与者还报告了在现场与系统交互的不同偏好，并且在不同的社交场景中更喜欢不同程度的主动性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4 Li4mjiAeqyjfqgdB0GhfkB96_sZcYjApSk9Cj9yHWyINESkK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s1999/image3.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;590&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4Li4mjiAeqyjfqgdB0GhfkB96_sZc YjApSk9Cj9yHWyINEskK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;参与者的&lt;a href=&quot;https://en.wikipedia.org/wiki/NASA-TLX&quot;>;任务负荷指数&lt;/a>;和&lt;a href=&quot;https:// en.wikipedia.org/wiki/Likert_scale&quot;>;李克特量表&lt;/a>; 对四个没有视觉字幕（“无 VC”）和三种视觉字幕模式的对话的评分（从 1 - 强烈不同意到 7 - 强烈同意）：自动-显示、自动建议和按需建议。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论和未来方向&lt;/h2>; &lt;p>; 这项工作提出一个实时视觉增强语言交流的系统，称为 Visual Captions，该系统使用从 246 名参与者收集的 1595 个视觉意图数据集进行训练，涵盖 15 个主题类别。我们向研究社区公开发布训练数据集 &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K&lt;/a>; 以支持该领域的进一步研究。我们还在 &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; 中部署了视觉字幕，它通过转录会议和增强摄像头视频流来促进 Google Meet 中的视频会议。 &lt;/p>; &lt;p>; Visual Captions 代表了通过实时视觉效果增强口头交流的重要一步。通过了解视觉提示在日常对话中的重要性，我们可以创建更有效的沟通工具并改善人们的联系方式。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是谷歌多个团队的合作成果。该项目的主要贡献者包括 Xingyu “Bruce” Liu、Vladimir Kirilyuk、Xiuxiu Yuan、Peggy Chi、Alex Olwal 和 Ruofei Du。&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;我们要感谢Jason Mayes、Max Spear、Na Li、Jun Zhang、Jing Jin、Yuan Ren、Adarsh Kowdle、Ping Yu、Darcy Philippon 和 Ezgi Oztelcan 等 ARChat 团队提供帮助的人。我们还要感谢许多与我们进行过富有洞察力讨论的人以及对手稿提供反馈的人，包括 Eric Turner、Yinda Zhang、Feitong Tan、Danhang Tang 和 Shahram Izadi。我们还要感谢我们的 CHI 审稿人提供的富有洞察力的反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1995595447829405143/comments/default&quot; rel =&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/19955954447829405143 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1995595447829405143&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language.html&quot; rel=&quot;alternate&quot; title=&quot;视觉字幕：使用大型语言模型通过动态视觉增强视频会议&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt; /uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:// img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYNNO0NMhCPTDsGDkVzs2FJvITsOknz8GcO4hJ7Eds57YVXd FtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s72-c/VisualCaptions.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5739375622760902222&lt;/id>;&lt;published>;2023-06-02T10: 02:00.000-07:00&lt;/published>;&lt;updated>;2023-06-02T10:02:37.777-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns# &quot; term=&quot;自动语音识别&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模态学习&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;AVFormer：将视觉注入零镜头 AV-ASR 的冻结语音模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由研究科学家 Arsha Nagrani 和 Paul Hongsuck Seo 发表, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIv3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s1150/AVFormer.png&quot; style=&quot;display: none ;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;Automatic speech recognition&lt;/a>; (ASR) is a well-established technology that is widely adopted for various applications such as conference calls, streamed video transcription and voice commands. While the challenges for this technology are centered around noisy &lt;em>;audio&lt;/em>; inputs, the &lt;em>;visual&lt;/em>; stream in multimodal videos (eg, TV, online edited videos) can provide strong cues for improving the robustness of ASR systems — this is called audiovisual ASR (AV-ASR). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Although lip motion can provide strong signals for speech recognition and is the most common area of focus for AV-ASR, the mouth is often not directly visible in &lt;em>;videos in the wild&lt;/em>; (eg, due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Egocentric_vision&quot;>;egocentric viewpoints&lt;/a>;, face coverings, and low resolution) and therefore, a new emerging area of research is &lt;em>;unconstrained&lt;/em>; AV-ASR (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.07684&quot;>;AVATAR&lt;/a>;), which investigates the contribution of entire visual frames, and not just the mouth region. &lt;/p>; &lt;p>; Building audiovisual datasets for training AV-ASR models, however, is challenging. Datasets such as &lt;a href=&quot;https://srvk.github.io/how2-dataset/&quot;>;How2&lt;/a>; and &lt;a href=&quot;https://gabeur.github.io/avatar-visspeech&quot;>;VisSpeech&lt;/a>; have been created from instructional videos online, but they are small in size. In contrast, the models themselves are typically large and consist of both visual and audio encoders, and so they tend to overfit on these small datasets. Nonetheless, there have been a number of recently released large-scale audio-only models that are heavily optimized via large-scale training on massive &lt;em>;audio-only&lt;/em>; data obtained from audio books, such as &lt;a href=&quot;https://github.com/facebookresearch/libri-light&quot;>;LibriLight&lt;/a>; and &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;. These models contain billions of parameters, are readily available, and show strong generalization across domains. &lt;/p>; &lt;p>; With the above challenges in mind, in “&lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>;”, we present a simple method for augmenting existing large-scale audio-only models with visual information, at the same time performing lightweight domain adaptation. AVFormer injects visual embeddings into a frozen ASR model (similar to how Flamingo &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;injects visual information&lt;/a>; into large language models for vision-text tasks) using lightweight trainable adaptors that can be trained on a small amount of weakly labeled video data with minimum additional training time and parameters. We also introduce a simple curriculum scheme during training, which we show is crucial to enable the model to jointly process audio and visual information effectively. The resulting AVFormer model achieves state-of-the-art zero-shot performance on three different AV-ASR benchmarks (How2, VisSpeech and &lt;a href=&quot;https://ego4d-data.org/&quot;>;Ego4D&lt;/a>;), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (ie, &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s958/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;286&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Unconstrained audiovisual speech recognition. We inject vision into a frozen speech model (&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;BEST-RQ&lt;/a>;, in grey) for zero-shot audiovisual ASR via lightweight modules to create a parameter- and data-efficient model called AVFormer (blue). The visual context can provide helpful clues for robust speech recognition especially when the audio signal is noisy (the visual loaf of bread helps correct the audio-only mistake “clove” to “loaf” in the generated transcript).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Injecting vision using lightweight modules&lt;/h2>; &lt;p>; Our goal is to add visual understanding capabilities to an existing audio-only ASR model while maintaining its generalization performance to various domains (both AV and audio-only domains). &lt;/p>; &lt;p>; To achieve this, we augment an existing state-of-the-art ASR model (&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;Best-RQ&lt;/a>;) with the following two components: (i) linear visual projector and (ii) lightweight adapters. The former projects visual features in the audio token embedding space. This process allows the model to properly connect separately pre-trained visual feature and audio input token representations. The latter then minimally modifies the model to add understanding of multimodal inputs from videos. We then train these additional modules on unlabeled web videos from the &lt;a href=&quot;https://www.di.ens.fr/willow/research/howto100m/&quot;>;HowTo100M dataset&lt;/a>;, along with the outputs of an ASR model as pseudo ground truth, while keeping the rest of the Best-RQ model frozen. Such lightweight modules enable data-efficiency and strong generalization of performance. &lt;/p>; &lt;p>; We evaluated our extended model on AV-ASR benchmarks in a zero-shot setting, where the model is never trained on a manually annotated AV-ASR dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Curriculum learning for vision injection&lt;/h2>; &lt;p>; After the initial evaluation, we discovered empirically that with a naïve single round of joint training, the model struggles to learn both the adapters and the visual projectors in one go. To mitigate this issue, we introduced a two-phase &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/8403835/&quot;>;curriculum learning strategy&lt;/a>; that decouples these two factors — domain adaptation and visual feature integration — and trains the network in a sequential manner. In the first phase, the adapter parameters are optimized without feeding visual tokens at all. Once the adapters are trained, we add the visual tokens and train the visual projection layers alone in the second phase while the trained adapters are kept frozen. &lt;/p>; &lt;p>; The first stage focuses on audio domain adaptation. By the second phase, the adapters are completely frozen and the visual projector must simply learn to generate visual prompts that project the visual tokens into the audio space. In this way, our curriculum learning strategy allows the model to incorporate visual inputs as well as adapt to new audio domains in AV-ASR benchmarks. We apply each phase just once, as an iterative application of alternating phases leads to performance degradation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJj6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s1318/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;1318&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJj6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall architecture and training procedure for AVFormer. The architecture consists of a frozen &lt;a href=&quot;https://arxiv.org/pdf/2005.08100.pdf&quot;>;Conformer&lt;/a>; encoder-decoder model, and a frozen &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; encoder (frozen layers shown in gray with a lock symbol), in conjunction with two lightweight trainable modules - (i) visual projection layer (orange) and bottleneck adapters (blue) to enable multimodal domain adaptation. We propose a two-phase curriculum learning strategy: the adapters (blue) are first trained without any visual tokens, after which the visual projection layer (orange) is tuned while all the other parts are kept frozen.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The plots below show that without curriculum learning, our AV-ASR model is worse than the audio-only baseline across all datasets, with the gap increasing as more visual tokens are added. In contrast, when the proposed two-phase curriculum is applied, our AV-ASR model performs significantly better than the baseline audio-only model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s1374/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1053&quot; data-original-width=&quot;1374&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effects of curriculum learning. Red and blue lines are for audiovisual models and are shown on 3 datasets in the zero-shot setting (lower &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;WER&lt;/a>; % is better). Using the curriculum helps on all 3 datasets (for How2 (a) and Ego4D (c) it is crucial for outperforming audio-only performance). Performance improves up until 4 visual tokens, at which point it saturates.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results in zero-shot AV-ASR&lt;/h2>; &lt;p>; We compare AVFormer to BEST-RQ, the audio version of our model, and AVATAR, the state of the art in AV-ASR, for zero-shot performance on the three AV-ASR benchmarks: How2, VisSpeech and Ego4D. AVFormer outperforms AVATAR and BEST-RQ on all, even outperforming both AVATAR and BEST-RQ when they are trained on LibriSpeech and the full set of HowTo100M. This is notable because for BEST-RQ, this involves training 600M parameters, while AVFormer only trains 4M parameters and therefore requires only a small fraction of the training dataset (5% of HowTo100M). Moreover, we also evaluate performance on LibriSpeech, which is audio-only, and AVFormer outperforms both baselines. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s3475/AVFormer_results%20(1).png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2129&quot; data-original-width=&quot;3475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s16000/AVFormer_results%20(1).png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison to state-of-the-art methods for zero-shot performance across different AV-ASR datasets. We also show performances on LibriSpeech which is audio-only. Results are reported as WER % (lower is better). AVATAR and BEST-RQ are finetuned end-to-end (all parameters) on HowTo100M whereas AVFormer works effectively even with 5% of the dataset thanks to the small set of finetuned parameters.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduce AVFormer, a lightweight method for adapting existing, frozen state-of-the-art ASR models for AV-ASR. Our approach is practical and efficient, and achieves impressive zero-shot performance. As ASR models get larger and larger, tuning the entire parameter set of pre-trained models becomes impractical (even more so for different domains). Our method seamlessly allows both domain transfer and visual input mixing in the same, parameter efficient model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Paul Hongsuck Seo, Arsha Nagrani and Cordelia Schmid.&lt;/em>; &lt;/p>; &lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5739375622760902222/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot; rel=&quot;alternate&quot; title=&quot;AVFormer: Injecting vision into frozen speech models for zero-shot AV-ASR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIv3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s72-c/AVFormer.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-791000644276225005&lt;/id>;&lt;published>;2023-06-01T10:25:00.000-07:00&lt;/published>;&lt;updated>;2023-06-01T10:25:21.235-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Retrieval-augmented visual-language pre-training&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s320/REVEAL%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Large-scale models, such as &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, &lt;a href=&quot;https://openai.com/research/language-models-are-few-shot-learners&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, have demonstrated the ability to store substantial amounts of knowledge when scaled to tens of billions of parameters and trained on large text and image datasets. These models achieve state-of-the-art results on downstream tasks, such as image captioning, visual question answering and open vocabulary recognition. Despite such achievements, these models require a massive volume of data for training and end up with a tremendous number of parameters (billions in many cases), resulting in significant computational requirements. Moreover, the data used to train these models can become outdated, requiring re-training every time the world&#39;s knowledge is updated. For example, a model trained just two years ago might yield outdated information about the current president of the United States. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the fields of natural language processing (&lt;a href=&quot;https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens&quot;>;RETRO&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>;) and computer vision (&lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>;), researchers have attempted to address these challenges using retrieval-augmented models. Typically, these models use a backbone that is able to process a single modality at a time, eg, only text or only images, to encode and retrieve information from a knowledge corpus. However, these retrieval-augmented models are unable to leverage all available modalities in a query and knowledge corpora, and may not find the information that is most helpful for generating the model&#39;s output. &lt;/p>; &lt;p>; To address these issues, in “&lt;a href=&quot;https://arxiv.org/abs/2212.05221&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory&lt;/a>;”, to appear at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce a visual-language model that learns to utilize a multi-source multi-modal “memory” to answer knowledge-intensive queries. REVEAL employs &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;neural representation learning&lt;/a>; to encode and convert diverse knowledge sources into a memory structure consisting of key-value pairs. The keys serve as indices for the memory items, while the corresponding values store pertinent information about those items. During training, REVEAL learns the key embeddings, value tokens, and the ability to retrieve information from this memory to address knowledge-intensive queries. This approach allows the model parameters to focus on reasoning about the query, rather than being dedicated to memorization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s1440/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We augment a visual-language model with the ability to retrieve multiple knowledge entries from a diverse set of knowledge sources, which helps generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Memory construction from multimodal knowledge corpora&lt;/h2>; &lt;p>; Our approach is similar to &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>; in that we precompute key and value embeddings of knowledge items from different sources and index them in a unified knowledge memory, where each knowledge item is encoded into a key-value pair. Each key is a &lt;em>;d&lt;/em>;-dimensional embedding vector, while each value is a sequence of token embeddings representing the knowledge item in more detail. In contrast to previous work, REVEAL leverages a diverse set of multimodal knowledge corpora, including the &lt;a href=&quot;https://research.google/pubs/pub42240/&quot;>;WikiData knowledge graph&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2103.01913&quot;>;Wikipedia passages and images&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2102.08981&quot;>;web image-text pairs&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;>;visual question answering data&lt;/a>;. Each knowledge item could be text, an image, a combination of both (eg, pages in Wikipedia) or a relationship or attribute from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; (eg, Barack Obama is 6&#39; 2” tall). During training, we continuously re-compute the memory key and value embeddings as the model parameters get updated. We update the memory asynchronously at every thousand training steps. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Scaling memory using compression &lt;/h3>; &lt;p>; A naïve solution for encoding a memory value is to keep the whole sequence of tokens for each knowledge item. Then, the model could fuse the input query and the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;>;top-k&lt;/a>; retrieved memory values by concatenating all their tokens together and feeding them into a &lt;a href=&quot;https://huggingface.co/blog/encoder-decoder&quot;>;transformer encoder-decoder&lt;/a>; pipeline. This approach has two issues: (1) storing hundreds of millions of knowledge items in memory is impractical if each memory value consists of hundreds of tokens and (2) the transformer encoder has a quadratic complexity with respect to the total number of tokens times &lt;em>;k&lt;/em>; for &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>;. Therefore, we propose to use the &lt;a href=&quot;https://www.deepmind.com/publications/perceiver-general-perception-with-iterative-attention&quot;>;Perceiver architecture&lt;/a>; to encode and compress knowledge items. The Perceiver model uses a transformer decoder to compress the full token sequence into an arbitrary length. This lets us retrieve top-&lt;em>;k&lt;/em>; memory entries for &lt;em>;k&lt;/em>; as large as a hundred. &lt;/p>; &lt;p>; The following figure illustrates the procedure of constructing the memory key-value pairs. Each knowledge item is processed through a multi-modal visual-language encoder, resulting in a sequence of image and text tokens. The key head then transforms these tokens into a compact embedding vector. The value head (perceiver) condenses these tokens into fewer ones, retaining the pertinent information about the knowledge item within them. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;776&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We encode the knowledge entries from different corpora into unified key and value embedding pairs, where the keys are used to index the memory and values contain information about the entries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-scale pre-training on image-text pairs&lt;/h2>; &lt;p>; To train the REVEAL model, we begin with the large-scale corpus, collected from the public Web with three billion image alt-text caption pairs, introduced in &lt;a href=&quot;https://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html&quot;>;LiT&lt;/a>;. Since the dataset is noisy, we add a filter to remove data points with captions shorter than 50 characters, which yields roughly 1.3 billion image caption pairs. We then take these pairs, combined with the text generation objective used in &lt;a href=&quot;https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html&quot;>;SimVLM&lt;/a>;, to train REVEAL. Given an image-text example, we randomly sample a prefix containing the first few tokens of the text. We feed the text prefix and image to the model as input with the objective of generating the rest of the text as output. The training goal is to condition the prefix and autoregressively generate the remaining text sequence. &lt;/p>; &lt;p>; To train all components of the REVEAL model end-to-end, we need to &lt;a href=&quot;https://arxiv.org/abs/1910.08475&quot;>;warm start&lt;/a>; the model to a good state (setting initial values to model parameters). Otherwise, if we were to start with random weights (cold-start), the retriever would often return irrelevant memory items that would never generate useful training signals. To avoid this cold-start problem, we construct an initial retrieval dataset with pseudo–ground-truth knowledge to give the pre-training a reasonable head start. &lt;/p>; &lt;p>; We create a modified version of the &lt;a href=&quot;https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html&quot;>;WIT&lt;/a>; dataset for this purpose. Each image-caption pair in WIT also comes with a corresponding Wikipedia passage (words surrounding the text). We put together the surrounding passage with the query image and use it as the pseudo ground-truth knowledge that corresponds to the input query. The passage provides rich information about the image and caption, which is useful for initializing the model. &lt;/p>; &lt;p>; To prevent the model from relying on low-level image features for retrieval, we apply random data augmentation to the input query image. Given this modified dataset that contains pseudo-retrieval ground-truth, we train the query and memory key embeddings to warm start the model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;REVEAL workflow&lt;/h2>; &lt;p>; The overall workflow of REVEAL consists of four primary steps. First, REVEAL encodes a multimodal input into a sequence of token embeddings along with a condensed query embedding. Then, the model translates each multi-source knowledge entry into unified pairs of key and value embeddings, with the key being utilized for memory indexing and the value encompassing the entire information about the entry. Next, REVEAL retrieves the top-&lt;em>;k&lt;/em>; most related knowledge pieces from multiple knowledge sources, returns the pre-processed value embeddings stored in memory, and re-encodes the values. Finally, REVEAL fuses the top-&lt;em>;k&lt;/em>; knowledge pieces through an attentive knowledge fusion layer by injecting the retrieval score (dot product between query and key embeddings) as a prior during attention calculation. This structure is instrumental in enabling the memory, encoder, retriever and the generator to be concurrently trained in an end-to-end fashion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1003&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall workflow of REVEAL.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate REVEAL on knowledge-based visual question answering tasks using &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; and &lt;a href=&quot;https://allenai.org/project/a-okvqa/home&quot;>;A-OKVQA&lt;/a>; datasets. We fine-tune our pre-trained model on the VQA tasks using the same generative objective where the model takes in an image-question pair as input and generates the text answer as output. We demonstrate that REVEAL achieves better results on the A-OKVQA dataset than earlier attempts that incorporate a fixed knowledge or the works that utilize large language models (eg, GPT-3) as an implicit source of knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. REVEAL achieves higher accuracy in comparison to previous works including &lt;a href=&quot;https://arxiv.org/abs/1908.02265&quot;>;ViLBERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1908.07490&quot;>;LXMERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.09734&quot;>;ClipCap&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>; and &lt;a href=&quot;https://prior.allenai.org/projects/gpv2&quot;>;GPV-2&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also evaluate REVEAL on the image captioning benchmarks using &lt;a href=&quot;https://cocodataset.org/#home&quot;>;MSCOCO&lt;/a>; and &lt;a href=&quot;https://nocaps.org/&quot;>;NoCaps&lt;/a>; dataset. We directly fine-tune REVEAL on the MSCOCO training split via the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;>;cross-entropy&lt;/a>; generative objective. We measure our performance on the MSCOCO test split and NoCaps evaluation set using the &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr&lt;/a>; metric, which is based on the idea that good captions should be similar to reference captions in terms of word choice, grammar, meaning, and content. Our results on MSCOCO caption and NoCaps datasets are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Image Captioning results on MSCOCO and NoCaps using the CIDEr metric. REVEAL achieves a higher score in comparison to &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2108.10904&quot;>;SimVLM&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/image-text-pre-training-with.html&quot;>;CoCa&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Below we show a couple of qualitative examples of how REVEAL retrieves relevant documents to answer visual questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s931/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;338&quot; data-original-width=&quot;931&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REVEAL can use knowledge from different sources to correctly answer the question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an end-to-end retrieval-augmented visual language (REVEAL) model, which contains a knowledge retriever that learns to utilize a diverse set of knowledge sources with different modalities. We train REVEAL on a massive image-text corpus with four diverse knowledge corpora, and achieve state-of-the-art results on knowledge-intensive visual question answering and image caption tasks. In the future we would like to explore the ability of this model for attribution, and apply it to a broader class of multimodal tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross and Alireza Fathi.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/791000644276225005/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot; rel=&quot;alternate&quot; title=&quot;Retrieval-augmented visual-language pre-training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s72-c/REVEAL%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8031728070441281505&lt;/id>;&lt;published>;2023-05-31T10:13:00.002-07:00&lt;/published>;&lt;updated>;2023-06-05T13:58:42.218-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Large sequence models for software development activities&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Petros Maniatis and Daniel Tarlow, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s1200/DIDACT-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Software isn&#39;t created in one dramatic step. It improves bit by bit, one little step at a time — editing, running unit tests, fixing build errors, addressing code reviews, editing some more, appeasing &lt;a href=&quot;https://en.wikipedia.org/wiki/Lint_(software)&quot;>;linters&lt;/a>;, and fixing more errors — until finally it becomes good enough to merge into a code repository. Software engineering isn&#39;t an isolated process, but a dialogue among human developers, code reviewers, bug reporters, software architects and tools, such as compilers, unit tests, linters and static analyzers. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we describe DIDACT (​​Dynamic Integrated Developer ACTivity), which is a methodology for training large machine learning (ML) models for software development. The novelty of DIDACT is that it uses &lt;em>;the process of software development &lt;/em>;as the source of training data for the model, rather than just &lt;em>;the polished end state &lt;/em>;of that process, the finished code. By exposing the model to the contexts that developers see as they work, paired with the actions they take in response, the model learns about the dynamics of software development and is more aligned with how developers spend their time. We leverage instrumentation of Google&#39;s software development to scale up the quantity and diversity of developer-activity data beyond previous works. Results are extremely promising along two dimensions: usefulness to professional software developers, and as a potential basis for imbuing ML models with general software development skills. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s1475/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;687&quot; data-original-width=&quot;1475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DIDACT is a multi-task model trained on development activities that include editing, debugging, repair, and code review.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built and deployed internally three DIDACT tools, &lt;a href=&quot;https://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot;>;Comment Resolution&lt;/a>; (which we recently announced), Build Repair, and Tip Prediction, each integrated at different stages of the development workflow. All three of these tools received enthusiastic feedback from thousands of internal developers. We see this as the ultimate test of usefulness: do professional developers, who are often experts on the code base and who have carefully honed workflows, leverage the tools to improve their productivity? &lt;/p>; &lt;p>; Perhaps most excitingly, we demonstrate how DIDACT is a first step towards a general-purpose developer-assistance agent. We show that the trained model can be used in a variety of surprising ways, via prompting with prefixes of developer activities, and by chaining together multiple predictions to roll out longer activity trajectories. We believe DIDACT paves a promising path towards developing agents that can generally assist across the software development process. &lt;/p>; &lt;br />; &lt;h2>;A treasure trove of data about the software engineering process&lt;/h2>; &lt;p>; Google&#39;s software engineering toolchains store every operation related to code as a log of interactions among tools and developers, and have done so for decades. In principle, one could use this record to replay in detail the key episodes in the “software engineering video” of how Google&#39;s codebase came to be, step-by-step — one code edit, compilation, comment, variable rename, etc., at a time. &lt;/p>; &lt;p>; Google code lives in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monorepo&quot;>;monorepo&lt;/a>;, a single repository of code for all tools and systems. A software developer typically experiments with code changes in a local &lt;a href=&quot;https://en.wikipedia.org/wiki/Copy-on-write&quot;>;copy-on-write&lt;/a>; workspace managed by a system called &lt;a href=&quot;https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext&quot;>;Clients in the Cloud&lt;/a>; (CitC). When the developer is ready to package a set of code changes together for a specific purpose (eg, fixing a bug), they create a changelist (CL) in &lt;a href=&quot;https://abseil.io/resources/swe-book/html/ch19.html&quot;>;Critique&lt;/a>;, Google&#39;s code-review system. As with other types of code-review systems, the developer engages in a dialog with a peer reviewer about functionality and style. The developer edits their CL to address reviewer comments as the dialog progresses. Eventually, the reviewer declares “LGTM!” (“looks good to me”), and the CL is merged into the code repository. &lt;/p>; &lt;p>; Of course, in addition to a dialog with the code reviewer, the developer also maintains a “dialog” of sorts with a plethora of other software engineering tools, such as the compiler, the testing framework, linters, static analyzers, fuzzers, etc. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/s1877/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1751&quot; data-original-width=&quot;1877&quot; height=&quot;597&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/w640-h597/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the intricate web of activities involved in developing software: small actions by the developer, interactions with a code reviewer, and invocations of tools such as compilers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A multi-task model for software engineering&lt;/h2>; &lt;p>; DIDACT utilizes interactions among engineers and tools to power ML models that assist Google developers, by suggesting or enhancing actions developers take — in context — while pursuing their software-engineering tasks. To do that, we have defined a number of tasks about individual developer activities: repairing a broken build, predicting a code-review comment, addressing a code-review comment, renaming a variable, editing a file, etc. We use a common formalism for each activity: it takes some &lt;em>;State&lt;/em>; (a code file), some &lt;em>;Intent&lt;/em>; (annotations specific to the activity, such as code-review comments or compiler errors), and produces an &lt;em>;Action&lt;/em>; (the operation taken to address the task). This Action is like a mini programming language, and can be extended for newly added activities. It covers things like editing, adding comments, renaming variables, marking up code with errors, etc. We call this language &lt;em>;DevScript&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s1200/DIDACT.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;762&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s16000/DIDACT.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The DIDACT model is prompted with a task, code snippets, and annotations related to that task, and produces development actions, eg, edits or comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This state-intent-action formalism enables us to capture many different tasks in a general way. What&#39;s more, DevScript is a concise way to express complex actions, without the need to output the whole state (the original code) as it would be after the action takes place; this makes the model more efficient and more interpretable. For example, a rename might touch a file in dozens of places, but a model can predict a single rename action. &lt;/p>; &lt;br />; &lt;h2>;An ML peer programmer&lt;/h2>; &lt;p>; DIDACT does a good job on individual assistive tasks. For example, below we show DIDACT doing code clean-up after functionality is mostly done. It looks at the code along with some final comments by the code reviewer (marked with “human” in the animation), and predicts edits to address those comments (rendered as a &lt;em>;diff&lt;/em>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/s897/DIDACT2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;761&quot; data-original-width=&quot;897&quot; height=&quot;543&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/w640-h543/DIDACT2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an initial snippet of code and the comments that a code reviewer attached to that snippet, the Pre-Submit Cleanup task of DIDACT produces edits (insertions and deletions of text) that address those comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The multimodal nature of DIDACT also gives rise to some surprising capabilities, reminiscent of &lt;a href=&quot;https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html&quot;>;behaviors emerging with scale&lt;/a>;. One such capability is &lt;em>;history augmentation&lt;/em>;, which can be enabled via prompting. Knowing what the developer did recently enables the model to make a better guess about what the developer should do next. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiP3TuqEx0gQZS_TtS2xhih97lUs25Qdj-v7TAfSaUloS4MMt7-SssJpvmt-xnk8DgB7TcAI987N7pmJv1uNo1d_UqofKAAyZlNINx84LesXSQ8heDvV6Ju3NRbvu1Ua3Ui1jc4CUjevwmFkDqLIBgMkfrcSUlHwEpNm6yvgqLAVjRjjXOHbi3gxw1viQ/s1052/DIDACT3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;632&quot; data-original-width=&quot;1052&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiP3TuqEx0gQZS_TtS2xhih97lUs25Qdj-v7TAfSaUloS4MMt7-SssJpvmt-xnk8DgB7TcAI987N7pmJv1uNo1d_UqofKAAyZlNINx84LesXSQ8heDvV6Ju3NRbvu1Ua3Ui1jc4CUjevwmFkDqLIBgMkfrcSUlHwEpNm6yvgqLAVjRjjXOHbi3gxw1viQ/s16000/DIDACT3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of history-augmented code completion in action.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A powerful such task exemplifying this capability is &lt;em>;history-augmented code completion&lt;/em>;. In the figure below, the developer adds a new function parameter (1), and moves the cursor into the documentation (2). Conditioned on the history of developer edits and the cursor position, the model completes the line (3) by correctly predicting the docstring entry for the new parameter. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s1048/DIDACT4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1048&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s16000/DIDACT4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of edit prediction, over multiple chained iterations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In an even more powerful history-augmented task, &lt;em>;edit prediction&lt;/em>;, the model can choose where to edit next in a fashion that is historically consistent&lt;strong>;. &lt;/strong>;If the developer deletes a function parameter (1), the model can use history to correctly predict an update to the docstring (2) that removes the deleted parameter (without the human developer manually placing the cursor there) and to update a statement in the function (3) in a syntactically (and — arguably — semantically) correct way. With history, the model can unambiguously decide how to continue the “editing video” correctly. Without history, the model wouldn&#39;t know whether the missing function parameter is intentional (because the developer is in the process of a longer edit to remove it) or accidental (in which case the model should re-add it to fix the problem). &lt;/p>; &lt;p>; The model can go even further. For example, we started with a blank file and asked the model to successively predict what edits would come next until it had written a full code file. The astonishing part is that the model developed code in a step-by-step way that would seem natural&lt;strong>; &lt;/strong>;to a developer: It started by first creating a fully working skeleton with imports, flags, and a basic main function. It then incrementally added new functionality, like reading from a file and writing results, and added functionality to filter out some lines based on a user-provided regular expression, which required changes across the file, like adding new flags. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DIDACT turns Google&#39;s software development process into training demonstrations for ML developer assistants, and uses those demonstrations to train models that construct code in a step-by-step fashion, interactively with tools and code reviewers. These innovations are already powering tools enjoyed by Google developers every day. The DIDACT approach complements the great strides taken by large language models at Google and elsewhere, towards technologies that ease toil, improve productivity, and enhance the quality of work of software engineers. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a multi-year collaboration among Google Research, Google Core Systems and Experiences, and DeepMind. We would like to acknowledge our colleagues Jacob Austin, Pascal Lamblin, Pierre-Antoine Manzagol, and Daniel Zheng, who join us as the key drivers of this project. This work could not have happened without the significant and sustained contributions of our partners at Alphabet (Peter Choy, Henryk Michalewski, Subhodeep Moitra, Malgorzata Salawa, Vaibhav Tulsyan, and Manushree Vijayvergiya), as well as the many people who collected data, identified tasks, built products, strategized, evangelized, and helped us execute on the many facets of this agenda (Ankur Agarwal, Paige Bailey, Marc Brockschmidt, Rodrigo Damazio Bovendorp, Satish Chandra, Savinee Dancs,&amp;nbsp;&lt;/em>;&lt;i>;Denis Davydenko,&amp;nbsp;&lt;/i>;&lt;em>;Matt Frazier, Alexander Frömmgen, Nimesh Ghelani, Chris Gorgolewski, Chenjie Gu, Vincent Hellendoorn, Franjo Ivančić, Marko Ivanković, Emily Johnston, Luka Kalinovcic, Lera Kharatyan, Jessica Ko, Markus Kusano, Kathy Nix, Christian Perez, Sara Qu, Marc Rasi, Marcus Revaj, Ballie Sandhu, Michael Sloan, Tom Small, Gabriela Surita, Maxim Tabachnyk, David Tattersall, Sara Toth, Kevin Villela, Sara Wiltberger, and Donald Duo Zhao) and our extremely supportive leadership (Martín Abadi, Joelle Barral, Jeff Dean, Madhura Dudhgaonkar, Douglas Eck, Zoubin Ghahramani, Hugo Larochelle, Chandu Thekkath, and Niranjan Tulpule). Thank you!&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8031728070441281505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot; rel=&quot;alternate&quot; title=&quot;Large sequence models for software development activities&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s72-c/DIDACT-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4195590947404721374&lt;/id>;&lt;published>;2023-05-26T12:08:00.002-07:00&lt;/published>;&lt;updated>;2023-05-26T12:09:00.761-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Foundation models for reasoning on charts&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Julian Eisenschlos, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s2200/MatCha.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Visual language is the form of communication that relies on pictorial symbols outside of text to convey information. It is ubiquitous in our digital life in the form of iconography, infographics, tables, plots, and charts, extending to the real world in street signs, comic books, food labels, etc. For that reason, having computers better understand this type of media can help with scientific communication and discovery, accessibility, and data transparency. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While computer vision models have made tremendous progress using learning-based solutions since the advent of &lt;a href=&quot;https://ieeexplore.ieee.org/document/5206848&quot;>;ImageNet&lt;/a>;, the focus has been on natural images, where all sorts of tasks, such as &lt;a href=&quot;https://www.image-net.org/&quot;>;classification&lt;/a>;, &lt;a href=&quot;https://visualqa.org/&quot;>;visual question answering&lt;/a>; (VQA), &lt;a href=&quot;https://cocodataset.org&quot;>;captioning, detection and segmentation&lt;/a>;, have been defined, studied and in some cases advanced to reach human performance. However, visual language has not garnered a similar level of attention, possibly because of the lack of large-scale training sets in this space. But over the last few years, new academic datasets have been created with the goal of evaluating question answering systems on visual language images, like &lt;a href=&quot;https://arxiv.org/abs/1909.00997&quot;>;PlotQA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicsVQA&lt;/a>;, and &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6aQ6tqQsTgI2X3TkQUK_xIK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;898&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6aQ6tqQsTgI2X3TkQUK_xIK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example from &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA&lt;/a>;. Answering the question requires reading the information and computing the sum and the difference.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Existing models built for these tasks relied on integrating &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_character_recognition&quot;>;optical character recognition&lt;/a>; (OCR) information and their coordinates into larger pipelines but the process is error prone, slow, and generalizes poorly. The prevalence of these methods was because existing end-to-end computer vision models based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) or &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; pre-trained on natural images could not be easily adapted to visual language. But existing models are ill-prepared for the challenges in answering questions on charts, including reading the relative height of bars or the angle of slices in pie charts, understanding axis scales, correctly mapping pictograms with their legend values with colors, sizes and textures, and finally performing numerical operations with the extracted numbers. &lt;/p>; &lt;p>; In light of these challenges, we propose “&lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering&lt;/a>;”. MatCha, which stands for math and charts, is a pixels-to-text foundation model (a pre-trained model with built-in inductive biases that can be fine-tuned for multiple applications) trained on two complementary tasks: (a) chart de-rendering and (b) math reasoning. In chart de-rendering, given a plot or chart, the image-to-text model is required to generate its underlying data table or the code used to render it. For math reasoning pre-training, we pick textual numerical reasoning datasets and render the input into images, which the image-to-text model needs to decode for answers. We also propose “&lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot: One-shot visual language reasoning by plot-to-table translation&lt;/a>;”, a model built on top of MatCha for one-shot reasoning on charts via translation to tables. With these methods we surpass the previous state of the art in ChartQA by more than 20% and match the best summarization systems that have 1000 times more parameters. Both papers will be presented at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Chart de-rendering&lt;/h2>; &lt;p>; Plots and charts are usually generated by an underlying data table and a piece of code. The code defines the overall layout of the figure (eg, type, direction, color/shape scheme) and the underlying data table establishes the actual numbers and their groupings. Both the data and code are sent to a compiler/rendering engine to create the final image. To understand a chart, one needs to discover the visual patterns in the image and effectively parse and group them to extract the key information. Reversing the plot rendering process demands all such capabilities and can thus serve as an ideal pre-training task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s624/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;335&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A chart created from a table in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Airbus_A380&quot;>;Airbus A380 Wikipedia page&lt;/a>; using random plotting options. The pre-training task for MatCha consists of recovering the source table or the source code from the image.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, it is challenging to simultaneously obtain charts, their underlying data tables, and their rendering code. To collect sufficient pre-training data, we independently accumulate [chart, code] and [chart, table] pairs. For [chart, code], we crawl all GitHub IPython notebooks with appropriate licenses and extract blocks with figures. A figure and the code block right before it are saved as a [chart, code] pair. For [chart, table] pairs, we explored two sources. For the first source, synthetic data, we manually write code to convert web-crawled Wikipedia tables from the &lt;a href=&quot;https://aclanthology.org/2020.acl-main.398/&quot;>;TaPas&lt;/a>; codebase to charts. We sampled from and combined several plotting options depending on the column types. In addition, we also add [chart, table] pairs generated in PlotQA to diversify the pre-training corpus. The second source is web-crawled [chart, table] pairs. We directly use the [chart, table] pairs crawled in the ChartQA training set, containing around 20k pairs in total from four websites: &lt;a href=&quot;statista.com&quot;>;Statista&lt;/a>;, &lt;a href=&quot;https://www.pewresearch.org/&quot;>;Pew&lt;/a>;, &lt;a href=&quot;https://ourworldindata.org/&quot;>;Our World in Data&lt;/a>;, and &lt;a href=&quot;https://www.oecd.org/&quot;>;OECD&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Math reasoning&lt;/h2>; &lt;p>; We incorporate numerical reasoning knowledge into MatCha by learning math reasoning skills from textual math datasets. We use two existing textual math reasoning datasets, &lt;a href=&quot;https://openreview.net/forum?id=H1gR5iR5FX&quot;>;MATH&lt;/a>; and &lt;a href=&quot;https://aclanthology.org/N19-1246/&quot;>;DROP&lt;/a>; for pre-training. MATH is synthetically created, containing two million training examples per module (type) of questions. DROP is a reading-comprehension–style QA dataset where the input is a paragraph context and a question. &lt;/p>; &lt;p>; To solve questions in DROP, the model needs to read the paragraph, extract relevant numbers and perform numerical computation. We found both datasets to be complementary. MATH contains a large number of questions across different categories, which helps us identify math operations needed to explicitly inject into the model. DROP&#39;s reading-comprehension format resembles the typical QA format wherein models simultaneously perform information extraction and reasoning. In practice, we render inputs of both datasets into images. The model is trained to decode the answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s624/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;218&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;To improve the math reasoning skills of MatCha we incorporate examples from MATH and DROP into the pre-training objective, by rendering the input text as images.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;End-to-end results&lt;/h2>; &lt;p>; We use a &lt;a href=&quot;https://arxiv.org/abs/2210.03347&quot;>;Pix2Struct&lt;/a>; model backbone, which is an image-to-text transformer tailored for website understanding, and pre-train it with the two tasks described above. We demonstrate the strengths of MatCha by fine-tuning it on several visual language tasks — tasks involving charts and plots for question answering and summarization where no access to the underlying table is possible. MatCha surpasses previous models&#39; performance by a large margin and also outperforms the previous state of the art, which assumes access to underlying tables. &lt;/p>; &lt;p>; In the figure below, we first evaluate two baseline models that incorporate information from an &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;OCR pipeline&lt;/a>;, which until recently was the standard approach for working with charts. The first is based on &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, the second on &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;VisionTaPas&lt;/a>;. We also compare against &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI-17B&lt;/a>;, which is a large (~1000 times larger than the other models) image plus text-to-text transformer trained on a diverse set of tasks but with limited capabilities for reading text and other forms of visual language. Finally, we report the Pix2Struct and MatCha model results. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s1646/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;696&quot; data-original-width=&quot;1646&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Experimental results on two chart QA benchmarks ChartQA &amp;amp; PlotQA (using relaxed accuracy) and a chart summarization benchmark chart-to-text (using BLEU4). Matcha surpasses the state of the art by a large margin on QA, compared to larger models, and matches these larger models on summarization.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For QA datasets, we use the &lt;a href=&quot;https://github.com/google-research/pix2struct/blob/main/pix2struct/metrics.py#L81&quot;>;official relaxed accuracy metric&lt;/a>; that allows for small relative errors in numerical outputs. For chart-to-text summarization, we report &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; scores. MatCha achieves noticeably improved results compared to baselines for question answering, and comparable results to PaLI in summarization, where large size and extensive long text/captioning generation pre-training are advantageous for this kind of long-form text generation. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Derendering plus large language model chains&lt;/h2>; &lt;p>; While extremely performant for their number of parameters, particularly on extractive tasks, we observed that fine-tuned MatCha models could still struggle with end-to-end complex reasoning (eg, mathematical operations involving large numbers or multiple steps). Thus, we also propose a two-step method to tackle this: 1) a model reads a chart, then outputs the underlying table, 2) a large language model (LLM) reads this output and then tries to answer the question solely based on the textual input. &lt;/p>; &lt;p>; For the first model, we fine-tuned MatCha solely on the chart-to-table task, increasing the output sequence length to guarantee it could recover all or most of the information in the chart. &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; is the resulting model. In the second stage, any LLM (such as &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;) can be used for the task, and we can rely on the standard methods to increase performance on LLMs, for example &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;>;self-consistency&lt;/a>;. We also experimented with &lt;a href=&quot;https://arxiv.org/abs/2211.12588&quot;>;program-of-thoughts&lt;/a>; where the model produces executable Python code to offload complex computations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s622/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;622&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the DePlot+LLM method. This is a real example using &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. The blue boxes are input to the LLM and the red boxes contain the answer generated by the LLMs. We highlight some of the key reasoning steps in each answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in the example above, the DePlot model in combination with LLMs outperforms fine-tuned models by a significant margin, especially so in the human-sourced portion of ChartQA, where the questions are more natural but demand more difficult reasoning. Furthermore, DePlot+LLM can do so without access to any training data. &lt;/p>; &lt;p>; We have released the new models and code at our &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/deplot&quot;>;GitHub repo&lt;/a>;, where you can try it out yourself in colab. Checkout the papers for &lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; for more details on the experimental results. We hope that our results can benefit the research community and make the information in charts and plots more accessible to everyone. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was carried out by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen and Yasemin Altun from our &lt;a href=&quot;https://research.google/teams/language/&quot;>;Language Team&lt;/a>; as part of Fangyu&#39;s internship project. Nigel Collier from Cambridge also was a collaborator. We would like to thank Joshua Howland, Alex Polozov, Shrestha Basu Mallick, Massimo Nicosia and William Cohen for their valuable comments and suggestions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4195590947404721374/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html&quot; rel=&quot;alternate&quot; title=&quot;Foundation models for reasoning on charts&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s72-c/MatCha.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3147300214577163215&lt;/id>;&lt;published>;2023-05-26T09:58:00.001-07:00&lt;/published>;&lt;updated>;2023-05-26T16:35:56.897-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Hardware&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Barkour: Benchmarking animal-level agility with quadruped robots&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ken Caluwaerts and Atil Iscen, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s320/barkour%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Creating robots that exhibit robust and dynamic locomotion capabilities, similar to animals or humans, has been a long-standing goal in the robotics community. In addition to completing tasks quickly and efficiently, agility allows legged robots to move through &lt;a href=&quot;https://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot;>;complex environments&lt;/a>; that are otherwise difficult to traverse. Researchers at Google have been pursuing agility for &lt;a href=&quot;https://arxiv.org/abs/1804.10332&quot;>;multiple years&lt;/a>; and across &lt;a href=&quot;https://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html&quot;>;various form factors&lt;/a>;. Yet, while researchers have enabled &lt;a href=&quot;https://www.science.org/doi/10.1126/scirobotics.abc5986&quot;>;robots to hike&lt;/a>; or &lt;a href=&quot;https://www.roboticsproceedings.org/rss11/p47.pdf&quot;>;jump over some obstacles&lt;/a>;, there is still no generally accepted benchmark that comprehensively measures robot agility or mobility. In contrast, benchmarks are driving forces behind the development of machine learning, such as &lt;a href=&quot;https://arxiv.org/abs/1409.0575&quot;>;ImageNet&lt;/a>; for computer vision, and &lt;a href=&quot;https://github.com/openai/gym&quot;>;OpenAI Gym&lt;/a>; for reinforcement learning (RL). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.14654&quot;>;Barkour: Benchmarking Animal-level Agility with Quadruped Robots&lt;/a>;”, we introduce the &lt;em>;Barkour &lt;/em>;agility benchmark for quadruped robots, along with a &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>;-based generalist locomotion policy. Inspired by dog agility competitions, a legged robot must sequentially display a variety of skills, including moving in different directions, traversing uneven terrains, and jumping over obstacles within a limited timeframe to successfully complete the benchmark. By providing a diverse and challenging obstacle course, the Barkour benchmark encourages researchers to develop locomotion controllers that move fast in a controllable and versatile way. Furthermore, by tying the performance metric to real dog performance, we provide an intuitive metric to understand the robot performance with respect to their animal counterparts. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/zola_vs_ap.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We invited a handful of &lt;a href=&quot;https://blog.google/inside-google/life-at-google/working-home-ruff-dooglers-make-it-little-better/&quot;>;dooglers&lt;/a>; to try the obstacle course to ensure that our agility objectives were realistic and challenging. Small dogs complete the obstacle course in approximately 10s, whereas our robot&#39;s typical performance hovers around 20s.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Barkour benchmark&lt;/h2>; &lt;p>; The Barkour scoring system uses a per obstacle and an overall course target time based on the target speed of small dogs in the &lt;a href=&quot;https://images.akc.org/pdf/rulebooks/REAGIL.pdf&quot;>;novice agility competitions&lt;/a>; (about 1.7m/s). Barkour scores range from 0 to 1, with 1 corresponding to the robot successfully traversing all the obstacles along the course within the allotted time of approximately 10 seconds, the average time needed for a similar-sized dog to traverse the course. The robot receives penalties for skipping, failing obstacles, or moving too slowly. &lt;/p>; &lt;p>; Our standard course consists of four unique obstacles in a 5m x 5m area. This is a denser and smaller setup than a typical dog competition to allow for easy deployment in a robotics lab. Beginning at the start table, the robot needs to weave through a set of poles, climb an A-frame, clear a 0.5m broad jump and then step onto the end table. We chose this subset of obstacles because they test a diverse set of skills while keeping the setup within a small footprint. As is the case for real dog agility competitions, the Barkour benchmark can be easily adapted to a larger course area and may incorporate a variable number of obstacles and course configurations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s1193/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;1193&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the Barkour benchmark&#39;s obstacle course setup, which consists of weave poles, an A-frame, a broad jump, and pause tables. The intuitive scoring mechanism, inspired by dog agility competitions, balances speed, agility and performance and can be easily modified to incorporate other types of obstacles or course configurations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning agile locomotion skills&lt;/h2>; &lt;p>; The Barkour benchmark features a diverse set of obstacles and a delayed reward system, which pose a significant challenge when training a single policy that can complete the entire obstacle course. So in order to set a strong performance baseline and demonstrate the effectiveness of the benchmark for robotic agility research, we adopt a student-teacher framework combined with a zero-shot sim-to-real approach. First, we train individual specialist locomotion skills (teacher) for different obstacles using on-policy RL methods. In particular, we leverage &lt;a href=&quot;https://github.com/google/brax&quot;>;recent advances&lt;/a>; in large-scale &lt;a href=&quot;https://arxiv.org/pdf/2108.10470.pdf&quot;>;parallel simulation&lt;/a>; to equip the robot with individual skills, including walking, slope climbing, and jumping policies. &lt;/p>; &lt;p>; Next, we train a single policy (student) that performs all the skills and transitions in between by using a student-teacher framework, based on the specialist skills we previously trained. We use simulation rollouts to create datasets of state-action pairs for each one of the specialist skills. This dataset is then distilled into a single Transformer-based generalist locomotion policy, which can handle various terrains and adjust the robot&#39;s gait based on the perceived environment and the robot&#39;s state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s1547/Barkour%20training.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;296&quot; data-original-width=&quot;1547&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s16000/Barkour%20training.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During deployment, we pair the locomotion transformer policy that is capable of performing multiple skills with a navigation controller that provides velocity commands based on the robot&#39;s position. Our trained policy controls the robot based on the robot&#39;s surroundings represented as an elevation map, velocity commands, and on-board sensory information provided by the robot.&lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/Generalist.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Deployment pipeline for the locomotion transformer architecture. At deployment time, a high-level navigation controller guides the real robot through the obstacle course by sending commands to the locomotion transformer policy.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Robustness and repeatability are difficult to achieve when we aim for peak performance and maximum speed. Sometimes, the robot might fail when overcoming an obstacle in an agile way. To handle failures we train a &lt;a href=&quot;https://arxiv.org/pdf/2110.05457.pdf&quot;>;recovery policy&lt;/a>; that quickly gets the robot back on its feet, allowing it to continue the episode. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We evaluate the Transformer-based generalist locomotion policy using custom-built quadruped robots and show that by optimizing for the proposed benchmark, we obtain agile, robust, and versatile skills for our robot in the real world. We further provide analysis for various design choices in our system and their impact on the system performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/s1895/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1275&quot; data-original-width=&quot;1895&quot; height=&quot;269&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/w400-h269/image4.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model of the custom-built robots used for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We deploy both the specialist and generalist policies to hardware (zero-shot sim-to-real). The robot&#39;s target trajectory is provided by a set of waypoints along the various obstacles. In the case of the specialist policies, we switch between specialist policies by using a hand-tuned policy switching mechanism that selects the most suitable policy given the robot&#39;s position. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/3obs_side.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Typical performance of our agile locomotion policies on the Barkour benchmark. Our custom-built quadruped robot robustly navigates the terrain&#39;s obstacles by leveraging various skills learned using RL in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that very often our policies can handle unexpected events or even hardware degradation resulting in good average performance, but failures are still possible. As illustrated in the image below, in case of failures, our recovery policy quickly gets the robot back on its feet, allowing it to continue the episode. By combining the recovery policy with a simple walk-back-to-start policy, we are able to run repeated experiments with minimal human intervention to measure the robustness. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/recovery_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative example of robustness and recovery behaviors. The robot trips and rolls over after heading down the A-frame. This triggers the recovery policy, which enables the robot to get back up and continue the course.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that across a large number of evaluations, the single generalist locomotion transformer policy and the specialist policies with the policy switching mechanism achieve similar performance. The locomotion transformer policy has a slightly lower average Barkour score, but exhibits smoother transitions between behaviors and gaits. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;75%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/8x8_cont_bright_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Measuring robustness of the different policies across a large number of runs on the Barkour benchmark.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/s1313/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1313&quot; height=&quot;300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/w640-h300/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Histogram of the agility scores for the locomotion transformer policy. The highest scores shown in blue (0.75 - 0.9) represent the runs where the robot successfully completes all obstacles. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that developing a benchmark for legged robotics is an important first step in quantifying progress toward animal-level agility. To establish a strong baseline, we investigated a zero-shot sim-to-real approach, taking advantage of large-scale parallel simulation and recent advancements in training Transformer-based architectures. Our findings demonstrate that Barkour is a challenging benchmark that can be easily customized, and that our learning-based method for solving the benchmark provides a quadruped robot with a single low-level policy that can perform a variety of agile low-level skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We would like to thank our co-authors at Google DeepMind and our collaborators at Google Research: Wenhao Yu, J. Chase Kew, Tingnan Zhang, Daniel Freeman, Kuang-Hei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang, Nathan Batchelor, Steven Bohez, Federico Casarini, Jose Enrique Chen, Omar Cortes, Erwin Coumans, Adil Dostmohamed, Gabriel Dulac-Arnold, Alejandro Escontrela, Erik Frey, Roland Hafner, Deepali Jain, Yuheng Kuang, Edward Lee, Linda Luu, Ofir Nachum, Ken Oslund, Jason Powell, Diego Reyes, Francesco Romano, Feresteh Sadeghi, Ron Sloat, Baruch Tabanpour, Daniel Zheng, Michael Neunert, Raia Hadsell, Nicolas Heess, Francesco Nori, Jeff Seto, Carolina Parada, Vikas Sindhwani, Vincent Vanhoucke, and Jie Tan. We would also like to thank Marissa Giustina, Ben Jyenis, Gus Kouretas, Nubby Lee, James Lubin, Sherry Moore, Thinh Nguyen, Krista Reymann, Satoshi Kataoka, Trish Blazina, and the members of the robotics team at Google DeepMind for their contributions to the project.Thanks to John Guilyard for creating the animations in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3147300214577163215/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html&quot; rel=&quot;alternate&quot; title=&quot;Barkour: Benchmarking animal-level agility with quadruped robots&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s72-c/barkour%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4176009881387884637&lt;/id>;&lt;published>;2023-05-25T16:09:00.005-07:00&lt;/published>;&lt;updated>;2023-06-08T14:27:41.308-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Unsupervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private clustering for large-scale datasets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Vincent Cohen-Addad and Alessandro Epasto, Research Scientists, Google Research, Graph Mining team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s900/COVID.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a central problem in &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning (ML) with many applications across domains in both industry and academic research more broadly. At its core, clustering consists of the following problem: given a set of data elements, the goal is to partition the data elements into groups such that similar objects are in the same group, while dissimilar objects are in different groups. This problem has been studied in math, computer science, operations research and statistics for more than 60 years in its myriad variants. Two common forms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;clustering&lt;/a>; are metric clustering, in which the elements are points in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_space&quot;>;metric space&lt;/a>;, like in the &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; problem, and graph clustering, where the elements are nodes of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graph&lt;/a>; whose edges represent similarity among them. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s596/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;592&quot; height=&quot;320&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s320/image1.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; clustering problem, we are given a set of points in a metric space with the objective to identify &lt;em>;k&lt;/em>; representative points, called centers (here depicted as triangles), so as to minimize the sum of the squared distances from each point to its closest center. &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Kmeans_toomany.PNG&quot;>;Source&lt;/a>;, rights: CC-BY-SA-4.0&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Despite the extensive literature on algorithm design for clustering, few practical works have focused on rigorously protecting the user&#39;s privacy during clustering. When clustering is applied to personal data (eg, the queries a user has made), it is necessary to consider the privacy implications of using a clustering solution in a real system and how much information the output solution reveals about the input data. &lt;/p>; &lt;p>; To ensure privacy in a rigorous sense, one solution is to develop &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differentially private&lt;/a>; (DP) clustering algorithms. These algorithms ensure that the output of the clustering does not reveal private information about a specific data element (eg, whether a user has made a given query) or sensitive data about the input graph (eg, a relationship in a social network). Given the importance of privacy protections in unsupervised machine learning, in recent years Google has invested in research on &lt;a href=&quot;https://arxiv.org/abs/2008.08007&quot;>;theory&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2021/10/practical-differentially-private.html?hl=el&amp;amp;m=1&quot;>;practice&lt;/a>; of differentially private &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;metric&lt;/a>; or &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/da645920dcd3bd35b0dae329894bad80-Abstract-Conference.html&quot;>;graph&lt;/a>; clustering, and differential privacy in a variety of contexts, eg,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;heatmaps&lt;/a>;&amp;nbsp;or &lt;a href=&quot;https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&quot;>;tools&lt;/a>; to design DP algorithms. &lt;/p>; &lt;p>; Today we are excited to announce two important updates: 1) a &lt;a href=&quot;https://arxiv.org/abs/2302.00037&quot;>;new differentially-private algorithm&lt;/a>; for hierarchical graph clustering, which we&#39;ll be presenting at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>;, and 2) the &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/hst_clustering&quot;>;open-source release&lt;/a>; of the code of a scalable differentially-private &lt;em>;k&lt;/em>;-means algorithm. This code brings differentially private &lt;em>;k&lt;/em>;-means clustering to large scale datasets using distributed computing. Here, we will also discuss our work on clustering technology for a recent launch in the health domain for informing public health authorities. &lt;/p>; &lt;br />; &lt;h2>;Differentially private hierarchical clustering&lt;/h2>; &lt;p>; Hierarchical clustering is a popular clustering approach that consists of recursively partitioning a dataset into clusters at an increasingly finer granularity. A well known example of hierarchical clustering is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Phylogenetic_tree&quot;>;phylogenetic tree&lt;/a>; in biology in which all life on Earth is partitioned into finer and finer groups (eg, kingdom, phylum, class, order, etc.). A hierarchical clustering algorithm receives as input a graph representing the similarity of entities and learns such recursive partitions in an unsupervised way. Yet at the time of our research no algorithm was known to compute hierarchical clustering of a graph with edge privacy, ie, preserving the privacy of the vertex interactions. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://doi.org/10.48550/arXiv.2302.00037&quot;>;Differentially-Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>;”, we consider how well the problem can be approximated in a DP context and establish firm upper and lower bounds on the privacy guarantee. We design an approximation algorithm (the first of its kind) with a polynomial running time that achieves both an additive error that scales with the number of nodes &lt;em>;n&lt;/em>; (of order &lt;em>;n&lt;sup>;2.5&lt;/sup>;&lt;/em>;) and a multiplicative approximation of O(log&lt;sup>;½&lt;/sup>; n), with the multiplicative error identical to the non-private setting. We further provide a new lower bound on the additive error (of order &lt;em>;n&lt;sup>;2&lt;/sup>;&lt;/em>;) for any private algorithm (irrespective of its running time) and provide an exponential-time algorithm that matches this lower bound. Moreover, our paper includes a beyond-worst-case analysis focusing on the &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html&quot;>;hierarchical stochastic block model&lt;/a>;, a standard random graph model that exhibits a natural hierarchical clustering structure, and introduces a private algorithm that returns a solution with an additive cost over the optimum that is negligible for larger and larger graphs, again matching the non-private state-of-the-art approaches. We believe this work expands the understanding of privacy preserving algorithms on graph data and will enable new applications in such settings. &lt;/p>; &lt;br />; &lt;h2>;Large-scale differentially private clustering&lt;/h2>; &lt;p>; We now switch gears and discuss our work for metric space clustering. Most prior work in DP metric clustering has focused on improving the approximation guarantees of the algorithms on the &lt;em>;k&lt;/em>;-means objective, leaving scalability questions out of the picture. Indeed, it is not clear how efficient non-private algorithms such as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;>;k-means++&lt;/a>;&lt;/em>; or &lt;em>;&lt;a href=&quot;https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf&quot;>;k-means//&lt;/a>;&lt;/em>; can be made differentially private without sacrificing drastically either on the approximation guarantees or the scalability. On the other hand, both scalability and privacy are of primary importance at Google. For this reason, we recently published &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;multiple&lt;/a>; &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;papers&lt;/a>; that address the problem of designing efficient differentially private algorithms for clustering that can scale to massive datasets. Our goal is, moreover, to offer scalability to large scale input datasets, even when the target number of centers, &lt;em>;k&lt;/em>;, is large. &lt;/p>; &lt;p>; We work in the &lt;a href=&quot;https://www.cs.umd.edu/~gasarch/MPC/mpc.pdf&quot;>;massively parallel computation&lt;/a>; (MPC) model, which is a computation model representative of modern distributed computation architectures. The model consists of several machines, each holding only part of the input data, that work together with the goal of solving a global problem while minimizing the amount of communication between machines. We present a &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;differentially private constant factor approximation algorithm&lt;/a>; for &lt;em>;k&lt;/em>;-means that only requires a constant number of rounds of synchronization. Our algorithm builds upon our &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;previous work&lt;/a>; on the problem (with code &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/hst_clustering&quot;>;available here&lt;/a>;), which was the first differentially-private clustering algorithm with provable approximation guarantees that can work in the MPC model. &lt;/p>; &lt;p>; The DP constant factor approximation algorithm drastically improves on the previous work using a two phase approach. In an initial phase it computes a crude approximation to “seed” the second phase, which consists of a more sophisticated distributed algorithm. Equipped with the first-step approximation, the second phase relies on results from the &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3406325.3451022&quot;>;Coreset literature&lt;/a>; to subsample a relevant set of input points and find a good differentially private clustering solution for the input points. We then prove that this solution generalizes with approximately the same guarantee to the entire input. &lt;/p>; &lt;br />; &lt;h2>;Vaccination search insights via DP clustering&lt;/h2>; &lt;p>; We then apply these advances in differentially private clustering to real-world applications. One example is our application of our differentially-private clustering solution for publishing COVID vaccine-related queries, while providing strong privacy protections for the users. &lt;/p>; &lt;p>; The goal of &lt;a href=&quot;https://google-research.github.io/vaccination-search-insights/?&quot;>;Vaccination Search Insights&lt;/a>; (VSI) is to help public health decision makers (health authorities, government agencies and nonprofits) identify and respond to communities&#39; information needs regarding COVID vaccines. In order to achieve this, the tool allows users to explore at different geolocation granularities (zip-code, county and state level in the US) the top themes searched by users regarding COVID queries. In particular, the tool visualizes statistics on trending queries rising in interest in a given locale and time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s800/COVIDSearchStats.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s16000/COVIDSearchStats.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of the output of the tool. Displayed on the left, the top searches related to Covid vaccines during the period Oct 10-16 2022. On the right, the queries that have had rising importance during the same period and compared to the previous week.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To better help identifying the themes of the trending searches, the tool clusters the search queries based on their semantic similarity. This is done by applying a custom-designed &lt;em>;k&lt;/em>;-means–based algorithm run over search data that has been anonymized using the DP Gaussian mechanism to add noise and remove low-count queries (thus resulting in a differentially clustering). The method ensures strong differential privacy guarantees for the protection of the user data. &lt;/p>; &lt;p>; This tool provided fine-grained data on COVID vaccine perception in the population at unprecedented scales of granularity, something that is especially relevant to understand the needs of the marginalized communities disproportionately affected by COVID. This project highlights the impact of our investment in research in differential privacy, and unsupervised ML methods. We are looking to other important areas where we can apply these clustering techniques to help guide decision making around global health challenges, like search queries on &lt;a href=&quot;https://blog.google/technology/health/dr-von-nguyens-temperature-check-on-public-health/&quot;>;climate change–related challenges&lt;/a>; such as air quality or extreme heat. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Jacob Imola, Silvio Lattanzi, Jason Lee, Mohammad Mahdian, Vahab Mirrokni, Andres Munoz Medina, Shyam Narayanan, Mark Phillips, David Saulpic, Chris Schwiegelshohn, Sergei Vassilvitskii, Peilin Zhong, and the members of the&lt;/em>;&amp;nbsp;&lt;i>;Health AI&lt;/i>;&lt;em>; team that made the VSI launch possible: Shailesh Bavadekar, Adam Boulanger, Tague Griffith, Mansi Kansal, Chaitanya Kamath, Akim Kumok, Yael Mayer, Tomer Shekel, Megan Shum, Charlotte Stanton, Mimi Sun, Swapnil Vispute, and Mark Young. &lt;/em>; &lt;/p>; &lt;p>; &lt;em>;For more information on the &lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/graph-mining/&quot;>;Graph Mining team&lt;/a>; (part of &lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/&quot;>;Algorithm and Optimization&lt;/a>;) visit our pages.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4176009881387884637/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private clustering for large-scale datasets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s72-c/COVID.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6927879920303666871&lt;/id>;&lt;published>;2023-05-25T10:03:00.008-07:00&lt;/published>;&lt;updated>;2023-06-01T08:59:51.641-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google I/O&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google Research at I/O 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by James Manyika, SVP Google Research and Technology &amp;amp; Society, and Jeff Dean, Chief Scientist, Google DeepMind and Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s1200/GoogleIO.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Wednesday, May 10th was an exciting day for the &lt;a href=&quot;https://research.google&quot;>;Google Research&lt;/a>; community as we watched the results of months and years of our foundational and applied work get announced on the &lt;a href=&quot;https://io.google/2023/&quot;>;Google I/O&lt;/a>; stage. With the quick pace of announcements on stage, it can be difficult to convey the substantial effort and unique innovations that underlie the technologies we presented. So today, we&#39;re excited to reveal more about the research efforts behind some of the many compelling announcements at &lt;a href=&quot;https://www.youtube.com/playlist?list=PL590L5WQmH8dAqv03RCMbZrbzxqCn6W3O&quot;>;this year&#39;s I/O&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;br />; &lt;h2>;PaLM 2 &lt;/h2>; &lt;p>; Our next-generation large language model (LLM), &lt;a href=&quot;https://ai.google/discover/palm2&quot;>;PaLM 2&lt;/a>;, is built on advances in &lt;a href=&quot;https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training&quot;>;compute-optimal scaling&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;scaled instruction-fine tuning&lt;/a>; and &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;improved dataset mixture&lt;/a>;. By fine-tuning and instruction-tuning the model for different purposes, we have been able to integrate state-of-the-art capabilities into over 25 Google products and features, where it is already helping to inform, assist and delight users. For example: &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; is an early experiment that lets you collaborate with generative AI and helps to boost productivity, accelerate ideas and fuel curiosity. It builds on &lt;a href=&quot;https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html&quot;>;advances in deep learning efficiency&lt;/a>; and leverages &lt;a href=&quot;https://ai.google/static/documents/google-about-bard.pdf&quot;>;reinforcement learning from human feedback&lt;/a>; to provide more relevant responses and increase the model&#39;s ability to follow instructions. Bard is now available in 180 countries, where users can interact with it in English, Japanese and Korean, and thanks to the multilingual capabilities afforded by PaLM 2, support for 40 languages is coming soon. &lt;/li>;&lt;li>;With &lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;Search Generative Experience&lt;/a>; we&#39;re taking more of the work out of searching, so you&#39;ll be able to understand a topic faster, uncover new viewpoints and insights, and get things done more easily. As part of this experiment, you&#39;ll see an AI-powered snapshot of key information to consider, with links to dig deeper. &lt;/li>;&lt;li>;&lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; is an easy-to-use prototyping environment for the &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;PaLM API&lt;/a>;, powered by PaLM 2. In fact, internal user engagement with early prototypes of MakerSuite accelerated the development of our PaLM 2 model itself. MakerSuite grew out of research focused on prompting tools, or tools explicitly designed for customizing and controlling LLMs. This line of research includes &lt;a href=&quot;https://research.google/pubs/pub51353/&quot;>;PromptMaker&lt;/a>; (precursor to MakerSuite), and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517582&quot;>;AI Chains&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3491101.3519729&quot;>;PromptChainer&lt;/a>; (one of the first research efforts demonstrating the utility of LLM chaining). &lt;/li>;&lt;li>;Project &lt;a href=&quot;https://thoughtful.withgoogle.com/about&quot;>;Tailwind&lt;/a>; also made use of early research prototypes of MakerSuite to develop features to help writers and researchers explore ideas and improve their prose; its AI-first notebook prototype used PaLM 2 to allow users to ask questions of the model grounded in documents they define. &lt;/li>;&lt;li>;&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>; is our state-of-the-art medical LLM, built on PaLM 2. Med-PaLM 2 achieved &lt;a href=&quot;https://arxiv.org/abs/2305.09617&quot;>;86.5% performance&lt;/a>; on US Medical Licensing Exam–style questions, illustrating its exciting potential for health. We&#39;re now exploring multimodal capabilities to synthesize inputs like X-rays. &lt;/li>;&lt;li>;&lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Codey&lt;/a>; is a version of PaLM 2 fine-tuned on source code to function as a developer assistant. It supports a broad range of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;Code AI&lt;/a>; features, including code completions, code explanation, bug fixing, source code migration, error explanations, and more. Codey is available through our trusted tester program via IDEs (&lt;a href=&quot;https://blog.google/technology/developers/google-colab-ai-coding-features/&quot;>;Colab&lt;/a>;, &lt;a href=&quot;https://developer.android.com/studio/preview/studio-bot&quot;>;Android Studio&lt;/a>;, &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-ai-for-google-cloud&quot;>;Duet AI for Cloud&lt;/a>;, &lt;a href=&quot;https://techcrunch.com/2023/05/10/googles-firebase-gets-ai-extensions-opens-up-its-marketplace/&quot;>;Firebase&lt;/a>;) and via a &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;3P-facing API&lt;/a>;. &lt;/li>; &lt;/ul>; &lt;p>; Perhaps even more exciting for developers, we have opened up the &lt;a href=&quot;https://makersuite.google.com/&quot;>;PaLM APIs &amp;amp; MakerSuite&lt;/a>; to provide the community opportunities to innovate using this groundbreaking technology. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/s1002/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1002&quot; height=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/w640-h616/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PaLM 2 has advanced coding capabilities that enable it to find code errors and make suggestions in a number of different languages.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Imagen&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Imagen family of image generation and editing models&lt;/a>; builds on advances in large &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based language models and &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>;. This family of models is being incorporated into multiple Google products, including: &lt;/p>; &lt;ul>; &lt;li>;Image generation in &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>;Google Slides&lt;/a>; and Android&#39;s &lt;a href=&quot;https://blog.google/products/android/new-android-features-generative-ai/&quot;>;Generative AI wallpaper&lt;/a>; are powered by our text-to-image generation features. &lt;/li>;&lt;li>;&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-new-ai-models-opens-generative-ai-studio&quot;>;Google Cloud&#39;s Vertex AI&lt;/a>; enables image generation, image editing, image upscaling and fine-tuning to help enterprise customers meet their business needs. &lt;/li>;&lt;li>;&lt;a href=&quot;https://developers.googleblog.com/2023/05/how-its-made-io-flip-adds-twist-to.html&quot;>;I/O Flip&lt;/a>;, a digital take on a classic card game, features Google developer mascots on cards that were entirely AI generated. This game showcased a fine-tuning technique called &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; for adapting pre-trained image generation models. Using just a handful of images as &lt;a href=&quot;https://dreambooth.github.io/&quot;>;inputs for fine-tuning&lt;/a>;, it allows users to generate personalized images in minutes. With DreamBooth, users can synthesize a subject in diverse scenes, poses, views, and lighting conditions that don&#39;t appear in the reference images. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s1649/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;1649&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;I/O Flip presents custom card decks designed using DreamBooth.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Phenaki&lt;/h2>; &lt;p>; &lt;a href=&quot;https://sites.research.google/phenaki/&quot;>;Phenaki&lt;/a>;, Google&#39;s Transformer-based text-to-video generation model was featured in the I/O pre-show. Phenaki is a &lt;a href=&quot;https://openreview.net/forum?id=vOEXS39nOF&quot;>;model that can synthesize realistic videos&lt;/a>; from textual prompt sequences by leveraging two main components: an encoder-decoder model that compresses videos to discrete embeddings and a transformer model that translates text embeddings to video tokens. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOOX_c2imHsESoim3U4K0eNWE97rtrGkVWd7TZ1icscOT3qf3O_BdvBakgEk7KwAYMIbzHCb7J_xL7rXxRbe4o-m0U9wdc4CN8oxMAliornJInsRbjNe0h2c7XEXjkoDGF8oavzodCis4oH2vMwwh19du1oLle_JZvtO1KoraOOpWEUuyNHVUiVzhgGQ/s200/panda_bear_highres.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;200&quot; height=&quot;200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOOX_c2imHsESoim3U4K0eNWE97rtrGkVWd7TZ1icscOT3qf3O_BdvBakgEk7KwAYMIbzHCb7J_xL7rXxRbe4o-m0U9wdc4CN8oxMAliornJInsRbjNe0h2c7XEXjkoDGF8oavzodCis4oH2vMwwh19du1oLle_JZvtO1KoraOOpWEUuyNHVUiVzhgGQ/s1600/panda_bear_highres.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivgQpjnthO9y35MrLxLMfWGkUwNWnaqD3Pt7rDSDKg-dThTWICJL7NGW5E7PGJnoiZZA_hk6Rgw3ChWCn7YSNu5QEpgfTbgG2TBaQ7WLPs-TPZ89cVvvHEaFP-AqqRxSox4ogjZ889Cz06pbDVL7P0s8VXJdl7m4MuqNtJLsVsK89ScepRJIt94Z9_bw/s635/Screenshot%202023-06-01%2010.56.18%20AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;635&quot; height=&quot;162&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivgQpjnthO9y35MrLxLMfWGkUwNWnaqD3Pt7rDSDKg-dThTWICJL7NGW5E7PGJnoiZZA_hk6Rgw3ChWCn7YSNu5QEpgfTbgG2TBaQ7WLPs-TPZ89cVvvHEaFP-AqqRxSox4ogjZ889Cz06pbDVL7P0s8VXJdl7m4MuqNtJLsVsK89ScepRJIt94Z9_bw/s320/Screenshot%202023-06-01%2010.56.18%20AM.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/s128/image4.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;128&quot; data-original-width=&quot;128&quot; height=&quot;200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/w200-h200/image4.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s548/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;269&quot; data-original-width=&quot;548&quot; height=&quot;157&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s320/image6.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;br />; &lt;h2>;ARCore and the Scene Semantic API&lt;/h2>; &lt;p>; Among the new features of &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; announced by the AR team at I/O, the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;Scene Semantic API&lt;/a>; can recognize pixel-wise semantics in an outdoor scene. This helps users create custom AR experiences based on the features in the surrounding area. This API is empowered by the outdoor semantic segmentation model, leveraging our recent works around the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLab&lt;/a>; architecture and an egocentric outdoor scene understanding dataset. The latest ARCore release also includes an improved monocular depth model that provides higher accuracy in outdoor scenes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/s486/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;242&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/w199-h400/image3.gif&quot; width=&quot;199&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scene Semantics API uses DeepLab-based semantic segmentation model to provide accurate pixel-wise labels in a scene outdoors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chirp&lt;/h2>; &lt;p>; &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;Chirp&lt;/a>; is Google&#39;s family of state-of-the-art &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Models&lt;/a>; trained on 12 million hours of speech to enable automatic speech recognition (ASR) for 100+ languages. The models can perform ASR on under-resourced languages, such as Amharic, Cebuano, and Assamese, in addition to widely spoken languages like English and Mandarin. Chirp is able to cover such a wide variety of languages by leveraging &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;self-supervised learning on unlabeled multilingual dataset with fine-tuning on a smaller set of labeled data&lt;/a>;. Chirp is now available in the Google Cloud &lt;a href=&quot;https://cloud.google.com/speech-to-text&quot;>;Speech-to-Text API&lt;/a>;, allowing users to perform inference on the model through a simple interface. You can get started with Chirp &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;here&lt;/a>;. &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s591/image7.gif&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;250&quot; data-original-width=&quot;591&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s16000/image7.gif&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;MusicLM&lt;/h2>; &lt;p>; At I/O, we launched &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM, a text-to-music model&lt;/a>; that generates 20 seconds of music from a text prompt. &lt;a href=&quot;https://aitestkitchen.withgoogle.com/experiments/music-lm&quot;>;You can try it yourself on AI Test Kitchen&lt;/a>;, or see it featured during the I/O preshow, where electronic musician and composer &lt;a href=&quot;https://en.wikipedia.org/wiki/Dan_Deacon&quot;>;Dan Deacon&lt;/a>; used MusicLM in his performance. &lt;/p>; &lt;p>; MusicLM, which consists of models powered by &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2208.12415&quot;>;MuLAN&lt;/a>;, can make music (from text, humming, images or video) and musical accompaniments to singing. AudioLM generates high quality audio with long-term consistency. It maps audio to a sequence of discrete tokens and casts audio generation as a language modeling task. To synthesize longer outputs efficiently, it used a novel approach we&#39;ve developed called &lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm&lt;/a>;. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s1927/keywordhero.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1927&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s16000/keywordhero.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;Universal Translator dubbing&lt;/h2>; &lt;p>; Our dubbing efforts leverage dozens of ML technologies to translate the full expressive range of video content, making videos accessible to audiences across the world. These technologies have been used to &lt;a href=&quot;https://www.youtube.com/watch?v=S-iIV5Oo0n0&quot;>;dub videos&lt;/a>; across a variety of products and content types, including educational content, advertising campaigns, and creator content, with more to come. We use deep learning technology to achieve &lt;a href=&quot;https://developers.googleblog.com/2022/12/improving-video-voice-dubbing-through-deep-learning.html&quot;>;voice preservation and lip matching&lt;/a>; and enable high-quality video translation. We&#39;ve built this product to include human review for quality, safety checks to help prevent misuse, and we make it accessible only to authorized partners. &lt;/p>; &lt;br />; &lt;h2>;AI for global societal good&lt;/h2>; &lt;p>; We are applying our AI technologies to solve some of the biggest global challenges, like mitigating climate change, adapting to a warming planet and improving human health and wellbeing. For example: &lt;/p>; &lt;ul>; &lt;li>;Traffic engineers use our Green Light recommendations to reduce stop-and-go traffic at intersections and improve the flow of traffic in cities from Bangalore to Rio de Janeiro and Hamburg. Green Light models each intersection, analyzing traffic patterns to develop recommendations that make traffic lights more efficient — for example, by better synchronizing timing between adjacent lights, or adjusting the “green time” for a given street and direction. &lt;/li>;&lt;li>;We&#39;ve also expanded global coverage on the &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>; to 80 countries, as part of our efforts to predict riverine floods and alert people who are about to be impacted before disaster strikes. Our &lt;a href=&quot;https://sites.research.google/floodforecasting&quot;>;flood forecasting efforts&lt;/a>; rely on &lt;a href=&quot;https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;hydrological models&lt;/a>; informed by satellite observations, weather forecasts and in-situ measurements. &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s1672/image5.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;713&quot; data-original-width=&quot;1672&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s16000/image5.png&quot; />;&lt;/a>;&lt;/div>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Technologies for inclusive and fair ML applications&lt;/h2>; &lt;p>; With our continued investment in AI technologies, we are emphasizing responsible AI development with the goal of making our models and tools useful and impactful while also ensuring fairness, safety and alignment with our &lt;a href=&quot;https://ai.google/principles/&quot;>;AI Principles&lt;/a>;. Some of these efforts were highlighted at I/O, including: &lt;/p>; &lt;ul>; &lt;li>;The release of the &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;Monk Skin Tone Examples (MST-E) Dataset&lt;/a>; to help practitioners gain a deeper understanding of the MST scale and train human annotators for more consistent, inclusive, and meaningful skin tone annotations. You can read more about this and other developments on our &lt;a href=&quot;https://skintone.google/&quot;>;website&lt;/a>;. This is an advancement on the open source release of the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone (MST) Scale&lt;/a>; we launched last year to enable developers to build products that are more inclusive and that better represent their diverse users. &lt;/li>;&lt;li>;A &lt;a href=&quot;https://www.kaggle.com/competitions/asl-fingerspelling/overview/description&quot;>;new Kaggle competition&lt;/a>; (open until August 10th) in which the ML community is tasked with creating a model that can quickly and accurately identify American Sign Language (ASL) fingerspelling — where each letter of a word is spelled out in ASL rapidly using a single hand, rather than using the specific signs for entire words — and translate it into written text. Learn more about the &lt;a href=&quot;https://www.youtube.com/watch?v=q3xKB3dfvtA&quot;>;fingerspelling Kaggle competition&lt;/a>;, which features a song from &lt;a href=&quot;https://www.deafandloud.com/&quot;>;Sean Forbes&lt;/a>;, a deaf musician and rapper. We also &lt;a href=&quot;https://www.youtube.com/watch?v=WC9x3jp_nV8&quot;>;showcased at I/O&lt;/a>; the winning algorithm from the prior year&#39;s competition powers &lt;a href=&quot;https://play.google.com/store/apps/details?id=edu.gatech.popsignai&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;PopSign&lt;/a>;, an ASL learning app for parents of deaf or hard of hearing children created by Georgia Tech and Rochester Institute of Technology (RIT). &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Building the future of AI together&lt;/h2>; &lt;p>; It&#39;s inspiring to be part of a community of so many talented individuals who are leading the way in developing state-of-the-art technologies, responsible AI approaches and exciting user experiences. We are in the midst of a period of incredible and transformative change for AI. Stay tuned for more updates about the ways in which the Google Research community is boldly exploring the frontiers of these technologies and using them responsibly to benefit people&#39;s lives around the world. We hope you&#39;re as excited as we are about the future of AI technologies and we invite you to engage with our teams through the references, sites and tools that we&#39;ve highlighted here. &lt;/p>;&lt;p>;&lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6927879920303666871/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research at I/O 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s72-c/GoogleIO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5648346519353613408&lt;/id>;&lt;published>;2023-05-23T10:51:00.007-07:00&lt;/published>;&lt;updated>;2023-05-31T12:19:41.513-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Semantic Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Resolving code review comments with ML&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alexander Frömmgen, Staff Software Engineer, and Lera Kharatyan, Senior Software Engineer, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s2000/comments2code.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Code-change reviews are a critical part of the software development process at scale, &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;taking&lt;/a>; a significant amount of the code authors&#39; and the code reviewers&#39; time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;millions of reviewer comments&lt;/a>; per year, and authors require an average of ~60 minutes &lt;a href=&quot;https://research.google/pubs/pub49446/&quot;>;active shepherding time&lt;/a>; between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must do to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code review process, eg, by proposing code changes based on a comment&#39;s text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe our application of recent advances in large sequence models (using the &lt;a href=&quot;https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot;>;DIDACT methodology&lt;/a>;) in a real-world setting to automatically resolve code review comments in the day-to-day development workflow at Google. As of today, code-change authors at Google address a substantial amount of reviewer comments by applying an ML-suggested edit. We expect that to reduce time spent on code reviews by hundreds of thousands of hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers&#39; productivity and allows them to focus on more creative and complex tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Predicting the code edit&lt;/h2>; &lt;p>; We started by training a model that predicts code edits needed to address reviewer comments. The model is pre-trained on various coding tasks and related developer activities (eg, renaming a variable, repairing a broken build, editing a file). It&#39;s then fine-tuned for this specific task with reviewed code changes, the reviewer comments, and the edits the author performed to address those comments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s1523/figure1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1523&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s16000/figure1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of an ML-suggested edit of refactorings that are spread within the code.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Google uses a &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2854146&quot;>;monorepo&lt;/a>;, a single repository for all of its software artifacts, which allows our training dataset to include all unrestricted code used to build Google&#39;s most recent software, as well as previous versions. &lt;/p>; &lt;p>; To improve the model quality, we iterated on the training dataset. For example, we compared the model performance for datasets with a single reviewer comment per file to datasets with multiple comments per file, and experimented with classifiers to clean up the training data based on a small, curated dataset to choose the model with the best offline precision and recall metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Serving infrastructure and user experience&lt;/h2>; &lt;p>; We designed and implemented the feature on top of the trained model, focusing on the overall user experience and developer efficiency. As part of this, we explored different user experience (UX) alternatives through a series of user studies. We then refined the feature based on insights from an internal beta (ie, a test of the feature in development) including user feedback (eg, a “Was this helpful?” button next to the suggested edit). &lt;/p>; &lt;p>; The final model was calibrated for a target &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&quot;>;precision&lt;/a>; of 50%. That is, we tuned the model and the suggestions filtering, so that 50% of suggested edits on our evaluation dataset are correct. In general, increasing the target precision reduces the number of shown suggested edits, and decreasing the target precision leads to more incorrect suggested edits. Incorrect suggested edits take the developers time and reduce the developers&#39; trust in the feature. We found that a target precision of 50% provides a good balance. &lt;/p>; &lt;p>; At a high level, for every new reviewer comment, we generate the model input in the same format that is used for training, query the model, and generate the suggested code edit. If the model is confident in the prediction and a few additional heuristics are satisfied, we send the suggested edit to downstream systems. The downstream systems, ie, the code review frontend and the integrated development environment (IDE), expose the suggested edits to the user and log user interactions, such as preview and apply events. A dedicated pipeline collects these logs and generates aggregate insights, eg, the overall acceptance rates as reported in this blog post. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s1250/figure%202c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1250&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s16000/figure%202c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Architecture of the ML-suggested edits infrastructure. We process code and infrastructure from multiple services, get the model predictions and surface the predictions in the code review tool and IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The developer interacts with the ML-suggested edits in the code review tool and the IDE. Based on insights from the user studies, the integration into the code review tool is most suitable for a streamlined review experience. The IDE integration provides additional functionality and supports 3-way merging of the ML-suggested edits (left in the figure below) in case of conflicting local changes on top of the reviewed code state (right) into the merge result (center). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s958/figure%203%20(25%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s16000/figure%203%20(25%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;3-way-merge UX in IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Offline evaluations indicate that the model addresses 52% of comments with a target precision of 50%. The online metrics of the beta and the full internal launch confirm these offline metrics, ie, we see model suggestions above our target model confidence for around 50% of all relevant reviewer comments. 40% to 50% of all previewed suggested edits are applied by code authors. &lt;/p>; &lt;p>; We used the “not helpful” feedback during the beta to identify recurring failure patterns of the model. We implemented serving-time heuristics to filter these and, thus, reduce the number of shown incorrect predictions. With these changes, we traded quantity for quality and observed an increased real-world acceptance rate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s1753/figure%204%20(50%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1753&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s16000/figure%204%20(50%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Code review tool UX. The suggestion is shown as part of the comment and can be previewed, applied and rated as helpful or not helpful.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our beta launch showed a &lt;em>;discoverability challenge&lt;/em>;: code authors only previewed ~20% of all generated suggested edits. We modified the UX and introduced a prominent “Show ML-edit” button (see the figure above) next to the reviewer comment, leading to an overall preview rate of ~40% at launch. We additionally found that suggested edits in the code review tool are often not applicable due to conflicting changes that the author did during the review process. We addressed this with a button in the code review tool that opens the IDE in a merge view for the suggested edit. We now observe that more than 70% of these are applied in the code review tool and fewer than 30% are applied in the IDE. All these changes allowed us to increase the overall fraction of reviewer comments that are addressed with an ML-suggested edit by a factor of 2 from beta to the full internal launch. At Google scale, these results help automate the resolution of hundreds of thousands of comments each year. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s1920/figure%207.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;887&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s16000/figure%207.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Suggestions filtering funnel.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see ML-suggested edits addressing a wide range of reviewer comments in production. This includes simple localized refactorings and refactorings that are spread within the code, as shown in the examples throughout the blog post above. The feature addresses longer and less formally-worded comments that require code generation, refactorings and imports. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s1479/figure%205c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;651&quot; data-original-width=&quot;1479&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s16000/figure%205c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of a suggestion for a longer and less formally worded comment that requires code generation, refactorings and imports.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The model can also respond to complex comments and produce extensive code edits (shown below). The generated test case follows the existing unit test pattern, while changing the details as described in the comment. Additionally, the edit suggests a comprehensive name for the test reflecting the test semantics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s1497/figure%206c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1497&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s16000/figure%206c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the model&#39;s ability to respond to complex comments and produce extensive code edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this post, we introduced an ML-assistance feature to reduce the time spent on code review related changes. At the moment, a substantial amount of all actionable code review comments on supported languages are addressed with applied ML-suggested edits at Google. A 12-week A/B experiment across all Google developers will further measure the impact of the feature on the overall developer productivity. &lt;/p>; &lt;p>; We are working on improvements throughout the whole stack. This includes increasing the quality and recall of the model and building a more streamlined experience for the developer with improved discoverability throughout the review process. As part of this, we are investigating the option of showing suggested edits to the reviewer while they draft comments and expanding the feature into the IDE to enable code-change authors to get suggested code edits for natural-language commands. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This is the work of many people in Google Core Systems &amp;amp; Experiences team, Google Research, and DeepMind. We&#39;d like to specifically thank Peter Choy for bringing the collaboration together, and all of our team members for their key contributions and useful advice, including Marcus Revaj, Gabriela Surita, Maxim Tabachnyk, Jacob Austin, Nimesh Ghelani, Dan Zheng, Peter Josling, Mariana Stariolo, Chris Gorgolewski, Sascha Varkevisser, Katja Grünwedel, Alberto Elizondo, Tobias Welp, Paige Bailey, Pierre-Antoine Manzagol, Pascal Lamblin, Chenjie Gu, Petros Maniatis, Henryk Michalewski, Sara Wiltberger, Ambar Murillo, Satish Chandra, Madhura Dudhgaonkar, Niranjan Tulpule, Zoubin Ghahramani, Juanjo Carin, Danny Tarlow, Kevin Villela, Stoyan Nikolov, David Tattersall, Boris Bokowski, Kathy Nix, Mehdi Ghissassi, Luis C. Cobo, Yujia Li, David Choi, Kristóf Molnár, Vahid Meimand, Amit Patel, Brett Wiltshire, Laurent Le Brun, Mingpan Guo, Hermann Loose, Jonas Mattes, Savinee Dancs. Thanks to John Guilyard for creating the graphics in this post.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5648346519353613408/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot; rel=&quot;alternate&quot; title=&quot;Resolving code review comments with ML&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s72-c/comments2code.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3261601760457492933&lt;/id>;&lt;published>;2023-05-19T09:59:00.002-07:00&lt;/published>;&lt;updated>;2023-05-19T10:01:19.784-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Making ML models differentially private: Best practices and open challenges&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Natalia Ponomareva and Alex Kurakin, Staff Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s320/DPfy%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large machine learning (ML) models are ubiquitous in modern applications: from &lt;a href=&quot;https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters&quot;>;spam filters&lt;/a>; to &lt;a href=&quot;https://developers.google.com/machine-learning/recommendation/overview/types&quot;>;recommender systems&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;virtual assistants&lt;/a>;. These models achieve remarkable performance partially due to the abundance of available training data. However, these data can sometimes contain private information, including personal identifiable information, copyright material, etc. Therefore, protecting the privacy of the training data is critical to practical, applied ML. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential Privacy&lt;/a>; (DP) is one of the most widely accepted technologies that allows reasoning about data anonymization in a formal way. In the context of an ML model, DP can guarantee that each individual user&#39;s contribution will not result in a significantly different model. A model&#39;s privacy guarantees are characterized by a tuple (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;), where smaller values of both represent stronger DP guarantees and better privacy. &lt;/p>; &lt;p>; While there are successful &lt;a href=&quot;https://research.google/pubs/pub52351/&quot;>;examples&lt;/a>; of &lt;a href=&quot;https://ai.googleblog.com/2022/02/federated-learning-with-formal.html&quot;>;protecting training data&lt;/a>; using DP, obtaining good utility with differentially private ML (DP-ML) techniques can be challenging. First, there are inherent privacy/computation tradeoffs that may limit a model&#39;s utility. Further, DP-ML models often require architectural and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter&lt;/a>; tuning, and guidelines on how to do this effectively are limited or difficult to find. Finally, non-rigorous privacy reporting makes it challenging to compare and choose the best DP methods. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy&lt;/a>;”, to appear in the &lt;em>;&lt;a href=&quot;https://www.jair.org/index.php/jair&quot;>;Journal of Artificial Intelligence Research&lt;/a>;&lt;/em>;, we discuss the current state of DP-ML research. We provide an overview of common techniques for obtaining DP-ML models and discuss research, engineering challenges, mitigation techniques and current open questions. We will present tutorials based on this work at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>; and &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-ML methods&lt;/h2>; &lt;p>; DP can be introduced during the ML model development process in three places: (1) at the input data level, (2) during training, or (3) at inference. Each option provides privacy protections at different stages of the ML development process, with the weakest being when DP is introduced at the prediction level and the strongest being when introduced at the input level. Making the input data differentially private means that any model that is trained on this data will also have DP guarantees. When introducing DP during the training, only that particular model has DP guarantees. DP at the prediction level means that only the model&#39;s predictions are protected, but the model itself is not differentially private. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s1821/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;1821&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The task of introducing DP gets progressively easier from the left to right.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; DP is commonly introduced during training (DP-training). &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;Gradient noise injection&lt;/a>; methods, like &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, and their extensions are currently the most practical methods for achieving DP guarantees in complex models like large deep neural networks. &lt;/p>; &lt;p>; DP-SGD builds off of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; (SGD) optimizer with two modifications: (1) per-example gradients are clipped to a certain norm to limit sensitivity (the influence of an individual example on the overall model), which is a slow and computationally intensive process, and (2) a noisy gradient update is formed by taking aggregated gradients and adding noise that is proportional to the sensitivity and the strength of privacy guarantees. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s1439/DP-ML.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;494&quot; data-original-width=&quot;1439&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s16000/DP-ML.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP-SGD is a modification of SGD that involves a) clipping per-example gradients to limit the sensitivity and b) adding the noise, calibrated to the sensitivity and privacy guarantees, to the aggregated gradients, before the gradient update step. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Existing DP-training challenges&lt;/h2>; &lt;p>; Gradient noise injection methods usually exhibit: (1) loss of utility, (2) slower training, and (3) an increased &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_footprint&quot;>;memory footprint&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Loss of utility&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The best method for reducing utility drop is to use more computation. Using larger batch sizes and/or more iterations is one of the most prominent and practical ways of improving a model&#39;s performance. Hyperparameter tuning is also extremely important but often overlooked. The utility of DP-trained models is sensitive to the total amount of noise added, which depends on hyperparameters, like the clipping norm and batch size. Additionally, other hyperparameters like the learning rate should be re-tuned to account for noisy gradient updates. &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Another option is to obtain more data or use public data of similar distribution. This can be done by leveraging publicly available checkpoints, like &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/1910.10683.pdf&quot;>;T5&lt;/a>;, and fine-tuning them using private data. &lt;/p>; &lt;p>; &lt;strong>;Slower training&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Most gradient noise injection methods limit sensitivity via clipping per-example gradients, considerably slowing down &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;>;backpropagation&lt;/a>;. This can be addressed by choosing an efficient DP framework that efficiently implements per-example clipping. &lt;/p>; &lt;p>; &lt;strong>;Increased memory footprint&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;DP-training requires significant memory for computing and storing per-example gradients. Additionally, it requires significantly larger batches to obtain better utility. Increasing the computation resources (eg, the number and size of accelerators) is the simplest solution for extra memory requirements. Alternatively, &lt;a href=&quot;https://arxiv.org/abs/2109.12298&quot;>;several&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2201.12328&quot;>;works&lt;/a>; advocate for gradient accumulation where smaller batches are combined to simulate a larger batch before the gradient update is applied. Further, some algorithms (eg, &lt;a href=&quot;https://arxiv.org/pdf/2110.05679.pdf&quot;>;ghost clipping&lt;/a>;, which is based on &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;>;this paper&lt;/a>;) avoid per-example gradient clipping altogether. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Best practices&lt;/h2>; &lt;p>; The following best practices can attain rigorous DP guarantees with the best model utility possible. &lt;/p>; &lt;p>; &lt;strong>;Choosing the right privacy unit:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;First, we should be clear about a model&#39;s privacy guarantees. This is encoded by selecting the “privacy unit,” which represents the neighboring dataset concept (ie, datasets where only one row is different). Example-level protection is a common choice in the research literature, but may not be ideal, however, for user-generated data if individual users contributed multiple records to the training dataset. For such a case, user-level protection might be more appropriate. For text and sequence data, the choice of the unit is harder since in most applications individual training examples are not aligned to the semantic meaning embedded in the text. &lt;/p>; &lt;p>; &lt;strong>;Choosing privacy guarantees:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;We outline three broad tiers of privacy guarantees and encourage practitioners to choose the lowest possible tier below: &lt;/p>; &lt;ul style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;Tier 1 — Strong privacy guarantees:&lt;/em>; Choosing &lt;em>;ε&lt;/em>; ≤ 1 provides a strong privacy guarantee, but frequently results in a significant utility drop for large models and thus may only be feasible for smaller models. &lt;/li>;&lt;li>;&lt;em>;Tier 2 — Reasonable privacy guarantees:&lt;/em>; We advocate for the currently undocumented, but still widely used, goal for DP-ML models to achieve an &lt;em>;ε&lt;/em>; ≤ 10. &lt;/li>;&lt;li>;&lt;em>;Tier 3 — Weak privacy guarantees:&lt;/em>; Any finite &lt;em>;ε&lt;/em>; is an improvement over a model with no formal privacy guarantee. However, for &lt;em>;ε&lt;/em>; &amp;gt; 10, the DP guarantee alone cannot be taken as sufficient evidence of data anonymization, and additional measures (eg, empirical privacy auditing) may be necessary to ensure the model protects user data. &lt;/li>; &lt;/ul>; &lt;p>; &lt;strong>;Hyperparameter tuning&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Choosing hyperparameters requires optimizing over three inter-dependent objectives: 1) model utility, 2) privacy cost &lt;em>;ε&lt;/em>;, and 3) computation cost. Common strategies take two of the three as constraints, and focus on optimizing the third. We provide methods that will maximize the utility with a limited number of trials, eg, tuning with privacy and computation constraints. &lt;/p>; &lt;p>; &lt;strong>;Reporting privacy guarantees: &lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;A lot of works on DP for ML report only &lt;em>;ε&lt;/em>; and possibly &lt;em>;δ &lt;/em>;values for their training procedure. However, we believe that practitioners should provide a comprehensive overview of model guarantees that includes: &lt;/p>; &lt;ol style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;DP setting:&lt;/em>; Are the results assuming central DP with a trusted service provider, &lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;local DP&lt;/a>;, or some other setting? &lt;/li>;&lt;li>;&lt;em>;Instantiating the DP definition:&lt;/em>; &lt;ol type=&quot;a&quot;>; &lt;li>;&lt;em>;Data accesses covered:&lt;/em>; Whether the DP guarantee applies (only) to a single training run or also covers hyperparameter tuning etc. &lt;/li>;&lt;li>;&lt;em>;Final mechanism&#39;s output&lt;/em>;: What is covered by the privacy guarantees and can be released publicly (eg, model checkpoints, the full sequence of privatized gradients, etc.) &lt;/li>;&lt;li>;&lt;em>;Unit of privacy&lt;/em>;: The selected “privacy unit” (example-level, user-level, etc.) &lt;/li>;&lt;li>;&lt;em>;Adjacency definition&lt;/em>; for DP “neighboring” datasets: A description of how neighboring datasets differ (eg, add-or-remove, replace-one, zero-out-one). &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;Privacy accounting details:&lt;/em>; Providing accounting details, eg, composition and amplification, are important for proper comparison between methods and should include: &lt;ol type=&quot;a&quot;>; &lt;li>;Type of accounting used, eg, &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;-based accounting, PLD accounting, etc. &lt;/li>;&lt;li>;Accounting assumptions and whether they hold (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_regression#Maximum_likelihood-based_parameter_estimation&quot;>;Poisson&lt;/a>; sampling was assumed for privacy amplification but data shuffling was used in training). &lt;/li>;&lt;li>;Formal DP statement for the model and tuning process (eg, the specific &lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;-DP or &lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/1605.02065.pdf&quot;>;ρ-zCDP&lt;/a>;&lt;/em>; values). &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;Transparency and verifiability:&lt;/em>; When possible, complete open-source code using standard DP libraries for the key mechanism implementation and accounting components. &lt;/li>; &lt;/ol>; &lt;p>; &lt;strong>;Paying attention to all the components used:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Usually, DP-training is a straightforward application of DP-SGD or other algorithms. However, some components or losses that are often used in ML models (eg, &lt;a href=&quot;https://www.baeldung.com/cs/contrastive-learning#:~:text=Contrastive%20Loss,and%20dissimilar%20samples%20far%20apart.&quot;>;contrastive losses&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;graph neural network&lt;/a>; layers) should be examined to ensure privacy guarantees are not violated. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open questions&lt;/h2>; &lt;p>; While DP-ML is an active research area, we highlight the broad areas where there is room for improvement. &lt;/p>; &lt;p>; &lt;strong>;Developing better accounting methods&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Our current understanding of DP-training &lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;strong>; &lt;/strong>;&lt;/em>;guarantees&lt;strong>; &lt;/strong>;relies on a number of techniques, like Rényi DP composition and privacy amplification. We believe that better accounting methods for existing algorithms will demonstrate that DP guarantees for ML models are actually better than expected. &lt;/p>; &lt;p>; &lt;strong>;Developing better algorithms:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The computational burden of using gradient noise injection for DP-training comes from the need to use larger batches and limit per-example sensitivity. Developing methods that can use smaller batches or identifying other ways (apart from per-example clipping) to limit the sensitivity would be a breakthrough for DP-ML. &lt;/p>; &lt;p>; &lt;strong>;Better optimization techniques&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Directly applying the same DP-SGD recipe is believed to be suboptimal for adaptive optimizers because the noise added to privatize the gradient may accumulate in learning rate computation. Designing theoretically grounded DP adaptive optimizers remains an active research topic. Another potential direction is to better understand the surface of DP loss, since for standard (non-DP) ML models flatter regions have been shown to &lt;a href=&quot;https://arxiv.org/abs/2010.01412&quot;>;generalize&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;>;better&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Identifying architectures that are more robust to noise&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;There&#39;s an opportunity to better understand whether we need to adjust the architecture of an existing model when introducing DP. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;survey paper&lt;/a>; summarizes the current research related to making ML models DP, and provides practical tips on how to achieve the best privacy-utility trade offs. Our hope is that this work will serve as a reference point for the practitioners who want to effectively apply DP to complex ML models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank Hussein Hazimeh, Zheng Xu , Carson Denison , H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien and Abhradeep Thakurta, Badih Ghazi, Chiyuan Zhang for the help preparing this blog post, paper and tutorials content. Thanks to John Guilyard for creating the graphics in this post, and Ravi Kumar for comments.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3261601760457492933/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot; rel=&quot;alternate&quot; title=&quot;Making ML models differentially private: Best practices and open challenges&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s72-c/DPfy%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-234098392689663352&lt;/id>;&lt;published>;2023-05-18T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-05-18T14:08:39.899-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparse video tubes for joint video and image vision transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s320/TubeViT%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Video understanding is a challenging problem that requires reasoning about both spatial information (eg, for objects in a scene, including their locations and relations) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Temporal_information_retrieval#:~:text=Temporal%20information%20retrieval%20(T%2DIR,of%20the%20user%20information%20needs.&quot;>;temporal information&lt;/a>; for activities or events shown in a video. There are many video understanding applications and tasks, such as &lt;a href=&quot;https://cloud.google.com/video-intelligence&quot;>;understanding the semantic content of web videos&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/02/robot-see-robot-do.html&quot;>;robot perception&lt;/a>;. However, current works, such as &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2102.05095&quot;>;TimeSFormer&lt;/a>;, densely process the video and require significant compute, especially as model size plus video length and resolution increase. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>;”, to be presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>;, we introduce a simple technique that turns a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer&lt;/a>; (ViT) model image encoder into an efficient video backbone using sparse video tubes (learnable visual representations of samples from the video) to reduce the model&#39;s compute needs. This approach can seamlessly process both images and videos, which allows it to leverage both image and video data sources during training. This training further enables our sparse tubes ViT model to coalesce image and video backbones together to serve a dual role as either an image or video backbone (or both), depending on the input. We demonstrate that this model is scalable, can be adapted to large pre-trained ViTs without requiring full fine-tuning, and achieves state-of-the-art results across many video classification benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s860/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;219&quot; data-original-width=&quot;860&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using sparse video tubes to sample a video, combined with a standard ViT encoder, leads to an efficient visual representation that can be seamlessly shared with image inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building a joint image-video backbone&lt;/h2>; &lt;p>; Our sparse tube ViT uses a standard ViT backbone, consisting of a stack of &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; layers, that processes video information. Previous methods, such as ViViT, densely tokenize the video and then apply &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;factorized attention&lt;/a>;, ie, the attention weights for each token are computed separately for the temporal and spatial dimensions. In the standard ViT architecture, self-attention is computed over the whole token sequence. When using videos as input, token sequences become quite long, which can make this computation slow. Instead, in the method we propose, the video is sparsely sampled using &lt;em>;video tubes&lt;/em>;, which are 3D learnable visual representations of various shapes and sizes (described in more detail below) from the video. These tubes are used to sparsely sample the video using a &lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/strided-convolutions-wfUhx&quot;>;large temporal stride&lt;/a>;, ie, when a tube kernel is only applied to a few locations in the video, rather than every pixel. &lt;/p>; &lt;p>; By sparsely sampling the video tubes, we can use the same global self-attention module, rather than factorized attention like ViViT. We experimentally show that the addition of factorized attention layers can harm the performance due to the uninitialized weights. This single stack of transformer layers in the ViT backbone also enables better sharing of the weights and improves performance. Sparse video tube sampling is done by using a large spatial and temporal stride that selects tokens on a fixed grid. The large stride reduces the number of tokens in the full network, while still capturing both spatial and temporal information and enabling the efficient processing of all tokens. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Sparse video tubes&lt;/h2>; &lt;p>; Video tubes are 3D grid-based cuboids that can have different shapes or categories and capture different information with strides and starting locations that can overlap. In the model, we use three distinct tube shapes that capture: (1) only spatial information (resulting in a set of 2D image patches), (2) long temporal information (over a small spatial area), and (3) both spatial and temporal information equally. Tubes that capture only spatial information can be applied to both image and video inputs. Tubes that capture long temporal information or both temporal and spatial information equally are only applied to video inputs. Depending on the input video size, the three tube shapes are applied to the model multiple times to generate tokens. &lt;/p>; &lt;p>; A fixed position embedding, which captures the global location of each tube (including any strides, offsets, etc.) relative to all the other tubes, is applied to the video tubes. Different from the previous learned position embeddings, this fixed one better enables sparse, overlapping sampling. Capturing the global location of the tube helps the model know where each came from, which is especially helpful when tubes overlap or are sampled from distant video locations. Next, the tube features are concatenated together to form a set of &lt;em>;N&lt;/em>; tokens. These tokens are processed by a standard ViT encoder. Finally, we apply an attention pooling to compress all the tokens into a single representation and input to a fully connected (FC) layer to make the classification (eg, playing soccer, swimming, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s474/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;474&quot; data-original-width=&quot;346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our video ViT model works by sampling sparse video tubes from the video (shown at the bottom) to enable either or both image or video inputs to be seamlessly processed. These tubes have different shapes and capture different video features. Tube 1 (&lt;strong>;yellow&lt;/strong>;) only captures spatial information, resulting in a set of 2D patches that can be applied to image inputs. Tube 2 (&lt;strong>;red&lt;/strong>;) captures temporal information and some spatial information and tube 3 (&lt;strong>;green&lt;/strong>;) equally captures both temporal and spatial information (ie, the spatial size of the tube &lt;em>;x&lt;/em>; and &lt;em>;y&lt;/em>; are the same as the number of frames &lt;em>;t&lt;/em>;). Tubes 2 and 3 can only be applied to video inputs. The position embedding is added to all the tube features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaling video ViTs&lt;/h2>; &lt;p>; The process of building video backbones is computationally intensive, but our sparse tube ViT model enables computationally efficient scaling of video models, leveraging previously trained image backbones. Since image backbones can be adapted to a video backbone, large image backbones can be turned into large video backbones. More specifically, one can transfer the learned video feature representations from a small tube ViT to a large pre-trained image ViT and train the resulting model with video data for only a few steps, as opposed to a full training from scratch. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s899/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;899&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our approach enables scaling a sparse tube ViT in a more efficient way. Specifically, the video features from a small video ViT (&lt;strong>;top network&lt;/strong>;) can be transferred to a large, pre-trained image ViT (&lt;strong>;bottom network&lt;/strong>;), and further fine-tuned. This requires fewer training steps to achieve strong performance with the large model. This is beneficial as large video models might be prohibitively expensive to train from scratch.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our sparse tube ViT approach using &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;>;Kinetics-400&lt;/a>; (shown below), Kinetics-600 and Kinetics-700 datasets and compare its performance to a long list of prior methods. We find that our approach outperforms all prior methods. Importantly, it outperforms all state-of-the-art methods trained jointly on image+video datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBVbXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBVbXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance compared to several prior works on the popular Kinetics-400 video dataset. Our sparse tube ViT outperforms state-of-the-art methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore, we test our sparse tube ViT model on the &lt;a href=&quot;https://developer.qualcomm.com/software/ai-datasets/something-something&quot;>;Something-Something V2&lt;/a>; dataset, which is commonly used to evaluate more dynamic activities, and also report that it outperforms all prior state-of-the-art approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the Something-Something V2 video dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualizing some learned kernels&lt;/h2>; &lt;p>; It is interesting to understand what kind of rudimentary features are being learned by the proposed model. We visualize them below, showing both the 2D patches, which are shared for both images and videos, and video tubes. These visualizations show the 2D or 3D information being captured by the projection layer. For example, in the 2D patches, various common features, like edges and colors, are detected, while the 3D tubes capture basic shapes and how they may change over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s549/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;285&quot; data-original-width=&quot;549&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualizations of patches and tubes learned the sparse tube ViT model. Top row are the 2D patches and the remaining two rows are snapshots from the learned video tubes. The tubes show each patch for the 8 or 4 frames to which they are applied.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusions&lt;/h2>; &lt;p>; We have presented a new sparse tube ViT, which can turn a ViT encoder into an efficient video model, and can seamlessly work with both image and video inputs. We also showed that large video encoders can be bootstrapped from small video encoders and image-only ViTs. Our approach outperforms prior methods across several popular video understanding benchmarks. We believe that this simple representation can facilitate much more efficient learning with input videos, seamlessly incorporate either image or video inputs and effectively eliminate the bifurcation of image and video models for future multimodal understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is conducted by AJ Piergiovanni, Weicheng Kuo and Anelia Angelova, who are now at Google DeepMind. We thank Abhijit Ogale, Luowei Zhou, Claire Cui and our colleagues in Google Research for their helpful discussions, comments, and support.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/234098392689663352/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot; rel=&quot;alternate&quot; title=&quot;Sparse video tubes for joint video and image vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s72-c/TubeViT%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2749680625311121514&lt;/id>;&lt;published>;2023-05-18T10:12:00.006-07:00&lt;/published>;&lt;updated>;2023-05-18T16:14:50.453-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: PAIR&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Lucas Dixon and Michael Terry, co-leads, PAIR, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; PAIR (People + AI Research) first &lt;a href=&quot;https://blog.google/technology/ai/pair-people-ai-research-initiative/&quot;>;launched&lt;/a>; in 2017 with the belief that “AI can go much further — and be more useful to all of us — if we build systems with people in mind at the start of the process.” We continue to focus on making AI more understandable, interpretable, fun, and usable by more people around the world. It&#39;s a mission that is particularly timely given the emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chatbot&quot;>;chatbots&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, PAIR is part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team within Google Research, and our work spans this larger research space: We advance &lt;a href=&quot;https://pair.withgoogle.com/research/&quot;>;foundational research&lt;/a>; on human-AI interaction (HAI) and machine learning (ML); we publish educational materials, including the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;PAIR Guidebook&lt;/a>; and &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;Explorables&lt;/a>; (such as the recent Explorable looking at &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;); and we develop software tools like the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; to help people understand and debug ML behaviors. Our inspiration this year is &quot;changing the way people think about what &lt;em>;THEY&lt;/em>; can do with AI.” This vision is inspired by the rapid emergence of generative AI technologies, such as large language models (LLMs) that power chatbots like &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>;, and new generative media models like Google&#39;s &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;, and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;. In this blog post, we review recent PAIR work that is changing the way we engage with AI. &lt;/p>; &lt;br />; &lt;h2>;Generative AI research&lt;/h2>; &lt;p>; Generative AI is creating a lot of excitement, and PAIR is involved in a range of related research, from &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;>;using language models to create generative agents&lt;/a>;&amp;nbsp;to studying how artists adopted generative image models like &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; and &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;. These latter &quot;text-to-image&quot; models let a person input a text-based description of an image for the model to generate (eg, &quot;a gingerbread house in a forest in a cartoony style&quot;). In a forthcoming paper titled “&lt;a href=&quot;https://arxiv.org/abs/2303.12253&quot;>;The Prompt Artists&lt;/a>;” (to appear in &lt;a href=&quot;https://cc.acm.org/2023/&quot;>;Creativity and Cognition 2023&lt;/a>;), we found that users of generative image models strive not only to create beautiful images, but also to create unique, innovative styles. To help achieve these styles, some would even seek unique vocabulary to help develop their visual style. For example, they may visit architectural blogs to learn what domain-specific vocabulary they can adopt to help produce distinctive images of buildings. &lt;/p>; &lt;p>; We are also researching solutions to challenges faced by prompt creators who, with generative AI, are essentially programming without using a programming language. As an example, we developed &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;new methods&lt;/a>; for extracting semantically meaningful structure from natural language prompts. We have applied these structures to prompt editors to provide features similar to those found in other programming environments, such as semantic highlighting, autosuggest, and structured data views. &lt;/p>; &lt;p>; The growth of generative LLMs has also opened up new techniques to solve important long-standing problems. &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;Agile classifiers&lt;/a>; are one approach we&#39;re taking to leverage the semantic and syntactic strengths of LLMs to solve classification problems related to safer online discourse, such as nimbly blocking newer types of toxic language as quickly as it may evolve online. The big advance here is the ability to develop high quality classifiers from very small datasets — as small as 80 examples. This suggests a positive future for online discourse and better moderation of it: instead of collecting millions of examples to attempt to create universal safety classifiers for all use cases over months or years, more agile classifiers might be created by individuals or small organizations and tailored for their specific use cases, and iterated on and adapted in the time-span of a day (eg, to block a new kind of harassment being received or to correct unintended biases in models). As an example of their utility, these methods recently &lt;a href=&quot;https://www.aclweb.org/portal/content/semeval-2023-task-10-explainable-detection-online-sexism-edos&quot;>;won a SemEval competition&lt;/a>; to identify and explain sexism. &lt;/p>; &lt;p>; We&#39;ve also developed &lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>;new state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors and misbehaviours. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we also found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements on model accuracy. &lt;/p>; &lt;p>; Collectively, these methods are critical to help the scientific community improve generative models. They provide techniques for fast and effective content moderation and dialogue safety methods that help support creators whose content is the basis for generative models&#39; amazing outcomes. In addition, they provide direct tools to help debug model misbehavior which leads to better generation. &lt;/p>; &lt;br />; &lt;h2>;Visualization and education&lt;/h2>; &lt;p>; To lower barriers in understanding ML-related work, we regularly design and publish highly visual, interactive online essays, called &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, that provide accessible, hands-on ways to learn about key ideas in ML. For example, we recently published new AI Explorables on the topics of model confidence and unintended biases. In our latest Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” we discuss the problem with model confidence: models can sometimes be &lt;em>;very&lt;/em>; confident in their predictions… and yet completely incorrect. Why does this happen and what can be done about it? Our Explorable walks through these issues with interactive examples and shows how we can build models that have more appropriate confidence in their predictions by using a technique called &lt;a href=&quot;https://ai.googleblog.com/2021/11/model-ensembles-are-faster-than-you.html&quot;>;ensembling&lt;/a>;, which works by averaging the outputs of multiple models. Another Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/saliency/&quot;>;Searching for Unintended Biases with Saliency&lt;/a>;”, shows how spurious correlations can lead to unintended biases — and how techniques such as saliency maps can detect some biases in datasets, with the caveat that it can be difficult to see bias when it&#39;s more subtle and sporadic in a training set. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;543&quot; data-original-width=&quot;724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR designs and publishes AI Explorables, interactive essays on timely topics and new methods in ML research, such as “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” which looks at how and why models offer incorrect predictions with high confidence, and how “ensembling” the outputs of many models can help avoid this.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Transparency and the Data Cards Playbook&lt;/h2>; &lt;p>; Continuing to advance our goal of helping people to understand ML, we promote transparent documentation. In the past, PAIR and Google Cloud developed &lt;a href=&quot;http://modelcards.withgoogle.com&quot;>;model cards&lt;/a>;. Most recently, we presented &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3531146.3533231&quot;>;our work on Data Cards&lt;/a>; at &lt;a href=&quot;https://facctconference.org/2022/&quot;>;ACM FAccT&#39;22&lt;/a>; and open-sourced the &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;, a joint effort with the &lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;Technology, AI, Society, and Culture team&lt;/a>; (TASC). &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;The Data Cards Playbook&lt;/a>; is a toolkit of participatory activities and frameworks to help teams and organizations overcome obstacles when setting up a transparency effort. It was created using an iterative, multidisciplinary approach rooted in the experiences of over 20 teams at Google, and comes with four modules: Ask, Inspect, Answer and Audit. These modules contain a variety of resources that can help you customize Data Cards to your organization&#39;s needs: &lt;/p>; &lt;ul>; &lt;li>;18 Foundations: Scalable frameworks that anyone can use on any dataset type &lt;/li>;&lt;li>;19 Transparency Patterns: Evidence-based guidance to produce high-quality Data Cards at scale &lt;/li>;&lt;li>;33 Participatory Activities: Cross-functional workshops to navigate transparency challenges for teams &lt;/li>;&lt;li>;Interactive Lab: Generate interactive Data Cards from markdown in the browser &lt;/li>; &lt;/ul>; &lt;p>; The Data Cards Playbook is accessible as a learning pathway for startups, universities, and other research groups. &lt;/p>; &lt;br />; &lt;h2>;Software Tools&lt;/h2>; &lt;p>; Our team thrives on creating tools, toolkits, libraries, and visualizations that expand access and improve understanding of ML models. One such resource is &lt;a href=&quot;https://knowyourdata.withgoogle.com/&quot;>;Know Your Data&lt;/a>;, which allows researchers to test a model&#39;s performance for various scenarios through interactive qualitative exploration of datasets that they can use to find and fix unintended dataset biases. &lt;/p>; &lt;p>; Recently, PAIR released a new version of the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT) for model debugging and understanding. LIT v0.5 provides support for image and tabular data, new interpreters for tabular feature attribution, a &quot;Dive&quot; visualization for faceted data exploration, and performance improvements that allow LIT to scale to 100k dataset entries. You can find the &lt;a href=&quot;https://github.com/PAIR-code/lit/blob/main/RELEASE.md&quot;>;release notes&lt;/a>; and &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;code&lt;/a>; on GitHub. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s1920/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR&#39;s &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT), an open-source platform for visualization and understanding of ML models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; PAIR has also contributed to &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a tool for rapid prototyping with LLMs using prompt programming. MakerSuite builds on our earlier research on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;PromptMaker&lt;/a>;, which won an honorable mention at &lt;a href=&quot;https://chi2022.acm.org/&quot;>;CHI 2022. &lt;/a>;MakerSuite lowers the barrier to prototyping ML applications by broadening the types of people who can author these prototypes and by shortening the time spent prototyping models from months to minutes.&amp;nbsp;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s1216/reversedictionary.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;918&quot; data-original-width=&quot;1216&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s16000/reversedictionary.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A screenshot of MakerSuite, a tool for rapidly prototyping new ML models using prompt-based programming, which grew out of PAIR&#39;s prompt programming research.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Ongoing work&lt;/h2>; &lt;p>; As the world of AI moves quickly ahead, PAIR is excited to continue to develop new tools, research, and educational materials to help change the way people think about what THEY can do with AI. &lt;/p>; &lt;p>; For example, we recently conducted &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;an exploratory study&lt;/a>; with five designers (presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI&lt;/a>; this year) that looks at how people with no ML programming experience or training can use prompt programming to quickly prototype functional user interface mock-ups. This prototyping speed can help inform designers on how to integrate ML models into products, and enables them to conduct user research sooner in the product design process. &lt;/p>; &lt;p>; Based on this study, PAIR&#39;s researchers built &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser&lt;/a>;, a design tool plugin for authoring LLM-infused mock-ups. The plug-in introduces two novel LLM-interactions: input-output, which makes content interactive and dynamic, and frame-change, which directs users to different frames depending on their natural language input. The result is more tightly integrated UI and ML prototyping, all within a single interface. &lt;/p>; &lt;p>; Recent advances in AI represent a significant shift in how easy it is for researchers to customize and control models for their research objectives and goals.These capabilities are transforming the way we think about interacting with AI, and they create lots of new opportunities for the research community. PAIR is excited about how we can leverage these capabilities to make AI easier to use for more people. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Thanks to everyone in PAIR, to Reena Jana and to all of our collaborators. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2749680625311121514/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: PAIR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2478354033845809100&lt;/id>;&lt;published>;2023-05-16T12:22:00.001-07:00&lt;/published>;&lt;updated>;2023-05-16T12:23:42.794-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using reinforcement learning for dynamic planning in open-ended conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Deborah Cohen, Staff Research Scientist, and Craig Boutilier, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s1650/rlxtalk.png&quot; style=&quot;display: none;&quot; />; &lt;p>; As virtual assistants become ubiquitous, users increasingly interact with them to learn about new topics or obtain recommendations and expect them to deliver capabilities beyond narrow dialogues of one or two turns. Dynamic planning, namely the capability to look ahead and replan based on the flow of the conversation, is an essential ingredient for the making of engaging conversations with the deeper, open-ended interactions that users expect. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While large language models (LLMs) are now beating state-of-the-art approaches in many natural language processing benchmarks, they are typically trained to output the next best response, rather than planning ahead, which is required for multi-turn interactions. However, in the past few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;>;reinforcement learning&lt;/a>; (RL) has delivered incredible results addressing specific problems that involve dynamic planning, such as winning games and protein folding. &lt;/p>; &lt;p>; Today, we are sharing our recent advances in &lt;a href=&quot;https://arxiv.org/abs/2208.02294&quot;>;dynamic planning for human-to-assistant conversations&lt;/a>;, in which we enable an assistant to plan a multi-turn conversation towards a goal and adapt that plan in real-time by adopting an RL-based approach. Here we look at how to improve long interactions by applying RL to compose answers based on information extracted from reputable sources, rather than relying on content generated by a language model. We expect that future versions of this work could combine LLMs and RL in multi-turn dialogues. The deployment of RL “in the wild” in a large-scale dialogue system proved a formidable challenge due to the modeling complexity, tremendously large state and action spaces, and significant subtlety in designing reward functions. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What is dynamic planning?&lt;/h2>; &lt;p>; Many types of conversations, from gathering information to offering recommendations, require a flexible approach and the ability to modify the original plan for the conversation based on its flow. This ability to shift gears in the middle of a conversation is known as &lt;em>;dynamic planning&lt;/em>;, as opposed to &lt;em>;static planning&lt;/em>;, which refers to a more fixed approach. In the conversation below, for example, the goal is to engage the user by sharing interesting facts about cool animals. To begin, the assistant steers the conversation to sharks via a sound quiz. Given the user&#39;s lack of interest in sharks, the assistant then develops an updated plan and pivots the conversation to sea lions, lions, and then cheetahs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;908&quot; data-original-width=&quot;890&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The assistant dynamically modifies its original plan to talk about sharks and shares facts about other animals.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic composition&lt;/h2>; &lt;p>; To cope with the challenge of conversational exploration, we separate the generation of assistant responses into two parts: 1) &lt;em>;content generation&lt;/em>;, which extracts relevant information from reputable sources,&lt;em>; &lt;/em>;and 2) &lt;em>;flexible composition&lt;/em>; of such content into assistant responses. We refer to this two-part approach as &lt;em>;&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;dynamic composition&lt;/a>;&lt;/em>;. Unlike LLM methods, this approach gives the assistant the ability to fully control the source, correctness, and quality of the content that it may offer. At the same time, it can achieve flexibility via a learned dialogue manager that selects and combines the most appropriate content. &lt;/p>; &lt;p>; In an earlier paper, “&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;Dynamic Composition for Conversational Domain Exploration&lt;/a>;”, we describe a novel approach which consists of: (1) a collection of content providers, which offer candidates from different sources, such as news snippets, &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; facts, and questions; (2) a dialogue manager; and (3) a sentence fusion module. Each assistant response is incrementally constructed by the dialogue manager, which selects candidates proposed by the content providers. The selected sequence of utterances is then fused into a cohesive response. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic planning using RL&lt;/h2>; &lt;p>; At the core of the assistant response composition loop is a dialogue manager trained using &lt;em>;off-policy RL&lt;/em>;, namely an algorithm that evaluates and improves a policy that is different from the policy used by the agent (in our case, the latter is based on a supervised model). Applying RL to dialogue management presents several challenges, including a large state space (as the state represents the conversation state, which needs to account for the whole conversation history) and an effectively unbounded action space (that may include all existing words or sentences in natural language). &lt;/p>; &lt;p>; We address these challenges using a novel RL construction. First, we leverage powerful supervised models — specifically, &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;>;recurrent neural networks&lt;/a>; (RNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; — to provide a succinct and effective dialogue state representation. These state encoders are fed with the dialogue history, composed of a sequence of user and assistant turns, and output a representation of the dialogue state in the form of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_space&quot;>;latent vector&lt;/a>;. &lt;/p>; &lt;p>; Second, we use the fact that a relatively small set of reasonable candidate utterances or actions can be generated by content providers at each conversation turn, and limit the action space to these. Whereas the action space is typically fixed in RL settings, because all states share the same action space, ours is a non-standard space in which the candidate actions may differ with each state, since content providers generate different actions depending on the dialogue context. This puts us in the realm of stochastic action sets, a framework that formalizes cases where the set of actions available in each state is governed by an exogenous stochastic process, which we address using &lt;a href=&quot;https://www.ijcai.org/proceedings/2018/0650.pdf&quot;>;Stochastic Action Q-Learning&lt;/a>;, a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Q-learning&quot;>;Q-learning&lt;/a>; approach. Q-learning is a popular off-policy RL algorithm, which does not require a model of the environment to evaluate and improve the policy. We trained our model on a corpus of crowd-compute–rated conversations obtained using a supervised dialogue manager. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s1500/image4.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given the current dialogue history and a new user query, content providers generate candidates from which the assistant selects one. This process runs in a loop, and at the end the selected utterances are fused into a cohesive response.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Reinforcement learning model evaluation&lt;/h2>; &lt;p>; We compared our RL dialogue manager with a launched supervised transformer model in an experiment using Google Assistant, which conversed with users about animals. A conversation starts when a user triggers the experience by asking an animal-related query (eg, “How does a lion sound?”). The experiment was conducted using an &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B testing&lt;/a>; protocol, in which a small percentage of Assistant users were randomly sampled to interact with our RL-based assistant while other users interacted with the standard assistant. &lt;/p>; &lt;p>; We found that the RL dialogue manager conducts longer, more engaging conversations. It increases conversation length by 30% while improving user engagement metrics. We see an increase of 8% in cooperative responses to the assistant&#39;s questions — eg, “Tell me about lions,” in response to “Which animal do you want to hear about next?” Although there is also a large increase in nominally “non-cooperative” responses (eg, “No,” as a reply to a question proposing additional content, such as “Do you want to hear more?”), this is expected as the RL agent takes more risks by asking pivoting questions. While a user may not be interested in the conversational direction proposed by the assistant (eg, pivoting to another animal), the user will often continue to engage in a dialogue about animals. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s552/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;534&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;From the non-cooperative user response in the 3rd turn (“No.”) and the query “Make a dog sound,” in the 5th turn, the assistant recognizes that the user is mostly interested in animal sounds and modifies its plan, providing sounds and sound quizzes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In addition, some user queries contain explicit positive (eg, “Thank you, Google,” or “I&#39;m happy.”) or negative (eg, “Shut up,” or “Stop.”) feedback. While an order of magnitude fewer than other queries, they offer a direct measure of user (dis)satisfaction. The RL model increases explicit positive feedback by 32% and reduces negative feedback by 18%. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Learned dynamic planning characteristics and strategies&lt;/h2>; &lt;p>; We observe several characteristics of the (unseen) RL plan to improve user engagement while conducting longer conversations. First, the RL-based assistant ends 20% more turns in questions, prompting the user to choose additional content. It also better harnesses content diversity, including facts, sounds, quizzes, yes/no questions, open questions, etc. On average, the RL assistant uses 26% more distinct content providers per conversation than the supervised model. &lt;/p>; &lt;p>; Two observed RL planning strategies are related to the existence of sub-dialogues with different characteristics. Sub-dialogues about animal sounds are poorer in content and exhibit entity pivoting at every turn (ie, after playing the sound of a given animal, we can either suggest the sound of a different animal or quiz the user about other animal sounds). In contrast, sub-dialogues involving animal facts typically contain richer content and have greater conversation depth. We observe that RL favors the richer experience of the latter, selecting 31% more fact-related content. Lastly, when restricting analysis to fact-related dialogues, the RL assistant exhibits 60% more focus-pivoting turns, that is, conversational turns that change the focus of the dialogue. &lt;/p>; &lt;p>; Below, we show two example conversations, one conducted by the supervised model (left) and the second by the RL model (right), in which the first three user turns are identical. With a supervised dialogue manager, after the user declined to hear about “today&#39;s animal”, the assistant pivots back to animal sounds to maximize the immediate user satisfaction. While the conversation conducted by the RL model begins identically, it exhibits a different planning strategy to optimize the overall user engagement, introducing more diverse content, such as fun facts. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s1539/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1539&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the left conversation, conducted by the supervised model, the assistant maximizes the immediate user satisfaction. The right conversation, conducted by the RL model, shows different planning strategies to optimize the overall user engagement.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future research and challenges&lt;/h2>; &lt;p>; In the past few years, LLMs trained for language understanding and generation have demonstrated impressive results across multiple tasks, including dialogue. We are now exploring the use of an RL framework to empower LLMs with the capability of dynamic planning so that they can dynamically plan ahead and delight users with a more engaging experience. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias, Idan Szpektor and Gal Elidan. We would like to thank: Roee Aharoni, Moran Ambar, John Anderson, Ido Cohn, Mohammad Ghavamzadeh, Lotem Golany, Ziv Hodak, Adva Levin, Fernando Pereira, Shimi Salant, Shachar Shimoni, Ronit Slyper, Ariel Stolovich, Hagai Taitelbaum, Noam Velan, Avital Zipori and the CrowdCompute team led by Ashwin Kakarla. We thank Sophie Allweis for her feedback on this blogpost and Tom Small for the visualization.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2478354033845809100/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Using reinforcement learning for dynamic planning in open-ended conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s72-c/rlxtalk.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-14165165832846745&lt;/id>;&lt;published>;2023-05-15T13:59:00.001-07:00&lt;/published>;&lt;updated>;2023-05-15T14:40:42.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Larger language models do in-context learning differently&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s800/SULICL.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There have recently been tremendous advances in language models, partly because they can perform tasks with strong performance via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>; (ICL), a process whereby models are prompted with a few examples of input-label pairs before performing the task on an unseen evaluation example. In general, models&#39; success at in-context learning is enabled by: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;ul>; &lt;li>;Their use of semantic prior knowledge from pre-training to predict labels while following the format of in-context examples (eg, seeing examples of movie reviews with “positive sentiment” and “negative sentiment” as labels and performing &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>; using prior knowledge). &lt;/li>;&lt;li>;Learning the input-label mappings in context from the presented examples (eg, finding a pattern that positive reviews should be mapped to one label, and negative reviews should be mapped to a different label). &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.03846&quot;>;Larger language models do in-context learning differently&lt;/a>;”, we aim to learn about how these two factors (semantic priors and input-label mappings) interact with each other in ICL settings, especially with respect to the scale of the language model that&#39;s used. We investigate two settings to study these two factors — ICL with flipped labels (flipped-label ICL) and ICL with semantically-unrelated labels (SUL-ICL). In flipped-label ICL, labels of in-context examples are flipped so that semantic priors and input-label mappings disagree with each other. In SUL-ICL, labels of in-context examples are replaced with words that are semantically unrelated to the task presented in-context. We found that overriding prior knowledge is an emergent ability of model scale, as is the ability to learn in-context with semantically-unrelated labels. We also found that instruction tuning strengthens the use of prior knowledge more than it increases the capacity to learn input-label mappings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s625/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;625&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of flipped-label ICL and semantically-unrelated label ICL (SUL-ICL), compared with regular ICL, for a sentiment analysis task. Flipped-label ICL uses flipped labels, forcing the model to override semantic priors in order to follow the in-context examples. SUL-ICL uses labels that are not semantically related to the task, which means that models must learn input-label mappings in order to perform the task because they can no longer rely on the semantics of natural language labels.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiment design&lt;/h2>; &lt;p>; For a diverse dataset mixture, we experiment on seven &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) tasks that have been widely used: &lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;sentiment analysis&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/SetFit/subj&quot;>;subjective/objective classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/trec&quot;>;question classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/glue/viewer/qqp/validation&quot;>;duplicated-question recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/super_glue/viewer/rte/test&quot;>;entailment recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/financial_phrasebank&quot;>;financial sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://huggingface.co/datasets/ethos&quot;>;hate speech detection&lt;/a>;. We test five language model families, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;>;InstructGPT&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In this experiment, labels of in-context examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override their priors. In this setting, models that are able to override prior knowledge and learn input-label mappings in-context should experience a decrease in performance (since ground-truth evaluation labels are not flipped). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s1036/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1036&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The ability to override semantic priors when presented with flipped in-context example labels emerges with model scale. Smaller models cannot flip predictions to follow flipped labels (performance only decreases slightly), while larger models can do so (performance decreases to well below 50%).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that when no labels are flipped, larger models have better performance than smaller models (as expected). But when we flip more and more labels, the performance of small models stays relatively flat, but large models experience large performance drops to well-below random guessing (eg, 90% → 22.5% for code-davinci-002). &lt;/p>; &lt;p>; These results indicate that large models can override prior knowledge from pre-training when contradicting input-label mappings are presented in-context. Small models can&#39;t do this, making this ability an emergent phenomena of model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Semantically-unrelated labels&lt;/h2>; &lt;p>; In this experiment, we replace labels with semantically-irrelevant ones (eg, for sentiment analysis, we use “foo/bar” instead of “negative/positive”), which means that the model can only perform ICL by learning from input-label mappings. If a model mostly relies on prior knowledge for ICL, then its performance should decrease after this change since it will no longer be able to use semantic meanings of labels to make predictions. A model that can learn input–label mappings in-context, on the other hand, would be able to learn these semantically-unrelated mappings and should not experience a major drop in performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s1049/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;1049&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Small models rely more on semantic priors than large models do, as indicated by the greater decrease in performance for small models than for large models when using semantically-unrelated labels (ie, targets) instead of natural language labels. For each plot, models are shown in order of increasing model size (eg, for GPT-3 models, &lt;em>;a&lt;/em>; is smaller than &lt;em>;b&lt;/em>;, which is smaller than &lt;em>;c&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Indeed, we see that using semantically-unrelated labels results in a greater performance drop for small models. This suggests that smaller models primarily rely on their semantic priors for ICL rather than learning from the presented input-label mappings. Large models, on the other hand, have the ability to learn input-label mappings in-context when the semantic nature of labels is removed. &lt;/p>; &lt;p>; We also find that including more in-context examples (ie, exemplars) results in a greater performance improvement for large models than it does for small models, indicating that large models are better at learning from in-context examples than small models are. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s825/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;417&quot; data-original-width=&quot;825&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the SUL-ICL setup, larger models benefit more from additional examples than smaller models do.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Instruction tuning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a popular technique for improving model performance, which involves tuning models on various NLP tasks that are phrased as instructions (eg, “Question: What is the sentiment of the following sentence, &#39;This movie is great.&#39; Answer: Positive”). Since the process uses natural language labels, however, an open question is whether it improves the ability to learn input-label mappings or whether it strengthens the ability to recognize and apply semantic prior knowledge. Both of these would lead to an improvement in performance on standard ICL tasks, so it&#39;s unclear which of these occur. &lt;/p>; &lt;p>; We study this question by running the same two setups as before, only this time we focus on comparing standard language models (specifically, PaLM) with their instruction-tuned variants (Flan-PaLM). &lt;/p>; &lt;p>; First, we find that Flan-PaLM is better than PaLM when we use semantically-unrelated labels. This effect is very prominent in small models, as Flan-PaLM-8B outperforms PaLM-8B by 9.6% and almost catches up to PaLM-62B. This trend suggests that instruction tuning strengthens the ability to learn input-label mappings, which isn&#39;t particularly surprising. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s573/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;301&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned language models are better at learning input–label mappings than pre-training–only language models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More interestingly, we saw that Flan-PaLM is actually worse than PaLM at following flipped labels, meaning that the instruction tuned models were unable to override their prior knowledge (Flan-PaLM models don&#39;t reach below random guessing with 100% flipped labels, but PaLM models without instruction tuning can reach 31% accuracy in the same setting). These results indicate that instruction tuning must increase the extent to which models rely on semantic priors when they&#39;re available. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s1016/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;1016&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned models are worse than pre-training–only models at learning to override semantic priors when presented with flipped labels in-context.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Combined with the previous result, we conclude that although instruction tuning improves the ability to learn input-label mappings, it strengthens the usage of semantic prior knowledge more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We examined the extent to which language models learn in-context by utilizing prior knowledge learned during pre-training versus input-label mappings presented in-context. &lt;/p>; &lt;p>; We first showed that large language models can learn to override prior knowledge when presented with enough flipped labels, and that this ability emerges with model scale. We then found that successfully doing ICL using semantically-unrelated labels is another emergent ability of model scale. Finally, we analyzed instruction-tuned language models and saw that instruction tuning improves the capacity to learn input-label mappings but also strengthens the use of semantic prior knowledge even more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; These results underscore how the ICL behavior of language models can change depending on their scale, and that larger language models have an emergent ability to map inputs to many types of labels, a form of reasoning in which input-label mappings can potentially be learned for arbitrary symbols. Future research could help provide insights on why these phenomena occur with respect to model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. We would like to thank Sewon Min and our fellow collaborators at Google Research for their advice and helpful discussions.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/14165165832846745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Larger language models do in-context learning differently&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s72-c/SULICL.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4751112643381771806&lt;/id>;&lt;published>;2023-05-15T10:16:00.003-07:00&lt;/published>;&lt;updated>;2023-05-15T10:24:09.250-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Consensus and subjectivity of skin tone annotation for ML fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Candice Schumann, Software Engineer, and Gbolahan O. Olanubi, User Experience Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s888/MST-E.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Skin tone is an observable characteristic that is subjective, perceived differently by individuals (eg, depending on their location or culture) and thus is complicated to annotate. That said, the ability to reliably and accurately annotate skin tone is highly important in computer vision. This became apparent in 2018, when the &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; study highlighted that computer vision systems struggled to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. The study highlights the importance for computer researchers and practitioners to evaluate their technologies across the full range of skin tones and at intersections of identities. Beyond evaluating model performance on skin tone, skin tone annotations enable researchers to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3375627.3375832&quot;>;measure diversity&lt;/a>; and representation in &lt;a href=&quot;https://arxiv.org/abs/1901.10265&quot;>;image retrieval systems&lt;/a>;, &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;dataset collection&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2202.04053&quot;>;image generation&lt;/a>;. For all of these applications, a collection of meaningful and inclusive skin tone annotations is key. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s1196/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Monk Skin Tone (MST) Scale See more at &lt;a href=&quot;http://skintone.google&quot;>;skintone.google&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Last year, in a step toward more inclusive computer vision systems, Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team in Research partnered with &lt;a href=&quot;https://sociology.fas.harvard.edu/people/ellis-monk&quot;>;Dr. Ellis Monk&lt;/a>; to openly release the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone&lt;/a>; (MST) Scale, a skin tone scale that captures a broad spectrum of skin tones. In comparison to an industry standard scale like the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK481857/table/chapter6.t1/&quot;>;Fitzpatrick Skin-Type Scale&lt;/a>; designed for dermatological use, the MST offers a more inclusive representation across the range of skin tones and was designed for a broad range of applications, including computer vision. &lt;/p>; &lt;p>; Today we&#39;re announcing the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples&lt;/a>; (MST-E) dataset to help practitioners understand the MST scale and train their human annotators. This dataset has been made publicly available to enable practitioners everywhere to create more consistent, inclusive, and meaningful skin tone annotations. Along with this dataset, we&#39;re providing a set of recommendations, noted below, around the MST scale and MST-E dataset so we can all create products that work well for all skin tones. &lt;/p>; &lt;p>; Since we launched the MST, we&#39;ve been using it to improve Google&#39;s computer vision systems to make &lt;a href=&quot;https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/&quot;>;equitable image tools for everyone&lt;/a>; and to &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;improve representation of skin tone in Search&lt;/a>;. Computer vision researchers and practitioners outside of Google, like the curators of &lt;a href=&quot;https://ai.facebook.com/blog/casual-conversations-v2-dataset-measure-fairness/&quot;>;MetaAI&#39;s Casual Conversations&lt;/a>; dataset, are recognizing the value of MST annotations to provide additional insight into diversity and representation in datasets. Incorporation into widely available datasets like these are essential to give everyone the ability to ensure they are building more inclusive computer vision technologies and can test the quality of their systems and products across a wide range of skin tones. &lt;/p>; &lt;p>; Our team has continued to conduct research to understand how we can continue to advance our understanding of skin tone in computer vision. One of our core areas of focus has been skin tone annotation, the process by which human annotators are asked to review images of people and select the best representation of their skin tone. MST annotations enable a better understanding of the inclusiveness and representativeness of datasets across a wide range of skin tones, thus enabling researchers and practitioners to evaluate quality and fairness of their datasets and models. To better understand the effectiveness of MST annotations, we&#39;ve asked ourselves the following questions: &lt;/p>; &lt;ul>; &lt;li>;How do people think about skin tone across geographic locations? &lt;/li>;&lt;li>;What does global consensus of skin tone look like? &lt;/li>;&lt;li>;How do we effectively annotate skin tone for use in inclusive machine learning (ML)? &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;The MST-E dataset&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>; contains 1,515 images and 31 videos of 19 subjects spanning the 10 point MST scale, where the subjects and images were sourced through &lt;a href=&quot;https://tonl.co/&quot;>;TONL&lt;/a>;, a stock photography company focusing on diversity. The 19 subjects include individuals of different ethnicities and gender identities to help human annotators decouple the concept of skin tone from race. The primary goal of this dataset is to enable practitioners to train their human annotators and test for consistent skin tone annotations across various environment capture conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/s1072/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;950&quot; data-original-width=&quot;1072&quot; height=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/w640-h568/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MST-E image set contains 1,515 images and 31 videos featuring 19 models taken under various lighting conditions and facial expressions. Images by TONL. Copyright TONL.CO 2022 ALL RIGHTS RESERVED. Used with permission.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; All images of a subject were collected in a single day to reduce variation of skin tone due to seasonal or other temporal effects. Each subject was photographed in various poses, facial expressions, and lighting conditions. In addition, Dr. Monk annotated each subject with a skin tone label and then selected a “golden” image for each subject that best represents their skin tone. In our research we compare annotations made by human annotators to those made by Dr. Monk, an academic expert in social perception and inequality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Terms of use&lt;/h3>; &lt;p>; Each model selected as a subject provided consent for their images and videos to be released. TONL has given permission for these images to be released as part of MST-E and used for research or human-annotator-training purposes only. The images are not to be used to train ML models. &lt;/p>; &lt;br />; &lt;h2>;Challenges with forming consensus of MST annotations&lt;/h2>; &lt;p>; Although skin tone is easy for a person to see, it can be challenging to systematically annotate across multiple people due to issues with technology and the complexity of human social perception. &lt;/p>; &lt;p>; On the technical side, things like the pixelation, lighting conditions of an image, or a person&#39;s monitor settings can affect how skin tone appears on a screen. You might notice this yourself the next time you change the display setting while watching a show. The hue, saturation, and brightness could all affect how skin tone is displayed on a monitor. Despite these challenges, we find that human annotators are able to learn to become invariant to lighting conditions of an image when annotating skin tone. &lt;/p>; &lt;p>; On the social perception side, aspects of a person&#39;s life like their location, culture, and lived experience may affect how they annotate various skin tones. We found some evidence for this when we asked photographers in the United States and photographers in India to annotate the same image. The photographers in the United States viewed this person as somewhere between MST-5 &amp;amp; MST-7. However, the photographers in India viewed this person as somewhere between MST-3 &amp;amp; MST-5. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s1594/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;1594&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The distribution of Monk Skin Tone Scale annotations for this image from a sample of 5 photographers in the US and 5 photographers in India.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Continuing this exploration, we asked trained annotators from five different geographical regions (India, Philippines, Brazil, Hungary, and Ghana) to annotate skin tone on the MST scale. Within each market each image had 5 annotators who were drawn from a broader pool of annotators in that region. For example, we could have 20 annotators in a market, and select 5 to review a particular image. &lt;/p>; &lt;p>; With these annotations we found two important details. First, annotators within a region had similar levels of agreement on a single image. Second, annotations between regions were, on average, significantly different from each other. (&lt;em>;p&amp;lt;0.05&lt;/em>;). This suggests that people from the same geographic region may have a similar mental model of skin tone, but this mental model is not universal. &lt;/p>; &lt;p>; However, even with these regional differences, we also find that the consensus between all five regions falls close to the MST values supplied by Dr. Monk. This suggests that a geographically diverse group of annotators can get close to the MST value annotated by an MST expert. In addition, after training, we find no significant difference between annotations on well-lit images, versus poorly-lit images, suggesting that annotators can become invariant to different lighting conditions in an image — a non-trivial task for ML models. &lt;/p>; &lt;p>; The MST-E dataset allows researchers to study annotator behavior across curated subsets controlling for potential confounders. We observed similar regional variation when annotating much larger datasets with many more subjects. &lt;/p>; &lt;br />; &lt;h2>;Skin Tone annotation recommendations&lt;/h2>; &lt;p>; Our research includes four major findings. First, annotators within a similar geographical region have a consistent and shared mental model of skin tone. Second, these mental models differ across different geographical regions. Third, the MST annotation consensus from a geographically diverse set of annotators aligns with the annotations provided by an expert in social perception and inequality. And fourth, annotators can learn to become invariant to lighting conditions when annotating MST. &lt;/p>; &lt;p>; Given our research findings, there are a few recommendations for skin tone annotation when using the MST. &lt;/p>; &lt;ol>; &lt;li>;Having a geographically diverse set of annotators is important to gain accurate, or close to ground truth, estimates of skin tone. &lt;/li>;&lt;li>;Train human annotators using the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>;, which spans the entire MST spectrum and contains images in a variety of lighting conditions. This will help annotators become invariant to lighting conditions and appreciate the nuance and differences between the MST points. &lt;/li>;&lt;li>;Given the wide range of annotations we suggest having at least two annotators in at least five different geographical regions (10 ratings per image). &lt;/li>; &lt;/ol>; &lt;p>; Skin tone annotation, like other subjective annotation tasks, is difficult but possible. These types of annotations allow for a more nuanced understanding of model performance, and ultimately help us all to create products that work well for every person across the broad and diverse spectrum of skin tones. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;em>;We wish to thank our colleagues across Google working on fairness and inclusion in computer vision for their contributions to this work, especially Marco Andreetto, Parker Barnes, Ken Burke, Benoit Corda, Tulsee Doshi, Courtney Heldreth, Rachel Hornung, David Madras, Ellis Monk, Shrikanth Narayanan, Utsav Prabhu, Susanna Ricco, Sagar Savla, Alex Siegman, Komal Singh, Biao Wang, and Auriel Wright. We also would like to thank Annie Jean-Baptiste, Florian Koenigsberger, Marc Repnyek, Maura O&#39;Brien, and Dominique Mungin and the rest of the team who help supervise, fund, and coordinate our data collection.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4751112643381771806/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot; rel=&quot;alternate&quot; title=&quot;Consensus and subjectivity of skin tone annotation for ML fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s72-c/MST-E.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6793814472941708824&lt;/id>;&lt;published>;2023-05-12T13:56:00.000-07:00&lt;/published>;&lt;updated>;2023-05-12T13:56:21.572-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;F-VLM: Open-vocabulary object detection upon frozen vision and language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Weicheng Kuo and Anelia Angelova, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s320/F-VLM%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_detection&quot;>;Detection&lt;/a>; is a fundamental vision task that aims to localize and recognize objects in an image. However, the data collection process of manually annotating bounding boxes or instance masks is tedious and costly, which limits the modern detection vocabulary size to roughly 1,000 object classes. This is orders of magnitude smaller than the vocabulary people use to describe the visual world and leaves out many categories. Recent vision and language models (VLMs), such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>;, have demonstrated improved open-vocabulary visual recognition capabilities through learning from Internet-scale image-text pairs. These VLMs are applied to zero-shot classification using frozen model weights without the need for fine-tuning, which stands in stark contrast to the existing paradigms used for retraining or fine-tuning VLMs for &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection tasks&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Intuitively, to align the image content with the text description during training, VLMs may learn region-sensitive and discriminative features that are transferable to object detection. Surprisingly, features of a frozen VLM contain rich information that are both region sensitive for describing object shapes (second column below) and discriminative for region classification (third column below). In fact, feature grouping can nicely delineate object boundaries without any supervision. This motivates us to explore the use of frozen VLMs for open-vocabulary object detection with the goal to expand detection beyond the limited set of annotated categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s1999/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1238&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We explore the potential of frozen vision and language features for open-vocabulary detection. The &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;>;K-Means&lt;/a>; feature grouping reveals rich semantic and region-sensitive information where object boundaries are nicely delineated (column 2). The same frozen features can classify groundtruth (GT) regions well without fine-tuning (column 3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2209.15639&quot;>;F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;ICLR 2023&lt;/a>;, we introduce a simple and scalable open-vocabulary detection approach built upon frozen VLMs. F-VLM reduces the training complexity of an open-vocabulary detector to below that of a standard detector, obviating the need for &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;knowledge distillation&lt;/a>;, detection-tailored pre-training, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_supervision&quot;>;weakly supervised learning&lt;/a>;. We demonstrate that by preserving the knowledge of pre-trained VLMs completely, F-VLM maintains a similar philosophy to &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;>;ViTDet&lt;/a>; and decouples detector-specific learning from the more task-agnostic vision knowledge in the detector backbone. We are also releasing the F-VLM &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm&quot;>;code&lt;/a>; along with a demo on our &lt;a href=&quot;https://sites.google.com/corp/view/f-vlm/home&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning upon frozen vision and language models&lt;/h2>; &lt;p>; We desire to retain the knowledge of pretrained VLMs as much as possible with a view to minimize effort and cost needed to adapt them for open-vocabulary detection. We use a frozen VLM image encoder as the detector backbone and a text encoder for caching the detection text embeddings of offline dataset vocabulary. We take this VLM backbone and attach a &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;detector head&lt;/a>;, which predicts object regions for localization and outputs detection scores that indicate the probability of a detected box being of a certain category. The detection scores are the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; of region features (a set of bounding boxes that the detector head outputs) and category text embeddings. The category text embeddings are obtained by feeding the category names through the text model of pretrained VLM (which has both image and text models)r. &lt;/p>; &lt;p>; The VLM image encoder consists of two parts: 1) a &lt;a href=&quot;https://github.com/openai/CLIP/blob/main/clip/model.py#L139-L151&quot;>;feature extractor&lt;/a>; and 2) a feature &lt;a href=&quot;https://github.com/openai/CLIP/blob/a9b1bf5920416aaeaec965c25dd9e8f98c864f16/clip/model.py#LL152C8-L152C8&quot;>;pooling layer&lt;/a>;. We adopt the feature extractor for detector head training, which is the only step we train (on standard detection &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;data&lt;/a>;), to allow us to directly use frozen weights, inheriting rich semantic knowledge (eg, long-tailed categories like martini, fedora hat, pennant) from the VLM backbone. The detection losses include &lt;a href=&quot;https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/&quot;>;box regression&lt;/a>; and classification losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At training time, F-VLM is simply a detector with the last classification layer replaced by base-category text embeddings.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Region-level open-vocabulary recognition&lt;/h2>; &lt;p>; The ability to perform open-vocabulary recognition at region level (ie, bounding box level as opposed to image level) is integral to F-VLM. Since the backbone features are frozen, they do not overfit to the training categories (eg, donut, zebra) and can be directly cropped for region-level classification. F-VLM performs this open-vocabulary classification only at test time. To obtain the VLM features for a region, we apply the feature pooling layer on the cropped backbone output features. Because the pooling layer requires fixed-size inputs, eg, 7x7 for &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet50&lt;/a>; (R50) &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; backbone, we crop and resize the region features with the &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;ROI-Align layer&lt;/a>; (shown below). Unlike existing open-vocabulary detection &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approaches&lt;/a>;, we do not crop and resize the RGB image regions and cache their embeddings in a separate offline process, but train the detector head in one stage. This is simpler and makes more efficient use of disk storage space.. In addition, we do not crop VLM region features during training because the backbone features are frozen. &lt;/p>; &lt;p>; Despite never being trained on regions, the cropped region features maintain good open-vocabulary recognition capability. However, we observe the cropped region features are not sensitive enough to the localization quality of the regions, ie, a loosely vs. tightly localized box both have similar features. This may be good for classification, but is problematic for detection because we need the detection scores to reflect localization quality as well. To remedy this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geometric_mean&quot;>;geometric mean&lt;/a>; to combine the VLM scores with the detection scores for each region and category. The VLM scores indicate the probability of a detection box being of a certain category according to the pretrained VLM. The detection scores indicate the class probability distribution of each box based on the similarity of region features and input text embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At test time, F-VLM uses the region proposals to crop out the top-level features of the VLM backbone and compute the VLM score per region. The trained detector head provides the detection boxes and masks, while the final detection scores are a combination of detection and VLM scores.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We apply F-VLM to the popular &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, the best F-VLM achieves 32.8 &lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>; (AP) on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;APr&lt;/a>;), which outperforms the state of the art by 6.5 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask APr&lt;/a>; and many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision. F-VLM shows strong scaling property with frozen model capacity, while the number of trainable parameters is fixed. Moreover, F-VLM generalizes and scales well in the transfer detection tasks (eg, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets) by simply replacing the vocabularies without fine-tuning the model. We test the LVIS-trained models on the popular &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; datasets and demonstrate that the model can work very well without training on in-domain detection data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM outperforms &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;the state of the art&lt;/a>; (SOTA) on LVIS open-vocabulary detection benchmark and transfer object detection. On the x-axis, we show the LVIS metric mask AP on rare categories (APr), and the Objects365 (O365) metric box AP on all categories. The sizes of the detector backbones are as follows: Small(R50), Base (R50x4), Large(R50x16), Huge(R50x64). The naming follows CLIP convention.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We visualize F-VLM on open-vocabulary detection and transfer detection tasks (shown below). On LVIS and Objects365, F-VLM correctly detects both novel and common objects. A key benefit of open-vocabulary detection is to test on out-of-distribution data with categories given by users on the fly. See the F-VLM paper for more visualization on &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>;, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1148&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM open-vocabulary and transfer detections. &lt;strong>;Top:&lt;/strong>; Open-vocabulary detection on LVIS. We only show the novel categories for clarity. &lt;strong>;Bottom:&lt;/strong>; Transfer to &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; dataset shows accurate detection of many categories. Novel categories detected: fedora, martini, pennant, football helmet (LVIS); slide (Objects365).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training efficiency&lt;/h2>; &lt;p>; We show that F-VLM can achieve top performance with much less computational resources in the table below. Compared to the state-of-the-art &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approach&lt;/a>;, F-VLM can achieve better performance with 226x fewer resources and 57x faster wall clock time. Apart from training resource savings, F-VLM has potential for substantial memory savings at training time by running the backbone in inference mode. The F-VLM system runs almost as fast as a standard detector at inference time, because the only addition is a single attention pooling layer on the detected region features. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost &lt;br />;(per-core-hour)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost Savings&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;SOTA&lt;/a>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;26.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;460 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;8,000 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;1x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;118 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;565 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;71 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;113x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;27.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;226x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We provide additional results using the shorter &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;>;Detectron2&lt;/a>; training recipes (12 and 36 epochs), and show similarly strong performance by using a frozen backbone. The default setting is marked in gray. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Backbone&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;&lt;a href=&quot;https://arxiv.org/abs/2012.07177&quot;>;Large Scale Jitter&lt;/a>;&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;#Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Batch Size &lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present F-VLM – a simple open-vocabulary detection method which harnesses the power of frozen pre-trained large vision-language models to provide detection of novel objects. This is done without a need for knowledge distillation, detection-tailored pre-training, or weakly supervised learning. Our approach offers significant compute savings and obviates the need for image-level labels. F-VLM achieves the new state-of-the-art in open-vocabulary detection on the LVIS benchmark at system level, and shows very competitive transfer detection on other datasets. We hope this study can both facilitate further research in novel-object detection and help the community explore frozen VLMs for a wider range of vision tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;i>;This work is conducted by Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. We would like to thank our colleagues at Google Research for their advice and helpful discussions. &lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6793814472941708824/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot; rel=&quot;alternate&quot; title=&quot;F-VLM: Open-vocabulary object detection upon frozen vision and language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s72-c/F-VLM%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1014951953767598848&lt;/id>;&lt;published>;2023-05-12T10:03:00.003-07:00&lt;/published>;&lt;updated>;2023-05-12T10:03:43.317-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling conversational interaction on mobile with LLMs&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bryan Wang, Student Researcher, and Yang Li, Research Scientist, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s320/LLM4Mobile%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Intelligent assistants on mobile devices have significantly advanced language-based interactions for performing simple daily tasks, such as setting a timer or turning on a flashlight. Despite the progress, these assistants still face limitations in supporting conversational interactions in mobile user interfaces (UIs), where many user tasks are performed. For example, they cannot answer a user&#39;s question about specific information displayed on a screen. An agent would need to have a computational understanding of &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_user_interface&quot;>;graphical user interfaces&lt;/a>; (GUIs) to achieve such capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Prior research has investigated several important technical building blocks to enable conversational interaction with mobile UIs, including &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3472749.3474765&quot;>;summarizing a mobile screen&lt;/a>; for users to quickly understand its purpose, &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;mapping language instructions to UI actions&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.11103&quot;>;modeling GUIs &lt;/a>;so that they are more amenable for language-based interaction. However, each of these only addresses a limited aspect of conversational interaction and requires considerable effort in curating large-scale datasets and training dedicated models. Furthermore, there is a broad spectrum of conversational interactions that can occur on mobile UIs. Therefore, it is imperative to develop a lightweight and generalizable approach to realize conversational interaction. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/pdf/2209.08655.pdf&quot;>;“Enabling Conversational Interaction with Mobile UI using Large Language Models”&lt;/a>;, presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI 2023&lt;/a>;, we investigate the viability of utilizing large language models (LLMs) to enable diverse language-based interactions with mobile UIs. Recent pre-trained LLMs, such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have demonstrated abilities to adapt themselves to various downstream language tasks when being prompted with a handful of examples of the target task. We present a set of prompting techniques that enable interaction designers and developers to quickly prototype and test novel language interactions with users, which saves time and resources before investing in dedicated datasets and models. Since LLMs only take text tokens as input, we contribute a novel algorithm that generates the text representation of mobile UIs. Our results show that this approach achieves competitive performance using only two data examples per task. More broadly, we demonstrate LLMs&#39; potential to fundamentally transform the future workflow of conversational interaction design.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s1559/LLM4Mobile%201.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;553&quot; data-original-width=&quot;1559&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s16000/LLM4Mobile%201.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing our work on enabling various conversational interactions with mobile UI using LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prompting LLMs with UIs&lt;/h2>; &lt;p>; LLMs support in-context few-shot learning via prompting — instead of fine-tuning or re-training models for each new task, one can prompt an LLM with a few input and output data exemplars from the target task. For many natural language processing tasks, such as question-answering or translation, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompting&lt;/a>; performs competitively with &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;benchmark approaches&lt;/a>; that train a model specific to each task. However, language models can only take text input, while mobile UIs are multimodal, containing text, image, and structural information in their &lt;a href=&quot;https://developer.android.com/topic/performance/rendering/optimizing-view-hierarchies&quot;>;view hierarchy&lt;/a>; data (ie, the structural data containing detailed properties of UI elements) and screenshots. Moreover, directly inputting the view hierarchy data of a mobile screen into LLMs is not feasible as it contains excessive information, such as detailed properties of each UI element, which can exceed the input length limits of LLMs. &lt;/p>; &lt;p>; To address these challenges, we developed a set of techniques to prompt LLMs with mobile UIs. We contribute an algorithm that generates the text representation of mobile UIs using &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth-first_search&quot;>;depth-first search&lt;/a>; traversal to convert the Android UI&#39;s view hierarchy into HTML syntax. We also utilize &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain of thought prompting&lt;/a>;, which involves generating intermediate results and chaining them together to arrive at the final output, to elicit the reasoning ability of the LLM.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s1486/LLM4Mobile%202.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1486&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s16000/LLM4Mobile%202.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing the process of few-shot prompting LLMs with mobile UIs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our prompt design starts with a preamble that explains the prompt&#39;s purpose. The preamble is followed by multiple exemplars consisting of the input, a chain of thought (if applicable), and the output for each task. Each exemplar&#39;s input is a mobile screen in the HTML syntax. Following the input, chains of thought can be provided to elicit logical reasoning from LLMs. This step is not shown in the animation above as it is optional. The task output is the desired outcome for the target tasks, eg, a screen summary or an answer to a user question. Few-shot prompting can be achieved with more than one exemplar included in the prompt. During prediction, we feed the model the prompt with a new input screen appended at the end. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We conducted comprehensive experiments with four pivotal modeling tasks: (1) screen question-generation, (2) screen summarization, (3) screen question-answering, and (4) mapping instruction to UI action. Experimental results show that our approach achieves competitive performance using only two data examples per task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1402&quot; data-original-width=&quot;1999&quot; height=&quot;449&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/w640-h449/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 1: Screen question generation&lt;/h3>; &lt;p>; Given a mobile UI screen, the goal of screen question-generation is to synthesize coherent, grammatically correct natural language questions relevant to the UI elements requiring user input. &lt;/p>; &lt;p>; We found that LLMs can leverage the UI context to generate questions for relevant information. LLMs significantly outperformed the heuristic approach (template-based generation) regarding question quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s1999/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;977&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example screen questions generated by the LLM. The LLM can utilize screen contexts to generate grammatically correct questions relevant to each input field on the mobile UI, while the template approach falls short.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also revealed LLMs&#39; ability to combine relevant input fields into a single question for efficient communication. For example, the filters asking for the minimum and maximum price were combined into a single question: “What&#39;s the price range? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1234&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We observed that the LLM could use its prior knowledge to combine multiple related input fields to ask a single question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In an evaluation, we solicited human ratings on whether the questions were grammatically correct (Grammar) and relevant to the input fields for which they were generated (Relevance). In addition to the human-labeled language quality, we automatically examined how well LLMs can cover all the elements that need to generate questions (Coverage &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1&lt;/a>;). We found that the questions generated by LLM had almost perfect grammar (4.98/5) and were highly relevant to the input fields displayed on the screen (92.8%). Additionally, LLM performed well in terms of covering the input fields comprehensively (95.8%). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;Template &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Grammar &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;3.6 (out of 5) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;4.98 (out of 5)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Relevance &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;84.1% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;92.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Coverage F1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;95.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 2: Screen summarization&lt;/h3>; &lt;p>; Screen summarization is the automatic generation of descriptive language overviews that cover essential functionalities of mobile screens. The task helps users quickly understand the purpose of a mobile UI, which is particularly useful when the UI is not visually accessible. &lt;/p>; &lt;p>; Our results showed that LLMs can effectively summarize the essential functionalities of a mobile UI. They can generate more accurate summaries than the &lt;a href=&quot;https://arxiv.org/abs/2108.03353&quot;>;Screen2Words&lt;/a>; benchmark model that we previously introduced using UI-specific text, as highlighted in the colored text and boxes below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s1999/image10.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1421&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example summary generated by 2-shot LLM. We found the LLM is able to use specific text on the screen to compose more accurate summaries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Interestingly, we observed LLMs using their prior knowledge to deduce information not presented in the UI when creating summaries. In the example below, the LLM inferred the subway stations belong to the London Tube system, while the input UI does not contain this information. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1450&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM uses its prior knowledge to help summarize the screens.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Human evaluation rated LLM summaries as more accurate than the benchmark, yet they scored lower on metrics like &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>;. The mismatch between perceived quality and metric scores echoes &lt;a href=&quot;https://arxiv.org/abs/2209.12356&quot;>;recent work&lt;/a>; showing LLMs write better summaries despite automatic metrics not reflecting it. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;994&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1013&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>;: Screen summarization performance on automatic metrics. &lt;strong>;Right&lt;/strong>;: Screen summarization accuracy voted by human evaluators.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 3: Screen question-answering&lt;/h3>; &lt;p>; Given a mobile UI and an open-ended question asking for information regarding the UI, the model should provide the correct answer. We focus on factual questions, which require answers based on information presented on the screen. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1689&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example results from the screen QA experiment. The LLM significantly outperforms the off-the-shelf QA baseline model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We report performance using four metrics: Exact Matches (identical predicted answer to ground truth), Contains GT (answer fully containing ground truth), Sub-String of GT (answer is a sub-string of ground truth), and the &lt;a href=&quot;https://arxiv.org/abs/1911.03347&quot;>;Micro-F1&lt;/a>; score based on shared words between the predicted answer and ground truth across the entire dataset. &lt;/p>; &lt;p>; Our results showed that LLMs can correctly answer UI-related questions, such as &quot;what&#39;s the headline?&quot;. The LLM performed significantly better than baseline QA model &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;>;DistillBERT&lt;/a>;, achieving a 66.7% fully correct answer rate. Notably, the 0-shot LLM achieved an exact match score of 30.7%, indicating the model&#39;s intrinsic question answering capability. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Exact Matches&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Contains GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Sub-String of GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Micro-F1&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;30.7% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;6.5% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;5.6% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.2% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;65.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;10.0% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;62.9% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;66.7%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;12.6%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;5.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;64.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;DistillBERT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;36.0%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;8.5%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;9.9%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;37.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 4: Mapping instruction to UI action&lt;/h3>; &lt;p>; Given a mobile UI screen and natural language instruction to control the UI, the model needs to predict the ID of the object to perform the instructed action. For example, when instructed with &quot;Open Gmail,&quot; the model should correctly identify the Gmail icon on the home screen. This task is useful for controlling mobile apps using language input such as voice access. We introduced this &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;benchmark task&lt;/a>; previously. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1131&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example using data from the &lt;a href=&quot;https://github.com/google-research-datasets/seq2act&quot;>;PixelHelp dataset&lt;/a>;. The dataset contains interaction traces for common UI tasks such as turning on wifi. Each trace contains multiple steps and corresponding instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We assessed the performance of our approach using the Partial and Complete metrics from the &lt;a href=&quot;https://arxiv.org/abs/2005.03776&quot;>;Seq2Act&lt;/a>; paper. Partial refers to the percentage of correctly predicted individual steps, while Complete measures the portion of accurately predicted entire interaction traces. Although our LLM-based method did not surpass the benchmark trained on massive datasets, it still achieved remarkable performance with just two prompted data examples. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Partial&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Complete&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.29 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.69 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;31.67 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;2-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;75.28 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;34.44 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (in-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;40.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM (in-app)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;80.36&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;45.00&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Seq2Act&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;89.21&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;70.59&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Takeaways and conclusion&lt;/h2>; &lt;p>; Our study shows that prototyping novel language interactions on mobile UIs can be as easy as designing a data exemplar. As a result, an interaction designer can rapidly create functioning mock-ups to test new ideas with end users. Moreover, developers and researchers can explore different possibilities of a target task before investing significant efforts into developing new datasets and models. &lt;/p>; &lt;p>; We investigated the feasibility of prompting LLMs to enable various conversational interactions on mobile UIs. We proposed a suite of prompting techniques for adapting LLMs to mobile UIs. We conducted extensive experiments with the four important modeling tasks to evaluate the effectiveness of our approach. The results showed that compared to traditional machine learning pipelines that consist of expensive data collection and model training, one could rapidly realize novel language-based interactions using LLMs while achieving competitive performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We thank our paper co-author Gang Li, and appreciate the discussions and feedback from our colleagues Chin-Yi Cheng, Tao Li, Yu Hsiao, Michael Terry and Minsuk Chang. Special thanks to Muqthar Mohammad and Ashwin Kakarla for their invaluable assistance in coordinating data collection. We thank John Guilyard for helping create animations and graphics in the blog.&lt;/i>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1014951953767598848/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling conversational interaction on mobile with LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s72-c/LLM4Mobile%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-538722811879850400&lt;/id>;&lt;published>;2023-05-10T08:03:00.004-07:00&lt;/published>;&lt;updated>;2023-05-12T11:51:35.963-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Building better pangenomes to improve the equity of genomics&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Andrew Carroll, Product Lead, and Kishwar Shafin, Research Scientist, Genomics &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s320/image4.png&quot; style=&quot;display: none;&quot; />; &lt;p>; For decades, researchers worked together to assemble a complete copy of the molecular instructions for a human — a map of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_genome&quot;>;human genome&lt;/a>;. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Genome_Project&quot;>;first draft&lt;/a>; was finished in 2000, but with several missing pieces. Even when a complete &lt;a href=&quot;https://en.wikipedia.org/wiki/Reference_genome&quot;>;reference genome&lt;/a>; was &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;achieved in 2022&lt;/a>;, their work was not finished. A single reference genome can&#39;t incorporate known genetic variations, such as the variants for the gene determining whether a person has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Blood_type&quot;>;blood type&lt;/a>; A, B, AB or O. Furthermore, the reference genome &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-8674(19)30231-4&quot;>;didn&#39;t represent the vast diversity of human ancestries&lt;/a>;, making it less useful for detecting disease or finding cures for people from some backgrounds than others. For the past three years, we have been part of an international collaboration with 119 scientists across 60 institutions, called the &lt;a href=&quot;https://www.genome.gov/about-genomics/new-human-pangenome-reference&quot;>;Human Pangenome Research Consortium&lt;/a>;, to address these challenges by creating a new and more representative map of the human genome, a &lt;em>;&lt;a href=&quot;https://www.youtube.com/watch?v=swNtGe9QWAQ&quot;>;pangenome&lt;/a>;&lt;/em>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to share that today, in “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05896-x&quot;>;A draft human pangenome reference&lt;/a>;”, published in &lt;em>;Nature&lt;/em>;, this group is announcing the completion of the first human pangenome reference. The pangenome combines 47 individual genome reference sequences and better represents the genomic diversity of global populations. Building on Google&#39;s deep learning technologies and &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;past advances in genomics&lt;/a>;, we used tools based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; to tackle the challenges of building accurate pangenome sequences and using them for genome analysis. These contributions helped the consortium build an information-rich resource for geneticists, researchers and clinicians around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s1600/Pangenome.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s16000/Pangenome.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Using graphs to build pangenomes &lt;/h2>; &lt;p>; In the typical analysis workflow for high-throughput DNA sequencing, a &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_sequencer&quot;>;sequencing instrument&lt;/a>; reads millions of short pieces of an individual&#39;s genome, and a program called a &lt;a href=&quot;https://en.wikibooks.org/wiki/Next_Generation_Sequencing_(NGS)/Alignment&quot;>;mapper or aligner&lt;/a>; then estimates where those pieces best fit relative to the single, linear human reference sequence. Next, variant caller software identifies the unique parts of the individual&#39;s sequence relative to the reference. &lt;/p>; &lt;p>; But because humans carry a diverse set of sequences, sections that are present in an individual&#39;s DNA but are not in the reference genome can&#39;t be analyzed. One study of 910 African individuals found that a total of &lt;a href=&quot;https://www.theatlantic.com/science/archive/2018/11/human-genome-300-million-missing-letters-dna/576481/&quot;>;300 million DNA base pairs&lt;/a>; — 10% of the roughly three billion base pair reference genome — are not present in the previous linear reference but occur in at least one of the 910 individuals. &lt;/p>; &lt;p>; To address this issue, the consortium used &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph data structures&lt;/a>;, which are powerful for genomics because they can represent the sequences of many people simultaneously, which is needed to create a pangenome. Nodes in a graph genome contain the known set of sequences in a population, and paths through those nodes compactly describe the unique sequences of an individual&#39;s DNA. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s1920/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1081&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Schematic of a graph genome. Each color represents the sequence path of a different individual. Multiple paths passing through the same node indicate multiple individuals share that sequence, but some paths also show a &lt;a href=&quot;https://www.garvan.org.au/research/kinghorn-centre-for-clinical-genomics/learn-about-genomics/for-gp/genetics-refresher-1/types-of-variants&quot;>;single nucleotide variant&lt;/a>; (SNV), insertions, or deletions. Illustration credit Darryl Leja, &lt;a href=&quot;https://www.genome.gov/&quot;>;National Human Genome Research Institute&lt;/a>; (NHGRI).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/s1172/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1172&quot; data-original-width=&quot;1158&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/w632-h640/image1.png&quot; width=&quot;632&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Actual graph genome for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Major_histocompatibility_complex&quot;>;major histocompatibility complex&lt;/a>; (MHC) region of the genome. Genes in MHC regions are essential to immune function and are associated with a person&#39;s resistance and susceptibility to infectious disease and autoimmune disorders (eg, &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fgene.2021.671682/full&quot;>;ankylosing spondylitis&lt;/a>; and &lt;a href=&quot;https://www.hindawi.com/journals/jir/2012/584374/&quot;>;lupus)&lt;/a>;. The graph shows the linear human genome reference (green) and different individual person&#39;s sequence (gray). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Using graphs creates numerous challenges. They require reference sequences to be highly accurate and the development of new methods that can use their data structure as an input. However, new sequencing technologies (such as &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/519025v1&quot;>;consensus sequencing&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2008.01237&quot;>;phased assembly methods&lt;/a>;) have driven exciting progress towards solving these problems. &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.nature.com/articles/s41592-022-01730-w&quot;>;Long-read sequencing technology&lt;/a>;, which reads larger pieces of the genome (10,000 to millions of DNA characters long) at a time, are essential to the creation of high quality reference sequences because larger pieces can be stitched together into assembled genomes more easily than the short pieces read out by earlier technologies. &lt;a href=&quot;https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/short-read-sequencing/&quot;>;Short read sequencing&lt;/a>; reads pieces of the genome that are only 100 to 300 DNA characters long, but has been the highly scalable basis for high-throughput sequencing methods developed in the 2000s. Though long-read sequencing is newer and has advantages for reference genome creation, many informatics methods for short reads hadn&#39;t been developed for long read technologies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evolving DeepVariant for error correction&lt;/h2>; &lt;p>; Google initially developed &lt;a href=&quot;https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html&quot;>;DeepVariant&lt;/a>;, an open-source CNN variant caller framework that analyzes the short-read sequencing evidence of local regions of the genome. However, we were able to re-train DeepVariant to yield &lt;a href=&quot;https://www.nature.com/articles/s41587-019-0217-9&quot;>;accurate analysis of Pacific Bioscience&#39;s long-read data&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s640/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training and evaluation schematic for DeepVariant.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We next teamed up with researchers at the University of California, Santa Cruz (UCSC) &lt;a href=&quot;https://genomics.ucsc.edu/&quot;>;Genomics Institute&lt;/a>; to participate in a &lt;a href=&quot;https://precision.fda.gov/challenges/10&quot;>;United States Food and Drug Administration competition&lt;/a>; for another &lt;a href=&quot;https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html&quot;>;long-read sequencing technology from Oxford Nanopore&lt;/a>;. Together, we won the award for highest accuracy in the &lt;a href=&quot;https://nanoporetech.com/&quot;>;nanopore&lt;/a>; category, with a single nucleotide variants (SNVs) accuracy that matched short-read sequencing. This work has been used to &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;detect and treat genetic diseases in critically ill newborns&lt;/a>;. The use of DeepVariant on long-read technologies provided the foundation for the consortium&#39;s use of DeepVariant for error correction of pangenomes. &lt;/p>; &lt;p>; DeepVariant&#39;s ability to use multiple long-read sequencing modalities proved useful for error correction in the &lt;a href=&quot;https://sites.google.com/corp/ucsc.edu/t2tworkinggroup&quot;>;Telomere-to-Telomere (T2T) Consortium&lt;/a>;&#39;s effort that generated &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;the first complete assembly of a human genome&lt;/a>;. Completing this first genome set the stage to build the multiple reference genomes required for pangenomes, and T2T was already working closely with the &lt;a href=&quot;https://humanpangenomeproject.org/&quot;>;Human Pangenome Project&lt;/a>; (with many shared members) to scale those practices. &lt;/p>; &lt;p>; With a set of high-quality human reference genomes on the horizon, developing methods that could use those assemblies grew in importance. We worked to adapt DeepVariant to use the pangenome developed by the consortium. In partnership with UCSC, we built an end-to-end analysis workflow for &lt;a href=&quot;https://genome.cshlp.org/content/32/5/893.short&quot;>;graph-based variant detection&lt;/a>;, and demonstrated &lt;a href=&quot;https://www.science.org/doi/abs/10.1126/science.abg8871&quot;>;improved accuracy across several thousand samples&lt;/a>;. The use of the pangenome allows many previously missed variants to be correctly identified. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;258&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualization of variant calls in the &lt;a href=&quot;https://medlineplus.gov/genetics/gene/kcne1/&quot;>;KCNE1 gene&lt;/a>; (a gene with variants associated with cardiac arrhythmias and &lt;a href=&quot;https://www.ahajournals.org/doi/pdf/10.1161/JAHA.117.007837&quot;>;sudden death&lt;/a>;) using a pangenome reference versus the prior linear reference. Each dot represents a variant call that is either correct (&lt;strong>;blue dot&lt;/strong>;), incorrect (&lt;strong>;green dot&lt;/strong>;) — when a variant is identified but is not really there —or a missed variant call (&lt;strong>;red dot&lt;/strong>;). The top box shows variant calls made by DeepVariant using the pangenome reference while the bottom shows variant calls made by using the linear reference. Figure adapted from &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.07.09.499321v1&quot;>;A Draft Human Pangenome Reference.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improving pangenome sequences using transformers &lt;/h2>; &lt;p>; Just as new sequencing technologies enabled new pangenome approaches, new informatics technologies enabled improvements for sequencing methods. Google adapted transformer architectures from analysis of human language to genome sequences to develop &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.08.31.458403v1&quot;>;DeepConsensus&lt;/a>;. A key enabler for this was the development of a differentiable loss function that could handle the insertions and deletions common in sequencing data. This enabled us to have high accuracy without needing a decoder, allowing the speed required to keep up with terabytes of sequencer output. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/s940/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;647&quot; data-original-width=&quot;940&quot; height=&quot;441&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/w640-h441/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer architecture for DeepConsensus. DeepConsensus takes as input the repeated sequence of the DNA molecule, measured from fluorescent light detected by the addition of each base. DeepConsensus also uses as input the more detailed information about the sequencing process, including the duration of the light pulse (referred to here as pulse width or PW), the time between pulses (IP) the signal-to-noise ratio (SN) and which side of the double helix is being measured (strand).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;794&quot; data-original-width=&quot;1999&quot; height=&quot;254&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/w640-h254/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of alignment loss function in training evaluation of model output. Better accounting of insertions and deletions by a differentiable alignment function enables the model training process to better estimate errors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; DeepConsensus &lt;a href=&quot;https://blog.google/technology/health/a-new-genome-sequencing-tool-powered-with-our-technology/&quot;>;improves the yield and accuracy of instrument&lt;/a>; data. Because PacBio sequencing provides the primary sequence information for the 47 genome assemblies, we could apply DeepConsensus to improve those assemblies. With application of DeepConsensus, consortium members &lt;a href=&quot;https://www.nature.com/articles/s41587-023-01662-6&quot;>;built a genome assembler&lt;/a>; that was able to reach 99.9997% assembly base-level accuracies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We developed multiple new approaches to improve genetic sequencing methods, which we then used to construct pangenome references that enable more robust genome analysis. &lt;/p>; &lt;p>; But this is just the beginning of the story. In the next stage, a larger, worldwide group of scientists and clinicians will use this pangenome reference to study genetic diseases and make new drugs. And future pangenomes will represent even more individuals, realizing a vision summarized this way in a recent Nature story: “&lt;a href=&quot;https://www.nature.com/articles/d41586-023-01300-w&quot;>;Every base, everywhere, all at once&lt;/a>;.” &lt;em>;Read our post on the &lt;a href=&quot;https://blog.google/technology/health/first-pangenome-reference-nature-paper-ai/&quot;>;Keyword Blog&lt;/a>; to learn more about the human pangenome reference announcement.&lt;/em>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;Many people were involved in creating the pangenome reference, including 119 authors across 60 organizations, with the Human Pangenome Reference Consortium. This blog post highlights Google&#39;s contributions to the broader work. We thank the research groups at UCSC Genomics Institute (GI) under Professors Benedict Paten and Karen Miga, genome polishing efforts of Arang Rhie at National Institute of Health (NIH), Genome Assembly and Polishing of Adam Phillipy&#39;s group, and the standards group at National Institute of Standards and Technology (NIST) of Justin Zook. We thank Google contributors: Pi-Chuan Chang, Maria Nattestad, Daniel Cook, Alexey Kolesnikov, Anastaysia Belyaeva, and Gunjan Baid. We thank John Guilyard for his illustrative animation, and Lizzie Dorfman, Elise Kleeman, Erika Hayden, Cory McLean, Shravya Shetty, Greg Corrado, Katherine Chou, and Yossi Matias for their support, coordination, and leadership. Last but not least, thanks to the research participants that provided their DNA to help build the pangenome resource.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/538722811879850400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html&quot; rel=&quot;alternate&quot; title=&quot;Building better pangenomes to improve the equity of genomics&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s72-c/image4.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7102791933497880989&lt;/id>;&lt;published>;2023-05-04T14:59:00.004-07:00&lt;/published>;&lt;updated>;2023-05-04T14:59:57.911-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s800/MAMMUT.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Vision-language foundational models are built on the premise of a single pre-training followed by subsequent adaptation to multiple downstream tasks. Two main and disjoint training scenarios are popular: a &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;-style contrastive learning and next-token prediction. Contrastive learning trains the model to predict if image-text pairs correctly match, effectively building visual and text representations for the corresponding image and text inputs, whereas next-token prediction predicts the most likely next text token in a sequence, thus learning to generate text, according to the required task. &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;Contrastive learning&lt;/a>; enables &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval tasks&lt;/a>;, such as finding the image that best matches a certain description, and next-token learning enables text-generative tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation#Image_captioning&quot;>;Image Captioning&lt;/a>; and &lt;a href=&quot;https://visualqa.org/&quot;>;Visual Question Answering&lt;/a>; (VQA). While both approaches have demonstrated powerful results, when a model is pre-trained contrastively, it typically does not fare well on text-generative tasks and vice-versa. Furthermore, adaptation to other tasks is often done with complex or inefficient methods. For example, in order to extend a vision-language model to videos, some models need to do inference for each video frame separately. This limits the size of the videos that can be processed to only a few frames and does not fully take advantage of motion information available across frames. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Motivated by this, we present “&lt;a href=&quot;https://arxiv.org/abs/2303.16839&quot;>;A Simple Architecture for Joint Learning for MultiModal Tasks&lt;/a>;”, called MaMMUT, which is able to train jointly for these competing objectives and which provides a foundation for many vision-language tasks either directly or via simple adaptation. MaMMUT is a compact, 2B-parameter multimodal model that trains across contrastive, text generative, and localization-aware objectives. It consists of a single image encoder and a text decoder, which allows for a direct reuse of both components. Furthermore, a straightforward adaptation to video-text tasks requires only using the image encoder once and can handle many more frames than prior work. In line with recent language models (eg, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;>;GLaM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;GPT3&lt;/a>;), our architecture uses a decoder-only text model and can be thought of as a simple extension of language models. While modest in size, our model outperforms the state of the art or achieves competitive performance on &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (VideoQA), &lt;a href=&quot;https://ai.googleblog.com/2022/06/end-to-end-generative-pre-training-for.html&quot;>;video captioning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>;, and &lt;a href=&quot;https://visualqa.org/&quot;>;VQA&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s1182/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;1182&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s980/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s1120/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MaMMUT model enables a wide range of tasks such as image-text/text-image retrieval (&lt;b>;top left&lt;/b>; and &lt;b>;top right&lt;/b>;), VQA (&lt;b>;middle left&lt;/b>;), open-vocabulary detection (&lt;b>;middle right&lt;/b>;), and VideoQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder-only model architecture&lt;/h2>; &lt;p>; One surprising finding is that a single language-decoder is sufficient for all these tasks, which obviates the need for both complex constructs and training procedures presented before. For example, our model (presented to the left in the figure below) consists of a single visual encoder and single text-decoder, connected via &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;cross attention&lt;/a>;, and trains simultaneously on both contrastive and text-generative types of losses. Comparatively, prior work is either not able to handle image-text retrieval tasks, or applies only some losses to only some parts of the model. To enable multimodal tasks and fully take advantage of the decoder-only model, we need to jointly train both contrastive losses and text-generative captioning-like losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s1191/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1191&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT architecture (&lt;b>;left&lt;/b>;) is a simple construct consisting of a single vision encoder and a single text decoder. Compared to other popular vision-language models — eg, &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; (&lt;b>;middle&lt;/b>;) and &lt;a href=&quot;https://arxiv.org/abs/2107.07651&quot;>;ALBEF&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot;>;CoCa&lt;/a>; (&lt;b>;right&lt;/b>;) — it trains jointly and efficiently for multiple vision-language tasks, with both contrastive and text-generative losses, fully sharing the weights between the tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder two-pass learning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1801.10198&quot;>;Decoder-only models&lt;/a>; for language learning show clear advantages in performance with smaller model size (almost half the parameters). The main challenge for applying them to multimodal settings is to unify the contrastive learning (which uses unconditional sequence-level representation) with captioning (which optimizes the likelihood of a token conditioned on the previous tokens). We propose a two-pass approach to jointly learn these two conflicting types of text representations within the decoder. During the first pass, we utilize cross attention and &lt;a href=&quot;https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer&quot;>;causal masking&lt;/a>; to learn the caption generation task — the text features can attend to the image features and predict the tokens in sequence. On the second pass, we disable the cross-attention and causal masking to learn the contrastive task. The text features will not see the image features but can attend bidirectionally to all text tokens at once to produce the final text-based representation. Completing this two-pass approach within the same decoder allows for accommodating both types of tasks that were previously hard to reconcile. While simple, we show that this model architecture is able to provide a foundation for multiple multimodal tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s670/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;670&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT decoder-only two-pass learning enables both contrastive and generative learning paths by the same model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Another advantage of our architecture is that, since it is trained for these disjoint tasks, it can be seamlessly applied to multiple applications such as image-text and text-image retrieval, VQA, and captioning. &lt;/p>; &lt;p>; Moreover, MaMMUT easily adapts to video-language tasks. Previous approaches used a vision encoder to process each frame individually, which required applying it multiple times. This is slow and restricts the number of frames the model can handle, typically to only 6–8. With MaMMUT, we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>; for lightweight adaptation directly via the spatio-temporal information from the video. Furthermore, adapting the model to &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;Open-Vocabulary Detection&lt;/a>; is done by simply training to detect bounding-boxes via an object-detection head. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s632/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;204&quot; data-original-width=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Adaptation of the MaMMUT architecture to video tasks (&lt;b>;left&lt;/b>;) is simple and fully reuses the model. This is done by generating a video “tubes” feature representation, similar to image patches, that are projected to lower dimensional tokens and run through the vision encoder. Unlike prior approaches (&lt;b>;right&lt;/b>;) that need to run multiple individual images through the vision encoder, we use it only once.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Our model achieves excellent zero-shot results on image-text and text-image retrieval without any adaptation, outperforming all previous state-of-the-art models. The results on VQA are competitive with state-of-the-art results, which are achieved by much larger models. The &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI model&lt;/a>; (17B parameters) and the &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo model&lt;/a>; (80B) have the best performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>;, but MaMMUT (2B) has the same accuracy as the 15B PaLI. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s600/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the state of the art (SOTA) on Zero-Shot Image-Text (I2T) and Text-Image (T2I) retrieval on both &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; (&lt;b>;top&lt;/b>;) and &lt;a href=&quot;https://ieeexplore.ieee.org/document/7410660&quot;>;Flickr&lt;/a>; (&lt;b>;bottom&lt;/b>;) benchmarks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>; is competitive but does not outperform large models such as Flamingo-80B and PalI-17B. Performance is evaluated in the more challenging open-ended text generation setting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MaMMUT also outperforms the state-of-the-art on VideoQA, as shown below on the &lt;a href=&quot;https://ieeexplore.ieee.org/document/7780940&quot;>;MSRVTT-QA&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3123266.3123427&quot;>;MSVD-QA&lt;/a>; datasets. Note that we outperform much bigger models such as &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo&lt;/a>;, which is specifically designed for image+video pre-training and is pre-trained with both image-text and video-text data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s600/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s600/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the SOTA models on VideoQA tasks (MSRVTT-QA dataset, &lt;b>;top&lt;/b>;, MSVD-QA dataset,&lt;b>; bottom&lt;/b>;), outperforming much larger models, eg, the 5B GIT2 or Flamingo, which uses 80B parameters and is pre-trained for both image-language and vision-language tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results outperform the state-of-the-art on open-vocabulary detection fine-tuning as is also shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MAMMUT &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>; results on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; dataset compared to state-of-the-art methods. We report the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;average precisions for rare classes&lt;/a>; (APr) as is previously adopted in the literature.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key ingredients&lt;/h2>; &lt;p>; We show that joint training of both contrastive and text-generative objectives is not an easy task, and in our ablations we find that these tasks are served better by different design choices. We see that fewer cross-attention connections are better for retrieval tasks, but more are preferred by VQA tasks. Yet, while this shows that our model&#39;s design choices might be suboptimal for individual tasks, our model is more effective than more complex, or larger, models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s600/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s16000/image11.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s600/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Ablation studies showing that fewer cross-attention connections (1-2) are better for retrieval tasks (&lt;b>;top&lt;/b>;), whereas more connections favor text-generative tasks such as VQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented MaMMUT, a simple and compact vision-encoder language-decoder model that jointly trains a number of conflicting objectives to reconcile contrastive-like and text-generative tasks. Our model also serves as a foundation for many more vision-language tasks, achieving state-of-the-art or competitive performance on image-text and text-image retrieval, videoQA, video captioning, open-vocabulary detection and VQA. We hope it can be further used for more multimodal applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, and Anelia Angelova. We would like to thank Mojtaba Seyedhosseini, Vijay Vasudevan, Priya Goyal, Jiahui Yu, Zirui Wang, Yonghui Wu, Runze Li, Jie Mei, Radu Soricut, Qingqing Huang, Andy Ly, Nan Du, Yuxin Wu, Tom Duerig, Paul Natsev, Zoubin Ghahramani for their help and support. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7102791933497880989/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot; rel=&quot;alternate&quot; title=&quot;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s72-c/MAMMUT.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3611317650594898640&lt;/id>;&lt;published>;2023-05-03T10:22:00.000-07:00&lt;/published>;&lt;updated>;2023-05-03T10:22:54.056-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Joanne Truong, Student Researcher, and Wenhao Yu, Research Scientist, Robotics at Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s320/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Teaching mobile robots to navigate in complex outdoor environments is critical to real-world applications, such as delivery or &lt;a href=&quot;https://www.nasa.gov/stem-ed-resources/robotic-search-and-rescue-challenge.html&quot;>;search and rescue&lt;/a>;. However, this is also a challenging problem as the robot needs to perceive its surroundings, and then explore to identify feasible paths towards the goal. Another common challenge is that the robot needs to overcome uneven terrains, such as stairs, curbs, or rockbed on a trail, while avoiding obstacles and pedestrians. In our prior work, we investigated the second challenge by teaching a quadruped robot to tackle challenging &lt;a href=&quot;https://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html&quot;>;uneven obstacles&lt;/a>; and &lt;a href=&quot;https://research.google/pubs/pub51639/&quot;>;various outdoor terrains&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/pdf/2305.01098.pdf&quot;>;IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience&lt;/a>;”, we present our recent work to tackle the robotic challenge of reasoning about the perceived surroundings to identify a viable navigation path in outdoor environments. We introduce a learning-based indoor-to-outdoor transfer algorithm that uses deep reinforcement learning to train a navigation policy in simulated indoor environments, and successfully transfers that same policy to real outdoor environments. We also introduce Context-Maps (maps with environment observations created by a user), which are applied to our algorithm to enable efficient long-range navigation. We demonstrate that with this policy, robots can successfully navigate hundreds of meters in novel outdoor environments, around previously unseen outdoor obstacles (trees, bushes, buildings, pedestrians, etc.), and in different weather conditions (sunny, overcast, sunset). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/teaser.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;PointGoal navigation&lt;/h2>; &lt;p>; User inputs can tell a robot where to go with commands like “go to the Android statue”, pictures showing a target location, or by simply picking a point on a map. In this work, we specify the navigation goal (a selected point on a map) as a relative coordinate to the robot&#39;s current position (ie, “go to ∆x, ∆y”), this is also known as the &lt;a href=&quot;https://arxiv.org/abs/1807.06757&quot;>;PointGoal Visual Navigation&lt;/a>; (PointNav) task. PointNav is a general formulation for navigation tasks and is one of the standard choices for indoor navigation tasks. However, due to the diverse visuals, uneven terrains and long distance goals in outdoor environments, training PointNav policies for outdoor environments is a challenging task. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Indoor-to-outdoor transfer&lt;/h2>; &lt;p>; Recent successes in training wheeled and legged robotic agents to navigate in &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf&quot;>;indoor&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1911.00357.pdf&quot;>;environments&lt;/a>; were enabled by the development of fast, scalable simulators and the availability of large-scale datasets of &lt;a href=&quot;https://aihabitat.org/datasets/hm3d/&quot;>;photorealistic&lt;/a>; &lt;a href=&quot;https://svl.stanford.edu/igibson/&quot;>;3D scans&lt;/a>; &lt;a href=&quot;https://niessner.github.io/Matterport/&quot;>;of indoor&lt;/a>; &lt;a href=&quot;https://ai2thor.allenai.org/&quot;>;environments&lt;/a>;. To leverage these successes, we develop an indoor-to-outdoor transfer technique that enables our robots to learn from simulated indoor environments and to be deployed in real outdoor environments. &lt;/p>; &lt;p>; To overcome the differences between simulated indoor environments and real outdoor environments, we apply &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;kinematic control&lt;/a>; and image augmentation techniques in our learning system. When using kinematic control, we assume the existence of a reliable low-level &lt;a href=&quot;https://dev.bostondynamics.com/docs/concepts/autonomy/graphnav_and_robot_locomotion&quot;>;locomotion controller&lt;/a>; that can control the robot to precisely reach a new location. This assumption allows us to directly move the robot to the target location during simulation training through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler_method&quot;>;forward Euler integration&lt;/a>; and relieves us from having to explicitly model the underlying robot dynamics in simulation, which drastically improves the throughput of simulation data generation. &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;Prior work&lt;/a>; has shown that kinematic control can lead to better sim-to-real transfer compared to a &lt;a href=&quot;https://arxiv.org/pdf/2008.11867.pdf&quot;>;dynamic control approach&lt;/a>;, where full robot dynamics are modeled and a low-level locomotion controller is required for moving the robot. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/kin_v_dyn.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>; Kinematic control; &lt;strong>;Right&lt;/strong>;: Dynamic control&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We created an outdoor maze-like environment using objects found indoors for initial experiments, where we used Boston Dynamics&#39; &lt;a href=&quot;https://www.bostondynamics.com/products/spot&quot;>;Spot robot&lt;/a>; for test navigation. We found that the robot could navigate around novel obstacles in the new outdoor environment. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_short_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Spot robot successfully navigates around obstacles found in indoor environments, with a policy trained entirely in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; However, when faced with unfamiliar outdoor obstacles not seen during training, such as a large slope, the robot was unable to navigate the slope. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The robot is unable to navigate up slopes, as slopes are rare in indoor environments and the robot was not trained to tackle it.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to walk up and down slopes, we apply an image augmentation technique during the simulation training. Specifically, we randomly tilt the simulated camera on the robot during training. It can be pointed up or down within 30 degrees. This augmentation effectively makes the robot perceive slopes even though the floor is level. Training on these perceived slopes enables the robot to navigate slopes in the real-world. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By randomly tilting the camera angle during training in simulation, the robot is now able to walk up and down slopes.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Since the robots were only trained in simulated indoor environments, in which they typically need to walk to a goal just a few meters away, we find that the learned network failed to process longer-range inputs — eg, the policy failed to walk forward for 100 meters in an empty space. To enable the policy network to handle long-range inputs that are common for outdoor navigation, we normalize the goal vector by using the log of the goal distance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Context-Maps for complex long-range navigation &lt;/h2>; &lt;p>; Putting everything together, the robot can navigate outdoors towards the goal, while walking on uneven terrain, and avoiding trees, pedestrians and other outdoor obstacles. However, there is still one key component missing: the robot&#39;s ability to plan an efficient long-range path. At this scale of navigation, taking a wrong turn and backtracking can be costly. For example, we find that the local exploration strategy learned by standard PointNav policies are insufficient in finding a long-range goal and usually leads to a dead end (shown below). This is because the robot is navigating without context of its environment, and the optimal path may not be visible to the robot from the start. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_long_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Navigation policies without context of the environment do not handle complex long-range navigation goals.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to take the context into consideration and purposefully plan an efficient path, we provide a Context-Map (a binary image that represents a top-down occupancy map of the region that the robot is within) as additional observations for the robot. An example Context-Map is given below, where the black region denotes areas occupied by obstacles and white region is walkable by the robot. The green and red circle denotes the start and goal location of the navigation task. Through the Context-Map, we can provide hints to the robot (eg, the narrow opening in the route below) to help it plan an efficient navigation route. In our experiments, we create the Context-Map for each route guided by &lt;a href=&quot;https://www.google.com/maps&quot;>;Google Maps satellite&lt;/a>; images. We denote this variant of PointNav with environmental context, as &lt;em>;Context-Guided PointNav&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/s1113/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1113&quot; height=&quot;350&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/w640-h350/image12.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the Context-Map (&lt;strong>;right&lt;/strong>;) for a navigation task (&lt;strong>;left&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; It is important to note that the Context-Map does not need to be accurate because it only serves as a rough outline for planning. During navigation,&lt;strong>; &lt;/strong>;the robot still needs to rely on its onboard cameras to identify and adapt its path to pedestrians, which are absent on the map. In our experiments, a human operator quickly sketches the Context-Map from the satellite image, masking out the regions to be avoided. This Context-Map, together with other onboard sensory inputs, including depth images and relative position to the goal, are fed into a neural network with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention&lt;/a>; models (ie, &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>;), which are trained using &lt;a href=&quot;https://arxiv.org/abs/1911.00357&quot;>;DD-PPO&lt;/a>;, a distributed implementation of &lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;>;proximal policy optimization&lt;/a>;, in large-scale simulations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/s782/image15.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;782&quot; height=&quot;442&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/w640-h442/image15.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Context-Guided PointNav architecture consists of a 3-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) to process depth images from the robot&#39;s camera, and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) to process the goal vector. The features are passed into a &lt;a href=&quot;https://arxiv.org/abs/1406.1078&quot;>;gated recurrent unit&lt;/a>; (GRU). We use an additional CNN encoder to process the context-map (top-down map). We compute the &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;scaled dot product attention&lt;/a>; between the map and the depth image, and use a second GRU to process the attended features (Context Attn., Depth Attn.). The output of the policy are linear and angular velocities for the Spot robot to follow. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our system across three long-range outdoor navigation tasks. The provided Context-Maps are rough, incomplete environment outlines that omit obstacles, such as cars, trees, or chairs. &lt;/p>; &lt;p>; With the proposed algorithm, our robot can successfully reach the distant goal location 100% of the time, without a single collision or human intervention. The robot was able to navigate around pedestrians and real-world clutter that are not present on the context-map, and navigate on various terrain including dirt slopes and grass. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 1&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s1999/image14.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s16000/image14.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 2&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s1999/image16.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;697&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 3&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s1999/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work opens up robotic navigation research to the less explored domain of diverse outdoor environments. Our indoor-to-outdoor transfer algorithm uses zero real-world experience and does not require the simulator to model predominantly-outdoor phenomena (terrain, ditches, sidewalks, cars, etc). The success in the approach comes from a combination of a robust locomotion control, low sim-to-real gap in depth and map sensors, and large-scale training in simulation. We demonstrate that providing robots with approximate, high-level maps can enable long-range navigation in novel outdoor environments. Our results provide compelling evidence for challenging the (admittedly reasonable) hypothesis that a new simulator must be designed for every new scenario we wish to study. For more information, please see our &lt;a href=&quot;https://indoorsim2outdoorreal.github.io/&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We would like to thank Sonia Chernova, Tingnan Zhang, April Zitkovich, Dhruv Batra, and Jie Tan for advising and contributing to the project. We would also like to thank Naoki Yokoyama, Nubby Lee, Diego Reyes, Ben Jyenis, and Gus Kouretas for help with the robot experiment setup.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3611317650594898640/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot; rel=&quot;alternate&quot; title=&quot;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s72-c/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6812586756904863062&lt;/id>;&lt;published>;2023-04-30T23:37:00.004-07:00&lt;/published>;&lt;updated>;2023-05-17T15:37:33.732-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICLR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaXU0Qnf7lWC70SIqFUj3KcinepmKmqVwu369RYcajoFZI0UGtej6ZGJD7c0GsXJk_Wjg08CnBLtE7lLlz__azqzUNCgflaTp3F9mZyIxX7HBzJEdTN19WMlfVsu4mTww-LLhvZ4hhWqdVCQqjQbrOk6VumBCH6dA2YwdATT6I_QLKLHSUz_Upnw6zlg/s1200/Google%20Rwanda%20Logo-02.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;Eleventh International Conference on Learning Representations&lt;/a>; (ICLR 2023) is being held this week as a hybrid event in Kigali, Rwanda. We are proud to be a &lt;a href=&quot;https://iclr.cc/Conferences/2023/Sponsors&quot;>;Diamond Sponsor&lt;/a>; of ICLR 2023, a premier conference on deep learning, where Google researchers contribute at all levels. This year we are presenting over 100 papers and are actively involved in organizing and hosting a number of different events, including workshops and interactive sessions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you&#39;re registered for ICLR 2023, we hope you&#39;ll visit the Google booth to learn more about the exciting work we&#39;re doing across topics spanning representation and reinforcement learning, theory and optimization, social impact, safety and privacy, and applications from generative AI to speech and robotics. Continue below to find the many ways in which Google researchers are engaged at ICLR 2023, including workshops, papers, posters and talks (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;p>; Board Members include: &lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Senior Program Chairs include: &lt;strong>;&lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Workshop Chairs include: &lt;strong>;&lt;em>;Aisha Walcott-Bryant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rose Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Diversity, Equity &amp;amp; Inclusion Chairs include: &lt;strong>;&lt;em>;Rosanne Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outstanding Paper awards&lt;/h2>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lTt4KjHSsyl&quot;>;Emergence of Maps in the Memories of Blind Navigation Agents&lt;/a>; &lt;br />; &lt;em>;Erik Wijmans&lt;/em>;, &lt;em>;Manolis Savva&lt;/em>;, &lt;strong>;&lt;em>;Irfan Essa&lt;/em>;&lt;/strong>;, &lt;em>;Stefan Lee&lt;/em>;, &lt;em>;Ari S. Morcos&lt;/em>;, &lt;em>;Dhruv Batra&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FjNys5c7VyY&quot;>;DreamFusion: Text-to-3D Using 2D Diffusion&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>;, &lt;em>;Ajay Jain&lt;/em>;, &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ben Mildenhall&lt;/em>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Keynote speaker&lt;/h2>; &lt;p>; &lt;a href=&quot;https://iclr.cc/virtual/2023/invited-talk/14236&quot;>;Learned Optimizers: Why They&#39;re the Future, Why They&#39;re Hard, and What They Can Do Now&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jascha Sohl-Dickstein &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://www.kaggle.com/iclr-2023&quot;>;Kaggle@ICLR 2023: ML Solutions in Africa&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Julia Elliott&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Phil Culliton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ray Harvey &lt;/i>;&lt;/b>;&lt;br />; Facilitators: &lt;b>;&lt;i>;Julia Elliot&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Walter Reade &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://reincarnating-rl.github.io/&quot;>;Reincarnating Reinforcement Learning&lt;/a>; (Reincarnating RL) &lt;br />; Organizers include: &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ted Xiao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Max Schwarzer &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;br />; Panelists include: &lt;b>;&lt;i>;Marc G. Bellemare&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://rtml-iclr2023.github.io/organizers.html&quot;>;Trustworthy and Reliable Large-Scale Machine Learning Models&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Sanmi Koyejo &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Nicholas Carlini &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://physics4ml.github.io/organizers&quot;>;Physics for Machine Learning&lt;/a>; (Physics4ML) &lt;br />; Speakers include: &lt;b>;&lt;i>;Yasaman Bahri &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4abm.org/workshop_iclr2023/&quot;>;AI for Agent-Based Modelling Community&lt;/a>; (AI4ABM) &lt;br />; Organizers include: &lt;b>;&lt;i>;Pablo Samuel Castro &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/me-fomo2023&quot;>;Mathematical and Empirical Understanding of Foundation Models&lt;/a>; (ME-FoMo) &lt;br />; Organizers include: &lt;b>;&lt;i>;Mathilde Caron&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tengyu Ma&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanie Sedghi &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Yasaman Bahri&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yann Dauphin &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/&quot;>;Neurosymbolic Generative Models 2023&lt;/a>; (NeSy-GeMs) &lt;br />; Organizers include: &lt;b>;&lt;i>;Kevin Ellis &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Daniel Tarlow&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tuan Anh Le &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://domaingen.github.io/&quot;>;What Do We Need for Successful Domain Generalization?&lt;/a>; &lt;br />; Panelists include: &lt;b>;&lt;i>;Boqing Gong &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://pml4dc.github.io/iclr2023/&quot;>;The 4th Workshop on Practical ML for Developing Countries: Learning Under Limited/Low Resource Settings&lt;/a>; &lt;br />; Keynote Speaker: &lt;b>;&lt;i>;Adji Bousso Dieng &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://nasaharvest.github.io/ml-for-remote-sensing/iclr2023/#schedule&quot;>;Machine Learning for Remote Sensing&lt;/a>; &lt;br />; Speakers include: &lt;b>;&lt;i>;Abigail Annkah &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://mrl-workshop.github.io/iclr-2023/&quot;>;Multimodal Representation Learning (MRL): Perks and Pitfalls&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Petra Poklukar &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Arsha Nagrani &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/trustml-unlimited/home&quot;>;Pitfalls of Limited Data and Computation for Trustworthy ML&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Praneeth Netrapalli &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/trustml-unlimited/home&quot;>;Sparsity in Neural Networks: On Practical Limitations and Tradeoffs Between Sustainability and Efficiency&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Trevor Gale&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Utku Evci &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Aakanksha Chowdhery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeff Dean &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/tsrl4h-iclr2023/home&quot;>;Time Series Representation Learning for Health&lt;/a>; &lt;br />; Speakers include: &lt;b>;&lt;i>;Katherine Heller &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl4c.github.io/&quot;>;Deep Learning for Code&lt;/a>; (DL4C) &lt;br />; Organizers include: &lt;b>;&lt;i>;Gabriel Orlanski &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Alex Polozov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Tarlow &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://mlgh-2023.netlify.app/&quot;>;Machine Learning and Global Health&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Mercy Asiedu &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Joelle Barral&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Affinity Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://iclr.cc/virtual/2023/affinity-workshop/13838&quot;>;Tiny Papers Showcase Day&lt;/a>; (a DEI initiative) &lt;br />; Organizers include: &lt;b>;&lt;i>;Rosanne Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Papers&lt;/h2>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Z4s7sJYQM&quot;>;Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics for Advection-Dominated Systems&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhong Yi Wan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Leonardo Zepeda-Nunez&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anudhyan Boral&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fei Sha &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TatRHT_1cK&quot;>;Quantifying Memorization Across Neural Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daphne Ippolito&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Jagielski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katherine Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Florian Tramer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lTt4KjHSsyl&quot;>;Emergence of Maps in the Memories of Blind Navigation Agents&lt;/a>; (&lt;em>;Outstanding Paper Award&lt;/em>;) &lt;br />;&lt;i>; Erik Wijmans&lt;/i>;, &lt;i>;Manolis Savva&lt;/i>;, &lt;b>;&lt;i>;Irfan Essa&lt;/i>;&lt;/b>;, &lt;i>;Stefan Lee&lt;/i>;, &lt;i>;Ari S. Morcos&lt;/i>;, &lt;i>;Dhruv Batra &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4-k7kUavAj&quot;>;Offline Q-Learning on Diverse Multi-task Data Both Scales and Generalizes&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Aviral Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;i>;Xingyang Geng&lt;/i>;, &lt;b>;&lt;i>;George Tucker&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=WE_vluYUL-X&quot;>;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;i>; Shunyu Yao&lt;/i>;*, &lt;b>;&lt;i>;Jeffrey Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dian Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nan Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Izhak Shafran&lt;/i>;&lt;/b>;, &lt;i>;Karthik R. Narasimhan&lt;/i>;, &lt;b>;&lt;i>;Yuan Cao &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=_CDixzkzeyb&quot;>;Prompt-to-Prompt Image Editing with Cross-Attention Control&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Amir Hertz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ron Mokady&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jay Tenenbaum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Kfir Aberman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yael Pritch&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Cohen-Or &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FjNys5c7VyY&quot;>;DreamFusion: Text-to-3D Using 2D Diffusion&lt;/a>; (&lt;em>;Outstanding Paper Award&lt;/em>;) &lt;br />;&lt;i>;&lt;b>; Ben Poole&lt;/b>;&lt;/i>;, &lt;i>;Ajay Jain&lt;/i>;, &lt;b>;&lt;i>;Jonathan T. Barron&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ben Mildenhall &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HcUf-QwZeFh&quot;>;A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Hiroki Furuta&lt;/i>;&lt;/b>;, &lt;i>;Yusuke Iwasawa&lt;/i>;, &lt;i>;Yutaka Matsuo&lt;/i>;, &lt;b>;&lt;i>;Shixiang Shane Gu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OpC-9aBBVJe&quot;>;Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier&lt;/a>; &lt;br />;&lt;i>; Pierluca D&#39;Oro&lt;/i>;, &lt;b>;&lt;i>;Max Schwarzer&lt;/i>;&lt;/b>;, &lt;i>;Evgenii Nikishin&lt;/i>;, &lt;i>;Pierre-Luc Bacon&lt;/i>;, &lt;b>;&lt;i>;Marc G Bellemare&lt;/i>;&lt;/b>;, &lt;i>;Aaron Courville &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=DEGjDDV22pI&quot;>;Dichotomy of Control: Separating What You Can Control from What You Cannot&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Sherry Yang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Pieter Abbeel&lt;/i>;, &lt;b>;&lt;i>;Ofir Nachum &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=7JsGYvjE88d&quot;>;Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search&lt;/a>; &lt;br />;&lt;i>; Michał Zawalski&lt;/i>;, &lt;i>;Michał Tyrolski&lt;/i>;, &lt;i>;Konrad Czechowski&lt;/i>;, &lt;i>;Tomasz Odrzygóźdź&lt;/i>;, &lt;i>;Damian Stachura&lt;/i>;, &lt;i>;Piotr Piekos&lt;/i>;, &lt;b>;&lt;i>;Yuhuai Wu&lt;/i>;&lt;/b>;, &lt;i>;Łukasz Kucinski&lt;/i>;, &lt;i>;Piotr Miłos &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rvsbw2YthH_&quot;>;The Trade-Off Between Universality and Label Efficiency of Representations from Contrastive Learning&lt;/a>; &lt;br />;&lt;i>; Zhenmei Shi&lt;/i>;, &lt;i>;Jiefeng Chen&lt;/i>;, &lt;i>;Kunyang Li&lt;/i>;, &lt;i>;Jayaram Raghuram&lt;/i>;, &lt;b>;&lt;i>;Xi Wu&lt;/i>;&lt;/b>;, &lt;i>;Yingyu Liang&lt;/i>;, &lt;i>;Somesh Jha &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=yHY9NbQJ5BP&quot;>;Sparsity-Constrained Optimal Transport&lt;/a>; &lt;br />;&lt;i>; Tianlin Liu&lt;/i>;*, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mathieu Blondel &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=xSsW2Am-ukZ&quot;>;Unmasking the Lottery Ticket Hypothesis: What&#39;s Encoded in a Winning Ticket&#39;s Mask?&lt;/a>; &lt;br />;&lt;i>; Mansheej Paul&lt;/i>;, &lt;i>;Feng Chen&lt;/i>;, &lt;i>;Brett W. Larsen&lt;/i>;, &lt;i>;Jonathan Frankle&lt;/i>;, &lt;i>;Surya Ganguli&lt;/i>;, &lt;b>;&lt;i>;Gintare Karolina Dziugaite &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SJ0Lde3tRL&quot;>;Extreme Q-Learning: MaxEnt RL without Entropy&lt;/a>; &lt;br />;&lt;i>; Divyansh Garg&lt;/i>;, &lt;i>;Joey Hejna&lt;/i>;, &lt;b>;&lt;i>;Matthieu Geist,&lt;/i>;&lt;/b>; &lt;i>;Stefano Ermon &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SMa9EAovKMC&quot;>;Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs&lt;/a>; &lt;br />;&lt;i>; Albert Qiaochu Jiang&lt;/i>;, &lt;i>;Sean Welleck&lt;/i>;, &lt;b>;&lt;i>;Jin Peng Zhou&lt;/i>;&lt;/b>;, &lt;i>;Timothee Lacroix&lt;/i>;, &lt;i>;Jiacheng Liu&lt;/i>;, &lt;i>;Wenda Li&lt;/i>;, &lt;i>;Mateja Jamnik&lt;/i>;, &lt;i>;Guillaume Lample&lt;/i>;, &lt;b>;&lt;i>;Yuhuai Wu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>; &lt;br />;&lt;i>; Yuzhe Yang&lt;/i>;, &lt;i>;Xin Liu&lt;/i>;, &lt;i>;&lt;b>;Jiang Wu&lt;/b>;&lt;/i>;, &lt;i>;&lt;b>;Silviu Borac&lt;/b>;&lt;/i>;, &lt;i>;Dina Katabi&lt;/i>;, &lt;b>;&lt;i>;Ming-Zher Poh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel McDuff &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G2Q2Mh3avow&quot;>;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Andy Zeng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Maria Attarian&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Brian Ichter&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krzysztof Marcin Choromanski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adrian Wong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Stefan Welker&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Federico Tombari&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aveek Purohit&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael S. Ryoo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vikas Sindhwani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Johnny Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vincent Vanhoucke&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pete Florence &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=0g0X4H8yN4I&quot;>;What Learning Algorithm Is In-Context Learning? Investigations with Linear Models&lt;/a>; &lt;br />;&lt;i>; Ekin Akyurek&lt;/i>;*, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Jacob Andreas&lt;/i>;, &lt;i>;Tengyu Ma&lt;/i>;*, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Peot1SFDX0&quot;>;Preference Transformer: Modeling Human Preferences Using Transformers for RL&lt;/a>; &lt;br />;&lt;i>; Changyeon Kim&lt;/i>;, &lt;i>;Jongjin Park&lt;/i>;, &lt;i>;Jinwoo Shin&lt;/i>;, &lt;i>;Honglak Lee&lt;/i>;, &lt;i>;Pieter Abbeel&lt;/i>;, &lt;b>;&lt;i>;Kimin Lee &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=QCrw0u9LQ7&quot;>;Iterative Patch Selection for High-Resolution Image Recognition&lt;/a>; &lt;br />;&lt;i>; Benjamin Bergner&lt;/i>;, &lt;i>;Christoph Lippert&lt;/i>;, &lt;b>;&lt;i>;Aravindh Mahendran &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=MIMwy4kh9lf&quot;>;Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Weicheng Kuo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yin Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuye Gu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;AJ Piergiovanni&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Anelia Angelova &lt;/b>;&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JLg5aHHv7j&quot;>;(Certified!!) Adversarial Robustness for Free!&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Florian Tramér&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krishnamurthy (Dj) Dvijotham&lt;/i>;&lt;/b>;, &lt;i>;Leslie Rice&lt;/i>;, &lt;i>;Mingjie Sun&lt;/i>;, &lt;i>;J. Zico Kolter &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gU5sJ6ZggcX&quot;>;REPAIR: REnormalizing Permuted Activations for Interpolation Repair&lt;/a>; &lt;br />;&lt;i>; Keller Jordan&lt;/i>;, &lt;b>;&lt;i>;Hanie Sedghi&lt;/i>;&lt;/b>;, &lt;i>;Olga Saukh&lt;/i>;, &lt;i>;Rahim Entezari&lt;/i>;, &lt;b>;&lt;i>;Behnam Neyshabur &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=VM8batVBWvg&quot;>;Discrete Predictor-Corrector Diffusion Models for Image Synthesis&lt;/a>; &lt;br />;&lt;b>;&lt;i>; José Lezama&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tim Salimans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lu Jiang&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Huiwen Chang&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Jonathan Ho&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Irfan Essa &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=zH9GcZ3ZGXu&quot;>;Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Sravanti Addepalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anshul Nasery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Praneeth Netrapalli&lt;/i>;&lt;/b>;, &lt;i>;Venkatesh Babu R.&lt;/i>;, &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=-CoNloheTs&quot;>;An Exact Poly-time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Amit Daniely&lt;/i>;&lt;/b>;, &lt;i>;Elad Granot &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=fR3wGCk-IXp&quot;>;Language Models Are Multilingual Chain-of-Thought Reasoners&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Freda Shi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mirac Suzgun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Markus Freitag&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;i>;Suraj Srivats&lt;/i>;, &lt;i>;Soroush Vosoughi&lt;/i>;, &lt;b>;&lt;i>;Hyung Won Chung&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sebastian Ruder&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dipanjan Das&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JxpBP1JM15-&quot;>;Scaling Forward Gradient with Local Losses&lt;/a>; &lt;br />;&lt;i>; Mengye Ren&lt;/i>;*, &lt;b>;&lt;i>;Simon Kornblith&lt;/i>;&lt;/b>;, &lt;i>;Renjie Liao&lt;/i>;, &lt;b>;&lt;i>;Geoffrey Hinton &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=DWn1TEb2fK&quot;>;Treeformer: Dense Gradient Trees for Efficient Attention Computation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Lovish Madaan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srinadh Bhojanapalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Himanshu Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=NVZvalzCLg&quot;>;LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification&lt;/a>; &lt;br />;&lt;i>; Sharath Girish&lt;/i>;, &lt;i>;Kamal Gupta&lt;/i>;, &lt;b>;&lt;i>;Saurabh Singh&lt;/i>;&lt;/b>;, &lt;i>;Abhinav Shrivastava &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nG9RF9z1yy3&quot;>;DiffusER: Diffusion via Edit-Based Reconstruction&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Machel Reid&lt;/i>;&lt;/b>;, &lt;i>;Vincent J. Hellendoorn&lt;/i>;, &lt;i>;Graham Neubig &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ORp91sAbzI&quot;>;Leveraging Unlabeled Data to Track Memorization&lt;/a>; &lt;br />;&lt;i>; Mahsa Forouzesh&lt;/i>;, &lt;b>;&lt;i>;Hanie Sedghi&lt;/i>;&lt;/b>;, &lt;i>;Patrick Thiran &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4FBUihxz5nm&quot;>;A Mixture-of-Expert Approach to RL-Based Dialogue Management&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yinlam Chow&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aza Tulepbergenov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ofir Nachum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dhawal Gupta&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Moonkyung Ryu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Ghavamzadeh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Craig Boutilier &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rSUCajhLsQ&quot;>;Easy Differentially Private Linear Regression&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Kareem Amin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Joseph&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Monica Ribero&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergei Vassilvitskii &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals&lt;/a>; &lt;br />;&lt;i>; Sandeep Silwal&lt;/i>;*, &lt;b>;&lt;i>;Sara Ahmadian&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew Nystrom&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew McCallum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Deepak Ramachandran&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mehran Kazemi &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=sIoED-yPK9l&quot;>;Massively Scaling Heteroscedastic Classifiers&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Mark Collier&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rodolphe Jenatton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jesse Berent&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Effrosyni Kokiopoulou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TJ2nxciYCk-&quot;>;The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zonglin Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chong You&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srinadh Bhojanapalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daliang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sashank J. Reddi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ke Ye&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Chern&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruiqi Guo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gJW8hSGBys8&quot;>;Compositional Semantic Parsing with Large Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Andrew Drozdov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathanael Scharli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ekin Akyurek&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathan Scales&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xinying Song&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xinyun Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Bousquet&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ndYXTEL6cZz&quot;>;Extremely Simple Activation Shaping for Out-of-Distribution Detection&lt;/a>; &lt;br />;&lt;i>; Andrija Djurisic&lt;/i>;, &lt;i>;Nebojsa Bozanic&lt;/i>;, &lt;i>;Arjun Ashok&lt;/i>;, &lt;b>;&lt;i>;Rosanne Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=5MkYIYCbva&quot;>;Long Range Language Modeling via Gated State Spaces&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Harsh Mehta&lt;/i>;&lt;/b>;, &lt;i>;Ankit Gupta&lt;/i>;, &lt;i>;Ashok Cutkosky&lt;/i>;, &lt;i>;Behnam Neyshabur &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=sSt9fROSZRO&quot;>;Investigating Multi-task Pretraining and Generalization in Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Adrien Ali Taiga&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jesse Farebrother&lt;/i>;&lt;/b>;, &lt;i>;Aaron Courville&lt;/i>;, &lt;b>;&lt;i>;Marc G. Bellemare &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=k9CF4h3muD&quot;>;Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets&lt;/a>; &lt;br />;&lt;i>; Edo Cohen-Karlik&lt;/i>;, &lt;i>;Itamar Menuhin-Gruman&lt;/i>;, &lt;i>;Raja Giryes&lt;/i>;, &lt;i>;Nadav Cohen&lt;/i>;, &lt;b>;&lt;i>;Amir Globerson &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=CL-sVR9pvF&quot;>;Weighted Ensemble Self-Supervised Learning&lt;/a>; &lt;br />;&lt;i>; Yangjun Ruan&lt;/i>;*, &lt;b>;&lt;i>;Saurabh Singh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Warren Morningstar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alexander A. Alemi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Ioffe&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ian Fischer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joshua V. Dillon &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=0qSOodKmJaN&quot;>;Calibrating Sequence Likelihood Improves Conditional Language Generation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yao Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Misha Khalman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Joshi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shashi Narayan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Saleh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OIe3kpwl40D&quot;>;SMART: Sentences as Basic Units for Text Evaluation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Reinald Kim Amplayo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yao Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shashi Narayan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=9Nj_gNdvqYf&quot;>;Leveraging Importance Weights in Subset Selection&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Gui Citovsky&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Giulia DeSalvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srikumar Ramalingam&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Afshin Rostamizadeh&lt;/i>;&lt;/b>;, &lt;i>;Yunjuan Wang&lt;/i>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=oGDKSt9JrZi&quot;>;Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks&lt;/a>; &lt;br />;&lt;i>;&lt;b>;Jesse Farebrother&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Joshua Greaves&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;i>;Charline Le Lan&lt;/i>;, &lt;b>;&lt;i>;Ross Goroshin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pablo Samuel Castro&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Marc G. Bellemare &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=n70oyIlS4g&quot;>;An Extensible Multi-modal Multi-task Object Dataset with Materials&lt;/a>; &lt;br />;&lt;i>; Trevor Standley&lt;/i>;, &lt;i>;Ruohan Gao&lt;/i>;, &lt;b>;&lt;i>;Dawn Chen&lt;/i>;&lt;/b>;, &lt;i>;Jiajun Wu&lt;/i>;, &lt;i>;Silvio Savarese &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=7bJizxLKrR&quot;>;Measuring Forgetting of Memorized Training Examples&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Matthew Jagielski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Om Thakkar&lt;/i>;&lt;/b>;, &lt;i>;Florian Tramér&lt;/i>;, &lt;b>;&lt;i>;Daphne Ippolito&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katherine Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;i>;Eric Wallace&lt;/i>;, &lt;b>;&lt;i>;Shuang Song&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Abhradeep Thakurta&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nicolas Papernot&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=wCFB37bzud4&quot;>;Bidirectional Language Models Are Also Few-Shot Learners&lt;/a>; &lt;br />;&lt;i>; Ajay Patel&lt;/i>;, &lt;i>;Bryan Li&lt;/i>;, &lt;i>;Mohammad Sadegh Rasooli&lt;/i>;, &lt;b>;&lt;i>;Noah Constant&lt;/i>;&lt;/b>;, &lt;i>;Colin Raffel&lt;/i>;, &lt;i>;Chris Callison-Burch &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=xE-LtsE-xx&quot;>;Is Attention All That NeRF Needs?&lt;/a>; &lt;br />;&lt;i>; Mukund Varma T.&lt;/i>;, &lt;i>;Peihao Wang&lt;/i>;, &lt;i>;Xuxi Chen&lt;/i>;, &lt;i>;Tianlong Chen&lt;/i>;, &lt;b>;&lt;i>;Subhashini Venugopalan&lt;/i>;&lt;/b>;, &lt;i>;Zhangyang Wang &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfptQCEKVW4&quot;>;Automating Nearest Neighbor Search Configuration with Constrained Optimization&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Philip Sun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruiqi Guo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lLp-C5nTdJG&quot;>;Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; David Bieber&lt;/i>;&lt;/b>;, &lt;i>;Rishab Goel&lt;/i>;, &lt;b>;&lt;i>;Daniel Zheng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hugo Larochelle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Tarlow &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gmwDKo-4cY&quot;>;Composing Ensembles of Pre-trained Models via Iterative Consensus&lt;/a>; &lt;br />;&lt;i>; Shuang Li&lt;/i>;, &lt;i>;Yilun Du&lt;/i>;, &lt;i>;Joshua B. Tenenbaum&lt;/i>;, &lt;i>;Antonio Torralba&lt;/i>;, &lt;b>;&lt;i>;Igor Mordatch &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=oztkQizr3kk&quot;>;Λ-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection Among Cells&lt;/a>; &lt;br />;&lt;i>; Sajad Movahedi&lt;/i>;, &lt;i>;Melika Adabinejad&lt;/i>;, &lt;i>;Ayyoob Imani&lt;/i>;, &lt;i>;Arezou Keshavarz&lt;/i>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;i>;Azadeh Shakery&lt;/i>;, &lt;i>;Babak N. Araabi &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OjDkC57x5sz&quot;>;Blurring Diffusion Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Emiel Hoogeboom&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tim Salimans &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=bAMTaeqluh4&quot;>;Part-Based Models Improve Adversarial Robustness&lt;/a>; &lt;br />;&lt;i>; Chawin Sitawarin&lt;/i>;, &lt;i>;Kornrapat Pongmala&lt;/i>;, &lt;i>;Yizheng Chen&lt;/i>;, &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;i>;David Wagner &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=z0_V5O9cmNw&quot;>;Learning in Temporally Structured Environments&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Matt Jones&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tyler R. Scott&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mengye Ren&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gamaleldin ElSayed&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Katherine Hermann&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;David Mayo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael C. Mozer &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TFbwV6I0VLg&quot;>;SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models&lt;/a>; &lt;br />;&lt;i>; Ziyi Wu&lt;/i>;, &lt;i>;Nikita Dvornik&lt;/i>;, &lt;b>;&lt;i>;Klaus Greff&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Thomas Kipf&lt;/i>;&lt;/b>;, &lt;i>;Animesh Garg &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=I29Kt0RwChs&quot;>;Robust Algorithms on Adaptive Inputs from Bounded Adversaries&lt;/a>; &lt;br />;&lt;i>; Yeshwanth Cherapanamjeri&lt;/i>;, &lt;i>;Sandeep Silwal&lt;/i>;, &lt;i>;David P. Woodruff&lt;/i>;, &lt;i>;Fred Zhang&lt;/i>;, &lt;b>;&lt;i>;Qiuyi (Richard) Zhang&lt;/i>;&lt;/b>;, &lt;i>;Samson Zhou &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=EnrY5TOrbQ&quot;>;Agnostic Learning of General ReLU Activation Using Gradient Descent&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Pranjal Awasthi&lt;/i>;&lt;/b>;, &lt;i>;Alex Tang&lt;/i>;, &lt;i>;Aravindan Vijayaraghavan &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=3itjR9QxFw&quot;>;Analog Bits: Generating Discrete Data Using Diffusion Models with Self-Conditioning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ting Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruixiang Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Geoffrey Hinton &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lEkl0jdSb7B&quot;>;Any-Scale Balanced Samplers for Discrete Space&lt;/a>; &lt;br />;&lt;i>; Haoran Sun&lt;/i>;*, &lt;b>;&lt;i>;Bo Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Charles Sutton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanjun Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kPPVmUF6bM_&quot;>;Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation&lt;/a>; &lt;br />;&lt;i>; Ziqi Wang&lt;/i>;*, &lt;b>;&lt;i>;Yuexin Wu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frederick Liu&lt;/i>;&lt;/b>;, &lt;i>;Daogao Liu&lt;/i>;, &lt;b>;&lt;i>;Le Hou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hongkun Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Li&lt;/i>;&lt;/b>;, &lt;i>;Heng Ji &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=pOyi9KqE56b&quot;>;Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD&lt;/a>; &lt;br />;&lt;i>; Konstantinos E. Nikolakakis&lt;/i>;, &lt;i>;Farzin Haddadpour&lt;/i>;, &lt;b>;&lt;i>;Amin Karbasi&lt;/i>;&lt;/b>;, &lt;i>;Dionysios S. Kalogerias &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Ha2MnQM9Ph&quot;>;Causal Estimation for Text Data with (Apparent) Overlap Violations&lt;/a>; &lt;br />;&lt;i>; Lin Gui&lt;/i>;, &lt;b>;&lt;i>;Victor Veitch &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=AjC0KBjiMu&quot;>;Contrastive Learning Can Find an Optimal Basis for Approximately View-Invariant Functions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Daniel D. Johnson&lt;/i>;&lt;/b>;, &lt;i>;Ayoub El Hanchi&lt;/i>;, &lt;i>;Chris J. Maddison &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=j1zQGmQQOX1&quot;>;Differentially Private Adaptive Optimization with Delayed Preconditioners&lt;/a>; &lt;br />;&lt;i>; Tian Li&lt;/i>;, &lt;i>;Manzil Zaheer&lt;/i>;, &lt;i>;Ziyu Liu&lt;/i>;, &lt;b>;&lt;i>;Sashank Reddi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Brendan McMahan&lt;/i>;&lt;/b>;, &lt;i>;Virginia Smith &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=3KUfbI9_DQE&quot;>;Distributionally Robust Post-hoc Classifiers Under Prior Shifts&lt;/a>; &lt;br />;&lt;i>; Jiaheng Wei&lt;/i>;*, &lt;b>;&lt;i>;Harikrishna Narasimhan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ehsan Amid&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Wen-Sheng Chu&lt;/i>;&lt;/b>;, &lt;i>;Yang Liu&lt;/i>;, &lt;b>;&lt;i>;Abhishek Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ReDQ1OUQR0X&quot;>;Human Alignment of Neural Network Representations&lt;/a>; &lt;br />;&lt;i>; Lukas Muttenthaler&lt;/i>;, &lt;i>;Jonas Dippel&lt;/i>;, &lt;i>;Lorenz Linhardt&lt;/i>;, &lt;i>;Robert A. Vandermeulen&lt;/i>;, &lt;b>;&lt;i>;Simon Kornblith &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JpbLyEI5EwW&quot;>;Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data&lt;/a>; &lt;br />;&lt;i>; Spencer Frei&lt;/i>;, &lt;i>;Gal Vardi&lt;/i>;, &lt;b>;&lt;i>;Peter Bartlett&lt;/i>;&lt;/b>;, &lt;i>;Nathan Srebro&lt;/i>;, &lt;i>;Wei Hu &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kUmdmHxK5N&quot;>;Koopman Neural Operator Forecaster for Time-Series with Temporal Distributional Shifts&lt;/a>; &lt;br />;&lt;i>; Rui Wang&lt;/i>;*, &lt;b>;&lt;i>;Yihe Dong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sercan Ö. Arik&lt;/i>;&lt;/b>;, &lt;i>;Rose Yu &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=mQpmZVzXK1h&quot;>;Latent Variable Representation for Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tongzheng Ren&lt;/i>;&lt;/b>;, &lt;i>;Chenjun Xiao&lt;/i>;, &lt;b>;&lt;i>;Tianjun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;i>;Zhaoran Wang&lt;/i>;, &lt;i>;Sujay Sanghavi&lt;/i>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=WZH7099tgfM&quot;>;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathanael Scharli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Le Hou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathan Scales&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Claire Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Bousquet&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quoc Le&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ed Chi &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4rXMRuoJlai&quot;>;Mind&#39;s Eye: Grounded Language Model Reasoning Through Simulation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ruibo Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shixiang Shane Gu&lt;/i>;&lt;/b>;, &lt;i>;Te-Yen Wu&lt;/i>;, &lt;i>;Soroush Vosoughi&lt;/i>;, &lt;b>;&lt;i>;Claire Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew M. Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=H0HGljkxQFN&quot;>;MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models&lt;/a>; &lt;br />;&lt;i>; Chenglin Yang&lt;/i>;*, &lt;b>;&lt;i>;Siyuan Qiao&lt;/i>;&lt;/b>;, &lt;i>;Qihang Yu&lt;/i>;, &lt;i>;Xiaoding Yuan&lt;/i>;, &lt;b>;&lt;i>;Yukun Zhu&lt;/i>;&lt;/b>;, &lt;i>;Alan Yuille&lt;/i>;, &lt;b>;&lt;i>;Hartwig Adam&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Liang-Chieh Chen &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HtoA0oT30jC&quot;>;Novel View Synthesis with Diffusion Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Daniel Watson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;William Chan&lt;/i>;&lt;/b>;,&lt;b>;&lt;i>;&amp;nbsp;Ricardo&lt;/i>;&lt;/b>;&amp;nbsp;&lt;b>;&lt;i>;Martin-Brualla&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jonathan Ho&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrea Tagliasacchi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Norouzi&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=fYzLpCsGZVf&quot;>;On Accelerated Perceptrons and Beyond&lt;/a>; &lt;br />;&lt;i>; Guanghui Wang&lt;/i>;, &lt;i>;Rafael Hanashiro&lt;/i>;, &lt;i>;Etash Guha&lt;/i>;, &lt;b>;&lt;i>;Jacob Abernethy &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rJcLocAJpA6&quot;>;On Compositional Uncertainty Quantification for Seq2seq Graph Parsing&lt;/a>; &lt;br />;&lt;i>; Zi Lin&lt;/i>;*, &lt;b>;&lt;i>;Du Phan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Panupong Pasupat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeremiah Liu&lt;/i>;&lt;/b>;, &lt;i>;Jingbo Shang &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=jbIYfq4Tr-&quot;>;On the Robustness of Safe Reinforcement Learning Under Observational Perturbations&lt;/a>; &lt;br />;&lt;i>; Zuxin Liu&lt;/i>;, &lt;i>;Zijian Guo&lt;/i>;, &lt;i>;Zhepeng Cen&lt;/i>;, &lt;i>;Huan Zhang&lt;/i>;, &lt;b>;&lt;i>;Jie Tan&lt;/i>;&lt;/b>;, &lt;i>;Bo Li&lt;/i>;, &lt;i>;Ding Zhao &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=47KG_AvNqeZ&quot;>;Online Low Rank Matrix Completion&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Prateek Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Soumyabrata Pal &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kJUS5nD0vPB&quot;>;Out-of-Distribution Detection and Selective Generation for Conditional Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jie Ren&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Jiaming Luo&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Yao Zhao&lt;/i>;&lt;/b>;, &lt;i>;Kundan Krishna&lt;/i>;*, &lt;b>;&lt;i>;Mohammad Saleh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Balaji Lakshminarayanan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=mWVoBz4W0u&quot;>;PaLI: A Jointly-Scaled Multilingual Language-Image Model&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Xi Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiao Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Soravit Changpinyo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;AJ Piergiovanni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Piotr Padlewski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Salz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sebastian Goodman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adam Grycner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lucas Beyer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alexander Kolesnikov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nan Ding&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Keran Rong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hassan Akbari&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gaurav Mishra&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Linting Xue&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ashish V. Thapliyal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;James Bradbury&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Weicheng Kuo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mojtaba Seyedhosseini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chao Jia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Burcu Karagol Ayan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carlos Riquelme Ruiz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andreas Peter Steiner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anelia Angelova&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiaohua Zhai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Radu Soricut &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=vOEXS39nOF&quot;>;Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ruben Villegas&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Babaeizadeh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pieter-Jan Kindermans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hernan Moraldo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Han Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Taghi Saffar&lt;/i>;&lt;/b>;, &lt;i>;Santiago Castro&lt;/i>;*, &lt;i>;Julius Kunze&lt;/i>;*, &lt;b>;&lt;i>;Dumitru Erhan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gmL46YMpu2J&quot;>;Promptagator: Few-Shot Dense Retrieval from 8 Examples&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhuyun Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vincent Y. Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ji Ma&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Luan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jianmo Ni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Lu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anton Bakalov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Kelvin Guu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Keith B. Hall&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ming-Wei Chang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=MofT9KEF0kw&quot;>;Pushing the Accuracy-Group Robustness Frontier with Introspective Self-Play&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jeremiah Zhe Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krishnamurthy Dj Dvijotham&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jihyeon Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quan Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Balaji Lakshminarayanan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Deepak Ramachandran &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=XSEBx0iSjFQ&quot;>;Re-Imagen: Retrieval-Augmented Text-to-Image Generator&lt;/a>; &lt;brp>;&lt;b>;&lt;i>; Wenhu Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hexiang Hu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chitwan Saharia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;William W. Cohen &lt;/i>;&lt;/b>;&lt;/brp>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.01296.pdf&quot;>;Recitation-Augmented Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhiqing Sun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;i>;Yiming Yang&lt;/i>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=h9O0wsmL-cT&quot;>;Regression with Label Differential Privacy&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Badih Ghazi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pritish Kamath&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ravi Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ethan Leeman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pasin Manurangsi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Avinash Varadarajan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SNgLnzFQeiD&quot;>;Revisiting the Entropy Semiring for Neural Speech Recognition&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Oscar Chang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dongseong Hwang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Siohan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ALDM5SN2r7M&quot;>;Robust Active Distillation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Cenk Baykal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Khoa Trinh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fotis Iliopoulos&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gaurav Menghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Erik Vee &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=BYWWwSY2G5s&quot;>;Score-Based Continuous-Time Discrete Diffusion Models&lt;/a>; &lt;br />;&lt;i>; Haoran Sun&lt;/i>;*, &lt;b>;&lt;i>;Lijun Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanjun Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=1PL1NIMMrw&quot;>;Self-Consistency Improves Chain of Thought Reasoning in Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quoc Le&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ed H. Chi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sharan Narang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aakanksha Chowdhery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Ubc74gTVo3&quot;>;Self-Supervision Through Random Segments with Autoregressive Coding (RandSAC)&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tianyu Hua&lt;/i>;&lt;/b>;, &lt;i>;Yonglong Tian&lt;/i>;, &lt;i>;Sucheng Ren&lt;/i>;, &lt;b>;&lt;i>;Michalis Raptis&lt;/i>;&lt;/b>;, &lt;i>;Hang Zhao&lt;/i>;, &lt;i>;Leonid Sigal &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=T-qVtA3pAxG&quot;>;Serving Graph Compression for Graph Neural Networks&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Si Si&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;i>;Cho-Jui Hsieh&lt;/i>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TTLLGx3eet&quot;>;Sequential Attention for Feature Selection&lt;/a>; &lt;br />;&lt;i>; Taisuke Yasuda&lt;/i>;*, &lt;b>;&lt;i>;MohammadHossein Bateni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lin Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Fahrbach&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gang Fu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vahab Mirrokni &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=T5nUQDrM4u&quot;>;Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints&lt;/a>; &lt;br />;&lt;i>; Aran Komatsuzaki&lt;/i>;*, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;James Lee-Thorp&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carlos Riquelme&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joshua Ainslie&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FBMLeaXpZN&quot;>;Spectral Decomposition Representation for Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tongzheng Ren&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tianjun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lisa Lee&lt;/i>;&lt;/b>;, &lt;i>;Joseph Gonzalez&lt;/i>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=9yE2xEj0BH7&quot;>;Spotlight: Mobile UI Understanding Using Vision-Language Models with a Focus&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/02/a-vision-language-approach-for.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Gang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yang Li &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=8jU7wy7N7mA&quot;>;Supervision Complexity and Its Role in Knowledge Distillation&lt;/a>; &lt;br />;&lt;i>; Hrayr Harutyunyan&lt;/i>;*, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aditya Krishna Menon&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Seungyeon Kim&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=GVSf7Z7DbYL&quot;>;Teacher Guided Training: An Efficient Framework for Knowledge Transfer&lt;/a>; &lt;br />;&lt;i>; Manzil Zaheer&lt;/i>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Seungyeon Kim&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chong You&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Himanshu Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andreas Veit&lt;/i>;&lt;/b>;, &lt;i>;Rob Fergus&lt;/i>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gSHyqBijPFO&quot;>;TEMPERA: Test-Time Prompt Editing via Reinforcement Learning&lt;/a>; &lt;br />;&lt;i>; Tianjun Zhang&lt;/i>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Joseph E. Gonzalez &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6ruVLB727MC&quot;>;UL2: Unifying Language Learning Paradigms&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vinh Q. Tran&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xavier Garcia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hyung Won Chung&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dara Bahri&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tal Schuster&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Steven Zheng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Donald Metzler &lt;/i>;&lt;/b>;&lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; * Work done while at Google&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6812586756904863062/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-iclr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6812586756904863062&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6812586756904863062&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-iclr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICLR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaXU0Qnf7lWC70SIqFUj3KcinepmKmqVwu369RYcajoFZI0UGtej6ZGJD7c0GsXJk_Wjg08CnBLtE7lLlz__azqzUNCgflaTp3F9mZyIxX7HBzJEdTN19WMlfVsu4mTww-LLhvZ4hhWqdVCQqjQbrOk6VumBCH6dA2YwdATT6I_QLKLHSUz_Upnw6zlg/s72-c/Google%20Rwanda%20Logo-02.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1064513940498249945&lt;/id>;&lt;published>;2023-04-27T13:24:00.001-07:00&lt;/published>;&lt;updated>;2023-04-27T13:27:36.601-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An ML-based approach to better characterize lung diseases&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Babak Behsaz, Software Engineer, and Andrew Carroll, Product Lead, Genomics&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjukGQ3XqcJ6pKHbbMe80-OO_hrGogO7FUPtRlCGq7GH5ba83-6_MnzX5CS7MecI2rz4fbFuY1sHp4hb9mpV_zU95zSxA4mfucMRj_d-LtPNNKUAEEHwuE_Cqj1nvVDJN86RLZU2byBsemz-GiVEDIHVusSQHTerZVr0dPytu4HwGA5_9IYXIeiQ3pprg/s1400/copdgenomics.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The combination of the environment an individual experiences and their genetic predispositions determines the majority of their risk &lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/nejmsa073350&quot;>;for various diseases&lt;/a>;. Large national efforts, such as &lt;a href=&quot;https://www.ukbiobank.ac.uk/&quot;>;the UK Biobank&lt;/a>;, have created large, public resources to better understand the links between environment, genetics, and disease. This has the potential to help individuals better understand how to stay healthy, clinicians to treat illnesses, and scientists to develop new medicines. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One challenge in this process is how we make sense of the vast amount of clinical measurements — the UK Biobank has many petabytes of imaging, metabolic tests, and medical records spanning 500,000 individuals. To best use this data, we need to be able to represent the information present as succinct, informative labels about meaningful diseases and traits, a process called &lt;a href=&quot;https://en.wikipedia.org/wiki/Phenotype&quot;>;phenotyping&lt;/a>;. That is where we can use the ability of ML models to pick up on subtle intricate patterns in large amounts of data. &lt;/p>; &lt;p>; We&#39;ve previously demonstrated the ability to use ML models to &lt;a href=&quot;https://www.cell.com/ajhg/fulltext/S0002-9297(21)00188-9&quot;>;quickly phenotype&lt;/a>; at scale for retinal diseases. Nonetheless, these models were trained using labels from clinician judgment, and access to clinical-grade labels is a limiting factor due to the time and expense needed to create them. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41588-023-01372-4&quot;>;Inference of chronic obstructive pulmonary disease with deep learning on raw spirograms identifies new genetic loci and improves risk models&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/ng/&quot;>;Nature Genetics&lt;/a>;&lt;/em>;, we&#39;re excited to highlight a method for training accurate ML models for genetic discovery of diseases, even when using noisy and unreliable labels. We demonstrate the ability to train ML models that can phenotype directly from raw clinical measurement and unreliable medical record information. This reduced reliance on medical domain experts for labeling greatly expands the range of applications for our technique to a panoply of diseases and has the potential to improve their prevention, diagnosis, and treatment. We showcase this method with ML models that can better characterize lung function and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chronic_obstructive_pulmonary_disease&quot;>;chronic obstructive pulmonary disease&lt;/a>; (COPD). Additionally, we show the usefulness of these models by demonstrating a better ability to identify genetic variants associated with COPD, improved understanding of the biology behind the disease, and successful prediction of outcomes associated with COPD. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;ML for deeper understanding of exhalation &lt;/h2>; &lt;p>; For this demonstration, we focused on COPD, the &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death&quot;>;third leading cause of worldwide death in 2019&lt;/a>;, in which airway inflammation and impeded airflow can progressively reduce lung function. Lung function for COPD and other diseases is measured by recording an individual&#39;s exhalation volume over time (the record is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spirometer&quot;>;spirogram&lt;/a>;; see an example below). Although there are guidelines (called &lt;a href=&quot;https://bestpractice.bmj.com/topics/en-us/7/criteria&quot;>;GOLD&lt;/a>;) for determining COPD status from exhalation, these use only a few, specific data points in the curve and apply fixed thresholds to those values. Much of the rich data from these spirograms is discarded in this analysis of lung function. &lt;/p>; &lt;p>; We reasoned that ML models trained to classify spirograms would be able to use the rich data present more completely and result in more accurate and comprehensive measures of lung function and disease, similar to what we have seen in other classification tasks like &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6&quot;>;mammography&lt;/a>; or &lt;a href=&quot;https://ai.googleblog.com/2023/03/learning-from-deep-learning-case-study.html&quot;>;histology&lt;/a>;. We trained ML models to predict whether an individual has COPD using the full spirograms as inputs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw_vNOsI6b6xOptAAEdrfeY1biM7ZgLO6-bGD7ICW2681H_4foD5Gms6AHGDlS1cmWjx2iR8paxAvY8lLAT29uwrijzI-W7uQVmRtHSts0yBA46zJhw8mYRz_H4NZ7rwvMQWOQBGUAvpfsBHHj9kBlVzCc3fU2rUsh7sM_Dn5wNVgPeu2XmR7ERMbJGw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;664&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw_vNOsI6b6xOptAAEdrfeY1biM7ZgLO6-bGD7ICW2681H_4foD5Gms6AHGDlS1cmWjx2iR8paxAvY8lLAT29uwrijzI-W7uQVmRtHSts0yBA46zJhw8mYRz_H4NZ7rwvMQWOQBGUAvpfsBHHj9kBlVzCc3fU2rUsh7sM_Dn5wNVgPeu2XmR7ERMbJGw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spirometry and COPD status overview. Spirograms from lung function test showing a forced expiratory volume-time spirogram (&lt;b>;left&lt;/b>;), a forced expiratory flow-time spirogram (&lt;b>;middle&lt;/b>;), and an interpolated forced expiratory flow-volume spirogram (&lt;b>;right&lt;/b>;). The profile of individuals w/o COPD is different.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The common method of training models for this problem, &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;supervised learning&lt;/a>;, requires samples to be associated with labels. Determining those labels can require the effort of very time-constrained experts. For this work, to show that we do not necessarily need medically graded labels, we decided to use a variety of widely available sources of medical record information to create those labels without medical expert review. These labels are &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233028/&quot;>;less reliable and noisy&lt;/a>; for two reasons. First, there are gaps in the medical records of individuals because they use multiple health services. Second, COPD is often undiagnosed, meaning many with the disease will not be labeled as having it even if we compile the complete medical records. Nonetheless, we trained a model to predict these noisy labels from the spirogram curves and treat the model predictions as a quantitative COPD liability or risk score. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlWGiBTaBqut9XGz_wip5AXFwRVX2gLetKEdaPY0L75-Kz8eow-NIZdwJmjqbZ7bZMlpTzHgWqMya9HVQHlS2HnPaVWnl8vh0NiIlYM5IyEkM6tKLfSsKpSVCBDEXXue6UoHgMvz8ZMAE0_wQcK78tFbTDWmY3RWKhKi7phODzQU1aZ6wJUjIidLOlvQ/s1787/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1787&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlWGiBTaBqut9XGz_wip5AXFwRVX2gLetKEdaPY0L75-Kz8eow-NIZdwJmjqbZ7bZMlpTzHgWqMya9HVQHlS2HnPaVWnl8vh0NiIlYM5IyEkM6tKLfSsKpSVCBDEXXue6UoHgMvz8ZMAE0_wQcK78tFbTDWmY3RWKhKi7phODzQU1aZ6wJUjIidLOlvQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Noisy COPD status labels were derived using various medical record sources (clinical data). A COPD liability model is then trained to predict COPD status from raw flow-volume spirograms.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Predicting COPD outcomes&lt;/h2>; &lt;p>; We then investigated whether the risk scores produced by our model could better predict a variety of binary COPD outcomes (for example, an individual&#39;s COPD status, whether they were hospitalized for COPD or died from it). For comparison, we benchmarked the model relative to expert-defined measurements required to diagnose COPD, specifically &lt;a href=&quot;https://goldcopd.org/gold-spirometry-guide/&quot;>;FEV1/FVC&lt;/a>;, which compares specific points on the spirogram curve with a simple mathematical ratio. We observed an improvement in the ability to predict these outcomes as seen in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision-recall&lt;/a>; curves below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyIWs-H_se4QVdvrrr-8iJ3x2FmWcaGId2LeRr30Sw9trq_03KB94o05zDYAg8l0OW7idCf7G8sbs5PldkZI6WnaDJH3X6z7yv9IEjxLCM9T1Od8HKOtDraAAsOWfZyXCrs860q7iqrVbk4-N6bKniXSjQd64RKIJ_-5_eZKsmivu0qCYhBq1Z0b6xOA/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;728&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyIWs-H_se4QVdvrrr-8iJ3x2FmWcaGId2LeRr30Sw9trq_03KB94o05zDYAg8l0OW7idCf7G8sbs5PldkZI6WnaDJH3X6z7yv9IEjxLCM9T1Od8HKOtDraAAsOWfZyXCrs860q7iqrVbk4-N6bKniXSjQd64RKIJ_-5_eZKsmivu0qCYhBq1Z0b6xOA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Precision-recall curves for COPD status and outcomes for our ML model (green) compared to traditional measures. Confidence intervals are shown by lighter shading.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also observed that separating populations by their COPD model score was predictive of all-cause mortality. This plot suggests that individuals with higher COPD risk are more likely to die earlier from any causes and the risk probably has implications beyond just COPD. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4e3BTXxn3jtqNdnW3xh9mNLiMXRBqCWMfgKdM_TsWOPTbeY4AXm71cVOhvFDY7YR0wvIn2AzvtXnsyn94LFYqY35vVdPIZkF6aVa5zvvRxidRTTHAfe0S3KryH27fWS1VUyKRETqZfRsWwsBHP7c2khartsF3zWyBVQ7AtTK7_6X4TzgrM4M001ttQg/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;809&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4e3BTXxn3jtqNdnW3xh9mNLiMXRBqCWMfgKdM_TsWOPTbeY4AXm71cVOhvFDY7YR0wvIn2AzvtXnsyn94LFYqY35vVdPIZkF6aVa5zvvRxidRTTHAfe0S3KryH27fWS1VUyKRETqZfRsWwsBHP7c2khartsF3zWyBVQ7AtTK7_6X4TzgrM4M001ttQg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Survival analysis of a cohort of UK Biobank individuals stratified by their COPD model&#39;s predicted risk &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartile&quot;>;quartile&lt;/a>;. The decrease of the curve indicates individuals in the cohort dying over time. For example, p100 represents the 25% of the cohort with greatest predicted risk, while p50 represents the 2nd quartile.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Identifying the genetic links with COPD&lt;/h2>; &lt;p>; Since the goal of large scale biobanks is to bring together large amounts of both phenotype and genetic data, we also performed a test called a &lt;a href=&quot;https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies&quot;>;genome-wide association study&lt;/a>; (GWAS) to identify the genetic links with COPD and genetic predisposition. A GWAS measures the strength of the statistical association between a given genetic variant — a change in a specific position of DNA — and the observations (eg, COPD) across a cohort of cases and controls. Genetic associations discovered in this manner can inform drug development that modifies the activity or products of a gene, as well as expand our understanding of the biology for a disease. &lt;/p>; &lt;p>; We showed with our ML-phenotyping method that not only do we rediscover almost all known COPD variants found by manual phenotyping, but we also find many novel genetic variants significantly associated with COPD. In addition, we see good agreement on the effect sizes for the variants discovered by both our ML approach and the manual one (&lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;>;R&lt;sup>;2&lt;/sup>;&lt;/a>;=0.93), which provides strong evidence for validity of the newly found variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvSvAcIMwQ-L_1esTkw6ChFzo_sAbd1ESskmHs0i_9si-Gh91P1IVVnl8kRGUyTtLRSjw6jPbE4bPjUzUpyXawimkXhayASxac-ywLjcPIXOVTKf1fNnO9XZZsCX_a_1BmFFYrtH9eOcAHodEaiS51my3kpol0TvTHK7_Ov7ACF5jOyNbionY1nUVxmw/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;883&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvSvAcIMwQ-L_1esTkw6ChFzo_sAbd1ESskmHs0i_9si-Gh91P1IVVnl8kRGUyTtLRSjw6jPbE4bPjUzUpyXawimkXhayASxac-ywLjcPIXOVTKf1fNnO9XZZsCX_a_1BmFFYrtH9eOcAHodEaiS51my3kpol0TvTHK7_Ov7ACF5jOyNbionY1nUVxmw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left&lt;/b>;: A plot comparing the statistical power of genetic discovery using the &lt;em>;labels for our ML model&lt;/em>; (y-axis) with the statistical power of the &lt;em>;manual labels from a traditional study&lt;/em>; (&lt;em>;x&lt;/em>;-axis). A value above the &lt;em>;y &lt;/em>;= &lt;em>;x&lt;/em>; line indicates greater statistical power in our method. Green points indicate significant findings in our method that are not found using the traditional approach. Orange points are significant in the traditional approach but not ours. Blue points are significant in both. &lt;b>;Right&lt;/b>;: Estimates of the association effect between our method (&lt;em>;y&lt;/em>;-axis) and traditional method (&lt;em>;x&lt;/em>;-axis). Note that the relative values between studies are comparable but the absolute numbers are not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Finally, our collaborators at Harvard Medical School and Brigham and Women&#39;s Hospital further examined the plausibility of these findings by providing insights into the possible biological role of the novel variants in development and progression of COPD (you can see more discussion on these insights in the &lt;a href=&quot;https://www.nature.com/articles/s41588-023-01372-4&quot;>;paper&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We demonstrated that our earlier methods for phenotyping with ML can be expanded to a wide range of diseases and can provide novel and valuable insights. We made two key observations by using this to predict COPD from spirograms and discovering new genetic insights. First, domain knowledge was not necessary to make predictions from raw medical data. Interestingly, we showed the raw medical data is probably underutilized and the ML model can find patterns in it that are not captured by expert-defined measurements. Second, we do not need medically graded labels; instead, noisy labels defined from widely available medical records can be used to generate clinically predictive and genetically informative risk scores. We hope that this work will broadly expand the ability of the field to use noisy labels and will improve our collective understanding of lung function and disease. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;br />;&lt;/h2>; &lt;p>; &lt;em>;This work is the combined output of multiple contributors and institutions. We thank all contributors: Justin Cosentino, Babak Alipanahi, Zachary R. McCaw, Cory Y. McLean, Farhad Hormozdiari (Google), Davin Hill (Northeastern University), Tae-Hwi Schwantes-An and Dongbing Lai (Indiana University), Brian D. Hobbs and Michael H. Cho (Brigham and Women&#39;s Hospital, and Harvard Medical School). We also thank Ted Yun and Nick Furlotte for reviewing the manuscript, Greg Corrado and Shravya Shetty for support, and Howard Yang, Kavita Kulkarni, and Tammi Huynh for helping with publication logistics.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1064513940498249945/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1064513940498249945&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1064513940498249945&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html&quot; rel=&quot;alternate&quot; title=&quot;An ML-based approach to better characterize lung diseases&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjukGQ3XqcJ6pKHbbMe80-OO_hrGogO7FUPtRlCGq7GH5ba83-6_MnzX5CS7MecI2rz4fbFuY1sHp4hb9mpV_zU95zSxA4mfucMRj_d-LtPNNKUAEEHwuE_Cqj1nvVDJN86RLZU2byBsemz-GiVEDIHVusSQHTerZVr0dPytu4HwGA5_9IYXIeiQ3pprg/s72-c/copdgenomics.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8846807972710427001&lt;/id>;&lt;published>;2023-04-26T13:32:00.002-07:00&lt;/published>;&lt;updated>;2023-06-09T12:49:52.605-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Robust and efficient medical imaging with self-supervision&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shekoofeh Azizi, Senior Research Scientist, and Laura Culp, Senior Research Engineer, Google Research&lt;/span>;&lt;div>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvMtAszRzkUmNz3DliRRKOPMA0hi-EplY9vfJ9cWCZhFBUgnblKjVoObTYAfybreL7HEKh11g2unV5oR_KOWfkPxpe3Hzo6qWSWq4YxA9RuhkeGXdDJST2XvW2Nfl3L6kDpV1hA2b_vpBMVaw2_pxt2vWD2n92S9fDGAZp0a4c2dV2Ylf3XrUSLQxAoQ/w640-h640/REMEDIS%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;span style=&quot;font-size: small;&quot;>;&lt;i>;&lt;b>;Update — 2023/06/09:&lt;/b>; This post has been updated as work described is now published in &lt;a href=&quot;https://www.nature.com/articles/s41551-023-01049-7.epdf?sharing_token=j-OmnHn5lQolIUrR0hSMmNRgN0jAjWel9jnR3ZoTv0Npafk-xg_G732EaK7Ir-Ubs26fYYHOOB3t4-cx-wUeAQwADBfTPTm6eC7dqqODwvW0btvDrcDtLDijM0ZMnjojyGSqRtQYIYzyYuxEGu0AlmKyBBMLVjBqK3C6m8VOF8E%3D&quot;>;Nature Biomedical Engineering&lt;/a>;.&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>;Despite recent progress in the field of &lt;a href=&quot;https://www.nature.com/articles/s41746-020-00376-2&quot;>;medical artificial intelligence&lt;/a>; (AI), most existing models are &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6.epdf?author_access_token=V_LKV2xpSv9G1dhANYeWM9RgN0jAjWel9jnR3ZoTv0M5zwPVx5jT4z_z-YkUZTBT6_1AtRXi8QouJM7xB-oSN-cVBoH7f_QTgx-yQN3UBEVfkvO1_5urNT-CZHGCEQNGlCuO69tMQYak4SmdoDqyzg%3D%3D&quot;>;narrow&lt;/a>;, &lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2588763&quot;>;single-task&lt;/a>; systems that require large quantities of labeled data to train. Moreover, these models cannot be easily reused in new clinical contexts as they often require the collection, &lt;a href=&quot;https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html&quot;>;de-identification&lt;/a>; and annotation of site-specific data for every new deployment environment, which is both &lt;a href=&quot;https://pubs.rsna.org/doi/full/10.1148/radiol.2020192224&quot;>;laborious and expensive&lt;/a>;. This problem of &lt;em>;data-efficient generalization &lt;/em>;(a model&#39;s ability to generalize to new settings using minimal new data) continues to be a key translational challenge for medical machine learning (ML) models and has in turn, prevented their broad uptake in real world healthcare settings. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;foundation models&lt;/a>; offers a significant opportunity to rethink development of medical AI to make it more performant, safer, and equitable. These models are trained using data at scale, often by self-supervised learning. This process results in generalist models that can rapidly be adapted to new tasks and environments with less need for supervised data. With foundation models, it may be possible to safely and efficiently deploy models across various clinical contexts and environments. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41551-023-01049-7.epdf?sharing_token=j-OmnHn5lQolIUrR0hSMmNRgN0jAjWel9jnR3ZoTv0Npafk-xg_G732EaK7Ir-Ubs26fYYHOOB3t4-cx-wUeAQwADBfTPTm6eC7dqqODwvW0btvDrcDtLDijM0ZMnjojyGSqRtQYIYzyYuxEGu0AlmKyBBMLVjBqK3C6m8VOF8E%3D&quot;>;Robust and Efficient MEDical Imaging with Self-supervision&lt;/a>;” (REMEDIS), published in &lt;em>;&lt;a href=&quot;https://www.nature.com/articles/s41551-023-01049-7&quot;>;Nature Biomedical Engineering&lt;/a>;&lt;/em>;, we introduce a unified large-scale self-supervised learning framework for building foundation medical imaging models. This strategy combines large scale &lt;a href=&quot;https://arxiv.org/abs/2101.05913&quot;>;supervised transfer learning&lt;/a>; with &lt;a href=&quot;https://ai.googleblog.com/2021/10/self-supervised-learning-advances.html&quot;>;self-supervised learning&lt;/a>; and requires minimal task-specific customization. REMEDIS shows significant improvement in data-efficient generalization across medical imaging tasks and modalities with a 3–100x reduction in site-specific data for adapting models to new clinical contexts and environments. Building on this, we are excited to announce &lt;a href=&quot;https://github.com/google-research/medical-ai-research-foundations&quot;>;Medical AI Research Foundations&lt;/a>; (hosted by &lt;a href=&quot;https://doi.org/10.13026/psq3-vj24&quot;>;PhysioNet&lt;/a>;), an expansion of the public release of &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;chest X-ray Foundations&lt;/a>; in 2022. Medical AI Research Foundations is a collection of open-source non-diagnostic models (starting with REMEDIS models), APIs, and resources to help researchers and developers accelerate medical AI research. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large scale self-supervision for medical imaging&lt;/h2>; &lt;p>; REMEDIS uses a combination of natural (non-medical) images and unlabeled medical images to develop strong medical imaging foundation models. Its pre-training strategy consists of two steps. The first involves supervised representation learning on a large-scale dataset of labeled natural images (pulled from &lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;Imagenet 21k&lt;/a>; or &lt;a href=&quot;http://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html&quot;>;JFT&lt;/a>;) using the &lt;a href=&quot;https://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html&quot;>;Big Transfer&lt;/a>; (BiT) method. &lt;/p>; &lt;p>; The second step involves intermediate self-supervised learning, which does not require any labels and instead, trains a model to learn medical data representations independently of labels. The specific approach used for pre-training and learning representations is &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>;. The method works by maximizing agreement between differently augmented views of the same training example via a contrastive loss in a hidden layer of a feed-forward neural network with &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) outputs. However, REMEDIS is equally compatible with other contrastive self-supervised learning methods. This training method is applicable for healthcare environments as many hospitals acquire raw data (images) as a routine practice. While processes would have to be implemented to make this data usable within models (ie, patient consent prior to gathering the data, de-identification, etc.), the costly, time-consuming, and difficult task of labeling that data could be avoided using REMEDIS. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4_m0T9kVxqldLFRTdlqSqYTieFr_4z2BIwAmjDqk3tDO9keGFVIMWMFgJmEGdHT4boqPCUYsi66ekSGKQXeY_3X-s95_bjsgugMquMjfMzRg5TcsO35SBzCv9AleHuEYYCS_gYNHsvRZw07oI_ZPtRynbqxYNF_F8iTLSesAZMHWbhB54Nlhoz9nPvA/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;368&quot; data-original-width=&quot;1600&quot; height=&quot;147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4_m0T9kVxqldLFRTdlqSqYTieFr_4z2BIwAmjDqk3tDO9keGFVIMWMFgJmEGdHT4boqPCUYsi66ekSGKQXeY_3X-s95_bjsgugMquMjfMzRg5TcsO35SBzCv9AleHuEYYCS_gYNHsvRZw07oI_ZPtRynbqxYNF_F8iTLSesAZMHWbhB54Nlhoz9nPvA/w640-h147/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS leverages large-scale supervised learning using natural images and self-supervised learning using unlabeled medical data to create strong foundation models for medical imaging.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given ML model parameter constraints, it is important that our proposed approach works when using both small and large model architecture sizes. To study this in detail, we considered two ResNet architectures with commonly used depth and width multipliers, &lt;a href=&quot;https://colab.research.google.com/github/yashclone999/ResNet_MODEL/blob/master/ResNet50.ipynb&quot;>;ResNet-50&lt;/a>; (1×) and &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152&quot;>;ResNet-152&lt;/a>; (2×) as the backbone encoder networks. &lt;/p>; &lt;p>; After pre-training, the model was fine-tuned using labeled task-specific medical data and evaluated for in-distribution task performance. In addition, to evaluate the data-efficient generalization, the model was also optionally fine-tuned using small amounts of out-of-distribution (OOD) data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ895lMLLchi5kiP38Gi16aBaaLNEgOxOkKft4ZnMVthFW_NRS_6ZcU8mIgMO2QuvJ3jU0TTH_yE-UjPhO64MX2sMLCvo9qt-reXkYY5zQtYkgunAEittRQru0Zvzi8BT_MhzBnSF9PHqEmD0EEmMErJiw-dXbfX2uWWCEyDVq0pGjSuK3EhVqh9y-gQ/s2209/REMEDIS%201200x2000.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2209&quot; data-original-width=&quot;1215&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ895lMLLchi5kiP38Gi16aBaaLNEgOxOkKft4ZnMVthFW_NRS_6ZcU8mIgMO2QuvJ3jU0TTH_yE-UjPhO64MX2sMLCvo9qt-reXkYY5zQtYkgunAEittRQru0Zvzi8BT_MhzBnSF9PHqEmD0EEmMErJiw-dXbfX2uWWCEyDVq0pGjSuK3EhVqh9y-gQ/w352-h640/REMEDIS%201200x2000.png&quot; width=&quot;352&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS starts with representations initialized using large-scale natural image pretraining following the Big Transfer (BiT) method. We then adapt the model to the medical domain using intermediate contrastive self-supervised learning without using any labeled medical data. Finally, we fine-tune the model to specific downstream medical imaging tasks. We evaluate the ML model both in an in-distribution (ID) setting and in an out-of-distribution (OOD) setting to establish the data-efficient generalization performance of the model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation and results&lt;/h2>; &lt;p>; To evaluate the REMEDIS model&#39;s performance, we simulate realistic scenarios using retrospective de-identified data across a broad range of medical imaging tasks and modalities, including &lt;a href=&quot;https://ai.googleblog.com/2019/09/using-deep-learning-to-inform.html&quot;>;dermatology&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2018/12/improving-effectiveness-of-diabetic.html&quot;>;retinal imaging&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html&quot;>;chest X-ray interpretation&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/learning-from-deep-learning-case-study.html&quot;>;pathology&lt;/a>; and &lt;a href=&quot;https://blog.google/technology/health/improving-breast-cancer-screening&quot;>;mammography&lt;/a>;. We further introduce the notion of data-efficient generalization, capturing the model&#39;s ability to generalize to new deployment distributions with a significantly reduced need for expert annotated data from the new clinical setting. In-distribution performance is measured as (1) improvement in zero-shot generalization to OOD settings (assessing performance in an OOD evaluation set, with zero access to training data from the OOD dataset) and (2) significant reduction in the need for annotated data from the OOD settings to reach performance equivalent to clinical experts (or threshold demonstrating clinical utility). REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strongly supervised baseline. &lt;/p>; &lt;p>; More importantly, our strategy leads to data-efficient generalization of medical imaging models, matching strong supervised baselines resulting in a 3–100x reduction in the need for retraining data. While SimCLR is the primary self-supervised learning approach used in the study, we also show that REMEDIS is compatible with other approaches, such as &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo-V2&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.07922&quot;>;RELIC&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.03230&quot;>;Barlow Twins&lt;/a>;. Furthermore, the approach works across model architecture sizes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIyKlK_o3J1b62jj1RPy7HS2MvM4qrcXCHWI0iqckQt2yYgLRzqM4K2s3Xz3qqH4u6Mle1E5Ixvw9RDVuBLIQDNT9a3XJYKZAdrNoTjhn_FkVakBVVxLmDUlLBJ7Ap3IHzzOVtUTfQt3hyUCF9Y0ntbFKFHSfy5n8op0d2kYYszsv7DhnX3HxHQxWwg/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIyKlK_o3J1b62jj1RPy7HS2MvM4qrcXCHWI0iqckQt2yYgLRzqM4K2s3Xz3qqH4u6Mle1E5Ixvw9RDVuBLIQDNT9a3XJYKZAdrNoTjhn_FkVakBVVxLmDUlLBJ7Ap3IHzzOVtUTfQt3hyUCF9Y0ntbFKFHSfy5n8op0d2kYYszsv7DhnX3HxHQxWwg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS outperformed the supervised baseline pre-trained on JFT-300M for various medical tasks and demonstrated improved data-efficient generalization, reducing data needs by 3–100x for adapting models to new clinical settings. This could potentially translate to significant reduction in clinician hours saved annotating data and cost of developing robust medical imaging systems.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1Q1VS1l1Xd4SqEqJySE7xjxr1L9X_g0lWXY3c8IHBVTsOWQ1X2gwVMbFeAKXHGo6kH3J1PUDsG8OPWKV1zC0dtGNwa6jETWEl9G2affZxyEtzmwOkwxoiO6MntRwrMxRo425fKMT4Q775O-YzAQmW-7k3McZmFq1qmW1i0UwA1vygRADWOJP62psERA/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1683&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1Q1VS1l1Xd4SqEqJySE7xjxr1L9X_g0lWXY3c8IHBVTsOWQ1X2gwVMbFeAKXHGo6kH3J1PUDsG8OPWKV1zC0dtGNwa6jETWEl9G2affZxyEtzmwOkwxoiO6MntRwrMxRo425fKMT4Q775O-YzAQmW-7k3McZmFq1qmW1i0UwA1vygRADWOJP62psERA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS is compatible with &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo-V2&lt;/a>;,&lt;a href=&quot;https://arxiv.org/pdf/2010.07922.pdf&quot;>; RELIC&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.03230&quot;>;Barlow Twins&lt;/a>; as alternate self-supervised learning strategies. All the REMEDIS variants lead to data-efficient generalization improvements over the strong supervised baseline for dermatology condition classification (&lt;strong>;T1&lt;/strong>;), diabetic macular edema classification (&lt;strong>;T2&lt;/strong>;), and chest X-ray condition classification (&lt;strong>;T3&lt;/strong>;). The gray shaded area indicates the performance of the strong supervised baseline pre-trained on JFT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Medical AI Research Foundations&lt;/h2>; &lt;p>; Building on REMEDIS, we are excited to announce &lt;a href=&quot;https://doi.org/10.13026/psq3-vj24&quot;>;Medical AI Research Foundations&lt;/a>;, an expansion of the public release of &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;chest X-ray Foundations&lt;/a>; in 2022. Medical AI Research Foundations is a repository of open-source medical foundation models hosted by PhysioNet. This expands the previous API-based approach to also encompass non-diagnostic models, to help researchers and developers accelerate their medical AI research. We believe that REMEDIS and the release of the Medical AI Research Foundations are a step toward building medical models that can generalize across healthcare settings and tasks. &lt;/p>; &lt;p>; We are seeding Medical AI Research Foundations with REMEDIS models for chest X-ray and pathology (with related&amp;nbsp;&lt;a href=&quot;https://github.com/google-research/medical-ai-research-foundations&quot;>;code&lt;/a>;). Whereas the existing chest X-ray Foundation approach focuses on providing frozen embeddings for application-specific fine tuning from a model trained on several large private datasets, the REMEDIS models (trained on public datasets) enable users to fine-tune end-to-end for their application, and to run on local devices. We recommend users test different approaches based on their unique needs for their desired application. We expect to add more models and resources for training medical foundation models such as datasets and benchmarks in the future. We also welcome the medical AI research community to contribute to this. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; These results suggest that REMEDIS has the potential to significantly accelerate the development of ML systems for medical imaging, which can preserve their strong performance when deployed in a variety of changing contexts. We believe this is an important step forward for medical imaging AI to deliver a broad impact. Beyond the experimental results presented, the approach and insights described here have been integrated into several of &lt;a href=&quot;https://health.google/health-research/&quot;>;Google&#39;s medical imaging research projects&lt;/a>;, such as dermatology, mammography and radiology among others. We&#39;re using a similar self-supervised learning approach with our non-imaging foundation model efforts, such as &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; and &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;Med-PaLM 2&lt;/a>;. &lt;/p>; &lt;p>; With REMEDIS, we demonstrated the potential of foundation models for medical imaging applications. Such models hold exciting possibilities in medical applications with the opportunity of multimodal representation learning. The practice of medicine is inherently multimodal and incorporates information from images, electronic health records, sensors, wearables, genomics and more. We believe ML systems that leverage these data at scale using self-supervised learning with careful consideration of privacy, safety, fairness and ethics will help lay the groundwork for the next generation of learning health systems that scale world-class healthcare to everyone. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;This work involved extensive collaborative efforts from a multidisciplinary team of researchers, software engineers, clinicians, and cross-functional contributors across Google Health AI and Google Brain. In particular, we would like to thank our first co-author Jan Freyberg and our lead senior authors of these projects, Vivek Natarajan, Alan Karthikesalingam, Mohammad Norouzi and Neil Houlsby for their invaluable contributions and support. We also thank Lauren Winer, Sami Lachgar, Yun Liu and Karan Singhal for their feedback on this post and Tom Small for support in creating the visuals. Finally, we also thank the PhysioNet team for their support on hosting Medical AI Research Foundations. Users with questions can reach out to medical-ai-research-foundations at google.com.&lt;/i>; &lt;/p>; &lt;/div>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8846807972710427001/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8846807972710427001&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8846807972710427001&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html&quot; rel=&quot;alternate&quot; title=&quot;Robust and efficient medical imaging with self-supervision&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvMtAszRzkUmNz3DliRRKOPMA0hi-EplY9vfJ9cWCZhFBUgnblKjVoObTYAfybreL7HEKh11g2unV5oR_KOWfkPxpe3Hzo6qWSWq4YxA9RuhkeGXdDJST2XvW2Nfl3L6kDpV1hA2b_vpBMVaw2_pxt2vWD2n92S9fDGAZp0a4c2dV2Ylf3XrUSLQxAoQ/s72-w640-h640-c/REMEDIS%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;