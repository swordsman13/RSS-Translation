<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-09-08T16:32:47.949-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="datasets"></category><category term="Education"></category><category term="Neural Networks"></category><category term="Health"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="On-device Learning"></category><category term="AI for Social Good"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="Computer Science"></category><category term="Quantum AI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="Image Classification"></category><category term="Self-Supervised Learning"></category><category term="accessibility"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="NeurIPS"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Responsible AI"></category><category term="ACL"></category><category term="Audio"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ML Fairness"></category><category term="video"></category><category term="Awards"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Supervised Learning"></category><category term="User Experience"></category><category term="Collaboration"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="TTS"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Environment"></category><category term="Google Accelerated Science"></category><category term="Speech Recognition"></category><category term="DeepMind"></category><category term="Google Translate"></category><category term="Large Language Models"></category><category term="Video Analysis"></category><category term="statistics"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="UI"></category><category term="Vision Research"></category><category term="Acoustic Modeling"></category><category term="Interspeech"></category><category term="Systems"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Differential Privacy"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="crowd-sourcing"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Art"></category><category term="Biology"></category><category term="Climate"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Graphs"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="Weather"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1352&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-1569605132526995799&lt;/id>;&lt;发布>;2024-03-29T11:03:00.000-07:00&lt;/发布>;&lt;更新>;2024-03- 29T11:03:10.261-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Climate&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;天气&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;生成人工智能来量化天气预报的不确定性&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Lizao (Larry) Li, Google 研究部软件工程师和研究科学家 Rob Carver&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdl ATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif&quot;样式=&quot;显示：无；” />; &lt;p>; 准确的天气预报可以对人们的生活产生直接影响，从帮助做出日常决策（例如为一天的活动携带什么物品）到通知紧急行动（例如，在恶劣的天气条件下保护人们）。随着气候变化，准确及时的天气预报的重要性只会越来越大。认识到这一点，我们谷歌一直在投资天气和气候研究，以帮助确保未来的预报技术能够满足对可靠天气信息的需求。我们最近的一些创新包括 &lt;a href=&quot;https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot;>;MetNet-3&lt;/a>;、 Google 对未来长达 24 小时的高分辨率预测，以及 &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global- Weather-forecasting/&quot;>;GraphCast&lt;/a>;，一种可以预测最多未来 10 天天气的天气模型。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 天气本质上是随机的。为了量化不确定性，传统方法依靠基于物理的模拟来生成预测集合。然而，生成大型集合以便准确识别和表征罕见和极端天气事件的计算成本很高。考虑到这一点，我们很高兴地宣布我们旨在加速天气预报进展的最新创新，&lt;a href=&quot;https://www.science.org/doi/10.1126/sciadv.adk4489 &quot;>;可扩展整体包络扩散采样器&lt;/a>; (SEEDS)，最近发表在&lt;em>;&lt;a href=&quot;https://www.science.org/journal/sciadv&quot;>;科学进展&lt;/a>;&lt;/em >;。 SEEDS 是一种生成式 AI 模型，可以有效地大规模生成天气预报集合，而成本仅为传统基于物理的预报模型的一小部分。这项技术为天气和气候科学开辟了新的机遇，它代表了概率扩散模型在天气和气候预测中的首批应用之一，概率扩散模型是媒体生成最新进展背后的一种生成人工智能技术。 &lt;/p>; &lt;br />; &lt;h2>;概率预测的必要性：蝴蝶效应&lt;/h2>; &lt;p>; 1972 年 12 月，&lt;a href=&quot;https://www.aaas.org/&quot;>;美国科学促进会&lt;/a>;在华盛顿召开会议，麻省理工学院气象学教授&lt;a href=&quot;https://en.wikipedia.org/wiki/Edward_Norton_Lorenz&quot;>;Ed Lorenz&lt;/a>;做了题为“ ，“巴西蝴蝶翅膀的扇动是否会在德克萨斯州引发龙卷风？”这促成了“&lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;蝴蝶效应&lt;/a>;”一词。他在 1963 年发表的具有里程碑意义的论文的基础上进行了研究，在该论文中，他研究了“超远程天气预报”的可行性，并描述了当与数值天气预报模型及时集成时，初始条件的误差如何呈指数级增长。这种指数误差增长（称为混沌）会导致确定性可预测性限制，从而限制了决策中个人预测的使用，因为它们无法量化天气条件固有的不确定性。在预测飓风、热浪或洪水等极端天气事件时，这一点尤其成问题。 &lt;/p>; &lt;p>; 认识到确定性预报的局限性，世界各地的气象机构都发布了概率预报。此类预测基于确定性预测的集合，每个预测都是通过在初始条件中包含合成噪声和物理过程中的随机性来生成的。利用天气模型中快速的误差增长率，集合中的预测有目的地不同：初始不确定性被调整以生成尽可能不同的运行，并且天气模型中的随机过程在模型运行期间引入了额外的差异。通过对集合中的所有预测进行平均来减轻误差的增长，并且预测集合中的可变性量化了天气条件的不确定性。 &lt;/p>; &lt;p>; 虽然有效，但生成这些概率预测的计算成本很高。它们需要在大型超级计算机上多次运行高度复杂的数值天气模型。因此，许多业务天气预报只能为每个预报周期生成约 10-50 个集合成员。对于担心罕见但影响大的天气事件的可能性的用户来说，这是一个问题，这些天气事件通常需要更大的集合来评估几天之后的情况。例如，需要一个由 10,000 名成员组成的集合来预测发生概率为 1% 的事件的可能性，相对误差小于 10%。量化此类极端事件的概率可能有用，例如对于应急管理准备或能源贸易商。 &lt;/p>; &lt;br />; &lt;h2>;SEEDS：人工智能驱动的进步&lt;/h2>; &lt;p>; 在上述&lt;a href=&quot;https://www.science.org/doi/10.1126/sciadv.adk4489&quot;中>;论文&lt;/a>;，我们提出了可扩展集合包络扩散采样器（SEEDS），这是一种用于天气预报集合生成的生成人工智能技术。 SEEDS 基于&lt;a href=&quot;https://blog.research.google/2021/07/high-fidelity-image- Generation-using.html&quot;>;去噪扩散概率&lt;/a>;模型，这是一种状态最先进的生成式人工智能方法部分由谷歌研究院首创。 &lt;/p>; &lt;p>; SEEDS 可以根据运行数值天气预报系统的一到两次预报生成大型集合。生成的集合不仅可以产生可信的类似真实天气的预测，而且在技能指标（例如排名直方图）方面也可以匹配或超过基于物理的集合&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Root-mean-square_deviation&quot;>;均方根误差&lt;/a>; (RMSE) 和 &lt;a href= “https://www.tandfonline.com/doi/abs/10.1198/016214506000001437&quot;>;连续排名概率得分&lt;/a>; (CRPS)。特别是，生成的集合为预测分布的尾部分配了更准确的可能性，例如 ±2σ 和 ±3σ 天气事件。最重要的是，与超级计算机进行预测所需的计算时间相比，该模型的计算成本可以忽略不计。它在 Google Cloud TPUv3-32 实例上每 3 分钟具有 256 个集成成员（分辨率为 2°）的吞吐量，并且可以通过部署更多加速器轻松扩展到更高的吞吐量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1 BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;1000&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3 PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;SEEDS 生成更多数量级的样本来填充天气模式的分布。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;生成合理的天气预报&lt;/h2>; &lt;p>;众所周知，生成式人工智能可以生成非常详细的图像和视频。此属性对于生成与可能的天气模式一致的集合预报特别有用，这最终为下游应用带来最大的附加值。正如 Lorenz 指出的那样，“他们制作的[天气预报]地图应该看起来像真实的天气地图。”下图将 SEEDS 的预测与美国正在运行的天气预报系统的预测进行了对比 (&lt;a href=&quot;https:// www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/gefs.php&quot;>;全球集合预报系统&lt;/a>;，GEFS）&lt;a href=&quot;https://en.wikipedia 期间的特定日期.org/wiki/2022_European_heatwaves&quot;>;2022 年欧洲热浪&lt;/a>;。我们还将结果与高斯模型的预测进行了比较，该模型预测每个位置每个大气场的单变量平均值和标准差，这是一种常见且计算高效的方法但不太复杂的数据驱动方法。这种高斯模型旨在表征逐点后处理的输出，它忽略相关性并将每个网格点视为独立的随机变量。相比之下，真实的天气图将具有详细的&lt;em>;。 &lt;/p>; &lt;p>; 由于 SEEDS 直接模拟大气状态的联合分布，因此它可以真实地捕获对流层中部位势与平均海平面压力之间的空间协方差和相关性。密切相关，天气预报员通常使用它们来评估和验证预报。平均海平面压力的梯度是驱动地表风的因素，而对流层中部位势的梯度则产生高层风，从而改变大规模的天气模式。 &lt;/p>; &lt;p>; 下图所示的 SEEDS 生成的样本（Ca-Ch 帧）显示了葡萄牙西部的一个位势槽，其空间结构与美国实际预测或基于观测的再分析中发现的空间结构相似。尽管高斯模型可以充分预测边缘单变量分布，但它无法捕获跨领域或空间相关性。这阻碍了对这些异常现象对来自北非的热空气入侵可能产生的影响的评估，而北非的热空气入侵可能会加剧欧洲的热浪。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVu jZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s1999/image2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1675&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8u FWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;2022 年 7 月 14 日 0:00 UTC 欧洲各地的邮票地图。等值线代表平均海平面压力（虚线标记低于 1010 hPa 的等压线），而热图则描绘 500 hPa 压力水平下的位势高度。 (A)&lt;a href=&quot;https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5&quot;>;ERA5&lt;/a>;重新分析，真实观察的代理。 (Ba-Bb) 2 位成员将美国 7 天的运营预测用作我们模型的种子。 (Ca-Ch) 8 个来自 SEEDS 的样品。 (Da-Dh) 8 个非种子成员来自 7 天的美国业务集合预报。 (Ea-Ed) 来自逐点高斯模型的 4 个样本，该模型由整个美国作战集合的均值和方差参数化。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;更准确地报道极端事件&lt;/h2>; &lt;p>; 下面我们展示了极端高温期间里斯本附近2米处的温度和总水汽柱的联合分布活动时间：2022年7月14日，当地时间1:00。我们使用 2022 年 7 月 7 日发布的 7 天预测。对于每个图，我们使用 SEEDS 生成 16,384 个成员的集合。 ERA5 观测到的天气事件用星号表示。还显示了操作系综，其中正方形表示用于为生成的系综提供种子的预测，三角形表示其余系综成员。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k 6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;941&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqk KeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;SEEDS 提供了对 2022 年 7 月 14 日欧洲极端高温事件（以棕色星表示）的更好统计覆盖。每张图显示了葡萄牙里斯本附近网格点上的总柱积分水蒸气 (TCVW) 与温度的关系，这些样本来自我们的模型生成的 16,384 个样本（显示为绿点），以取自 2 个种子（蓝色方块）为条件美国 7 天业务集合预报（由稀疏的棕色三角形表示）。有效预报时间为当地时间1:00。实体轮廓级别对应于 SEEDS 内核密度的等比例，最外层包围质量的 95%，每个级别之间包围 11.875%。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; br />; &lt;p>; 根据美国作战小组的说法，在 7 天前观测到的事件发生的可能性非常小，以至于其 31 名成员都没有预测到近地表温度会像观测到的那样温暖。事实上，根据高斯核密度估计计算出的事件概率低于 1%，这意味着成员少于 100 人的集合不太可能包含像此事件一样极端的预测。相比之下，SEEDS 集合能够从两个播种预测中进行推断，提供可能的天气状态的范围，并更好地统计事件的覆盖范围。这样既可以量化事件发生的概率，又可以对事件发生的天气状况进行采样。具体来说，我们高度可扩展的生成方法可以创建非常大的集合，通过为任何用户定义的诊断提供超过给定阈值的天气状态样本来表征非常罕见的事件。 &lt;/p>; &lt;br />; &lt;h2>;结论和未来展望&lt;/h2>; &lt;p>; SEEDS 利用生成式人工智能的力量来生成与美国正在运行的预报系统相当的集合预报，但速度更快。本文报告的结果只需要来自操作系统的 2 个播种预测，该系统在当前版本中生成 31 个预测。这导致了一种混合预报系统，其中使用基于物理的模型计算的一些天气轨迹来播种扩散模型，该模型可以更有效地生成额外的预报。这种方法提供了当前操作天气预报范例的替代方案，其中统计仿真器节省的计算资源可以分配用于提高基于物理的模型的分辨率或更频繁地发布预报。 &lt;/p>; &lt;p>; 我们相信，SEEDS 只是人工智能在未来几年加速数值天气预报业务进展的众多方式之一。我们希望这一生成式人工智能在天气预报模拟和后处理方面的实用性演示将刺激其在气候风险评估等研究领域的应用，在这些领域中，生成大量气候预测集合对于准确量化未来的不确定性至关重要气候。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;所有 SEEDS 作者 Lizao Li、Rob Carver、Ignacio Lopez-Gomez、Fei Sha 和 John Anderson 共同撰写了这篇博文， Carla Bromberg 担任项目负责人。我们还要感谢动画设计者 Tom Small。我们 Google Research 的同事为 SEEDS 工作提供了宝贵的建议。其中，我们感谢 Leonardo Zepeda-Núñez、Zhong Yi Wan、Stephan Rasp、Stephan Hoyer 和 Tapio Schneider 的投入和有益的讨论。我们感谢泰勒·拉塞尔 (Tyler Russell) 提供的额外技术项目管理，以及亚历克斯·梅罗斯 (Alex Merose) 的数据协调和支持。我们还感谢 Cenk Gazen、Shreya Agrawal 和 Jason Hickey 在 SEEDS 工作的早期阶段进行的讨论。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1569605132526995799/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1569605132526995799&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html&quot; rel=&quot;alternate&quot; title=&quot;生成人工智能来量化天气预报的不确定性&quot; type=&quot;text/html&quot;/ >;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel =“http://schemas.google.com/g/2005#thumbnail”src =“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“16”>; &lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATD KSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt; id>;标签：blogger.com，1999：blog-8474926331452026626.post-1799535679952845079&lt;/id>;&lt;发布>;2024-03-28T13:53:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-29T12： 00:03.604-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;神经网络&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>; &lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;statistics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AutoBNN：组合概率时间序列预测贝叶斯神经网络&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Urs Köster&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4trPid -OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s320/AutoBNN.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;时间序列&lt;/a>;问题无处不在，从预测天气和交通模式到了解经济趋势。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_inference&quot;>;贝叶斯&lt;/a>;方法从对数据模式（先验概率）的假设开始，收集证据（例如，新的时间序列数据），并不断更新该假设以形成后验概率分布。传统的贝叶斯方法，例如高斯过程（GP）和&lt;a href=&quot;https://blog.tensorflow.org/2019/03/ Structure-time-series-modeling-in.html&quot;>;结构时间序列&lt;/a>;广泛用于对时间序列数据进行建模，例如常用的&lt;a href=&quot;https://gml.noaa.gov/ccgg /trends/&quot;>;冒纳罗亚二氧化碳&lt;/a>;数据集。然而，他们通常依赖领域专家来精心选择合适的模型组件，并且计算成本可能很高。神经网络等替代方案缺乏可解释性，因此很难理解它们如何生成预测，并且不能产生可靠的置信区间。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 为此，我们引入 &lt;a href=&quot;https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn&quot; >;AutoBNN&lt;/a>;，一个用 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; 编写的新开源包。 AutoBNN 自动发现可解释的时间序列预测模型，提供高质量的不确定性估计，并有效扩展以用于大型数据集。我们描述了 AutoBNN 如何将传统概率方法的可解释性与神经网络的可扩展性和灵活性结合起来。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;AutoBNN&lt;/h2>; &lt;p>; AutoBNN 基于 &lt;a href=&quot;https:/ /proceedings.mlr.press/v28/duvenaud13.html&quot;>;行&lt;/a>; &lt;a href=&quot;https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550&quot;>;&lt;/a>; &lt;a href= &quot;https://proceedings.mlr.press/v202/saad23a.html&quot;>;过去十年的研究&lt;/a>;通过使用具有学习能力的 GP 进行时间序列建模，提高了预测准确性&lt;a href=&quot;https:// www.cs.toronto.edu/~duvenaud/cookbook/&quot;>;内核&lt;/a>;结构。 GP 的核函数对有关正在建模的函数的假设进行编码，例如趋势、周期性或噪声的存在。对于学习的 GP 核，核函数是组合定义的：它是一个基本核（例如线性核、二次核、周期性核、周期性核） >;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function&quot;>;Matérn&lt;/a>;&lt;/code>; 或 &lt;code>;ExponentiatedQuadratic&lt;/code>;）或组合两个或使用&lt;code>;加法&lt;/code>;、&lt;code>;乘法&lt;/code>;或&lt;code>;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/170等运算符的多个内核函数.pdf&quot;>;ChangePoint&lt;/a>;&lt;/code>;。这种组合内核结构有两个相关的目的。首先，这很简单，对于数据专家（但不一定是 GP 专家）的用户可以为其时间序列构建合理的先验。其次，诸如&lt;a href=&quot;https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf&quot;>;顺序蒙特卡罗&lt;/a>;之类的技术可用于小型结构上的离散搜索，并可以输出可解释的结果。&lt;/p>; &lt;p>; AutoBNN 改进了这些想法，用&lt;a href=&quot;https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/&quot;>;贝叶斯神经网络&lt; /a>; (BNN)，同时保留组合内核结构。 BNN 是一种具有权重概率分布的神经网络，而不是一组固定的权重。这会导致输出分布，捕获预测中的不确定性。与 GP 相比，BNN 具有以下优势：首先，训练大型 GP 的计算成本很高，并且传统的训练算法按时间序列中数据点数量的立方进行缩放。相反，对于固定宽度，训练 BNN 通常与数据点的数量呈近似线性关系。其次，与 GP 训练操作相比，BNN 更适合 GPU 和 &lt;a href=&quot;https://cloud.google.com/tpu?hl=en&quot;>;TPU&lt;/a>; 硬件加速。第三，组合 BNN 可以轻松地与&lt;a href=&quot;https://arxiv.org/abs/2007.06823&quot;>;传统深度 BNN&lt;/a>; 结合，后者具有特征发现的能力。人们可以想象一种“混合”架构，其中用户指定 &lt;code>;Add&lt;/code>;(&lt;code>;Linear&lt;/code>;, &lt;code>;Periodic&lt;/code>;, &lt;code>;Deep&lt; 的顶级结构/code>;），而深度 BNN 则用来学习潜在高维协变量信息的贡献。 &lt;/p>; &lt;p>; 那么如何将具有组合内核的 GP 转换为 BNN？单层神经网络通常会随着神经元数量（或“宽度”）收敛到 GP &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-1-4612-0745-0_2 ”趋于无穷大&lt;/a>;。最近，研究人员&lt;a href=&quot;https://openreview.net/forum?id=gRwh5HkdaTm&quot;>;发现了&lt;/a>;另一个方向的对应关系——许多受欢迎的全科医生&lt;a href=&quot;https://www .cs.toronto.edu/~duvenaud/cookbook/&quot;>;内核&lt;/a>;（例如 &lt;code>;Matern&lt;/code>;、&lt;code>;ExponentiatedQuadratic&lt;/code>;、&lt;code>;Polynomial&lt;/code>; 或 &lt; code>;Periodic&lt;/code>;）可以通过适当选择激活函数和权重分布作为无限宽度的 BNN 获得。此外，即使宽度远小于无穷大，这些 BNN 仍然接近相应的 GP。例如，下图显示了 &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_matrix#:~:text=In%20probability%20theory%20and%20statistics,of%20a%20given 中的差异%20random%20vector&quot;>;观测对之间的协方差&lt;/a>;，以及真实 GP 及其对应的&lt;a href=&quot;https://en.wikipedia.org/wiki/Kriging&quot;>;回归&lt;/a>;结果width-10 神经网络版本。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_q F-eGv2C1QkU9oDCAS09FfoCne81yEAqC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s1350/image3.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;598&quot; data-original-width=&quot;1350&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_qF-eGv2C1QkU9oDCAS09FfoCne81yEA qC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;真实 GP 内核（顶行）与其宽度 10 神经网络之间的 &lt;a href=&quot;https://en.wikipedia.org/wiki/Gram_matrix&quot;>;Gram 矩阵&lt;/a>; 的比较网络近似值（底行）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; 样式=“左边距：自动”； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoidYqlAK2J1n4y71Qn- WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux 3AXzUENmJ0NA8Ch9dyICT15/s1328/image4.png&quot; style=&quot;margin-left: 自动; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;586&quot; data-original-width=&quot;1328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhoidYqlaK2J1n4y71Qn-WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC 2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux3AXzUENmJ0NA8Ch9dyICT15/s16000/image4.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;真实 GP 内核（顶行）与其宽度为 10 的神经网络近似值（底行）之间的回归结果比较。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;最后，通过&lt;code>;加法&lt;/code>;和&lt;code>;乘法&lt;/code>;的&lt;a href=&quot;https://arxiv.org/abs/1905.06076&quot;>;BNN类似物&lt;/a>;完成翻译GP 上的算子，并且通过将组件 BNN 的输出相加来直接产生 BNN 加法，BNN 乘法是通过乘以 BNN 隐藏层的激活然后应用共享密集层来实现的。因此，我们只能将具有相同隐藏宽度的 BNN 相乘。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用 AutoBNN&lt;/h2>; &lt;p>; AutoBNN &lt;a href=&quot;https://github .com/tensorflow/probability/tree/main/spinoffs/autobnn&quot;>;软件包&lt;/a>;在&lt;a href=&quot;https://www.tensorflow.org/probability&quot;>;Tensorflow Probability&lt;/a>;中提供。它在 &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>; 中实现并使用 &lt;a href=&quot;https://github.com/google/flax&quot;>;flax。 linen&lt;/a>; 神经网络库。它实现了迄今为止讨论的所有基本内核和运算符（&lt;code>;Linear&lt;/code>;、&lt;code>;Quadratic&lt;/code>;、&lt;code>;Matern&lt;/code>;、&lt;code>;ExponentiatedQuadratic&lt;/code>;、&lt; code>;Periodic&lt;/code>;、&lt;code>;Addition&lt;/code>;、&lt;code>;Multiplication&lt;/code>;）加上一个新内核和三个新运算符：&lt;/p>; &lt;ul>; &lt;li>;a &lt;code>;OneLayer &lt;/code>; 内核，单个隐藏层 &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;ReLU&lt;/a>; BNN，&lt;/li>;&lt;li>;a &lt;code >;&lt;a href=&quot;https://icml.cc/Conferences/2010/papers/170.pdf&quot;>;ChangePoint&lt;/a>;&lt;/code>; 运算符，允许在两个内核之间平滑切换，&lt;/li>;&lt;li>;一个 &lt;code>;LearnableChangePoint&lt;/code>; 运算符，与 &lt;code>;ChangePoint&lt;/code>; 相同，只是位置和斜率是给定先验分布并且可以从数据中学习，以及 &lt;/li>;&lt;li>;a &lt;code >;WeightedSum&lt;/code>; 运算符。 &lt;/li>; &lt;/ul>; &lt;p>; &lt;code>;WeightedSum&lt;/code>; 将两个或多个 BNN 与可学习的混合权重结合起来，其中可学习的权重遵循 &lt;a href=&quot;https://en.wikipedia.org/ wiki/Dirichlet_distribution&quot;>;狄利克雷先验&lt;/a>;。默认情况下，使用浓度为 1.0 的平坦狄利克雷分布。 &lt;/p>; &lt;p>; &lt;code>;WeightedSums&lt;/code>; 允许结构发现的“软”版本，即一次训练许多可能模型的线性组合。与离散结构的结构发现相比，例如在 AutoGP 中，这允许我们使用标准梯度方法来学习结构，而不是使用昂贵的离散优化。 WeightedSum 允许我们并行评估它们，而不是串联评估潜在的组合结构。 &lt;/p>; &lt;p>; 为了轻松实现探索，AutoBNN 定义了&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py&quot;>;模型数量包含顶级或内部 &lt;code>;WeightedSums&lt;/code>; 的结构&lt;/a>;。这些模型的名称可以用作任何 &lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/estimators.py&quot;>;估计器&lt;中的第一个参数/a>; 构造函数，并包含诸如 &lt;code>;&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py#L133&quot;>;sum_of_stumps&lt;/a 之类的内容>;&lt;/code>;（所有基本内核的 &lt;code>;WeightedSum&lt;/code>;）和 &lt;code>;sum_of_shallow&lt;/code>;（将基本内核与所有运算符的所有可能组合相加）。&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td 样式=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXz CeSCZBFDHIc1qYfK53EwEdngf1KykzCfpPiIg3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s1389/image2.png&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;255&quot; data-original-width=&quot;1389&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXzCesCZBFDHIc1qYfK53EwEdngf1KykzCfpPiI g3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;sum_of_stumps 模型的图示。顶行中的条显示每个基本内核贡献的量，底行显示基本内核所代表的函数。生成的加权和显示在上。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 下图演示了从&lt;a href=&quot;https://forecasters.org/resources/time-series-data/m3-competition/&quot;>;M3&lt;/a>; 数据集。六个基本结构是 &lt;code>;ExponentiatedQuadratic&lt;/code>; （这是相同的）作为径向基函数内核，或简称为 &lt;a href=&quot;https://en.wikipedia.org/wiki/Radial_basis_function_kernel&quot;>;RBF&lt;/a>;）、&lt;code>;Matern&lt;/code>;、&lt;code>;Linear &lt;/code>;、&lt;code>;Quadratic&lt;/code>;、&lt;code>;OneLayer&lt;/code>; 和 &lt;code>;Periodic&lt;/code>; 内核。该图显示了 32 个粒子集合中它们的权重的 MAP 估计值。所有高似然粒子对周期性分量给予较大权重，对线性分量、二次分量和单层分量给予较低权重>;，以及 &lt;code>;RBF&lt;/code>; 或 &lt;code>;Matern&lt;/code>; 的较大权重。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H 2-JZOmxxRJSm9ExG_PUr6U7iFl8nyp4lEaNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s868/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;868&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H2-JZOmxxRJSm9ExG_PUR6U7iFl8nyp4lE aNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;32 个粒子的基本核权重的 &lt;a href=&quot;https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php&quot;>;MAP&lt;/a>; 估计的平行坐标图。 &lt;code>;sum_of_stumps&lt;/code>; 模型在 M3 数据集的 N374 系列上进行训练（插入蓝色）。较暗的线对应于可能性较高的粒子。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;通过使用&lt;code>;WeightedSums&lt;/code>;作为其他算子的输入，可以表示丰富的组合结构，同时保持模型紧凑和可学习权重的数量较少。例如，我们包含 &lt;code>;sum_of_products&lt;/code>; 模型（如下图所示），它首先创建两个 &lt;code>;WeightedSums&lt;/code>; 的成对乘积，然后计算两个乘积的总和。通过将一些权重设置为零，我们可以创建许多不同的离散结构。该模型中可能的结构总数为 2&lt;sup>;16&lt;/sup>;，因为有 16 个可以打开或关闭的基本内核。所有这些结构都是通过仅训练这一模型来隐式探索的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQ tvzMR_548jt4h2lbTzfnp7ZUeYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s1754/AutoBNN%20illustration.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;640&quot; data-original-width=&quot;1754&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQtvzMR_548jt4h2lbTzfnp7ZU eYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s16000/AutoBNN%20illustration.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;“sum_of_products”模型的图示。四个 WeightedSum 中的每一个都具有与“sum_of_stumps”模型相同的结构。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 然而，我们发现某些内核组合（例如， &lt;code>;Periodic&lt;/code>; 与 &lt;code>;Matern&lt;/code>; 或 &lt;code>;ExponentiatedQuadratic&lt;/code>; 的乘积）会导致许多数据集过度拟合。为了防止这种情况，我们定义了 &lt;code>;sum_of_safe_shallow&lt;/code>; 等模型类，在使用 &lt;code>;WeightedSums&lt;/code>; 执行结构发现时排除此类乘积。 &lt;/p>; &lt;p>; 对于训练，AutoBNN 提供了 AutoBnnMapEstimator 和 AutoBnnMCMCEstimator 来分别执行 MAP 和 MCMC 推理。任一估计器都可以与六个&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/likelihoods.py&quot;>;似然函数&lt;/a>;中的任何一个组合，包括四种基于连续数据的具有不同噪声特征的正态分布，两种基于计数数据的负二项式分布。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31Pbx TQF29yH1cTJRdI​​-XkXmnZMR_imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s1076/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;280&quot; data-original-width=&quot;1076&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31PbxTQF29yH1cTJRdI​​-XkXmnZMR _imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 &lt;a href=&quot;https://gml.noaa.gov/ccgg/trends/&quot;>;Mauna Loa CO2&lt;/a 上运行 AutoBNN 的结果>; 我们的示例 &lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb&quot;>;colab&lt;/a>; 中的数据集。该模型捕获数据中的趋势和季节性成分。外推未来，均值预测略微低估了实际趋势，而 95% 置信区间逐渐增大。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 拟合如图所示的模型上面，所需要的只是以下 10 行代码，使用 &lt;a href=&quot;https://scikit-learn.org/stable/&quot;>;scikit-learn&lt;/a>; 启发的估计器接口：&lt;/p>; &lt;pre class=&quot;prettyprint&quot;>;导入 autobnn 作为 ab model = ab.operators.Add( bnns=(ab.kernels.PeriodicBNN(width=50), ab.kernels.LinearBNN(width=50), ab.kernels.MaternBNN (width=50))) estimator = ab.estimators.AutoBnnMapEstimator( model, &#39;normal_likelihood_logistic_noise&#39;, jax.random.PRNGKey(42), period=[12]) estimator.fit(my_training_data_xs, my_training_data_ys) 低、中、高 = estimator.predict_quantiles(my_training_data_xs) &lt;/pre>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; &lt;a href =&quot;https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn&quot;>;AutoBNN&lt;/a>; 为构建复杂的时间序列预测模型提供了强大而灵活的框架。通过将 BNN 和 GP 的优势与组合内核相结合，AutoBNN 为理解和预测复杂数据打开了一个充满可能性的世界。我们邀请社区尝试&lt;a href=&quot;https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb&quot; target=&quot;_blank&quot;>;colab&lt;/a>;，并且利用该库进行创新并解决现实世界的挑战。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;AutoBNN 由 Colin Carroll、Thomas Colthurst 编写，乌尔斯·科斯特和斯里尼瓦斯·瓦苏代万。我们要感谢 Kevin Murphy、Brian Patton 和 Feras Saad 的建议和反馈。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research .google/feeds/1799535679952845079/comments/default&quot; rel=&quot;replies&quot; title=&quot;帖子评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/ 03/autobnn-probabilistic-time-series.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com /feeds/8474926331452026626/posts/default/1799535679952845079&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 1799535679952845079&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/autobnn-probabilistic-time-series.html&quot; rel=&quot; ternate&quot; title=&quot;AutoBNN：使用组合贝叶斯神经网络进行概率时间序列预测&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4 trPid-OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s72-c/AutoBNN.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7061041222399769838&lt;/ id>;&lt;发布>;2024-03-20T13:54:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-20T13:54:06.249-07:00&lt;/更新>;&lt;category schema=&quot;http:// /www.blogger.com/atom/ns#&quot; term=&quot;健康&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt; /category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;肺癌筛查的计算机辅助诊断&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究部软件工程师 Atilla Kiraly 和产品经理 Rory Pilgrim &lt;/span>; &lt;img src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD7R8_tNqtH6D8X_KiMt ZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s320/PULMA%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 肺癌是全球癌症相关死亡的主要原因 &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/cancer#:~:text= 2020 年报告的%20most%20common%20causes%20of，直肠%20（916%20000%20deaths）%3B&quot;>;180 万人死亡&lt;/a>;。晚期诊断大大降低了生存机会。 &lt;a href=&quot;https://www.cdc.gov/cancer/lung/basic_info/screening.htm&quot;>;肺癌筛查&lt;/a>;通过&lt;a href=&quot;https://www.cancer.gov/about -cancer/diagnosis-staging/ct-scans-fact-sheet#:~:text=indicate%20real%20problems.-,Lung%20cancer,-Low%2Ddose%20CT&quot;>;计算机断层扫描&lt;/a>; (CT),它提供了详细的肺部 3D 图像，已被证明可以通过更早地检测潜在的癌症迹象，将高危人群的死亡率降低至少 20%。在美国，筛查涉及每年一次扫描，一些国家或病例建议或多或少地进行扫描。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/lung-cancer-screening&quot;>;美国预防服务工作组&lt;/a>;最近将肺癌筛查建议扩大了&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;大约80%&lt;/a>;，预计将增加筛查妇女以及少数种族和族裔群体的参与机会。然而，假阳性（即错误地报告无癌症患者的潜在癌症）可能会引起焦虑，并导致患者接受不必要的手术，同时增加医疗保健系统的成本。此外，根据医疗基础设施和放射科医生的可用性，筛查大量个体的效率可能具有挑战性。 &lt;/p>; &lt;p>; 在 Google，我们之前开发了&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;用于肺癌检测的机器学习 (ML) 模型&lt;/ a>;，并评估了它们自动检测和分类显示潜在癌症迹象的区域的能力。事实证明，在检测可能的癌症方面，其性能可与专家相媲美。虽然他们取得了很高的绩效，但要充分发挥他们的潜力，在现实环境中有效地传达发现是必要的。 &lt;/p>; &lt;p>; 为此，在“&lt;a href=&quot;https://pubs.rsna.org/doi/10.1148/ryai.230079&quot;>;肺癌筛查中的辅助人工智能：一项回顾性跨国研究美国和日本&lt;/a>;”，发表于&lt;em>;&lt;a href=&quot;https://pubs.rsna.org/journal/ai&quot;>;放射学人工智能&lt;/a>;&lt;/em>;，我们研究了机器学习如何建模可以有效地将发现传达给放射科医生。我们还引入了一个以用户为中心的通用界面，以帮助放射科医生利用此类模型进行肺癌筛查。该系统以 CT 成像作为输入，并使用四个类别（无怀疑、可能良性、可疑、高度可疑）以及相应的感兴趣区域输出癌症怀疑评级。我们通过美国和日本的随机读者研究，使用当地癌症评分系统 (&lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/ RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot;>;Lung-RADS V1.1&lt;/a>; 和 &lt;a href=&quot;https://www.jscts.org/pdf/guideline/gls3rdfig_english130621.pdf&quot;>;仙台分数&lt;/a>;）和模仿现实设置的图像查看器。我们发现，在两项读者研究中，读者特异性随着模型的帮助而增加。为了加快利用机器学习模型进行类似研究的进展，我们提供&lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>;开源代码&lt;/a>;处理 CT 图像并生成与放射科医生使用的&lt;a href=&quot;https://en.wikipedia.org/wiki/Picture_archiving_and_communication_system&quot;>;图片存档和通信系统&lt;/a>; (PACS) 兼容的图像。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开发一个接口来传达模型结果&lt;/h2>; &lt;p>; 将 ML 模型集成到放射科医生工作流程中涉及了解他们任务的细微差别和目标，以便为他们提供有意义的支持。在肺癌筛查方面，医院遵循定期更新的各个国家/地区特定指南。例如，在美国，Lung-RADs V1.1 指定 &lt;a href=&quot;https://www.acr.org/-/media/ACR/Files/RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf&quot; >;字母数字评分&lt;/a>;表示肺癌风险和后续建议&lt;em>;。 &lt;/em>;在评估患者时，放射科医生将 CT 加载到工作站中以读取病例、查找肺部结节或病变，并应用既定指南来确定后续决策。 &lt;/p>; &lt;p>; 我们的第一步是通过额外的训练来改进&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前开发的机器学习模型&lt;/a>;数据和架构改进，包括&lt;a href=&quot;https://research.google/pubs/attention-is-all-you-need/&quot;>;自我注意力&lt;/a>;。然后，我们没有针对特定的指南，而是尝试了一种独立于指南或其特定版本来传达人工智能结果的补充方式。具体来说，系统输出提供怀疑评级和定位（感兴趣区域），供用户结合自己的具体指南进行考虑。该界面可生成与 CT 研究直接相关的输出图像，无需更改用户的工作站。放射科医生只需要查看一小组附加图像。他们的系统或与系统的交互没有其他变化。 &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug- HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s857/image1.png&quot; style=&quot;mar&quot; gin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original -width =“857”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug-HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6 FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s16000/image1.png&quot; />;&lt;/a >;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统输出示例。放射科医生的评估结果在发现可疑病变的 CT 体积位置上进行可视化。总体怀疑显示在 CT 图像的顶部。圆圈突出显示可疑病变，而方形则显示从不同角度（称为矢状视图）对同一病变的渲染。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 辅助肺癌筛查系统包括13 个模型，并具有类似于&lt;a href=&quot;https://blog.google/technology/health/lung-cancer-prediction/&quot;>;之前的工作&lt;/a中使用的端到端系统的高级架构>;。这些模型相互协调，首先对肺部进行分割，获得总体评估，定位三个可疑区域，然后使用该信息为每个区域分配可疑评级。该系统使用 &lt;a href=&quot;https://cloud.google.com/kubernetes-engine&quot;>;Google Kubernetes Engine&lt;/a>; (GKE) 部署在 Google Cloud 上，该引擎提取映像、运行机器学习模型并提供了结果。这样可以实现可扩展性，并直接连接到&lt;a href=&quot;https://cloud.google.com/healthcare-api/docs/concepts/dicom&quot;>;DICOM 存储&lt;/a>;中存储图像的服务器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfk b9mGxEmL191sfpAclKD085x-u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s646/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;394&quot; data-original-width=&quot;646&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9mGxEmL191sfpAclKD085x- u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;辅助肺癌筛查系统的 Google Cloud 部署概述以及提供图像和计算结果的各个组件的定向调用流程。使用 Google Cloud 服务将图像提供给查看者和系统。该系统在 Google Kubernetes Engine 上运行，该引擎提取图像、处理图像并将其写回到 DICOM 存储中。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;读者研究&lt;/h2>; &lt;p>;为了评估系统在改善临床表现方面的效用，我们进行了两项读者研究（即设计的实验）使用预先存在的、去识别化的 CT 扫描来评估临床表现（比较有或没有技术帮助的专家表现）与 12 名放射科医生。我们向 6 位美国放射科医生和 6 位日本放射科医生介绍了 627 个具有挑战性的病例。在实验设置中，读者被分为两组，每个案例阅读两次，有或没有模型的帮助。读者被要求应用他们在临床实践中通常使用的评分指南，并报告他们对每个病例​​对癌症的总体怀疑。然后，我们比较了读者的反馈结果，以衡量模型对他们的工作流程和决策的影响。分数和怀疑水平是根据个人的实际癌症结果来判断的，以衡量敏感性、特异性和&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and -auc#:~:text=AUC%20stands%20for%20%22Area%20under,across%20all%20possible%20classification%20thresholds。&quot;>;ROC 曲线下面积&lt;/a>; (AUC) 值。这些是在有帮助和没有帮助的情况下进行比较的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1y LUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPmwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s794/image3.png&quot; style=&quot;margin-左：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;297&quot; data-original-width=&quot;794&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFP mwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;多案例多读者研究涉及每个读者对每个案例进行两次审查，一次有机器学习系统协助，一次没有机器学习系统协助。在此可视化中，读者首先在没有帮助的情况下查看 A 组（&lt;strong>;蓝色&lt;/strong>;），然后在冲洗期后在有帮助的情况下查看（&lt;strong>;橙色&lt;/strong>;）。第二个读者组遵循相反的路径，首先在帮助下阅读同一组案例 A 集。读者被随机分配到这些组中，以消除排序的影响。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;使用相同界面进行这些研究的能力突出了其对完全不同的癌症评分的普遍性系统，以及模型和辅助能力对不同患者群体的推广。我们的研究结果表明，当放射科医生在临床评估中使用该系统时，他们在没有可操作的肺癌发现的情况下正确识别肺部图像的能力（即&lt;em>;特异性&lt;/em>;）比之前提高了绝对 5-7%当他们不使用辅助系统时。这可能意味着每筛查 15-20 名患者，就有可能避免不必要的后续程序，从而减轻他们的焦虑和医疗保健系统的负担。反过来，这可以帮助提高肺癌筛查计划的可持续性，特别是随着&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/34636916/&quot;>;更多人有资格接受筛查&lt;/a >;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8 ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoBKt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s1999/image2.png&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoB Kt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;在美国和日本的读者研究中，通过机器学习模型的帮助，读者的特异性得到了提高。特异性值是根据可操作的发现（发现可疑的东西）与没有可操作的发现的读者评分得出的，并与个体的真实癌症结果进行比较。在模型辅助下，读者标记出较少的癌症阴性个体进行后续访问。对癌症阳性个体的敏感性保持不变。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过合作将其转化为现实世界的影响 &lt;/h2>; &lt;p>; 该系统结果表明，有可能减少随访次数、减少焦虑，并降低肺癌筛查的总体成本。为了将这项研究转化为现实世界的临床影响，我们正在与：&lt;a href=&quot;https://deephealth.com/&quot;>;DeepHealth&lt;/a>;，一家领先的人工智能健康信息学提供商；与印度领先的放射学服务提供商 &lt;a href=&quot;https://apolloradiologyintl.com/&quot;>;Apollo Radiology International&lt;/a>; 合作，探索将该系统纳入未来产品的途径。此外，我们还希望通过 &lt;a href=&quot;https://github.com/Google-Health/google-health/tree/master/ct_dicom&quot;>; 帮助其他研究人员研究如何最好地将 ML 模型结果集成到临床工作流程中开源代码&lt;/a>;用于读者研究并结合本博客中描述的见解。我们希望这将有助于加速医学影像研究人员对其人工智能模型进行读者研究，并促进该领域的转化研究。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;该项目的主要贡献者包括 Corbin Cunningham、Zaid Nabulsi、Ryan Najafi、Jie Yang、Charles Lau、Joseph R. Ledsam、叶文兴、Diego Ardila、Scott M. McKinney、Rory Pilgrim、Hiroaki Saito、Yasuteru Shimamura、Mozziyar Etemadi、Yun Liu、David Melnick、Sunny Jansen、Nadia Harhen 、David P. Nadich、Mikhail Fomitchev、Ziyad Helali、Shabir Adeel、Greg S. Corrado、Lily Peng、Daniel Tse、Shravya Shetty、Shruthi Prabhakara、Neeral Beladia 和 Krish Eswaran。感谢 Arnav Agharwal 和 Andrew Sellergren 的开源支持，以及 Vivek Natarajan 和 Michael D. Howell 的反馈。还要衷心感谢放射科医生在整个研究过程中通过图像解释和注释工作实现了这项工作，以及 Jonny Wong 和 Carli Sampson 协调读者研究。&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt; /content>;&lt;link href=&quot;http://blog.research.google/feeds/7061041222399769838/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://www.blogger.com/feeds/8474926331452026626/posts/default/7061041222399769838&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/ 2024/03/computer-aided-diagnosis-for-lung.html&quot; rel=&quot;alternate&quot; title=&quot;肺癌筛查的计算机辅助诊断&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI &lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http: //schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author >;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD 7R8_tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s72-c/PULMA%20hero.jpg&quot;宽度=&quot; 72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-4615278636568583418&lt;/id>;&lt;发布>;2024-03-20T09:06:00.000-07:00&lt;/发布>;&lt;更新>;2024-03-20T09:06:06.7 53 -07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“环境”>;&lt;/类别>;&lt;类别方案=“http://www.blogger” .com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;利用人工智能扩大全球对可靠洪水预报的访问&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt; span class=&quot;byline-author&quot;>;发布者：Yossi Matias，工程副总裁研究，Grey Nearing，研究科学家，Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX-dCHpcYt nSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s320/洪水%20预测% 20hero%20image.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 洪水是&lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;最常见的自然灾害&lt;/ a>;，并造成全球每年大约&lt;a href=&quot;https://www.swissre.com/risk-knowledge/mitigating-climate-risk/floods.html&quot;>;500亿美元&lt;/a>;的经济损失。 &lt;a href=&quot;https://library.wmo.int/records/item/57630-2021-state-of-climate-services-water?offset=1#:~:text=WMO%2DNo.,1278&amp;amp; text=More%20than%202%20billion%20people,for%20the%20past%2020%20years.&quot;>;自 2000 年以来，与洪水相关的灾害发生率增加了一倍多&lt;/a>;&lt;a href=&quot;https: //www.nature.com/articles/s41598-020-70816-2&quot;>;由于气候变化&lt;/a>;。近 &lt;a href=&quot;https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content&quot;>;15 亿人&lt;/a>;，占 19%世界人口面临着严重洪水事件的巨大风险。升级预警系统，让这些人群能够获得准确、及时的信息&lt;a href=&quot;https://elibrary.worldbank.org/doi/abs/10.1596/1813-9450-6058&quot;>;每年可以挽救数千人的生命&lt; /a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在可靠的洪水预报对全球人民生活的潜在影响的推动下，我们于 2017 年开始了洪水预报工作。通过此 &lt;a href=&quot;https ://blog.google/technology/ai/google-ai-global-flood-forecasting/&quot;>;多年历程&lt;/a>;，多年来，我们在推进研究的同时建立了实时运营系统洪水预报系统，&lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;在 Google 搜索、地图、Android 通知和通过&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;。不过，为了&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;在全球范围内扩展&lt;/a>;，尤其是在某些地方在无法获得准确的当地数据的地方，需要取得更多的研究进展。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;未测量流域的极端洪水全球预测&lt;/a>;”中，发表在&lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;自然&lt;/a>;&lt;/em>;中，我们展示了机器学习 (ML) 技术如何显着改善全球范围&lt;a href= “https://sites.research.google/floodforecasting/&quot;>;洪水预报&lt;/a>;相对于洪水相关数据稀缺的国家当前最先进的技术。借助这些基于人工智能的技术，我们将当前可用的全球临近预报的可靠性平均从零天延长到五天，并将非洲和亚洲地区的预报改进为与欧洲当前可用的预报类似。模型的评估是与欧洲中期天气预报中心 (&lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;) 合作进行的。 &lt;/p>; &lt;p>; 这些技术还使&lt;a href=&quot;http://g.co/floodhub&quot;>;洪水中心&lt;/a>;能够提前 7 天提供实时河流预报，&lt;a href =&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;覆盖&lt;/a>; 80 多个国家的河流河段。人们、社区、政府和国际组织可以利用这些信息来采取预期行动，帮助保护弱势群体。 &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: Both; text-align: center;&quot;>;&lt;iframe allowedfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot;frameborder=&quot;0&quot; height=&quot;360 “ src=&quot;https://www.youtube.com/embed/ET04pDj-RvM?si=WJJXEtwJqtyMRuC_?rel=0&amp;amp;&quot; width=&quot;640&quot; youtube-src-id=&quot;[ET04pDj-RvM]&quot;>;&lt;/iframe>;&lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;Google 的洪水预报 &lt;/h2>; &lt;p>; 为 FloodHub 工具提供支持的机器学习模型是与多个合作伙伴（包括学术界、政府、国际组织和机构）合作多年研究的成果。非政府组织。 &lt;/p>; &lt;p>; 2018 年，我们&lt;a href=&quot;https://blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;启动了试点&lt; /a>; 印度恒河-雅鲁藏布江流域的早期预警系统，&lt;a href=&quot;https://arxiv.org/abs/1901.09583&quot;>;假设&lt;/a>;认为机器学习可以帮助解决具有挑战性的问题可靠的大规模洪水预报。第二年，该试点进一步&lt;a href=&quot;https://blog.google/technology/ai/tracking-our-progress-on-flood-forecasting/&quot;>;扩大&lt;/a>;&lt;a href=&quot;https: //ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;通过结合淹没模型、实时水位测量、创建高程图和水文建模。 &lt;/p>; &lt;p>; 在与学者的&lt;a href=&quot;https://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html&quot;>;合作&lt;/a>;中，特别是，我们与&lt;a href=&quot;https://www.jku.at/en/institute-for-machine-learning/&quot;>;JKU 机器学习研究所&lt;/a>;一起探索了基于机器学习的水文模型，表明基于&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;LSTM&lt;/a>;的模型可以&lt;a href=&quot;https://hess. copernicus.org/articles/23/5089/2019/&quot;>;比传统的概念性和基于物理的&lt;a href=&quot;https://en.wikipedia.org/wiki/Hydrological_model&quot;>;水文学产生更准确的模拟&lt;/a>;模型&lt;/a>;。这项研究带来了&lt;a href=&quot;https://blog.research.google/2020/09/the-technology-behind-our-recent.html&quot;>;洪水预报改进&lt;/a>;，实现了&lt;a href= &quot;https://blog.google/technology/ai/flood-forecasts-india-bangladesh/&quot;>;扩大&lt;/a>;我们的预测覆盖范围，将印度和孟加拉国全部覆盖。我们还与耶鲁大学的研究人员合作，测试可增加&lt;a href=&quot;https://egc.yale.edu/about/perspectives/pande-and-coauthors-using-technology-save-lives-during-印度季风季节&quot;>;洪水警报的范围和影响&lt;/a>;。 &lt;/p>; &lt;p>; 我们的水文模型通过处理降水和物理流域信息等公开的天气数据来预测河流洪水。此类模型必须根据来自各个河流的&lt;a href=&quot;https://en.wikipedia.org/wiki/Stream_gauge&quot;>;水流测量站&lt;/a>;的长数据记录进行校准。全球河流流域（流域）拥有流量计的比例较低，流量计价格昂贵，但却是提供相关数据所必需的，而且水文模拟和预报提供流量计具有挑战性&lt;a href=&quot;https://www.tandfonline.com/doi /full/10.1080/02626667.2013.803183&quot;>;缺乏此基础设施的流域的预测&lt;/a>;。 &lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;国内生产总值&lt;/a>; (GDP) 下降与&lt;a href=&quot;https:// www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;洪水风险的脆弱性&lt;/a>;，并且一个国家的国民生产总值与公开数据量之间存在负相关关系。机器学习通过允许&lt;a href=&quot;https://www.pnas.org/doi/full/10.1073/pnas.1414439112&quot;>;在所有可用河流数据上训练单个模型&lt;/a>;来帮助解决这个问题适用于&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;没有可用数据&lt;/a>;的未计量盆地。通过这种方式，模型可以在全球范围内进行训练，并且可以对任何河流位置进行预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZr bCKnROTNn_hmOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s1051/Streamflow%20data%20from%20the %20Global%20Runoff%20Data%20Center.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width= “1051”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCKnROTNn_h mOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s16000/Streamflow%20data%20from%20the%20Global%20Runoff%20Data%20Center.jpg&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;公开可用的流量之间存在逆（对数）相关性一个国家的数据和国民生产总值。来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;的水流数据。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的学术合作促成了机器学习研究，该研究开发了&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091&quot;>;估计河流预报的不确定性&lt;/ a>; 并展示了 ML 河流预测模型&lt;a href=&quot;https://hess.copernicus.org/articles/25/2685/2021/hess-25-2685-2021-relations.html&quot;>;如何从多个数据中合成信息来源&lt;/a>;。他们证明，这些模型可以&lt;a href=&quot;https://hess.copernicus.org/articles/26/3377/2022/hess-26-3377-2022.html&quot;>;可靠地模拟极端事件&lt;/a>;，甚至当这些事件不是训练数据的一部分时。为了&lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;为开放科学做出贡献&lt;/a>;，我们将在 2023 年开放-在 &lt;em>;&lt;a href=&quot;https://www.nature.com/articles/s41597-023-01975-w&quot;>;Nature Scientific Data&lt;/a>;&lt;/ 中获取了社区驱动的大样本水文学数据集嗯>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;河流预报模型&lt;/h2>; &lt;p>; 国家和国际机构使用的大多数水文模型洪水预报和河流建模是状态空间模型，仅取决于日常输入（例如降水、温度等）和系统的当前状态（例如土壤湿度、积雪等）。 LSTM 是状态空间模型的一种变体，通过定义表示单个时间步长的神经网络来工作，其中处理输入数据（例如当前天气状况）以生成该时间步长的更新状态信息和输出值（流） 。 LSTM 按顺序应用来进行时间序列预测，从这个意义上说，其行为类似于科学家通常概念化水​​文系统的方式。根据经验，我们发现 &lt;a href=&quot;https://hess.copernicus.org/articles/23/5089/2019/&quot;>;LSTM 在河流预测任务上表现良好&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xx rB87mupNTPpioKT0wRgSSc1FwYDmfCUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s960/image1.gif&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB87mupNTPpioKT0wRgSSc1FwYDmf CUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;LSTM 的示意图，LSTM 是一种按时间顺序运行的神经网络。可以在&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;>;此处&lt;/a>;找到易于理解的入门读物。&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 我们的河流预报模型使用两个连续应用的 LSTM：（1）“后报”LSTM 摄取截至当前时间（或者更确切地说，预报发布时间）的历史天气数据（动态后报特征） ），（2）“预测”LSTM 从后报 LSTM 中提取状态以及预测的天气数据（动态预测特征）来做出未来的预测。将一年的历史天气数据输入到后报 LSTM 中，将 7 天的预报天气数据输入到预报 LSTM 中。静态特征包括流域的地理和地球物理特征，这些特征被输入到后报和预测 LSTM 中，并允许模型学习各种类型流域的不同水文行为和响应。 &lt;/p>; &lt;p>; 预测 LSTM 的输出被输入到使用 &lt;a href=&quot;https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf 的“头”层&quot;>;混合密度网络&lt;/a>;产生概率预测（即，水流概率分布的预测参数）。具体来说，该模型在每次预测时都会预测重尾概率密度函数的混合参数，称为“非对称拉普拉斯分布”时间步。结果是一个混合密度函数，称为&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf&quot;>;非对称拉普拉斯的可数混合&lt;/a>;（ CMAL）分布，表示特定时间特定河流中体积流量的概率预测。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ 3dm8TRicy_wkTVtM4Xio_mhQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s960/LSTM-based%20river %20forecast%20model.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm8TRicy_wkTVtM4Xio_m hQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s16000/LSTM-based%20river%20forecast%20model.jpeg&quot;/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;基于 LSTM 的河流预报模型架构。按顺序应用两个 LSTM，一个摄取历史天气数据，一个摄取预测天气数据。模型输出是每个预测时间步长的水流概率分布参数。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot; >; &lt;br />; &lt;/div>; &lt;h2>;输入和训练数据&lt;/h2>; &lt;p>; 该模型使用三种类型的公开数据输入，大部分来自政府来源：&lt;/p>; &lt;ol>; &lt;li>;&lt; em>;代表地理和地球物理变量的静态流域属性：&lt;/em>;来自 &lt;a href=&quot;https://www.Hydrosheds.org/Hydroatlas&quot;>;HydroATLAS 项目&lt;/a>;，包括长期气候指数等数据（降水量、温度、积雪比例）、土地覆盖和人为属性（例如，作为人类发展指标的夜间灯光指数）。 &lt;/li>;&lt;li>;&lt;em>;历史气象时间序列数据&lt;/em>;：用于在预测发布之前一年启动模型。数据来自&lt;a href=&quot;https://gpm.nasa.gov/data/imerg&quot;>;NASA IMERG&lt;/a>;，&lt;a href=&quot;https://psl.noaa.gov/data/gridded/ data.cpc.globalprecip.html&quot;>;NOAA CPC 全球统一日降水量分析&lt;/a>;，以及 &lt;a href=&quot;https://cds.climate.copernicus.eu/cdsapp#!/dataset/ reanalysis-era5-land?tab=overview&quot;>;ECMWF ERA5-land 再分析&lt;/a>;。变量包括每日总降水量、气温、太阳和热辐射、降雪和表面压力。 &lt;/li>;&lt;li>;&lt;em>;7 天预测范围内的预测气象时间序列&lt;/em>;：用作预测 LSTM 的输入。这些数据与上面列出的气象变量相同，来自 &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/datasets/set-i&quot;>;ECMWF HRES 大气模型&lt;/a>;。 &lt;/li>; &lt;/ol>; &lt;p>; 训练数据是来自&lt;a href=&quot;https://www.bafg.de/GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;的每日流量值&lt; /a>; 1980 - 2023 年期间。使用来自 5,680 个不同流域水流测量仪（如下所示）的数据训练单个水流预测模型，以改进&lt;a href=&quot;https://eartharxiv.org/repository/view/6363 /&quot;>;准确度&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET 634isx6xzUswrYfDwp8oueoWJ7c3hf0os-RISANrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s1417/gauge_locations_map(1 ).jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;689&quot; data-original-width=&quot;1417&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3​​YF6L3BweAC0hhMkET634isx6xzUswrYfDwp8oueo WJ7c3hf0os-RIsaNrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s16000/gauge_locations_map(1).jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;5,680 个流量计的位置，为 &lt;a href=&quot;https://www.bafg.de/ 的河流预报模型提供训练数据GRDC/EN/Home/homepage_node.html&quot;>;全球径流数据中心&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40 %;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;改进当前最先进的技术&lt;/h2>; &lt;p>;我们将我们的河流预测模型与&lt;a href=&quot;https://www .globalfloods.eu/&quot;>;GloFAS 版本 4&lt;/a>;，当前最先进的全球洪水预报系统。这些实验表明，机器学习可以更早地针对规模更大、影响力更大的事件提供准确的警告。 &lt;/p>; &lt;p>; 下图显示了预测河流位置不同严重程度事件时&lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1分数&lt;/a>;的分布世界各地，精确度为正负 1 天。 F1 分数是精确度和召回率的平均值，事件严重性通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Return_period#:~:text=A%20return%20period%2C%20also%20known 来衡量,river%20discharge%20flows%20to%20发生。&quot;>;返回期&lt;/a>;。例如，2 年重现期事件是预计平均每两年超过一次的水流量。我们的模型在长达 4 天或 5 天的交付时间内实现了可靠性评分，平均而言，与 GloFAS 即时预报（0 天交付时间）的可靠性相似或更好。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe4 6W8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s3908/分布%20of%20F1%20分数%20over %202-year%20.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1844&quot; data-original-width=&quot;3908 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe46W8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s16000/Distributions%20of%20F1%20scores%20over%202-year%20.jpeg&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/F- Score&quot;>;F1 得分&lt;/a>; 2014-2023 年期间全球 2,092 个流域的 2 年重现期事件，由 GloFAS（&lt;strong>;蓝色&lt;/strong>;）和我们的模型（&lt;strong>;橙色&lt;/strong>; >;) 在不同的交货时间。平均而言，我们的模型在统计上与 2 年（显示）以及 1 年、5 年和 10 年事件（未显示）提前 5 天的 GloFAS 即时预报（0 天提前时间）一样准确.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外（未显示），我们的模型在更大和更罕见的极端事件中实现了准确性，在 5 年重现期事件中的精确度和召回率得分在 1 年重现期事件中，与 GloFAS 的精度相似或更好。有关更多信息，请参阅&lt;a href=&quot;https://www.nature.com/articles/s41586-024-07145-1&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;展望未来&lt;/h2>; &lt;p>;洪水预报计划是我们&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;适应和恢复能力工作&lt;/a>;并体现了 Google 的承诺&lt;a href=&quot;https:// /research.google/teams/climate-and-sustainability/&quot;>;应对气候变化&lt;/a>;，同时帮助全球社区增强抵御能力。我们相信人工智能和机器学习将继续在帮助推动气候行动科学研究方面发挥关键作用。 &lt;/p>; &lt;p>; 我们积极&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/4-flood-forecasting-collaboration-case-studies-show-how-ai-can-help-与有需要的社区/&quot;>;与多个国际援助组织（例如人道主义数据中心和红十字会）合作&lt;/a>;，提供可行的洪水预报。此外，与&lt;a href=&quot;https://wmo.int/&quot;>;世界气象组织&lt;/a>; (WMO) 持续合作&lt;a href=&quot;https://blog.google/outreach-initiatives /sustainability/early-warning-system-wmo-google/&quot;>;支持气候灾害预警系统&lt;/a>;，我们正在进行一项研究，以帮助了解人工智能如何帮助解决国家洪水预报机构面临的现实挑战。 &lt;/p>; &lt;p>; 虽然这里介绍的工作表明洪水预报向前迈出了重要一步，但未来的工作还需要进一步将洪水预报覆盖范围扩大到全球更多地点以及其他类型的洪水相关事件和灾害，包括山洪和灾害。城市洪水。我们期待与学术界和专家界、地方政府和业界的合作伙伴继续合作，以实现这些目标。 &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4615278636568583418/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4615278636568583418&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html&quot; rel=&quot;alternate&quot; title=&quot;利用人工智能扩大全球获得可靠洪水预报的机会&quot; type= &quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif” width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX- dCHpcYtnSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s72-c/洪水%20forecasting%20hero%2 0image.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-520087429457973735&lt;/id>;&lt;已发布>;2024-03-19T13:15:00.000- 07:00&lt;/发布>;&lt;更新>;2024-03-19T13:15:33.664-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语= HCI&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;自我监督学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category >;&lt;title type=&quot;text&quot;>;ScreenAI：用于 UI 和视觉情境语言理解的视觉语言模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Srinivas Sunkara 和Gilles Baechler, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s320/ScreenAI%20-%20hero. jpeg“ style =” display：none;” />; &lt;p>;屏幕用户界面（UIS）和信息图表，例如图表，图表和表格，在促进丰富且交互式用户体验的过程中在人类通信和人机相互作用中起重要作用。 UIS和Intographics共享类似的设计原理和视觉语言（例如，图标和布局），提供了一个机会，可以构建一个可以理解，推理和与这些接口交互的单个模型。但是，由于其复杂性和多样化的演示格式，信息图表和UI提出了独特的建模挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>; &lt;p>;为此，我们介绍“ &lt;a href=&quot;https://arxiv.org/abs/2402.04615&quot;>; screenai：viewer-Language UI和信息图表理解的模型&lt;/a>;”。 Screenai改进了&lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>; pali Architecture &lt;/a>; &lt;/a>;带有来自&lt;a href =的灵活补丁策略“>; pix2struct &lt;/a>;。我们将ScreenAi训练数据集和任务的独特混合物，包括一个新颖的屏幕注释任务，该任务需要模型在屏幕上识别UI元素信息（即，类型，位置和描述）。这些文本注释提供了具有屏幕描述的大型语言模型（LLMS），从而使它们能够自动生成问答（QA），UI导航和摘要培训数据集。在仅5B参数时，Screenai在基于UI和信息图的任务上实现了最新的结果（&lt;a href=&quot;https://x-lance.github.io/websrc/&quot;>; websrc &lt;/a>;和&lt;a href=&quot;https://github.com/aburns4/motif&quot;>; Motif &lt;/a>;），以及&lt;a href =“ https://github.com/vis-nlp on https：href =” /chartqa“>;图表QA &lt;/a>;，&lt;a href=&quot;https://rrc.cvc.uab.es/?ch=17&amp;comp； comp; com = evaluation＆amp; amp.task = 1&quot;>; docvqa &lt;/a>;，和&lt;与类似大小的模型相比，href =“ https://arxiv.org/abs/2104.12756”>; Infocricavevqa &lt;/a>;。我们还将发布三个新数据集：&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readmereadmereadmereadmereadmereadmereadme-file#screen-antotation-annotation-natotation-dataset-dataset-dataset-details&quot;>;屏幕注释&lt;/ a>;评估模型的布局理解能力，以及&lt;a href =“ https://github.com/google-research-datasets/screen_qa/tree/main/main？目录“>; screenqa short &lt;/a>;和&lt;a href=&quot;https://github.com/google-research-datasets/screen_qa?tab=readme-eadme-ov-file#complexqa“target=&quot;_blank&quot;_blank&quot;_blank&quot;>; /a>;以更全面地评估其质量检查能力。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>; screenai &lt;/h2>; &lt;p>; screenai的体系结构基于&lt;a href =“ https：/https：/ /arxiv.org/abs/2209.06794&quot;>; pali &lt;/a>;由多模式编码器块和自动回归解码器组成。 pali编码器使用&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>; Vision Transformer &lt;/a>;（VIT）创建图像嵌入式图像和多模式编码器，该编码器采用图像和文本嵌入的一致性作为输入。这种灵活的体系结构使ScreenAI可以解决可以重铸为文本+图像到文本问题的视觉任务。 &lt;/p>; &lt;p>;在帕利体系结构的顶部，我们采用了Pix2struct中引入的灵活修补策略。选择网格尺寸而不是使用固定网格模式，以便它们保留输入图像的天然纵横比。这使ScreenAi可以在各种纵横比的图像上良好工作。 &lt;/p>; &lt;p>; ScreenAI模型分为两个阶段：一个预训练阶段，然后是微调阶段。首先，将自我监督的学习应用于自动生成数据标签，然后将其用于训练VIT和语言模型。 VIT在微调阶段被冷冻，其中大多数使用的数据由人类评估者手动标记。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s1600/image6.gif&quot; style =“ Margin-Left：auto; Margin-Right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 583” data-Original-width =“ 1600” src =“ https：// blogger。 googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “ text-align：中心;”>; screenai模型体系结构。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br/ >; &lt;/div>; &lt;h2>;数据生成&lt;/h2>; &lt;p>;要为ScreenAI创建预训练数据集，我们首先收集了来自各种设备的大量屏幕截图，包括台式机，移动和平板电脑。这是通过使用&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot; target=&quot;_blank&quot;>;公开访问的网页&lt;/a>;并遵循用于&lt;a href = &lt;a href =“ https://dl.acm.org/doi/10.1145/3126594.3126651“ target =“ _ blank”>; rico数据集&lt;/a>;用于移动应用程序。然后，我们基于&lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; target=&quot;_blank&quot;>; detr &lt;/a>;型号，该型号&lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot; &lt;a href=&quot;https://arxiv.org/abs/2005.12872 &lt;/a>;型号（例如，图像，象形图，按钮，文本）及其空间关系。象形图使用&lt;a href=&quot;https://arxiv.org/abs/2210.026663&quot; target=&quot;_blank&quot;>;图标分类器&lt;/a>;能够区分77种不同的图标类型。这种详细的分类对于解释通过图标传达的微妙信息至关重要。对于分类器不涵盖的图标以及信息图表和图像，我们使用Pali Image字幕模型来生成提供上下文信息的描述性字幕。我们还应用&lt;a href=&quot;https://cloud.google.com/use-cases/ocr&quot; target=&quot;_blank&quot;>;光学角色识别&lt;/a>;（OCR）引擎在屏幕上提取和注释文本内容。我们将OCR文本与先前的注释相结合，以创建每个屏幕的详细说明。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s1747/image2.png&quot; style=&quot;左键：自动右右：auto;“>; &lt;img border =“ 0” data-forminal-height =“ 1055” data-original-width =“ 1747” src =“ https：//blogger.googleusercontent。 com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align ：中心;“>;移动应用屏幕截图，带有生成的注释，其中包括UI元素及其描述，例如，&lt;code>; text &lt;/code>;元素还包含来自OCR的文本内容，&lt;code>; image &lt;/code &lt;/code &lt;/code &lt;/code>;元素包含图像字幕，&lt;code>; list_items &lt;/code>;包含其所有子元素。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h3>;基于llm的数据生成&lt;/h3>; &lt;p>;我们使用&lt;a href =“ https://blog.google/technology/google”来增强培训数据的多样性-palm-2-ai-large-Lange-Model/“>; PALM 2 &lt;/a>;在两步过程中生成输入输出对。首先，使用上面概述的技术生成屏幕注释，然后我们围绕该架构制定提示，以为LLM创建合成数据。此过程需要及时的工程和迭代精炼才能找到有效的提示。我们通过针对质量阈值进行人体验证来评估生成的数据的质量。态只会说json。不要写不是JSON的文字。给您以下移动屏幕截图，用文字描述。您能否产生有关屏幕快照的内容以及对它们的相应简短答案的5个问题？答案应尽可能短，仅包含必要的信息。您的答案应如下：问题：[{{问题：问题，答案：答案}}，...] {屏幕架构} &lt;/font>; &lt;/pre>; &lt;br />; &lt;br />; &lt;br />; &lt;table align = “中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto;“>; &lt;tbody>; &lt;tbody>; &lt;tr>; &lt;tr>; &lt;td class =” tr-caption“ style =” text-align：center;“>; QA数据生成的示例提示。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;通过组合LLMS的自然语言功能使用结构化的模式，我们模拟了广泛的用户交互和方案，以生成合成，现实的任务。特别是，我们生成三类任务：&lt;/p>; &lt;ul>; &lt;li>; &lt;strong>;问题回答&lt;/strong>;：要求该模型回答有关屏幕截图内容的问题，例如，“什么时候餐厅开放？” &lt;/li>; &lt;li>; &lt;strong>;屏幕导航&lt;/strong>;：要求该模型将自然语言转换为屏幕上的可执行动作，例如，“单击搜索按钮”。 &lt;/li>; &lt;li>; &lt;strong>;屏幕摘要&lt;/strong>;：要求该模型在一个或两个句子中汇总屏幕内容。 &lt;/li>; &lt;/ul>; &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;&#39;&#39; >;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHdEc-JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7 /S1398/image3.png“ style =”保证金 - 左：自动右：自动; =” 4R3V4C-HESNHDEC-JUUX31ZZMDHDDDHDDDDDD0WO6IBT7QBZFAYN_YX1MYH77K-ROO9FJD33SILNP0JLNJLNJLNJLNJLNJLNJLNJLNJLNDEHBSR7/s16000/s16000/s16000/Image imag.3.png >; &lt;td class =“ tr-caption” style =“ text-align：center;”>;我们的工作流的框图，用于使用现有的ScreenAI模型和LLMS生成QA，摘要和导航任务的数据。每个任务都使用自定义提示来强调所需方面，例如与计数，涉及推理等有关的问题，等等&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;br />; &lt;table align =“中心” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container” style =“ margin-left：auto; margin-right：auto;”>; &lt;tbody>; &lt;trbody>; &lt;tr>; &lt;tr>; &lt;td style =“ text-align：text-align： center;&quot;>;&lt;img height=&quot;540&quot; src=&quot;https://lh7-us.googleusercontent.com/LmUtXBMXK-zy_rMShHQ_Hk4vQeXu2Kpx8zfzjhE3uAREczbkbGTEjZ7OMTbqtB37lD4rF31xJsoWdVXNAXLbbM1Uc_01WZWmOfBg9RwyAUEToPpa1W38Pt117Zj5LrNfnxXqjXoAJDZd-zcAIgU4QSoBaAKsIrSi8_POI14F5hguN1NJL9a2RsrKg6WHz7w&quot; style=&quot;margin-left: auto; margin-right: auto; margin-top ：0px;” width =“ 705”/>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; llm生成的数据。屏幕质量检查，导航和摘要的示例。对于导航，操作边界框在屏幕截图上以红色显示。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt; BR/>; &lt;/div>; &lt;h2>;实验和结果&lt;/h2>; &lt;p>;如前所述，ScreenAI分为两个阶段：预训练和微调。培训前数据标签是使用自我监督学习获得的，微调数据标签来自人类评估者。 &lt;/p>; &lt;p>;我们使用公共质量检查，摘要和导航数据集以及与UIS相关的各种任务进行微调。对于QA，我们在多模式和文档理解字段中使用良好的基准，例如&lt;a href=&quot;https://github.com/vis-nlp/chartqa&quot;>; charticqa &lt;/a>;，&lt;a href =“ https ：//rrc.cvc.uab.es/？ch = 17＆amp; com =评估＆amp; task = 1“>; docvqa &lt;/a>;，&lt;a href =” = 17＆amp; com =任务“>;多页docvqa &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>; infocricovqa &lt;/a>;，&lt;a href =“ -vqa.github.io/&quot;>; ocr vqa &lt;/a>;，&lt;a href=&quot;https://x-lance.github.io/websrc/&quot;>; web src &lt;/a>;和&lt;a href =“ https ：//github.com/google-research-datasets/screen_qa“>; screenqa &lt;/a>;。对于导航，使用的数据集包括&lt;a href=&quot;https://github.com/google-research-datasets/uibert/uibert/tree/main&quot;>; referring表达式&lt;/a>;，&lt;a href =“ https：// github。 com/aburns4/motif“>;主题&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2209.15099 &quot;>; mug &lt;/a>;和&lt;a href =” https://github.com /google-Research/google-research/tree/master/android_in_the_wild>; android在野外&lt;/a>;。最后，我们使用&lt;a href=&quot;https://github.com/google-research-datasets/screen2words&quot;>; screen2words &lt;/a>;用于屏幕摘要和&lt;a href =“ https://paperswithcode.com/paper/paper/小部件限制生成 - 生成自然语言/评论/“>;窗口小部件字幕&lt;/a>;用于描述特定的UI元素。除了微调数据集外，我们还使用三个新颖的基准评估了微调的ScreenAI模型：&lt;/p>; &lt;ol>; &lt;li>;屏幕注释：启用评估模型布局注释和空间理解功能。 &lt;/li>; &lt;li>; ScreenQA简短：ScreenQA的一种变体，其基础真理答案已缩短，仅包含与其他质量检查任务相符的相关信息。 &lt;/li>; &lt;li>;复杂的screenqa：补充ScreenQA简短，带有更困难的问题（计数，算术，比较和不可吻合的问题），并包含具有各种纵横比的屏幕。 &lt;/li>; &lt;/ol>; &lt;p>;微调的Screenai模型在各种基于UI和信息图的任务上获得了最新的结果（&lt;a href =“ https：//x-lance.github。 io/webRC/“>; webRC &lt;/a>;和&lt;a href=&quot;https://github.com/aburns4/motif&quot;>; Motif &lt;/a>;）和&lt;a href =的最佳表现：//github.com/vis-nlp/chartqa“>;图表qa &lt;/a>;，&lt;a href =” https://rrc.cvc.uab.es/?ch=17＆comp = evaluation＆comptulation＆amp = evaluation＆amp; “>; docvqa &lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>; InfographicVQA &lt;/a>;与大小相似的模型相比。 Screenai在Screen2Words和OCR-VQA上实现了竞争性能。此外，我们报告了引入的新基准数据集的结果，以作为进一步研究的基准。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s1183/image2.png “ style =”保证金左：自动右右：自动;”>; &lt;img border =“ 0” data-original-height =“ 1137” data-original-width =“ 1183” blogger.googleusercontent.com/img/b/r29vz2xl/avvxseijjaw824ldvbrfu3c7oerx9ik9ik86dwnuq2nqlilpuzlpuzlpuzlpuzlpuzkkkkkkkkkzzzzsswxswsswsswsswsswsswsswsmfyoswns gwjrdsccj3umxtllya tllya fbnpydws1ya2dhdeyfihwl1mvcytwizdgfblxawoxukwwwwww1vllwfnwmnkq64b8wum5slnkgegdgxxlr7/s16000/s16000/s16000/image2.png2.png“/>; &lt;/a>; &lt;/a>; &lt;/a>; &lt;/a>;字幕“ style =” text-align：中心;“>;将ScreenAi的模型性能与类似大小的最新模型（SOTA）进行比较。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>;接下来，我们检查了Screenai的缩放功能，并观察到在所有任务中，增加模型尺寸可改善性能，并且改进的尺寸尚未在最大尺寸的情况下饱和。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style =“ text-align：center;”>; &lt;a href =“ https://blogger.googleusercontent.com/img/r29vz2xl/avvz2xl/avvxsejknmvtyz1rhm0wqgn7egbbb9lev3yuhrcamjt3 sbnrcamjt3ii _9 az.1girchhhrcamjirjiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagiagir g2fxrfnziiwssi_ynqgwkgyo6ykaw05cfl9oys869f7dmyjcthlj6c0clwzmagp8hm9mxdck92dckk92d4pl24pl2ujz-tippl2ujz-ti4ppl2ujz-ti4czsqolzsqolzsqolzlecmlgelwbjl9fbjl9fbjl9fbjl9fbjl9fbjl9fbjtjkwip1。 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ TR-CAPTION”样式=“ Text-Align：Center;”>;模型性能随大小增加而增加，即使在5B参数的最大尺寸下，性能也没有饱和。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>;我们与统一一起介绍了Screenai模型使我们能够开发从所有这些域中利用数据的自我监督的学习任务的表示。我们还说明了使用LLM的数据生成的影响，并通过修改训练混合物来研究改善模型性能在特定方面。我们应用所有这些技术来构建经过多任务训练的模型，以在许多公共基准上采用最先进的方法来竞争性能。但是，我们还注意到，我们的方法仍然落后于大型模型，需要进一步的研究来弥合这一差距。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; Wang，Fedir Zubach，Hassan Mansoor，Vincent Etter，Victor Carbune，Jason Lin，Jindong Chen和Abhanshu Sharma。我们感谢Fangyu Liu，Xi Chen，Efi Kokiopoulou，Jesse Berent，Gabriel Barcik，Lukas Zilka，Oriana Riva，Gang Li，Yang Li，Radu Soricut，Radu Soricut和Tania Bedrax-Weiss以及Rahul Aralikatte，Hao，Hao，Hao，以及RAHULAX-WEIS Cheng和Daniel Kim在数据准备方面的支持。我们还要感谢Jay Yagnik，Blaise Aguera Y Arcas，Ewa Dominowska，David Petrou和Matt Sharifi的领导，愿景和支持。我们非常感激能够帮助我们在这篇文章中创建动画。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/5200874294579797373735/comments/comments/comments/comments/default “ rel =”回复“ title =” post注释“ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/screenai-viseal-visual-visual-language-model--model-- for-ui.html＃comment-form“ rel =” reply =“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http:/ http://www.blogger.com/feeds/847492626331452026262626262626262626/posts /default/520087429457973735“ rel =“ edit” type =“ application/atom+xml”/>; &lt;link href =” “ type =” application/atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/screenai-visual-visual-visual-visual-language-model-model-for-ui.html“ title =“ screenai：UI和视觉上的语言理解的视觉语言模型” type =“ text/html”/>; &lt;aunder>; &lt;names>; google ai &lt;/name>; &lt;uri>; http：//www.blogger。 com/profile/120986265147775266161 &lt;/uri>; &lt;email>; noreply@blogger.com &lt;/email>; &lt;gd：image height =“ 16” =“ https://img1.blogblog.com/img/b16-rounded.gif” width =“ 16”>; &lt;/gd：image>; &lt;/furet>; &lt;媒体：thumbnail height =“ 72” url =“ url =” https：https：https：https：https：https：https： //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s72-c/ScreenAI%20-%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search 。 -4328167517765145678 &lt;/id>; &lt;/id>; &lt;出版>; 2024-03-19T08：00：00.000-07：00 &lt;/00 &lt;/publined>; &lt;更新>; 2024-03-19T08：00. 00：00.150-07：00.150-07：00 &lt;/opect>; &lt;/00 &lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/ “ http://www.blogger.com/atom/ns#” term =“ crowd-sourcing”>; &lt;/category>; &lt;category scheme =“ http://www.blogger.com/atom/ns#” term = “数据集”>; &lt;/category>; &lt;类别方案=“ http://www.blogger.com/atom/ns#” term =“多样性”>; &lt;/category>; &lt;类别>; .com/atom/ns＃“ term =“ health”>; &lt;/category>; &lt;title type =“ text”>; scin：代表性皮肤病学图像的新资源&lt;/stitle>; &lt;content type =“ html”>; =&quot;byline-author&quot;>;Posted by Pooja Rao, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s1600/ scinhero.png“ style =” display：none;” />; &lt;p>;健康数据集在研究和医学教育中起着至关重要的作用，但是创建代表现实世界的数据集可能会很具有挑战性。例如，皮肤病学条件的外观和严重程度各不相同，并且在肤色之间表现出不同的表现。然而，现有的皮肤病学图像数据集通常缺乏日常状况（例如皮疹，过敏和感染）的代表，并且偏向更轻的肤色。此外，种族和种族信息经常缺少，阻碍了我们评估差异或创建解决方案的能力。 &lt;/p>; &lt;a name=&#39;more&#39;>; &lt;/a>; &lt;p>;要解决这些限制，我们正在发布&lt;a href=&quot;https://github.com/google-research-research-datasets/scin&quot;>;皮肤状况图像网络（SCIN）数据集&lt;/a>;与医生与&lt;a href=&quot;https://med.stanford.edu/&quot;>; Stanford Medicine合作。我们设计了SCIN，以反映人们在线搜索的广泛关注，并补充了临床数据集中通常发现的条件类型。它包含各种肤色和身体部位的图像，有助于确保未来的AI工具有效地适用于所有人。我们已经制作了&lt;a href=&quot;https://github.com/google-research-datasets/scin&quot;>; SCIN数据集&lt;/a>;作为研究人员，教育者和开发人员，以及以及采取仔细的步骤来保护贡献者的隐私。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s1327/image1.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 1118” data-Original-width =“ 1327” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：center;“>; scin数据集中的图像和元数据示例。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;div style =” line-height： 40％;“>; &lt;br>; &lt;/div>; &lt;h2>;数据集组成&lt;/h2>; &lt;p>; SCIN数据集当前包含10,000多张皮肤，指甲或头发状况，这是由经历它们的个人直接贡献的。根据机构审查委员会批准的研究，在美国个人的知情同意下，自愿做出了所有贡献。为了提供回顾性皮肤科医生标签的背景，要求贡献者拍摄特写图像，并从稍微远离拍摄图像。他们可以选择自我报告人口统计信息和&lt;a href=&quot;https://en.wikipedia.org/wiki/fitzpatrick_scale&quot;>; tanning Propents &lt;/a>;（自我报告的Fitzpatrick皮肤类型，IE，IE，SFST） ，并描述与它们的关注有关的质地，持续时间和症状。 &lt;/p>; &lt;p>;一到三个皮肤科医生标记了每个贡献，最多五个皮肤病学条件，以及每个标签的置信度评分。 SCIN数据集包含这些单独的标签，以及从中得出的聚集和加权鉴别诊断，可用于模型测试或训练。这些标签是追溯分配的，并不等于临床诊断，但是它们使我们能够将SCIN数据集中皮肤病条件的分布与现有数据集进行比较。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s1851/image2.png&quot; imageanchor =“ 1” style =“边距左：自动; margin-right：auto;”>; &lt;img border =“ 0” data-Original-height =“ 775” data-Original-width =“ 1851” src =“ src =” https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption“ style =” text-align：中心;“>; SCIN数据集在很大程度上包含过敏，炎症和传染性状况，而来自临床来源的数据集则集中于良性和恶性&lt;a href &lt;a href =“ https:///en.wikipedia.orgg/ wiki/delasm“>;肿瘤&lt;/a>;。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/take>; &lt;p>;，而许多现有的皮肤病学数据集则集中于恶性肿瘤和良性肿瘤，并旨在帮助皮肤癌诊断，SCIN数据集主要由常见的过敏，炎症和感染状况组成。 SCIN数据集中的大多数图像都显示出早期的问题 - 超过一半的照片在照片前不到一周，而30％的出现在拍摄图像之前的一天不到一天。在卫生系统中很少看到此时间窗口中的条件，因此在现有的皮肤病学数据集中的代表性不足。 &lt;/p>; &lt;p>;我们还获得了菲茨帕特里克皮肤类型（估计的FST或EFST）和外行标记估计&lt;a href=&quot;htttps:///en.wikipedia.org/wiki/wiki/monk_skin_skin_scale &quot;>; monk蒙克肤色&quot;>; monk_skin_scale&quot;>; &lt;/a>;（emst）图像。这样可以将皮肤状况和皮肤类型分布与现有皮肤病学数据集的患者进行比较。尽管我们没有选择性地瞄准任何皮肤类型或肤色，但与临床来源的类似数据集相比，SCIN数据集具有平衡的Fitzpatrick皮肤类型分布（3、4、5和6类型）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s1851 /image3.png“ ImageAnchor =“ 1”样式=“边距左：自动右：自动;”>; &lt;img border =“ 0” data-original-height =“ 620” data-original-width =“ 1851&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;自我报告和皮肤科医生估计的scin数据集中的fitzpatrick皮肤类型分布与现有的未增强的皮肤病数据集相比a href =“ https://github.com/mattgroh/fitzpatrick17k”>;（fitzpatrick17k17k &lt;/a>;，&lt;a href=&quot;https://wwwww.fc.up.ppt.ppt.pt.pt/addi/addi/addi/addi/ph2%20database.htmll>; ph²&lt;/a>;，&lt;a href=&quot;https://www.it.pt/automaticpage?id=3459&quot;>; skinl2 &lt;/a>;和&lt;a href =“ https：//www.ncbi.nlm。 nih.gov/pmc/articles/pmc7479321/“>; pad-ufes-20 &lt;/a>;）。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;p>; &lt;a href =“ href =” https：https： //en.wikipedia.org/wiki/fitzpatrick_scale&quot;>; fitzpatrick皮肤类型&lt;/a>;量表最初是作为照相量表开发的，用于测量皮肤类型对UV辐射的响应，并且在皮肤病学研究中广泛使用。和尚肤色量表是一种较新的10遮盖量表，可测量肤色而不是皮肤色谱，从而捕捉到较深的肤色之间的差异更细微的差异。虽然这两种量表都旨在使用图像进行回顾性估算，但这些标签的包含旨在使未来对皮肤病学中皮肤类型和音调表示的研究。例如，SCIN数据集为美国人群中这些皮肤类型和色调的分布提供了初始基准。 &lt;/p>; &lt;p>; SCIN数据集对女性和年轻人的表现很高，可能反映了各种因素的组合。这些可能包括皮肤状况发生率的差异，在线寻求健康信息的倾向以及愿意为跨人口统计研究做出贡献的差异。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>;众包方法&lt;/h2>; &lt;p>;为了创建SCIN数据集，我们使用了一种新颖的众库方法，我们在随附的&lt;a href=&quot;https://arxiv.org/abs/2402.18545&quot;>;研究论文&lt;/a>;与调查人员合作，&lt;a href =“ https://med.stanford.edu /“>;斯坦福医学&lt;/a>;。这种方法使个人能够在医疗研究中发挥积极作用。它使我们能够在他们的健康问题的早期阶段接触到他们寻求正式护理之前。至关重要的是，此方法使用网络搜索结果页面上的广告（许多人的健康之旅的起点）与参与者建立联系。 &lt;/p>; &lt;p>;我们的结果表明，众包可以产生低垃圾邮件速率的高质量数据集。超过97.5％的贡献是皮肤状况的真实图像。在执行了进一步的过滤步骤以排除SCIN数据集范围不超出范围并删除重复项的图像之后，我们能够释放在8个月的研究期内获得的近90％的贡献。大多数图像敏锐而曝光良好。大约一半的贡献包括自我报告的人口统计，80％包含与皮肤状况有关的自我报告的信息，例如纹理，持续时间或其他症状。我们发现，皮肤科医生回顾性分配鉴别诊断的能力更多地取决于自我报告的信息的可用性，而不是图像质量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s1999/image4.png&quot; imageanchor =“ 1” style =“边距左：auto; margin-right：auto;”>; &lt;img border =“ 0” data-eriginal-height =“ 1610” data-Original-width =“ 1999” src =“ https ：//blogger.googleusercontent.com/img/b/r29vz2xl/avvxsej1qmrinpok_qmh5jjtktgqytbbqytbrfhwdfxlbbrfhwdfbbrfhwdfhwdfzy9l_jg8l_jgbkbkbkbkkkxa ue8ojxa7qwtgy7qwtgy76gpkpkpksw5efbjqu5pk5pk5pplxp3plipp3 tlihucp3p3phip3p3phip3phip3phip3phio oc.plio oc.fipphio ococ t.pphio ocococo oc.plio og t ZZAB6FDUFHR-WAW1M81CAMBQXMPIZSN8P3VYLYS8B9CCZZOZOZI- VB9D1NWZKK8NCNPTSCDWWH1FMEF4Q8DDJHO6DR/s16000/s16000/s16000/image 4.png.4.png“/>; &lt;/a>; -caption“ style =” text-align：中心;“>;皮肤科医生对标签的信心（比例为1-5）取决于自我报告的人口统计和症状信息的可用性。&lt;/td>; &lt;/td>; &lt;/tr>; &lt;/tr>; &lt;/tbody >; &lt;/table>; &lt;p>;虽然无法保证完美的图像去识别，但保护贡献图像的个人的隐私是创建SCIN数据集时的重中之重。通过知情同意书，贡献者知道潜在的重新识别风险，并建议避免将图像上载具有识别功能。提交后隐私保护措施包括手动修订或裁剪，以排除潜在的识别区域，反向图像搜索以排除公开可用的副本以及去除元数据或汇总。 scin &lt;a href=&quot;https://github.com/google-research-datasets/scin?tab=license-1-ov-file#readme&quot;>;数据使用许可证&lt;/a>;禁止尝试重新识别贡献者的尝试。 &lt;/p>; &lt;p>;我们希望SCIN数据集将是那些致力于推进包容性皮肤病学研究，教育和AI工具开发的人的有用资源。通过演示传统数据集创建方法的替代方案，Scin在自我报告的数据或回顾性标签是可行的领域为更多代表性的数据集铺平了道路。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br>; &lt;/div>; &lt;h2>; bensentedgments &lt;/h2>; &lt;p>; &lt;em>;我们感谢所有的共同作者Abbi Ward ，吉米·李（Jimmy Li），朱莉·王（Julie Wang），斯里拉姆·拉克斯米纳（Sriram Lakshminarasimhan），阿什利·卡里克（Ashley Carrick），比尔森·坎帕纳（Bilson Campana），杰伊·哈特福德（Jay Hartford），杰伊·哈特福德（Jay Hartford），普拉德普·库玛（Pradeep Kumar），蒂亚·蒂亚西里索（Tiya Tiyasirisokchai），桑尼·维尔曼尼（Sunny Virmani ），史蒂文·林（Steven Lin）（斯坦福医学），贾斯汀·科（Stanford Ko）（斯坦福医学），艾伦·卡尔西卡扬灵（Alan Karthikesalingam）和克里斯托弗·苏格尔（Christopher Spers）。我们还要感谢Yetunde Ibitoye，Sami Lachgar，Lisa Lehmann，Javier Perez，Margaret Ann Smith（Stanford Medicine），Rachelle Sico，Amit Talreja，Annisah Um&#39;rani和Wayne Westerlind对这项工作的重要贡献。最后，我们感谢Heather Cole-Lewis，Naama Hammel，Ivor Horn，Michael Howell，Yun Liu和Eric Teasley对研究设计和手稿的见解。 &lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =“ http://blog.research.google/feeds/43281675177765145678/comments/comments/default” /atom+xml“/>; &lt;link href =” http://blog.research.google/2024/03/scin-new-resource-for-representative.html#comment-form“ 0注释“ type =” text/html“/>; &lt;link href =” http://www.blogger.com/feeds/8474926331452026626/posts/posts/posts/default/43281675177777777765145145678 “/>; &lt;link href =” http://www.blogger.com/feeds/847492633145202626/ ：//blog.research.google/2024/03/scin-new-resource-for-merpresentative.html“ rel =“替代” title =“ scin：代表性皮肤病学图像的新资源”类型=“ text/html” />; &lt;ause>; &lt;name>; Google AI &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147775266161 &lt;/uri>; &lt;Email>; noreply@blogger.com &lt;/emaglogger.com &lt;/email>; height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” src =“ https://img1.blogblog.com/img/img/b16-round.gif.gif” >;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s72-c/SCINHero .png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt; entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-757976001746578714&lt;/id>;&lt;published>;2024-03-18T11:41:00.000-07:00&lt;/published>;&lt;updated>;2024-03 -18T12:01:42.865-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot; http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MELON: Reconstructing 3D objects from images with unknown poses&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mark Matthews, Senior Software Engineer, and Dmitry Lagun, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s320/MELON%20HERO.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; A person&#39;s prior experience and understanding of the world generally enables them to easily infer what an object looks like in whole, even if only looking at a few 2D pictures of it. Yet the capacity for a computer to reconstruct the shape of an object in 3D given only a few images has remained a difficult algorithmic problem for years. This fundamental computer vision task has applications ranging from the creation of e-commerce 3D models to autonomous vehicle navigation. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A key part of the problem is how to determine the exact positions from which images were taken, known as &lt;em>;pose inference&lt;/em>;. If camera poses are known, a range of successful techniques — such as &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;neural radiance fields&lt;/a>; (NeRF) or &lt;a href=&quot;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&quot;>;3D Gaussian Splatting&lt;/a>; — can reconstruct an object in 3D. But if these poses are not available, then we face a difficult “chicken and egg” problem where we could determine the poses if we knew the 3D object, but we can&#39;t reconstruct the 3D object until we know the camera poses. The problem is made harder by pseudo-symmetries — ie, many objects look similar when viewed from different angles. For example, square objects like a chair tend to look similar every 90° rotation. Pseudo-symmetries of an object can be revealed by rendering it on a turntable from various angles and plotting its photometric &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-similarity&quot;>;self-similarity&lt;/a>; map. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s1764/image5.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;923&quot; data-original-width=&quot;1764&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Self-Similarity map of a toy truck model. &lt;strong>;Left:&lt;/strong>; The model is rendered on a turntable from various &lt;a href=&quot;https://en.wikipedia.org/wiki/Azimuth&quot;>;azimuthal angles&lt;/a>;, θ. &lt;strong>;Right:&lt;/strong>; The average &lt;a href=&quot;https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm&quot;>;L2&lt;/a>; RGB similarity of a rendering from θ with that of θ*. The pseudo-similarities are indicated by the dashed red lines.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The diagram above only visualizes one dimension of rotation. It becomes even more complex (and difficult to visualize) when introducing more degrees of freedom. Pseudo-symmetries make the problem &lt;em>;ill-posed&lt;/em>;, with naïve approaches often converging to local minima. In practice, such an approach might mistake the back view as the front view of an object, because they share a similar silhouette. Previous techniques (such as &lt;a href=&quot;https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/&quot;>;BARF&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2205.15768&quot;>;SAMURAI&lt;/a>;) side-step this problem by relying on an initial pose estimate that starts close to the global minima. But how can we approach this if those aren&#39;t available? &lt;/p>; &lt;p>; Methods, such as &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_GNeRF_GAN-Based_Neural_Radiance_Field_Without_Posed_Camera_ICCV_2021_paper.pdf&quot;>;GNeRF&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3503161.3548078&quot;>;VMRF&lt;/a>; leverage &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;>;generative adversarial networks&lt;/a>; (GANs) to overcome the problem. These techniques have the ability to artificially “amplify” a limited number of training views, aiding reconstruction. GAN techniques, however, often have complex, sometimes unstable, training processes, making robust and reliable convergence difficult to achieve in practice. A range of other successful methods, such as &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.html&quot;>;SparsePose&lt;/a>; or &lt;a href=&quot;https://rust-paper.github.io/&quot;>;RUST&lt;/a>;, can infer poses from a limited number views, but require pre-training on a large dataset of posed images, which aren&#39;t always available, and can suffer from “domain-gap” issues when inferring poses for different types of images. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;MELON: NeRF with Unposed Images in SO(3)&lt;/a>;”, spotlighted at &lt;a href=&quot;https://3dvconf.github.io/2024/&quot;>;3DV 2024&lt;/a>;, we present a technique that can determine object-centric camera poses entirely from scratch while reconstructing the object in 3D. &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON&lt;/a>; (Modulo Equivalent Latent Optimization of NeRF) is one of the first techniques that can do this without initial pose camera estimates, complex training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. We demonstrate that MELON can reconstruct a NeRF from unposed images with state-of-the-art accuracy while requiring as few as 4–6 images of an object. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MELON&lt;/h2>; &lt;p>; We leverage two key techniques to aid convergence of this ill-posed问题。 The first is a very lightweight, dynamically trained &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) encoder that regresses camera poses from training images. We pass a downscaled training image to a four layer CNN that infers the camera pose. This CNN is initialized from noise and requires no pre-training. Its capacity is so small that it forces similar looking images to similar poses, providing an implicit regularization greatly aiding convergence. &lt;/p>; &lt;p>; The second technique is a &lt;em>;modulo loss&lt;/em>; that simultaneously considers pseudo symmetries of an object. We render the object from a fixed set of viewpoints for each training image, backpropagating the loss only through the view that best fits the training image. This effectively considers the plausibility of multiple views for each image. In practice, we find &lt;em>;N&lt;/em>;=2 views (viewing an object from the other side) is all that&#39;s required in most cases, but sometimes get better results with &lt;em>;N&lt;/em>;=4 for square objects. &lt;/p>; &lt;p>; These two techniques are integrated into standard NeRF training, except that instead of fixed camera poses, poses are inferred by the CNN and duplicated by the modulo loss. Photometric gradients back-propagate through the best-fitting cameras into the CNN. We observe that cameras generally converge quickly to globally optimal poses (see animation below). After training of the neural field, MELON can synthesize novel views using standard NeRF rendering methods. &lt;/p>; &lt;p>; We simplify the problem by using the &lt;a href=&quot;https://github.com/bmild/nerf&quot;>;NeRF-Synthetic&lt;/a>; dataset, a popular benchmark for NeRF research and common in the pose-inference literature. This synthetic dataset has cameras at precisely fixed distances and a consistent “up” orientation, requiring us to infer only the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spherical_coordinate_system&quot;>;polar coordinates&lt;/a>; of相机。 This is the same as an object at the center of a globe with a camera always pointing at it, moving along the surface. We then only need the latitude and longitude (2 degrees of freedom) to specify the camera pose. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s1315/image4.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;395&quot; data-original-width=&quot;1315&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;MELON uses a dynamically trained lightweight CNN encoder that predicts a pose for each image. Predicted poses are replicated by the &lt;em>;modulo loss, &lt;/em>;which only penalizes the smallest L2 distance from the ground truth color. At evaluation time, the neural field can be used to generate novel views.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We compute two key metrics to evaluate MELON&#39;s performance on the NeRF Synthetic dataset. The error in orientation between the ground truth and inferred poses can be quantified as a single angular error that we average across all training images, the pose error. We then test the accuracy of MELON&#39;s rendered objects from novel views by measuring the &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR) against held out test views. We see that MELON quickly converges to the approximate poses of most cameras within the first 1,000 steps of training, and achieves a competitive PSNR of 27.5 dB after 50k steps. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s960/image1 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Convergence of MELON on a toy truck model during optimization. &lt;strong>;Left&lt;/strong>;: Rendering of the NeRF. &lt;strong>;Right&lt;/strong>;: Polar plot of predicted (blue &lt;em>;x&lt;/em>;), and ground truth (red dot) cameras.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MELON achieves similar results for other scenes in the NeRF Synthetic dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s1999/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Reconstruction quality comparison between ground-truth (GT) and MELON on NeRF-Synthetic scenes after 100k training steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style =&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Noisy images&lt;/h3>; &lt;p>; MELON also works well when performing &lt;a href=&quot;https://en.wikipedia. org/wiki/View_synthesis&quot;>;novel view synthesis&lt;/a>; from extremely noisy, unposed images. We add varying amounts, &lt;em>;σ&lt;/em>;, of &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise&quot;>;white Gaussian noise&lt;/a>; to the training images. For example, the object in &lt;em>;σ&lt;/em>;=1.0 below is impossible to make out, yet MELON can determine the pose and generate novel views of the object. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s1182/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1182&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Novel view synthesis from noisy unposed 128×128 images. Top: Example of noise level present in training views. Bottom: Reconstructed model from noisy training views and mean angular pose error.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This perhaps shouldn&#39;t be too surprising, given that techniques like &lt;a href=&quot;https://bmild.github.io/rawnerf/&quot;>;RawNeRF&lt;/a>; have demonstrated NeRF&#39;s excellent de-noising capabilities with known camera poses. The fact that MELON works for noisy images of unknown camera poses so robustly was unexpected. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present MELON, a technique that can determine object-centric camera poses to reconstruct objects in 3D without the need for approximate pose initializations, complex GAN training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. Though we only demonstrated MELON on synthetic images we are adapting our technique to work in real world conditions. See the &lt;a href=&quot;https://arxiv.org/abs/2303.08096&quot;>;paper&lt;/a>; and &lt;a href=&quot;https://melon-nerf.github.io/&quot;>;MELON site&lt;/a>; to learn more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our paper co-authors Axel Levy, Matan Sela, and Gordon Wetzstein, as well as Florian Schroff and Hartwig Adam for continuous help in building this technology. We also thank Matthew Brown, Ricardo Martin-Brualla and Frederic Poitevin for their helpful feedback on the paper draft. We also acknowledge the use of the computational resources at the SLAC Shared Scientific Data Facility (SDF).&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/757976001746578714/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/757976001746578714&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html&quot; rel=&quot;alternate&quot; title=&quot;MELON: Reconstructing 3D objects from images with unknown poses&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s72-c/MELON%20HERO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-977556648557231190&lt;/id>;&lt;published>;2024-03-15T11:22:00.000-07:00&lt;/published>;&lt;updated>;2024-03-15T11:22:13.760-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;HEAL: A framework for health equity assessment of machine learning performance&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mike Schaekermann, Research Scientist, Google Research, and Ivor Horn, Chief Health Equity Officer &amp;amp; Director, Google Core&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s1600/HEAL-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Health equity is a major societal concern worldwide with disparities having many causes. These sources include limitations in access to healthcare, differences in clinical treatment, and even fundamental differences in the diagnostic technology. In dermatology for example, skin cancer outcomes are worse for populations such as minorities, those with lower socioeconomic status, or individuals with limited healthcare access. While there is great promise in recent advances in machine learning (ML) and artificial intelligence (AI) to help improve healthcare, this transition from research to bedside must be accompanied by a careful understanding of whether and how they impact health equity. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;em>;Health equity&lt;/em>; is defined by public health organizations as fairness of opportunity for everyone to be as healthy as possible. Importantly, equity may be different from &lt;em>;equality&lt;/em>;. For example, people with greater barriers to improving their health may require more or different effort to experience this fair opportunity. Similarly, equity is not &lt;em>;fairness&lt;/em>; as defined in the AI for healthcare literature. Whereas AI fairness often strives for equal performance of the AI technology across different patient populations, this does not center the goal of prioritizing performance with respect to pre-existing health disparities. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s1999 /image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1609&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Health equity considerations. An intervention (eg, an ML-based tool, indicated in dark blue) promotes health equity if it helps reduce existing disparities in health outcomes (indicated in lighter blue).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In “&lt;a href=&quot;https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00058-0/fulltext&quot;>;Health Equity Assessment of machine Learning performance (HEAL): a framework and dermatology AI model case study&lt;/a>;”, published in &lt;a href=&quot;https://www.thelancet.com/journals/eclinm/home&quot;>;&lt;i>;The Lancet eClinicalMedicine&lt;/i>;&lt;/a>;, we propose a methodology to quantitatively assess whether ML-based health technologies perform equitably. In other words, does the ML model perform well for those with the worst health outcomes for the condition(s) the model is meant to address? This goal anchors on the principle that health equity should prioritize and measure model performance with respect to disparate health outcomes, which may be due to a number of factors that include structural inequities (eg, demographic, social, cultural, political, economic, environmental and geographic). &lt;/p>; &lt;br />; &lt;h2>;The health equity framework (HEAL)&lt;/h2>; &lt;p>; The HEAL framework proposes a 4-step process to estimate the likelihood that an ML-based health technology performs equitably: &lt;/p>; &lt;ol>; &lt;li>; Identify factors associated with health inequities and define tool performance metrics, &lt;/li>; &lt;li>; Identify and quantify pre-existing health disparities, &lt;/li>; &lt;li>; Measure the performance of the tool for each subpopulation, &lt;/li>; &lt;li>; Measure the likelihood that the tool prioritizes performance with respect to health disparities. &lt;/li>; &lt;/ol>; &lt;p>; The final step&#39;s output is termed the HEAL metric, which quantifies how anticorrelated the ML model&#39;s performance is with health disparities. In other words, does the model perform better with populations that have the worse health outcomes? &lt;/p>; &lt;p>; This 4-step process is designed to inform improvements for making ML model performance more equitable, and is meant to be iterative and re-evaluated on a regular basis. For example, the availability of health outcomes data in step (2) can inform the choice of demographic factors and brackets in step (1), and the framework can be applied again with new datasets, models and populations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s1999/image1.jpg &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1352&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Framework for Health Equity Assessment of machine Learning performance (HEAL).&amp;nbsp;Our guiding principle is to avoid exacerbating health inequities, and these steps help us identify disparities and assess for inequitable model performance to move towards better outcomes for all.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With this work, we take a step towards encouraging explicit assessment of the health equity considerations of AI technologies, and encourage prioritization of efforts during model development to reduce health inequities for subpopulations exposed to structural inequities that can precipitate disparate outcomes. We should note that the present framework does not model causal relationships and, therefore, cannot quantify the actual impact a new technology will have on reducing health outcome disparities. However, the HEAL metric may help identify opportunities for improvement, where the current performance is not prioritized with respect to pre-existing health disparities. &lt;/p>; &lt;br />; &lt;h2>;Case study on a dermatology model&lt;/h2>; &lt;p>; As an illustrative case study, we applied the framework to a dermatology model, which utilizes a convolutional neural network similar to that described in &lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;prior work&lt;/a>;. This example dermatology model was trained to classify 288 skin conditions using a development dataset of 29k cases. The input to the model consists of three photos of a skin concern along with demographic information and a brief structured medical history. The output consists of a ranked list of possible matching skin conditions. &lt;/p>; &lt;p>; Using the HEAL framework, we evaluated this model by assessing whether it prioritized performance with respect to pre-existing health outcomes. The model was designed to predict possible dermatologic conditions (from a list of hundreds) based on photos of a skin concern and patient metadata. Evaluation of the model is done using a top-3 agreement metric, which quantifies how often the top 3 output conditions match the most likely condition as suggested by a dermatologist panel. The HEAL metric is computed via the anticorrelation of this top-3 agreement with health outcome rankings. &lt;/p>; &lt;p>; We used a dataset of 5,420 teledermatology cases, enriched for diversity in age, sex and race/ethnicity, to retrospectively evaluate the model&#39;s HEAL metric. The dataset consisted of “store-and-forward” cases from patients of 20 years or older from primary care providers in the USA and skin cancer clinics in Australia. Based on a review of the literature, we decided to explore race/ethnicity, sex and age as potential factors of inequity, and used sampling techniques to ensure that our evaluation dataset had sufficient representation of all race/ethnicity, sex and age groups. To quantify pre-existing health outcomes for each subgroup we relied on measurements from &lt;a href=&quot;https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates/global-health-estimates-leading-causes-of-dalys&quot;>;public&lt;/a>; &lt;a href=&quot;https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30925-9/fulltext&quot;>;databases&lt;/a>; endorsed by the World Health Organization, such as &lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4427&quot;>;Years of Life Lost&lt;/a>; (YLLs) and &lt;a href=&quot;https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158&quot;>;Disability-Adjusted Life Years&lt;/a>; (DALYs; years of life lost plus years lived with disability). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s1511 /Table1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot; 1511&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s16000/Table1.png&quot; />;&lt;/a>;&lt;/td>;&lt; /tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;HEAL metric for all dermatologic conditions across race/ethnicity subpopulations, including health outcomes (YLLs per 100,000), model performance ( top-3 agreement), and rankings for health outcomes and tool performance.&lt;br />;(* Higher is better; measures the likelihood the model performs equitably with respect to the axes in this table.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s1518/Table2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;316&quot; data-original-width=&quot;1518&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s16000/Table2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;HEAL metric for all dermatologic conditions across sexes, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table &lt;p>; Our analysis estimated that the model was 80.5% likely to perform equitably across race/ethnicity subgroups and 92.1% likely to perform equitably across sexes. &lt;/p>; &lt;p>; However, while the model was likely to perform equitably across age groups for cancer conditions specifically, we discovered that it had room for improvement across age groups for non-cancer conditions. For example, those 70+ have the poorest health outcomes related to non-cancer skin conditions, yet the model didn&#39;t prioritize performance for this subgroup. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s1508/Table3 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;1508&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s16000/Table3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;HEAL metrics for all cancer and non-cancer dermatologic conditions across age groups, including health outcomes (DALYs per 100,000), model performance (top -3 agreement), and rankings for health outcomes and tool performance. (* As above.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Putting things in context&lt;/h2>; &lt;p>; For holistic evaluation, the HEAL metric cannot be employed隔离中。 Instead this metric should be contextualized alongside many other factors ranging from computational efficiency and data privacy to ethical values, and aspects that may influence the results (eg, selection bias or differences in representativeness of the evaluation data across demographic groups). &lt;/p>; &lt;p>; As an adversarial example, the HEAL metric can be artificially improved by deliberately reducing model performance for the most advantaged subpopulation until performance for that subpopulation is worse than all others. For illustrative purposes, given subpopulations A and B where A has worse health outcomes than B, consider the choice between two models: Model 1 (M1) performs 5% better for subpopulation A than for subpopulation B. Model 2 (M2) performs 5% worse on subpopulation A than B. The HEAL metric would be higher for M1 because it prioritizes performance on a subpopulation with worse outcomes. However, M1 may have absolute performances of just 75% and 70% for subpopulations A and B respectively, while M2 has absolute performances of 75% and 80% for subpopulations A and B respectively. Choosing M1 over M2 would lead to worse overall performance for all subpopulations because some subpopulations are worse-off while no subpopulation is better-off. &lt;/p>; &lt;p>; Accordingly, the HEAL metric should be used alongside a &lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_efficiency&quot;>;Pareto condition&lt;/a>; (discussed further in the paper), which restricts model changes so that outcomes for each subpopulation are either unchanged or improved compared to the status quo, and performance does not worsen for any subpopulation. &lt;/p>; &lt;p>; The HEAL framework, in its current form, assesses the likelihood that an ML-based model prioritizes performance for subpopulations with respect to pre-existing health disparities for specific subpopulations. This differs from the goal of understanding whether ML will reduce disparities in outcomes across subpopulations in reality. Specifically, modeling improvements in outcomes requires a causal understanding of steps in the care journey that happen both before and after use of any given model. Future research is needed to address this gap. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The HEAL framework enables a quantitative assessment of the likelihood that health AI technologies prioritize performance with respect to health disparities. The case study demonstrates how to apply the framework in the dermatological domain, indicating a high likelihood that model performance is prioritized with respect to health disparities across sex and race/ethnicity, but also revealing the potential for improvements for non-cancer conditions across age. The case study also illustrates limitations in the ability to apply all recommended aspects of the framework (eg, mapping societal context, availability of data), thus highlighting the complexity of health equity considerations of ML-based tools. &lt;/p>; &lt;p>; This work is a proposed approach to address a grand challenge for AI and health equity, and may provide a useful evaluation framework not only during model development, but during pre-implementation and real-world monitoring stages, eg, in the form of health equity dashboards. We hold that the strength of the HEAL framework is in its future application to various AI tools and use cases and its refinement in the process. Finally, we acknowledge that a successful approach towards understanding the impact of AI technologies on health equity needs to be more than a set of metrics. It will require a set of goals agreed upon by a community that represents those who will be most impacted by a model. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The research described here is joint work across many teams at Google. We are grateful to all our co-authors: Terry Spitz, Malcolm Pyles, Heather Cole-Lewis, Ellery Wulczyn, Stephen R. Pfohl, Donald Martin, Jr., Ronnachai Jaroensri, Geoff Keeling, Yuan Liu, Stephanie Farquhar, Qinghan Xue, Jenna Lester, Cían Hughes, Patricia Strachan, Fraser Tan, Peggy Bui, Craig H. Mermel, Lily H. Peng, Yossi Matias, Greg S. Corrado, Dale R. Webster, Sunny Virmani, Christopher Semturs, Yun Liu, and Po-Hsuan Cameron Chen. We also thank Lauren Winer, Sami Lachgar, Ting-An Lin, Aaron Loh, Morgan Du, Jenny Rizk, Renee Wong, Ashley Carrick, Preeti Singh, Annisah Um&#39;rani, Jessica Schrouff, Alexander Brown, and Anna Iurchenko for their support of this project.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/977556648557231190/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/977556648557231190&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/heal-framework-for-health-equity.html&quot; rel=&quot;alternate&quot; title=&quot;HEAL: A framework for health equity assessment of machine learning performance&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s72-c/HEAL-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7868032799856333119&lt;/id>;&lt;published>;2024-03-14T12:38:00.000-07:00&lt;/published>;&lt;updated>;2024-03-14T12:38:11.597-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Cappy: Outperforming and boosting large multi-task language models with a small scorer&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yun Zhu and Lijuan Liu, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s320/Cappy%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language model (LLM) advancements have led to a new paradigm that unifies various natural language processing (NLP) tasks within an instruction-following framework. This paradigm is exemplified by recent multi-task LLMs, such as &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FLAN&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML&lt;/a>;. First, multi-task data is gathered with each task following a task-specific template, where each labeled example is converted into an instruction (eg, &lt;em>;&quot;&lt;/em>;Put the concepts together to form a sentence: ski, mountain , skier&lt;em>;”&lt;/em>;) paired with a corresponding response (eg, &lt;em>;&quot;&lt;/em>;Skier skis down the mountain&lt;em>;&quot;&lt;/em>;). These instruction-response pairs are used to train the LLM, resulting in a conditional generation model that takes an instruction as input and generates a response. Moreover, multi-task LLMs have exhibited remarkable task-wise generalization capabilities as they can address unseen tasks by understanding and solving brand-new instructions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s640/Cappy%20instruction-following.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;177&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s16000/Cappy%20instruction-following.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The demonstration of the instruction-following pre-training of multi-task LLMs, eg, FLAN. Pre-training tasks under this paradigm improves the performance for unseen tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Due to the complexity of understanding and solving various tasks solely using instructions, the size of multi-task LLMs typically spans from several billion parameters to hundreds of billions (eg, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FLAN-11B&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0-11B&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.12017&quot;>;OPT-IML-175B&lt;/a>;). As a result, operating such sizable models poses significant challenges because they demand considerable computational power and impose substantial requirements on the memory capacities of GPUs and TPUs, making their training and inference expensive and inefficient. Extensive storage is required to maintain a unique LLM copy for each downstream task. Moreover, the most powerful multi-task LLMs (eg, FLAN-PaLM-540B) are closed-sourced, making them impossible to be adapted. However, in practical applications, harnessing a single multi-task LLM to manage all conceivable tasks in a zero-shot manner remains difficult, particularly when dealing with complex tasks, personalized tasks and those that cannot be succinctly defined using instructions. On the other hand, the size of downstream training data is usually insufficient to train a model well without incorporating rich prior knowledge. Hence, it is long desired to adapt LLMs with downstream supervision while bypassing storage, memory, and access issues. &lt;/p>; &lt;p>; Certain &lt;em>;parameter-efficient tuning&lt;/em>; strategies, including &lt;a href=&quot;https://aclanthology.org/2021.acl-long.353.pdf&quot;>;prompt tuning&lt;/a>; and &lt;a href=&quot;https://openreview.net/pdf?id=nZeVKeeFYf9&quot;>;adapters&lt;/a>;, substantially diminish storage requirements, but they still perform back-propagation through LLM parameters during the tuning process, thereby keeping their memory demands high. Additionally, some &lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/2301.00234.pdf&quot;>;in-context learning&lt;/a>;&lt;/em>; techniques circumvent parameter tuning by integrating a limited number of supervised examples into the instruction. However, these techniques are constrained by the model&#39;s maximum input length, which permits only a few samples to guide task resolution. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.06720&quot;>;Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer&lt;/a>;”, presented at &lt;a href=&quot;https://nips.cc/virtual/2023/index.html&quot;>;NeurIPS 2023&lt;/a>;, we propose a novel approach that enhances the performance and efficiency of multi-task LLMs. We introduce a lightweight pre-trained scorer, Cappy, based on continual pre-training on top of &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; with merely 360 million parameters. Cappy takes in an instruction and a candidate response as input, and produces a score between 0 and 1, indicating an estimated correctness of the response with respect to the instruction. Cappy functions either independently on classification tasks or serves as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy efficiently enables downstream supervision without requiring any finetuning, which avoids the need for back-propagation through LLM parameters and reduces memory requirements. Finally, adaptation with Cappy doesn&#39;t require access to LLM parameters as it is compatible with closed-source multi-task LLMs, such as those only accessible via WebAPIs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s1999/Cappy %20overview.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;975&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s16000/Cappy%20overview.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Cappy takes an instruction and response pair as input and outputs a score ranging from 0 to 1, indicating an estimation of the correctness of the response with respect to the instruction.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>; Pre-training&lt;/h2>; &lt;p>; We begin with the same dataset collection, which includes 39 diverse datasets from &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>; that were used to train &lt;a href=&quot;https://arxiv.org/abs/2110.08207&quot;>;T0&lt;/a>;. This collection encompasses a wide range of task types, such as question answering, sentiment analysis, and summarization. Each dataset is associated with one or more templates that convert each instance from the original datasets into an instruction paired with its ground truth response. &lt;/p>; &lt;p>; Cappy&#39;s regression modeling requires each pre-training data instance to include an instruction-response pair along with a correctness annotation for the response, so we produce a dataset with correctness annotations that range from 0 to 1. For every instance within a generation task, we leverage an existing multi-task LLM to generate multiple responses by sampling, conditioned on the given instruction. Subsequently, we assign an annotation to the pair formed by the instruction and every response, using the similarity between the response and the ground truth response of the instance. Specifically, we employ &lt;a href=&quot;https://aclanthology.org/W04-1013/&quot;>;Rouge-L&lt;/a>;, a commonly-used metric for measuring overall multi-task performance that has demonstrated a strong alignment with human evaluation, to calculate this similarity as a form of weak supervision. &lt;/p>; &lt;p>; As a result, we obtain an effective regression dataset of 160 million instances paired with correctness score annotations. The final Cappy model is the result of continuous pre-training using the regression dataset on top of the &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;>;RoBERTa&lt;/a>; model. The pre-training of Cappy is conducted on Google&#39;s &lt;a href=&quot;https://arxiv.org/abs/2304.01433&quot;>;TPU-v4&lt;/a>;, with &lt;a href=&quot;https://arxiv.org/pdf/2310.16355.pdf&quot;>;RedCoast&lt;/a>;, a lightweight toolkit for automating distributed training. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s1999/Cappy%20data%20augmentation .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;438&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s16000/Cappy%20data%20augmentation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Data augmentation with a multi-task LLM to construct a weakly supervised regression dataset for Cappy&#39;s pre-training and fine-tuning.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Applying Cappy&lt;/h2>; &lt;p>; Cappy solves practical tasks within a candidate-selection mechanism. More specifically, given an instruction and a set of candidate responses, Cappy produces a score for each candidate response. This is achieved by inputting the instruction alongside each individual response, and then assigning the response with the highest score as its prediction. In classification tasks, all candidate responses are inherently predefined. For example, for an instruction of a sentiment classification task (eg, “Based on this review, would the user recommend this product?: &#39;Stunning even for the non-gamer.&#39;”), the candidate responses are “Yes” or “不”。 In such scenarios, Cappy functions independently. On the other hand, in generation tasks, candidate responses are not pre-defined, requiring an existing multi-task LLM to yield the candidate responses. In this case, Cappy serves as an auxiliary component of the multi-task LLM, enhancing its decoding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Adapting multi-task LLMs with Cappy &lt;/h3>; &lt;p>; When there is available downstream training data, Cappy enables effective and efficient adaptation of multi-task LLMs on downstream tasks. Specifically, we fine-tune Cappy to integrate downstream task information into LLM predictions. This process involves creating a separate regression dataset specific to the downstream training data with the same data annotation process used to construct the pre-training data. As a result, the fine-tuned Cappy collaborates with a multi-task LLM, boosting the LLM&#39;s performance on the downstream task. &lt;/p>; &lt;p>; In contrast to other LLM tuning strategies, adapting LLMs with Cappy significantly reduces the high demand for device memory as it avoids the need for back-propagation through LLM parameters for downstream tasks. Moreover, Cappy adaptation does not rely on the access to LLM parameters, making it compatible with closed-source multi-task LLMs, such as the ones only accessible via WebAPIs. Compared with in-context learning approaches, which circumvent model tuning by attaching training examples to the instruction prefix, Cappy is not restricted by the LLM&#39;s maximum input length. Thus, Cappy can incorporate an unlimited number of downstream training examples. Cappy can also be applied with other adaptation methods, such as fine-tuning and in-context learning, further boosting their overall performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s1999/Cappy%20downstream%20adaptation .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s16000/Cappy%20downstream%20adaptation.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Downstream adaptation comparison between Cappy and approaches that rely on an LLM&#39;s parameters, such as fine-tuning and prompt tuning. Cappy&#39;s application enhances multi-task LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We assess Cappy&#39;s performance across eleven held-out language understanding classification tasks from &lt;a href=&quot;https://arxiv.org/abs/2202.01279&quot;>;PromptSource&lt;/a>;. We demonstrate that Cappy, with 360M parameters, outperforms OPT-175B and OPT-IML-30B, and matches the accuracy of the best existing multi-task LLMs (T0-11B and OPT-IML-175B). These findings highlight Cappy&#39;s capabilities and parameter efficiency, which can be credited to its scoring-based pre-training strategy that integrates contrastive information by differentiating between high-quality and low-quality responses. On the contrary, previous multi-task LLMs depend exclusively on &lt;a href=&quot;https://en.wikipedia.org/wiki/Teacher_forcing&quot;>;teacher-forcing training&lt;/a>; that utilizes only the ground truth responses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s1999/Cappy%20accuracy .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s16000/Cappy%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The overall accuracy averaged over eleven test tasks from PromptSource. “RM” refers to a &lt;a href=&quot;https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2&quot;>;pre-trained RLHF reward model&lt;/a>;. Cappy matches the best ones among existing multi-task LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also examine the adaptation of multi-task LLMs with Cappy on complex tasks from &lt;a href=&quot;https://arxiv.org/abs/2206.04615&quot;>;BIG-Bench&lt;/a>;, a set of manually curated tasks that are considered beyond the capability of many LLMs. We focus on all the 45 generation BIG-Bench tasks, specifically those that do not offer pre-established answer choices. We evaluate the performance using the Rouge-L score (representing the overall similarity between model generations and corresponding ground truths) on every test set, reporting the average score across 45 tests. In this experiment, all variants of FLAN-T5 serve as the backbone LLMs, and the foundational FLAN-T5 models are frozen. These results, shown below, suggest that Cappy enhances the performance of FLAN-T5 models by a large margin, consistently outperforming the most effective baseline achieved through sample selection using self-scoring of the LLM itself. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s1999/Cappy%20averaged %20Rouge-L%20score.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1625&quot; data-original-width=&quot;1999 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s16000/Cappy%20averaged%20Rouge-L%20score.png&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The averaged Rouge-L score over 45 complex tasks within BIG-Bench. The x-axis refers to FLAN-T5 models of different sizes. Every dashed line represents an approach working on FLAN-T5s. Self-scoring refers to using the cross-entropy of LLM to select responses. Cappy enhances the performance of FLAN-T5 models by a large margin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduce Cappy, a novel approach that enhances the performance and efficiency of multi-task LLMs. In our experiments, we adapt a single LLM to several domains with Cappy. In the future, Cappy as a pre-trained model can potentially be used in other creative ways beyond on single LLMs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;Thanks to Bowen Tan, Jindong Chen, Lei Meng, Abhanshu Sharma and Ewa Dominowska for their valuable feedback. We would also like to thank Eric Xing and Zhiting Hu for their suggestions. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7868032799856333119/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7868032799856333119&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html&quot; rel=&quot;alternate&quot; title=&quot;Cappy: Outperforming and boosting large multi-task language models with a small scorer&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s72-c/Cappy%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6090481872694489715&lt;/id>;&lt;published>;2024-03-12T14:15:00.000-07:00&lt;/published>;&lt;updated>;2024-03-19T09:12:05.568-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Talk like a graph: Encoding graphs for large language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s1600/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine all the things around you — your friends, tools in your kitchen, or even the parts of your bike. They are all connected in different ways. In computer science, the term &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graph&lt;/a>; &lt;/em>;is used to describe connections between objects. Graphs consist of nodes (the objects themselves) and edges (connections between two nodes, indicating a relationship between them). Graphs are everywhere now. The internet itself is a giant graph of websites linked together. Even the knowledge search engines use is organized in a graph-like way. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Furthermore, consider the remarkable advancements in artificial intelligence — such as chatbots that can write stories in seconds, and even software that can interpret medical reports. This exciting progress is largely thanks to large language models (LLMs). New LLM technology is constantly being developed for different uses. &lt;/p>; &lt;p>; Since graphs are everywhere and LLM technology is on the rise, in “&lt;a href=&quot;https://openreview.net/forum?id=IuXR1CCrSi&quot;>;Talk like a Graph: Encoding Graphs for Large Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/&quot;>;ICLR 2024&lt;/a>;, we present a way to teach powerful LLMs how to better reason with graph information. Graphs are a useful way to organize information, but LLMs are mostly trained on regular text. The objective is to test different techniques to see what works best and gain practical insights. Translating graphs into text that LLMs can understand is a remarkably complex task. The difficulty stems from the inherent complexity of graph structures with multiple nodes and the intricate web of edges that connect them. Our work studies how to take a graph and translate it into a format that an LLM can understand. We also design a benchmark called &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>; to study different approaches on different graph reasoning problems and show how to &lt;em>;phrase&lt;/em>; a graph-related problem in a way that enables the LLM to solve the graph problem. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: 1) the graph encoding method, 2) the nature of the graph task itself, and 3) interestingly, the very structure of the graph considered. These findings give us clues on how to best represent graphs for LLMs. Picking the right method can make the LLM up to 60% better at graph tasks! &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s1600/image7.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Pictured, the process of encoding a graph as text using two different approaches and feeding the text and a question about the graph to the LLM.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Graphs as text&lt;/h2>; &lt;p>; To be able to systematically find out what is the best way to translate a graph to text, we first design a benchmark called &lt;em>;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/graphqa&quot;>;GraphQA&lt;/a>;&lt;/em>;. Think of GraphQA as an exam designed to evaluate powerful LLMs on graph-specific problems. We want to see how well LLMs can understand and solve problems that involve graphs in different setups. To create a comprehensive and realistic exam for LLMs, we don&#39;t just use one type of graph, we use a mix of graphs ensuring breadth in the number of connections. This is mainly because different graph types make solving such problems easier or harder. This way, GraphQA can help expose biases in how an LLM thinks about the graphs, and the whole exam gets closer to a realistic setup that LLMs might encounter in the real world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s1368/image6.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;291&quot; data-original-width=&quot;1368&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Overview of our framework for reasoning with graphs using LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; GraphQA focuses on simple tasks related to graphs, like checking if an edge exists , calculating the number of nodes or edges, finding nodes that are connected to a specific node, and checking for cycles in a graph. These tasks might seem basic, but they require understanding the relationships between nodes and edges. By covering different types of challenges, from identifying patterns to creating new connections, GraphQA helps models learn how to analyze graphs effectively. These basic tasks are crucial for more complex reasoning on graphs, like finding the shortest path between nodes, detecting communities, or identifying influential nodes. Additionally, GraphQA includes generating random graphs using various algorithms like &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;scale-free networks&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabasi-Albert model&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;stochastic block model&lt;/a>;, as well as simpler graph structures like paths, complete graphs, and star graphs, providing a diverse set of data for training. &lt;/p>; &lt;p>; When working with graphs, we also need to find ways to ask graph-related questions that LLMs can understand. &lt;em>;Prompting heuristics&lt;/em>; are different strategies for doing this. Let&#39;s break down the common ones: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Zero-shot&lt;/em>;: simply describe the task (&quot;Is there a cycle in this graph?&quot;) and tell the LLM to go为了它。 No examples provided. &lt;/li>;&lt;li>;&lt;em>;Few-shot&lt;/em>;: This is like giving the LLM a mini practice test before the real deal. We provide a few example graph questions and their correct answers. &lt;/li>;&lt;li>;&lt;em>;Chain-of-Thought&lt;/em>;: Here, we show the LLM how to break down a problem step-by-step with examples. The goal is to teach it to generate its own &quot;thought process&quot; when faced with new graphs. &lt;/li>;&lt;li>;&lt;em>;Zero-CoT&lt;/em>;: Similar to CoT, but instead of training examples, we give the LLM a simple prompt, like &quot;Let&#39;s think step-by-step,&quot; to trigger its own problem-solving breakdown. &lt;/li>;&lt;li>;&lt;em>;BAG (build a graph)&lt;/em>;: This is specifically for graph tasks. We add the phrase &quot;Let&#39;s build a graph...&quot; to the description, helping the LLM focus on the graph structure. &lt;/li>; &lt;/ul>; &lt;p>; We explored different ways to translate graphs into text that LLMs can work with. Our key questions were: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Node encoding&lt;/em>;: How do we represent individual nodes? Options tested include simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer&quot;>;integers&lt;/a>;, common names (people, characters), and letters. &lt;/li>;&lt;li>;&lt;em>;Edge encoding&lt;/em>;: How do we describe the relationships between nodes? Methods involved parenthesis notation, phrases like &quot;are friends&quot;, and symbolic representations like arrows. &lt;/li>; &lt;/ul>; &lt;p>; Various node and edge encodings were combined systematically. This led to functions like the ones in the following figure: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/s855/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;404&quot; data-original-width=&quot;855&quot; height=&quot;302&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/w640-h302/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of graph encoding functions used to encode graphs via text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Analysis and results&lt;/h2>; &lt;p>; We carried out three key experiments: one to test how LLMs handle graph tasks, and two to understand how the size of the LLM and different graph shapes affected performance. We run all our experiments on GraphQA. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;How LLMs handle graph tasks &lt;/h3>; &lt;p>; In this experiment, we tested how well pre-trained LLMs tackle graph problems like identifying connections, cycles, and node degrees. Here is what we learned: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;LLMs struggle:&lt;/em>; On most of these basic tasks, LLMs did not do much better than a random guess. &lt;/li>;&lt;li>;&lt;em>;Encoding matters significantly&lt;/em>;: How we represent the graph as text has a great effect on LLM performance. The &quot;incident&quot; encoding excelled for most of the tasks in general. &lt;/li>; &lt;/ul>; &lt;p>; Our results are summarized in the following chart. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s1864/image8 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of various graph encoder functions based on their accuracy on different graph tasks. The main conclusion from this figure is that the graph encoding functions matter significantly.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h3>;Bigger is (usually) better &lt;/h3>; &lt;p>; In this experiment, we wanted to see if the size of the LLM (in terms of the number of parameters) affects how well they can handle graph problems 。 For that, we tested the same graph tasks on the XXS, XS, S, and L sizes of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>;. Here is a summary of our findings: &lt;/p>; &lt;ul>; &lt;li>;In general, bigger models did better on graph reasoning tasks. It seems like the extra parameters gave them space to learn more complex patterns. &lt;/li>;&lt;li>;Oddly, size didn&#39;t matter as much for the “edge existence” task (finding out if two nodes in a graph are connected). &lt;/li>;&lt;li>;Even the biggest LLM couldn&#39;t consistently beat a simple baseline solution on the cycle check problem (finding out if a graph contains a cycle or not). This shows LLMs still have room to improve with certain graph tasks. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/s1227/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;959&quot; data-original-width=&quot;1227&quot; height=&quot;500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/w640-h500/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of model capacity on graph reasoning task for PaLM 2-XXS, XS, S, and L.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Do different graph shapes confuse LLMs &lt;/h3>; &lt;p>; We wondered if the &quot;shape&quot; of a graph (how nodes are connected) influences how well LLMs can solve problems on it. Think of the following figure as different examples of graph shapes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s1400/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1400&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Samples of graphs generated with different graph generators from GraphQA. ER, BA, SBM, and SFN refers to &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabási–Albert&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;Stochastic Block Model&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;Scale-Free Network&lt;/a>; respectively.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that graph structure has a big impact on LLM performance. For example, in a task asking if a cycle exists, LLMs did great on tightly interconnected graphs (cycles are common there) but struggled on path graphs (where cycles never happen). Interestingly, providing some mixed examples helped it adapt. For instance, for cycle check, we added some examples containing a cycle and some examples with no cycles as few-shot examples in our prompt. Similar patterns occurred with other tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s1864/image5.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1864&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Comparing different graph generators on different graph tasks. The main observation here is that graph structure has a significant impact on the LLM&#39;s performance. ER, BA, SBM, and SFN refers to &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős–Rényi&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model&quot;>;Barabási–Albert&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_block_model&quot;>;Stochastic Block Model&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Scale-free_network&quot;>;Scale-Free Network&lt;/a>; respectively.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In short, we dug deep into how to best represent graphs as text so LLMs can understand them. We found three major factors that make a difference: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;How to translate the graph to text&lt;/em>;: how we represent the graph as text significantly influences LLM performance. The incident encoding excelled for most of the tasks in general.. &lt;/li>;&lt;li>;&lt;em>;Task type&lt;/em>;: Certain types of graph questions tend to be harder for LLMs, even with a good translation from graph to文本。 &lt;/li>;&lt;li>;&lt;em>;Graph structure&lt;/em>;: Surprisingly, the &quot;shape&quot; of the graph that on which we do inference (dense with connections, sparse, etc.) influences how well an LLM does. &lt;/li>; &lt;/ul>; &lt;p>; This study revealed key insights about how to prepare graphs for LLMs. The right encoding techniques can significantly boost an LLM&#39;s accuracy on graph problems (ranging from around 5% to over 60% improvement). Our new benchmark, GraphQA, will help drive further research in this area. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to express our gratitude to our co-author, Jonathan Halcrow, for his valuable contributions to this work. We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. We would also like to extend special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6090481872694489715/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6090481872694489715&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html&quot; rel=&quot;alternate&quot; title=&quot;Talk like a graph: Encoding graphs for large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s72-c/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-470840348983280912&lt;/id>;&lt;published>;2024-03-11T12:08:00.000-07:00&lt;/published>;&lt;updated>;2024-03-11T12:13:03.824-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Chain-of-table: Evolving tables in the reasoning chain for table understanding&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zilong Wang, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNcD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s832/Chain-of-Table.png&quot; style=&quot;display: none;&quot; />; &lt;p>; People use tables every day to organize and interpret complex information in a structured, easily accessible format. Due to the ubiquity of such tables, reasoning over tabular data has long been a central topic in &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP). Researchers in this field have aimed to leverage language models to help users answer questions, verify statements, and analyze data based on tables. However, language models are trained over large amounts of plain text, so the inherently structured nature of tabular data can be difficult for language models to fully comprehend and utilize. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Recently, &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_language_model&quot;>;large language models&lt;/a>; (LLMs) have achieved outstanding performance across diverse &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural-language_understanding&quot;>;natural language understanding&lt;/a>; (NLU) tasks by generating reliable reasoning chains, as shown in works like &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2205.10625&quot;>;Least-to-Most&lt;/a>;. However, the most suitable way for LLMs to reason over tabular data remains an open question. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2401.04398&quot;>;Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding&lt;/a>;”, we propose a framework to tackle table understanding tasks, where we train LLMs to outline their reasoning step by step, updating a given table iteratively to reflect each part of a thought process, akin to how people solve the table-based problems. This enables the LLM to transform the table into simpler and more manageable segments so that it can understand and analyze each part of the table in depth. This approach has yielded significant improvements and achieved new state-of-the-art results on the &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1909.02164&quot;>;TabFact&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>; benchmarks. The figure below shows the high-level overview of the proposed Chain-of-Table and other methods. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s1478/image2.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1478&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Given a complex table where a cyclist&#39;s nationality and name are in the same cell, (a) generic, multi-step reasoning is unable to provide the correct answer (b) program-aided reasoning generates and executes programs (eg , SQL queries) to deliver the answer, but falls short in accurately addressing the question. In contrast, (c) Chain-of-Table iteratively samples a chain of operations that effectively transform the complex table into a version specifically tailored to the question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chain-of-Table&lt;/h2>; &lt;p>; In Chain-of-Table, we guide LLMs using &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning&lt;/a>; to iteratively generate operations and to update the table to represent its reasoning chain over tabular data. This enables LLMs to dynamically plan the next operation based on the results of previous ones. This continuous evolution of the table forms a chain, which provides a more structured and clear representation of the reasoning process for a given problem and enables more accurate and reliable predictions from the LLM. &lt;/p>; &lt;p>; For example, when asked, “Which actor has the most NAACP image awards?” the Chain-of-Table framework prompts an LLM to generate tabular operations mirroring tabular reasoning processes. It first identifies the relevant columns. Then, it aggregates rows based on shared content. Finally, it reorders the aggregated results to yield a final table that clearly answers the posed question. &lt;/p>; &lt;p>; These operations transform the table to align with the question presented. To balance performance with computational expense on large tables, we construct the operation chain according to a subset of tabular rows.. Meanwhile, the step-by-step operations reveal the underlying reasoning process through the display of intermediate results from the tabular operations, fostering enhanced interpretability and understanding. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;983&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Illustration of the tabular reasoning process in Chain-of-Table. This iterative process involves dynamically planning an operation chain and accurately storing intermediate results in the transformed tables. These intermediate tables serve as a tabular thought process that can guide the LLM to land to the correct answer more reliably.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Chain-of-Table consists of three main stages. In the first stage, it instructs the LLM to dynamically plan the next operation by in-context learning. Specifically, the prompt involves three components as shown in the following figure: &lt;/p>; &lt;ol>; &lt;li>; The question &lt;em>;Q&lt;/em>;: “Which country had the most cyclists finish in the top 3?” &lt;/li>;&lt;li>; The operation history &lt;em>;chain&lt;/em>;: &lt;code>;f_add_col(Country)&lt;/code>; and &lt;code>;f_select_row(1, 2, 3)&lt;/code>;. &lt;/li>;&lt;li>; The latest intermediate table &lt;em>;T&lt;/em>;: the transformed intermediate table. &lt;/li>; &lt;/ol>; &lt;p>; By providing the triplet &lt;em>;(T, Q, chain)&lt;/em>; in the prompt, the LLM can observe the previous tabular reasoning process and select the next operation from the operation pool to complete the reasoning chain step by step. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s1958/image1.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1233&quot; data-original-width=&quot;1958&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;Illustration of how Chain-of-Table selects the next operation from the operation pool and generates the arguments for the operation.(a) Chain-of-Table samples the next operation from the operation pool. (b) It takes the selected operation as input and generates its arguments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After the next operation &lt;em>;f&lt;/em>; is determined, in the second stage, we need to generate the arguments. As above, Chain-of-Table considers three components in the prompt as shown in the figure: (1) the question, (2) the selected operation and its required arguments, and (3) the latest intermediate table. &lt;/p>; &lt;p>; For instance, when the operation &lt;code>;f_group_by&lt;/code>; is selected, it requires a header name as its argument. &lt;/p>; &lt;p>; The LLM selects a suitable header within the table. Equipped with the selected operation and the generated arguments, Chain-of-Table executes the operation and constructs a new intermediate table for the following reasoning. &lt;/p>; &lt;p>; Chain-of-Table iterates the previous two stages to plan the next operation and generate the required arguments. During this process, we create an operation chain acting as a proxy for the tabular reasoning steps. These operations generate intermediate tables presenting the results of each step to the LLM. Consequently, the output table contains comprehensive information about the intermediate phases of tabular reasoning. In our final stage, we employ this output table in formulating the final query and prompt the LLM along with the question for the final answer. &lt;/p>; &lt;br />; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; We use &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2-S&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;&amp;nbsp;as the backbone LLMs and conduct the experiments on three public table understanding benchmarks: &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1909.02164&quot;>;TabFact&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2104.00369&quot;>;FeTaQA&lt;/a>;. WikiTQ and FeTaQA are datasets for table-based question answering. TabFact is a table-based fact verification benchmark. In this blogpost, we will focus on the results on WikiTQ and TabFact. We compare Chain-of-Table with the generic reasoning methods (eg, End-to-End QA, Few-Shot QA, and &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>;) and the program-aided methods (eg, &lt;a href=&quot;https://arxiv.org/abs/2204.00498&quot;>;Text-to-SQL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot;>;Binder&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Dater&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;More accurate answers&lt;/h3>; &lt;p>; Compared to the generic reasoning methods and program-aided reasoning methods, Chain-of-Table achieves better performance across &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates&quot;>;GPT 3.5&lt;/a>;. This is attributed to the dynamically sampled operations and the informative intermediate tables. &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s1018/ChainOfTableUnderstanding.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;755&quot; data-original-width=&quot;1018&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s16000/ChainOfTableUnderstanding.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Understanding results on WikiTQ and TabFact with PaLM 2 and GPT 3.5 compared with various models.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Better robustness on harder questions&lt;/h3>; &lt;p>; In Chain-of-Table, longer operation chains indicate the higher difficulty and complexity of the questions and their corresponding tables. We categorize the test samples according to their operation lengths in Chain-of-Table. We compare Chain-of-Table with Chain-of-Thought and Dater, as representative generic and program-aided reasoning methods. We illustrate this using results from &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; on &lt;a href=&quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>;. &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIaEsHJohvlrSna7/s1548/CoTOpChainLength.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;624&quot; data-original-width=&quot;1548&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIaEsHJohvlrSna7/s16000/CoTOpChainLength.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of Chain-of-Thought, Dater, and the proposed Chain-of-Table on WikiTQ for questions that require an operation chain of varying lengths. Our proposed atomic operations significantly improve performance over generic and program-aided reasoning counterparts.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Notably, Chain-of-Table consistently surpasses both baseline methods across all operation chain lengths, with a significant margin up to 11.6% compared with &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>;, and up to 7.9% compared with &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Dater&lt;/a>;. Moreover, the performance of Chain-of-Table declines gracefully with increasing number of operations compared to other baseline methods, exhibiting only a minimal decrease when the number of operations increases from four to five. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Better robustness with larger tables&lt;/h3>; &lt;p>; We categorize the tables from &lt;a href= &quot;https://arxiv.org/abs/1508.00305&quot;>;WikiTQ&lt;/a>; into three groups based on token number: small (&amp;lt;2000 tokens), medium (2000 to 4000 tokens) and large (&amp;gt;4000 tokens) 。 We then compare Chain-of-Table with &lt;a href=&quot;https://arxiv.org/abs/2301.13808&quot;>;Dater&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2210.02875&quot;>;Binder&lt;/a>;, the two latest and strongest baselines. &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1008&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Performance of Binder, Dater, and the proposed Chain-of-Table on small (&amp;lt;2000 tokens), medium (2000 to 4000 tokens), and large (&amp;gt;4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.)&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Performance of Binder, Dater, and the proposed Chain-of-Table on small (&amp;lt;2000 tokens), medium (2000 to 4000 tokens), and large (&amp;gt;4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.) &lt;/p>; &lt;p>; As anticipated, the performance decreases with larger input tables, as models are required to reason through longer contexts. Nevertheless, the performance of the proposed Chain-of-Table diminishes gracefully, achieving a significant 10+% improvement over the second best competing method when dealing with large tables. This demonstrates the efficacy of the reasoning chain in handling long tabular inputs. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our proposed Chain-of-Table method enhances the reasoning capability of LLMs by leveraging the tabular structure to express intermediate steps for table-based reasoning. It instructs LLMs to dynamically plan an operation chain according to the input table and its associated question. This evolving table design sheds new light on the understanding of prompting LLMs for table understanding. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister. Thanks to Chih-Kuan Yeh and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/470840348983280912/comments/default&quot; rel =&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables- in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default /470840348983280912&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/470840348983280912&quot; rel=&quot;self&quot; type =&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html&quot; rel=&quot;alternate&quot; title= &quot;Chain-of-table: Evolving tables in the reasoning chain for table understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNcD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s72-c/Chain-of-Table.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376&lt;/id>;&lt;published >;2024-03-08T11:33:00.000-08:00&lt;/published>;&lt;updated>;2024-03-13T09:18:01.747-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Image Classification&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Health-specific embedding tools for dermatology and pathology&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Steiner, Clinical Research Scientist, Google Health, and Rory Pilgrim, Product Manager, Google Research&lt;/span>; &lt;img src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s16000/Path%20+%20Derm%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; There&#39;s a worldwide shortage of access to medical imaging expert interpretation across specialties including &lt;a href=&quot;https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage&quot;>;radiology&lt;/a>;, &lt;a href=&quot;https://www.aad.org/dw/monthly/2021/december/feature-running-dry&quot;>;dermatology&lt;/a>; and &lt;a href=&quot;https://proscia.com/infographic-the-state-of-the-pathology-workforce-2022/&quot;>;pathology&lt;/a>;. Machine learning (ML) technology can help ease this burden by powering tools that enable doctors to interpret these images more accurately and efficiently. However, the development and implementation of such ML tools are often limited by the availability of high-quality data, ML expertise, and computational resources. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One way to catalyze the use of ML for medical imaging is via domain-specific models that utilize deep learning (DL) to capture the information in medical images as compressed numerical vectors (called embeddings). These embeddings represent a type of pre-learned understanding of the important features in an image. Identifying patterns in the embeddings reduces the amount of data, expertise, and compute needed to train performant models as compared to &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;>;working with high-dimensional data&lt;/a>;, such as images, directly. Indeed, these embeddings can be used to perform a variety of downstream tasks within the specialized domain (see animated graphic below). This framework of leveraging pre-learned understanding to solve related tasks is similar to that of a seasoned guitar player quickly learning a new song by ear. Because the guitar player has already built up a foundation of skill and understanding, they can quickly pick up the patterns and groove of a new song. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s1600/Path%20+% 20Derm%20train%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s16000/Path%20+%20Derm%20train%20LP.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation is used to convert a small dataset of (image, label) pairs into (embedding, label) pairs 。 These pairs can then be used to train a task-specific classifier using a linear probe, (ie, a lightweight linear classifier) as represented in this graphic, or other types of models using the embeddings as input.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s1600/Path%20+%20Derm%20-%20evaluate%20LP.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s16000/Path%20+%20Derm%20-%20evaluate%20LP.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Once the linear probe is trained, it can be used to make predictions on embeddings from new images. These predictions can be compared to ground truth information in order to evaluate the linear probe&#39;s performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In order to make this type of embedding model available and drive further development of ML tools in medical imaging, we are excited to release two domain-specific tools for research use: &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; and &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/path-foundation&quot;>;Path Foundation&lt;/a>;. This follows on the strong response we&#39;ve already received from researchers using the &lt;a href=&quot;https://blog.research.google/2022/07/simplified-transfer-learning-for-chest.html&quot;>;CXR Foundation&lt;/a>; embedding tool for chest radiographs and represents a portion of our expanding research offerings across multiple medical-specialized modalities. These embedding tools take an image as input and produce a numerical vector (the embedding) that is specialized to the domains of dermatology and digital pathology images, respectively. By running a dataset of chest X-ray, dermatology, or pathology images through the respective embedding tool, researchers can obtain embeddings for their own images, and use these embeddings to quickly develop new models for their applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Path Foundation&lt;/h2>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2310.13259&quot;>;Domain-specific optimization and diverse evaluation of self-supervised models for histopathology&lt;/a>;”, we showed that self-supervised learning (SSL) models for pathology images outperform traditional pre-training approaches and enable efficient training of classifiers for downstream tasks. This effort focused on &lt;a href=&quot;https://en.wikipedia.org/wiki/H%26E_stain&quot;>;hematoxylin and eosin&lt;/a>; (H&amp;amp;E) stained slides, the principal tissue stain in diagnostic pathology that enables pathologists to visualize cellular features under a microscope. The performance of linear classifiers trained using the output of the SSL models matched that of prior DL models trained on orders of magnitude more labeled data. &lt;/p>; &lt;p>; Due to substantial differences between digital pathology images and “natural image” photos, this work involved several pathology-specific optimizations during model training. One key element is that &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/&quot;>;whole-slide images&lt;/a>; (WSIs) in pathology can be 100,000 pixels across (thousands of times larger than typical smartphone photos) and are analyzed by experts at multiple magnifications (zoom levels). As such, the WSIs are typically broken down into smaller tiles or patches for computer vision and DL applications. The resulting images are information dense with cells or tissue structures distributed throughout the frame instead of having distinct semantic objects or foreground vs. background variations, thus creating unique challenges for robust SSL and feature extraction. Additionally, physical (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Microtome&quot;>;cutting&lt;/a>;) and chemical (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Fixation_(histology)&quot;>;fixing&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Staining&quot;>;staining&lt;/a>;) processes used to prepare the samples can influence image appearance dramatically. &lt;/p>; &lt;p>; Taking these important aspects into consideration, pathology-specific SSL optimizations included helping the model learn &lt;a href=&quot;https://arxiv.org/abs/2206.12694&quot;>;stain-agnostic features&lt;/a>;, generalizing the model to patches from multiple magnifications, &lt;a href=&quot;https://blog.research.google/2020/02/generating-diverse-synthetic-medical.html&quot;>;augmenting&lt;/a>; the data to mimic scanning and image post processing, and custom data balancing to improve input heterogeneity for SSL training. These approaches were extensively evaluated using a broad set of benchmark tasks involving 17 different tissue types over 12 different tasks. &lt;/p>; &lt;p>; Utilizing the vision transformer (&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot;>;ViT-S/16&lt;/a>;) architecture, Path Foundation was selected as the best performing model from the optimization and evaluation process described above (and illustrated in the figure below). This model thus provides an important balance between performance and model size to enable valuable and scalable use in generating embeddings over the many individual image patches of large pathology WSIs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s1999/Path%20+%20Derm% 20SSL.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1097&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s16000/Path%20+%20Derm%20SSL.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SSL training with pathology-specific optimizations for Path Foundation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The value of domain-specific image representations can also be seen in the figure below, which shows the linear probing performance improvement of Path Foundation (as measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;>; AUROC&lt;/a>;) compared to traditional pre-training on natural images (&lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;ImageNet-21k&lt;/a>;). This includes evaluation for tasks such as &lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2665774&quot;>;metastatic breast cancer detection in lymph nodes&lt;/a>;, &lt;a href=&quot;https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225&quot;>;prostate cancer grading&lt;/a>;, and &lt;a href=&quot;https://www.nature.com/articles/s41523-022-00478-y&quot;>;breast cancer grading&lt;/a>;, among others. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s1999/Path %20+%20Derm%20embeddings.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;890&quot; data-original-width=&quot; 1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s16000/Path%20+%20Derm%20embeddings.png&quot; />;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Path Foundation embeddings significantly outperform traditional ImageNet embeddings as evaluated by linear probing across multiple evaluation tasks in histopathology .&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Derm Foundation&lt;/ h2>; &lt;p>; &lt;a href=&quot;https://github.com/Google-Health/imaging-research/tree/master/derm-foundation&quot;>;Derm Foundation&lt;/a>; is an embedding tool derived from our research in applying DL to &lt;a href=&quot;https://blog.research.google/2019/09/using-deep-learning-to-inform.html&quot;>;interpret images of dermatology conditions&lt;/a>; and includes our recent work that adds &lt;a href=&quot;https://arxiv.org/abs/2402.15566&quot;>;improvements to generalize better to new datasets&lt;/a>;. Due to its dermatology-specific pre-training it has a latent understanding of features present in images of skin conditions and can be used to quickly develop models to classify skin conditions. The model underlying the API is a &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>;BiT ResNet-101x3&lt;/a>; trained in two stages. The first pre-training stage uses contrastive learning, similar to &lt;a href=&quot;https://arxiv.org/abs/2010.00747&quot;>;ConVIRT&lt;/a>;, to train on a large number of image-text pairs &lt;a href=&quot;https://blog.research.google/2017/07/revisiting-unreasonable-effectiveness.html&quot;>;from the internet&lt;/a>;. In the second stage, the image component of this pre-trained model is then fine-tuned for condition classification using clinical datasets, such as those from teledermatology services. &lt;/p>; &lt;p>; Unlike histopathology images, dermatology images more closely resemble the real-world images used to train many of today&#39;s computer vision models. However, for specialized dermatology tasks, creating a high-quality model may still require a large dataset. With Derm Foundation, researchers can use their own smaller dataset to retrieve domain-specific embeddings, and use those to build smaller models (eg, linear classifiers or other small non-linear models) that enable them to validate their research or product ideas. To evaluate this approach, we trained models on a downstream task using teledermatology data. Model training involved varying dataset sizes (12.5%, 25%, 50%, 100%) to compare embedding-based linear classifiers against fine-tuning. &lt;/p>; &lt;p>; The modeling variants considered were: &lt;/p>; &lt;ul>; &lt;li>;A linear classifier on frozen embeddings from &lt;a href=&quot;https://github.com/google-research/big_transfer&quot;>;BiT-M&lt;/a>; (a standard pre-trained image model) &lt;/li>;&lt;li>;Fine-tuned version of BiT-M with an extra dense layer for the downstream task &lt;/li>;&lt;li>;A linear classifier on frozen embeddings from the Derm Foundation API &lt;/li>;&lt;li>;Fine-tuned version of the model underlying the Derm Foundation API with an extra layer for the downstream task &lt;/li>; &lt;/ul>; &lt;p>; We found that models built on top of the Derm Foundation embeddings for dermatology-related tasks achieved significantly higher quality than those built solely on embeddings or fine tuned from BiT-M. This advantage was found to be most pronounced for smaller training dataset sizes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s1240/Path%20+%20Derm%20task% 20accuracy.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;842&quot; data-original-width=&quot;1240&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s16000/Path%20+%20Derm%20task%20accuracy.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;These results demonstrate that the Derm Foundation tooI can serve as a useful starting point to accelerate skin-related modeling tasks. We aim to enable other researchers to build on the underlying features and representations of dermatology that the model has learned. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; However, there are limitations with this analysis. We&#39;re still exploring how well these embeddings generalize across task types, patient populations, and image settings. Downstream models built using Derm Foundation still require careful evaluation to understand their expected performance in the intended setting. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Access Path and Derm Foundation&lt;/h2>; &lt;p>; We envision that the Derm Foundation and Path Foundation embedding tools will enable a range of use cases, including efficient development of models for diagnostic tasks, quality assurance and pre-analytical workflow improvements, image indexing and curation, and biomarker discovery and validation. We are releasing both tools to the research community so they can explore the utility of the embeddings for their own dermatology and pathology data. &lt;/p>; &lt;p>; To get access, please sign up to each tool&#39;s terms of service using the following Google Forms. &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSe5icNBzU_lO2CwjLLIOwbqIcWnJC-m4Sl7MgvI9Lng3QT6Zg/viewform?resourcekey=0-dahJtiVe2CqYkNEdWPcXgw&quot;>;Derm Foundation Access Form&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://docs.google.com/forms/d/1auyo2VkzlzuiAXavZy1AWUyQHAqO7T3BLK-7ofKUvug/edit?resourcekey=0-Z9pRxjDI-kaDEUIiNfMAWQ#question=1168037695&amp;field=173852432&quot;>;Path Foundation Access Form&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>; After gaining access to each tool, you can use the API to retrieve embeddings from dermatology images or digital pathology images stored in Google Cloud. Approved users who are just curious to see the model and embeddings in action can use the provided example Colab notebooks to train models using public data for classifying &lt;a href=&quot;https://github.com/Google-Health/imaging-research/blob/master/derm-foundation/derm_foundation_demo.ipynb&quot;>;six common skin conditions&lt;/a>; or identifying tumors in &lt;a href=&quot;https://github.com/Google-Health/imaging-research/blob/master/path-foundation/linear-classifier-demo.ipynb&quot;>;histopathology patches&lt;/a>;. We look forward to seeing the range of use-cases these tools can unlock. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the many collaborators who helped make this work possible including Yun Liu, Can Kirmizi, Fereshteh Mahvar, Bram Sterling, Arman Tajback, Kenneth Philbrik, Arnav Agharwal, Aurora Cheung, Andrew Sellergren, Boris Babenko, Basil Mustafa, Jan Freyberg, Terry Spitz, Yuan Liu, Pinal Bavishi, Ayush Jain, Amit Talreja, Rajeev Rikhye, Abbi Ward, Jeremy Lai, Faruk Ahmed, Supriya Vijay,Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni Agarwal, Ellery Wulczyn, Jonathan Krause, Fayaz Jamil, Tom Small, Annisah Um&#39;rani, Lauren Winer, Sami Lachgar, Yossi Matias, Greg Corrado, and Dale Webster.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1106624361649572376/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1106624361649572376&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1106624361649572376&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/health-specific-embedding-tools-for.html&quot; rel=&quot;alternate&quot; title=&quot;Health-specific embedding tools for dermatology and pathology&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s72-c/Path%20+%20Derm%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1765359719068432739&lt;/id>;&lt;published>;2024-03-07T10:15:00.000-08:00&lt;/published>;&lt;updated>;2024-03-07T10:19:31.177-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Social learning: Collaborative learning with large language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Amirkeivan Mohtashami, Research Intern, and Florian Hartmann, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs) have significantly improved the state of the art for solving tasks specified using natural language, often reaching performance close to that of people. As these models increasingly enable assistive agents, it could be beneficial for them to learn effectively from each other, much like people do in social settings, which would allow LLM-based agents to improve each other&#39;s performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To discuss the learning processes of humans, Bandura and Walters &lt;a href=&quot;https://books.google.ch/books/about/Social_Learning_Theory.html?id=IXvuAAAAMAAJ&amp;amp;redir_esc=y&quot;>;described&lt;/a>; the concept of &lt;em>;social learning&lt;/em>; in 1977, outlining different models of observational learning used by people. One common method of learning from others is through a &lt;em>;verbal instruction&lt;/em>; (eg, from a teacher) that describes how to engage in a particular behavior. Alternatively, learning can happen through a &lt;em>;live model&lt;/em>; by mimicking a live example of the behavior. &lt;/p>; &lt;p>; Given the success of LLMs mimicking human communication, in our paper “&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;Social Learning: Towards Collaborative Learning with Large Language Models&lt;/a>;”, we investigate whether LLMs are able to learn from each other using social learning. To this end, we outline a framework for social learning in which LLMs share knowledge with each other in a privacy-aware manner using natural language. We evaluate the effectiveness of our framework on various datasets, and propose quantitative methods that measure privacy in this setting. In contrast to previous approaches to collaborative learning, such as common &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;federated learning&lt;/a>; approaches that often rely on gradients, in our framework, agents teach each other purely using natural language. &lt;/p>; &lt;br />; &lt;h2>;Social learning for LLMs&lt;/h2>; &lt;p>; To extend social learning to language models, we consider the scenario where a student LLM should learn to solve a task from multiple teacher entities that already know that task. In our paper, we evaluate the student&#39;s performance on a variety of tasks, such as &lt;a href=&quot;https://dl.acm.org/doi/10.1145/2034691.2034742&quot;>;spam detection&lt;/a>; in short text messages (SMS), solving &lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;grade school math problems&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/1905.10044&quot;>;answering questions&lt;/a>; based on a given text. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s900/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;381&quot; data-original-width=&quot;900&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visualization of the social learning process: A teacher model provides instructions or few-shot examples to a student model without sharing its private data.&lt;/td>;&lt; /tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Language models have shown a remarkable capacity to perform tasks given only a handful of examples–a process called &lt;a href=&quot;https://arxiv.org/abs/2005.14165 &quot;>;few-shot learning&lt;/a>;. With this in mind, we provide human-labeled examples of a task that enables the teacher model to teach it to a student. One of the main use cases of social learning arises when these examples cannot be directly shared with the student due, for example, to privacy concerns. &lt;/p>; &lt;p>; To illustrate this, let&#39;s look at a hypothetical example for a spam detection task. A teacher model is located on device where some users volunteer to mark incoming messages they receive as either “spam” or “not spam”. This is useful data that could help train a student model to differentiate between spam and not spam, but sharing personal messages with other users is a breach of privacy and should be avoided. To prevent this, a social learning process can transfer the knowledge from the teacher model to the student so it learns what spam messages look like without needing to share the user&#39;s personal text messages. &lt;/p>; &lt;p>; We investigate the effectiveness of this social learning approach by analogy with the established human social learning theory that we discussed above. In these experiments, we use &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-S&lt;/a>; models for both the teacher and the student. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1117&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;A systems view of social learning: At training time, multiple teachers teach the student. At inference time, the student is using what it learned from the teachers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Synthetic examples&lt;/h3>; &lt;p>; As a counterpart to the live teaching model described for traditional social learning, we propose a learning method where the teachers generate new synthetic examples for the task and share them with the student. This is motivated by the idea that one can create a new example that is sufficiently different from the original one, but is just as educational. Indeed, we observe that our generated examples are sufficiently different from the real ones to preserve privacy while still enabling performance comparable to that achieved using the original examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s1456/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;The 8 generated examples perform as well as the original data for several tasks (see our&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;paper &lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We evaluate the efficacy of learning through synthetic examples on our task suite. Especially when the number of examples is high enough, eg, n = 16, we observe no statistically significant difference between sharing original data and teaching with synthesized data via social learning for the majority of tasks, indicating that the privacy improvement does not have to come at the cost of model quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHr8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s1456/image4 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1456&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHr8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Generating 16 instead of just 8 examples further reduces the performance gap relative to the original examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;br />; &lt;p>; The one exception is spam detection, for which teaching with synthesized data yields lower accuracy. This may be because the training procedure of current models makes them biased to only generate non-spam examples. In the &lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;paper&lt;/a>;, we additionally look into aggregation methods for selecting good subsets of examples to use. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Synthetic instruction&lt;/h3>; &lt;p>; Given the success of language models in following instructions, the verbal instruction model can also be naturally adapted to language models by having the teachers generate an instruction for the task. Our experiments show that providing such a generated instruction effectively improves performance over zero-shot prompting, reaching accuracies comparable to few-shot prompting with original examples. However, we did find that the teacher model may fail on certain tasks to provide a good instruction, for example due to a complicated formatting requirement of the output. &lt;/p>; &lt;p>; For &lt;a href=&quot;https://arxiv.org/abs/1606.06031&quot;>;Lambada&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2110.14168&quot;>;GSM8k&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;Random Insertion&lt;/a>;, providing synthetic examples performs better than providing generated instructions, whereas in the other tasks generated instruction obtains a higher accuracy. This observation suggests that the choice of the teaching model depends on the task at hand, similar to how the most effective method for teaching people varies by task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s1451/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;880&quot; data-original-width=&quot;1451&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Depending on the task, generating instructions can work better than generating new examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Memorization of the private examples&lt;/h2>; &lt;p>; We want teachers in social learning to teach the student without revealing specifics from the original data. To quantify how prone this process is to leaking information, we used &lt;a href=&quot;https://research.google/pubs/the-secret-sharer-evaluating-and-testing-unintended-memorization-in-neural-networks/&quot;>;Secret Sharer&lt;/a>;, a popular method for quantifying to what extent a model memorizes its training data, and adapted it to the social learning setting. We picked this method since it had previously been &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;used&lt;/a>; for evaluating memorization in federated learning 。 &lt;/p>; &lt;p>; To apply the Secret Sharer method to social learning, we design “canary” data points such that we can concretely measure how much the training process memorized them. These data points are included in the datasets used by teachers to generate new examples. After the social learning process completes, we can then measure how much more confident the student is in the secret data points the teacher used, compared to similar ones that were not shared even with the teachers. &lt;/p>; &lt;p>; In our analysis, discussed in detail in the &lt;a href=&quot;https://arxiv.org/abs/2312.11441&quot;>;paper&lt;/a>;, we use canary examples that include names and codes. Our results show that the student is only slightly more confident in the canaries the teacher used. In contrast, when the original data points are directly shared with the student, the confidence in the included canaries is much higher than in the held-out set. This supports the conclusion that the teacher does indeed use its data to teach without simply copying it over. &lt;/p>; &lt;br />; &lt;h2>;Conclusion and next steps&lt;/h2>; &lt;p>; We introduced a framework for social learning that allows language models with access to private data to transfer knowledge through textual communication while maintaining the privacy of that数据。 In this framework, we identified sharing examples and sharing instructions as basic models and evaluated them on multiple tasks. Furthermore, we adapted the Secret Sharer metric to our framework, proposing a metric for measuring data leakage. &lt;/p>; &lt;p>; As next steps, we are looking for ways of improving the teaching process, for example by adding feedback loops and iteration. Furthermore, we want to investigate using social learning for modalities other than text. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to acknowledge and thank Matt Sharifi, Sian Gooding, Lukas Zilka, and Blaise Aguera y Arcas, who are all co-authors on the paper. Furthermore, we would like to thank Victor Cărbune, Zachary Garrett, Tautvydas Misiunas, Sofia Neata and John Platt for their feedback, which greatly improved the paper. We&#39;d also like to thank Tom Small for creating the animated figure.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1765359719068432739/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1765359719068432739&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/social-learning-collaborative-learning.html&quot; rel=&quot;alternate&quot; title=&quot;Social learning: Collaborative learning with large language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8393293208018757284&lt;/id>;&lt;published>;2024-03-06T10:26:00.000-08:00&lt;/published>;&lt;updated>;2024-03-06T14:44:03.387-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Croissant: a metadata format for ML-ready datasets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Omar Benjelloun, Software Engineer, Google Research, and Peter Mattson, Software Engineer, Google Core ML and President, MLCommons Association&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s1600/CroissantHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Machine learning (ML) practitioners looking to reuse existing datasets to train an ML model often spend a lot of time understanding the data, making sense of its organization, or figuring out what subset to use as features. So much time, in fact, that progress in the field of ML is hampered by a fundamental obstacle: the wide variety of data representations. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML datasets cover a broad range of content types, from text and structured data to images, audio, and video. Even within datasets that cover the same types of content, every dataset has a unique &lt;em>;ad hoc&lt;/em>; arrangement of files and data formats. This challenge reduces productivity throughout the entire ML development process, from finding the data to training the model. It also impedes development of badly needed tooling for working with datasets. &lt;/p>; &lt;p>; There are general purpose metadata formats for datasets such as &lt;a href=&quot;http://schema.org/Dataset&quot;>;schema.org&lt;/a>; and &lt;a href=&quot;https://www.w3.org/TR/vocab-dcat-3/&quot;>;DCAT&lt;/a>;. However, these formats were designed for data discovery rather than for the specific needs of ML data, such as the ability to extract and combine data from structured and unstructured sources, to include metadata that would enable &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;responsible use&lt;/a>; of the data, or to describe ML usage characteristics such as defining training, test and validation sets. &lt;/p>; &lt;p>; Today, we&#39;re introducing &lt;a href=&quot;https://mlcommons.org/croissant&quot;>;Croissant&lt;/a>;, a new metadata format for ML-ready datasets. Croissant was developed collaboratively by a community from industry and academia, as part of the &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; effort. The Croissant format doesn&#39;t change how the actual data is represented (eg, image or text file formats) — it provides a standard way to describe and organize it. Croissant builds upon &lt;a href=&quot;https://schema.org/&quot;>;schema.org&lt;/a>;, the de facto standard for publishing structured data on the Web, which is already used by over 40M datasets. Croissant augments it with comprehensive layers for ML relevant metadata, data resources, data organization, and default ML semantics. &lt;/p>; &lt;p>; In addition, we are announcing support from major tools and repositories: Today, three widely used collections of ML datasets — &lt;a href=&quot;http://www.kaggle.com/datasets&quot;>;Kaggle&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets?other=croissant&amp;amp;sort=trending&quot;>;Hugging Face&lt;/a>;, and &lt;a href=&quot;https://openml.org/search?type=data&quot;>;OpenML&lt;/a>; — will begin supporting the Croissant format for the datasets they host; the &lt;a href=&quot;http://g.co/datasetsearch&quot;>;Dataset Search&lt;/a>; tool lets users search for Croissant datasets across the Web; and popular ML frameworks, including &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, and &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;, can load Croissant datasets easily using the &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>;TensorFlow Datasets&lt;/a>; (TFDS) package. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Croissant&lt;/h2>; &lt;p>; This 1.0 release of Croissant includes a complete &lt;a href=&quot;https://mlcommons.org/croissant/1.0&quot;>;specification&lt;/a>; of the format, a set of &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/datasets&quot;>;example datasets&lt;/a>;, an open source &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/python/mlcroissant&quot;>;Python library&lt;/a>; to validate, consume and generate Croissant metadata, and an open source &lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>;visual editor&lt;/a>; to load, inspect and create Croissant dataset descriptions in an intuitive way. &lt;/p>; &lt;p>; Supporting Responsible AI (RAI) was a key goal of the Croissant effort from the start. We are also releasing the first version of the &lt;a href=&quot;https://mlcommons.org/croissant/RAI/1.0&quot;>;Croissant RAI vocabulary&lt;/a>; extension, which augments Croissant with key properties needed to describe important RAI use cases such as data life cycle management, data labeling, participatory data, ML safety and fairness evaluation, explainability, and compliance. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Why a shared format for ML data?&lt;/h2>; &lt;p>; The majority of ML work is actually data work. The training data is the “code” that determines the behavior of a model. Datasets can vary from a collection of text used to train a large language model (LLM) to a collection of driving scenarios (annotated videos) used to train a car&#39;s collision avoidance system. However, the steps to develop an ML model typically follow the same iterative data-centric process: (1) find or collect data, (2) clean and refine the data, (3) train the model on the data, (4) test the model on more data, (5) discover the model does not work, (6) analyze the data to find out why, (7) repeat until a workable model is achieved. Many steps are made harder by the lack of a common format. This “data development burden” is especially heavy for resource-limited research and early-stage entrepreneurial efforts. &lt;/p>; &lt;p>; The goal of a format like Croissant is to make this entire process easier. For instance, the metadata can be leveraged by search engines and dataset repositories to make it easier to find the right dataset. The data resources and organization information make it easier to develop tools for cleaning, refining, and analyzing data. This information and the default ML semantics make it possible for ML frameworks to use the data to train and test models with a minimum of code. Together, these improvements substantially reduce the data development burden. &lt;/p>; &lt;p>; Additionally, dataset authors care about the discoverability and ease of use of their datasets. Adopting Croissant improves the value of their datasets, while only requiring a minimal effort, thanks to the available creation tools and support from ML data platforms. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What can Croissant do today?&lt;/h2>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;908&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Croissant ecosystem: Users can Search for Croissant datasets, download them from major repositories, and easily load them into their favorite ML frameworks. They can create, inspect and modify Croissant metadata using the Croissant editor.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Today, users can find Croissant datasets at: &lt;/p>; &lt;ul>; &lt;li>;Google &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;>;Dataset Search&lt;/a>;, which offers a Croissant filter. &lt;/li>;&lt;li>;&lt;a href=&quot;https://huggingface.co/datasets?other=croissant&amp;amp;sort=trending&quot;>;HuggingFace&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;http://kaggle.com/datasets&quot;>;Kaggle&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://openml.org/search?type=data&quot;>;OpenML&lt;/a>; &lt;/li>; &lt;/ul>; &lt;p>; With a Croissant dataset, it is possible to: &lt;/p>; &lt;ul>; &lt;li>;Ingest data easily via &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>;TensorFlow Datasets&lt;/a>; for use in popular ML frameworks like &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, and &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;. &lt;/li>;&lt;li>;Inspect and modify the metadata using the &lt;a href=&quot;https://huggingface.co/spaces/MLCommons/croissant-editor&quot;>;Croissant editor UI&lt;/a>; (&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>;github&lt;/a>;). &lt;/li>; &lt;/ul>; &lt;p>; To publish a Croissant dataset, users can: &lt;/p>; &lt;ul>; &lt;li>;Use the &lt;a href=&quot;https://huggingface.co/spaces/MLCommons/croissant-editor&quot;>;Croissant editor UI&lt;/a>; (&lt;a href=&quot;https://github.com/mlcommons/croissant/tree/main/editor&quot;>;github&lt;/a>;) to generate a large portion of Croissant metadata automatically by analyzing the data the user provides, and to fill important metadata fields such as RAI properties. &lt;/li>;&lt;li>;Publish the Croissant information as part of their dataset Web page to make it discoverable and reusable. &lt;/li>;&lt;li>;Publish their data in one of the repositories that support Croissant, such as Kaggle, HuggingFace and OpenML, and automatically generate Croissant metadata. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; We are excited about Croissant&#39;s potential to help ML practitioners, but making this format truly useful requires the support of the community. We encourage dataset creators to consider providing Croissant metadata. We encourage platforms hosting datasets to provide Croissant files for download and embed Croissant metadata in dataset Web pages so that they can be made discoverable by dataset search engines. Tools that help users work with ML datasets, such as labeling or data analysis tools should also consider supporting Croissant datasets. Together, we can reduce the data development burden and enable a richer ecosystem of ML research and development. &lt;/p>; &lt;p>; We encourage the community to &lt;a href=&quot;http://mlcommons.org/croissant&quot;>;join us&lt;/a>; in contributing to the effort. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Croissant was developed by the &lt;a href=&quot;https://datasetsearch.research.google.com/&quot;>;Dataset Search&lt;/a>;, &lt;a href=&quot;https://www.kaggle.com/&quot;>;Kaggle&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/datasets&quot;>;TensorFlow Datasets&lt;/a>; teams from Google, as part of an &lt;a href=&quot;http://mlcommons.org&quot;>;MLCommons&lt;/a>; community working group, which also includes contributors from these organizations: Bayer, cTuning Foundation, DANS-KNAW, Dotphoton, Harvard, Hugging Face, Kings College London, LIST, Meta, NASA, North Carolina State University, Open Data Institute, Open University of Catalonia, Sage Bionetworks, and TU Eindhoven.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8393293208018757284/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8393293208018757284&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8393293208018757284&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html&quot; rel=&quot;alternate&quot; title=&quot;Croissant: a metadata format for ML-ready datasets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s72-c/CroissantHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2754526782497247497&lt;/id>;&lt;published>;2024-03-04T07:06:00.000-08:00&lt;/published>;&lt;updated>;2024-03-05T08:40:45.490-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at APS 2024&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kate Weber and Shannon Leon, Google Research, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s1600/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Today the &lt;a href=&quot;https://www.aps.org/meetings/meeting.cfm?name=MAR24&quot;>;2024 March Meeting&lt;/a>; of the &lt;a href=&quot;https://www.aps.org/&quot;>;American Physical Society&lt;/a>; (APS) kicks off in Minneapolis, MN. A premier conference on topics ranging across physics and related fields, APS 2024 brings together researchers, students, and industry professionals to share their discoveries and build partnerships with the goal of realizing fundamental advances in physics-related sciences and technology. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; This year, Google has a strong presence at APS with a booth hosted by the Google &lt;a href=&quot;https://quantumai.google/&quot;>;Quantum AI&lt;/a>; team, 50+ talks throughout the conference, and participation in conference organizing activities, special sessions and events. Attending APS 2024 in person? Come visit Google&#39;s Quantum AI booth to learn more about the exciting work we&#39;re doing to solve some of the field&#39;s most interesting challenges. &lt;!--Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions).-->; &lt;/p>; &lt;p>; You can learn more about the latest cutting edge work we are presenting at the conference along with our schedule of booth events below (Googlers listed in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Session Chairs include: &lt;strong>;Aaron Szasz&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Booth Activities&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;em>;This schedule is subject to change. Please visit the Google Quantum AI booth for more information.&lt;/em>; &lt;/p>; &lt;p>; Crumble: A prototype interactive tool for visualizing QEC circuits &lt;br />; Presenter: &lt;strong>;Matt McEwen&lt;/strong>; &lt;br />; Tue, Mar 5 | 11:00 AM CST &lt;/p>; &lt;p>; Qualtran: An open-source library for effective resource estimation of fault tolerant algorithms &lt;br />; Presenter: &lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; Tue, Mar 5 | 2:30 PM CST &lt;/p>; &lt;p>; Qualtran: An open-source library for effective resource estimation of fault tolerant algorithms &lt;br />; Presenter: &lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; Thu, Mar 7 | 11:00 AM CST &lt;/p>; &lt;p>; $5M XPRIZE / Google Quantum AI competition to accelerate quantum applications Q&amp;amp;A &lt;br />; Presenter: &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; Thu, Mar 7 | 11:00 AM CST &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Talks&lt;/h2>; &lt;h3>;Monday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/A45.1&quot;>;Certifying highly-entangled states from few single-qubit measurements&lt;/a>; &lt;br />; Presenter: &lt;strong>;Hsin-Yuan Huang&lt;/strong>; &lt;br />; Author: &lt;strong>;Hsin-Yuan Huang&lt;/strong>; &lt;br />; &lt;em>;Session A45: New Frontiers in Machine Learning Quantum Physics&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/A51.2&quot;>;Toward high-fidelity analog quantum simulation with superconducting qubits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Trond Andersen&lt;/strong>; &lt;br />; Authors: &lt;strong>;Trond I Andersen&lt;/strong>;, &lt;strong>;Xiao Mi&lt;/strong>;, &lt;strong>;Amir H Karamlou&lt;/strong>;, &lt;strong>;Nikita Astrakhantsev&lt;/strong>;, &lt;strong>;Andrey Klots&lt;/strong>;, &lt;strong>;Julia Berndtsson&lt;/strong>;, &lt;strong>;Andre Petukhov&lt;/strong>;, &lt;strong>;Dmitry Abanin&lt;/strong>;, &lt;strong>;Lev B Ioffe&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;Session A51: Applications on Noisy Quantum Hardware I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B50.6&quot;>;Measuring circuit errors in context for surface code circuits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Dripto M Debroy&lt;/strong>; &lt;br />; Authors: &lt;strong>;Dripto M Debroy&lt;/strong>;, &lt;strong>;Jonathan A Gross&lt;/strong>;, &lt;strong>;Élie Genois&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>; &lt;br />; &lt;em>;Session B50: Characterizing Noise with QCVV Techniques&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.6&quot;>;Quantum computation of stopping power for inertial fusion target design I: Physics overview and the limits of classical algorithms&lt;/a>; &lt;br />; Presenter: Andrew D. Baczewski &lt;br />; Authors: &lt;strong>;Nicholas C. Rubin&lt;/strong>;, Dominic W. Berry, Alina Kononov, &lt;strong>;Fionn D. Malone&lt;/strong>;, &lt;strong>;Tanuj Khattar&lt;/strong>;, Alec White, &lt;strong>;Joonho Lee&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>;, &lt;strong>;Ryan Babbush&lt;/strong>;, Andrew D. Baczewski &lt;br />; &lt;em>;Session B51: Heterogeneous Design for Quantum Applications&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.12352.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B51.7&quot;>;Quantum computation of stopping power for inertial fusion target design II: Physics overview and the limits of classical algorithms&lt;/a>; &lt;br />; Presenter: &lt;strong>;Nicholas C. Rubin&lt;/strong>; &lt;br />; Authors: &lt;strong>;Nicholas C. Rubin&lt;/strong>;, Dominic W. Berry, Alina Kononov, &lt;strong>;Fionn D. Malone&lt;/strong>;, &lt;strong>;Tanuj Khattar&lt;/strong>;, Alec White, &lt;strong>;Joonho Lee&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>;, &lt;strong>;Ryan Babbush&lt;/strong>;, Andrew D. Baczewski &lt;br />; &lt;em>;Session B51: Heterogeneous Design for Quantum Applications&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.12352.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B56.4&quot;>;Calibrating Superconducting Qubits: From NISQ to Fault Tolerance&lt;/a>; &lt;br />; Presenter: &lt;strong>;Sabrina S Hong&lt;/strong>; &lt;br />; Author: &lt;strong>;Sabrina S Hong&lt;/strong>; &lt;br />; &lt;em>;Session B56: From NISQ to Fault Tolerance&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B31.9&quot;>;Measurement and feedforward induced entanglement negativity transition&lt;/a>; &lt;br />; Presenter: &lt;strong>;Ramis Movassagh&lt;/strong>; &lt;br />; Authors: Alireza Seif, Yu-Xin Wang,&lt;strong>; Ramis Movassagh&lt;/strong>;, Aashish A. Clerk &lt;br />; &lt;em>;Session B31: Measurement Induced Criticality in Many-Body Systems&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2310.18305.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/B52.9&quot;>;Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments&lt;/a>; &lt;br />; Presenter: &lt;strong>;Salvatore Mandra&lt;/strong>; &lt;br />; Authors: &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Sergei V Isakov&lt;/strong>;, &lt;strong>;Salvatore Mandra&lt;/strong>;, &lt;strong>;Benjamin Villalonga&lt;/strong>;, &lt;strong>;X. Mi&lt;/strong>;, &lt;strong>;Sergio Boixo&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;Session B52: Quantum Algorithms and Complexity&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2306.15970.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/D60.4&quot;>;Accurate thermodynamic tables for solids using Machine Learning Interaction Potentials and Covariance of Atomic Positions&lt;/a>; &lt;br />; Presenter: Mgcini K Phuthi &lt;br />; Authors: Mgcini K Phuthi, Yang Huang, Michael Widom, &lt;strong>;Ekin D Cubuk&lt;/strong>;, Venkat Viswanathan &lt;br />; &lt;em>;Session D60: Machine Learning of Molecules and Materials: Chemical Space and Dynamics&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Tuesday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/F50.4&quot;>;IN-Situ Pulse Envelope Characterization Technique (INSPECT)&lt;/a>; &lt;br />; Presenter: &lt;strong>;Zhang Jiang&lt;/strong>; &lt;br />; Authors: &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Jonathan A Gross&lt;/strong>;, &lt;strong>;Élie Genois&lt;/strong>; &lt;br />; &lt;em>;Session F50: Advanced Randomized Benchmarking and Gate Calibration&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/F50.11&quot;>;Characterizing two-qubit gates with dynamical decoupling&lt;/a>; &lt;br />; Presenter: &lt;strong>;Jonathan A Gross&lt;/strong>; &lt;br />; Authors: &lt;strong>;Jonathan A Gross&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Élie Genois, Dripto M Debroy&lt;/strong>;, Ze-Pei Cian*, &lt;strong>;Wojciech Mruczkiewicz&lt;/strong>; &lt;br />; &lt;em>;Session F50: Advanced Randomized Benchmarking and Gate Calibration&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/EE01.2&quot;>;Statistical physics of regression with quadratic models&lt;/a>; &lt;br />; Presenter: Blake Bordelon &lt;br />; Authors: Blake Bordelon, Cengiz Pehlevan, &lt;strong>;Yasaman Bahri&lt;/strong>; &lt;br />; &lt;em>;Session EE01: V: Statistical and Nonlinear Physics II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G51.2&quot;>;Improved state preparation for first-quantized simulation of electronic structure&lt;/a>; &lt;br />; Presenter: &lt;strong>;William J Huggins&lt;/strong>; &lt;br />; Authors: &lt;strong>;William J Huggins&lt;/strong>;, &lt;strong>;Oskar Leimkuhler&lt;/strong>;, &lt;strong>;Torin F Stetina&lt;/strong>;, &lt;strong>;Birgitta Whaley&lt;/strong>; &lt;br />; &lt;em>;Session G51: Hamiltonian Simulation&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G30.2&quot;>;Controlling large superconducting quantum processors&lt;/a>; &lt;br />; Presenter: &lt;strong>;Paul V. Klimov&lt;/strong>; &lt;br />; Authors: &lt;strong>;Paul V. Klimov&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;Alexandre Bourassa&lt;/strong>;, &lt;strong>;Sabrina Hong&lt;/strong>;, &lt;strong>;Andrew Dunsworth&lt;/strong>;, &lt;strong>;Kevin J. Satzinger&lt;/strong>;, &lt;strong>;William P. Livingston&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Murphy Y. Niu&lt;/strong>;, &lt;strong>;Trond I. Andersen&lt;/strong>;, &lt;strong>;Yaxing Zhang&lt;/strong>;, &lt;strong>;Desmond Chik&lt;/strong>;, &lt;strong>;Zijun Chen&lt;/strong>;, &lt;strong>;Charles Neill&lt;/strong>;, &lt;strong>;Catherine Erickson&lt;/strong>;, &lt;strong>;Alejandro Grajales Dau&lt;/strong>;, &lt;strong>;Anthony Megrant&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>;, &lt;strong>;Alexander N. Korotkov&lt;/strong>;, &lt;strong>;Julian Kelly&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>; &lt;br />; &lt;em>;Session G30: Commercial Applications of Quantum Computing&lt;/em>;&lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2308.02321.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G50.5&quot;>;Gaussian boson sampling: Determining quantum advantage&lt;/a>; &lt;br />; Presenter: Peter D Drummond &lt;br />; Authors: Peter D Drummond, Alex Dellios, Ned Goodman, Margaret D Reid, &lt;strong>;Ben Villalonga&lt;/strong>; &lt;br />; &lt;em>;Session G50: Quantum Characterization, Verification, and Validation II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/G50.8&quot;>;Attention to complexity III: learning the complexity of random quantum circuit states&lt;/a>; &lt;br />; Presenter: Hyejin Kim &lt;br />; Authors: Hyejin Kim, Yiqing Zhou, Yichen Xu, Chao Wan, Jin Zhou, &lt;strong>;Yuri D Lensky&lt;/strong>;, Jesse Hoke, &lt;strong>;Pedram Roushan&lt;/strong>;, Kilian Q Weinberger, Eun-Ah Kim &lt;br />; &lt;em>;Session G50: Quantum Characterization, Verification, and Validation II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/K48.10&quot;>;Balanced coupling in superconducting circuits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Daniel T Sank&lt;/strong>; &lt;br />; Authors: &lt;strong>;Daniel T Sank&lt;/strong>;, &lt;strong>;Sergei V Isakov&lt;/strong>;, &lt;strong>;Mostafa Khezri&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>; &lt;br />; &lt;em>;Session K48: Strongly Driven Superconducting Systems&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/K49.12&quot;>;Resource estimation of Fault Tolerant algorithms using Qᴜᴀʟᴛʀᴀɴ&lt;/a>; &lt;br />; Presenter: &lt;strong>;Tanuj Khattar&lt;/strong>; &lt;br />; Author: &lt;strong>;Tanuj Khattar&lt;/strong>;, &lt;b>;Matthew Harrigan&lt;/b>;, &lt;b>;Fionn D. Malone&lt;/b>;, &lt;b>;Nour Yosri&lt;/b>;, &lt;b>;Nicholas C. Rubin&lt;/b>;&lt;br />; &lt;em>;Session K49: Algorithms and Implementations on Near-Term Quantum Computers&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Wednesday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/M24.1&quot;>;Discovering novel quantum dynamics with superconducting qubits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; Author: &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;Session M24: Analog Quantum Simulations Across Platforms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/M27.7&quot;>;Deciphering Tumor Heterogeneity in Triple-Negative Breast Cancer: The Crucial Role of Dynamic Cell-Cell and Cell-Matrix Interactions&lt;/a>; &lt;br />; Presenter: Susan Leggett &lt;br />; Authors: Susan Leggett, Ian Wong, Celeste Nelson, Molly Brennan, &lt;strong>;Mohak Patel&lt;/strong>;, Christian Franck, Sophia Martinez, Joe Tien, Lena Gamboa, Thomas Valentin, Amanda Khoo, Evelyn K Williams &lt;br />; &lt;em>;Session M27: Mechanics of Cells and Tissues II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48.2&quot;>;Toward implementation of protected charge-parity qubits&lt;/a>; &lt;br />; Presenter: Abigail Shearrow &lt;br />; Authors: Abigail Shearrow, Matthew Snyder, Bradley G Cole, Kenneth R Dodge, Yebin Liu, Andrey Klots, &lt;strong>;Lev B Ioffe&lt;/strong>;, Britton L Plourde, Robert McDermott &lt;br />; &lt;em>;Session N48: Unconventional Superconducting Qubits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N48.3&quot;>;Electronic capacitance in tunnel junctions for protected charge-parity qubits&lt;/a>; &lt;br />; Presenter: Bradley G Cole &lt;br />; Authors: Bradley G Cole, Kenneth R Dodge, Yebin Liu, Abigail Shearrow, Matthew Snyder, &lt;strong>;Andrey Klots&lt;/strong>;, &lt;strong>;Lev B Ioffe&lt;/strong>;, Robert McDermott, BLT Plourde &lt;br />; &lt;em>;Session N48: Unconventional Superconducting Qubits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.7&quot;>;Overcoming leakage in quantum error correction&lt;/a>; &lt;br />; Presenter: &lt;strong>;Kevin C. Miao&lt;/strong>; &lt;br />; Authors: &lt;strong>;Kevin C. Miao&lt;/strong>;, &lt;strong>;Matt McEwen&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>;, &lt;strong>;Dvir Kafri&lt;/strong>;, &lt;strong>;Leonid P. Pryadko&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>;Alex Opremcak&lt;/strong>;, &lt;strong>;Kevin J. Satzinger&lt;/strong>;, &lt;strong>;Zijun Chen&lt;/strong>;, &lt;strong>;Paul V. Klimov&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;Rajeev Acharya&lt;/strong>;, &lt;strong>;Kyle Anderson&lt;/strong>;, &lt;strong>;Markus Ansmann&lt;/strong>;, &lt;strong>;Frank Arute&lt;/strong>;, &lt;strong>;Kunal Arya&lt;/strong>;, &lt;strong>;Abraham Asfaw&lt;/strong>;, &lt;strong>;Joseph C. Bardin&lt;/strong>;, &lt;strong>;Alexandre Bourassa&lt;/strong>;, &lt;strong>;Jenna Bovaird&lt;/strong>;, &lt;strong>;Leon Brill&lt;/strong>;, &lt;strong>;Bob B. Buckley&lt;/strong>;, &lt;strong>;David A. Buell&lt;/strong>;, &lt;strong>;Tim Burger&lt;/strong>;, &lt;strong>;Brian Burkett&lt;/strong>;, &lt;strong>;Nicholas Bushnell&lt;/strong>;, &lt;strong>;Juan Campero&lt;/strong>;, &lt;strong>;Ben Chiaro&lt;/strong>;, &lt;strong>;Roberto Collins&lt;/strong>;, &lt;strong>;Paul Conner&lt;/strong>;, &lt;strong>;Alexander L. Crook&lt;/strong>;, &lt;strong>;Ben Curtin&lt;/strong>;, &lt;strong>;Dripto M. Debroy&lt;/strong>;, &lt;strong>;Sean Demura&lt;/strong>;, &lt;strong>;Andrew Dunsworth&lt;/strong>;, &lt;strong>;Catherine Erickson&lt;/strong>;, &lt;strong>;Reza Fatemi&lt;/strong>;, &lt;strong>;Vinicius S. Ferreira&lt;/strong>;, &lt;strong>;Leslie Flores Burgos&lt;/strong>;, &lt;strong>;Ebrahim Forati&lt;/strong>;, &lt;strong>;Austin G. Fowler&lt;/strong>;, &lt;strong>;Brooks Foxen&lt;/strong>;, &lt;strong>;Gonzalo Garcia&lt;/strong>;, &lt;strong>;William Giang&lt;/strong>;, &lt;strong>;Craig Gidney&lt;/strong>;, &lt;strong>;Marissa Giustina&lt;/strong>;, &lt;strong>;Raja Gosula&lt;/strong>;, &lt;strong>;Alejandro Grajales Dau&lt;/strong>;, &lt;strong>;Jonathan A. Gross&lt;/strong>;, &lt;strong>;Michael C. Hamilton&lt;/strong>;, &lt;strong>;Sean D. Harrington&lt;/strong>;, &lt;strong>;Paula Heu&lt;/strong>;, &lt;strong>;Jeremy Hilton&lt;/strong>;, &lt;strong>;Markus R. Hoffmann&lt;/strong>;, &lt;strong>;Sabrina Hong&lt;/strong>;, &lt;strong>;Trent Huang&lt;/strong>;, &lt;strong>;Ashley Huff&lt;/strong>;, &lt;strong>;Justin Iveland&lt;/strong>;, &lt;strong>;Evan Jeffrey&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>;, &lt;strong>;Julian Kelly&lt;/strong>;, &lt;strong>;Seon Kim&lt;/strong>;, &lt;strong>;Fedor Kostritsa&lt;/strong>;, &lt;strong>;John Mark Kreikebaum&lt;/strong>;, &lt;strong>;David Landhuis&lt;/strong>;, &lt;strong>;Pavel Laptev&lt;/strong>;, &lt;strong>;Lily Laws&lt;/strong>;, &lt;strong>;Kenny Lee&lt;/strong>;, &lt;strong>;Brian J. Lester&lt;/strong>;, &lt;strong>;Alexander T. Lill&lt;/strong>;, &lt;strong>;Wayne Liu&lt;/strong>;, &lt;strong>;Aditya Locharla&lt;/strong>;, &lt;strong>;Erik Lucero&lt;/strong>;, &lt;strong>;Steven Martin&lt;/strong>;, &lt;strong>;Anthony Megrant&lt;/strong>;, &lt;strong>;Xiao Mi&lt;/strong>;, &lt;strong>;Shirin Montazeri&lt;/strong>;, &lt;strong>;Alexis Morvan&lt;/strong>;, &lt;strong>;Ofer Naaman&lt;/strong>;, &lt;strong>;Matthew Neeley&lt;/strong>;, &lt;strong>;Charles Neill&lt;/strong>;, &lt;strong>;Ani Nersisyan&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Jiun How Ng&lt;/strong>;, &lt;strong>;Anthony Nguyen&lt;/strong>;, &lt;strong>;Murray Nguyen&lt;/strong>;, &lt;strong>;Rebecca Potter&lt;/strong>;, &lt;strong>;Charles Rocque&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>;, &lt;strong>;Kannan Sankaragomathi&lt;/strong>;, &lt;strong>;Christopher Schuster&lt;/strong>;, &lt;strong>;Michael J. Shearn&lt;/strong>;, &lt;strong>;Aaron Shorter&lt;/strong>;, &lt;strong>;Noah Shutty&lt;/strong>;, &lt;strong>;Vladimir Shvarts&lt;/strong>;, &lt;strong>;Jindra Skruzny&lt;/strong>;, &lt;strong>;W. Clarke Smith&lt;/strong>;, &lt;strong>;George Sterling&lt;/strong>;, &lt;strong>;Marco Szalay&lt;/strong>;, &lt;strong>;Douglas Thor&lt;/strong>;, &lt;strong>;Alfredo Torres&lt;/strong>;, &lt;strong>;Theodore White&lt;/strong>;, &lt;strong>;Bryan WK Woo&lt;/strong>;, &lt;strong>;Z. Jamie Yao&lt;/strong>;, &lt;strong>;Ping Yeh&lt;/strong>;, &lt;strong>;Juhwan Yoo&lt;/strong>;, &lt;strong>;Grayson Young&lt;/strong>;, &lt;strong>;Adam Zalcman&lt;/strong>;, &lt;strong>;Ningfeng Zhu&lt;/strong>;, &lt;strong>;Nicholas Zobrist&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, &lt;strong>;Andre Petukhov&lt;/strong>;, &lt;strong>;Alexander N. Korotkov&lt;/strong>;, &lt;strong>;Daniel Sank&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>; &lt;br />; &lt;em>;Session N51: Quantum Error Correction Code Performance and Implementation I&lt;/em>; &lt;br />; &lt;a href=&quot;https://www.nature.com/articles/s41567-023-02226-w&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.11&quot;>;Modeling the performance of the surface code with non-uniform error distribution: Part 1&lt;/a>; &lt;br />; Presenter: &lt;strong>;Yuri D Lensky&lt;/strong>; &lt;br />; Authors: &lt;strong>;Yuri D Lensky&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Igor Aleiner&lt;/strong>; &lt;br />; &lt;em>;Session N51: Quantum Error Correction Code Performance and Implementation I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/N51.12&quot;>;Modeling the performance of the surface code with non-uniform error distribution: Part 2&lt;/a>; &lt;br />; Presenter: &lt;strong>;Volodymyr Sivak&lt;/strong>; &lt;br />; Authors: &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>;, &lt;strong>;Henry Schurkus&lt;/strong>;, &lt;strong>;Dvir Kafri&lt;/strong>;, &lt;strong>;Yuri D Lensky&lt;/strong>;, &lt;strong>;Paul Klimov&lt;/strong>;, &lt;strong>;Kostyantyn Kechedzhi&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>; &lt;br />; &lt;em>;Session N51: Quantum Error Correction Code Performance and Implementation I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q51.7&quot;>;Highly optimized tensor network contractions for the simulation of classically challenging quantum computations&lt;/a>; &lt;br />; Presenter: &lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; Author: &lt;strong>;Benjamin Villalonga&lt;/strong>; &lt;br />; &lt;em>;Session Q51: Co-evolution of Quantum Classical Algorithms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Q61.7&quot;>;Teaching modern quantum computing concepts using hands-on open-source software at all levels&lt;/a>; &lt;br />; Presenter: &lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; Author: &lt;strong>;Abraham Asfaw&lt;/strong>; &lt;br />; &lt;em>;Session Q61: Teaching Quantum Information at All Levels II&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Thursday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.1&quot;>;New circuits and an open source decoder for the color code&lt;/a>; &lt;br />; Presenter: &lt;strong>;Craig Gidney&lt;/strong>; &lt;br />; Authors: &lt;strong>;Craig Gidney&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;Session S51: Quantum Error Correction Code Performance and Implementation II&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2312.08813.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S18.2&quot;>;Performing Hartree-Fock many-body physics calculations with large language models&lt;/a>; &lt;br />; Presenter: &lt;strong>;Eun-Ah Kim&lt;/strong>; &lt;br />; Authors: &lt;strong>;Eun-Ah Kim&lt;/strong>;, Haining Pan, &lt;strong>;Nayantara Mudur&lt;/strong>;, William Taranto,&lt;strong>; Subhashini Venugopalan&lt;/strong>;, &lt;strong>;Yasaman Bahri&lt;/strong>;, &lt;strong>;Michael P Brenner&lt;/strong>; &lt;br />; &lt;em>;Session S18: Data Science, AI and Machine Learning in Physics I&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S51.5&quot;>;New methods for reducing resource overhead in the surface code&lt;/a>; &lt;br />; Presenter: &lt;strong>;Michael Newman&lt;/strong>; &lt;br />; Authors: &lt;strong>;Craig M Gidney&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Peter Brooks&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>; &lt;br />; &lt;em>;Session S51: Quantum Error Correction Code Performance and Implementation II&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2312.04522.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/S49.10&quot;>;Challenges and opportunities for applying quantum computers to drug design&lt;/a>; &lt;br />; Presenter: Raffaele Santagati &lt;br />; Authors: Raffaele Santagati, Alan Aspuru-Guzik, &lt;strong>;Ryan Babbush&lt;/strong>;, Matthias Degroote, Leticia Gonzalez, Elica Kyoseva, Nikolaj Moll, Markus Oppel, Robert M. Parrish, &lt;strong>;Nicholas C. Rubin&lt;/strong>;, Michael Streif, Christofer S. Tautermann, Horst Weiss, Nathan Wiebe, Clemens Utschig-Utschig &lt;br />; &lt;em>;Session S49: Advances in Quantum Algorithms for Near-Term Applications&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2301.04114.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T45.1&quot;>;Dispatches from Google&#39;s hunt for super-quadratic quantum advantage in new applications&lt;/a>; &lt;br />; Presenter: &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; Author: &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;Session T45: Recent Advances in Quantum Algorithms&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/T48.11&quot;>;Qubit as a reflectometer&lt;/a>; &lt;br />; Presenter: &lt;strong>;Yaxing Zhang&lt;/strong>; &lt;br />; Authors: &lt;strong>;Yaxing Zhang&lt;/strong>;, &lt;strong>;Benjamin Chiaro&lt;/strong>; &lt;br />; &lt;em>;Session T48: Superconducting Fabrication, Packaging, &amp;amp; Validation&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W14.3&quot;>;Random-matrix theory of measurement-induced phase transitions in nonlocal Floquet quantum circuits&lt;/a>; &lt;br />; Presenter: Aleksei Khindanov &lt;br />; Authors: Aleksei Khindanov, &lt;strong>;Lara Faoro&lt;/strong>;, &lt;strong>;Lev Ioffe&lt;/strong>;, &lt;strong>;Igor Aleiner&lt;/strong>; &lt;br />; &lt;em>;Session W14: Measurement-Induced Phase Transitions&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W58.5&quot;>;Continuum limit of finite density many-body ground states with MERA&lt;/a>; &lt;br />; Presenter: Subhayan Sahu &lt;br />; Authors: Subhayan Sahu, &lt;strong>;Guifré Vidal&lt;/strong>; &lt;br />; &lt;em>;Session W58: Extreme-Scale Computational Science Discovery in Fluid Dynamics and Related Disciplines II&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W50.8&quot;>;Dynamics of magnetization at infinite temperature in a Heisenberg spin chain&lt;/a>; &lt;br />; Presenter: &lt;strong>;Eliott Rosenberg&lt;/strong>; &lt;br />; Authors: &lt;strong>;Eliott Rosenberg&lt;/strong>;, &lt;strong>;Trond Andersen&lt;/strong>;, Rhine Samajdar, &lt;strong>;Andre Petukhov&lt;/strong>;, Jesse Hoke*,&lt;strong>; Dmitry Abanin&lt;/strong>;, &lt;strong>;Andreas Bengtsson&lt;/strong>;, &lt;strong>;Ilya Drozdov&lt;/strong>;, &lt;strong>;Catherine Erickson&lt;/strong>;,&lt;strong>; Paul Klimov&lt;/strong>;, &lt;strong>;Xiao Mi&lt;/strong>;, &lt;strong>;Alexis Morvan&lt;/strong>;, &lt;strong>;Matthew Neeley&lt;/strong>;, &lt;strong>;Charles Neill&lt;/strong>;, &lt;strong>;Rajeev Acharya&lt;/strong>;, &lt;strong>;Richard Allen&lt;/strong>;, &lt;strong>;Kyle Anderson&lt;/strong>;, &lt;strong>;Markus Ansmann&lt;/strong>;, &lt;strong>;Frank Arute&lt;/strong>;, &lt;strong>;Kunal Arya&lt;/strong>;, &lt;strong>;Abraham Asfaw&lt;/strong>;, &lt;strong>;Juan Atalaya&lt;/strong>;, &lt;strong>;Joseph Bardin&lt;/strong>;, &lt;strong>;A. Bilmes&lt;/strong>;, &lt;strong>;Gina Bortoli&lt;/strong>;, &lt;strong>;Alexandre Bourassa&lt;/strong>;, &lt;strong>;Jenna Bovaird&lt;/strong>;, &lt;strong>;Leon Brill&lt;/strong>;, &lt;strong>;Michael Broughton&lt;/strong>;, &lt;strong>;Bob B. Buckley&lt;/strong>;, &lt;strong>;David Buell&lt;/strong>;, &lt;strong>;Tim Burger&lt;/strong>;, &lt;strong>;Brian Burkett&lt;/strong>;, &lt;strong>;Nicholas Bushnell&lt;/strong>;, &lt;strong>;Juan Campero&lt;/strong>;, &lt;strong>;Hung-Shen Chang&lt;/strong>;, &lt;strong>;Zijun Chen&lt;/strong>;, &lt;strong>;Benjamin Chiaro&lt;/strong>;, &lt;strong>;Desmond Chik&lt;/strong>;, &lt;strong>;Josh Cogan&lt;/strong>;, &lt;strong>;Roberto Collins&lt;/strong>;, &lt;strong>;Paul Conner&lt;/strong>;, &lt;strong>;William Courtney&lt;/strong>;, &lt;strong>;Alexander Crook&lt;/strong>;, &lt;strong>;Ben Curtin&lt;/strong>;, &lt;strong>;Dripto Debroy&lt;/strong>;, &lt;strong>;Alexander Del Toro Barba&lt;/strong>;, &lt;strong>;Sean Demura&lt;/strong>;, &lt;strong>;Agustin Di Paolo&lt;/strong>;, &lt;strong>;Andrew Dunsworth&lt;/strong>;, &lt;strong>;Clint Earle&lt;/strong>;, &lt;strong>;E. Farhi&lt;/strong>;, &lt;strong>;Reza Fatemi&lt;/strong>;, &lt;strong>;Vinicius Ferreira&lt;/strong>;, &lt;strong>;Leslie Flores&lt;/strong>;, &lt;strong>;Ebrahim Forati&lt;/strong>;, &lt;strong>;Austin Fowler&lt;/strong>;, &lt;strong>;Brooks Foxen&lt;/strong>;, &lt;strong>;Gonzalo Garcia&lt;/strong>;, &lt;strong>;Élie Genois&lt;/strong>;, &lt;strong>;William Giang&lt;/strong>;, &lt;strong>;Craig Gidney&lt;/strong>;, &lt;strong>;Dar Gilboa&lt;/strong>;, &lt;strong>;Marissa Giustina&lt;/strong>;, &lt;strong>;Raja Gosula&lt;/strong>;, &lt;strong>;Alejandro Grajales Dau&lt;/strong>;, &lt;strong>; Jonathan Gross&lt;/strong>;, &lt;strong>;Steve Habegger&lt;/strong>;, &lt;strong>;Michael Hamilton&lt;/strong>;, &lt;strong>;Monica Hansen&lt;/strong>;, &lt;strong>;Matthew Harrigan&lt;/strong>;, &lt;strong>; Sean Harrington&lt;/strong>;, &lt;strong>;Paula Heu&lt;/strong>;, &lt;strong>;Gordon Hill&lt;/strong>;, &lt;strong>;Markus Hoffmann&lt;/strong>;, &lt;strong>;Sabrina Hong&lt;/strong>;, &lt;strong>; Trent Huang&lt;/strong>;, &lt;strong>;Ashley Huff&lt;/strong>;, &lt;strong>;William Huggins&lt;/strong>;, &lt;strong>;Lev Ioffe&lt;/strong>;, &lt;strong>;Sergei Isakov&lt;/strong>;, &lt;strong>; Justin Iveland&lt;/strong>;, &lt;strong>;Evan Jeffrey&lt;/strong>;, &lt;strong>;Zhang Jiang&lt;/strong>;, &lt;strong>;Cody Jones&lt;/strong>;, &lt;strong>;Pavol Juhas&lt;/strong>;, &lt;strong>; D . Kafri&lt;/strong>;, &lt;strong>;Tanuj Khattar&lt;/strong>;, &lt;strong>;Mostafa Khezri&lt;/strong>;, &lt;strong>;Mária Kieferová&lt;/strong>;, &lt;strong>;Seon Kim&lt;/strong>;, &lt;strong>;Alexei Kitaev&lt;/strong>;, &lt;strong>;Andrey Klots&lt;/strong>;, &lt;strong>;Alexander Korotkov&lt;/strong>;, &lt;strong>;Fedor Kostritsa&lt;/strong>;, &lt;strong>;John Mark Kreikebaum&lt;/strong>;, &lt;strong>;David Landhuis&lt;/strong>;, &lt;strong>;Pavel Laptev&lt;/strong>;, &lt;strong>;Kim Ming Lau&lt;/strong>;, &lt;strong>;Lily Laws&lt;/strong>;, &lt;strong>;Joonho Lee&lt;/strong>;, &lt;strong>;Kenneth Lee&lt;/strong>;, &lt;strong>;Yuri Lensky&lt;/strong>;, &lt;strong>;Brian Lester&lt;/strong>;, &lt;strong>;Alexander Lill&lt;/strong>;, &lt;strong>;Wayne Liu&lt;/strong>;, &lt;strong>;William P. Livingston&lt;/strong>;, &lt;strong>;A. Locharla&lt;/strong>;, &lt;strong>;Salvatore Mandrà&lt;/strong>;, &lt;strong>;Orion Martin&lt;/strong>;, &lt;strong>;Steven Martin&lt;/strong>;, &lt;strong>;Jarrod McClean&lt;/strong>;, &lt;strong>;Matthew McEwen&lt;/strong>;, &lt;strong>;Seneca Meeks&lt;/strong>;, &lt;strong>;Kevin Miao&lt;/strong>;, &lt;strong>;Amanda Mieszala&lt;/strong>;, &lt;strong>;Shirin Montazeri&lt;/strong>;, &lt;strong>;Ramis Movassagh&lt;/strong>;, &lt;strong>;Wojciech Mruczkiewicz&lt;/strong>;, &lt;strong>;Ani Nersisyan&lt;/strong>;, &lt;strong>;Michael Newman&lt;/strong>;, &lt;strong>;Jiun How Ng&lt;/strong>;, &lt;strong>;Anthony Nguyen&lt;/strong>;, &lt;strong>;Murray Nguyen&lt;/strong>;, &lt;strong>;M. Niu&lt;/strong>;, &lt;strong>;Thomas O&#39;Brien&lt;/strong>;, &lt;strong>;Seun Omonije&lt;/strong>;, &lt;strong>;Alex Opremcak&lt;/strong>;, &lt;strong>;Rebecca Potter&lt;/strong>;, &lt;strong>;Leonid Pryadko&lt;/strong>;, &lt;strong>;Chris Quintana&lt;/strong>;, &lt;strong>;David Rhodes&lt;/strong>;, &lt;strong>;Charles Rocque&lt;/strong>;, &lt;strong>;N. Rubin&lt;/strong>;, &lt;strong>;Negar Saei&lt;/strong>;, &lt;strong>;Daniel Sank&lt;/strong>;, &lt;strong>;Kannan Sankaragomathi&lt;/strong>;, &lt;strong>;Kevin Satzinger&lt;/strong>;, &lt;strong>;Henry Schurkus&lt;/strong>;, &lt;strong>;Christopher Schuster&lt;/strong>;, &lt;strong>;Michael Shearn&lt;/strong>;, &lt;strong>;Aaron Shorter&lt;/strong>;, &lt;strong>;Noah Shutty&lt;/strong>;, &lt;strong>;Vladimir Shvarts&lt;/strong>;, &lt;strong>;Volodymyr Sivak&lt;/strong>;, &lt;strong>;Jindra Skruzny&lt;/strong>;, &lt;strong>;Clarke Smith&lt;/strong>;, &lt;strong>;Rolando Somma&lt;/strong>;, &lt;strong>;George Sterling&lt;/strong>;, &lt;strong>;Doug Strain&lt;/strong>;, &lt;strong>;Marco Szalay&lt;/strong>;, &lt;strong>;Douglas Thor&lt;/strong>;, &lt;strong>;Alfredo Torres&lt;/strong>;, &lt;strong>;Guifre Vidal&lt;/strong>;, &lt;strong>;Benjamin Villalonga&lt;/strong>;, &lt;strong>;Catherine Vollgraff Heidweiller&lt;/strong>;, &lt;strong>;Theodore White&lt;/strong>;, &lt;strong>;Bryan Woo&lt;/strong>;, &lt;strong>;Cheng Xing&lt;/strong>;, &lt;strong>;Jamie Yao&lt;/strong>;, &lt;strong>;Ping Yeh&lt;/strong>;, &lt;strong>;Juhwan Yoo&lt;/strong>;, &lt;strong>;Grayson Young&lt;/strong>;, &lt;strong>;Adam Zalcman&lt;/strong>;, &lt;strong>;Yaxing Zhang&lt;/strong>;, &lt;strong>;Ningfeng Zhu&lt;/strong>;, &lt;strong>;Nicholas Zobrist&lt;/strong>;, &lt;strong>;Hartmut Neven&lt;/strong>;, &lt;strong>;Ryan Babbush&lt;/strong>;, &lt;strong>;Dave Bacon&lt;/strong>;, &lt;strong>;Sergio Boixo&lt;/strong>;, &lt;strong>;Jeremy Hilton&lt;/strong>;, &lt;strong>;Erik Lucero&lt;/strong>;, &lt;strong>;Anthony Megrant&lt;/strong>;, &lt;strong>;Julian Kelly&lt;/strong>;, &lt;strong>;Yu Chen&lt;/strong>;, &lt;strong>;Vadim Smelyanskiy&lt;/strong>;, Vedika Khemani, Sarang Gopalakrishnan,&lt;strong>; Tomaž Prosen&lt;/strong>;, &lt;strong>;Pedram Roushan&lt;/strong>; &lt;br />; &lt;em>;Session W50: Quantum Simulation of Many-Body Physics&lt;/em>; &lt;br />; &lt;a href=&quot;https://arxiv.org/pdf/2306.09333.pdf&quot;>;Link to Paper&lt;/a>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/W50.13&quot;>;The fast multipole method on a quantum computer&lt;/a>; &lt;br />; Presenter: Kianna Wan &lt;br />; Authors: Kianna Wan, Dominic W Berry, &lt;strong>;Ryan Babbush&lt;/strong>; &lt;br />; &lt;em>;Session W50: Quantum Simulation of Many-Body Physics&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Friday&lt;/h3>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y43.1&quot;>;The quantum computing industry and protecting national security: what tools will work?&lt;/a>; &lt;br />; Presenter: &lt;strong>;Kate Weber&lt;/strong>; &lt;br />; Author: &lt;strong>;Kate Weber&lt;/strong>; &lt;br />; &lt;em>;Session Y43: Industry, Innovation, and National Security: Finding the Right Balance&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Y46.3&quot;>;Novel charging effects in the fluxonium qubit&lt;/a>; &lt;br />; Presenter: &lt;strong>;Agustin Di Paolo&lt;/strong>; &lt;br />; Authors: &lt;strong>;Agustin Di Paolo&lt;/strong>;, Kyle Serniak, Andrew J Kerman, &lt;strong>;William D Oliver&lt;/strong>; &lt;br />; &lt;em>;Session Y46: Fluxonium-Based Superconducting Quibits&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Z46.3&quot;>;Microwave Engineering of Parametric Interactions in Superconducting Circuits&lt;/a>; &lt;br />; Presenter: &lt;strong>;Ofer Naaman&lt;/strong>; &lt;br />; Author: &lt;strong>;Ofer Naaman&lt;/strong>; &lt;br />; &lt;em>;Session Z46: Broadband Parametric Amplifiers and Circulators&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://meetings.aps.org/Meeting/MAR24/Session/Z62.3&quot;>;Linear spin wave theory of large magnetic unit cells using the Kernel Polynomial Method&lt;/a>; &lt;br />; Presenter: Harry Lane &lt;br />; Authors: Harry Lane, Hao Zhang, David A Dahlbom, Sam Quinn, &lt;strong>;Rolando D Somma&lt;/strong>;, Martin P Mourigal, Cristian D Batista, Kipton Barros &lt;br />; &lt;em>;Session Z62: Cooperative Phenomena, Theory&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;b>;*&lt;/b>;&lt;/sup>;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2754526782497247497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2754526782497247497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/03/google-at-aps-2024.html&quot; rel=&quot;alternate&quot; title=&quot;Google at APS 2024&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1695264277638670894&lt;/id>;&lt;published>;2024-02-22T12:05:00.000-08:00&lt;/published>;&lt;updated>;2024-02-23T10:07:08.500-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPrism: A foundational visual encoder for video understanding&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Long Zhao, Senior Research Scientist, and Ting Liu, Senior Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; An astounding number of videos are available on the Web, covering a variety of content from everyday moments people share to historical moments to scientific observations, each of which contains a unique record of the world. The right tools could help researchers analyze these videos, transforming how we understand the world around us. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Videos offer dynamic visual content far more rich than static images, capturing movement, changes, and dynamic relationships between entities. Analyzing this complexity, along with the immense diversity of publicly available video data, demands models that go beyond traditional image understanding. Consequently, many of the approaches that best perform on video understanding still rely on specialized models tailor-made for particular tasks. Recently, there has been exciting progress in this area using video foundation models (ViFMs), such as &lt;a href=&quot;https://arxiv.org/abs/2109.14084&quot;>;VideoCLIP&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;. However, building a ViFM that handles the sheer diversity of video data remains a challenge. &lt;/p>; &lt;p>; With the goal of building a single model for general-purpose video understanding, we introduce “&lt;a href=&quot;https://arxiv.org/abs/2402.13217&quot;>;VideoPrism: A Foundational Visual Encoder for Video Understanding&lt;/a>;”. VideoPrism is a ViFM designed to handle a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA). We propose innovations in both the pre-training data as well as the modeling strategy. We pre-train VideoPrism on a massive and diverse dataset: 36 million high-quality video-text pairs and 582 million video clips with noisy or machine-generated parallel text. Our pre-training approach is designed for this hybrid data, to learn both from video-text pairs and the videos themselves. VideoPrism is incredibly easy to adapt to new video understanding challenges, and achieves state-of-the-art performance using a single frozen model. &lt;/p>;&lt;p>;&lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/teaser.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism is a general-purpose video encoder that enables state-of-the-art results over a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering, by producing video representations from a single frozen model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Pre-training data&lt;/h2>; &lt;p>; A powerful ViFM needs a very large collection of videos on which to train — similar to other foundation models (FMs), such as those for large language models (LLMs). Ideally, we would want the pre-training data to be a representative sample of all the videos in the world. While naturally most of these videos do not have perfect captions or descriptions, even imperfect text can provide useful information about the semantic content of the video. &lt;/p>; &lt;p>; To give our model the best possible starting point, we put together a massive pre-training corpus consisting of several public and private datasets, including &lt;a href=&quot;https://rowanzellers.com/merlot/&quot;>;YT-Temporal-180M&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2307.06942&quot;>;InternVid&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2204.00679&quot;>;VideoCC&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2007.14937&quot;>;WTS-70M&lt;/a>;, etc. This includes 36 million carefully selected videos with high-quality captions, along with an additional 582 million clips with varying levels of noisy text (like auto-generated transcripts). To our knowledge, this is the largest and most diverse video training corpus of its kind. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s1999/image18 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;779&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Statistics on the video-text pre-training data. The large variations of the&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2104.14806&quot;>;CLIP similarity scores&lt;/a>;&amp;nbsp;(the higher, the better) demonstrate the diverse caption quality of our pre-training data, which is a byproduct of the various ways used to harvest the text.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Two-stage training&lt;/h2>; &lt;p>; The VideoPrism model architecture stems from the standard &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;vision transformer&lt;/a>; (ViT) with a factorized design that sequentially encodes spatial and temporal information following &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>;. Our training approach leverages both the high-quality video-text data and the video data with noisy text mentioned above. To start, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning&quot;>;contrastive learning&lt;/a>; (an approach that minimizes the distance between positive video-text pairs while maximizing the distance between negative video-text pairs) to teach our model to match videos with their own text descriptions, including imperfect ones. This builds a foundation for matching semantic language content to visual content. &lt;/p>; &lt;p>; After video-text contrastive training, we leverage the collection of videos without text descriptions. Here, we build on the &lt;a href=&quot;https://arxiv.org/abs/2212.04500&quot;>;masked video modeling framework&lt;/a>; to predict masked patches in a video, with a few improvements. We train the model to predict both the video-level global embedding and token-wise embeddings from the first-stage model to effectively leverage the knowledge acquired in that stage. We then randomly shuffle the predicted tokens to prevent the model from learning shortcuts. &lt;/p>; &lt;p>; What is unique about VideoPrism&#39;s setup is that we use two complementary pre-training signals: text descriptions and the visual content within a video. Text descriptions often focus on what things look like, while the video content provides information about movement and visual dynamics. This enables VideoPrism to excel in tasks that demand an understanding of both appearance and motion. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; We conduct extensive evaluation on VideoPrism across four broad categories of video understanding tasks, including video classification and localization, video-text retrieval, video captioning, question answering, and scientific video understanding. VideoPrism achieves state-of-the-art performance on 30 out of 33 video understanding benchmarks — all with minimal adaptation of a single, frozen model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999 /image20.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot; 1959&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png&quot; width=&quot;628 &quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism compared to the previous best-performing FMs.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />; &lt;/div>; &lt;h3>;Classification and localization&lt;/h3>; &lt;p>; We evaluate VideoPrism on an existing large-scale video understanding benchmark (&lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;VideoGLUE&lt;/a>;) covering classification and localization tasks. We find that (1) VideoPrism outperforms all of the other state-of-the-art FMs, and (2) no other single model consistently came in second place. This tells us that VideoPrism has learned to effectively pack a variety of video signals into one encoder — from semantics at different granularities to appearance and motion cues — and it works well across a variety of video sources. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s1816/image12.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1816&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;VideoPrism outperforms state-of-the-art approaches (including &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;, &lt;a href=&quot; https://arxiv.org/abs/2104.11178&quot;>;VATT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2212.03191&quot;>;InternVideo&lt;/a>;, and &lt;a href=&quot;https ://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>;) on the &lt;a href=&quot;https://arxiv.org/abs/2307.03166&quot;>;video understanding benchmark&lt;/a>;. In this plot, we show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. On &lt;a href=&quot;http://vuchallenge.org/charades.html&quot;>;Charades&lt;/a>;, &lt;a href=&quot;http://activity-net.org/&quot;>;ActivityNet&lt;/a>;, &lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA&lt;/a>;, and &lt;a href=&quot;https://research.google.com/ava/&quot;>;AVA-K&lt;/a>;, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision&quot;>;mean average precision&lt;/a>; (mAP) as the evaluation metric. On the other datasets, we report top-1 accuracy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Combining with LLMs&lt;/h3>; &lt;p>; We further explore combining VideoPrism with LLMs to unlock its ability to handle various video-language tasks. In particular, when paired with a text encoder (following &lt;a href=&quot;https://arxiv.org/abs/2111.07991&quot;>;LiT&lt;/a>;) or a language decoder (such as &lt;a href=&quot;https://arxiv.org/abs/2305.10403&quot;>;PaLM-2&lt;/a>;), VideoPrism can be utilized for video-text retrieval, video captioning, and video QA tasks. We compare the combined models on a broad and challenging set of vision-language benchmarks. VideoPrism sets the new state of the art on most benchmarks. From the visual results, we find that VideoPrism is capable of understanding complex motions and appearances in videos (eg, the model can recognize the different colors of spinning objects on the window in the visual examples below). These results demonstrate that VideoPrism is strongly compatible with language models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1028&quot; height=&quot;580&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/w640-h580/VideoPrismResults.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism achieves competitive results compared with state-of-the-art approaches (including &lt;a href=&quot;https:// arxiv.org/abs/2212.04979&quot;>;VideoCoCa&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.16058&quot;>;UMT&lt;/a>; and &lt;a href=&quot;https://arxiv. org/abs/2204.14198&quot;>;Flamingo&lt;/a>;) on multiple video-text retrieval (top) and video captioning and video QA (bottom) benchmarks. We also show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. We report the Recall@1 on &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video- and-language/&quot;>;MASRVTT&lt;/a>;, &lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX&lt;/a>;, and &lt;a href=&quot; https://cs.stanford.edu/people/ranjaykrishna/densevid/&quot;>;ActivityNet&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr score&lt;/a>; on &lt; a href=&quot;https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/&quot;>;MSRVTT- Cap&lt;/a>;, &lt;a href=&quot;https://eric-xw.github.io/vatex-website/index.html&quot;>;VATEX-Cap&lt;/a>;, and &lt;a href=&quot;http:// youcook2.eecs.umich.edu/&quot;>;YouCook2&lt;/a>;, top-1 accuracy on &lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSRVTT-QA&lt;/a>; and &lt;a href=&quot;https://github.com/xudejing/video-question-answering&quot;>;MSVD-QA&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/cmp-lg /9406033&quot;>;WUPS index&lt;/a>; on &lt;a href=&quot;https://doc-doc.github.io/docs/nextqa.html&quot;>;NExT-QA&lt;/a>;.&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com /garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width =&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/spin_roller_skating.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt; video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/garyzhao/videoprism-blog/raw/main/making_ice_cream_ski_lifting.mp4 &quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show qualitative results using VideoPrism with a text encoder for video-text retrieval (first row) and adapted to a language decoder for video QA (second and third row). For video-text retrieval examples, the blue bars indicate the embedding similarities between the videos and the text queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Scientific applications&lt;/h3>; &lt;p>; Finally, we test VideoPrism on datasets used by scientists across domains, including fields such as ethology, behavioral neuroscience, and ecology. These datasets typically require domain expertise to annotate, for which we leverage existing scientific datasets open-sourced by the community including &lt;a href=&quot;https://data.caltech.edu/records/zrznw-w7386&quot;>;Fly vs. Fly&lt;/a>;, &lt;a href=&quot;https://data.caltech.edu/records/s0vdx-0k302&quot;>;CalMS21&lt;/a>;, &lt;a href=&quot;https://shirleymaxx.github.io/ChimpACT/&quot;>;ChimpACT&lt;/a>;, and &lt;a href=&quot;https://dirtmaxim.github.io/kabr/&quot;>;KABR&lt;/a>;. VideoPrism not only performs exceptionally well, but actually surpasses models designed specifically for those tasks. This suggests tools like VideoPrism have the potential to transform how scientists analyze video data across different fields. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1 -s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200 &quot; height=&quot;397&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png&quot; width =&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;VideoPrism outperforms the domain experts on various scientific benchmarks 。 We show the absolute score differences to highlight the relative improvements of VideoPrism. We report mean average precision (mAP) for all datasets, except for KABR which uses class-averaged top-1 accuracy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With VideoPrism, we introduce a powerful and versatile video encoder that sets a new standard for general-purpose video understanding. Our emphasis on both building a massive and varied pre-training dataset and innovative modeling techniques has been validated through our extensive evaluations. Not only does VideoPrism consistently outperform strong baselines, but its unique ability to generalize positions it well for tackling an array of real-world applications. Because of its potential broad use, we are committed to continuing further responsible research in this space, guided by our &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. We hope VideoPrism paves the way for future breakthroughs at the intersection of AI and video analysis, helping to realize the potential of ViFMs across domains such as scientific discovery, education, and healthcare. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This blog post is made on behalf of all the VideoPrism authors: Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong 。 We sincerely thank David Hendon for their product management efforts, and Alex Siegman, Ramya Ganeshan, and Victor Gomes for their program and resource management efforts. We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu for the discussions, support, and feedback that greatly contributed to this work. We are grateful to Jay Yagnik, Rahul Sukthankar, and Tomas Izo for their enthusiastic support for this project. Lastly, we thank Tom Small, Jennifer J. Sun, Hao Zhou, Nitesh B. Gundavarapu, Luke Friedman, and Mikhail Sirotenko for the tremendous help with making this blog post.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1695264277638670894/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1695264277638670894&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html&quot; rel=&quot;alternate&quot; title=&quot;VideoPrism: A foundational visual encoder for video understanding&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4343235509909091741&lt;/id>;&lt;published>;2024-02-21T12:15:00.000-08:00&lt;/published>;&lt;updated>;2024-02-21T12:15:36.694-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Gboard&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advances in private training for production on-device language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zheng Xu, Research Scientist, and Yanxiang Zhang, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Language models (LMs) trained to predict the next word given input text are the key technology for many applications [&lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;2&lt;/a>;]. In &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;Gboard&lt;/a>;, LMs are used to improve users&#39; typing experience by supporting features like &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;next word prediction&lt;/a>; (NWP), &lt;a href=&quot;https:// support.google.com/gboard/answer/7068415&quot;>;Smart Compose&lt;/a>;,&lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>; smart completion&lt;/a>; and &lt; a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;suggestion&lt;/a>;, &lt;a href=&quot;https://support.google.com/gboard/answer/2811346&quot;>;slide to type&lt;/a>;&lt;span style=&quot;text-decoration: underline;&quot;>;,&lt;/span>; and &lt;a href=&quot;https://support.google.com/gboard/answer/7068415&quot;>;proofread&lt;/一个>;。 Deploying models on users&#39; devices rather than enterprise servers has advantages like lower latency and better privacy for model usage. While training on-device models directly from user data effectively improves the utility performance for applications such as NWP and &lt;a href=&quot;https://blog.research.google/2021/11/predicting-text-selections-with.html&quot;>;smart text selection&lt;/a>;, protecting the privacy of user data for model training is important. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/image45.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1600&quot; data-original-width=&quot;1996&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Gboard features powered by on-device language models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In this blog we discuss how years of research advances now power the private training of Gboard LMs, since the proof-of-concept development of &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;federated learning&lt;/a>; (FL) in 2017 and formal &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;differential privacy&lt;/a>; (DP) guarantees in 2022. &lt;a href=&quot;https://blog.research.google/2017/04/federated-learning-collaborative.html&quot;>;FL&lt;/a>; enables mobile phones to collaboratively learn a model while keeping all the training data on device, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;DP&lt;/a>; provides a quantifiable measure of data anonymization. Formally, DP is often characterized by (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) with smaller values representing stronger guarantees. Machine learning (ML) models are considered to have &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable DP guarantees for ε=10 and strong DP guarantees for ε=1&lt;/a>; when &lt;em>;δ&lt;/em>; is small. &lt;/p>; &lt;p>; As of today, all NWP neural network LMs in Gboard are trained with FL with formal DP guarantees, and all future launches of Gboard LMs trained on user data require DP. These 30+ Gboard on-device LMs are launched in 7+ languages and 15+ countries, and satisfy (&lt;em>;ɛ&lt;/em>;, &lt;em>;δ&lt;/em>;)-DP guarantees of small &lt;em>;δ&lt;/em>; of 10&lt;sup>;-10&lt;/sup>; and ɛ between 0.994 and 13.69. To the best of our knowledge, this is the largest known deployment of user-level DP in production at Google or anywhere, and the first time a strong DP guarantee of &lt;em>;ɛ&lt;/em>; &amp;lt; 1 is announced for models trained directly on user data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Privacy principles and practices in Gboard&lt;/h2>; &lt;p>; In “&lt;a href=&quot;https ://arxiv.org/abs/2306.14793&quot;>;Private Federated Learning in Gboard&lt;/a>;”, we discussed how different &lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3501293&quot; >;privacy principles&lt;/a>; are currently reflected in production models, including: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Transparency and user control&lt;/em>;: We provide disclosure of what data is used, what purpose it is used for, how it is processed in various channels, and how Gboard users can easily &lt;a href=&quot;https://support.google.com/gboard/answer/12373137&quot;>;configure&lt;/a>; the data usage in learning模型。 &lt;/li>;&lt;li>;&lt;em>;Data minimization&lt;/em>;: FL immediately aggregates only focused updates that improve a specific model. &lt;a href=&quot;https://eprint.iacr.org/2017/281.pdf&quot;>;Secure aggregation&lt;/a>; (SecAgg) is an encryption method to further guarantee that only aggregated results of the ephemeral updates can be accessed. &lt;/li>;&lt;li>;&lt;em>;Data anonymization&lt;/em>;: DP is applied by the server to prevent models from memorizing the unique information in individual user&#39;s training data. &lt;/li>;&lt;li>;&lt;em>;Auditability and verifiability&lt;/em>;: We have made public the key algorithmic approaches and privacy accounting in open-sourced code (&lt;a href=&quot;https://github.com/tensorflow/federated/blob/main/tensorflow_federated/python/aggregators/differential_privacy.py&quot;>;TFF aggregator&lt;/a>;, &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/dp_query/tree_aggregation_query.py&quot;>;TFP DPQuery&lt;/a>;, &lt;a href=&quot;https://github.com/google-research/federated/blob/master/dp_ftrl/blogpost_supplemental_privacy_accounting.ipynb&quot;>;DP accounting&lt;/a>;, and &lt;a href=&quot;https://github.com/google/federated-compute&quot;>;FL system&lt;/a>;). &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;A brief history&lt;/h3>; &lt;p>; In recent years, FL has become the default method for training &lt;a href=&quot;https://arxiv.org/abs/1811.03604&quot;>;Gboard on-device LMs&lt;/a>; from user data. In 2020, a DP mechanism that &lt;a href=&quot;https://arxiv.org/abs/1710.06963&quot;>;clips and adds noise&lt;/a>; to model updates was used to &lt;a href=&quot;https://arxiv.org/abs/2009.10031&quot;>;prevent memorization&lt;/a>; for training the Spanish LM in Spain, which satisfies finite DP guarantees (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 3&lt;/a>; described in “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML“&lt;/a>; guide). In 2022, with the help of the &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-Follow-The-Regularized-Leader (DP-FTRL) algorithm&lt;/a>;, the Spanish LM became the first production neural network trained directly on user data announced with &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;a formal DP guarantee of (ε=8.9, δ=10&lt;sup>;-10&lt;/sup>;)-DP&lt;/a>; (equivalent to the reported &lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;ρ=0.81&lt;/a>;&lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1605.02065&quot;>;zero-Concentrated-Differential-Privacy&lt;/a>;), and therefore satisfies &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable privacy guarantees&lt;/a>; (&lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 2&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Differential privacy by default in federated learning &lt;/h2>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;Federated Learning of Gboard Language Models with Differential Privacy&lt;/a>;”, we announced that all the NWP neural network LMs in Gboard have DP guarantees, and all future launches of Gboard LMs trained on user data require DP guarantees. DP is enabled in FL by applying the following practices: &lt;/p>; &lt;ul>; &lt;li>;Pre-train the model with the &lt;a href=&quot;https://arxiv.org/abs/2010.11934&quot;>;multilingual&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;C4&lt;/a>; dataset. &lt;/li>;&lt;li>;Via simulation experiments on public datasets, find a large DP-noise-to-signal ratio that allows for high utility. Increasing the number of clients contributing to one round of model update improves privacy while keeping the noise ratio fixed for good utility, up to the point the DP target is met, or the maximum allowed by the system and the size of the population. &lt;/li>;&lt;li>;Configure the parameter to restrict the frequency each client can contribute (eg, once every few days) based on computation budget and estimated population in &lt;a href=&quot;https://arxiv.org/abs/1902.01046&quot;>;the FL system&lt;/a>;. &lt;/li>;&lt;li>;Run &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; training with limits on the magnitude of per-device updates chosen either via &lt;a href=&quot;https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e2b3a90e103368b6b6&quot;>;adaptive clipping&lt;/a>;, or fixed based on experience. &lt;/li>; &lt;/ul>; &lt;p>; SecAgg can be additionally applied by adopting the &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;advances in improving computation and communication for scales and sensitivity&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N /s1600/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;1600&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Federated learning with differential privacy and (SecAgg).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Reporting DP guarantees&lt;/h3>; &lt;p>; The DP guarantees of launched Gboard NWP LMs are visualized in the barplot below 。 The &lt;em>;x&lt;/em>;-axis shows LMs labeled by language-locale and trained on corresponding populations; the &lt;em>;y&lt;/em>;-axis shows the &lt;em>;ε&lt;/em>; value when &lt;em>;δ&lt;/em>; is fixed to a small value of 10&lt;sup>;-10&lt;/sup>; for &lt;a href=&quot;https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf&quot;>;(ε, δ)-DP&lt;/a>; (lower is better). The utility of these models are either significantly better than previous non-neural models in production, or comparable with previous LMs without DP, measured based on user-interactions metrics during A/B testing. For example, by applying the best practices, the DP guarantee of the Spanish model in Spain is improved from &lt;em>;&lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;ε=8.9&lt;/a>;&lt;/em>; to &lt;em>;ε&lt;/em>;=5.37. SecAgg is additionally used for training the Spanish model in Spain and English model in the US. More details of the DP guarantees are reported in &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;the appendix &lt;/a>;following the &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;guidelines outlined&lt;/a>; in “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML&lt;/a>;”. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Towards stronger DP guarantees&lt;/h2>; &lt;p>; The &lt;em>;ε&lt;/em>;~10 DP guarantees of many launched LMs are already considered &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;reasonable&lt;/a>; for ML models in practice, while the journey of DP FL in Gboard continues for improving user typing experience while protecting data privacy. We are excited to announce that, for the first time, production LMs of Portuguese in Brazil and Spanish in Latin America are trained and launched with a DP guarantee of &lt;em>;ε&lt;/em>; ≤ 1, which satisfies &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;Tier 1 strong privacy guarantees&lt;/a>;. Specifically, the (&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP guarantee is achieved by running the advanced &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;Matrix Factorization DP-FTRL&lt;/a>; (MF-DP-FTRL) algorithm, with 12,000+ devices participating in every training round of server model update larger than the &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;common setting of 6500+ devices&lt;/a>;, and a carefully configured policy to restrict each client to at most participate twice in the total 2000 rounds of training in 14 days in the large Portuguese user population of Brazil. Using a similar setting, the es-US Spanish LM was trained in a large population combining multiple countries in Latin America to achieve (&lt;em>;ε&lt;/em>;=0.994, &lt;em>;δ&lt;/em>;=10&lt;sup>;-10&lt;/sup>;)-DP. The &lt;em>;ε&lt;/em>; ≤ 1 es-US model significantly improved the utility in many countries, and launched in Colombia, Ecuador, Guatemala, Mexico, and Venezuela. For the smaller population in Spain, the DP guarantee of es-ES LM is improved from &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;ε=5.37&lt;/a>;&lt;/em>; to &lt;em>;ε&lt;/em>;=3.42 by only replacing &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>; without increasing the number of devices participating every round. More technical details are disclosed in the &lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/federated/blob/master/mf_dpftrl_matrices/privacy_accounting.ipynb&quot;>;colab&lt;/a>; for privacy会计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s1999/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;709&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP guarantees for Gboard NWP LMs (the purple bar represents the first es-ES launch of ε=8.9; cyan bars represent privacy improvements for models trained with &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;MF-DP-FTRL&lt;/a>;; &lt;a href=&quot;https://blog.research.google/2023/05/making-ml-models-differentially-private.html&quot;>;tiers &lt;/a>;are from “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML&lt;/a>;“ guide; en-US* and es-ES* are additionally trained with SecAgg).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Discussion and next steps&lt;/h2>; &lt;p>; Our experience suggests that DP can be achieved in practice through system algorithm co-design on client participation, and that both privacy and utility can be strong when populations are large &lt;em>;and&lt;/em>; a large number of devices&#39; contributions are aggregated. Privacy-utility-computation trade-offs can be improved by &lt;a href=&quot;https://arxiv.org/abs/2305.18465&quot;>;using public data&lt;/a>;, the &lt;a href=&quot;https://arxiv.org/abs/2306.08153&quot;>;new MF-DP-FTRL algorithm&lt;/a>;, &lt;a href=&quot;https://github.com/google/differential-privacy&quot;>;and tightening accounting&lt;/a>;. With these techniques, a strong DP guarantee of &lt;em>;ε&lt;/em>; ≤ 1 is possible but still challenging. Active research on empirical privacy auditing [&lt;a href=&quot;https://arxiv.org/abs/2302.03098&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2305.08846&quot;>;2&lt;/a>;] suggests that DP models are potentially more private than the worst-case DP guarantees imply. While we keep pushing the frontier of algorithms, which dimension of privacy-utility-computation should be prioritized? &lt;/p>; &lt;p>; We are actively working on all privacy aspects of ML, including extending DP-FTRL to &lt;a href=&quot;https://blog.research.google/2023/03/distributed-differential-privacy-for.html&quot;>;distributed DP&lt;/a>; and improving &lt;a href=&quot;https://arxiv.org/abs/2306.14793&quot;>;auditability and verifiability&lt;/a>;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;Trusted Execution Environment&lt;/a>; opens the opportunity for substantially increasing the model size with verifiable privacy. The recent &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;breakthrough in large LMs&lt;/a>; (LLMs) motivates us to &lt;a href=&quot;https://arxiv.org/abs/2305.12132&quot;>;rethink&lt;/a>; the usage of &lt;a href=&quot;https://arxiv.org/abs/2212.06470&quot;>;public&lt;/a>; information in private training and more future interactions between LLMs, on-device LMs, and Gboard production. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The authors would like to thank Peter Kairouz, Brendan McMahan, and Daniel Ramage for their early feedback on the blog post itself, Shaofeng Li and Tom Small for helping with the animated figures, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The collaborators below directly contribute to the presented results:&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Research and algorithm development: Galen Andrew, Stanislav Chiknavaryan, Christopher A. Choquette-Choo, Arun Ganesh, Peter Kairouz, Ryan McKenna, H. Brendan McMahan, Jesse Rosenstock, Timon Van Overveldt, Keith Rush, Shuang Song, Thomas Steinke, Abhradeep Guha Thakurta, Om Thakkar, and Yuanbo Zhang.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Infrastructure, production and leadership support: Mingqing Chen, Stefan Dierauf, Billy Dou, Hubert Eichner, Zachary Garrett, Jeremy Gillula, Jianpeng Hou, Hui Li, Xu Liu, Wenzhi Mao, Brett McLarnon, Mengchen Pei, Daniel Ramage, Swaroop Ramaswamy, Haicheng Sun, Andreas Terzis, Yun Wang, Shanshan Wu, Yu Xiao, and Shumin Zhai.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4343235509909091741/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4343235509909091741&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/advances-in-private-training-for.html&quot; rel=&quot;alternate&quot; title=&quot;Advances in private training for production on-device language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025&lt;/id>;&lt;published>;2024-02-14T10:32:00.000-08:00&lt;/published>;&lt;updated>;2024-02-14T10:32:25.557-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Learning the importance of training data under concept drift&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Nishant Jain, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s1600/temporalreweightinghero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The constantly changing nature of the world around us poses a significant challenge for the development of AI models. Often, models are trained on longitudinal data with the hope that the training data used will accurately represent inputs the model may receive in the future. More generally, the default assumption that all training data are equally relevant often breaks in practice. For example, the figure below shows images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; nonstationary learning benchmark, and it illustrates how visual features of objects evolve significantly over a 10 year span (a phenomenon we refer to as &lt;em>;slow concept drift&lt;/em>;), posing a challenge for object categorization models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;662&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Sample images from the CLEAR benchmark. (Adapted from Lin et al&lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;.&lt;/a>;)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Alternative approaches, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Online_machine_learning&quot;>;online&lt;/a>; and &lt;a href=&quot;https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning&quot;>;continual learning&lt;/a>;, repeatedly update a model with small amounts of recent data in order to keep it current. This implicitly prioritizes recent data, as the learnings from past data are gradually erased by subsequent updates. However in the real world, different kinds of information lose relevance at different rates, so there are two key issues: 1) By design they focus &lt;em>;exclusively&lt;/em>; on the most recent data and lose any signal from older data that被删除。 2) Contributions from data instances decay &lt;em>;uniformly over time&lt;/em>; irrespective of the contents of the data. &lt;/p>; &lt;p>; In our recent work, “&lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;Instance-Conditional Timescales of Decay for Non-Stationary Learning&lt;/a>;”, we propose to assign each instance an importance score during training in order to maximize model performance on future data. To accomplish this, we employ an auxiliary model that produces these scores using the training instance as well as its age. This model is jointly learned with the primary model. We address both the above challenges and achieve significant gains over other robust learning methods on a range of benchmark datasets for nonstationary learning. For instance, on a &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;recent large-scale benchmark&lt;/a>; for nonstationary learning (~39M photos over a 10 year period), we show up to 15% relative accuracy gains through learned reweighting of training data. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;The challenge of concept drift for supervised learning&lt;/h2>; &lt;p>; To gain quantitative insight into slow concept drift, we built classifiers on a &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;recent photo categorization task&lt;/a>;, comprising roughly 39M photographs sourced from social media websites over a 10 year period. We compared offline training, which iterated over all the training data multiple times in random order, and continual training, which iterated multiple times over each month of data in sequential (temporal) order. We measured model accuracy both during the training period and during a subsequent period where both models were frozen, ie, not updated further on new data (shown below). At the end of the training period (left panel, x-axis = 0), both approaches have seen the same amount of data, but show a large performance gap. This is due to &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368&quot;>;catastrophic forgetting&lt;/a>;, a problem in continual learning where a model&#39;s knowledge of data from early on in the training sequence is diminished in an uncontrolled manner. On the other hand, forgetting has its advantages — over the test period (shown on the right), the continual trained model degrades much less rapidly than the offline model because it is less dependent on older data. The decay of both models&#39; accuracy in the test period is confirmation that the data is indeed evolving over time, and both models become increasingly less relevant. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1554&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparing offline and continually trained models on the photo classification task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt; div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-sensitive reweighting of training data&lt;/h2>; &lt;p>; We design a method combining the benefits of offline learning (the flexibility of effectively reusing all available data) and continual learning (the ability to downplay older data) to address slow concept drift. We build upon offline learning, then add careful control over the influence of past data and an optimization objective, both designed to reduce model decay in the future. &lt;/p>; &lt;p>; Suppose we wish to train a model, &lt;em>;M&lt;/em>;,&lt;em>; &lt;/em>;given some training data collected over time. We propose to also train a helper model that assigns a weight to each point based on its contents and age. This weight scales the contribution from that data point in the training objective for &lt;em>;M&lt;/em>;. The objective of the weights is to improve the performance of &lt;em>;M&lt;/em>; on future data. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;our work&lt;/a>;, we describe how the helper model can be &lt;em>;meta-learned, &lt;/em>;ie, learned alongside &lt;em>;M&lt;/em>; in a manner that helps the learning of the model &lt;em>;M&lt;/em>; itself. A key design choice of the helper model is that we separated out instance- and age-related contributions in a factored manner. Specifically, we set the weight by combining contributions from multiple different fixed timescales of decay, and learn an approximate “assignment” of a given instance to its most suited timescales. We find in our experiments that this form of the helper model outperforms many other alternatives we considered, ranging from unconstrained joint functions to a single timescale of decay (exponential or linear), due to its combination of simplicity and expressivity. Full details may be found in the &lt;a href=&quot;https://arxiv.org/abs/2212.05908&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Instance weight scoring&lt;/h2>; &lt;p>; The top figure below shows that our learned helper model indeed up-weights more modern-looking objects in the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR object recognition challenge&lt;/a>;; older-looking objects are correspondingly down-weighted. On closer examination (bottom figure below, gradient-based &lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;feature importance&lt;/a>; assessment), we see that the helper model focuses on the primary object within the image, as opposed to, eg, background features that may spuriously be correlated with instance age. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Sample images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; benchmark (camera &amp;amp; computer categories) assigned the highest and lowest weights respectively by our helper model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Feature importance analysis of our helper model on sample images from the &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;CLEAR&lt;/a>; benchmark.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Gains on large-scale data &lt;/h3>; &lt;p>; We first study the large-scale &lt;a href=&quot;https://arxiv.org/abs/2108.09020&quot;>;photo categorization task&lt;/a>; (PCAT) on the &lt;a href=&quot;https://arxiv.org/abs/1503.01817&quot;>;YFCC100M dataset&lt;/a>; discussed earlier, using the first five years of data for training and the next five years as test data. Our method (shown in red below) improves substantially over the no-reweighting baseline (black) as well as many other robust learning techniques. Interestingly, our method deliberately trades off accuracy on the distant past (training data unlikely to reoccur in the future) in exchange for marked improvements in the test period. Also, as desired, our method degrades less than other baselines in the test period. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;Comparison of our method and relevant baselines on the PCAT dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot; >; &lt;br>; &lt;/div>; &lt;h3>;Broad applicability&lt;/h3>; &lt;p>; We validated our findings on a wide range of nonstationary learning challenge datasets sourced from the academic literature (see &lt;a href=&quot;https://arxiv .org/abs/2108.09020&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.06289&quot;>;2&lt;/a>;, &lt;a href=&quot;https://arxiv.org /abs/2211.14238&quot;>;3&lt;/a>;, &lt;a href=&quot;https://proceedings.mlr.press/v206/awasthi23b/awasthi23b.pdf&quot;>;4&lt;/a>; for details) that spans data sources and modalities (photos, satellite images, social media text, medical records, sensor readings, tabular data) and sizes (ranging from 10k to 39M instances). We report significant gains in the test period when compared to the nearest published benchmark method for each dataset (shown below). Note that the previous best-known method may be different for each dataset. These results showcase the broad applicability of our approach. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s765/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;765&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class= &quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance gain of our method on a variety of tasks studying natural concept drift. Our reported gains are over the previous best-known method for each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Extensions to continual learning&lt;/h3>; &lt;p>; Finally, we consider an interesting extension of our work. The work above described how offline learning can be extended to handle concept drift using ideas inspired by continual learning. However, sometimes offline learning is infeasible — for example, if the amount of training data available is too large to maintain or process. We adapted our approach to continual learning in a straightforward manner by applying temporal reweighting &lt;em>;within the context of &lt;/em>;each bucket of data being used to sequentially update the model. This proposal still retains some limitations of continual learning, eg, model updates are performed only on most-recent data, and all optimization decisions (including our reweighting) are only made over that data. Nevertheless, our approach consistently beats regular continual learning as well as a wide range of other continual learning algorithms on the photo categorization benchmark (see below). Since our approach is complementary to the ideas in many baselines compared here, we anticipate even larger gains when combined with them. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;Results of our method adapted to continual learning, compared to the latest baselines.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We addressed the challenge of data drift in learning by combining the strengths of previous approaches — offline learning with its effective reuse of data, and continual learning with its emphasis on more recent data. We hope that our work helps improve model robustness to concept drift in practice, and generates increased interest and new ideas in addressing the ubiquitous problem of slow concept drift. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank Mike Mozer for many interesting discussions in the early phase of this work, as well as very helpful advice and feedback during its development.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5605933033299261025/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/learning-importance-of-training-data.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5605933033299261025&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/learning-importance-of-training-data.html&quot; rel=&quot;alternate&quot; title=&quot;Learning the importance of training data under concept drift&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5933365460125094774&lt;/id>;&lt;published>;2024-02-13T14:11:00.000-08:00&lt;/published>;&lt;updated>;2024-02-13T14:11:49.258-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DP-Auditorium: A flexible library for auditing differential privacy&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mónica Ribero Díaz, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential privacy&lt;/a>; (DP) is a property of randomized mechanisms that limit the influence of any individual user&#39;s information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies &lt;a href=&quot;https://blog.research.google/2022/02/federated-learning-with-formal.html&quot;>;across&lt;/a>; &lt;a href=&quot;https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf&quot;>;industries&lt;/a>; and government applications (eg, &lt;a href=&quot;https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/differential-privacy.html&quot;>;the US census&lt;/a>;) without compromising individual user identities. As its adoption increases, it&#39;s important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, &lt;a href=&quot;https://arxiv.org/pdf/1603.01699.pdf&quot;>;researchers compared&lt;/a>; six sparse vector technique (SVT) variations and found that only two of the six actually met the asserted privacy保证。 Even when mathematical proofs are correct, the code implementing the mechanism is vulnerable to human error. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, practical and efficient DP auditing is challenging primarily due to the inherent randomness of the mechanisms and the probabilistic nature of the tested guarantees. In addition, a range of guarantee types exist, (eg, &lt;a href=&quot;https://dl.acm.org/doi/10.1007/11681878_14&quot;>;pure DP&lt;/a>;, &lt;a href=&quot;https://link.springer.com/chapter/10.1007/11761679_29&quot;>;approximate DP&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/pdf/1603.01887.pdf&quot;>;concentrated DP&lt;/a>;), and this diversity contributes to the complexity of formulating the auditing problem. Further, debugging mathematical proofs and code bases is an intractable task given the volume of proposed mechanisms. While &lt;em>;ad hoc&lt;/em>; testing techniques exist under specific assumptions of mechanisms, few efforts have been made to develop an extensible tool for testing DP mechanisms. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;DP-Auditorium: A Large Scale Library for Auditing Differential Privacy&lt;/a>;”, we introduce an &lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/python/dp_auditorium&quot;>;open source library&lt;/a>; for auditing DP guarantees with only black-box access to a mechanism (ie, without any knowledge of the mechanism&#39;s internal properties). DP-Auditorium is implemented in Python and provides a flexible interface that allows contributions to continuously improve its testing capabilities. We also introduce new testing algorithms that perform divergence optimization over function spaces for Rényi DP, pure DP, and approximate DP. We demonstrate that DP-Auditorium can efficiently identify DP guarantee violations, and suggest which tests are most suitable for detecting particular bugs under various privacy guarantees. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP guarantees&lt;/h2>; &lt;p>; The output of a DP mechanism is a sample drawn from a probability distribution (&lt;em>;M&lt;/em>; (&lt;em>;D&lt;/em>;)) that satisfies a mathematical property ensuring the privacy of user data. A DP guarantee is thus tightly related to properties between pairs of probability distributions. A mechanism is differentially private if the probability distributions determined by &lt;i>;M&lt;/i>; on dataset &lt;em>;D&lt;/em>; and a neighboring dataset &lt;em>;D&#39;&lt;/em>;, which differ by only one record, are &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_indistinguishability&quot;>;indistinguishable&lt;/a>;&lt;/em>; under a given divergence metric. &lt;/p>; &lt;p>; For example, the classical &lt;a href=&quot;https://software.imdea.org/~federico/pubs/2013.ICALP.pdf&quot;>;approximate DP&lt;/a>; definition states that a mechanism is approximately DP with parameters (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;) if the &lt;a href=&quot;https://arxiv.org/pdf/1508.00335.pdf&quot;>;hockey-stick divergence&lt;/a>; of order &lt;em>;e&lt;sup>;ε&lt;/sup>;&lt;/em>;, between &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;), is at most &lt;em>;δ&lt;/em>;. Pure DP is a special instance of approximate DP where &lt;em>;δ = 0&lt;/em>;. Finally, a mechanism is considered &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>; with parameters (&lt;em>;𝛼&lt;/em>;, &lt;em>;ε)&lt;/em>; if the &lt;a href=&quot;https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy&quot;>;Rényi divergence&lt;/a>; of order &lt;em>;𝛼&lt;/em>;, is at most &lt;em>;ε&lt;/em>; (where &lt;em>;ε&lt;/em>; is a small positive value). In these three definitions, &lt;em>;ε &lt;/em>;is not interchangeable but intuitively conveys the same concept; larger values of &lt;em>;ε&lt;/em>; imply larger divergences between the two distributions or less privacy, since the two distributions are easier to distinguish. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-Auditorium&lt;/h2>; &lt;p>; DP-Auditorium comprises two main components: property testers and dataset finders. Property testers take samples from a mechanism evaluated on specific datasets as input and aim to identify privacy guarantee violations in the provided datasets. Dataset finders suggest datasets where the privacy guarantee may fail. By combining both components, DP-Auditorium enables (1) automated testing of diverse mechanisms and privacy definitions and, (2) detection of bugs in privacy-preserving mechanisms. We implement various private and non-private mechanisms, including simple mechanisms that compute the mean of records and more complex mechanisms, such as different SVT and &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;gradient descent&lt;/a>; mechanism variants. &lt;/p>; &lt;p>; &lt;strong>;Property testers&lt;/strong>; determine if evidence exists to reject the hypothesis that a given divergence between two probability distributions, &lt;em>;P&lt;/em>; and &lt;em>;Q&lt;/em>;, is bounded by a prespecified budget determined by the DP guarantee being tested. They compute a lower bound from samples from &lt;em>;P&lt;/em>; and &lt;em>;Q,&lt;/em>; rejecting the property if the lower bound value exceeds the expected divergence. No guarantees are provided if the result is indeed bounded. To test for a range of privacy guarantees, DP-Auditorium introduces three novel testers: (1) HockeyStickPropertyTester, (2) RényiPropertyTester, and (3) MMDPropertyTester. Unlike other approaches, these testers don&#39;t depend on explicit histogram approximations of the tested distributions. They rely on variational representations of the hockey-stick divergence, Rényi divergence, and &lt;a href=&quot;https://jmlr.csail.mit.edu/papers/v13/gretton12a.html&quot;>;maximum mean discrepancy&lt;/a>; (MMD) that enable the estimation of divergences through optimization over function spaces. As a baseline, we implement &lt;a href=&quot;https://arxiv.org/abs/1806.06427&quot;>;HistogramPropertyTester&lt;/a>;, a commonly used approximate DP tester. While our three testers follow a similar approach, for brevity, we focus on the HockeyStickPropertyTester in this post. &lt;/p>; &lt;p>; Given two neighboring datasets, &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>;, the HockeyStickPropertyTester finds a lower bound,&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp;for the hockey-stick divergence between &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) that holds with high probability. Hockey-stick divergence enforces that the two distributions &lt;em>;M&lt;/em>;(&lt;em>;D) &lt;/em>;and &lt;em>;M&lt;/em>;(&lt;em>;D&#39;&lt;/em>;) are close under an approximate DP guarantee. Therefore, if a privacy guarantee claims that the hockey-stick divergence is at most &lt;em>;δ&lt;/em>;, and&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; >; &lt;em>;δ&lt;/em>;, then with high probability the divergence is higher than what was promised on &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>; and the mechanism cannot satisfy the given approximate DP guarantee 。 The lower bound&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; is computed as an empirical and tractable counterpart of a variational formulation of the hockey-stick divergence (see &lt;a href=&quot;https://arxiv.org/pdf/2307.05608.pdf&quot;>;the paper&lt;/a>; for more details) 。 The accuracy of&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;&amp;nbsp; increases with the number of samples drawn from the mechanism, but decreases as the variational formulation is simplified. We balance these factors in order to ensure that&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>; &amp;nbsp; is both accurate and easy to compute. &lt;/p>; &lt;p>; &lt;strong>;Dataset finders&lt;/strong>; use &lt;a href=&quot;https://arxiv.org/pdf/2207.13676.pdf&quot;>;black-box optimization&lt;/a>; to find datasets &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>; that maximize&lt;i>;&lt;span style=&quot;bottom: 9px; left: 9px; position: relative; transfrom: scale(4,0.5);&quot;>;^&lt;/span>;δ&lt;/i>;, a lower bound on the divergence value &lt;em>;δ&lt;/em>;. Note that black-box optimization techniques are specifically designed for settings where deriving gradients for an objective function may be impractical or even impossible. These optimization techniques oscillate between exploration and exploitation phases to estimate the shape of the objective function and predict areas where the objective can have optimal values. In contrast, a full exploration algorithm, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&quot;>;grid search method&lt;/a>;, searches over the full space of neighboring datasets &lt;em>;D&lt;/em>; and &lt;em>;D&#39;&lt;/em>;. DP-Auditorium implements different dataset finders through the open sourced black-box optimization library &lt;a href=&quot;https://github.com/google/vizier&quot;>;Vizier&lt;/a>;. &lt;/p>; &lt;p>; Running existing components on a new mechanism only requires defining the mechanism as a Python function that takes an array of data &lt;em>;D&lt;/em>; and a desired number of samples &lt;em>;n&lt;/em>; to be output by the mechanism computed on &lt;em>;D&lt;/em>;. In addition, we provide flexible wrappers for testers and dataset finders that allow practitioners to implement their own testing and dataset search algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key results&lt;/h2>; &lt;p>; We assess the effectiveness of DP-Auditorium on five private and nine non-private mechanisms with diverse output spaces. For each property tester, we repeat the test ten times on fixed datasets using different values of &lt;em>;ε&lt;/em>;, and report the number of times each tester identifies privacy bugs. While no tester consistently outperforms the others, we identify bugs that would be missed by previous techniques (HistogramPropertyTester). Note that the HistogramPropertyTester is not applicable to SVT mechanisms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s785/image22.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;409&quot; data-original-width=&quot;785&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s16000/image22.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Number of times each property tester finds the privacy violation for the tested non-private mechanisms. NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Laplace_Mechanism&quot;>;Laplace&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Gaussian_Mechanism&quot;>;Gaussian&lt;/a>; mechanisms for computing the mean.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also analyze the implementation of a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py&quot;>;DP gradient descent algorithm&lt;/a>; (DP-GD) in TensorFlow that computes gradients of the loss function on private data. To preserve privacy, DP-GD employs a clipping mechanism to bound the &lt;a href=&quot;https://mathworld.wolfram.com/L2-Norm.html&quot;>;l2-norm&lt;/a>; of the gradients by a value &lt;em>;G&lt;/em>;, followed by the addition of Gaussian noise. This implementation incorrectly assumes that the noise added has a scale of &lt;em>;G&lt;/em>;, while in reality, the scale is &lt;em>;sG&lt;/em>;, where &lt;em>;s&lt;/em>; is a positive scalar 。 This discrepancy leads to an approximate DP guarantee that holds only for values of &lt;em>;s&lt;/em>; greater than or equal to 1. &lt;/p>; &lt;p>; We evaluate the effectiveness of property testers in detecting this bug and show that HockeyStickPropertyTester and RényiPropertyTester exhibit superior performance in identifying privacy violations, outperforming MMDPropertyTester and HistogramPropertyTester. Notably, these testers detect the bug even for values of &lt;em>;s&lt;/em>; as high as 0.6. It is worth highlighting that &lt;em>;s &lt;/em>;= 0.5 corresponds to a &lt;a href=&quot;https://github.com/tensorflow/privacy/blob/308cbda4db6ccad5d1e7d56248727274e4c0c79e/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py#L445C1-L446C1&quot;>;common error&lt;/a>; in literature that involves missing a factor of two when accounting for the privacy budget &lt;em>;ε&lt;/em>;. DP-Auditorium successfully captures this bug as shown below. For more details see section 5.6 &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;here&lt;/a>;. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;836&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Estimated divergences and test thresholds for different values of &lt;em>;s&lt;/em>; when testing DP-GD with the HistogramPropertyTester (&lt;strong>;left&lt;/strong>;) and the HockeyStickPropertyTester (&lt;strong>;right&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s828/image20.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;333&quot; data-original-width=&quot;828&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s16000/image20.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Estimated divergences and test thresholds for different values of &lt;em>;s&lt;/em>; when testing DP-GD with the RényiPropertyTester (&lt;strong>;left&lt;/strong>;) and the MMDPropertyTester (&lt;strong>;right&lt;/strong>;)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To test dataset finders, we compute the number of datasets explored before finding a privacy violation. On average, the majority of bugs are discovered in less than 10 calls to dataset finders. Randomized and exploration/exploitation methods are more efficient at finding datasets than grid search. For more details, see the &lt;a href=&quot;https://arxiv.org/abs/2307.05608&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DP is one of the most powerful frameworks for data protection. However, proper implementation of DP mechanisms can be challenging and prone to errors that cannot be easily detected using traditional unit testing methods. A unified testing framework can help auditors, regulators, and academics ensure that private mechanisms are indeed private. &lt;/p>; &lt;p>; DP-Auditorium is a new approach to testing DP via divergence optimization over function spaces. Our results show that this type of function-based estimation consistently outperforms previous black-box access testers. Finally, we demonstrate that these function-based estimators allow for a better discovery rate of privacy bugs compared to histogram estimation. By &lt;a href=&quot;https://github.com/google/differential-privacy/tree/main/python/dp_auditorium&quot;>;open sourcing&lt;/a>; DP-Auditorium, we aim to establish a standard for end-to-end testing of new differentially private algorithms. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described here was done jointly with Andrés Muñoz Medina, William Kong and Umar Syed. We thank Chris Dibak and Vadym Doroshenko for helpful engineering support and interface suggestions for our library.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5933365460125094774/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5933365460125094774&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html&quot; rel=&quot;alternate&quot; title=&quot;DP-Auditorium: A flexible library for auditing differential privacy&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3264694155710647310&lt;/id>;&lt;published>;2024-02-06T11:17:00.000-08:00&lt;/published>;&lt;updated>;2024-02-06T11:17:53.968-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graph Mining&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Graph neural networks in TensorFlow&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s1600/TFGNN%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks 。 Discrete mathematics and computer science have a long history of formalizing such networks as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graphs&lt;/a>;&lt;/em>;, consisting of &lt;em>;nodes&lt;/em>; connected by &lt;em>;edges&lt;/em>; in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;Graph neural networks&lt;/a>;, or GNNs for short, have emerged as a powerful technique to leverage both the graph&#39;s connectivity (as in the older algorithms &lt;a href=&quot;http://perozzi.net/projects/deepwalk/&quot;>;DeepWalk&lt;/a>; and &lt;a href=&quot;https://snap.stanford.edu/node2vec/&quot;>;Node2Vec&lt;/a>;) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What&#39;s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph&#39;s &lt;em>;discrete&lt;/em>;, &lt;em>;relational&lt;/em>; information in a &lt;em>;continuous&lt;/em>; way so that it can be included naturally in another deep learning system. &lt;/p>; &lt;p>; We are excited to announce the release of &lt;a href=&quot;https://github.com/tensorflow/gnn&quot;>;TensorFlow GNN 1.0&lt;/a>; (TF-GNN), a production-tested library for building GNNs at large scales. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs, where types of objects and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types, and TF-GNN&#39;s heterogeneous focus makes it natural to represent them. &lt;/p>; &lt;p>; Inside TensorFlow, such graphs are represented by objects of type &lt;code>;tfgnn.GraphTensor&lt;/code>;. This is a composite tensor type (a collection of tensors in one Python class) accepted as a &lt;a href=&quot;https://en.wikipedia.org/wiki/First-class_citizen&quot;>;first-class citizen&lt;/a>; in &lt;code>;tf.data.Dataset&lt;/code>;, &lt;code>;tf.function&lt;/code>;, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level &lt;a href=&quot;https://www.tensorflow.org/guide/keras&quot;>;Keras API&lt;/a>;, or directly using the &lt;code>;tfgnn.GraphTensor&lt;/code>; primitive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;GNNs: Making predictions for an object in context&lt;/h2>; &lt;p>; For illustration, let&#39;s look at one typical application of TF-GNN: predicting a property of a certain type of node in a graph defined by cross-referencing tables of a huge database. For example, a citation database of Computer Science (CS) arXiv papers with one-to-many cites and many-to-one cited relationships where we would like to predict the subject area of each paper. &lt;/p>; &lt;p>; Like most neural networks, a GNN is trained on a dataset of many labeled examples (~millions), but each training step consists only of a much smaller batch of training examples (say, hundreds). To scale to millions, the GNN gets trained on a stream of reasonably small subgraphs from the underlying graph. Each subgraph contains enough of the original data to compute the GNN result for the labeled node at its center and train the model. This process — typically referred to as subgraph sampling — is extremely consequential for GNN training. Most existing tooling accomplishes sampling in a batch way, producing static subgraphs for training. TF-GNN provides tooling to improve on this by sampling dynamically and interactively. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Pictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;p>; TF-GNN 1.0 debuts a flexible Python API to configure dynamic or batch subgraph sampling at all relevant scales: interactively in a Colab notebook (like &lt;a href=&quot;https://colab.research.google.com/github /tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;this one&lt;/a>;), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by &lt;a href =&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>; for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md&quot;>;in-memory&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md&quot;>;beam-based&lt;/a>; sampling, respectively. &lt;/p>; &lt;p>; On those same sampled subgraphs, the GNN&#39;s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node&#39;s neighborhood. One classical approach is &lt;a href=&quot;https://research.google/pubs/neural-message-passing-for-quantum-chemistry/&quot;>;message-passing neural networks&lt;/a>;. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After &lt;em>;n&lt;/em>; rounds, the hidden state of the root node reflects the aggregate information from all nodes within &lt;em>;n&lt;/em>; edges (pictured below for &lt;em>;n&lt;/em>; = 2) 。 The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;511&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The training setup is completed by placing an output layer on top of the GNN&#39;s hidden state for the labeled nodes, computing the &lt;em>;loss &lt;/em>;(to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training. &lt;/p>; &lt;p>; Beyond supervised training (ie, minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (ie, without labels). This lets us compute a &lt;em>;continuous&lt;/em>; representation (or &lt;em>;embedding&lt;/em>;) of the &lt;em>;discrete&lt;/em>; graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building GNN architectures&lt;/h2>; &lt;p>; The TF-GNN library supports building and training GNNs at various levels of abstraction. &lt;/p>; &lt;p>; At the highest level, users can take any of the predefined models bundled with the library that are expressed in Keras layers. Besides a small collection of models from the research literature, TF-GNN comes with a highly configurable model template that provides a curated selection of modeling choices that we have found to provide strong baselines on many of our in-house problems. The templates implement GNN layers; users need only to initialize the Keras layers. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;865&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s16000/TFGNN%20code1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; At the lowest level, users can write a GNN model from scratch in terms of primitives for passing data around the graph, such as broadcasting data from a node to all its outgoing edges or pooling data into a node from all its incoming edges (eg, computing the sum of incoming messages). TF-GNN&#39;s graph data model treats nodes, edges and whole input graphs equally when it comes to features or hidden states, making it straightforward to express not only node-centric models like the MPNN discussed above but also more general forms of &lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;GraphNets&lt;/a>;. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md&quot;>;user guide&lt;/a>; and &lt;a href=&quot;https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models&quot;>;model collection&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training orchestration&lt;/h2>; &lt;p>; While advanced users are free to do custom model training, the &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md&quot;>;TF-GNN Runner&lt;/a>; also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s1400/TFGNN%20code2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;508&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s16000/TFGNN%20code2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The Runner provides ready-to-use solutions for ML pains like distributed training and &lt;code>;tfgnn.GraphTensor&lt;/code>; padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;392&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s16000/TFGNN%20code3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Additionally, the TF-GNN Runner also includes an implementation of &lt;a href=&quot;https://www.tensorflow.org/tutorials/interpretability/integrated_gradients&quot;>;integrated gradients&lt;/a>; for use in model attribution. Integrated gradients output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In short, we hope TF-GNN will be useful to advance the application of GNNs in TensorFlow at scale and fuel further innovation in the field. If you&#39;re curious to find out more, please try our &lt;a href=&quot;https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb&quot;>;Colab demo&lt;/a>; with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our &lt;a href=&quot;https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/overview.md&quot;>;user guides and Colabs&lt;/a>;, or take a look at our &lt;a href=&quot;https://arxiv.org/abs/2207.03522&quot;>;paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The TF-GNN release 1.0 was developed by a collaboration between Google Research: Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle, Google Core ML: Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe, and Google DeepMind:&lt;strong>; &lt;/strong>;Alvaro Sanchez-Gonzalez and Lisa Wang.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3264694155710647310/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3264694155710647310&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html&quot; rel=&quot;alternate&quot; title=&quot;Graph neural networks in TensorFlow&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6956767920612914706&lt;/id>;&lt;published>;2024-02-02T11:07:00.000-08:00&lt;/published>;&lt;updated>;2024-02-07T16:05:00.722-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A decoder-only foundation model for time-series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Yichen Zhou, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time-series&lt;/a>; forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural科学。 In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&quot;>;improving demand forecasting accuracy&lt;/a>; can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (eg, DL models performed well in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0169207021001874&quot;>;M5 competition&lt;/a>;). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;translation&lt;/a>;, &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/&quot;>;retrieval-augmented generation&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligent_code_completion&quot;>;code completion&lt;/a>;. These models are trained on massive amounts of &lt;em>;textual &lt;/em>;data derived from a variety of sources like &lt;a href=&quot;https://commoncrawl.org/&quot;>;common crawl&lt;/a>; and open-source code that allows them to identify patterns in languages. This makes them very powerful &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-shot_learning&quot;>;zero-shot&lt;/a>; tools; for instance, &lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;when paired with retrieval&lt;/a>;, they can answer questions about and summarize current events 。 &lt;/p>; &lt;p>; Despite DL-based forecasters largely &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;outperforming&lt;/a>; traditional methods and progress being made in &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;reducing training and inference costs&lt;/a>;, they face challenges: most DL architectures require &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting&quot;>;long and involved training and validation cycles&lt;/a>; before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like &lt;a href=&quot;https://en.wikipedia.org/wiki/Customer_demand_planning&quot;>;retail demand planning&lt;/a>;. &lt;/p>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/pdf/2310.10688.pdf&quot;>;A decoder-only foundation model for time-series forecasting&lt;/a>;”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>;Google Cloud Vertex AI&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A decoder-only foundation model for time-series forecasting&lt;/h2>; &lt;p>; LLMs are usually trained in a &lt;a href=&quot;https://arxiv.org/pdf/1801.10198.pdf&quot;>;decoder-only&lt;/a>; fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformer&lt;/a>; layers that produce an output corresponding to each input token (it cannot attend to future tokens) 。 Finally, the output corresponding to the &lt;em>;i&lt;/em>;-th token summarizes all the information from previous tokens and predicts the (&lt;em>;i&lt;/em>;+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”. &lt;/p>; &lt;p>; A foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining数据集。 Similar to LLMs, we use stacked transformer layers (self-attention and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;>;feedforward&lt;/a>; layers) as the main building blocks for the TimesFM model 。 In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;long-horizon forecasting work&lt;/a>;. The task then is to forecast the (&lt;em>;i&lt;/em>;+1)-th patch of time-points given the &lt;em>;i&lt;/em>;-th output at the end of the stacked transformer layers. &lt;/p>; &lt;p>; However, there are several key differences from language models. Firstly, we need a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with &lt;a href=&quot;https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/&quot;>;positional encodings&lt;/a>; (PE). For that, we use a residual block similar to our prior work in &lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;long-horizon forecasting&lt;/a>;. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, ie, the output patch length can be larger than the input patch length. &lt;/p>; &lt;p>; Consider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;674&quot; data-original-width=&quot;1084&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TimesFM architecture.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Pretraining data&lt;/h2>; &lt;p>; Just like LLMs get better with more tokens, TimesFM requires a large volume of legitimate time series data to learn and improve. We have spent a great amount of time creating and assessing our training datasets, and the following is what we have found works best: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Synthetic data helps with the basics.&lt;/strong>; Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting. &lt;/p>;&lt;/div>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;strong>;Real-world data adds real-world flavor.&lt;/strong>; We comb through available public time series datasets, and selectively put together a large corpus of 100 billion time-points. Among these datasets there are &lt;a href=&quot;https://trends.google.com/trends/&quot;>;Google Trends&lt;/a>; and &lt;a href=&quot;https://meta.wikimedia.org/wiki/Research:Page_view&quot;>;Wikipedia Pageviews&lt;/a>;, which track what people are interested in, and that nicely mirrors trends and patterns in many other real-world time series. This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training. &lt;/p>;&lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Zero-shot evaluation results&lt;/h2>; &lt;p>; We evaluate TimesFM zero-shot on data not seen during training using popular time-series benchmarks. We observe that TimesFM performs better than most statistical methods like &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average&quot;>;ARIMA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_smoothing&quot;>;ETS&lt;/a>; and can match or outperform powerful DL models like &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; that have been &lt;em>;explicitly trained&lt;/em>; on the target time-series. &lt;/p>; &lt;p>; We used the &lt;a href=&quot;https://huggingface.co/datasets/monash_tsf&quot;>;Monash Forecasting Archive&lt;/a>; to evaluate TimesFM&#39;s out-of-the-box performance. This archive contains tens of thousands of time-series from various domains like traffic, weather, and demand forecasting covering frequencies ranging from few minutes to yearly data. Following existing literature, we inspect the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;>;mean absolute error&lt;/a>; (MAE) &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;appropriately scaled&lt;/a>; so that it can be averaged across the datasets. We see that zero-shot (ZS) TimesFM is better than most supervised approaches, including recent deep learning models. We also compare TimesFM to &lt;a href=&quot;https://platform.openai.com/docs/models/gpt-3-5&quot;>;GPT-3.5&lt;/a>; for forecasting using a specific prompting technique proposed by &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime(ZS)&lt;/a>;. We demonstrate that TimesFM performs better than llmtime(ZS) despite being orders of magnitude smaller. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;876&quot; data-original-width=&quot;1476&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Scaled MAE (the lower the better) of TimesFM(ZS) against other supervised and zero-shot approaches on Monash datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Most of the Monash datasets are short or medium horizon, ie, the prediction length is not too long. We also test TimesFM on popular benchmarks for long horizon forecasting against a recent state-of-the-art baseline &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>; (and other long-horizon forecasting baselines). In the next figure, we plot the MAE on &lt;a href=&quot;https://paperswithcode.com/dataset/ett&quot;>;ETT&lt;/a>; datasets for the task of predicting 96 and 192 time-points into the future. The metric has been calculated on the last test window of each dataset (as done by the &lt;a href=&quot;https://arxiv.org/abs/2310.07820&quot;>;llmtime&lt;/a>; paper). We see that TimesFM not only surpasses the performance of llmtime(ZS) but also matches that of the supervised PatchTST model explicitly trained on the respective datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;433&quot; data-original-width=&quot;735&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Last window MAE (the lower the better) of TimesFM(ZS) against llmtime(ZS) and long-horizon forecasting baselines on ETT datasets.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We train a decoder- only foundation model for time-series forecasting using a large pretraining corpus of 100B real world time-points, the majority of which was search interest time-series data derived from Google Trends and pageviews from Wikipedia. We show that even a relatively small 200M parameter pretrained model that uses our TimesFM architecture displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov and Yichen Zhou.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6956767920612914706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6956767920612914706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html&quot; rel=&quot;alternate&quot; title=&quot;A decoder-only foundation model for time-series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502&lt;/id>;&lt;published>;2024-02-02T09:49:00.000-08:00&lt;/published>;&lt;updated>;2024-02-02T09:49:36.211-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML Fairness&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Intervening on early readouts for mitigating spurious features and simplicity bias&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Machine learning models in the real world are often trained on limited data that may contain unintended &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias_(statistics)&quot;>;statistical biases&lt;/a >;。 For example, in the &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CELEBA&lt;/a>; celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as &lt;a href=&quot;https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging&quot;>;medical diagnosis&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Surprisingly, recent work has also discovered an inherent tendency of deep networks to &lt;em>;amplify such statistical biases&lt;/em>;, through the so-called &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf&quot;>;simplicity bias&lt;/a>; of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. &lt;/p>; &lt;p>; With the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying &lt;em>;early readouts&lt;/em>; and &lt;em>;feature forgetting&lt;/em>; 。 First, in “&lt;a href=&quot;https://arxiv.org/abs/2310.18590&quot;>;Using Early Readouts to Mediate Featural Bias in Distillation&lt;/a>;”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;>;model distillation&lt;/a>;, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “&lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;Overcoming Simplicity Bias in Deep Networks using a Feature Sieve&lt;/a>;”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model&#39;s ability to generalize to unseen domains compared to previous approaches. Our &lt;a href=&quot;https://ai.google/responsibility/principles&quot;>;AI Principles&lt;/a>; and our &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;1080&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation comparing hypothetical responses from two models trained with and without the feature sieve.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Early readouts for debiasing distillation&lt;/h2>; &lt;p>; We first illustrate the diagnostic value of &lt;em>;early readouts&lt;/em >; and their application in debiased distillation, ie, making sure that the student model inherits the teacher model&#39;s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the &lt;a href=&quot;https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e&quot;>;cross-entropy loss&lt;/a>; between student outputs and the ground-truth labels) and teacher matching (minimizing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL divergence&lt;/a>; loss between student and teacher outputs for any given input). &lt;/p>; &lt;p>; Suppose one trains a linear decoder, ie, a small auxiliary neural network named as &lt;em>;Aux,&lt;/em>; on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model&#39;s dependence on potentially spurious features. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;796&quot; data-original-width=&quot;1128&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrating the usage of early readouts (ie, output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used this signal to modulate the contribution of the teacher in the distillation loss on a per-instance basis, and found significant improvements in the trained student model as a result. &lt;/p>; &lt;p>; We evaluated our approach on standard benchmark datasets known to contain spurious correlations (&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;>;Waterbirds&lt;/a>;, &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;>;CelebA&lt;/a>;, &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/civil_comments&quot;>;CivilComments&lt;/a>;, &lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;>;MNLI&lt;/a>;). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color 。 Thus, a measure of model performance is its &lt;em>;worst group accuracy&lt;/em>;, ie, the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our &lt;a href=&quot;https://arxiv.org/pdf/2310.18590.pdf&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1270&quot; height=&quot;511&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overcoming simplicity bias with a feature sieve&lt;/h2>; &lt;p>; In a second, closely related project, we intervene directly on the information provided by early readouts, to improve &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;feature learning&lt;/a>; and &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/generalization/video-lecture&quot;>;generalization&lt;/a>;. The workflow alternates between &lt;em>;identifying &lt;/em>;problematic features and &lt;em>;erasing identified features&lt;/em>; from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;604&quot; data-original-width=&quot;1098&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;Training workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We describe the identification and erasure steps in more detail: &lt;/p>; &lt;ul>; &lt;li>;&lt;b>;Identifying simple features&lt;/b>;: We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. &lt;/li>;&lt;li>;&lt;b>;Applying the feature sieve&lt;/b>;: We aim to erase the identified features in the early layers of the neural network with the use of a novel &lt;em>;forgetting loss&lt;/em>;,&lt;em>; L&lt;sub>;f &lt;/sub>;&lt;/em>;, which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged. &lt;/li>; &lt;/ul>; &lt;p>; We can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter search&lt;/a>; to maximize validation accuracy, a standard measure of generalization. Since we include “no-forgetting” (ie, the baseline model) in the search space, we expect to find settings that are at least as good as the baseline. &lt;/p>; &lt;p>; Below we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (&lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;) and animal categorization (&lt;a href=&quot;https://arxiv.org/pdf/1906.02899v3.pdf&quot;>;NICO&lt;/a>;). Feature importance was estimated using post-hoc gradient-based importance scoring (&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;>;GRAD-CAM&lt;/a>;), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;850&quot; data-original-width=&quot;1616&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Feature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this ability to learn better, generalizable features, we show substantial gains over a range of relevant baselines on real-world spurious feature benchmark datasets: &lt;a href=&quot;https://github.com/alinlab/BAR&quot;>;BAR&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2104.06885.pdf&quot;>;CelebA Hair&lt;/a>;, &lt;a href=&quot;https://nico.thumedialab.com/&quot;>;NICO&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/imagenet_a&quot;>;ImagenetA&lt;/a>;, by margins up to 11% (see figure below). More details are available in &lt;a href=&quot;https://arxiv.org/abs/2301.13293&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1082&quot; data-original-width=&quot;844&quot; height=&quot;640&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png&quot; width=&quot;501&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We hope that our work on early readouts and their use in feature sieving for generalization will both spur the development of a new class of adversarial feature learning approaches and help improve the generalization capability and robustness of deep learning systems. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;The work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at &lt;a href=&quot;https://www.iitb.ac.in/&quot;>;IIT Bombay&lt;/a>;. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2254287928040727502/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2254287928040727502&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/02/intervening-on-early-readouts-for.html&quot; rel=&quot;alternate&quot; title=&quot;Intervening on early readouts for mitigating spurious features and simplicity bias&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984&lt;/id>;&lt;published>;2024-01-31T13:59:00.000-08:00&lt;/published>;&lt;updated>;2024-01-31T13:59:36.056-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MobileDiffusion: Rapid text-to-image generation on-device&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;>;diffusion models&lt;/a>; have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (eg, &lt;a href=&quot;https://stability.ai/news/stable-diffusion-public-release&quot;>;Stable Diffusion&lt;/a>;, &lt;a href=&quot;https://openai.com/research/dall-e&quot;>;DALL·E&lt;/a>;, and &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;). While recent advancements in inference solutions on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;Android&lt;/a>; via MediaPipe and &lt;a href=&quot;https://github.com/apple/ml-stable-diffusion&quot;>;iOS&lt;/a>; via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To that end, in “&lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices&lt;/a>;”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style =&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;369&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rapid text-to-image generation on-device.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires &lt;a href=&quot;https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html&quot;>;iterative denoising&lt;/a>; to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature. &lt;/p>; &lt;p>; The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.00927&quot;>;DPM&lt;/a>;) or distillation techniques (eg, &lt;a href=&quot;https://arxiv.org/ abs/2202.00512&quot;>;progressive distillation&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.01469&quot;>;consistency distillation&lt;/a>;), the number of necessary sampling steps have significantly reduced from several hundreds到个位数。 Some recent techniques, like &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.&quot;>;Adversarial Diffusion Distillation&lt;/a>;, even reduce to a single necessary step. &lt;/p>; &lt;p>; However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (eg, &lt;a href=&quot;https://snap-research.github.io/SnapFusion/&quot;>;SnapFusion&lt;/a>;). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MobileDiffusion&lt;/h2>; &lt;p>; Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model&#39;s architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion&#39;s &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;UNet architecture&lt;/a>;. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion. &lt;/p>; &lt;p>; The design of MobileDiffusion follows that of &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;latent diffusion models&lt;/a>;. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP-ViT/L14&lt;/a>;, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Diffusion UNet&lt;/h3>; &lt;p>; As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (eg, data, optimizer) to study the effects of different architectures. &lt;/p>; &lt;p>; In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of &lt;a href=&quot;https://arxiv.org/abs/2301.11093&quot;>;UViT&lt;/a>; architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV /s915/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;249&quot; data-original-width=&quot;915&quot; src =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Convolution blocks, in particular &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;>;separable convolution&lt;/a>;. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance. &lt;/p>; &lt;p>; In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of &lt;a href=&quot;https://arxiv.org/pdf/2110.12894.pdf&quot;>;FLOPs&lt;/a>; (floating-point operations) and number of parameters. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Comparison of some diffusion UNets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt; br />; &lt;/div>; &lt;h3>;Image decoder&lt;/h3>; &lt;p>; In addition to the UNet, we also optimized the image decoder. We trained a &lt;a href=&quot;https://arxiv.org/abs/2012.03715&quot;>;variational autoencoder&lt;/a>; (VAE) to encode an &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size. To further enhance efficiency, we design a lightweight decoder architecture by pruning the original&#39;s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our &lt;a href=&quot;https://arxiv.org/abs/2311.16567&quot;>;paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png&quot; style=&quot;margin- left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;789&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Decoder&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;#Params (M)&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;PSNR↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;SSIM↑&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;LPIPS↓&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;SD&lt;/b>; &lt;/td>; &lt;td>;49.5 &lt;/td>; &lt;td>;26.7 &lt;/td>; &lt;td>;0.76 &lt;/td>; &lt;td>;0.037 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours&lt;/b>; &lt;/td>; &lt;td>;39.3 &lt;/td>; &lt;td>;30.0 &lt;/td>; &lt;td>;0.83 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;b>;Ours-Lite&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;9.8 &lt;/td>; &lt;td>;30.2 &lt;/td>; &lt;td>;0.84 &lt;/td>; &lt;td>;0.032 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Quality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&quot;>;peak signal-to-noise ratio&lt;/a>; (PSNR), &lt;a href=&quot;https://en.wikipedia.org/wiki/Structural_similarity&quot;>;structural similarity index measure&lt;/a>; (SSIM), and &lt;a href=&quot;https://arxiv.org/abs/1801.03924&quot;>;Learned Perceptual Image Patch Similarity&lt;/a>; (LPIPS).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;One-step sampling&lt;/h3>; &lt;p>; In addition to optimizing the model architecture, we adopt a &lt;a href=&quot;https://arxiv.org/abs/2311.09257&quot;>;DiffusionGAN hybrid&lt;/a>; to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (eg, &lt;a href=&quot;https://arxiv.org/abs/2301.09515&quot;>;StyleGAN-T&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2303.05511&quot;>;GigaGAN&lt;/a>;) confront similar complexities, resulting in highly intricate and expensive training. &lt;/p>; &lt;p>; To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training. &lt;/p>; &lt;p>; The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ -UILKw/s960/image7.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;576&quot; data-original-width=&quot;960 &quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of DiffusionGAN fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling 。 With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1728&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images generated by our MobileDiffusion&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1184&quot; src= &quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency measurements (&lt;b>;s&lt;/b>;) on mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google&#39;s &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;responsible AI practices&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.&lt; /em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5966553114967673984/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot; />;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5966553114967673984&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http: //blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html&quot; rel=&quot;alternate&quot; title=&quot;MobileDiffusion: Rapid text-to-image generation on-device&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd :image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot; 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c /InstantTIGO%20hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt; /entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5144906729109253495&lt;/id>;&lt;published>;2024-01-26T11:56:00.000-08:00&lt;/published>;&lt;updated >;2024-01-26T11:56:23.553-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term =&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Mixed-input matrix multiplication performance optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Manish Gupta , Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;display: none ;” />; &lt;p>; AI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs). LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver &lt;a href=&quot;https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e&quot;>;tens of exaflops&lt;/a>; of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The bulk of an LLM&#39;s memory and compute are consumed by &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;>;weights&lt;/a>; in &lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;matrix multiplication&lt;/a>; operations. Using narrower &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Primitive_data_type&quot;>;data types&lt;/a>;&lt;/em>; reduces memory consumption. For example, storing weights in the 8-bit &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_(computer_science)&quot;>;integer&lt;/a>; (ie, U8 or S8) data type reduces the memory footprint by 4× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;>;single-precision&lt;/a>; (F32) and 2× relative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Half-precision_floating-point_format&quot;>;half-precision&lt;/a>; (F16) or &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format&quot;>;bfloat16&lt;/a>; (BF16). Furthermore, &lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;previous work has&lt;/a>; shown that LLM models running matrix multiplications with &lt;em>;weights&lt;/em>; in S8 and &lt;em>;input&lt;/em>; in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as &lt;em>;weight-only quantization&lt;/em>; and requires efficient implementation of matrix multiplication with &lt;em>;mixed-inputs&lt;/em>;, eg, half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations. &lt;/p>; &lt;p>; To that end, in this blog we focus on mapping mixed-input matrix multiplication onto the &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/&quot;>;NVIDIA Ampere architecture&lt;/a>;. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; repository. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Memory footprint for an 175B parameter LLM model with various data types formats.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The matrix-multiply-accumulate operation&lt;/h2>; &lt;p>; Modern AI hardware accelerators such as &lt;a href= &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google&#39;s TPU&lt;/a>; and &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA&#39;s GPU&lt;/a>; multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the &lt;em>;matrix-multiply-accumulate&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;) operation. For the rest of the blog the reference to &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation are fixed in hardware 。 This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. &lt;/p>; &lt;p>; The Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation is defined by specifying two input matrices (eg, &lt;em>;A&lt;/em>; &amp;amp; &lt;em>;B&lt;/em>;, shown below) to produce a result matrix, &lt;em>;C&lt;/em>;. The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation natively supports mixed-precision. &lt;em>;&lt;a href=&quot;https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/&quot;>;Mixed-precision Tensor Cores&lt;/a>;&lt;/em>; allow mixing input (&lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) data type with the result (&lt;em>;C&lt;/em>;) data type. In contrast, &lt;em>;mixed-input &lt;/em>;matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Tensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Challenges of mixed -input matrix multiplication&lt;/h2>; &lt;p>; To simplify the discussion, we restrict to a specific example of mixed-input matrix multiplication: F16 for user input and U8 for the model weights (written as F16 * U8). The techniques described here work for various combinations of mixed-input data types. &lt;/p>; &lt;p>; A GPU programmer can access a &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;hierarchy of memory&lt;/a>;, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a &lt;em>;warp&lt;/em>;. The supported data type &lt;em>;and&lt;/em>; layout within a warp are fixed for an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Data type conversion &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the &lt;em>;mixed-input&lt;/em>; matrix multiplication to hardware-supported &lt;em>;mixed-precision&lt;/em>; Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Layout conformance &lt;/h3>; &lt;p>; The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix &lt;em>;B&lt;/em>; of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called &lt;em>;layout conformance&lt;/em>; and needs to be achieved in the software. &lt;/p>; &lt;p>; The figure below shows an &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation consuming matrix &lt;em>;A&lt;/em>; and matrix &lt;em>;B&lt;/em>; from registers to produce matrix &lt;em>;C&lt;/em>; in registers, distributed across one warp. The thread &lt;em>;T0&lt;/em>; is highlighted and zoomed in to show the weight matrix &lt;em>;B&lt;/em>; goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;The mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). (Original figure source &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100&lt;/a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Software strategies addressing challenges&lt;/h2>; &lt;p>; A typical data type conversion involves a sequence of operations on 32-bit registers, shown below. Each rectangular block represents a register and the adjoining text are the operations. The entire sequence shows the conversion from 4xU8 to 2x(2xF16). The sequence involves roughly 10 operations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; from 4xU8 to 2x(2xF16) in 32-bit registers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; There are many ways of achieving layout conformance. Two of the existing solutions are: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Narrower bitwidth shared memory loads&lt;/em>;: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in &lt;em>;two&lt;/em>; 32-bit registers, with each register containing 2xF16 values (shown above for the matrix &lt;em>;B&lt;/em>;&#39;s thread &lt;em>;T0&lt;/em>;). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth. &lt;/li>;&lt;li>;&lt;em>;Pre-processing in global memory&lt;/em>;: An &lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;alternative strategy&lt;/a>; involves rearranging the data within the global memory (one level above the shared memory in &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;memory hierarchy&lt;/a>;), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; adopts this method to effectively address layout conformance challenges. &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimized software strategies&lt;/h2>; &lt;p>; To further optimize and reduce the overhead of data type conversion and layout conformance, we have implemented &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a>;&lt;/code>; and &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;&lt;/code>;, respectively. &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. &lt;/p>; &lt;p>; The conversion sequence for &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>; is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. &lt;code>;FastNumericArrayConvertor&lt;/code>; uses the &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt&quot;>;permute byte&lt;/a>;&lt;/code>; to rearrange bytes of 4xU8 into two registers. Additionally, &lt;code>;FastNumericArrayConvertor&lt;/code>; does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in &lt;em>;two&lt;/em>; 32-bit registers containing 2x(2xF16) values. The &lt;code>;FastNumericArrayConvertor&lt;/code>; for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; utilizes &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;permute bytes&lt;/a>;&lt;/code>; and packed arithmetic, reducing the number of instructions in the data type conversion.&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations 。 &lt;/p>; &lt;p>; NVIDIA Ampere architecture provides a load matrix instruction (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;). The &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the &lt;em>;shape&lt;/em>; and &lt;em>;layout&lt;/em>; that &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>; consume. The use of &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; &lt;em>;reduces&lt;/em>; the number of load instructions and &lt;em>;increases&lt;/em>; the memory bandwidth utilization. Since the &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; instruction moves U8 data to registers, the layout after the load conforms with U8*U8 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation, and not with F16*F16 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; operation 。 We implemented &lt;code>;FragmentShuffler&lt;/code>; to rearrange the data within registers using shuffle (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; operations to achieve the layout conformance. &lt;/p>;&lt;p>; The most significant contribution of this work is to achieve layout conformance through register shuffles, avoiding offline pre-processing in global memory or narrower bitwidth shared memory loads. Furthermore, we provide implementations for &lt;code>;FastNumericArrayConvertor&lt;/code>; covering data type conversion from &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8-to-F16&lt;/a>;, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8-to-BF16&lt;/a>;, and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8-to-BF16&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance results&lt;/h2>; &lt;p>; We measured the performance of eight mixed-input variants of &lt;em>;our method&lt;/em>; (shown below in blue and red; varying the data types of matrix &lt;em>;A&lt;/em>; and &lt;em>;B&lt;/em>;) and two &lt;em>;mixed-precision&lt;/em>; data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Mixed-input matrix multiplication performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape &lt;code>;m=3456, n=4096, k=2048.&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p >; &lt;em>;We would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia. We would like to thank our NVIDIA partners Rawn Henry, Pradeep Ramani, Vijay Thakkar, Haicheng Wu, Andrew Kerr, Matthew Nicely, and Vartika Singh.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:/ /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;Mixed-input matrix multiplication performance optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;published >;2024-01-23T14:27:00.000-08:00&lt;/published>;&lt;updated>;2024-01-23T14:27:09.785-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer: Scaling transformers for graph-structured data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;Graphs&lt;/a>;, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A common approach to learning on graphs are &lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;graph neural networks&lt;/a>; (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2&quot;>;message-passing&lt;/a>; framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors. &lt;/p>; &lt;p>; Recently, &lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;graph transformer models&lt;/a>; have emerged as a popular alternative to message-passing GNNs. These models build on the success of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer architectures&lt;/a>; in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism&lt;em>; &lt;/em>;that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see &lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot;>;the first open problem here&lt;/a>;). &lt;/p>; &lt;p>; A natural remedy is to use a &lt;em>;sparse&lt;/em>; interaction graph with fewer edges. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;Many sparse and efficient transformers have been proposed&lt;/a>; to eliminate the quadratic bottleneck for sequences, however, they do not generally extend to graphs in a principled manner. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer: Sparse Transformers for Graphs&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;spectral graph theory&lt;/a>;, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Expander graphs&lt;/h2>; &lt;p>; A key idea at the heart of Exphormer is the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph&quot;>;expander graphs&lt;/a>;, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, ie, a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes. &lt;/p>; &lt;p>; A common class of expander graphs are &lt;em>;d&lt;/em>;-regular expanders, in which there are &lt;em>;d&lt;/em>; edges from every node (ie, every node has degree &lt;em>;d&lt;/em>;). The quality of an expander graph is measured by its &lt;em>;spectral gap&lt;/em>;, an algebraic property of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjacency_matrix&quot;>;adjacency matrix&lt;/a>; (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Ramanujan_graph&quot;>;Ramanujan graphs&lt;/a>; — they achieve a gap of &lt;em>;d&lt;/em>; - 2*√(&lt;em>;d&lt;/em>;-1), which is essentially the best possible among &lt;em>;d&lt;/em>;-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of &lt;em>;d&lt;/em>;. We use a &lt;a href=&quot;https://arxiv.org/abs/cs/0405020&quot;>;randomized expander construction of Friedman&lt;/a>;, which produces near-Ramanujan graphs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;843&quot; data-original-width=&quot;800&quot; height=&quot;320&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968&quot;>;&lt;span face=&quot;Arial, sans- serif&quot; style=&quot;font-size: 10pt;字体风格：斜体； font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; font-variant-position: normal; vertical-align: baseline; white-space-collapse: preserve;&quot;>;Expander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.&lt;/span>;&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse &lt;em>;d&lt;/em>;-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that &lt;em>;d&lt;/em>; is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.&lt;/p>; &lt;br />; &lt;h2>;Exphormer: Constructing a sparse interaction graph&lt;/h2>; &lt;p>; Exphormer combines expander edges with the input graph and virtual nodes. More specifically, the sparse attention mechanism of Exphormer builds an interaction graph consisting of three types of edges: &lt;/p>; &lt;ul>; &lt;li>;Edges from the input graph (&lt;em>;local attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from a constant-degree expander graph (&lt;em>;expander attention&lt;/em>;) &lt;/li>;&lt;li>;Edges from every node to a small set of virtual nodes (&lt;em>;global attention&lt;/em>;) &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span id=&quot;docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3&quot;>;&lt;span face=&quot;Arial, sans-serif&quot; style=&quot;font-size: 10pt;字体风格：斜体； font-variant-alternates: normal; font-variant-east-asian: normal; font-variant-numeric: normal; font-variant-position: normal; vertical-align: baseline; white-space-collapse: preserve;&quot;>;Exphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.&lt;/span>;&lt;/ span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Each component serves a specific purpose: the edges from the input graph retain the inductive bias from the input graph structure (which typically gets lost in a fully-connected attention module). Meanwhile, expander edges allow good global connectivity and random walk mixing properties (which spectrally approximate the complete graph with far fewer edges). Finally, virtual nodes serve as global “memory sinks” that can directly communicate with every node. While this results in additional edges from each virtual node equal to the number of nodes in the input graph, the resulting graph is still sparse. The degree of the expander graph and the number of virtual nodes are hyperparameters to tune for improving the quality指标。 &lt;/p>; &lt;p>; Furthermore, since we use an expander graph of constant degree and a small constant number of virtual nodes for the global attention, the resulting sparse attention mechanism is linear in the size of the original input graph, ie, it models a number of direct interactions on the order of the total number of nodes and edges. &lt;/p>; &lt;p>; We additionally show that Exphormer is as expressive as the dense transformer and obeys universal approximation properties. In particular, when the sparse attention graph of Exphormer is augmented with self loops (edges connecting a node to itself), it can universally approximate continuous functions [&lt;a href=&quot;https://arxiv.org/abs/1912.10077&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>;2&lt;/a>;]. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Relation to sparse Transformers for sequences&lt;/h3>; &lt;p>; It is interesting to compare Exphormer to sparse attention methods for sequences. Perhaps the architecture most conceptually similar to our approach is &lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>;BigBird&lt;/a>;, which builds an interaction graph by combining different components. BigBird also uses virtual nodes, but, unlike Exphormer, it uses window attention and random attention from an &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model&quot;>;Erdős-Rényi&lt;/a>; random graph model for the remaining components. &lt;/p>; &lt;p>; Window attention in BigBird looks at the tokens surrounding a token in a sequence — the local neighborhood attention in Exphormer can be viewed as a generalization of window attention to graphs. &lt;/p>; &lt;p>; The Erdős-Rényi graph on &lt;em>;n&lt;/em>; nodes, &lt;em>;G(n, p)&lt;/em>;, which connects every pair of nodes independently with probability &lt;em>;p&lt;/em>;, also functions as an expander graph for suitably high &lt;em>;p&lt;/em>;. However, a superlinear number of edges (Ω(&lt;em>;n&lt;/em>; log &lt;em>;n&lt;/em>;)) is needed to ensure that an Erdős-Rényi graph is connected, let alone a good expander. On the other hand, the expanders used in Exphormer have only a &lt;em>;linear&lt;/em>; number of edges. &lt;/p>; &lt;br />; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; Earlier works have shown the use of full graph Transformer-based models on datasets with graphs of size up to 5,000 nodes. To evaluate the performance of Exphormer, we build upon the celebrated &lt;a href=&quot;https://github.com/rampasek/GraphGPS&quot;>;GraphGPS framework&lt;/a>; [&lt;a href=&quot;https://arxiv.org/abs/2205.12454&quot;>;3&lt;/a>;], which combines both message passing and graph transformers and achieves state-of-the-art performance on a number of datasets. We show that replacing dense attention with Exphormer for the graph attention component in the GraphGPS framework allows one to achieve models with comparable or better performance, often with fewer trainable parameters. &lt;/p>; &lt;p>; Furthermore, Exphormer notably allows graph transformer architectures to scale well beyond the usual graph size limits mentioned above. Exphormer can scale up to datasets of 10,000+ node graphs, such as the &lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>;Coauthor dataset&lt;/a>;, and even beyond to larger graphs such as the well-known &lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>;ogbn-arxiv dataset&lt;/a>;, a citation network, which consists of 170K nodes and 1.1 million edges. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance .png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original- height=&quot;190&quot; data-original-width=&quot;1522&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot; https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper&#39;s publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;&quot;>;&lt;img alt=&quot;&quot; border=&quot;0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results comparing Exphormer to standard GraphGPS on the five &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>; datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td align=&quot;left&quot;>;&lt;strong>;Model&amp;nbsp;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PascalVOC-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;COCO-SP&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;F1 score &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Func&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;AP &lt;/font>;&lt;strong>;↑&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;Peptides-Struct&amp;nbsp;&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MAE &lt;/font>;&lt;strong>;↓&lt;/strong>;&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&amp;nbsp;PCQM-Contact&lt;/strong>; &lt;br>; &amp;nbsp;&lt;font size=&quot;-1&quot;>;MRR &lt;/font>;&lt;strong>;↑&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>;&lt;td colspan=&quot;6&quot;>;&lt;div style=&quot;line-height: 40%;&quot;>;&lt;br />;&lt;/div>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;Standard GraphGPS&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.375 ± 0.011&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.341 ± 0.004&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;&lt;strong>;0.654 ± 0.004&lt;/strong>; &amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.250 ± 0.001&amp;nbsp; &lt;/td>; &lt;td align=&quot;center&quot;>;&amp;nbsp;0.334 ± 0.001 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Exphormer (ours)&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.398 ± 0.004&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.346 ± 0.001 &amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;em>;&amp;nbsp;0.653 ± 0.004&amp;nbsp;&lt;/em>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong >;&lt;em>;&amp;nbsp;0.248 ± 0.001&amp;nbsp;&lt;/em>;&lt;/strong>; &lt;/td>; &lt;td align=&quot;center&quot;>;&lt;strong>;&lt;em>;&amp;nbsp;0.364 ± 0.002&lt;/em>;&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>;-->; &lt;p>; Finally, we observe that Exphormer, which creates an overlay graph of small diameter via expanders, exhibits the ability to effectively learn long-range dependencies 。 The &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;Long Range Graph Benchmark&lt;/a>;&amp;nbsp;is a suite of five graph learning datasets designed to measure the ability of models to capture long-range interactions 。 Results show that Exphormer-based models outperform standard GraphGPS models (which were previously state-of-the-art on four out of five datasets at the time of publication). &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Graph transformers have emerged as an important architecture for ML that adapts the highly successful sequence-based transformers used in NLP to graph-structured data. Scalability has, however, proven to be a major challenge in enabling the use of graph transformers on datasets with large graphs. In this post, we have presented Exphormer, a sparse attention framework that uses expander graphs to improve scalability of graph transformers. Exphormer is shown to have important theoretical properties and exhibit strong empirical performance, particularly on datasets where it is crucial to learn long range dependencies. For more information, we point the reader to a short presentation &lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;video&lt;/a>; from ICML 2023. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our research collaborators Hamed Shirzad and Danica J. Sutherland from The University of British Columbia as well as Ali Kemal Sinop from Google Research. Special thanks to Tom Small for creating the animation used in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1418736582601940076/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1418736582601940076&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html&quot; rel=&quot;alternate&quot; title=&quot;Exphormer: Scaling transformers for graph-structured data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;