<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com,1999:blog-8474926331452026626</id><updated> 2023-06-18T05:01:07.306-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="conference"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="NLP"></category><category term="CVPR"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Machine Intelligence"></category><category term="Computer Science"></category><category term="On-device Learning"></category><category term="MOOC"></category><category term="HCI"></category><category term="Security and Privacy"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="Self-Supervised Learning"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="NeurIPS"></category><category term="accessibility"></category><category term="optimization"></category><category term="Android"></category><category term="Audio"></category><category term="ACL"></category><category term="Awards"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="DeepMind"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Responsible AI"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Acoustic Modeling"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Semantic Models"></category><category term="Systems"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Recommender Systems"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Social Networks"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="RAI-HCT Highlights"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="Differential Privacy"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新消息。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1243&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2312998356111901143&lt;/id>;&lt;published>;2023-06-15T13:53:00.000-07:00&lt;/published>;&lt;updated>;2023-06- 15T13:53:27.418-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android&quot;>;&lt;/category>;&lt;category scheme=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;速度就是你所需要的：通过 GPU 感知的大型扩散模型的设备上加速优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Juhyun Lee 和 Raman Sarokin 发布，软件工程师，核心系统和经验&lt;/span>; SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s700/SpeedHero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;扩散模型&lt;/a>;的扩散&lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;图像生成&lt;/a>;导致模型大小和推理工作量显着增加。移动环境中的设备端 ML 推理需要细致的性能优化和考虑因资源限制而产生的权衡。在成本效率和用户隐私需求的驱动下，在设备上运行大型扩散模型 (LDM) 的推理会带来更大的挑战，因为这些模型需要大量内存和计算需求。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们在题为“&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;Speed Is All You”的工作中应对这一挑战需要：通过 GPU 感知优化在设备上加速大型扩散模型&lt;/a>;”（将在 &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/ 上展示） a>; &lt;a href=&quot;https://sites.google.com/corp/view/ecv23&quot;>;计算机视觉的高效深度学习&lt;/a>;）研讨会，重点关注移动设备上基础 LDM 模型的优化执行显卡。在这篇博文中，我们总结了在全分辨率（512x512 像素）下成功执行大型扩散模型（如&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;稳定扩散&lt;/a>;）所采用的核心技术以及在现代智能手机上进行 20 次迭代，具有原始模型的高性能推理速度，而无需蒸馏不到 12 秒。正如&lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;我们之前的博文&lt;/a>;中所讨论的，GPU 加速的 ML 推理是通常受内存性能限制，LDM 的执行也不例外。因此，我们优化的中心主题是高效的内存输入/输出 (I/O)，即使这意味着选择内存高效的算法而不是那些优先考虑算术逻辑单元效率的算法。最终，我们的主要目标是减少 ML 推理的整体延迟。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RD x-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s512/image4.jpg&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;512&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14 ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;Mobile GPU 上 LDM 的样本输出，带有提示文本：“一张可爱的小狗周围有鲜花的逼真高分辨率图像”。&lt;/td>;&lt;/tr>; /tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;提高记忆效率的注意力模块&lt;/h2>; &lt;p>; ML 推理引擎通常提供各种优化的 ML 操作。尽管如此，实现最佳性能仍然具有挑战性，因为在 GPU 上执行单个神经网络运算符会产生一定的开销。为了减轻这种开销，ML 推理引擎采用了广泛的运算符融合规则，将多个运算符合并为一个运算符，从而减少张量元素之间的迭代次数，同时最大限度地提高每次迭代的计算量。例如，&lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; 利用运算符融合将计算量大的操作（如卷积）与后续激活函数（如 &lt;a href=&quot; https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;修正线性单元&lt;/a>;，合二为一。 &lt;/p>; &lt;p>; 一个明显的优化机会是在 LDM 的&lt;a href=&quot;https://arxiv.org/pdf/2304.11267.pdf&quot;>;降噪器模型&lt;/a>;中采用的大量使用的注意力块.注意块允许模型通过为重要区域分配更高的权重来关注输入的特定部分。有多种方法可以优化注意力模块，我们根据哪种优化表现更好，有选择地采用下面解释的两种优化之一。 &lt;/p>; &lt;p>; 我们称之为&lt;em>;部分融合 &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;>;softmax&lt;/a>;&lt;/em>; 的第一个优化删除了注意力模块中 softmax 和矩阵乘法之间需要大量内存写入和读取。让注意力块只是 &lt;b>;&lt;em>;Y &lt;/em>;&lt;/b>;= softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) * &lt;b 形式的简单矩阵乘法>;&lt;em>;W&lt;/em>;&lt;/b>; 其中 &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; 和 &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; 是二维形状矩阵&lt;em>;a&lt;/em>;×&lt;em>;b&lt;/em>; 和 &lt;em>;b&lt;/em>;×&lt;em>;c&lt;/em>;，分别为（如下上半部分所示）。 &lt;/p>; &lt;p>; 为了数值稳定性，&lt;b>;&lt;em>;T = &lt;/em>;&lt;/b>;softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) 通常用三个计算通过：&lt;/p>; &lt;ol>; &lt;li>;确定列表中的最大值，&lt;em>;即&lt;/em>;.,&lt;em>; &lt;/em>;对于矩阵&lt;b>;&lt;em>;X&lt;中的每一行/em>;&lt;/b>; &lt;/li>;&lt;li>;求和每个列表项的指数与最大值的差值（来自pass 1）&lt;/li>;&lt;li>;除以项目的指数减去最大值值与传递 2 的总和 &lt;/li>; &lt;/ol>; &lt;p>; 天真地执行这些传递会导致临时中间张量的大量内存写入&lt;strong>;&lt;em>;T&lt;/em>;&lt;/strong>;保存整个 softmax 函数的输出。如果我们只存储第 1 遍和第 2 遍的结果，标记为 &lt;b>;&lt;em>;m&lt;/em>;&lt;/b>; 和 &lt;b>;&lt;em>;s&lt;/em>;&lt;/b>;，我们就可以绕过这个大内存写入，分别是小向量，每个都有 &lt;em>;a&lt;/em>; 个元素，而 &lt;strong>;&lt;em>;T &lt;/em>;&lt;/strong>; 有 &lt;em>;a·b &lt;/em>; 个元素.通过这种技术，我们能够将数十甚至数百兆字节的内存消耗减少多个数量级（如下图下半部分所示）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO 4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s568/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;457&quot; data-original-width=&quot;568&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgX qd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;注意模块。 &lt;b>;顶部&lt;/b>;：一个朴素的注意力块，由一个 SOFTMAX（所有三个通道）和一个 &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;MATMUL&lt;/a >;，需要为大的中间张量 &lt;em>;T&lt;/em>; 写入大量内存。 &lt;b>;底部&lt;/b>;：我们在 MATMUL 中使用部分融合 softmax 的内存高效注意力块只需要为 &lt;em>;m&lt;/em>; 和 s 存储两个小的中间张量。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; 另一个优化涉及使用 &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;>;FlashAttention&lt;/a>;，这是一种 I/O 感知的精确注意力算法。该算法减少了 GPU 高带宽内存访问次数，使其非常适合我们的内存带宽受限用例。但是，我们发现此技术仅适用于具有特定大小且需要大量寄存器的 &lt;a href=&quot;https://en.wikipedia.org/wiki/Static_random-access_memory&quot;>;SRAM&lt;/a>;。因此，我们仅将此技术用于一组选定 GPU 上具有特定大小的注意力矩阵。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Winograd fast convolution for 3×3 convolution layers&lt;/h2>; &lt;p>; 常见LDMs的backbone heavy依赖于 3×3 卷积层（过滤器大小为 3×3 的卷积），包括解码器中超过 90% 的层。尽管增加了内存消耗和数值错误，但我们发现 &lt;a href=&quot;https://arxiv.org/abs/1509.09308&quot;>;Winograd 快速卷积&lt;/a>;在加速卷积方面很有效。与卷积中使用的&lt;em>;过滤器大小&lt;/em>; 3x3 不同，&lt;em>;tile 大小&lt;/em>; 是指一次处理的输入张量的子区域的大小。在&lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_logic_unit&quot;>;算术逻辑单元&lt;/a>; (ALU) 使用方面，增加分块大小可提高卷积效率。然而，这种改进是以增加内存消耗为代价的。我们的测试表明，4×4 的图块大小可实现计算效率和内存利用率之间的最佳折衷。 &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;/td>; &lt;td>;&lt;/td>; &lt;td colspan=&quot;2 &quot;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;内存使用情况&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp ;图块大小&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS &quot;>;FLOPS&lt;/a>; 节省&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;中间张量&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;权重&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;2×2 &lt;/td >; &lt;td>;2.25× &lt;/td>; &lt;td>;4.00× &lt;/td>; &lt;td>;1.77× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;4×4&lt;/b>; &lt;/ td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;2.25×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;6×6 &lt;/td>; &lt;td>;5.06× &lt;/td>; &lt;td>;1.80× &lt;/td>; &lt;td>;7.12× &lt;/td>; &lt;/tr>; &lt;tr >; &lt;td>;8×8 &lt;/td>; &lt;td>;5.76× &lt;/td>; &lt;td>;1.56× &lt;/td>; &lt;td>;11.1× &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Winograd 对 3×3 卷积的不同分块尺寸的影响。&lt;/td >;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;专门的运算符融合以提高内存效率&lt;/h2>; &lt;p>;我们发现，与当前现成的设备上 GPU 加速 ML 推理引擎提供的相比，在移动 GPU 上高效地推断 LDM 需要 LDM 中常用层和单元的融合窗口大得多。因此，我们开发了专门的实现，可以执行比典型融合规则所允许的神经运算符范围更大。具体来说，我们专注于两个专业：&lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;>;高斯误差线性单元&lt;/a>;（GELU ) 和 &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;>;group normalization&lt;/a>; 层。&lt;/p>; &lt;p>; GELU 与 &lt;a href=&quot;https:/ /en.wikipedia.org/wiki/Hyperbolic_functions&quot;>;hyperbolic tangent&lt;/a>; 函数需要写入和读取七个辅助中间张量（在下图中显示为浅橙色圆角矩形），从输入张量读取 &lt;em >;x&lt;/em>; 三次，并跨八个 GPU 程序写入输出张量 &lt;em>;y&lt;/em>; 一次，每个程序实现标记的操作（浅蓝色矩形）。在单个着色器中执行八个操作的自定义 GELU 实现（如下图底部所示）可以绕过中间张量的所有内存 I/O。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK 1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s958/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;210&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruD K0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text- align: center;&quot;>;GELU 实现。 &lt;strong>;上&lt;/strong>;：带有内置操作的简单实现需要 8 次内存写入和 10 次读取。 &lt;strong>;底部&lt;/strong>;：我们的自定义 GELU 仅需要 1 次内存读取（对于 &lt;em>;x&lt;/em>;）和 1 次写入（对于 &lt;em>;y&lt;/em>;）。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; 在应用所有这些优化之后，我们进行了在高端移动设备上测试 Stable Diffusion 1.5（图像分辨率 512x512，20 次迭代）。使用我们的 GPU 加速 ML 推理模型运行稳定扩散使用 2,093MB 的权重和 84MB 的中间张量。使用最新的高端智能手机，Stable Diffusion 可以在 12 秒内运行。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuU fRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s522/image3.gif&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3us n9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;Stable Diffusion 在现代智能手机上运行不到 12 秒。请注意，在每次迭代后运行解码器以显示此动画 GIF 中的中间输出会导致约 2 倍的减速。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 事实证明，对大型模型执行设备上的 ML 推理是一项巨大的挑战，包括模型文件大小的限制、大量的运行时内存要求和延长的推理延迟。通过认识到内存带宽使用是主要瓶颈，我们将努力转向优化内存带宽利用率并在 ALU 效率和内存效率之间取得微妙的平衡。因此，我们为大型扩散模型实现了最先进的推理延迟。您可以在&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;论文&lt;/a>;中了解有关这项工作的更多信息。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们要感谢 Yu-Hui Chen， Jiuqiang Tang、Frank Barchard、Yang Zhao、Joe Zou、Khanh LeViet、Chuo-Ling Chang、Andrei Kulik、Lu Wang 和 Matthias Grundmann。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:/ /ai.googleblog.com/feeds/2312998356111901143/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog. com/2023/06/speed-is-all-you-need-on-device.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href= &quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger. com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is -all-you-need-on-device.html&quot; rel=&quot;alternate&quot; title=&quot;速度就是你所需要的：通过 GPU 感知优化在设备上加速大型扩散模型&quot; type=&quot;text/html&quot;/>; &lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height= &quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt; /gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJ Vs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s72-c/SpeedHero.png &quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3072875289355194363&lt;/id>;&lt;published>;2023-06-14T09:00:00.000-07:00&lt;/published>;&lt;更新>;2023-06-14T09 :00:29.970-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;增强现实&quot;>;&lt;/category>;&lt;category scheme=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;计算摄影&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;谷歌地图&quot; >;&lt;/category>;&lt;title type=&quot;text&quot;>;用 NeRF 重建室内空间&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;软件工程师 Marcos Seefelder 和 Daniel Duckworth， Google Research 研究软件工程师&lt;/span>; MnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s320/Immersive%20View%20hero.gif”样式=&quot;显示：无；&quot; />; &lt;p>; 在选择场地时，我们经常会遇到以下问题：这家餐厅的氛围是否适合约会？有好的户外座位吗？是否有足够的屏幕来观看比赛？虽然照片和视频可能会部分回答此类问题，但它们并不能替代您身临其境的感觉，即使无法亲临现场也是如此。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 交互式、照片级真实感和多维的沉浸式体验可以弥补这一差距并重现空间的感觉和氛围，使用户能够自然地并直观地找到他们需要的信息。为了解决这个问题，Google 地图推出了&lt;a href=&quot;https://blog.google/products/maps/google-maps-updates-immersive-view-trip-planning&quot;>;沉浸式视图&lt;/a>;，它利用先进技术在机器学习 (ML) 和计算机视觉领域融合数十亿张&lt;a href=&quot;https://www.google.com/streetview/&quot;>;街景&lt;/a>;和航拍图像，创建丰富的数字模型世界。除此之外，它还会在顶部提供有用的信息，例如天气、交通以及某个地方的繁忙程度。沉浸式视图提供餐厅、咖啡馆和其他场所的室内视图，为用户提供虚拟的近距离观察，帮助他们自信地决定去哪里。 &lt;/p>; &lt;p>; 今天，我们将介绍在沉浸式视图中提供这些室内视图所付出的努力。我们建立在&lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;>;神经辐射场&lt;/a>; (NeRF) 的基础上，这是一种融合照片以产生逼真、多元的最先进方法- 神经网络内的维度重建。我们描述了我们创建 NeRF 的流程，其中包括使用 DSLR 相机自定义空间照片捕捉、图像处理和场景再现。我们利用了 Alphabet 的&lt;a href=&quot;https://arxiv.org/abs/2008.02268&quot;>;最近&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2111.12077&quot;>;advances&lt;/a >; &lt;a href=&quot;https://arxiv.org/abs/2202.05263&quot;>;在该领域&lt;/a>;设计一种在视觉保真度方面匹配或优于现有技术水平的方法。然后将这些模型作为交互式 360° 视频嵌入到精心策划的飞行路径中，使它们能够在智能手机上使用。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/introduction_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/ video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align : center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;沉浸式视图中的阿姆斯特丹 The Seafood Bar 重建。&lt;/td>;&lt;/tr >; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;从照片到 NeRFs&lt;/h2>; &lt;p>; 在我们工作的核心是 &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;NeRF&lt;/a>;，这是一种最近开发的 3D 重建和新视图合成方法。给定一组描述场景的照片，NeRF 将这些照片提炼成&lt;a href=&quot;https://arxiv.org/abs/2111.11426&quot;>;神经场&lt;/a>;，然后可用于从视点渲染照片不存在于原始集合中。 &lt;/p>; &lt;p>; 虽然 NeRF 在很大程度上解决了重建的挑战，但基于真实世界数据的面向用户的产品带来了各种各样的挑战。例如，重建质量和用户体验应在各个场所保持一致，从昏暗的酒吧到路边咖啡馆再到酒店餐厅。同时，应尊重隐私并删除任何可能的个人身份信息。重要的是，应该始终如一、高效地捕捉场景，可靠地实现高质量的重建，同时最大限度地减少捕捉必要照片所需的工作量。最后，无论手头使用何种设备，所有移动用户都应获得相同的自然体验。 &lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/ dinnerf-blog/overview_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-标题容器&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center ;&quot;>;The Immersive View室内重建管线。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>; 捕获 &amp;amp;预处理&lt;/h2>; &lt;p>; 生成高质量 NeRF 的第一步是仔细捕捉场景：可以从中导出 3D 几何和颜色的密集照片集。为了获得尽可能好的重建质量，应该从多个不同的方向观察每个表面。模型拥有的关于物体表面的信息越多，它就越能更好地发现物体的形状及其与光的交互方式。 &lt;/p>; &lt;p>; 此外，NeRF 模型对相机和场景本身进行了进一步的假设。例如，大多数相机属性（例如白平衡和光圈）假定在整个拍摄过程中是固定的。同样，假设场景本身在时间上被冻结：应避免灯光变化和移动。这必须与实际问题相平衡，包括捕获所需的时间、可用照明、设备重量和隐私。我们与专业摄影师合作，制定了一项策略，可以在一小时内使用 DSLR 相机快速可靠地拍摄场地照片。迄今为止，这种方法已用于我们所有的 NeRF 重建。 &lt;/p>; &lt;p>; 一旦捕获上传到我们的系统，处理就开始了。由于照片可能无意中包含敏感信息，我们会自动扫描并模糊可识别个人身份的内容。然后，我们应用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Structure_from_motion&quot;>;structure-from-motion&lt;/a>; 管道来解决每张照片的 &lt;a href=&quot;https://www. mathworks.com/help/vision/ug/camera-calibration.html&quot;>;相机参数&lt;/a>;：它相对于其他照片的位置和方向，以及镜头属性，例如 &lt;a href=&quot;https://en.wikipedia .org/wiki/Focal_length&quot;>;焦距&lt;/a>;。这些参数将每个像素与 3D 空间中的一个点和一个方向相关联，并构成 NeRF 重建过程中的关键信号。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeRF 重建&lt;/h2>; &lt;p>; 与许多 ML 模型不同，新的 NeRF 模型是从在每个捕获的位置上划痕。为了在目标计算预算内获得尽可能好的重建质量，我们结合了 Alphabet 开发的关于 NeRF 的各种已发表作品的特征。其中一些包括：&lt;/p>; &lt;ul>; &lt;li>;我们构建在 &lt;a href=&quot;https://jonbarron.info/mipnerf360/&quot;>;mip-NeRF 360&lt;/a>; 上，这是性能最好的之一迄今为止的 NeRF 模型。虽然比 Nvidia 广泛使用的 &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;Instant NGP&lt;/a>; 计算量更大，但我们发现 mip-NeRF 360 始终产生更少的伪影和更高的伪影重建质量。 &lt;/li>;&lt;li>;我们将 &lt;a href=&quot;https://nerf-w.github.io/&quot;>;NeRF in the Wild&lt;/a>; 中引入的低维生成潜在优化 (GLO) 向量合并为模型辐射网络的辅助输入。这些是学习的实值潜在向量，为每个图像嵌入了外观信息。通过将每个图像分配到它自己的潜在向量中，该模型可以捕获诸如光照变化之类的现象，而无需求助于多云几何形状，这是随意 NeRF 捕获中的常见伪像。 &lt;/li>;&lt;li>;我们还结合了 &lt;a href=&quot;https://waymo.com/research/block-nerf/&quot;>;Block-NeRF&lt;/a>; 中介绍的曝光条件。与不可解释的模型参数 GLO 矢量不同，曝光直接来自照片的&lt;a href=&quot;https://en.wikipedia.org/wiki/Exif&quot;>;元数据&lt;/a>;，并作为附加输入提供给模型的辐射网络。这有两大好处：它开启了改变 &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;>;ISO&lt;/a>; 的可能性，并提供了一种控制图像亮度的方法推理时间。我们发现这两个属性对于捕捉和重建光线昏暗的场所都是非常宝贵的。 &lt;/li>; &lt;/ul>; &lt;p>; 我们在提供不同权衡点的 TPU 或 GPU 加速器上训练每个 NeRF 模型。与所有 Google 产品一样，我们继续寻找新的改进方法，从降低计算要求到提高重建质量。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot;宽度=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/side_by_side_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/ video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align : center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的方法与 mip-NeRF 360 基线的并排比较。&lt; /td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可扩展的用户体验&lt;/ h2>; &lt;p>; 一旦 NeRF 被训练，我们就有能力从我们选择的任何视角和相机镜头生成场景的新照片。我们的目标是提供有意义且有用的用户体验：不仅是重建本身，还有引导式交互式游览，让用户可以通过智能手机舒适地自由探索空间。 &lt;/p>; &lt;p>; 为此，我们设计了一个可控的 360° 视频播放器，它模拟沿着预定义的路径在室内空间飞行，允许用户自由地环顾四周并向前或向后移动。作为第一个探索这项新技术的 Google 产品，360° 视频被选为交付生成内容的格式有几个原因。 &lt;/p>; &lt;p>; 在技术方面，&lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;实时推理&lt;/a>;和&lt;a href=&quot;https:/ /phog.github.io/snerg/&quot;>;baked representations&lt;/a>; 仍然是每个客户端的资源密集型（无论是在设备上还是在云计算上），依赖它们会限制能够访问这个的用户数量经验。通过使用视频，我们能够利用与 YouTube 使用的相同的视频管理和服务基础设施，扩展视频的存储和交付给所有用户。在操作方面，视频让我们对探索体验有更清晰的编辑控制，并且更容易检查大量的质量。 &lt;/p>; &lt;p>; 虽然我们曾考虑直接使用 360° 相机捕捉空间，但使用 NeRF 重建和渲染空间有几个优势。虚拟相机可以在太空中的任何地方飞行，包括越过障碍物和穿过窗户，并且可以使用任何所需的相机镜头。与现场录制不同，摄像机路径也可以事后编辑以提高平滑度和速度。 NeRF 捕获也不需要使用专门的相机硬件。 &lt;/p>; &lt;p>; 我们的 360° 视频是通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting&quot;>;光线投射&lt;/a>;通过虚拟球形相机的每个像素呈现的，并且合成场景的可见元素。每个视频都遵循由摄影师在拍摄过程中拍摄的一系列关键帧照片定义的平滑路径。每张图片的相机位置是在运动结构过程中计算的，图片序列被平滑地插值到飞行路径中。 &lt;/p>; &lt;p>; 为了在不同场地保持速度一致，我们通过捕获成对图像来校准每个场地的距离，每对图像相距 3 米。通过了解空间中的测量值，我们可以缩放生成的模型，并以自然速度渲染所有视频。 &lt;/p>; &lt;p>; 最终体验在&lt;a href=&quot;https://blog.google/products/maps/three-maps-updates-io-2022/&quot;>;沉浸式视图&lt;/a中呈现给用户>;：用户可以无缝飞入餐厅和其他室内场所，并通过逼真的 360° 视频飞来探索空间。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open research questions&lt;/h2>; &lt;p>; 我们相信这个功能是许多人的第一步在通往普遍可用、人工智能驱动的沉浸式体验的旅程中。从 NeRF 研究的角度来看，更多的问题仍然悬而未决。其中一些包括：&lt;/p>; &lt;ol>; &lt;li>;通过场景分割增强重建，将语义信息添加到场景中，例如，可以使场景可搜索且更易于导航。 &lt;/li>;&lt;li>;使 NeRF 除室内外还适用于室外照片集。通过这样做，我们将在世界的每个角落解锁类似的体验，并改变用户体验户外世界的方式。 &lt;/li>;&lt;li>;通过设备上的神经渲染实现实时、交互式 3D 探索。 &lt;/li>; &lt;/ol>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/outdoors_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/ video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align : center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用在街景全景图上训练的 NeRF 模型重建室外场景。&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 随着我们的不断发展，我们期待与社区互动并为社区做出贡献，以打造下一代沉浸式体验。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是谷歌多个团队的合作成果.该项目的贡献者包括 Jon Barron、Julius Beres、Daniel Duckworth、Roman Dudko、Magdalena Filak、Mike Harm、Peter Hedman、Claudio Martella、Ben Mildenhall、Cardin Moffett、Etienne Pot、Konstantinos Rematas、Yves Sallat、Marcos Seefelder、Lilyana Sirakovat、 Sven Tresp 和 Peter Zhizhin。&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;此外，我们还要感谢 Luke Barrington、Daniel Filip、Tom Funkhouser、Charles Goran、Pramod Gupta、Mario Lučić、Isalo Montacute 和 Dan Thomasset 提供宝贵的反馈和建议。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3072875289355194363/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html#comment-表单&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;编辑&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;self&quot; type=&quot;application/atom+ xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot; rel=&quot;alternate&quot; title=&quot;用 NeRF 重建室内空间&quot; 类型=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/电子邮件>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif &quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPE fyZf9HixVdNGhOsvMnlZKC6gnTq0oEFw0-YjU -yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s72-c/Immersive%20View%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/m rss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>; 0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7719238929287079732&lt;/id>;&lt;published>;2023-06-13T10:18:00.004-07 :00&lt;/published>;&lt;updated>;2023-06-13T13:54:56.343-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机愿景”>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger. com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;通过人类注意力的预测模型实现愉悦的用户体验&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class =&quot;byline-author&quot;>;由 Google Research 高级研究科学家 Junfeng He 和研究员 Kai Kohlhoff 发表&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM 08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s1400/HumanAttention.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人们具有非凡的能力来接收大量信息（估计大约有 10&lt;sup>;10&lt;/sup>; 比特/秒进入视网膜）并选择性地关注一些与任务相关且有趣的信息用于进一步处理的区域（例如，记忆、理解、行动）。因此，对人类注意力进行建模（其结果通常称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;显着性&lt;/a>;模型）引起了神经科学领域的兴趣，心理学、&lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;>;人机交互&lt;/a>; (HCI) 和 &lt;a href=&quot;https://en .wikipedia.org/wiki/Computer_vision&quot;>;计算机视觉&lt;/a>;。预测哪些区域可能吸引注意力的能力在图形、摄影、图像压缩和处理以及视觉质量测量等领域有许多重要的应用。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们&lt;a href=&quot;https://ai.googleblog.com/2021/05/accelerating-eye-movement-research-for .html&quot;>;之前讨论过&lt;/a>;使用机器学习和基于智能手机的注视估计加速眼球运动研究的可能性，这在早期需要专门的硬件，每台成本高达 30,000 美元。相关研究包括“&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/look-to-speak/&quot;>;Look to Speak/&quot;>;帮助有无障碍需求的用户（例如，人们&lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;ALS&lt;/a>;）用眼睛交流，最近发表的“&lt;a href=&quot;https://ai.googleblog .com/2023/04/differentially-private-heatmaps.html&quot;>;Differentially private-heatmaps&lt;/a>;&quot; 技术来计算热图，例如注意力图，同时保护用户的隐私。 &lt;/p>; &lt;p>; 在此博客中，我们展示了两篇论文（一篇来自 &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;>;CVPR 2022&lt;/a>;，一篇刚刚被 &lt;a href =&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;）突出了我们最近在人类注意力建模领域的研究：“&lt;a href=&quot;https://openaccess.thecvf.com/ content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf“>;用于减少视觉干扰的深度显着性先验&lt;/a>;”和“&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives _User-Aware_Saliency_Modeling_CVPR_2023_paper。 pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;&quot;，以及最近关于图像压缩的显着性驱动渐进加载的研究（&lt;a href=&quot;https://opensource.googleblog.com/2021/09 /using-saliency-in-progressive-jpeg-xl-images.html&quot;>;1&lt;/a>;，&lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center -model.html&quot;>;2&lt;/a>;)。我们展示了人类注意力的预测模型如何实现令人愉悦的用户体验，例如图像编辑以最大程度地减少视觉混乱、干扰或伪影、图像压缩以更快地加载网页或应用程序，以及引导 ML 模型实现更直观的类人解释和模型性能.我们专注于图像编辑和图像压缩，并讨论在这些应用程序的上下文中建模的最新进展。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;注意力引导的图像编辑&lt;/h2>; &lt;p>; 人类注意力模型通常将图像作为输入（例如，自然图像或网页截图），并&lt;a href=&quot;https://dl.acm.org/doi/10.1109/TPAMI.2019.2935715&quot;>;预测热图作为输出&lt;/a>;。图像上的预测热图是&lt;a href=&quot;https://www.computer.org/csdl/journal/tp/2019/03/08315047/17D45Vw15wU&quot;>;根据地面实况注意力数据进行评估&lt;/a>;，其中通常由眼动仪收集或&lt;a href=&quot;https://bubbleview.namwkim.org/&quot;>;通过鼠标悬停/点击近似&lt;/a>;。以前的模型利用手工制作的视觉线索特征，例如颜色/亮度对比、边缘和形状，而最近的方法基于深度神经网络自动学习判别特征，来自 &lt;a href=&quot;https://arxiv.org/abs/ 1805.01047&quot;>;卷积&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/pdf/1610.01708.pdf&quot;>;循环神经网络&lt;/a>; 到更新的 &lt;a href=&quot;https://www. sciencedirect.com/science/article/pii/S0925231222004714&quot;>;视觉变换器网络&lt;/a>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;”（更多信息在这个&lt;a href=&quot;https://deep-saliency-prior.github.io/&quot;>;项目网站&lt;/a>;）上，我们利用深度显着性模型进行戏剧性但视觉逼真的编辑，这可以显着改变观察者的注意力到不同的图像区域。例如，移除背景中分散注意力的物体可以减少照片中的混乱，从而提高用户满意度。同样，在视频会议中，减少背景中的混乱可能会增加对主要发言人的关注（&lt;a href=&quot;https://deep-saliency-prior.github.io/supplementary/index.html&quot;>;示例演示在这里&lt;/一>;）。 &lt;/p>; &lt;p>; 为了探索可以实现哪些类型的编辑效果以及这些效果如何影响观众的注意力，我们开发了一个优化框架，使用可区分的预测显着性模型来引导图像中的视觉注意力。我们的方法采用了最先进的深度显着性模型。给定输入图像和表示干扰区域的二元掩模，将在预测显着性模型的指导下编辑掩模内的像素，从而减少掩蔽区域内的显着性。为了确保编辑后的图像自然逼真，我们精心选择了四种图像编辑操作：两种标准图像编辑操作，即重新着色和图像变形（移位）；和两个学习运算符（我们没有明确定义编辑操作），即多层卷积滤波器和生成模型（&lt;a href=&quot;https://arxiv.org/abs/1912.04958&quot;>;GAN&lt;/a >;). &lt;/p>; &lt;p>; 通过这些运算符，我们的框架可以产生各种强大的效果，如下图所示，包括重新着色、修复、伪装、对象编辑或插入以及面部属性编辑。重要的是，所有这些效果都完全由单一的、预训练的显着性模型驱动，无需任何额外的监督或训练。请注意，我们的目标不是与产生每种效果的专用方法竞争，而是展示如何通过深度显着性模型中嵌入的知识来指导多个编辑操作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlAB hC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s1999/image2.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1300&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEm MbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;减少视觉干扰的示例，由具有多个运算符的显着性模型引导。在每个示例中，干扰区域都标记在显着图（红色边框）的顶部。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt; br>; &lt;/div>; &lt;h2>;通过用户感知显着性建模来丰富体验&lt;/h2>; &lt;p>; 先前的研究假设对整个人群使用单一显着性模型。然而，人类的注意力因人而异——虽然对显着线索的检测相当一致，但他们的顺序、解释和注视分布可能有很大差异。这提供了为个人或团体创建个性化用户体验的机会。在“&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”中，我们介绍了一个用户-aware 显着性模型，第一个可以使用单一模型预测一个用户、一组用户和一般人群的注意力的模型。 &lt;/p>; &lt;p>; 如下图所示，该模型的核心是每个参与者的视觉偏好与每个用户的注意力图和自适应用户掩码的组合。这需要在训练数据中提供每个用户的注意力注释，例如 &lt;a href=&quot;https://www.nature.com/articles/s41467-020-18360-5&quot;>;OSIE 自然移动注视数据集图片; FiWI&lt;/a>; 和 &lt;a href=&quot;http://vision.cs.stonybrook.edu/~soura/websaliency.html&quot;>;WebSaliency&lt;/a>; 网页数据集。该模型不是预测代表所有用户注意力的单个显着图，而是预测每个用户的注意力图来编码个人的注意力模式。此外，该模型采用用户掩码（大小等于参与者数量的二元向量）来指示当前样本中参与者的存在，这使得可以选择一组参与者并将他们的偏好组合成一个单一的热图。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3 hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s1218/image1.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;724&quot; data-original-width=&quot;1218&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtV nCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用户感知显着性模型框架的概述。示例图像来自 &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; 图像集。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;p>; 在推理过程中，用户掩码允许对参与者的任意组合进行预测。在下图中，前两行是两个不同组的参与者（每组三个人）对图像的注意力预测。 &lt;a href=&quot;https://arxiv.org/pdf/1805.01047.pdf&quot;>;传统的注意力预测模型&lt;/a>;将预测相同的注意力热图。我们的模型可以区分两组（例如，第二组比第一组更少关注面部，更多关注食物）。类似地，最后两行是网页上对两个不同参与者的预测，我们的模型显示出不同的偏好（例如，第二个参与者比第一个参与者更关注左侧区域）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY 855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s961/image3.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;961&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A 0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;预测注意力与基本事实 (GT)。 EML-Net：来自最先进模型的预测，该模型对两个参与者/组具有相同的预测。我们的：我们提出的用户感知显着性模型的预测，可以正确预测每个参与者/组的独特偏好。第一张图片来自&lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>;图片集，第二张图片来自&lt;a href=&quot; https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html&quot;>;FiWI&lt;/a>;。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line -height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;以显着特征为中心的渐进式图像解码&lt;/h2>; &lt;p>; 除了图像编辑，人类注意力模型还可以改善用户的浏览体验。浏览时最令人沮丧和烦人的用户体验之一是等待加载带有图像的网页，尤其是在网络连接性较低的情况下。在这种情况下改善用户体验的一种方法是对图像进行渐进式解码，随着数据的下载，解码和显示分辨率越来越高的图像部分，直到全分辨率图像准备就绪。渐进式解码通常按顺序进行（例如，从左到右，从上到下）。借助预测性注意力模型 (&lt;a href=&quot;https://github.com/google/attention-center&quot;>;1&lt;/a>;，&lt;a href=&quot;https://opensource.googleblog.com/2021/09 /using-saliency-in-progressive-jpeg-xl-images.html&quot;>;2&lt;/a>;)，我们可以根据显着性解码图像，从而可以发送显示最显着区域细节所需的数据第一的。例如，在人像中，面部字节的优先级高于散焦背景字节。因此，用户可以更早地感知到更好的图像质量，并体验到显着减少的等待时间。更多详细信息，请参阅我们的开源博客文章（&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;post 1 &lt;/a>;，&lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;发布 2&lt;/a>;）。因此，预测性注意力模型可以帮助压缩图像和更快地加载带有图像的网页，改进大图像和流媒体/VR 应用程序的渲染。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们已经展示了人类注意力的预测模型如何使令人愉悦的用户通过图像编辑等应用程序体验，可以减少用户图像或照片中的混乱、干扰或伪影，以及渐进式图像解码，可以大大减少用户在完全呈现图像时的感知等待时间。我们的用户感知显着性模型可以为个人用户或群体进一步个性化上述应用程序，从而实现更丰富和更独特的体验。 &lt;/p>; &lt;p>; 预测注意力模型的另一个有趣方向是它们是否可以帮助提高计算机视觉模型在对象分类或检测等任务中的鲁棒性。例如，在“&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;教师生成的空间注意力标签提高了对比模型的鲁棒性和准确性&lt; /a>;”，我们表明预测性人类注意力模型可以指导&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;对比学习&lt;/a>;模型实现更好的表示并提高准确性/鲁棒性分类任务（在 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; 和 &lt;a href=&quot;https://paperswithcode.com/dataset/imagenet-c&quot; >;ImageNet-C&lt;/a>; 数据集）。在这个方向上的进一步研究可以实现诸如利用放射科医生对医学图像的注意力来改进健康筛查或诊断，或在复杂的驾驶场景中利用人类注意力来指导自动驾驶系统等应用。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作涉及多学科软件团队的协作努力工程师、研究人员和跨职能贡献者。我们要感谢论文/研究的所有共同作者，包括 Kfir Aberman、Gamaleldin F. Elsayed、Moritz Firsching、Shi Chen、Nachiappan Valliappan、Yushi Yao、Chang Ye、Yossi Gandelsman、Inbar Mosseri、David E. Jacobes、Yael Pritch、Shaolei Shen 和 Xinyu Ye。我们还要感谢团队成员 Oscar Ramirez、Venky Ramachandran 和 Tim Fujita 的帮助。最后，我们感谢 Vidhya Navalpakkam 在发起和监督这项工作方面的技术领导。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7719238929287079732/ comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user -experiences.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/默认/7719238929287079732&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot; rel=&quot;alternate&quot; title=&quot;启用愉快通过人类注意力预测模型的用户体验&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4 ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s72-c/HumanAttention.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail >;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2015281937814138473&lt;/id>;&lt;published>;2023-06-09T12 :09:00.001-07:00&lt;/published>;&lt;updated>;2023-06-12T07:58:34.790-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns #&quot; term=&quot;Image Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Imagen Editor 和 EditBench：推进和评估文本引导的图像修复&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class= &quot;byline-author&quot;>;由 Google Research 研究工程师 Su Wang 和 Ceslee Montgomery 发布&lt;/span>; kf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj- 6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s320/Imagen%20Editor%20&amp;amp;%20EditBench%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 在过去的几年中，&lt;a href=&quot;https://en.wikipedia.org/wiki/Text-to-image_model&quot;>;文本到图像生成&lt;/a>;的研究呈爆炸式增长的突破（特别是 &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;、&lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; , &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; 等）自然渗透到相关主题中。特别是，文本引导图像编辑 (TGIE) 是一项实际任务，涉及编辑生成和拍摄的视觉效果，而不是完全重做它们。当重新创建视觉效果非常耗时或不可行时（例如，调整度假照片中的对象或完善从头生成的可爱小狗的细粒度细节），快速、自动化和可控的编辑是一种方便的解决方案。此外，TGIE 代表了改进基础模型本身训练的重要机会。多模态模型需要不同的数据才能正确训练，而 TGIE 编辑可以生成和重组高质量和可扩展的合成数据，这可能是最重要的，可以提供优化训练数据沿任何给定轴分布的方法。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2212.06909&quot;>;Imagen Editor 和 EditBench：推进和评估文本引导Image Inpainting&lt;/a>;”，将在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; 上展示，我们介绍 &lt;a href=&quot;https://imagen.research .google/editor/&quot;>;Imagen Editor&lt;/a>;，用于掩蔽&lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;修复&lt; /a>; — 即，当用户在叠加层或“遮罩”（通常在绘图类型界面中生成）旁边提供文本说明时，指示他们想要修改的图像区域。我们还介绍了 &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;EditBench&lt;/a>;，这是一种衡量图像编辑模型质量的方法。 EditBench 超越了&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;通常&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=bKBhQhPeKaF&quot;>;使用&lt;/ a>; &lt;a href=&quot;https://arxiv.org/abs/1910.13321&quot;>;粗粒度&lt;/a>;“这个图像是否匹配这个文本”方法，并深入到各种类型的属性、对象和场景以便更细粒度地了解模型性能。特别是，它在不忽视图像质量的情况下，非常强调图文对齐的忠实度。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s1261/image2。 png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https:/ /1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot; tr-caption&quot; style=&quot;text-align: center;&quot;>;给定图像、用户定义的蒙版和文本提示，Imagen Editor 对指定区域进行本地化编辑。该模型有意义地结合了用户的意图并执行逼真的编辑。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Imagen Editor&lt;/h2>; &lt;p>; Imagen Editor 是一个&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html&quot;>;扩散-基于模型&lt;/a>; 在 &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>; 上进行微调以进行编辑。它的目标是改进语言输入的表示、细粒度控制和高保真输出。 Imagen Editor 从用户那里获取三个输入：1) 要编辑的图像，2) 用于指定编辑区域的二进制掩码，以及 3) 文本提示——所有三个输入都会引导输出样本。 &lt;/p>; &lt;p>; Imagen Editor 依靠三种核心技术进行高质量的文本引导图像修复。首先，不同于之前的修复模型（例如，&lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1801.07892&quot; >;Context Attention&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;>;Gated Convolution&lt;/a>;）应用随机框和笔划掩码，Imagen Editor 采用对象检测器掩码策略一个&lt;a href=&quot;https://arxiv.org/abs/1801.04381​​&quot;>;对象检测器模块&lt;/a>;，在训练期间生成对象掩码。对象蒙版基于检测到的对象而不是随机补丁，并允许在编辑文本提示和蒙版区域之间进行更有原则的对齐。从经验上讲，该方法有助于模型避免当蒙版区域很小或仅部分覆盖对象时忽略文本提示的普遍问题（例如，&lt;a href=&quot;https://arxiv.org/abs/2204.14217&quot;>; CogView2&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH 6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/s648/image9.png&quot; 样式=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;648&quot; height=&quot;328&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv _kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/w640-h328/image9.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;随机蒙版（&lt;strong>;左&lt;/strong>;）经常捕获背景或与对象边界相交，定义仅从图像上下文就可以合理修复的区域。对象遮罩（&lt;strong>;右&lt;/strong>;）更难仅从图像上下文修复，鼓励模型在训练期间更多地依赖文本输入。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 接下来，在训练和推理期间，Imagen Editor 通过调节全分辨率（本作品中为 1024×1024）、输入图像和掩码的逐通道连接（类似于 &lt;a href= &quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887996&quot;>;SR3&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;调色板&lt;/a >; 和 &lt;a href=&quot;https://arxiv.org/abs/2112.10741&quot;>;滑行&lt;/a>;）。对于基础扩散 64×64 模型和 64×64→256×256 超分辨率模型，我们应用参数化下采样卷积（例如，带步长的卷积），我们根据经验发现这对高保真度至关重要。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6 QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s646/image4.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;365&quot; data-original-width=&quot;646&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1Rg XTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Imagen 针对图像编辑进行了微调。所有扩散模型，即基础模型和超分辨率 (SR) 模型，都以高分辨率 1024×1024 图像和蒙版输入为条件。为此，引入了新的卷积图像编码器。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 最后，在推理时我们应用 &lt;a href=&quot;https://arxiv .org/abs/2207.12598&quot;>;无分类器指导&lt;/a>; (CFG) 使样本偏向特定条件，在本例中为文本提示。 CFG 在文本条件和非条件模型预测之间进行插值，以确保生成的图像与文本引导图像修复的输入文本提示之间的强对齐。我们遵循 &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;Imagen Video&lt;/a>; 并使用高引导权重和引导振荡（在引导权重值范围内振荡的引导计划）。在基础模型（stage-1 64x diffusion）中，确保与文本的强对齐最为关键，我们使用在 1 到 30 之间振荡的指导权重计划。我们观察到高指导权重与振荡指导相结合会产生最佳效果样本保真度和文本图像对齐之间的权衡。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EditBench&lt;/h2>; &lt;p>; 用于文本引导图像修复评估的EditBench数据集包含240张图像，有 120 个生成图像和 120 个自然图像。生成的图像由 &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; 合成，自然图像由 &lt;a href=&quot;https://arxiv.org/abs/1602.07332 &quot;>;Visual Genome&lt;/a>; 和&lt;a href=&quot;https://arxiv.org/abs/1811.00982&quot;>;Open Images&lt;/a>; 数据集。 EditBench 捕获各种语言、图像类型和文本提示特异性级别（即简单、丰富和完整的字幕）。每个示例都包含 (1) 蒙版输入图像，(2) 输入文本提示，以及 (3) 用作自动度量参考的高质量输出图像。为了深入了解不同模型的相对优势和劣势，EditBench 提示旨在测试三个类别的细粒度细节：(1) 属性（例如，材料、颜色、形状、尺寸、数量）； (2) 对象类型（例如，常见、稀有、文本渲染）； (3) 场景（例如，室内、室外、写实或绘画）。为了解不同的提示规范如何影响模型性能，我们提供了三种文本提示类型：单一属性 (Mask Simple) 或蒙版对象的多属性描述 (Mask Rich) – 或整个图像描述 (Full Image) . Mask Rich 特别探索了模型处理复杂属性绑定和包含的能力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau 9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s531/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;531&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4 Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;完整图像用作成功修复的参考。遮罩以自由形式、非暗示形状覆盖目标对象。我们评估 Mask Simple、Mask Rich 和 Full Image 提示，与传统的文本到图像模型一致。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 由于固有的弱点在现有的自动评估指标（&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;CLIPScore&lt;/a>; 和 &lt;a href=&quot;https://datasets-benchmarks-proceedings.neurips.cc/paper /2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf&quot;>;CLIP-R-Precision&lt;/a>;) 对于 TGIE，我们将人工评估作为 EditBench 的黄金标准。在下面的部分中，我们将演示如何将 EditBench 应用于模型评估。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;评估&lt;/h2>; &lt;p>; 我们评估 Imagen Editor 模型——使用对象遮罩 (IM)和随机掩蔽 (IM-RM) — 与可比模型相比，&lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;稳定扩散&lt;/a>; (SD) 和 &lt;a href=&quot;https:/ /cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; (DL2)。 Imagen Editor 在所有 EditBench 评估类别中都以可观的优势优于这些模型。 &lt;/p>; &lt;p>; 对于完整图像提示，&lt;em>;单图像人工评估&lt;/em>; 提供二元答案以确认图像是否与说明匹配。对于 Mask Simple 提示，单图像人工评估确认对象和属性是否正确呈现和正确绑定（例如，对于红猫，红色桌子上的白猫将是不正确的绑定）。 &lt;em>;并排人工评估&lt;/em>;仅使用 Mask Rich 提示对 IM 与其他三个模型（IM-RM、DL2 和 SD）中的每一个进行并排比较，并指出哪个图像与标题更匹配以实现文本图像对齐，以及哪个图像最逼真。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1V YKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/s800/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;703&quot; data-original-width=&quot;800&quot; height=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzX lRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/w640-h562/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;人工评价。全图提示引出注释者对文本图像对齐的整体印象； Mask Simple 和 Mask Rich 检查是否正确包含特定属性、对象和属性绑定。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 对于单图像人类评估，IM全面获得最高评分（比第二高的表现模型高 10-13%）。对于其余的，性能顺序是IM-RM＞； DL2>; SD（有 3-6% 的差异）除了 Mask Simple，其中 IM-RM 落后 4-8%。由于 Full 和 Mask Rich 涉及相对更多的语义内容，我们推测 IM-RM 和 IM 受益于更高性能的 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning -with-t5.html&quot;>;T5 XXL&lt;/a>; 文本编码器。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJ yEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s1117/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1117&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQt IBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;通过提示类型对 EditBench 上文本引导图像修复的单图像人工评估。对于 Mask Simple 和 Mask Rich 提示，如果编辑的图像准确地包括提示中指定的每个属性和对象，包括正确的属性绑定，则文本-图像对齐是正确的。请注意，由于不同的评估设计，Full vs. Mask-only 提示，结果比较不直接。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; EditBench 专注于精细-粒度注释，因此我们评估对象和属性类型的模型。对于对象类型，IM 在所有类别中均处于领先地位，在常见、稀有和文本渲染方面的表现比排名第二的模型高 10-11%。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Q h-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s1086/image8.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1086&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE 0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;在 EditBench Mask Simple 上按对象类型进行单图像人工评估。作为一个队列，模型在对象渲染方面比文本渲染更好。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 对于属性类型，IM 的评级要高得多（13- 16%）比第二高的性能模型高，除了在数量上，DL2 仅落后 1%。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-18 5GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s1086/image10.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;607&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6 evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;EditBench Mask Simple 上的单图像人工评估（按属性类型）。对象屏蔽提高了对提示属性的全面遵守（IM 与 IM-RM）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 并排比较与其他一对一模型相比，IM 在文本对齐方面领先，并且有很大的余量，与 SD、DL2 和 IM-RM 相比，注释者更喜欢它。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s636/image6.png&quot; 样式= &quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;319&quot; data-original-width=&quot;636&quot; src=&quot;https://1.bp ... -align: center;&quot;>;对图像真实感和图像真实性的并排人类评估EditBench Mask Rich 提示上的文本图像对齐。对于文本图像对齐，Imagen Editor 在所有比较中都是首选。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 最后，我们说明了具有代表性的并排比较所有型号。有关更多示例，请参阅&lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;论文&lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kS u6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s766/image3.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;766&quot; data-original-width=&quot;638&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7Q nLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= “text-align: center;”>;Mask Simple 与 Mask Rich 提示的示例模型输出。与使用随机屏蔽训练的相同模型相比，对象屏蔽提高了 Imagen Editor 对提示的细粒度遵守。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height : 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们展示了 Imagen Editor 和 EditBench，在文本引导图像修复及其评估方面取得了重大进展。 Imagen Editor 是从 Imagen 微调的文本引导图像修复。 EditBench 是文本引导图像修复的综合系统基准，可跨多个维度评估性能：属性、对象和场景。请注意，出于对负责任 AI 的担忧，我们不会向公众发布 Imagen Editor。另一方面，EditBench 已&lt;a href=&quot;https://imagen.research.google/editor/&quot;>;完整发布&lt;/a>;，以造福于研究社区。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;感谢 Gunjan Baid、Nicole Brichtova、Sara Mahdavi 、Kathy Meier-Hellstern、Zarana Parekh、Anusha Ramesh、Tris Warkentin、Austin Waters 和 Vijay Vasudevan 的慷慨支持。我们感谢 Igor Karpov、Isabel Kraus-Liang、Raghava Ram Pamidigantam、Mahesh Maddinala 和所有匿名人工注释者的协调以完成人工评估任务。我们感谢 Huiwen Chang、Austin Tarango 和 Douglas Eck 提供论文反馈。感谢 Erica Moreira 和 Victor Gomes 在资源协调方面的帮助。最后，感谢 DALL-E 2 的作者允许我们将他们的模型输出用于研究目的。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com /feeds/2015281937814138473/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/ imagen-editor-and-editbench-advancing.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com /feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/ 2015281937814138473&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot; rel =&quot;alternate&quot; title=&quot;Imagen Editor 和 EditBench：推进和评估文本引导图像修复&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www .blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#缩略图&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb 5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s72-c/Imagen%20Editor%20&amp;%20EditBench%20hero.jpg&quot; width=&quot; 72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签:blogger.com,1999:blog-8474926331452026626.post-779195312945173416&lt;/id>;&lt;published>;2023-06-07T11:07:00.001-07:00&lt;/published>;&lt;updated>;2023-06-07T11:07:21.3 99 -07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;使用 SQuId 评估多种语言的语音合成&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Thibault Sellam 发表, 谷歌研究科学家&lt;/span>; 0OIQLAl29XK-MAh3XpOP05CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s320/SQuId%20hero.jpg&quot; style=&quot;显示：无;&quot; />; &lt;p>; 之前，我们介绍了&lt;a href=&quot;https://blog.google/technology/ai/ways-ai-is-scaling-helpful/&quot;>;1,000 种语言计划&lt;/a>;和&lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; 以制作语音和语言技术为目标可供全球数十亿用户使用。这一承诺的一部分涉及开发高质量的语音合成技术，这些技术建立在诸如 &lt;a href=&quot;https://ai.googleblog.com/2022/04/vdtts-visually-driven-text-to-speech. html&quot;>;VDTTS&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;，对于那些会说多种不同的语言。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7 -f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdbjCvVENZEDd6CweDk0fKiNJjGUd7cjgtfT9ddHQbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy2 1q520QkcSuA/s234/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;159&quot; data-original-width= “234” src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7-f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdBjCvVENZEDd6CweD k0fKiNJjGUd7cjgtfT9ddHQqbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy21q520QkcSuA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>; &lt;br />; &lt;p>; 开发一个新模型后，必须评估它生成的语音是否准确和自然：内容必须与任务相关，发音正确，语气适当，并且不应有声学伪影，例如裂纹或信号相关噪声。这种评估是多语言语音系统发展的主要瓶颈。 &lt;/p>; &lt;p>; 评估语音合成模型质量的最流行方法是人工评估：文本转语音 (TTS) 工程师从最新模型中生成几千条语音，将它们发送给人工评估，然后几天后收到结果。此评估阶段通常涉及&lt;em>;听力测试&lt;/em>;，在此期间，数十名注释者一个接一个地聆听话语，以确定它们听起来有多自然。虽然人类在检测一段文本听起来是否自然方面仍然不败，但这个过程可能不切实际——尤其是在研究项目的早期阶段，当工程师需要快速反馈来测试和重新制定他们的方法时。人工评估是昂贵、耗时的，并且可能受到感兴趣语言的评估者可用性的限制。 &lt;/p>; &lt;p>; 另一个阻碍进步的障碍是不同的项目和机构通常使用不同的评级、平台和协议，这使得同类比较变得不可能。在这方面，语音合成技术落后于文本生成，研究人员长期以来一直使用 &lt;a href=&quot;https://aclanthology.org/P02-1040/&quot;>;BLEU&lt;/a>; 等自动指标来补充人类评估，或者，最近，&lt;a href=&quot;https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html&quot;>;BLEURT&lt;/a>;。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;SQuId：测量多种语言中的语音自然度&lt;/a>;”中，将在 &lt;a href=&quot; https://2023.ieeeicassp.org/&quot;>;ICASSP 2023&lt;/a>;，我们介绍了 SQuId（语音质量识别），一个 600M 参数的回归模型，描述了一段语音在多大程度上听起来很自然。 SQuId 基于 &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>;（一种由 Google 开发的预训练语音文本模型），根据超过一百万个质量评分进行微调跨越 42 种语言并在 65 种语言中进行了测试。我们演示了如何使用 SQuId 来补充人类评级以评估多种语言。这是迄今为止此类已发表的最大成果。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用 SQuId 评估 TTS&lt;/h2>; &lt;p>; SQuId 背后的主要假设是训练回归基于先前收集的评级的模型可以为我们提供一种评估 TTS 模型质量的低成本方法。因此，该模型可以成为 TTS 研究人员评估工具箱的宝贵补充，提供近乎即时但不太准确的人工评估替代方案。 &lt;/p>; &lt;p>; SQuId 将话语作为输入和一个可选的语言环境标记（即语言的本地化变体，例如“巴西葡萄牙语”或“英国英语”）。它返回 1 到 5 之间的分数，表示波形听起来的自然程度，较高的值表示更自然的波形。 &lt;/p>; &lt;p>; 在内部，该模型包括三个组件：(1) 编码器，(2) 池化/回归层，以及 (3) 全连接层。首先，&lt;em>;编码器&lt;/em>;将频谱图作为输入并将其嵌入到一个较小的二维矩阵中，该矩阵包含 3,200 个大小为 1,024 的向量，其中每个向量编码一个时间步长。池化/回归层聚合向量，附加语言环境标签，并将结果馈送到返回分数的全连接层。最后，我们应用特定于应用程序的后处理来重新缩放或标准化分数，使其在 [1, 5] 范围内，这对于自然度人类评级很常见。我们用回归损失端到端地训练整个模型。 &lt;/p>; &lt;p>; 编码器是迄今为止最大和最重要的模型部分。我们使用了 &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>;，一个预先存在的 600M 参数 &lt;a href=&quot;http://www.interspeech2020.org/index. php?m=content&amp;amp;c=index&amp;amp;a=show&amp;amp;catid=418&amp;amp;id=1331&quot;>;Conformer&lt;/a>; 针对语音（51 种语言）和文本（101 种语言）进行预训练。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h 4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s475/image3.png&quot; 样式=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;226&quot; data-original-width=&quot;475&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf 82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;SQuId 模型。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 为了训练和评估模型，我们创建了 SQuId 语料库：包含 190 万个数据的集合对 66 种语言的话语进行评分，为 2,000 多个研究和产品 TTS 项目收集。 SQuId 语料库涵盖了多种系统，包括连接模型和神经模型，适用于广泛的用例，例如行车路线和虚拟助手。人工检查显示 SQuId 暴露于大量 TTS 错误，例如声学伪影（例如，噼啪声和爆裂声）、不正确的韵律（例如，英语中没有升调的问题）、文本规范化错误（例如，用语言表达“7” /7”作为“七除以七”而不是“七月七日”），或发音错误（例如，将“tough”表达为“toe”）。&lt;/p>; &lt;p>;训练多语言系统时出现的常见问题是训练数据可能不是对所有感兴趣的语言都是统一可用的。SQuId 也不例外。下图说明了每个语言环境的语料库大小。我们看到分布主要以美国英语为主。&lt;/p >; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQu dsSVzOyqbt1cWaLKzpadU- dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s3362/locales_blog.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;3362&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQudsSVzOyqbt1cWaLKzpadU-dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF 0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s16000/locales_blog.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;SQuId 数据集中的区域分布。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 当存在这样的变化时，我们如何才能为所有语言提供良好的性能？灵感来自 &lt;a href=&quot;https ://ai.googleblog.com/2019/10/exploring-massively-multilingual.html&quot;>;之前关于机器翻译的工作&lt;/a>;&lt;strong>;，&lt;/strong>;以及&lt;a href=&quot;https:/ /ieeexplore.ieee.org/document/6424230&quot;>;过去的工作&lt;/a>; 来自 &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02 /DNN-MultiLingual-ICASSP2013.pdf&quot;>;语音文献&lt;/a>;，我们决定为所有语言训练一个模型，而不是为每种语言使用单独的模型。假设是，如果模型足够大，则可能会发生&lt;em>;跨区域转移&lt;/em>;：由于对其他区域进行联合训练，模型在每个区域的准确度都会提高。正如我们的实验所示，跨区域设置被证明是性能的强大驱动力。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实验结果&lt;/h2>; &lt;p>; 为了了解 SQuId 的整体性能，我们将其与自定义的进行比较Big-SSL-MOS 模型（在&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;论文&lt;/a>;中描述），一个受 &lt;a href=&quot;https://arxiv .org/abs/2110.02635&quot;>;MOS-SSL&lt;/a>;，最先进的 TTS 评估系统。 Big-SSL-MOS 基于 &lt;a href=&quot;https://arxiv.org/abs/2108.06209&quot;>;w2v-BERT&lt;/a>; 并在 &lt;a href=&quot;https://arxiv.org /pdf/2203.11389.pdf&quot;>;VoiceMOS&#39;22 Challenge&lt;/a>; 数据集，评估时最受欢迎的数据集。我们试验了该模型的几种变体，发现 SQuId 的准确度提高了 50.0%。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNf kx7z6odaf2hEJGSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/s1131 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;550&quot; data-original-width=&quot;1131&quot; height=&quot; 311&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNfkx7z6odaf2hEJ GSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/w640-h311/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId 与最先进的基线对比。我们使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&quot;>;Kendall Tau&lt;/a>; 衡量与人类评级的一致性，其中较高的值表示较高的准确性。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; 为了解跨区域传输的影响，我们进行了一系列消融研究。我们改变了训练集中引入的语言环境数量，并测量了对 SQuId 准确性的影响。英语在数据集中已经过多，添加语言环境的影响可以忽略不计。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXS sNmDnnM5Q51fZ7kEjPCp2Ph667_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/s949/image5.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;881&quot; data-original-width=&quot;949&quot; height=&quot;371&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXSsNmDnnM5Q51fZ7kEjPCp2Ph6 67_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/w400-h371/image5.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId 在美国英语上的表现，在微调期间使用 1、8 和 42 语言环境。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;p>; 然而，跨区域传输对于大多数其他区域更有效：&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;样式 =“左边距：自动； margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbjhO- kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwibw691lLHscb277kB9RtJXd7MEL2n StMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/s1320/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;549&quot; data-original-width=&quot;1320&quot; height=&quot;266&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEhbjhO-kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwib w691lLHscb277kB9RtJXd7MEL2nStMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/w640-h266/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;SQuId 在四个选定语言环境（韩语、法语、泰语和泰米尔语）上的性能，在微调期间使用 1、8 和 42 个语言环境。对于每个语言环境，我们还提供训练集大小。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 为了将迁移推向极限，我们在训练期间保留了 24 个语言环境，并将它们专门用于测试。因此，我们测量到SQuId 可以在多大程度上处理它以前从未见过的语言。下图表明，尽管效果不统一，但跨区域传输有效。&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing= &quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizF9U8Wxz- zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGxarp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZV p0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/s823/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;823&quot; height=&quot;264&quot; src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEizF9U8Wxz-zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGx arp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZVp0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/w640-h264/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId 在四个“零样本”区域设置上的性能；在微调期间使用 1、8 和 42 个区域设置。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; 跨语言环境何时运作，如何运作？我们在&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;论文&lt;/ a>;，并表明尽管语言相似性发挥了作用（例如，巴西葡萄牙语培训有助于欧洲葡萄牙语），但它远不是唯一重要的因素。&lt;/p>; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来工作&lt;/h2>; &lt;p>; 我们介绍了 SQuId，这是一个 600M 参数回归模型，它利用 SQuId 数据集和跨区域学习来评估语音质量并描述听起来多么自然。我们证明 SQuId 可以在多种语言的评估中补充人类评估者。未来的工作包括提高准确性、扩大所涵盖的语言范围以及处理新的错误类型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这篇文章的作者现在是谷歌的一部分深度思维。非常感谢本文的所有作者：Ankur Bapna、Joshua Camp、Diana Mackinnon、Ankur P. Parikh 和 Jason Riesa。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai. googleblog.com/feeds/779195312945173416/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023 /06/evaluating-speech-synthesis-in-many.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www. blogger.com/feeds/8474926331452026626/posts/default/779195312945173416&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/779195312945173416&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/evaluating-speech-synthesis-in-many. html&quot; rel=&quot;alternate&quot; title=&quot;使用 SQuId 评估多种语言的语音合成&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger .com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheQlvvXBg5s0pU2XQwb7gxDhd_sNEHcE2EifW94vv-gYgpKF8V8BG5hKqw91qF5IzbZFnp2vZr1Am0OIQLAl29XK-MAh3XpOP0 5CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s72-c/SQuId%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com /mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1995595447829405143&lt;/ id>;&lt;published>;2023-06-06T09:58:00.000-07:00&lt;/published>;&lt;updated>;2023-06-06T09:58:41.452-07:00&lt;/updated>;&lt;category scheme=&quot;http:/ /www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt; /category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;视觉字幕：使用大型语言模型以动态视觉增强视频会议&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布人：谷歌增强现实研究科学家 Ruofei Du 和高级研究科学家 Alex Olwal&lt; /span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYNNO0NMhCPTDsGDkVzs2FJv ITsOknz8GcO4hJ7Eds57YVXdFtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s745/VisualCaptions.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; 视频会议的最新进展通过实时字幕和噪声消除等功能显着改善了远程视频通信。然而，在各种情况下，动态视觉增强有助于更好地传达复杂和细微的信息。例如，在讨论在日本餐厅点什么时，您的朋友可以分享视觉效果，帮助您更有信心点“寿喜烧”。或者在谈论您最近去旧金山的家庭旅行时，您可能想展示一张您个人相册中的照片。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://research.google/pubs/pub52074/&quot;>;视觉字幕：通过 On-the 增强口头交流-fly Visuals&lt;/a>;”，在 &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/95817&quot;>;ACM CHI 2023&lt;/a>; 上展示，我们介绍了一个系统使用口头提示通过实时视觉增强同步视频通信。我们微调了一个大型语言模型，以使用我们的&lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;数据集&lt;/a>;在开放式词汇对话中主动建议相关视觉效果为此目的策划。我们将 Visual Captions 开源为 &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; 项目的一部分，该项目旨在通过实时转录快速构建增强通信的原型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA 1HeOomCg2vQk61Flc7P58lAPXiiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s640/image11 .gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;277&quot; data-original-width=&quot;640&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA1HeOomCg2vQk61Flc7P58lAPX iiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;视觉字幕通过实时视觉效果促进口头交流。该系统甚至可以抵抗实时语音到文本转录中经常出现的典型错误。例如，在上下文之外，转录模型将“码头”一词误解为“对”，但 Visual Captions 仍然推荐圣莫尼卡码头的图像。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt; br />; &lt;h2>;通过动态视觉增强口头交流的设计空间&lt;/h2>; &lt;p>; 我们邀请了 10 名内部参与者，每个参与者都有不同的技术和非技术背景，包括软件工程师、研究人员、用户体验设计师、视觉艺术家、学生等讨论他们对潜在的实时视觉增强服务的特殊需求和愿望。在两个会议中，我们介绍了设想系统的低保真原型，然后是现有文本到图像系统的视频演示。这些讨论形成了一个具有八个维度的设计空间，用于实时对话的视觉增强，在下面标记为 D1 到 D8。 &lt;/p>; &lt;p>; 视觉增强可以与对话同步或异步（D1：时间），可用于表达和理解语音内容（D2：主题），并且可以使用各种不同的视觉来应用内容、视觉类型和视觉来源（D3：视觉）。这种视觉增强可能会有所不同，具体取决于会议的规模（D4：规模）以及会议是在同一地点还是远程设置（D5：空间）。这些因素也会影响视觉效果是应该私下展示、参与者之间共享还是对所有人公开（D6：隐私）。参与者还确定了他们希望在进行对话时与系统交互的不同方式（D7：启动）。例如，人们提出了不同级别的“主动性”，这表示用户希望模型采取主动性的程度。最后，参与者设想了不同的交互方法，例如，使用语音或手势进行输入。 （D8：互动）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hp tVhSDF4gwBpPY3Ed6m6cMGzzT358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s1999/image6 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1547&quot; data-original-width=&quot;1999&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hptVhSDF4gwBpPY3Ed6m6cMGzzT 358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用动态视觉增强口头交流的设计空间。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 由这个初始通知反馈，我们设计视觉字幕专注于生成语义相关的&lt;em>;视觉内容&lt;/em>;、&lt;em>;类型&lt;/em>;和&lt;em>;源&lt;/em>;的&lt;em>;同步&lt;/em>;视觉效果.虽然这些最初的探索性会议的参与者参与的是一对一的远程对话，但在野外部署视觉字幕通常是一对多的（例如，一个人向观众做演示）和多对多的-许多场景（例如，会议中多人之间的讨论）。 &lt;/p>; &lt;p>; 因为最能补充对话的视觉效果在很大程度上取决于讨论的上下文，所以我们需要一个专门用于此目的的训练集。因此，我们收集了 1595 个四元组的数据集，包括&lt;em>;语言 (1)&lt;/em>;、&lt;em>;视觉内容 (2)&lt;/em>;、&lt;em>;类型 (3)&lt;/em>; 和 &lt;em >;source (4)&lt;/em>; 跨越各种语境，包括日常对话、讲座和旅游指南。例如，“我很想看到它！”对应视觉内容“笑脸”、视觉类型“emoji”、视觉来源“大众搜索”。 “她有没有告诉你我们去墨西哥旅行的事？”对应视觉内容“墨西哥旅行一张照片”、视觉类型“照片”、视觉来源“个人相册”。我们为研究社区公开发布了这个&lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K 数据集&lt;/a>;。 &lt;/p>; &lt;br />; &lt;h2>;视觉意图预测模型&lt;/h2>; &lt;p>; 为了预测哪些视觉可以补充对话，我们使用 VC1.5K 数据集训练了一个基于大型语言模型的视觉意图预测模型.对于训练，我们将每个视觉意图解析为“&lt;code style=&quot;font-size: small;&quot;>;&lt;Visual Type>; of &lt;Visual Content>; from &lt;Visual Source>;&lt;/code>;”的格式。 &lt;/p>; &lt;pre class=&quot;prettyprint&quot; style=&quot;font-size: small; margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;{&quot;prompt&quot;: &quot;&lt;;前两句>; →&quot;, &quot;completion&quot;: &quot;&lt;Visual Type 1>; of &quot;&lt;Visual Type 1>;;来自&quot;&lt;Visual Source 1>;; &lt;Visual Type 2>; of &quot;&lt;Visual Type 2>;;来自 &quot;&amp;lt;Visual Source 2&amp;gt;; ... \𝑛&quot;} &lt;/pre>; &lt;p>; 使用这种格式，该系统可以处理开放式词汇对话并根据上下文预测视觉内容、视觉源和视觉类型。有趣的是，我们发现它优于基于关键字的方法，后者无法处理开放式词汇示例，例如“你的艾米阿姨将在本周六拜访”，并且无法建议相关的视觉类型或视觉来源。&lt;/p>; &lt;table align= &quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;文本对齐：居中；&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWW CNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevobH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s2552/vc-examples.png&quot; style=&quot;margin-left: auto; margin-右：自动；”>;&lt;img border=&quot;0&quot; data-original-height=&quot;1900&quot; data-original-width=&quot;2552&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/ AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWWCNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevo bH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s16000/vc-examples.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;例子我们的模型的意图预测。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;我们使用 VC1.5K 数据集中的 1276 (80%) 个示例来微调大型语言模型和其余的319 (20%) 个例子作为测试数据。我们使用令牌准确度指标衡量微调模型的性能，即模型正确预测的批次中令牌的百分比。在训练期间，我们的模型达到了 97% 的训练标记准确率和 87% 的验证标记准确率。 &lt;/p>; &lt;br />; &lt;h2>;性能&lt;/h2>; &lt;p>; 为了评估经过训练的视觉字幕模型的实用性，我们邀请了 89 名参与者执行 846 项任务。他们被要求对六个定性陈述提供“1 - 强烈不同意”到“7 - 强烈同意”的反馈。大多数参与者更喜欢在对话中看到视觉效果（Q1，83% ≥ 5 – 有点同意）。此外，他们认为显示的视觉效果有用且信息丰富（Q2，82% ≥ 5-有点同意），高质量（Q3，82% ≥ 5-有点同意），并且与原始演讲相关（Q4，84% ≥ 5 – 有点同意）。参与者还发现预测的视觉类型（Q5，87% ≥ 5-有点同意）和视觉来源（Q6，86% ≥ 5-有点同意）在给定相应对话的上下文的情况下是准确的。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9 OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsUVfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s1828/image2.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1828&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsU VfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;研究参与者对视觉预测模型的技术评估结果。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 有了这个经过微调的视觉意图预测模型，我们在 &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; 平台上开发了 Visual Captions，它可以直接在视频会议平台的摄像头流上添加新的交互式小部件，例如&lt;a href=&quot;https://apps.google.com/meet/&quot;>;Google Meet&lt;/a>;。如下图系统工作流程所示，Visual Captions 会自动捕获用户的语音，检索最后的句子，每 100 毫秒将其输入视觉意图预测模型，检索相关视觉效果，然后实时建议视觉效果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSL y_XLW3_l6NAtJJ-tkrxOYrL_kuW2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s1756/image7.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;662&quot; data-original-width=&quot;1756&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSLy_XLW3_l6NAtJJ-tkrxOYrL_ku W2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;Visual Captions 的系统工作流程。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Visual Captions 在建议视觉效果时提供三个级别的主动性：&lt; /p>; &lt;ul>; &lt;li>;&lt;em>;自动显示&lt;/em>;（高主动性）：系统自动搜索并向所有会议参与者公开显示视觉效果。无需用户交互。 &lt;/li>;&lt;li>;&lt;em>;自动建议&lt;/em>;（中等主动性）：建议的视觉效果显示在私有滚动视图中。然后用户单击视觉对象以公开显示它。在这种模式下，系统主动推荐视觉效果，但用户决定何时显示什么。 &lt;/li>;&lt;li>;&lt;em>;On-demand-suggest &lt;/em>;（低主动性）：系统只会在用户按下空格键时建议视觉效果。 &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;定量和定性评估：用户研究&lt;/h2>; &lt;p>; 我们在受控实验室研究中评估了视觉字幕 (&lt;em>;n &lt;/em>;= 26 ) 和野外部署研究 (&lt;em>;n&lt;/em>; = 10)。参与者发现，实时视觉效果有助于解释不熟悉的概念、解决语言歧义并使对话更具吸引力，从而促进现场对话。参与者还报告了在现场与系统交互的不同偏好，并且在不同的社交场景中更喜欢不同程度的主动性。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4 Li4mjiAeqyjfqgdB0GhfkB96_sZcYjApSk9Cj9yHWyINESkK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s1999/image3.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;590&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4Li4mjiAeqyjfqgdB0GhfkB96_sZc YjApSk9Cj9yHWyINEskK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;参与者的&lt;a href=&quot;https://en.wikipedia.org/wiki/NASA-TLX&quot;>;任务负荷指数&lt;/a>;和&lt;a href=&quot;https:// en.wikipedia.org/wiki/Likert_scale&quot;>;李克特量表&lt;/a>; 对四个没有视觉字幕（“无 VC”）和三种视觉字幕模式的对话的评分（从 1 - 强烈不同意到 7 - 强烈同意）：自动-显示、自动建议和按需建议。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;结论和未来方向&lt;/h2>; &lt;p>; 这项工作提出一个实时视觉增强语言交流的系统，称为 Visual Captions，该系统使用从 246 名参与者收集的 1595 个视觉意图数据集进行训练，涵盖 15 个主题类别。我们向研究社区公开发布训练数据集 &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K&lt;/a>; 以支持该领域的进一步研究。我们还在 &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; 中部署了视觉字幕，它通过转录会议和增强摄像头视频流来促进 Google Meet 中的视频会议。 &lt;/p>; &lt;p>; Visual Captions 代表了通过实时视觉效果增强口头交流的重要一步。通过了解视觉提示在日常对话中的重要性，我们可以创建更有效的沟通工具并改善人们的联系方式。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作是谷歌多个团队的合作成果。该项目的主要贡献者包括 Xingyu “Bruce” Liu、Vladimir Kirilyuk、Xiuxiu Yuan、Peggy Chi、Alex Olwal 和 Ruofei Du。&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;我们要感谢Jason Mayes、Max Spear、Na Li、Jun Zhang、Jing Jin、Yuan Ren、Adarsh Kowdle、Ping Yu、Darcy Philippon 和 Ezgi Oztelcan 等 ARChat 团队提供帮助的人。我们还要感谢许多与我们进行过富有洞察力讨论的人以及对手稿提供反馈的人，包括 Eric Turner、Yinda Zhang、Feitong Tan、Danhang Tang 和 Shahram Izadi。我们还要感谢我们的 CHI 审稿人提供的富有洞察力的反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1995595447829405143/comments/default&quot; rel =&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/19955954447829405143 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1995595447829405143&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language.html&quot; rel=&quot;alternate&quot; title=&quot;视觉字幕：使用大型语言模型通过动态视觉增强视频会议&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt; /uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:// img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYNNO0NMhCPTDsGDkVzs2FJvITsOknz8GcO4hJ7Eds57YVXd FtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s72-c/VisualCaptions.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5739375622760902222&lt;/id>;&lt;published>;2023-06-02T10: 02:00.000-07:00&lt;/published>;&lt;updated>;2023-06-02T10:02:37.777-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns# &quot; term=&quot;自动语音识别&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模态学习&quot;>;&lt;/category>;&lt;title type=&quot;text &quot;>;AVFormer：将视觉注入零镜头 AV-ASR 的冻结语音模型&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由研究科学家 Arsha Nagrani 和 Paul Hongsuck Seo 发表, 谷歌研究&lt;/span>; zOw32WW-ny6kBThTgHIV3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s1150/AVFormer.png&quot; style=&quot;显示：无;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;自动语音识别&lt;/a>; (ASR) 是一项成熟的技术，广泛应用于各种应用，例如电话会议、流媒体视频转录和语音命令。虽然这项技术的挑战集中在嘈杂的&lt;em>;音频&lt;/em>;输入上，但多模式视频（例如电视、在线编辑视频）中的&lt;em>;视觉&lt;/em>;流可以为提高鲁棒性提供强有力的线索ASR 系统——这称为视听 ASR (AV-ASR)。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 虽然唇部运动可以为语音识别提供强烈的信号，并且是 AV-ASR 最常见的焦点区域，但嘴巴通常不能直接看到&lt;em>;野外视频&lt;/em>;（例如，由于&lt;a href=&quot;https://en.wikipedia.org/wiki/Egocentric_vision&quot;>;以自我为中心的观点&lt;/a>;、面罩和低分辨率）因此，一个新兴的研究领域是&lt;em>;无约束&lt;/em>; AV-ASR（例如，&lt;a href=&quot;https://arxiv.org/abs/2206.07684&quot;>;AVATAR&lt;/a>;），它调查整个视觉框架的贡献，而不仅仅是嘴巴区域。 &lt;/p>; &lt;p>; 然而，构建用于训练 AV-ASR 模型的视听数据集具有挑战性。 &lt;a href=&quot;https://srvk.github.io/how2-dataset/&quot;>;How2&lt;/a>; 和 &lt;a href=&quot;https://gabeur.github.io/avatar-visspeech&quot;>; 等数据集VisSpeech&lt;/a>; 是根据在线教学视频创建的，但它们的体积很小。相比之下，模型本身通常很大并且包含视觉和音频编码器，因此它们往往会过度拟合这些小数据集。尽管如此，最近发布了一些大型纯音频模型，这些模型通过对从有声读物中获取的海量&lt;em>;纯音频&lt;/em>;数据进行大规模训练进行了大量优化，例如 &lt;a href= “https://github.com/facebookresearch/libri-light”>;LibriLight&lt;/a>; 和 &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;。这些模型包含数十亿个参数，随时可用，并显示出跨领域的强大泛化能力。 &lt;/p>; &lt;p>; 考虑到上述挑战，在“&lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV- ASR&lt;/a>;”，我们提出了一种使用视觉信息增强现有大规模纯音频模型的简单方法，同时执行轻量级域自适应。 AVFormer 将视觉嵌入注入冻结的 ASR 模型（类似于 Flamingo &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;注入视觉信息&lt;/a>;到用于视觉文本任务的大型语言模型中）使用轻量级可训练适配器，这些适配器可以在少量弱标记视频数据上进行训练，同时最少的额外训练时间和参数。我们还在训练期间引入了一个简单的课程方案，我们表明这对于使模型能够有效地联合处理音频和视觉信息至关重要。由此产生的 AVFormer 模型在三种不同的 AV-ASR 基准（How2、VisSpeech 和 &lt;a href=&quot;https://ego4d-data.org/&quot;>;Ego4D&lt;/a>;）上实现了最先进的零样本性能)，同时在传统的纯音频语音识别基准（即 &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;）上也保持了良好的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcu B1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s958/image2.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;286&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f _i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;不受约束的视听语音识别。我们将视觉注入冻结语音模型（&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;BEST-RQ&lt;/a>;，灰色），通过轻量级模块实现零镜头视听 ASR创建一个名为 AVFormer（蓝色）的参数和数据高效模型。视觉上下文可以为稳健的语音识别提供有用的线索，尤其是当音频信号嘈杂时（视觉面包有助于纠正生成的转录本中仅音频错误“丁香”到“面包”）。&lt;/td>;&lt;/ tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;使用轻量级模块注入视觉&lt;/h2>; &lt;p>; 我们的目标是将视觉理解能力添加到现有的纯音频 ASR 模型，同时保持其对各个领域（AV 和纯音频领域）的泛化性能。 &lt;/p>; &lt;p>; 为实现这一目标，我们增强了现有的最先进的 ASR 模型（&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;Best-RQ&lt;/ a>;) 具有以下两个组件：(i) 线性视觉投影仪和 (ii) 轻型适配器。前者在音频标记嵌入空间中投射视觉特征。此过程允许模型正确连接单独预训练的视觉特征和音频输入标记表示。后者随后对模型进行了最低限度的修改，以增加对来自视频的多模式输入的理解。然后，我们在来自 &lt;a href=&quot;https://www.di.ens.fr/willow/research/howto100m/&quot;>;HowTo100M 数据集&lt;/a>; 的未标记网络视频上训练这些额外的模块，以及一个ASR 模型作为伪地面真值，同时保持 Best-RQ 模型的其余部分冻结。这种轻量级模块可实现数据效率和性能的强大泛化。 &lt;/p>; &lt;p>; 我们在零样本设置中评估了我们在 AV-ASR 基准测试上的扩展模型，其中该模型从未在手动注释的 AV-ASR 数据集上进行过训练。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;视觉注入的课程学习&lt;/h2>; &lt;p>; 经过初步评估，我们凭经验发现通过简单的单轮联合训练，模型很难一次性学习适配器和视觉投影仪。为了缓解这个问题，我们引入了一个两阶段的&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/8403835/&quot;>;课程学习策略&lt;/a>;来分离这两个因素——领域适应和视觉特征整合——并以顺序方式训练网络。在第一阶段，适配器参数在根本不提供视觉标记的情况下进行优化。训练适配器后，我们添加视觉标记并在第二阶段单独训练视觉投影层，同时训练的适配器保持冻结状态。 &lt;/p>; &lt;p>; 第一阶段侧重于音频域自适应。到第二阶段，适配器完全冻结，视觉投射器必须简单地学习生成视觉提示，将视觉标记投射到音频空间中。通过这种方式，我们的课程学习策略允许模型结合视觉输入并适应 AV-ASR 基准测试中的新音频领域。我们只应用每个阶段一次，因为交替阶段的迭代应用会导致性能下降。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnx vzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJ6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s1318/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;1318&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4- e019DmRCxRllEIJ6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AVFormer 的总体架构和训练过程。该架构由一个冻结的 &lt;a href=&quot;https://arxiv.org/pdf/2005.08100.pdf&quot;>;Conformer&lt;/a>; 编码器-解码器模型和一个冻结的 &lt;a href=&quot;https://openai. com/research/clip&quot;>;CLIP&lt;/a>; 编码器（冻结层以灰色显示并带有锁符号），结合两个轻量级可训练模块 - (i) 视觉投影层（橙色）和瓶颈适配器（蓝色）以启用多模态域适应。我们提出了一个两阶段课程学习策略：首先在没有任何视觉标记的情况下训练适配器（蓝色），然后调整视觉投影层（橙色），同时所有其他部分保持冻结。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; 下图显示，在没有课程学习的情况下，我们的 AV-ASR 模型在所有数据集中都比纯音频基线差，随着更多视觉标记的添加，差距越来越大。相比之下，当应用所提出的两阶段课程时，我们的 AV-ASR 模型的性能明显优于基线纯音频模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6Vn ZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s1374/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1053&quot; data-original-width=&quot;1374&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBA FKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;课程学习的效果。红线和蓝线用于视听模型，并显示在零样本设置中的 3 个数据集上（&lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;WER&lt;/a>; % 越低越好).使用课程有助于所有 3 个数据集（对于 How2 (a) 和 Ego4D (c)，它对于超越纯音频性能至关重要）。性能提高到 4 个视觉标记，此时达到饱和。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/ div>; &lt;h2>;零样本 AV-ASR 的结果&lt;/h2>; &lt;p>; 我们将 AVFormer 与 BEST-RQ（我们模型的音频版本）和 AVATAR（AV-ASR 中最先进的技术）进行比较，以获得零样本- 在三个 AV-ASR 基准测试中的镜头性能：How2、VisSpeech 和 Ego4D。 AVFormer 在所有方面都优于 AVATAR 和 BEST-RQ，当它们在 LibriSpeech 和全套 HowTo100M 上接受训练时，甚至优于 AVATAR 和 BEST-RQ。这是值得注意的，因为对于 BEST-RQ，这涉及训练 600M 个参数，而 AVFormer 只训练 4M 个参数，因此只需要训练数据集的一小部分（HowTo100M 的 5%）。此外，我们还评估了仅音频的 LibriSpeech 的性能，并且 AVFormer 优于两个基线。 &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-Q VO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s3475/AVFormer_results%20(1).png “ imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2129&quot; data-original-width=&quot;3475&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s16000/AVFormer_results%20(1).png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>; &lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;比较不同 AV-ASR 数据集上零镜头性能的最先进方法。我们还在纯音频的 LibriSpeech 上展示表演。结果报告为 WER %（越低越好）。 AVATAR 和 BEST-RQ 在 HowTo100M 上进行了端到端（所有参数）微调，而 AVFormer 由于微调参数集很小，即使使用 5% 的数据集也能有效工作。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们介绍 AVFormer，一种用于适应现有冻结状态的轻量级方法用于 AV-ASR 的最先进的 ASR 模型。我们的方法实用且高效，并实现了令人印象深刻的零样本性能。随着 ASR 模型变得越来越大，调整预训练模型的整个参数集变得不切实际（对于不同领域更是如此）。我们的方法无缝地允许在同一个参数有效模型中进行域转移和视觉输入混合。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Arsha 的 Paul Hongsuck Seo 进行Nagrani 和 Cordelia Schmid。&lt;/em>; &lt;/p>; &lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5739375622760902222/comments/default&quot; rel=&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html# comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel =&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;self&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot; rel=&quot;alternate&quot; title=&quot;AVFormer：将视觉注入零镜头 AV-ASR 的冻结语音模型&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/ uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1 .blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIV3wtQVrpjdMd hMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s72-c/AVFormer.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt; /media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-791000644276225005&lt;/id>;&lt;published>;2023 -06-01T10:25:00.000-07:00&lt;/published>;&lt;updated>;2023-06-01T10:25:21.235-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com /atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;检索增强视觉语言预训练&lt;/stitle>; &lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由学生研究员 Ziniu Hu 和 Google Research Perception Team 研究科学家 Alireza Fathi 发表&lt;/span>; &lt;img src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12h sRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s320/REVEAL%20hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 大型模型，例如&lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;， &lt;a href=&quot;https://openai.com/research/language-models-are-few-shot-learners&quot;>;GPT-3&lt;/a>;，&lt;a href=&quot;https://ai.googleblog.com /2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with -a-single-visual-language-model&quot;>;Flamingo&lt;/a>; 和 &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html &quot;>;PaLI&lt;/a>;，当扩展到数百亿个参数并在大型文本和图像数据集上训练时，已经证明了存储大量知识的能力。这些模型在图像字幕、视觉问答和开放词汇识别等下游任务上取得了最先进的结果。尽管取得了这些成就，但这些模型需要大量数据进行训练，并最终产生大量参数（在许多情况下为数十亿），从而导致大量的计算需求。此外，用于训练这些模型的数据可能会过时，每次世界知识更新时都需要重新训练。例如，两年前训练的模型可能会产生有关美国现任总统的过时信息。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在自然语言处理领域（&lt;a href=&quot;https://www.deepmind.com/publications/improving-language-models- by-retrieving-from-trillions-of-tokens&quot;>;RETRO&lt;/a>;，&lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM &lt;/a>;) 和计算机视觉 (&lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>;)，研究人员尝试使用检索增强模型来应对这些挑战。通常，这些模型使用能够一次处理单一模态（例如，仅文本或仅图像）的主干来编码和检索来自知识语料库的信息。然而，这些检索增强模型无法利用查询和知识语料库中的所有可用模式，并且可能无法找到对生成模型输出最有帮助的信息。 &lt;/p>; &lt;p>; 为了解决这些问题，在“&lt;a href=&quot;https://arxiv.org/abs/2212.05221&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory &lt;/a>;”，出现在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;，我们介绍了一种视觉语言模型，该模型学习利用多源多-模态“记忆”来回答知识密集型查询。 REVEAL 采用&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;神经表征学习&lt;/a>;将不同的知识源编码并转换为由键值对组成的记忆结构。键用作内存项的索引，而相应的值存储有关这些项的相关信息。在训练期间，REVEAL 学习键嵌入、值标记以及从该内存中检索信息以解决知识密集型查询的能力。这种方法允许模型参数专注于对查询的推理，而不是专注于记忆。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23 lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s1440/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1440&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK -Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;我们增强了视觉语言模型，使其能够从不同的知识源中检索多个知识条目，这有助于生成。&lt;/td>;&lt;/tr>;&lt;/ tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;多模态知识语料库的记忆构建&lt;/h2>; &lt;p>; 我们的方法类似于 &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>;，因为我们预先计算来自不同的来源并将它们索引在统一的知识记忆中，其中每个知识项都被编码成一个键值对。每个键都是一个 &lt;em>;d&lt;/em>; 维嵌入向量，而每个值是一系列更详细地表示知识项的标记嵌入。与之前的工作相比，REVEAL 利用了一组多样化的多模式知识语料库，包括 &lt;a href=&quot;https://research.google/pubs/pub42240/&quot;>;WikiData 知识图谱&lt;/a>;、&lt;a href=&quot; https://arxiv.org/abs/2103.01913&quot;>;维基百科段落和图片&lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2102.08981&quot;>;网络图片-文本对&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;>;视觉问答数据&lt;/a>;。每个知识项可以是文本、图像、两者的组合（例如，维基百科中的页面）或者来自&lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;知识图的关系或属性&lt;/a>;（例如，巴拉克奥巴马身高 6 英尺 2 英寸）。在训练期间，随着模型参数的更新，我们不断地重新计算记忆键和值嵌入。我们在每千次训练步骤中异步更新内存。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;使用压缩缩放内存&lt;/h3>; &lt;p>; 编码内存值的一个天真的解决方案是为每个知识项保留整个标记序列。然后，该模型可以融合输入查询和 &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;>;top-k&lt;/a>; 通过将所有标记连接在一起检索到的内存值，并且将它们送入&lt;a href=&quot;https://huggingface.co/blog/encoder-decoder&quot;>;transformer encoder-decoder&lt;/a>; 管道。这种方法有两个问题：（1）如果每个内存值由数百个标记组成，则在内存中存储数亿个知识项是不切实际的；（2）transformer 编码器相对于标记总数的二次复杂度乘以 &lt; em>;k&lt;/em>; &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;自注意力&lt;/a>;。因此，我们建议使用&lt;a href=&quot;https://www.deepmind.com/publications/perceiver-general-perception-with-iterative-attention&quot;>;感知器架构&lt;/a>;来编码和压缩知识项。 Perceiver 模型使用转换器解码器将完整的令牌序列压缩为任意长度。这让我们可以检索最大 &lt;em>;k&lt;/em>; 的 top-&lt;em>;k&lt;/em>; 内存条目，大到一百。 &lt;/p>; &lt;p>; 下图说明了构造内存键值对的过程。每个知识项都通过多模式视觉语言编码器进行处理，从而产生一系列图像和文本标记。然后密钥头将这些标记转换为紧凑的嵌入向量。价值头（感知者）将这些标记压缩成更少的标记，并在其中保留有关知识项的相关信息。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJ OoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s1999/image1.png&quot; style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;776&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6c zk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;我们将来自不同语料库的知识条目编码成统一的键和值嵌入对，其中键用于索引记忆，值包含有关条目的信息。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;图像-文本对的大规模预训练&lt;/h2>; &lt;p >; 为了训练 REVEAL 模型，我们从大型语料库开始，该语料库是从公共网络收集的，包含 30 亿个图像替代文本字幕对，在 &lt;a href=&quot;https://ai.googleblog.com/2022/ 04/locked-image-tuning-adding-language.html&quot;>;LiT&lt;/a>;。由于数据集嘈杂，我们添加了一个过滤器来删除标题少于 50 个字符的数据点，这产生了大约 13 亿个图像标题对。然后，我们将这些对与 &lt;a href=&quot;https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html&quot;>;SimVLM&lt; 中使用的文本生成目标相结合/a>;，训练 REVEAL。给定一个图像文本示例，我们随机抽取一个包含文本前几个标记的前缀。我们将文本前缀和图像作为输入提供给模型，目的是生成其余文本作为输出。训练目标是调节前缀并自回归生成剩余的文本序列。 &lt;/p>; &lt;p>; 为了端到端地训练 REVEAL 模型的所有组件，我们需要&lt;a href=&quot;https://arxiv.org/abs/1910.08475&quot;>;热启动&lt;/a>;模型到良好状态（将初始值设置为模型参数）。否则，如果我们从随机权重开始（冷启动），检索器通常会返回不相关的记忆项目，这些项目永远不会产生有用的训练信号。为了避免这种冷启动问题，我们构建了一个具有伪真值知识的初始检索数据集，为预训练提供了一个合理的先机。 &lt;/p>; &lt;p>; 我们创建了 &lt;a href=&quot;https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html&quot;>;WIT&lt;/a 的修改版本>; 用于此目的的数据集。 WIT 中的每个图像-标题对还带有相应的维基百科段落（文本周围的单词）。我们将周围的段落与查询图像放在一起，并将其用作与输入查询对应的伪地面真值知识。该段落提供了有关图像和标题的丰富信息，这对于初始化模型很有用。 &lt;/p>; &lt;p>; 为了防止模型依赖低级图像特征进行检索，我们对输入查询图像应用了随机数据增强。给定这个包含伪检索基本事实的修改后的数据集，我们训练查询和记忆键嵌入以热启动模型。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;REVEAL 工作流程&lt;/h2>; &lt;p>; REVEAL 的整体工作流程包括四个主要步骤。首先，REVEAL 将多模态输入编码为一系列标记嵌入以及压缩查询嵌入。 Then, the model translates each multi-source knowledge entry into unified pairs of key and value embeddings, with the key being utilized for memory indexing and the value encompassing the entire information about the entry. Next, REVEAL retrieves the top-&lt;em>;k&lt;/em>; most related knowledge pieces from multiple knowledge sources, returns the pre-processed value embeddings stored in memory, and re-encodes the values. Finally, REVEAL fuses the top-&lt;em>;k&lt;/em>; knowledge pieces through an attentive knowledge fusion layer by injecting the retrieval score (dot product between query and key embeddings) as a prior during attention calculation. This structure is instrumental in enabling the memory, encoder, retriever and the generator to be concurrently trained in an end-to-end fashion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1003&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall workflow of REVEAL.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate REVEAL on knowledge-based visual question answering tasks using &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; and &lt;a href=&quot;https://allenai.org/project/a-okvqa/home&quot;>;A-OKVQA&lt;/a>; datasets. We fine-tune our pre-trained model on the VQA tasks using the same generative objective where the model takes in an image-question pair as input and generates the text answer as output. We demonstrate that REVEAL achieves better results on the A-OKVQA dataset than earlier attempts that incorporate a fixed knowledge or the works that utilize large language models (eg, GPT-3) as an implicit source of knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. REVEAL achieves higher accuracy in comparison to previous works including &lt;a href=&quot;https://arxiv.org/abs/1908.02265&quot;>;ViLBERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1908.07490&quot;>;LXMERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.09734&quot;>;ClipCap&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>; and &lt;a href=&quot;https://prior.allenai.org/projects/gpv2&quot;>;GPV-2&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also evaluate REVEAL on the image captioning benchmarks using &lt;a href=&quot;https://cocodataset.org/#home&quot;>;MSCOCO&lt;/a>; and &lt;a href=&quot;https://nocaps.org/&quot;>;NoCaps&lt;/a>; dataset. We directly fine-tune REVEAL on the MSCOCO training split via the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;>;cross-entropy&lt;/a>; generative objective. We measure our performance on the MSCOCO test split and NoCaps evaluation set using the &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr&lt;/a>; metric, which is based on the idea that good captions should be similar to reference captions in terms of word choice, grammar, meaning, and content. Our results on MSCOCO caption and NoCaps datasets are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Image Captioning results on MSCOCO and NoCaps using the CIDEr metric. REVEAL achieves a higher score in comparison to &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2108.10904&quot;>;SimVLM&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/image-text-pre-training-with.html&quot;>;CoCa&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Below we show a couple of qualitative examples of how REVEAL retrieves relevant documents to answer visual questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s931/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;338&quot; data-original-width=&quot;931&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REVEAL can use knowledge from different sources to correctly answer the question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an end-to-end retrieval-augmented visual language (REVEAL) model, which contains a knowledge retriever that learns to utilize a diverse set of knowledge sources with different modalities. We train REVEAL on a massive image-text corpus with four diverse knowledge corpora, and achieve state-of-the-art results on knowledge-intensive visual question answering and image caption tasks. In the future we would like to explore the ability of this model for attribution, and apply it to a broader class of multimodal tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross and Alireza Fathi.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/791000644276225005/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot; rel=&quot;alternate&quot; title=&quot;Retrieval-augmented visual-language pre-training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s72-c/REVEAL%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8031728070441281505&lt;/id>;&lt;published>;2023-05-31T10:13:00.003-07:00&lt;/published>;&lt;updated>;2023-06-12T15:09:48.808-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Large sequence models for software development activities&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Petros Maniatis and Daniel Tarlow, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s1200/DIDACT-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Software isn&#39;t created in one dramatic step. It improves bit by bit, one little step at a time — editing, running unit tests, fixing build errors, addressing code reviews, editing some more, appeasing &lt;a href=&quot;https://en.wikipedia.org/wiki/Lint_(software)&quot;>;linters&lt;/a>;, and fixing more errors — until finally it becomes good enough to merge into a code repository. Software engineering isn&#39;t an isolated process, but a dialogue among human developers, code reviewers, bug reporters, software architects and tools, such as compilers, unit tests, linters and static analyzers. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we describe DIDACT (​​Dynamic Integrated Developer ACTivity), which is a methodology for training large machine learning (ML) models for software development. The novelty of DIDACT is that it uses &lt;em>;the process of software development &lt;/em>;as the source of training data for the model, rather than just &lt;em>;the polished end state &lt;/em>;of that process, the finished code. By exposing the model to the contexts that developers see as they work, paired with the actions they take in response, the model learns about the dynamics of software development and is more aligned with how developers spend their time. We leverage instrumentation of Google&#39;s software development to scale up the quantity and diversity of developer-activity data beyond previous works. Results are extremely promising along two dimensions: usefulness to professional software developers, and as a potential basis for imbuing ML models with general software development skills. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s1475/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;687&quot; data-original-width=&quot;1475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DIDACT is a multi-task model trained on development activities that include editing, debugging, repair, and code review.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built and deployed internally three DIDACT tools, &lt;a href=&quot;https://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot;>;Comment Resolution&lt;/a>; (which we recently announced), Build Repair, and Tip Prediction, each integrated at different stages of the development workflow. All three of these tools received enthusiastic feedback from thousands of internal developers. We see this as the ultimate test of usefulness: do professional developers, who are often experts on the code base and who have carefully honed workflows, leverage the tools to improve their productivity? &lt;/p>; &lt;p>; Perhaps most excitingly, we demonstrate how DIDACT is a first step towards a general-purpose developer-assistance agent. We show that the trained model can be used in a variety of surprising ways, via prompting with prefixes of developer activities, and by chaining together multiple predictions to roll out longer activity trajectories. We believe DIDACT paves a promising path towards developing agents that can generally assist across the software development process. &lt;/p>; &lt;br />; &lt;h2>;A treasure trove of data about the software engineering process&lt;/h2>; &lt;p>; Google&#39;s software engineering toolchains store every operation related to code as a log of interactions among tools and developers, and have done so for decades. In principle, one could use this record to replay in detail the key episodes in the “software engineering video” of how Google&#39;s codebase came to be, step-by-step — one code edit, compilation, comment, variable rename, etc., at a time. &lt;/p>; &lt;p>; Google code lives in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monorepo&quot;>;monorepo&lt;/a>;, a single repository of code for all tools and systems. A software developer typically experiments with code changes in a local &lt;a href=&quot;https://en.wikipedia.org/wiki/Copy-on-write&quot;>;copy-on-write&lt;/a>; workspace managed by a system called &lt;a href=&quot;https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext&quot;>;Clients in the Cloud&lt;/a>; (CitC). When the developer is ready to package a set of code changes together for a specific purpose (eg, fixing a bug), they create a changelist (CL) in &lt;a href=&quot;https://abseil.io/resources/swe-book/html/ch19.html&quot;>;Critique&lt;/a>;, Google&#39;s code-review system. As with other types of code-review systems, the developer engages in a dialog with a peer reviewer about functionality and style. The developer edits their CL to address reviewer comments as the dialog progresses. Eventually, the reviewer declares “LGTM!” (“looks good to me”), and the CL is merged into the code repository. &lt;/p>; &lt;p>; Of course, in addition to a dialog with the code reviewer, the developer also maintains a “dialog” of sorts with a plethora of other software engineering tools, such as the compiler, the testing framework, linters, static analyzers, fuzzers, etc. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/s1877/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1751&quot; data-original-width=&quot;1877&quot; height=&quot;597&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/w640-h597/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the intricate web of activities involved in developing software: small actions by the developer, interactions with a code reviewer, and invocations of tools such as compilers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A multi-task model for software engineering&lt;/h2>; &lt;p>; DIDACT utilizes interactions among engineers and tools to power ML models that assist Google developers, by suggesting or enhancing actions developers take — in context — while pursuing their software-engineering tasks. To do that, we have defined a number of tasks about individual developer activities: repairing a broken build, predicting a code-review comment, addressing a code-review comment, renaming a variable, editing a file, etc. We use a common formalism for each activity: it takes some &lt;em>;State&lt;/em>; (a code file), some &lt;em>;Intent&lt;/em>; (annotations specific to the activity, such as code-review comments or compiler errors), and produces an &lt;em>;Action&lt;/em>; (the operation taken to address the task). This Action is like a mini programming language, and can be extended for newly added activities. It covers things like editing, adding comments, renaming variables, marking up code with errors, etc. We call this language &lt;em>;DevScript&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s1200/DIDACT.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;762&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s16000/DIDACT.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The DIDACT model is prompted with a task, code snippets, and annotations related to that task, and produces development actions, eg, edits or comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This state-intent-action formalism enables us to capture many different tasks in a general way. What&#39;s more, DevScript is a concise way to express complex actions, without the need to output the whole state (the original code) as it would be after the action takes place; this makes the model more efficient and more interpretable. For example, a rename might touch a file in dozens of places, but a model can predict a single rename action. &lt;/p>; &lt;br />; &lt;h2>;An ML peer programmer&lt;/h2>; &lt;p>; DIDACT does a good job on individual assistive tasks. For example, below we show DIDACT doing code clean-up after functionality is mostly done. It looks at the code along with some final comments by the code reviewer (marked with “human” in the animation), and predicts edits to address those comments (rendered as a &lt;em>;diff&lt;/em>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/s897/DIDACT2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;761&quot; data-original-width=&quot;897&quot; height=&quot;543&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/w640-h543/DIDACT2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an initial snippet of code and the comments that a code reviewer attached to that snippet, the Pre-Submit Cleanup task of DIDACT produces edits (insertions and deletions of text) that address those comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The multimodal nature of DIDACT also gives rise to some surprising capabilities, reminiscent of &lt;a href=&quot;https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html&quot;>;behaviors emerging with scale&lt;/a>;. One such capability is &lt;em>;history augmentation&lt;/em>;, which can be enabled via prompting. Knowing what the developer did recently enables the model to make a better guess about what the developer should do next. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgwd-Dvh_esu_26f2rO6DKwMpGyDF0wKHzasGuD3RhXNJiJR5cM613KLSuabNp5hvKV9dF2vzHd7XkUOKD4d7l1cOyV2ovYwxWLECmudUKDk5bmVNOg5eImsqUJbkUkPWk7yOiCogafub7eHb9B3cytjGInIICcMoWFrTlsgqfN3B9IvHFPbWbwfIkZQ/s1044/CitCMonster.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;633&quot; data-original-width=&quot;1044&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgwd-Dvh_esu_26f2rO6DKwMpGyDF0wKHzasGuD3RhXNJiJR5cM613KLSuabNp5hvKV9dF2vzHd7XkUOKD4d7l1cOyV2ovYwxWLECmudUKDk5bmVNOg5eImsqUJbkUkPWk7yOiCogafub7eHb9B3cytjGInIICcMoWFrTlsgqfN3B9IvHFPbWbwfIkZQ/s16000/CitCMonster.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of history-augmented code completion in action.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A powerful such task exemplifying this capability is &lt;em>;history-augmented code completion&lt;/em>;. In the figure below, the developer adds a new function parameter (1), and moves the cursor into the documentation (2). Conditioned on the history of developer edits and the cursor position, the model completes the line (3) by correctly predicting the docstring entry for the new parameter. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s1048/DIDACT4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1048&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s16000/DIDACT4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of edit prediction, over multiple chained iterations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In an even more powerful history-augmented task, &lt;em>;edit prediction&lt;/em>;, the model can choose where to edit next in a fashion that is historically consistent&lt;strong>;. &lt;/strong>;If the developer deletes a function parameter (1), the model can use history to correctly predict an update to the docstring (2) that removes the deleted parameter (without the human developer manually placing the cursor there) and to update a statement in the function (3) in a syntactically (and — arguably — semantically) correct way. With history, the model can unambiguously decide how to continue the “editing video” correctly. Without history, the model wouldn&#39;t know whether the missing function parameter is intentional (because the developer is in the process of a longer edit to remove it) or accidental (in which case the model should re-add it to fix the problem). &lt;/p>; &lt;p>; The model can go even further. For example, we started with a blank file and asked the model to successively predict what edits would come next until it had written a full code file. The astonishing part is that the model developed code in a step-by-step way that would seem natural&lt;strong>; &lt;/strong>;to a developer: It started by first creating a fully working skeleton with imports, flags, and a basic main function. It then incrementally added new functionality, like reading from a file and writing results, and added functionality to filter out some lines based on a user-provided regular expression, which required changes across the file, like adding new flags. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DIDACT turns Google&#39;s software development process into training demonstrations for ML developer assistants, and uses those demonstrations to train models that construct code in a step-by-step fashion, interactively with tools and code reviewers. These innovations are already powering tools enjoyed by Google developers every day. The DIDACT approach complements the great strides taken by large language models at Google and elsewhere, towards technologies that ease toil, improve productivity, and enhance the quality of work of software engineers. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a multi-year collaboration among Google Research, Google Core Systems and Experiences, and DeepMind. We would like to acknowledge our colleagues Jacob Austin, Pascal Lamblin, Pierre-Antoine Manzagol, and Daniel Zheng, who join us as the key drivers of this project. This work could not have happened without the significant and sustained contributions of our partners at Alphabet (Peter Choy, Henryk Michalewski, Subhodeep Moitra, Malgorzata Salawa, Vaibhav Tulsyan, and Manushree Vijayvergiya), as well as the many people who collected data, identified tasks, built products, strategized, evangelized, and helped us execute on the many facets of this agenda (Ankur Agarwal, Paige Bailey, Marc Brockschmidt, Rodrigo Damazio Bovendorp, Satish Chandra, Savinee Dancs,&amp;nbsp;&lt;/em>;&lt;i>;Denis Davydenko,&amp;nbsp;&lt;/i>;&lt;em>;Matt Frazier, Alexander Frömmgen, Nimesh Ghelani, Chris Gorgolewski, Chenjie Gu, Vincent Hellendoorn, Franjo Ivančić, Marko Ivanković, Emily Johnston, Luka Kalinovcic, Lera Kharatyan, Jessica Ko, Markus Kusano, Kathy Nix, Christian Perez, Sara Qu, Marc Rasi, Marcus Revaj, Ballie Sandhu, Michael Sloan, Tom Small, Gabriela Surita, Maxim Tabachnyk, David Tattersall, Sara Toth, Kevin Villela, Sara Wiltberger, and Donald Duo Zhao) and our extremely supportive leadership (Martín Abadi, Joelle Barral, Jeff Dean, Madhura Dudhgaonkar, Douglas Eck, Zoubin Ghahramani, Hugo Larochelle, Chandu Thekkath, and Niranjan Tulpule). Thank you!&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8031728070441281505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot; rel=&quot;alternate&quot; title=&quot;Large sequence models for software development activities&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s72-c/DIDACT-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4195590947404721374&lt;/id>;&lt;published>;2023-05-26T12:08:00.002-07:00&lt;/published>;&lt;updated>;2023-05-26T12:09:00.761-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Foundation models for reasoning on charts&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Julian Eisenschlos, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s2200/MatCha.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Visual language is the form of communication that relies on pictorial symbols outside of text to convey information. It is ubiquitous in our digital life in the form of iconography, infographics, tables, plots, and charts, extending to the real world in street signs, comic books, food labels, etc. For that reason, having computers better understand this type of media can help with scientific communication and discovery, accessibility, and data transparency. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 自 &lt;a href=&quot;https://ieeeexplore.ieee.org 出现以来，计算机视觉模型使用基于学习的解决方案取得了巨大进步/document/5206848&quot;>;ImageNet&lt;/a>;，重点一直放在自然图像上，其中有各种任务，例如&lt;a href=&quot;https://www.image-net.org/&quot;>;分类&lt;/ a>;、&lt;a href=&quot;https://visualqa.org/&quot;>;视觉问答&lt;/a>; (VQA)、&lt;a href=&quot;https://cocodataset.org&quot;>;字幕、检测和分割&lt;/ a>;，已经被定义、研究并在某些情况下被改进以达到人类的表现。然而，视觉语言并没有获得同等程度的关注，这可能是因为该领域缺乏大规模的训练集。但在过去几年中，已经创建了新的学术数据集，其目标是评估视觉语言图像上的问答系统，例如 &lt;a href=&quot;https://arxiv.org/abs/1909.00997&quot;>;PlotQA&lt;/a>; 、&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicsVQA&lt;/a>; 和 &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA &lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6a Q6tqQsTgI2X3TkQUK_xIK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s1999/image5.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;898&quot; data-original-width=&quot;1999&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6aQ6tqQsTgI2X3TkQUK_x IK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA&lt;/a>; 示例。回答问题需要阅读信息并计算和与差。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 为这些任务构建的现有模型依赖于集成 &lt;a href=&quot;https: //en.wikipedia.org/wiki/Optical_character_recognition&quot;>;光学字符识别&lt;/a>; (OCR) 信息及其坐标进入更大的管道，但该过程容易出错、速度慢且泛化能力差。这些方法的流行是因为现有的端到端计算机视觉模型基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 或 &lt;在自然图像上预先训练的 href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; 无法轻易适应视觉语言。但现有模型在回答图表问题时准备不足，包括阅读条形图的相对高度或饼图中切片的角度、理解轴刻度、正确映射象形图及其图例值、颜色、大小和纹理，最后对提取的数字进行数值运算。 &lt;/p>; &lt;p>; 鉴于这些挑战，我们提出“&lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha：通过数学推理和图表去渲染增强视觉语言预训练&lt;/a>; ”。 MatCha 代表数学和图表，是一种像素到文本的基础模型（一种预训练模型，具有内置的归纳偏差，可以针对多种应用进行微调），在两个互补任务上进行训练：(a) 图表去渲染和（b）数学推理。在图表反渲染中，给定一个绘图或图表，图像到文本模型需要生成其底层数据表或用于渲染它的代码。对于数学推理预训练，我们选择文本数字推理数据集并将输入渲染为图像，图像到文本模型需要对其进行解码以获得答案。我们还提出了“&lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot：通过绘图到表格翻译的一次性视觉语言推理&lt;/a>;”，一个建立在 MatCha 之上的模型通过转换为表格在图表上进行一次性推理。通过这些方法，我们超越了 ChartQA 之前的最先进技术 20% 以上，并且与具有 1000 倍以上参数的最佳摘要系统相匹配。这两篇论文都将在 &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>; 上发表。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;图表反渲染&lt;/h2>; &lt;p>; 图表通常由底层数据表生成和一段代码。代码定义图形的总体布局（例如，类型、方向、颜色/形状方案），基础数据表建立实际数字及其分组。数据和代码都被发送到编译器/渲染引擎以创建最终图像。要理解图表，需要发现图像中的视觉模式，并有效地解析和分组它们以提取关键信息。逆转情节渲染过程需要所有这些能力，因此可以作为理想的预训练任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s624/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;335&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A chart created from a table in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Airbus_A380&quot;>;Airbus A380 Wikipedia page&lt;/a>; using random plotting options. The pre-training task for MatCha consists of recovering the source table or the source code from the image.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, it is challenging to simultaneously obtain charts, their underlying data tables, and their rendering code. To collect sufficient pre-training data, we independently accumulate [chart, code] and [chart, table] pairs. For [chart, code], we crawl all GitHub IPython notebooks with appropriate licenses and extract blocks with figures. A figure and the code block right before it are saved as a [chart, code] pair. For [chart, table] pairs, we explored two sources. For the first source, synthetic data, we manually write code to convert web-crawled Wikipedia tables from the &lt;a href=&quot;https://aclanthology.org/2020.acl-main.398/&quot;>;TaPas&lt;/a>; codebase to charts. We sampled from and combined several plotting options depending on the column types. In addition, we also add [chart, table] pairs generated in PlotQA to diversify the pre-training corpus. The second source is web-crawled [chart, table] pairs. We directly use the [chart, table] pairs crawled in the ChartQA training set, containing around 20k pairs in total from four websites: &lt;a href=&quot;statista.com&quot;>;Statista&lt;/a>;, &lt;a href=&quot;https://www.pewresearch.org/&quot;>;Pew&lt;/a>;, &lt;a href=&quot;https://ourworldindata.org/&quot;>;Our World in Data&lt;/a>;, and &lt;a href=&quot;https://www.oecd.org/&quot;>;OECD&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Math reasoning&lt;/h2>; &lt;p>; We incorporate numerical reasoning knowledge into MatCha by learning math reasoning skills from textual math datasets. We use two existing textual math reasoning datasets, &lt;a href=&quot;https://openreview.net/forum?id=H1gR5iR5FX&quot;>;MATH&lt;/a>; and &lt;a href=&quot;https://aclanthology.org/N19-1246/&quot;>;DROP&lt;/a>; for pre-training. MATH is synthetically created, containing two million training examples per module (type) of questions. DROP is a reading-comprehension–style QA dataset where the input is a paragraph context and a question. &lt;/p>; &lt;p>; To solve questions in DROP, the model needs to read the paragraph, extract relevant numbers and perform numerical computation. We found both datasets to be complementary. MATH contains a large number of questions across different categories, which helps us identify math operations needed to explicitly inject into the model. DROP&#39;s reading-comprehension format resembles the typical QA format wherein models simultaneously perform information extraction and reasoning. In practice, we render inputs of both datasets into images. The model is trained to decode the answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s624/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;218&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;To improve the math reasoning skills of MatCha we incorporate examples from MATH and DROP into the pre-training objective, by rendering the input text as images.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;End-to-end results&lt;/h2>; &lt;p>; We use a &lt;a href=&quot;https://arxiv.org/abs/2210.03347&quot;>;Pix2Struct&lt;/a>; model backbone, which is an image-to-text transformer tailored for website understanding, and pre-train it with the two tasks described above. We demonstrate the strengths of MatCha by fine-tuning it on several visual language tasks — tasks involving charts and plots for question answering and summarization where no access to the underlying table is possible. MatCha surpasses previous models&#39; performance by a large margin and also outperforms the previous state of the art, which assumes access to underlying tables. &lt;/p>; &lt;p>; In the figure below, we first evaluate two baseline models that incorporate information from an &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;OCR pipeline&lt;/a>;, which until recently was the standard approach for working with charts. The first is based on &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, the second on &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;VisionTaPas&lt;/a>;. We also compare against &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI-17B&lt;/a>;, which is a large (~1000 times larger than the other models) image plus text-to-text transformer trained on a diverse set of tasks but with limited capabilities for reading text and other forms of visual language. Finally, we report the Pix2Struct and MatCha model results. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s1646/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;696&quot; data-original-width=&quot;1646&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Experimental results on two chart QA benchmarks ChartQA &amp;amp; PlotQA (using relaxed accuracy) and a chart summarization benchmark chart-to-text (using BLEU4). Matcha surpasses the state of the art by a large margin on QA, compared to larger models, and matches these larger models on summarization.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For QA datasets, we use the &lt;a href=&quot;https://github.com/google-research/pix2struct/blob/main/pix2struct/metrics.py#L81&quot;>;official relaxed accuracy metric&lt;/a>; that allows for small relative errors in numerical outputs. For chart-to-text summarization, we report &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; scores. MatCha achieves noticeably improved results compared to baselines for question answering, and comparable results to PaLI in summarization, where large size and extensive long text/captioning generation pre-training are advantageous for this kind of long-form text generation. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Derendering plus large language model chains&lt;/h2>; &lt;p>; While extremely performant for their number of parameters, particularly on extractive tasks, we observed that fine-tuned MatCha models could still struggle with end-to-end complex reasoning (eg, mathematical operations involving large numbers or multiple steps). Thus, we also propose a two-step method to tackle this: 1) a model reads a chart, then outputs the underlying table, 2) a large language model (LLM) reads this output and then tries to answer the question solely based on the textual input. &lt;/p>; &lt;p>; For the first model, we fine-tuned MatCha solely on the chart-to-table task, increasing the output sequence length to guarantee it could recover all or most of the information in the chart. &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; is the resulting model. In the second stage, any LLM (such as &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;) can be used for the task, and we can rely on the standard methods to increase performance on LLMs, for example &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;>;self-consistency&lt;/a>;. We also experimented with &lt;a href=&quot;https://arxiv.org/abs/2211.12588&quot;>;program-of-thoughts&lt;/a>; where the model produces executable Python code to offload complex computations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s622/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;622&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the DePlot+LLM method. This is a real example using &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. The blue boxes are input to the LLM and the red boxes contain the answer generated by the LLMs. We highlight some of the key reasoning steps in each answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in the example above, the DePlot model in combination with LLMs outperforms fine-tuned models by a significant margin, especially so in the human-sourced portion of ChartQA, where the questions are more natural but demand more difficult reasoning. Furthermore, DePlot+LLM can do so without access to any training data. &lt;/p>; &lt;p>; We have released the new models and code at our &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/deplot&quot;>;GitHub repo&lt;/a>;, where you can try it out yourself in colab. Checkout the papers for &lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; for more details on the experimental results. We hope that our results can benefit the research community and make the information in charts and plots more accessible to everyone. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was carried out by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen and Yasemin Altun from our &lt;a href=&quot;https://research.google/teams/language/&quot;>;Language Team&lt;/a>; as part of Fangyu&#39;s internship project. Nigel Collier from Cambridge also was a collaborator. We would like to thank Joshua Howland, Alex Polozov, Shrestha Basu Mallick, Massimo Nicosia and William Cohen for their valuable comments and suggestions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4195590947404721374/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html&quot; rel=&quot;alternate&quot; title=&quot;Foundation models for reasoning on charts&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s72-c/MatCha.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3147300214577163215&lt;/id>;&lt;published>;2023-05-26T09:58:00.001-07:00&lt;/published>;&lt;updated>;2023-05-26T16:35:56.897-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Hardware&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Barkour: Benchmarking animal-level agility with quadruped robots&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ken Caluwaerts and Atil Iscen, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s320/barkour%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Creating robots that exhibit robust and dynamic locomotion capabilities, similar to animals or humans, has been a long-standing goal in the robotics community. In addition to completing tasks quickly and efficiently, agility allows legged robots to move through &lt;a href=&quot;https://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot;>;complex environments&lt;/a>; that are otherwise difficult to traverse. Researchers at Google have been pursuing agility for &lt;a href=&quot;https://arxiv.org/abs/1804.10332&quot;>;multiple years&lt;/a>; and across &lt;a href=&quot;https://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html&quot;>;various form factors&lt;/a>;. Yet, while researchers have enabled &lt;a href=&quot;https://www.science.org/doi/10.1126/scirobotics.abc5986&quot;>;robots to hike&lt;/a>; or &lt;a href=&quot;https://www.roboticsproceedings.org/rss11/p47.pdf&quot;>;jump over some obstacles&lt;/a>;, there is still no generally accepted benchmark that comprehensively measures robot agility or mobility. In contrast, benchmarks are driving forces behind the development of machine learning, such as &lt;a href=&quot;https://arxiv.org/abs/1409.0575&quot;>;ImageNet&lt;/a>; for computer vision, and &lt;a href=&quot;https://github.com/openai/gym&quot;>;OpenAI Gym&lt;/a>; for reinforcement learning (RL). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.14654&quot;>;Barkour: Benchmarking Animal-level Agility with Quadruped Robots&lt;/a>;”, we introduce the &lt;em>;Barkour &lt;/em>;agility benchmark for quadruped robots, along with a &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>;-based generalist locomotion policy. Inspired by dog agility competitions, a legged robot must sequentially display a variety of skills, including moving in different directions, traversing uneven terrains, and jumping over obstacles within a limited timeframe to successfully complete the benchmark. By providing a diverse and challenging obstacle course, the Barkour benchmark encourages researchers to develop locomotion controllers that move fast in a controllable and versatile way. Furthermore, by tying the performance metric to real dog performance, we provide an intuitive metric to understand the robot performance with respect to their animal counterparts. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/zola_vs_ap.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We invited a handful of &lt;a href=&quot;https://blog.google/inside-google/life-at-google/working-home-ruff-dooglers-make-it-little-better/&quot;>;dooglers&lt;/a>; to try the obstacle course to ensure that our agility objectives were realistic and challenging. Small dogs complete the obstacle course in approximately 10s, whereas our robot&#39;s typical performance hovers around 20s.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Barkour benchmark&lt;/h2>; &lt;p>; The Barkour scoring system uses a per obstacle and an overall course target time based on the target speed of small dogs in the &lt;a href=&quot;https://images.akc.org/pdf/rulebooks/REAGIL.pdf&quot;>;novice agility competitions&lt;/a>; (about 1.7m/s). Barkour scores range from 0 to 1, with 1 corresponding to the robot successfully traversing all the obstacles along the course within the allotted time of approximately 10 seconds, the average time needed for a similar-sized dog to traverse the course. The robot receives penalties for skipping, failing obstacles, or moving too slowly. &lt;/p>; &lt;p>; Our standard course consists of four unique obstacles in a 5m x 5m area. This is a denser and smaller setup than a typical dog competition to allow for easy deployment in a robotics lab. Beginning at the start table, the robot needs to weave through a set of poles, climb an A-frame, clear a 0.5m broad jump and then step onto the end table. We chose this subset of obstacles because they test a diverse set of skills while keeping the setup within a small footprint. As is the case for real dog agility competitions, the Barkour benchmark can be easily adapted to a larger course area and may incorporate a variable number of obstacles and course configurations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s1193/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;1193&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the Barkour benchmark&#39;s obstacle course setup, which consists of weave poles, an A-frame, a broad jump, and pause tables. The intuitive scoring mechanism, inspired by dog agility competitions, balances speed, agility and performance and can be easily modified to incorporate other types of obstacles or course configurations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning agile locomotion skills&lt;/h2>; &lt;p>; The Barkour benchmark features a diverse set of obstacles and a delayed reward system, which pose a significant challenge when training a single policy that can complete the entire obstacle course. So in order to set a strong performance baseline and demonstrate the effectiveness of the benchmark for robotic agility research, we adopt a student-teacher framework combined with a zero-shot sim-to-real approach. First, we train individual specialist locomotion skills (teacher) for different obstacles using on-policy RL methods. In particular, we leverage &lt;a href=&quot;https://github.com/google/brax&quot;>;recent advances&lt;/a>; in large-scale &lt;a href=&quot;https://arxiv.org/pdf/2108.10470.pdf&quot;>;parallel simulation&lt;/a>; to equip the robot with individual skills, including walking, slope climbing, and jumping policies. &lt;/p>; &lt;p>; Next, we train a single policy (student) that performs all the skills and transitions in between by using a student-teacher framework, based on the specialist skills we previously trained. We use simulation rollouts to create datasets of state-action pairs for each one of the specialist skills. This dataset is then distilled into a single Transformer-based generalist locomotion policy, which can handle various terrains and adjust the robot&#39;s gait based on the perceived environment and the robot&#39;s state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s1547/Barkour%20training.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;296&quot; data-original-width=&quot;1547&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s16000/Barkour%20training.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During deployment, we pair the locomotion transformer policy that is capable of performing multiple skills with a navigation controller that provides velocity commands based on the robot&#39;s position. Our trained policy controls the robot based on the robot&#39;s surroundings represented as an elevation map, velocity commands, and on-board sensory information provided by the robot.&lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/Generalist.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Deployment pipeline for the locomotion transformer architecture. At deployment time, a high-level navigation controller guides the real robot through the obstacle course by sending commands to the locomotion transformer policy.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Robustness and repeatability are difficult to achieve when we aim for peak performance and maximum speed. Sometimes, the robot might fail when overcoming an obstacle in an agile way. To handle failures we train a &lt;a href=&quot;https://arxiv.org/pdf/2110.05457.pdf&quot;>;recovery policy&lt;/a>; that quickly gets the robot back on its feet, allowing it to continue the episode. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We evaluate the Transformer-based generalist locomotion policy using custom-built quadruped robots and show that by optimizing for the proposed benchmark, we obtain agile, robust, and versatile skills for our robot in the real world. We further provide analysis for various design choices in our system and their impact on the system performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/s1895/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1275&quot; data-original-width=&quot;1895&quot; height=&quot;269&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/w400-h269/image4.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model of the custom-built robots used for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We deploy both the specialist and generalist policies to hardware (zero-shot sim-to-real). The robot&#39;s target trajectory is provided by a set of waypoints along the various obstacles. In the case of the specialist policies, we switch between specialist policies by using a hand-tuned policy switching mechanism that selects the most suitable policy given the robot&#39;s position. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/3obs_side.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Typical performance of our agile locomotion policies on the Barkour benchmark. Our custom-built quadruped robot robustly navigates the terrain&#39;s obstacles by leveraging various skills learned using RL in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that very often our policies can handle unexpected events or even hardware degradation resulting in good average performance, but failures are still possible. As illustrated in the image below, in case of failures, our recovery policy quickly gets the robot back on its feet, allowing it to continue the episode. By combining the recovery policy with a simple walk-back-to-start policy, we are able to run repeated experiments with minimal human intervention to measure the robustness. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/recovery_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative example of robustness and recovery behaviors. The robot trips and rolls over after heading down the A-frame. This triggers the recovery policy, which enables the robot to get back up and continue the course.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that across a large number of evaluations, the single generalist locomotion transformer policy and the specialist policies with the policy switching mechanism achieve similar performance. The locomotion transformer policy has a slightly lower average Barkour score, but exhibits smoother transitions between behaviors and gaits. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;75%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/8x8_cont_bright_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Measuring robustness of the different policies across a large number of runs on the Barkour benchmark.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/s1313/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1313&quot; height=&quot;300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/w640-h300/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Histogram of the agility scores for the locomotion transformer policy. The highest scores shown in blue (0.75 - 0.9) represent the runs where the robot successfully completes all obstacles. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that developing a benchmark for legged robotics is an important first step in quantifying progress toward animal-level agility. To establish a strong baseline, we investigated a zero-shot sim-to-real approach, taking advantage of large-scale parallel simulation and recent advancements in training Transformer-based architectures. Our findings demonstrate that Barkour is a challenging benchmark that can be easily customized, and that our learning-based method for solving the benchmark provides a quadruped robot with a single low-level policy that can perform a variety of agile low-level skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We would like to thank our co-authors at Google DeepMind and our collaborators at Google Research: Wenhao Yu, J. Chase Kew, Tingnan Zhang, Daniel Freeman, Kuang-Hei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang, Nathan Batchelor, Steven Bohez, Federico Casarini, Jose Enrique Chen, Omar Cortes, Erwin Coumans, Adil Dostmohamed, Gabriel Dulac-Arnold, Alejandro Escontrela, Erik Frey, Roland Hafner, Deepali Jain, Yuheng Kuang, Edward Lee, Linda Luu, Ofir Nachum, Ken Oslund, Jason Powell, Diego Reyes, Francesco Romano, Feresteh Sadeghi, Ron Sloat, Baruch Tabanpour, Daniel Zheng, Michael Neunert, Raia Hadsell, Nicolas Heess, Francesco Nori, Jeff Seto, Carolina Parada, Vikas Sindhwani, Vincent Vanhoucke, and Jie Tan. We would also like to thank Marissa Giustina, Ben Jyenis, Gus Kouretas, Nubby Lee, James Lubin, Sherry Moore, Thinh Nguyen, Krista Reymann, Satoshi Kataoka, Trish Blazina, and the members of the robotics team at Google DeepMind for their contributions to the project.Thanks to John Guilyard for creating the animations in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3147300214577163215/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html&quot; rel=&quot;alternate&quot; title=&quot;Barkour: Benchmarking animal-level agility with quadruped robots&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s72-c/barkour%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4176009881387884637&lt;/id>;&lt;published>;2023-05-25T16:09:00.005-07:00&lt;/published>;&lt;updated>;2023-06-08T14:27:41.308-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Unsupervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private clustering for large-scale datasets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Vincent Cohen-Addad and Alessandro Epasto, Research Scientists, Google Research, Graph Mining team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s900/COVID.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a central problem in &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning (ML) with many applications across domains in both industry and academic research more broadly. At its core, clustering consists of the following problem: given a set of data elements, the goal is to partition the data elements into groups such that similar objects are in the same group, while dissimilar objects are in different groups. This problem has been studied in math, computer science, operations research and statistics for more than 60 years in its myriad variants. Two common forms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;clustering&lt;/a>; are metric clustering, in which the elements are points in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_space&quot;>;metric space&lt;/a>;, like in the &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; problem, and graph clustering, where the elements are nodes of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graph&lt;/a>; whose edges represent similarity among them. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s596/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;592&quot; height=&quot;320&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s320/image1.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; clustering problem, we are given a set of points in a metric space with the objective to identify &lt;em>;k&lt;/em>; representative points, called centers (here depicted as triangles), so as to minimize the sum of the squared distances from each point to its closest center. &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Kmeans_toomany.PNG&quot;>;Source&lt;/a>;, rights: CC-BY-SA-4.0&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Despite the extensive literature on algorithm design for clustering, few practical works have focused on rigorously protecting the user&#39;s privacy during clustering. When clustering is applied to personal data (eg, the queries a user has made), it is necessary to consider the privacy implications of using a clustering solution in a real system and how much information the output solution reveals about the input data. &lt;/p>; &lt;p>; To ensure privacy in a rigorous sense, one solution is to develop &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differentially private&lt;/a>; (DP) clustering algorithms. These algorithms ensure that the output of the clustering does not reveal private information about a specific data element (eg, whether a user has made a given query) or sensitive data about the input graph (eg, a relationship in a social network). Given the importance of privacy protections in unsupervised machine learning, in recent years Google has invested in research on &lt;a href=&quot;https://arxiv.org/abs/2008.08007&quot;>;theory&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2021/10/practical-differentially-private.html?hl=el&amp;amp;m=1&quot;>;practice&lt;/a>; of differentially private &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;metric&lt;/a>; or &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/da645920dcd3bd35b0dae329894bad80-Abstract-Conference.html&quot;>;graph&lt;/a>; clustering, and differential privacy in a variety of contexts, eg,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;heatmaps&lt;/a>;&amp;nbsp;or &lt;a href=&quot;https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&quot;>;tools&lt;/a>; to design DP algorithms. &lt;/p>; &lt;p>; Today we are excited to announce two important updates: 1) a &lt;a href=&quot;https://arxiv.org/abs/2302.00037&quot;>;new differentially-private algorithm&lt;/a>; for hierarchical graph clustering, which we&#39;ll be presenting at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>;, and 2) the &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/hst_clustering&quot;>;open-source release&lt;/a>; of the code of a scalable differentially-private &lt;em>;k&lt;/em>;-means algorithm. This code brings differentially private &lt;em>;k&lt;/em>;-means clustering to large scale datasets using distributed computing. Here, we will also discuss our work on clustering technology for a recent launch in the health domain for informing public health authorities. &lt;/p>; &lt;br />; &lt;h2>;Differentially private hierarchical clustering&lt;/h2>; &lt;p>; Hierarchical clustering is a popular clustering approach that consists of recursively partitioning a dataset into clusters at an increasingly finer granularity. A well known example of hierarchical clustering is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Phylogenetic_tree&quot;>;phylogenetic tree&lt;/a>; in biology in which all life on Earth is partitioned into finer and finer groups (eg, kingdom, phylum, class, order, etc.). A hierarchical clustering algorithm receives as input a graph representing the similarity of entities and learns such recursive partitions in an unsupervised way. Yet at the time of our research no algorithm was known to compute hierarchical clustering of a graph with edge privacy, ie, preserving the privacy of the vertex interactions. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://doi.org/10.48550/arXiv.2302.00037&quot;>;Differentially-Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>;”, we consider how well the problem can be approximated in a DP context and establish firm upper and lower bounds on the privacy guarantee. We design an approximation algorithm (the first of its kind) with a polynomial running time that achieves both an additive error that scales with the number of nodes &lt;em>;n&lt;/em>; (of order &lt;em>;n&lt;sup>;2.5&lt;/sup>;&lt;/em>;) and a multiplicative approximation of O(log&lt;sup>;½&lt;/sup>; n), with the multiplicative error identical to the non-private setting. We further provide a new lower bound on the additive error (of order &lt;em>;n&lt;sup>;2&lt;/sup>;&lt;/em>;) for any private algorithm (irrespective of its running time) and provide an exponential-time algorithm that matches this lower bound. Moreover, our paper includes a beyond-worst-case analysis focusing on the &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html&quot;>;hierarchical stochastic block model&lt;/a>;, a standard random graph model that exhibits a natural hierarchical clustering structure, and introduces a private algorithm that returns a solution with an additive cost over the optimum that is negligible for larger and larger graphs, again matching the non-private state-of-the-art approaches. We believe this work expands the understanding of privacy preserving algorithms on graph data and will enable new applications in such settings. &lt;/p>; &lt;br />; &lt;h2>;Large-scale differentially private clustering&lt;/h2>; &lt;p>; We now switch gears and discuss our work for metric space clustering. Most prior work in DP metric clustering has focused on improving the approximation guarantees of the algorithms on the &lt;em>;k&lt;/em>;-means objective, leaving scalability questions out of the picture. Indeed, it is not clear how efficient non-private algorithms such as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;>;k-means++&lt;/a>;&lt;/em>; or &lt;em>;&lt;a href=&quot;https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf&quot;>;k-means//&lt;/a>;&lt;/em>; can be made differentially private without sacrificing drastically either on the approximation guarantees or the scalability. On the other hand, both scalability and privacy are of primary importance at Google. For this reason, we recently published &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;multiple&lt;/a>; &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;papers&lt;/a>; that address the problem of designing efficient differentially private algorithms for clustering that can scale to massive datasets. Our goal is, moreover, to offer scalability to large scale input datasets, even when the target number of centers, &lt;em>;k&lt;/em>;, is large. &lt;/p>; &lt;p>; We work in the &lt;a href=&quot;https://www.cs.umd.edu/~gasarch/MPC/mpc.pdf&quot;>;massively parallel computation&lt;/a>; (MPC) model, which is a computation model representative of modern distributed computation architectures. The model consists of several machines, each holding only part of the input data, that work together with the goal of solving a global problem while minimizing the amount of communication between machines. We present a &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;differentially private constant factor approximation algorithm&lt;/a>; for &lt;em>;k&lt;/em>;-means that only requires a constant number of rounds of synchronization. Our algorithm builds upon our &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;previous work&lt;/a>; on the problem (with code &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/hst_clustering&quot;>;available here&lt;/a>;), which was the first differentially-private clustering algorithm with provable approximation guarantees that can work in the MPC model. &lt;/p>; &lt;p>; The DP constant factor approximation algorithm drastically improves on the previous work using a two phase approach. In an initial phase it computes a crude approximation to “seed” the second phase, which consists of a more sophisticated distributed algorithm. Equipped with the first-step approximation, the second phase relies on results from the &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3406325.3451022&quot;>;Coreset literature&lt;/a>; to subsample a relevant set of input points and find a good differentially private clustering solution for the input points. We then prove that this solution generalizes with approximately the same guarantee to the entire input. &lt;/p>; &lt;br />; &lt;h2>;Vaccination search insights via DP clustering&lt;/h2>; &lt;p>; We then apply these advances in differentially private clustering to real-world applications. One example is our application of our differentially-private clustering solution for publishing COVID vaccine-related queries, while providing strong privacy protections for the users. &lt;/p>; &lt;p>; The goal of &lt;a href=&quot;https://google-research.github.io/vaccination-search-insights/?&quot;>;Vaccination Search Insights&lt;/a>; (VSI) is to help public health decision makers (health authorities, government agencies and nonprofits) identify and respond to communities&#39; information needs regarding COVID vaccines. In order to achieve this, the tool allows users to explore at different geolocation granularities (zip-code, county and state level in the US) the top themes searched by users regarding COVID queries. In particular, the tool visualizes statistics on trending queries rising in interest in a given locale and time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s800/COVIDSearchStats.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s16000/COVIDSearchStats.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of the output of the tool. Displayed on the left, the top searches related to Covid vaccines during the period Oct 10-16 2022. On the right, the queries that have had rising importance during the same period and compared to the previous week.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To better help identifying the themes of the trending searches, the tool clusters the search queries based on their semantic similarity. This is done by applying a custom-designed &lt;em>;k&lt;/em>;-means–based algorithm run over search data that has been anonymized using the DP Gaussian mechanism to add noise and remove low-count queries (thus resulting in a differentially clustering). The method ensures strong differential privacy guarantees for the protection of the user data. &lt;/p>; &lt;p>; This tool provided fine-grained data on COVID vaccine perception in the population at unprecedented scales of granularity, something that is especially relevant to understand the needs of the marginalized communities disproportionately affected by COVID. This project highlights the impact of our investment in research in differential privacy, and unsupervised ML methods. We are looking to other important areas where we can apply these clustering techniques to help guide decision making around global health challenges, like search queries on &lt;a href=&quot;https://blog.google/technology/health/dr-von-nguyens-temperature-check-on-public-health/&quot;>;climate change–related challenges&lt;/a>; such as air quality or extreme heat. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Jacob Imola, Silvio Lattanzi, Jason Lee, Mohammad Mahdian, Vahab Mirrokni, Andres Munoz Medina, Shyam Narayanan, Mark Phillips, David Saulpic, Chris Schwiegelshohn, Sergei Vassilvitskii, Peilin Zhong, and the members of the&lt;/em>;&amp;nbsp;&lt;i>;Health AI&lt;/i>;&lt;em>; team that made the VSI launch possible: Shailesh Bavadekar, Adam Boulanger, Tague Griffith, Mansi Kansal, Chaitanya Kamath, Akim Kumok, Yael Mayer, Tomer Shekel, Megan Shum, Charlotte Stanton, Mimi Sun, Swapnil Vispute, and Mark Young. &lt;/em>; &lt;/p>; &lt;p>; &lt;em>;For more information on the &lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/graph-mining/&quot;>;Graph Mining team&lt;/a>; (part of &lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/&quot;>;Algorithm and Optimization&lt;/a>;) visit our pages.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4176009881387884637/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private clustering for large-scale datasets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s72-c/COVID.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6927879920303666871&lt;/id>;&lt;published>;2023-05-25T10:03:00.008-07:00&lt;/published>;&lt;updated>;2023-06-01T08:59:51.641-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google I/O&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google Research at I/O 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by James Manyika, SVP Google Research and Technology &amp;amp; Society, and Jeff Dean, Chief Scientist, Google DeepMind and Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s1200/GoogleIO.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Wednesday, May 10th was an exciting day for the &lt;a href=&quot;https://research.google&quot;>;Google Research&lt;/a>; community as we watched the results of months and years of our foundational and applied work get announced on the &lt;a href=&quot;https://io.google/2023/&quot;>;Google I/O&lt;/a>; stage. With the quick pace of announcements on stage, it can be difficult to convey the substantial effort and unique innovations that underlie the technologies we presented. So today, we&#39;re excited to reveal more about the research efforts behind some of the many compelling announcements at &lt;a href=&quot;https://www.youtube.com/playlist?list=PL590L5WQmH8dAqv03RCMbZrbzxqCn6W3O&quot;>;this year&#39;s I/O&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;br />; &lt;h2>;PaLM 2 &lt;/h2>; &lt;p>; Our next-generation large language model (LLM), &lt;a href=&quot;https://ai.google/discover/palm2&quot;>;PaLM 2&lt;/a>;, is built on advances in &lt;a href=&quot;https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training&quot;>;compute-optimal scaling&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;scaled instruction-fine tuning&lt;/a>; and &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;improved dataset mixture&lt;/a>;. By fine-tuning and instruction-tuning the model for different purposes, we have been able to integrate state-of-the-art capabilities into over 25 Google products and features, where it is already helping to inform, assist and delight users. For example: &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; is an early experiment that lets you collaborate with generative AI and helps to boost productivity, accelerate ideas and fuel curiosity. It builds on &lt;a href=&quot;https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html&quot;>;advances in deep learning efficiency&lt;/a>; and leverages &lt;a href=&quot;https://ai.google/static/documents/google-about-bard.pdf&quot;>;reinforcement learning from human feedback&lt;/a>; to provide more relevant responses and increase the model&#39;s ability to follow instructions. Bard is now available in 180 countries, where users can interact with it in English, Japanese and Korean, and thanks to the multilingual capabilities afforded by PaLM 2, support for 40 languages is coming soon. &lt;/li>;&lt;li>;With &lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;Search Generative Experience&lt;/a>; we&#39;re taking more of the work out of searching, so you&#39;ll be able to understand a topic faster, uncover new viewpoints and insights, and get things done more easily. As part of this experiment, you&#39;ll see an AI-powered snapshot of key information to consider, with links to dig deeper. &lt;/li>;&lt;li>;&lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; is an easy-to-use prototyping environment for the &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;PaLM API&lt;/a>;, powered by PaLM 2. In fact, internal user engagement with early prototypes of MakerSuite accelerated the development of our PaLM 2 model itself. MakerSuite grew out of research focused on prompting tools, or tools explicitly designed for customizing and controlling LLMs. This line of research includes &lt;a href=&quot;https://research.google/pubs/pub51353/&quot;>;PromptMaker&lt;/a>; (precursor to MakerSuite), and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517582&quot;>;AI Chains&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3491101.3519729&quot;>;PromptChainer&lt;/a>; (one of the first research efforts demonstrating the utility of LLM chaining). &lt;/li>;&lt;li>;Project &lt;a href=&quot;https://thoughtful.withgoogle.com/about&quot;>;Tailwind&lt;/a>; also made use of early research prototypes of MakerSuite to develop features to help writers and researchers explore ideas and improve their prose; its AI-first notebook prototype used PaLM 2 to allow users to ask questions of the model grounded in documents they define. &lt;/li>;&lt;li>;&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>; is our state-of-the-art medical LLM, built on PaLM 2. Med-PaLM 2 achieved &lt;a href=&quot;https://arxiv.org/abs/2305.09617&quot;>;86.5% performance&lt;/a>; on US Medical Licensing Exam–style questions, illustrating its exciting potential for health. We&#39;re now exploring multimodal capabilities to synthesize inputs like X-rays. &lt;/li>;&lt;li>;&lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Codey&lt;/a>; is a version of PaLM 2 fine-tuned on source code to function as a developer assistant. It supports a broad range of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;Code AI&lt;/a>; features, including code completions, code explanation, bug fixing, source code migration, error explanations, and more. Codey is available through our trusted tester program via IDEs (&lt;a href=&quot;https://blog.google/technology/developers/google-colab-ai-coding-features/&quot;>;Colab&lt;/a>;, &lt;a href=&quot;https://developer.android.com/studio/preview/studio-bot&quot;>;Android Studio&lt;/a>;, &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-ai-for-google-cloud&quot;>;Duet AI for Cloud&lt;/a>;, &lt;a href=&quot;https://techcrunch.com/2023/05/10/googles-firebase-gets-ai-extensions-opens-up-its-marketplace/&quot;>;Firebase&lt;/a>;) and via a &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;3P-facing API&lt;/a>;. &lt;/li>; &lt;/ul>; &lt;p>; Perhaps even more exciting for developers, we have opened up the &lt;a href=&quot;https://makersuite.google.com/&quot;>;PaLM APIs &amp;amp; MakerSuite&lt;/a>; to provide the community opportunities to innovate using this groundbreaking technology. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/s1002/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1002&quot; height=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/w640-h616/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PaLM 2 has advanced coding capabilities that enable it to find code errors and make suggestions in a number of different languages.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Imagen&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Imagen family of image generation and editing models&lt;/a>; builds on advances in large &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based language models and &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>;. This family of models is being incorporated into multiple Google products, including: &lt;/p>; &lt;ul>; &lt;li>;Image generation in &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>;Google Slides&lt;/a>; and Android&#39;s &lt;a href=&quot;https://blog.google/products/android/new-android-features-generative-ai/&quot;>;Generative AI wallpaper&lt;/a>; are powered by our text-to-image generation features. &lt;/li>;&lt;li>;&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-new-ai-models-opens-generative-ai-studio&quot;>;Google Cloud&#39;s Vertex AI&lt;/a>; enables image generation, image editing, image upscaling and fine-tuning to help enterprise customers meet their business needs. &lt;/li>;&lt;li>;&lt;a href=&quot;https://developers.googleblog.com/2023/05/how-its-made-io-flip-adds-twist-to.html&quot;>;I/O Flip&lt;/a>;, a digital take on a classic card game, features Google developer mascots on cards that were entirely AI generated. This game showcased a fine-tuning technique called &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; for adapting pre-trained image generation models. Using just a handful of images as &lt;a href=&quot;https://dreambooth.github.io/&quot;>;inputs for fine-tuning&lt;/a>;, it allows users to generate personalized images in minutes. With DreamBooth, users can synthesize a subject in diverse scenes, poses, views, and lighting conditions that don&#39;t appear in the reference images. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s1649/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;1649&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;I/O Flip presents custom card decks designed using DreamBooth.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Phenaki&lt;/h2>; &lt;p>; &lt;a href=&quot;https://sites.research.google/phenaki/&quot;>;Phenaki&lt;/a>;, Google&#39;s Transformer-based text-to-video generation model was featured in the I/O pre-show. Phenaki is a &lt;a href=&quot;https://openreview.net/forum?id=vOEXS39nOF&quot;>;model that can synthesize realistic videos&lt;/a>; from textual prompt sequences by leveraging two main components: an encoder-decoder model that compresses videos to discrete embeddings and a transformer model that translates text embeddings to video tokens. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOOX_c2imHsESoim3U4K0eNWE97rtrGkVWd7TZ1icscOT3qf3O_BdvBakgEk7KwAYMIbzHCb7J_xL7rXxRbe4o-m0U9wdc4CN8oxMAliornJInsRbjNe0h2c7XEXjkoDGF8oavzodCis4oH2vMwwh19du1oLle_JZvtO1KoraOOpWEUuyNHVUiVzhgGQ/s200/panda_bear_highres.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;200&quot; height=&quot;200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOOX_c2imHsESoim3U4K0eNWE97rtrGkVWd7TZ1icscOT3qf3O_BdvBakgEk7KwAYMIbzHCb7J_xL7rXxRbe4o-m0U9wdc4CN8oxMAliornJInsRbjNe0h2c7XEXjkoDGF8oavzodCis4oH2vMwwh19du1oLle_JZvtO1KoraOOpWEUuyNHVUiVzhgGQ/s1600/panda_bear_highres.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivgQpjnthO9y35MrLxLMfWGkUwNWnaqD3Pt7rDSDKg-dThTWICJL7NGW5E7PGJnoiZZA_hk6Rgw3ChWCn7YSNu5QEpgfTbgG2TBaQ7WLPs-TPZ89cVvvHEaFP-AqqRxSox4ogjZ889Cz06pbDVL7P0s8VXJdl7m4MuqNtJLsVsK89ScepRJIt94Z9_bw/s635/Screenshot%202023-06-01%2010.56.18%20AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;635&quot; height=&quot;162&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivgQpjnthO9y35MrLxLMfWGkUwNWnaqD3Pt7rDSDKg-dThTWICJL7NGW5E7PGJnoiZZA_hk6Rgw3ChWCn7YSNu5QEpgfTbgG2TBaQ7WLPs-TPZ89cVvvHEaFP-AqqRxSox4ogjZ889Cz06pbDVL7P0s8VXJdl7m4MuqNtJLsVsK89ScepRJIt94Z9_bw/s320/Screenshot%202023-06-01%2010.56.18%20AM.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/s128/image4.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;128&quot; data-original-width=&quot;128&quot; height=&quot;200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/w200-h200/image4.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s548/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;269&quot; data-original-width=&quot;548&quot; height=&quot;157&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s320/image6.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;br />; &lt;h2>;ARCore and the Scene Semantic API&lt;/h2>; &lt;p>; Among the new features of &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; announced by the AR team at I/O, the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;Scene Semantic API&lt;/a>; can recognize pixel-wise semantics in an outdoor scene. This helps users create custom AR experiences based on the features in the surrounding area. This API is empowered by the outdoor semantic segmentation model, leveraging our recent works around the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLab&lt;/a>; architecture and an egocentric outdoor scene understanding dataset. The latest ARCore release also includes an improved monocular depth model that provides higher accuracy in outdoor scenes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/s486/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;242&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/w199-h400/image3.gif&quot; width=&quot;199&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scene Semantics API uses DeepLab-based semantic segmentation model to provide accurate pixel-wise labels in a scene outdoors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chirp&lt;/h2>; &lt;p>; &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;Chirp&lt;/a>; is Google&#39;s family of state-of-the-art &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Models&lt;/a>; trained on 12 million hours of speech to enable automatic speech recognition (ASR) for 100+ languages. The models can perform ASR on under-resourced languages, such as Amharic, Cebuano, and Assamese, in addition to widely spoken languages like English and Mandarin. Chirp is able to cover such a wide variety of languages by leveraging &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;self-supervised learning on unlabeled multilingual dataset with fine-tuning on a smaller set of labeled data&lt;/a>;. Chirp is now available in the Google Cloud &lt;a href=&quot;https://cloud.google.com/speech-to-text&quot;>;Speech-to-Text API&lt;/a>;, allowing users to perform inference on the model through a simple interface. You can get started with Chirp &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;here&lt;/a>;. &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s591/image7.gif&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;250&quot; data-original-width=&quot;591&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s16000/image7.gif&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;MusicLM&lt;/h2>; &lt;p>; At I/O, we launched &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM, a text-to-music model&lt;/a>; that generates 20 seconds of music from a text prompt. &lt;a href=&quot;https://aitestkitchen.withgoogle.com/experiments/music-lm&quot;>;You can try it yourself on AI Test Kitchen&lt;/a>;, or see it featured during the I/O preshow, where electronic musician and composer &lt;a href=&quot;https://en.wikipedia.org/wiki/Dan_Deacon&quot;>;Dan Deacon&lt;/a>; used MusicLM in his performance. &lt;/p>; &lt;p>; MusicLM, which consists of models powered by &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2208.12415&quot;>;MuLAN&lt;/a>;, can make music (from text, humming, images or video) and musical accompaniments to singing. AudioLM generates high quality audio with long-term consistency. It maps audio to a sequence of discrete tokens and casts audio generation as a language modeling task. To synthesize longer outputs efficiently, it used a novel approach we&#39;ve developed called &lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm&lt;/a>;. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s1927/keywordhero.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1927&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s16000/keywordhero.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;Universal Translator dubbing&lt;/h2>; &lt;p>; Our dubbing efforts leverage dozens of ML technologies to translate the full expressive range of video content, making videos accessible to audiences across the world. These technologies have been used to &lt;a href=&quot;https://www.youtube.com/watch?v=S-iIV5Oo0n0&quot;>;dub videos&lt;/a>; across a variety of products and content types, including educational content, advertising campaigns, and creator content, with more to come. We use deep learning technology to achieve &lt;a href=&quot;https://developers.googleblog.com/2022/12/improving-video-voice-dubbing-through-deep-learning.html&quot;>;voice preservation and lip matching&lt;/a>; and enable high-quality video translation. We&#39;ve built this product to include human review for quality, safety checks to help prevent misuse, and we make it accessible only to authorized partners. &lt;/p>; &lt;br />; &lt;h2>;AI for global societal good&lt;/h2>; &lt;p>; We are applying our AI technologies to solve some of the biggest global challenges, like mitigating climate change, adapting to a warming planet and improving human health and wellbeing. For example: &lt;/p>; &lt;ul>; &lt;li>;Traffic engineers use our Green Light recommendations to reduce stop-and-go traffic at intersections and improve the flow of traffic in cities from Bangalore to Rio de Janeiro and Hamburg. Green Light models each intersection, analyzing traffic patterns to develop recommendations that make traffic lights more efficient — for example, by better synchronizing timing between adjacent lights, or adjusting the “green time” for a given street and direction. &lt;/li>;&lt;li>;We&#39;ve also expanded global coverage on the &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>; to 80 countries, as part of our efforts to predict riverine floods and alert people who are about to be impacted before disaster strikes. Our &lt;a href=&quot;https://sites.research.google/floodforecasting&quot;>;flood forecasting efforts&lt;/a>; rely on &lt;a href=&quot;https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;hydrological models&lt;/a>; informed by satellite observations, weather forecasts and in-situ measurements. &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s1672/image5.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;713&quot; data-original-width=&quot;1672&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s16000/image5.png&quot; />;&lt;/a>;&lt;/div>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Technologies for inclusive and fair ML applications&lt;/h2>; &lt;p>; With our continued investment in AI technologies, we are emphasizing responsible AI development with the goal of making our models and tools useful and impactful while also ensuring fairness, safety and alignment with our &lt;a href=&quot;https://ai.google/principles/&quot;>;AI Principles&lt;/a>;. Some of these efforts were highlighted at I/O, including: &lt;/p>; &lt;ul>; &lt;li>;The release of the &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;Monk Skin Tone Examples (MST-E) Dataset&lt;/a>; to help practitioners gain a deeper understanding of the MST scale and train human annotators for more consistent, inclusive, and meaningful skin tone annotations. You can read more about this and other developments on our &lt;a href=&quot;https://skintone.google/&quot;>;website&lt;/a>;. This is an advancement on the open source release of the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone (MST) Scale&lt;/a>; we launched last year to enable developers to build products that are more inclusive and that better represent their diverse users. &lt;/li>;&lt;li>;A &lt;a href=&quot;https://www.kaggle.com/competitions/asl-fingerspelling/overview/description&quot;>;new Kaggle competition&lt;/a>; (open until August 10th) in which the ML community is tasked with creating a model that can quickly and accurately identify American Sign Language (ASL) fingerspelling — where each letter of a word is spelled out in ASL rapidly using a single hand, rather than using the specific signs for entire words — and translate it into written text. Learn more about the &lt;a href=&quot;https://www.youtube.com/watch?v=q3xKB3dfvtA&quot;>;fingerspelling Kaggle competition&lt;/a>;, which features a song from &lt;a href=&quot;https://www.deafandloud.com/&quot;>;Sean Forbes&lt;/a>;, a deaf musician and rapper. We also &lt;a href=&quot;https://www.youtube.com/watch?v=WC9x3jp_nV8&quot;>;showcased at I/O&lt;/a>; the winning algorithm from the prior year&#39;s competition powers &lt;a href=&quot;https://play.google.com/store/apps/details?id=edu.gatech.popsignai&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;PopSign&lt;/a>;, an ASL learning app for parents of deaf or hard of hearing children created by Georgia Tech and Rochester Institute of Technology (RIT). &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Building the future of AI together&lt;/h2>; &lt;p>; It&#39;s inspiring to be part of a community of so many talented individuals who are leading the way in developing state-of-the-art technologies, responsible AI approaches and exciting user experiences. We are in the midst of a period of incredible and transformative change for AI. Stay tuned for more updates about the ways in which the Google Research community is boldly exploring the frontiers of these technologies and using them responsibly to benefit people&#39;s lives around the world. We hope you&#39;re as excited as we are about the future of AI technologies and we invite you to engage with our teams through the references, sites and tools that we&#39;ve highlighted here. &lt;/p>;&lt;p>;&lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6927879920303666871/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research at I/O 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s72-c/GoogleIO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5648346519353613408&lt;/id>;&lt;published>;2023-05-23T10:51:00.007-07:00&lt;/published>;&lt;updated>;2023-05-31T12:19:41.513-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Semantic Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Resolving code review comments with ML&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alexander Frömmgen, Staff Software Engineer, and Lera Kharatyan, Senior Software Engineer, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s2000/comments2code.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Code-change reviews are a critical part of the software development process at scale, &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;taking&lt;/a>; a significant amount of the code authors&#39; and the code reviewers&#39; time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;millions of reviewer comments&lt;/a>; per year, and authors require an average of ~60 minutes &lt;a href=&quot;https://research.google/pubs/pub49446/&quot;>;active shepherding time&lt;/a>; between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must do to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code review process, eg, by proposing code changes based on a comment&#39;s text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe our application of recent advances in large sequence models (using the &lt;a href=&quot;https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot;>;DIDACT methodology&lt;/a>;) in a real-world setting to automatically resolve code review comments in the day-to-day development workflow at Google. As of today, code-change authors at Google address a substantial amount of reviewer comments by applying an ML-suggested edit. We expect that to reduce time spent on code reviews by hundreds of thousands of hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers&#39; productivity and allows them to focus on more creative and complex tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Predicting the code edit&lt;/h2>; &lt;p>; We started by training a model that predicts code edits needed to address reviewer comments. The model is pre-trained on various coding tasks and related developer activities (eg, renaming a variable, repairing a broken build, editing a file). It&#39;s then fine-tuned for this specific task with reviewed code changes, the reviewer comments, and the edits the author performed to address those comments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s1523/figure1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1523&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s16000/figure1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of an ML-suggested edit of refactorings that are spread within the code.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Google uses a &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2854146&quot;>;monorepo&lt;/a>;, a single repository for all of its software artifacts, which allows our training dataset to include all unrestricted code used to build Google&#39;s most recent software, as well as previous versions. &lt;/p>; &lt;p>; To improve the model quality, we iterated on the training dataset. For example, we compared the model performance for datasets with a single reviewer comment per file to datasets with multiple comments per file, and experimented with classifiers to clean up the training data based on a small, curated dataset to choose the model with the best offline precision and recall metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Serving infrastructure and user experience&lt;/h2>; &lt;p>; We designed and implemented the feature on top of the trained model, focusing on the overall user experience and developer efficiency. As part of this, we explored different user experience (UX) alternatives through a series of user studies. We then refined the feature based on insights from an internal beta (ie, a test of the feature in development) including user feedback (eg, a “Was this helpful?” button next to the suggested edit). &lt;/p>; &lt;p>; The final model was calibrated for a target &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&quot;>;precision&lt;/a>; of 50%. That is, we tuned the model and the suggestions filtering, so that 50% of suggested edits on our evaluation dataset are correct. In general, increasing the target precision reduces the number of shown suggested edits, and decreasing the target precision leads to more incorrect suggested edits. Incorrect suggested edits take the developers time and reduce the developers&#39; trust in the feature. We found that a target precision of 50% provides a good balance. &lt;/p>; &lt;p>; At a high level, for every new reviewer comment, we generate the model input in the same format that is used for training, query the model, and generate the suggested code edit. If the model is confident in the prediction and a few additional heuristics are satisfied, we send the suggested edit to downstream systems. The downstream systems, ie, the code review frontend and the integrated development environment (IDE), expose the suggested edits to the user and log user interactions, such as preview and apply events. A dedicated pipeline collects these logs and generates aggregate insights, eg, the overall acceptance rates as reported in this blog post. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s1250/figure%202c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1250&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s16000/figure%202c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Architecture of the ML-suggested edits infrastructure. We process code and infrastructure from multiple services, get the model predictions and surface the predictions in the code review tool and IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The developer interacts with the ML-suggested edits in the code review tool and the IDE. Based on insights from the user studies, the integration into the code review tool is most suitable for a streamlined review experience. The IDE integration provides additional functionality and supports 3-way merging of the ML-suggested edits (left in the figure below) in case of conflicting local changes on top of the reviewed code state (right) into the merge result (center). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s958/figure%203%20(25%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s16000/figure%203%20(25%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;3-way-merge UX in IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Offline evaluations indicate that the model addresses 52% of comments with a target precision of 50%. The online metrics of the beta and the full internal launch confirm these offline metrics, ie, we see model suggestions above our target model confidence for around 50% of all relevant reviewer comments. 40% to 50% of all previewed suggested edits are applied by code authors. &lt;/p>; &lt;p>; We used the “not helpful” feedback during the beta to identify recurring failure patterns of the model. We implemented serving-time heuristics to filter these and, thus, reduce the number of shown incorrect predictions. With these changes, we traded quantity for quality and observed an increased real-world acceptance rate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s1753/figure%204%20(50%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1753&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s16000/figure%204%20(50%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Code review tool UX. The suggestion is shown as part of the comment and can be previewed, applied and rated as helpful or not helpful.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our beta launch showed a &lt;em>;discoverability challenge&lt;/em>;: code authors only previewed ~20% of all generated suggested edits. We modified the UX and introduced a prominent “Show ML-edit” button (see the figure above) next to the reviewer comment, leading to an overall preview rate of ~40% at launch. We additionally found that suggested edits in the code review tool are often not applicable due to conflicting changes that the author did during the review process. We addressed this with a button in the code review tool that opens the IDE in a merge view for the suggested edit. We now observe that more than 70% of these are applied in the code review tool and fewer than 30% are applied in the IDE. All these changes allowed us to increase the overall fraction of reviewer comments that are addressed with an ML-suggested edit by a factor of 2 from beta to the full internal launch. At Google scale, these results help automate the resolution of hundreds of thousands of comments each year. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s1920/figure%207.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;887&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s16000/figure%207.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Suggestions filtering funnel.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see ML-suggested edits addressing a wide range of reviewer comments in production. This includes simple localized refactorings and refactorings that are spread within the code, as shown in the examples throughout the blog post above. The feature addresses longer and less formally-worded comments that require code generation, refactorings and imports. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s1479/figure%205c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;651&quot; data-original-width=&quot;1479&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s16000/figure%205c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of a suggestion for a longer and less formally worded comment that requires code generation, refactorings and imports.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The model can also respond to complex comments and produce extensive code edits (shown below). The generated test case follows the existing unit test pattern, while changing the details as described in the comment. Additionally, the edit suggests a comprehensive name for the test reflecting the test semantics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s1497/figure%206c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1497&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s16000/figure%206c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the model&#39;s ability to respond to complex comments and produce extensive code edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this post, we introduced an ML-assistance feature to reduce the time spent on code review related changes. At the moment, a substantial amount of all actionable code review comments on supported languages are addressed with applied ML-suggested edits at Google. A 12-week A/B experiment across all Google developers will further measure the impact of the feature on the overall developer productivity. &lt;/p>; &lt;p>; We are working on improvements throughout the whole stack. This includes increasing the quality and recall of the model and building a more streamlined experience for the developer with improved discoverability throughout the review process. As part of this, we are investigating the option of showing suggested edits to the reviewer while they draft comments and expanding the feature into the IDE to enable code-change authors to get suggested code edits for natural-language commands. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This is the work of many people in Google Core Systems &amp;amp; Experiences team, Google Research, and DeepMind. We&#39;d like to specifically thank Peter Choy for bringing the collaboration together, and all of our team members for their key contributions and useful advice, including Marcus Revaj, Gabriela Surita, Maxim Tabachnyk, Jacob Austin, Nimesh Ghelani, Dan Zheng, Peter Josling, Mariana Stariolo, Chris Gorgolewski, Sascha Varkevisser, Katja Grünwedel, Alberto Elizondo, Tobias Welp, Paige Bailey, Pierre-Antoine Manzagol, Pascal Lamblin, Chenjie Gu, Petros Maniatis, Henryk Michalewski, Sara Wiltberger, Ambar Murillo, Satish Chandra, Madhura Dudhgaonkar, Niranjan Tulpule, Zoubin Ghahramani, Juanjo Carin, Danny Tarlow, Kevin Villela, Stoyan Nikolov, David Tattersall, Boris Bokowski, Kathy Nix, Mehdi Ghissassi, Luis C. Cobo, Yujia Li, David Choi, Kristóf Molnár, Vahid Meimand, Amit Patel, Brett Wiltshire, Laurent Le Brun, Mingpan Guo, Hermann Loose, Jonas Mattes, Savinee Dancs. Thanks to John Guilyard for creating the graphics in this post.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5648346519353613408/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot; rel=&quot;alternate&quot; title=&quot;Resolving code review comments with ML&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s72-c/comments2code.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3261601760457492933&lt;/id>;&lt;published>;2023-05-19T09:59:00.002-07:00&lt;/published>;&lt;updated>;2023-05-19T10:01:19.784-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Making ML models differentially private: Best practices and open challenges&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Natalia Ponomareva and Alex Kurakin, Staff Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s320/DPfy%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large machine learning (ML) models are ubiquitous in modern applications: from &lt;a href=&quot;https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters&quot;>;spam filters&lt;/a>; to &lt;a href=&quot;https://developers.google.com/machine-learning/recommendation/overview/types&quot;>;recommender systems&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;virtual assistants&lt;/a>;. These models achieve remarkable performance partially due to the abundance of available training data. However, these data can sometimes contain private information, including personal identifiable information, copyright material, etc. Therefore, protecting the privacy of the training data is critical to practical, applied ML. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential Privacy&lt;/a>; (DP) is one of the most widely accepted technologies that allows reasoning about data anonymization in a formal way. In the context of an ML model, DP can guarantee that each individual user&#39;s contribution will not result in a significantly different model. A model&#39;s privacy guarantees are characterized by a tuple (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;), where smaller values of both represent stronger DP guarantees and better privacy. &lt;/p>; &lt;p>; While there are successful &lt;a href=&quot;https://research.google/pubs/pub52351/&quot;>;examples&lt;/a>; of &lt;a href=&quot;https://ai.googleblog.com/2022/02/federated-learning-with-formal.html&quot;>;protecting training data&lt;/a>; using DP, obtaining good utility with differentially private ML (DP-ML) techniques can be challenging. First, there are inherent privacy/computation tradeoffs that may limit a model&#39;s utility. Further, DP-ML models often require architectural and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter&lt;/a>; tuning, and guidelines on how to do this effectively are limited or difficult to find. Finally, non-rigorous privacy reporting makes it challenging to compare and choose the best DP methods. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy&lt;/a>;”, to appear in the &lt;em>;&lt;a href=&quot;https://www.jair.org/index.php/jair&quot;>;Journal of Artificial Intelligence Research&lt;/a>;&lt;/em>;, we discuss the current state of DP-ML research. We provide an overview of common techniques for obtaining DP-ML models and discuss research, engineering challenges, mitigation techniques and current open questions. We will present tutorials based on this work at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>; and &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-ML methods&lt;/h2>; &lt;p>; DP can be introduced during the ML model development process in three places: (1) at the input data level, (2) during training, or (3) at inference. Each option provides privacy protections at different stages of the ML development process, with the weakest being when DP is introduced at the prediction level and the strongest being when introduced at the input level. Making the input data differentially private means that any model that is trained on this data will also have DP guarantees. When introducing DP during the training, only that particular model has DP guarantees. DP at the prediction level means that only the model&#39;s predictions are protected, but the model itself is not differentially private. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s1821/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;1821&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The task of introducing DP gets progressively easier from the left to right.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; DP is commonly introduced during training (DP-training). &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;Gradient noise injection&lt;/a>; methods, like &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, and their extensions are currently the most practical methods for achieving DP guarantees in complex models like large deep neural networks. &lt;/p>; &lt;p>; DP-SGD builds off of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; (SGD) optimizer with two modifications: (1) per-example gradients are clipped to a certain norm to limit sensitivity (the influence of an individual example on the overall model), which is a slow and computationally intensive process, and (2) a noisy gradient update is formed by taking aggregated gradients and adding noise that is proportional to the sensitivity and the strength of privacy guarantees. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s1439/DP-ML.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;494&quot; data-original-width=&quot;1439&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s16000/DP-ML.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP-SGD is a modification of SGD that involves a) clipping per-example gradients to limit the sensitivity and b) adding the noise, calibrated to the sensitivity and privacy guarantees, to the aggregated gradients, before the gradient update step. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Existing DP-training challenges&lt;/h2>; &lt;p>; Gradient noise injection methods usually exhibit: (1) loss of utility, (2) slower training, and (3) an increased &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_footprint&quot;>;memory footprint&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Loss of utility&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The best method for reducing utility drop is to use more computation. Using larger batch sizes and/or more iterations is one of the most prominent and practical ways of improving a model&#39;s performance. Hyperparameter tuning is also extremely important but often overlooked. The utility of DP-trained models is sensitive to the total amount of noise added, which depends on hyperparameters, like the clipping norm and batch size. Additionally, other hyperparameters like the learning rate should be re-tuned to account for noisy gradient updates. &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Another option is to obtain more data or use public data of similar distribution. This can be done by leveraging publicly available checkpoints, like &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/1910.10683.pdf&quot;>;T5&lt;/a>;, and fine-tuning them using private data. &lt;/p>; &lt;p>; &lt;strong>;Slower training&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Most gradient noise injection methods limit sensitivity via clipping per-example gradients, considerably slowing down &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;>;backpropagation&lt;/a>;. This can be addressed by choosing an efficient DP framework that efficiently implements per-example clipping. &lt;/p>; &lt;p>; &lt;strong>;Increased memory footprint&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;DP-training requires significant memory for computing and storing per-example gradients. Additionally, it requires significantly larger batches to obtain better utility. Increasing the computation resources (eg, the number and size of accelerators) is the simplest solution for extra memory requirements. Alternatively, &lt;a href=&quot;https://arxiv.org/abs/2109.12298&quot;>;several&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2201.12328&quot;>;works&lt;/a>; advocate for gradient accumulation where smaller batches are combined to simulate a larger batch before the gradient update is applied. Further, some algorithms (eg, &lt;a href=&quot;https://arxiv.org/pdf/2110.05679.pdf&quot;>;ghost clipping&lt;/a>;, which is based on &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;>;this paper&lt;/a>;) avoid per-example gradient clipping altogether. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Best practices&lt;/h2>; &lt;p>; The following best practices can attain rigorous DP guarantees with the best model utility possible. &lt;/p>; &lt;p>; &lt;strong>;Choosing the right privacy unit:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;First, we should be clear about a model&#39;s privacy guarantees. This is encoded by selecting the “privacy unit,” which represents the neighboring dataset concept (ie, datasets where only one row is different). Example-level protection is a common choice in the research literature, but may not be ideal, however, for user-generated data if individual users contributed multiple records to the training dataset. For such a case, user-level protection might be more appropriate. For text and sequence data, the choice of the unit is harder since in most applications individual training examples are not aligned to the semantic meaning embedded in the text. &lt;/p>; &lt;p>; &lt;strong>;Choosing privacy guarantees:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;We outline three broad tiers of privacy guarantees and encourage practitioners to choose the lowest possible tier below: &lt;/p>; &lt;ul style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;Tier 1 — Strong privacy guarantees:&lt;/em>; Choosing &lt;em>;ε&lt;/em>; ≤ 1 provides a strong privacy guarantee, but frequently results in a significant utility drop for large models and thus may only be feasible for smaller models. &lt;/li>;&lt;li>;&lt;em>;Tier 2 — Reasonable privacy guarantees:&lt;/em>; We advocate for the currently undocumented, but still widely used, goal for DP-ML models to achieve an &lt;em>;ε&lt;/em>; ≤ 10. &lt;/li>;&lt;li>;&lt;em>;Tier 3 — Weak privacy guarantees:&lt;/em>; Any finite &lt;em>;ε&lt;/em>; is an improvement over a model with no formal privacy guarantee. However, for &lt;em>;ε&lt;/em>; &amp;gt; 10, the DP guarantee alone cannot be taken as sufficient evidence of data anonymization, and additional measures (eg, empirical privacy auditing) may be necessary to ensure the model protects user data. &lt;/li>; &lt;/ul>; &lt;p>; &lt;strong>;Hyperparameter tuning&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Choosing hyperparameters requires optimizing over three inter-dependent objectives: 1) model utility, 2) privacy cost &lt;em>;ε&lt;/em>;, and 3) computation cost. Common strategies take two of the three as constraints, and focus on optimizing the third. We provide methods that will maximize the utility with a limited number of trials, eg, tuning with privacy and computation constraints. &lt;/p>; &lt;p>; &lt;strong>;Reporting privacy guarantees: &lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;A lot of works on DP for ML report only &lt;em>;ε&lt;/em>; and possibly &lt;em>;δ &lt;/em>;values for their training procedure. However, we believe that practitioners should provide a comprehensive overview of model guarantees that includes: &lt;/p>; &lt;ol style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;DP setting:&lt;/em>; Are the results assuming central DP with a trusted service provider, &lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;local DP&lt;/a>;, or some other setting? &lt;/li>;&lt;li>;&lt;em>;Instantiating the DP definition:&lt;/em>; &lt;ol type=&quot;a&quot;>; &lt;li>;&lt;em>;Data accesses covered:&lt;/em>; Whether the DP guarantee applies (only) to a single training run or also covers hyperparameter tuning etc. &lt;/li>;&lt;li>;&lt;em>;Final mechanism&#39;s output&lt;/em>;: What is covered by the privacy guarantees and can be released publicly (eg, model checkpoints, the full sequence of privatized gradients, etc.) &lt;/li>;&lt;li>;&lt;em>;Unit of privacy&lt;/em>;: The selected “privacy unit” (example-level, user-level, etc.) &lt;/li>;&lt;li>;&lt;em>;Adjacency definition&lt;/em>; for DP “neighboring” datasets: A description of how neighboring datasets differ (eg, add-or-remove, replace-one, zero-out-one). &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;Privacy accounting details:&lt;/em>; Providing accounting details, eg, composition and amplification, are important for proper comparison between methods and should include: &lt;ol type=&quot;a&quot;>; &lt;li>;Type of accounting used, eg, &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;-based accounting, PLD accounting, etc. &lt;/li>;&lt;li>;Accounting assumptions and whether they hold (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_regression#Maximum_likelihood-based_parameter_estimation&quot;>;Poisson&lt;/a>; sampling was assumed for privacy amplification but data shuffling was used in training). &lt;/li>;&lt;li>;Formal DP statement for the model and tuning process (eg, the specific &lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;-DP or &lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/1605.02065.pdf&quot;>;ρ-zCDP&lt;/a>;&lt;/em>; values). &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;Transparency and verifiability:&lt;/em>; When possible, complete open-source code using standard DP libraries for the key mechanism implementation and accounting components. &lt;/li>; &lt;/ol>; &lt;p>; &lt;strong>;Paying attention to all the components used:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Usually, DP-training is a straightforward application of DP-SGD or other algorithms. However, some components or losses that are often used in ML models (eg, &lt;a href=&quot;https://www.baeldung.com/cs/contrastive-learning#:~:text=Contrastive%20Loss,and%20dissimilar%20samples%20far%20apart.&quot;>;contrastive losses&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;graph neural network&lt;/a>; layers) should be examined to ensure privacy guarantees are not violated. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open questions&lt;/h2>; &lt;p>; While DP-ML is an active research area, we highlight the broad areas where there is room for improvement. &lt;/p>; &lt;p>; &lt;strong>;Developing better accounting methods&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Our current understanding of DP-training &lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;strong>; &lt;/strong>;&lt;/em>;guarantees&lt;strong>; &lt;/strong>;relies on a number of techniques, like Rényi DP composition and privacy amplification. We believe that better accounting methods for existing algorithms will demonstrate that DP guarantees for ML models are actually better than expected. &lt;/p>; &lt;p>; &lt;strong>;Developing better algorithms:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The computational burden of using gradient noise injection for DP-training comes from the need to use larger batches and limit per-example sensitivity. Developing methods that can use smaller batches or identifying other ways (apart from per-example clipping) to limit the sensitivity would be a breakthrough for DP-ML. &lt;/p>; &lt;p>; &lt;strong>;Better optimization techniques&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Directly applying the same DP-SGD recipe is believed to be suboptimal for adaptive optimizers because the noise added to privatize the gradient may accumulate in learning rate computation. Designing theoretically grounded DP adaptive optimizers remains an active research topic. Another potential direction is to better understand the surface of DP loss, since for standard (non-DP) ML models flatter regions have been shown to &lt;a href=&quot;https://arxiv.org/abs/2010.01412&quot;>;generalize&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;>;better&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Identifying architectures that are more robust to noise&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;There&#39;s an opportunity to better understand whether we need to adjust the architecture of an existing model when introducing DP. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;survey paper&lt;/a>; summarizes the current research related to making ML models DP, and provides practical tips on how to achieve the best privacy-utility trade offs. Our hope is that this work will serve as a reference point for the practitioners who want to effectively apply DP to complex ML models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank Hussein Hazimeh, Zheng Xu , Carson Denison , H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien and Abhradeep Thakurta, Badih Ghazi, Chiyuan Zhang for the help preparing this blog post, paper and tutorials content. Thanks to John Guilyard for creating the graphics in this post, and Ravi Kumar for comments.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3261601760457492933/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot; rel=&quot;alternate&quot; title=&quot;Making ML models differentially private: Best practices and open challenges&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s72-c/DPfy%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-234098392689663352&lt;/id>;&lt;published>;2023-05-18T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-05-18T14:08:39.899-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparse video tubes for joint video and image vision transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s320/TubeViT%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Video understanding is a challenging problem that requires reasoning about both spatial information (eg, for objects in a scene, including their locations and relations) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Temporal_information_retrieval#:~:text=Temporal%20information%20retrieval%20(T%2DIR,of%20the%20user%20information%20needs.&quot;>;temporal information&lt;/a>; for activities or events shown in a video. There are many video understanding applications and tasks, such as &lt;a href=&quot;https://cloud.google.com/video-intelligence&quot;>;understanding the semantic content of web videos&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/02/robot-see-robot-do.html&quot;>;robot perception&lt;/a>;. However, current works, such as &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2102.05095&quot;>;TimeSFormer&lt;/a>;, densely process the video and require significant compute, especially as model size plus video length and resolution increase. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>;”, to be presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>;, we introduce a simple technique that turns a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer&lt;/a>; (ViT) model image encoder into an efficient video backbone using sparse video tubes (learnable visual representations of samples from the video) to reduce the model&#39;s compute needs. This approach can seamlessly process both images and videos, which allows it to leverage both image and video data sources during training. This training further enables our sparse tubes ViT model to coalesce image and video backbones together to serve a dual role as either an image or video backbone (or both), depending on the input. We demonstrate that this model is scalable, can be adapted to large pre-trained ViTs without requiring full fine-tuning, and achieves state-of-the-art results across many video classification benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s860/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;219&quot; data-original-width=&quot;860&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using sparse video tubes to sample a video, combined with a standard ViT encoder, leads to an efficient visual representation that can be seamlessly shared with image inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building a joint image-video backbone&lt;/h2>; &lt;p>; Our sparse tube ViT uses a standard ViT backbone, consisting of a stack of &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; layers, that processes video information. Previous methods, such as ViViT, densely tokenize the video and then apply &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;factorized attention&lt;/a>;, ie, the attention weights for each token are computed separately for the temporal and spatial dimensions. In the standard ViT architecture, self-attention is computed over the whole token sequence. When using videos as input, token sequences become quite long, which can make this computation slow. Instead, in the method we propose, the video is sparsely sampled using &lt;em>;video tubes&lt;/em>;, which are 3D learnable visual representations of various shapes and sizes (described in more detail below) from the video. These tubes are used to sparsely sample the video using a &lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/strided-convolutions-wfUhx&quot;>;large temporal stride&lt;/a>;, ie, when a tube kernel is only applied to a few locations in the video, rather than every pixel. &lt;/p>; &lt;p>; By sparsely sampling the video tubes, we can use the same global self-attention module, rather than factorized attention like ViViT. We experimentally show that the addition of factorized attention layers can harm the performance due to the uninitialized weights. This single stack of transformer layers in the ViT backbone also enables better sharing of the weights and improves performance. Sparse video tube sampling is done by using a large spatial and temporal stride that selects tokens on a fixed grid. The large stride reduces the number of tokens in the full network, while still capturing both spatial and temporal information and enabling the efficient processing of all tokens. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Sparse video tubes&lt;/h2>; &lt;p>; Video tubes are 3D grid-based cuboids that can have different shapes or categories and capture different information with strides and starting locations that can overlap. In the model, we use three distinct tube shapes that capture: (1) only spatial information (resulting in a set of 2D image patches), (2) long temporal information (over a small spatial area), and (3) both spatial and temporal information equally. Tubes that capture only spatial information can be applied to both image and video inputs. Tubes that capture long temporal information or both temporal and spatial information equally are only applied to video inputs. Depending on the input video size, the three tube shapes are applied to the model multiple times to generate tokens. &lt;/p>; &lt;p>; A fixed position embedding, which captures the global location of each tube (including any strides, offsets, etc.) relative to all the other tubes, is applied to the video tubes. Different from the previous learned position embeddings, this fixed one better enables sparse, overlapping sampling. Capturing the global location of the tube helps the model know where each came from, which is especially helpful when tubes overlap or are sampled from distant video locations. Next, the tube features are concatenated together to form a set of &lt;em>;N&lt;/em>; tokens. These tokens are processed by a standard ViT encoder. Finally, we apply an attention pooling to compress all the tokens into a single representation and input to a fully connected (FC) layer to make the classification (eg, playing soccer, swimming, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s474/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;474&quot; data-original-width=&quot;346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our video ViT model works by sampling sparse video tubes from the video (shown at the bottom) to enable either or both image or video inputs to be seamlessly processed. These tubes have different shapes and capture different video features. Tube 1 (&lt;strong>;yellow&lt;/strong>;) only captures spatial information, resulting in a set of 2D patches that can be applied to image inputs. Tube 2 (&lt;strong>;red&lt;/strong>;) captures temporal information and some spatial information and tube 3 (&lt;strong>;green&lt;/strong>;) equally captures both temporal and spatial information (ie, the spatial size of the tube &lt;em>;x&lt;/em>; and &lt;em>;y&lt;/em>; are the same as the number of frames &lt;em>;t&lt;/em>;). Tubes 2 and 3 can only be applied to video inputs. The position embedding is added to all the tube features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaling video ViTs&lt;/h2>; &lt;p>; The process of building video backbones is computationally intensive, but our sparse tube ViT model enables computationally efficient scaling of video models, leveraging previously trained image backbones. Since image backbones can be adapted to a video backbone, large image backbones can be turned into large video backbones. More specifically, one can transfer the learned video feature representations from a small tube ViT to a large pre-trained image ViT and train the resulting model with video data for only a few steps, as opposed to a full training from scratch. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s899/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;899&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our approach enables scaling a sparse tube ViT in a more efficient way. Specifically, the video features from a small video ViT (&lt;strong>;top network&lt;/strong>;) can be transferred to a large, pre-trained image ViT (&lt;strong>;bottom network&lt;/strong>;), and further fine-tuned. This requires fewer training steps to achieve strong performance with the large model. This is beneficial as large video models might be prohibitively expensive to train from scratch.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our sparse tube ViT approach using &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;>;Kinetics-400&lt;/a>; (shown below), Kinetics-600 and Kinetics-700 datasets and compare its performance to a long list of prior methods. We find that our approach outperforms all prior methods. Importantly, it outperforms all state-of-the-art methods trained jointly on image+video datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBVbXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBVbXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance compared to several prior works on the popular Kinetics-400 video dataset. Our sparse tube ViT outperforms state-of-the-art methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore, we test our sparse tube ViT model on the &lt;a href=&quot;https://developer.qualcomm.com/software/ai-datasets/something-something&quot;>;Something-Something V2&lt;/a>; dataset, which is commonly used to evaluate more dynamic activities, and also report that it outperforms all prior state-of-the-art approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the Something-Something V2 video dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualizing some learned kernels&lt;/h2>; &lt;p>; It is interesting to understand what kind of rudimentary features are being learned by the proposed model. We visualize them below, showing both the 2D patches, which are shared for both images and videos, and video tubes. These visualizations show the 2D or 3D information being captured by the projection layer. For example, in the 2D patches, various common features, like edges and colors, are detected, while the 3D tubes capture basic shapes and how they may change over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s549/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;285&quot; data-original-width=&quot;549&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualizations of patches and tubes learned the sparse tube ViT model. Top row are the 2D patches and the remaining two rows are snapshots from the learned video tubes. The tubes show each patch for the 8 or 4 frames to which they are applied.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusions&lt;/h2>; &lt;p>; We have presented a new sparse tube ViT, which can turn a ViT encoder into an efficient video model, and can seamlessly work with both image and video inputs. We also showed that large video encoders can be bootstrapped from small video encoders and image-only ViTs. Our approach outperforms prior methods across several popular video understanding benchmarks. We believe that this simple representation can facilitate much more efficient learning with input videos, seamlessly incorporate either image or video inputs and effectively eliminate the bifurcation of image and video models for future multimodal understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is conducted by AJ Piergiovanni, Weicheng Kuo and Anelia Angelova, who are now at Google DeepMind. We thank Abhijit Ogale, Luowei Zhou, Claire Cui and our colleagues in Google Research for their helpful discussions, comments, and support.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/234098392689663352/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot; rel=&quot;alternate&quot; title=&quot;Sparse video tubes for joint video and image vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s72-c/TubeViT%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2749680625311121514&lt;/id>;&lt;published>;2023-05-18T10:12:00.006-07:00&lt;/published>;&lt;updated>;2023-05-18T16:14:50.453-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: PAIR&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Lucas Dixon and Michael Terry, co-leads, PAIR, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; PAIR (People + AI Research) first &lt;a href=&quot;https://blog.google/technology/ai/pair-people-ai-research-initiative/&quot;>;launched&lt;/a>; in 2017 with the belief that “AI can go much further — and be more useful to all of us — if we build systems with people in mind at the start of the process.” We continue to focus on making AI more understandable, interpretable, fun, and usable by more people around the world. It&#39;s a mission that is particularly timely given the emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chatbot&quot;>;chatbots&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, PAIR is part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team within Google Research, and our work spans this larger research space: We advance &lt;a href=&quot;https://pair.withgoogle.com/research/&quot;>;foundational research&lt;/a>; on human-AI interaction (HAI) and machine learning (ML); we publish educational materials, including the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;PAIR Guidebook&lt;/a>; and &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;Explorables&lt;/a>; (such as the recent Explorable looking at &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;); and we develop software tools like the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; to help people understand and debug ML behaviors. Our inspiration this year is &quot;changing the way people think about what &lt;em>;THEY&lt;/em>; can do with AI.” This vision is inspired by the rapid emergence of generative AI technologies, such as large language models (LLMs) that power chatbots like &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>;, and new generative media models like Google&#39;s &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;, and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;. In this blog post, we review recent PAIR work that is changing the way we engage with AI. &lt;/p>; &lt;br />; &lt;h2>;Generative AI research&lt;/h2>; &lt;p>; Generative AI is creating a lot of excitement, and PAIR is involved in a range of related research, from &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;>;using language models to create generative agents&lt;/a>;&amp;nbsp;to studying how artists adopted generative image models like &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; and &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;. These latter &quot;text-to-image&quot; models let a person input a text-based description of an image for the model to generate (eg, &quot;a gingerbread house in a forest in a cartoony style&quot;). In a forthcoming paper titled “&lt;a href=&quot;https://arxiv.org/abs/2303.12253&quot;>;The Prompt Artists&lt;/a>;” (to appear in &lt;a href=&quot;https://cc.acm.org/2023/&quot;>;Creativity and Cognition 2023&lt;/a>;), we found that users of generative image models strive not only to create beautiful images, but also to create unique, innovative styles. To help achieve these styles, some would even seek unique vocabulary to help develop their visual style. For example, they may visit architectural blogs to learn what domain-specific vocabulary they can adopt to help produce distinctive images of buildings. &lt;/p>; &lt;p>; We are also researching solutions to challenges faced by prompt creators who, with generative AI, are essentially programming without using a programming language. As an example, we developed &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;new methods&lt;/a>; for extracting semantically meaningful structure from natural language prompts. We have applied these structures to prompt editors to provide features similar to those found in other programming environments, such as semantic highlighting, autosuggest, and structured data views. &lt;/p>; &lt;p>; The growth of generative LLMs has also opened up new techniques to solve important long-standing problems. &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;Agile classifiers&lt;/a>; are one approach we&#39;re taking to leverage the semantic and syntactic strengths of LLMs to solve classification problems related to safer online discourse, such as nimbly blocking newer types of toxic language as quickly as it may evolve online. The big advance here is the ability to develop high quality classifiers from very small datasets — as small as 80 examples. This suggests a positive future for online discourse and better moderation of it: instead of collecting millions of examples to attempt to create universal safety classifiers for all use cases over months or years, more agile classifiers might be created by individuals or small organizations and tailored for their specific use cases, and iterated on and adapted in the time-span of a day (eg, to block a new kind of harassment being received or to correct unintended biases in models). As an example of their utility, these methods recently &lt;a href=&quot;https://www.aclweb.org/portal/content/semeval-2023-task-10-explainable-detection-online-sexism-edos&quot;>;won a SemEval competition&lt;/a>; to identify and explain sexism. &lt;/p>; &lt;p>; We&#39;ve also developed &lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>;new state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors and misbehaviours. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we also found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements on model accuracy. &lt;/p>; &lt;p>; Collectively, these methods are critical to help the scientific community improve generative models. They provide techniques for fast and effective content moderation and dialogue safety methods that help support creators whose content is the basis for generative models&#39; amazing outcomes. In addition, they provide direct tools to help debug model misbehavior which leads to better generation. &lt;/p>; &lt;br />; &lt;h2>;Visualization and education&lt;/h2>; &lt;p>; To lower barriers in understanding ML-related work, we regularly design and publish highly visual, interactive online essays, called &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, that provide accessible, hands-on ways to learn about key ideas in ML. For example, we recently published new AI Explorables on the topics of model confidence and unintended biases. In our latest Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” we discuss the problem with model confidence: models can sometimes be &lt;em>;very&lt;/em>; confident in their predictions… and yet completely incorrect. Why does this happen and what can be done about it? Our Explorable walks through these issues with interactive examples and shows how we can build models that have more appropriate confidence in their predictions by using a technique called &lt;a href=&quot;https://ai.googleblog.com/2021/11/model-ensembles-are-faster-than-you.html&quot;>;ensembling&lt;/a>;, which works by averaging the outputs of multiple models. Another Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/saliency/&quot;>;Searching for Unintended Biases with Saliency&lt;/a>;”, shows how spurious correlations can lead to unintended biases — and how techniques such as saliency maps can detect some biases in datasets, with the caveat that it can be difficult to see bias when it&#39;s more subtle and sporadic in a training set. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;543&quot; data-original-width=&quot;724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR designs and publishes AI Explorables, interactive essays on timely topics and new methods in ML research, such as “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” which looks at how and why models offer incorrect predictions with high confidence, and how “ensembling” the outputs of many models can help avoid this.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Transparency and the Data Cards Playbook&lt;/h2>; &lt;p>; Continuing to advance our goal of helping people to understand ML, we promote transparent documentation. In the past, PAIR and Google Cloud developed &lt;a href=&quot;http://modelcards.withgoogle.com&quot;>;model cards&lt;/a>;. Most recently, we presented &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3531146.3533231&quot;>;our work on Data Cards&lt;/a>; at &lt;a href=&quot;https://facctconference.org/2022/&quot;>;ACM FAccT&#39;22&lt;/a>; and open-sourced the &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;, a joint effort with the &lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;Technology, AI, Society, and Culture team&lt;/a>; (TASC). &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;The Data Cards Playbook&lt;/a>; is a toolkit of participatory activities and frameworks to help teams and organizations overcome obstacles when setting up a transparency effort. It was created using an iterative, multidisciplinary approach rooted in the experiences of over 20 teams at Google, and comes with four modules: Ask, Inspect, Answer and Audit. These modules contain a variety of resources that can help you customize Data Cards to your organization&#39;s needs: &lt;/p>; &lt;ul>; &lt;li>;18 Foundations: Scalable frameworks that anyone can use on any dataset type &lt;/li>;&lt;li>;19 Transparency Patterns: Evidence-based guidance to produce high-quality Data Cards at scale &lt;/li>;&lt;li>;33 Participatory Activities: Cross-functional workshops to navigate transparency challenges for teams &lt;/li>;&lt;li>;Interactive Lab: Generate interactive Data Cards from markdown in the browser &lt;/li>; &lt;/ul>; &lt;p>; The Data Cards Playbook is accessible as a learning pathway for startups, universities, and other research groups. &lt;/p>; &lt;br />; &lt;h2>;Software Tools&lt;/h2>; &lt;p>; Our team thrives on creating tools, toolkits, libraries, and visualizations that expand access and improve understanding of ML models. One such resource is &lt;a href=&quot;https://knowyourdata.withgoogle.com/&quot;>;Know Your Data&lt;/a>;, which allows researchers to test a model&#39;s performance for various scenarios through interactive qualitative exploration of datasets that they can use to find and fix unintended dataset biases. &lt;/p>; &lt;p>; Recently, PAIR released a new version of the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT) for model debugging and understanding. LIT v0.5 provides support for image and tabular data, new interpreters for tabular feature attribution, a &quot;Dive&quot; visualization for faceted data exploration, and performance improvements that allow LIT to scale to 100k dataset entries. You can find the &lt;a href=&quot;https://github.com/PAIR-code/lit/blob/main/RELEASE.md&quot;>;release notes&lt;/a>; and &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;code&lt;/a>; on GitHub. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s1920/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR&#39;s &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT), an open-source platform for visualization and understanding of ML models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; PAIR has also contributed to &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a tool for rapid prototyping with LLMs using prompt programming. MakerSuite builds on our earlier research on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;PromptMaker&lt;/a>;, which won an honorable mention at &lt;a href=&quot;https://chi2022.acm.org/&quot;>;CHI 2022. &lt;/a>;MakerSuite lowers the barrier to prototyping ML applications by broadening the types of people who can author these prototypes and by shortening the time spent prototyping models from months to minutes.&amp;nbsp;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s1216/reversedictionary.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;918&quot; data-original-width=&quot;1216&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s16000/reversedictionary.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A screenshot of MakerSuite, a tool for rapidly prototyping new ML models using prompt-based programming, which grew out of PAIR&#39;s prompt programming research.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Ongoing work&lt;/h2>; &lt;p>; As the world of AI moves quickly ahead, PAIR is excited to continue to develop new tools, research, and educational materials to help change the way people think about what THEY can do with AI. &lt;/p>; &lt;p>; For example, we recently conducted &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;an exploratory study&lt;/a>; with five designers (presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI&lt;/a>; this year) that looks at how people with no ML programming experience or training can use prompt programming to quickly prototype functional user interface mock-ups. This prototyping speed can help inform designers on how to integrate ML models into products, and enables them to conduct user research sooner in the product design process. &lt;/p>; &lt;p>; Based on this study, PAIR&#39;s researchers built &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser&lt;/a>;, a design tool plugin for authoring LLM-infused mock-ups. The plug-in introduces two novel LLM-interactions: input-output, which makes content interactive and dynamic, and frame-change, which directs users to different frames depending on their natural language input. The result is more tightly integrated UI and ML prototyping, all within a single interface. &lt;/p>; &lt;p>; Recent advances in AI represent a significant shift in how easy it is for researchers to customize and control models for their research objectives and goals.These capabilities are transforming the way we think about interacting with AI, and they create lots of new opportunities for the research community. PAIR is excited about how we can leverage these capabilities to make AI easier to use for more people. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Thanks to everyone in PAIR, to Reena Jana and to all of our collaborators. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2749680625311121514/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: PAIR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2478354033845809100&lt;/id>;&lt;published>;2023-05-16T12:22:00.001-07:00&lt;/published>;&lt;updated>;2023-05-16T12:23:42.794-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using reinforcement learning for dynamic planning in open-ended conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Deborah Cohen, Staff Research Scientist, and Craig Boutilier, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s1650/rlxtalk.png&quot; style=&quot;display: none;&quot; />; &lt;p>; As virtual assistants become ubiquitous, users increasingly interact with them to learn about new topics or obtain recommendations and expect them to deliver capabilities beyond narrow dialogues of one or two turns. Dynamic planning, namely the capability to look ahead and replan based on the flow of the conversation, is an essential ingredient for the making of engaging conversations with the deeper, open-ended interactions that users expect. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While large language models (LLMs) are now beating state-of-the-art approaches in many natural language processing benchmarks, they are typically trained to output the next best response, rather than planning ahead, which is required for multi-turn interactions. However, in the past few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;>;reinforcement learning&lt;/a>; (RL) has delivered incredible results addressing specific problems that involve dynamic planning, such as winning games and protein folding. &lt;/p>; &lt;p>; Today, we are sharing our recent advances in &lt;a href=&quot;https://arxiv.org/abs/2208.02294&quot;>;dynamic planning for human-to-assistant conversations&lt;/a>;, in which we enable an assistant to plan a multi-turn conversation towards a goal and adapt that plan in real-time by adopting an RL-based approach. Here we look at how to improve long interactions by applying RL to compose answers based on information extracted from reputable sources, rather than relying on content generated by a language model. We expect that future versions of this work could combine LLMs and RL in multi-turn dialogues. The deployment of RL “in the wild” in a large-scale dialogue system proved a formidable challenge due to the modeling complexity, tremendously large state and action spaces, and significant subtlety in designing reward functions. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What is dynamic planning?&lt;/h2>; &lt;p>; Many types of conversations, from gathering information to offering recommendations, require a flexible approach and the ability to modify the original plan for the conversation based on its flow. This ability to shift gears in the middle of a conversation is known as &lt;em>;dynamic planning&lt;/em>;, as opposed to &lt;em>;static planning&lt;/em>;, which refers to a more fixed approach. In the conversation below, for example, the goal is to engage the user by sharing interesting facts about cool animals. To begin, the assistant steers the conversation to sharks via a sound quiz. Given the user&#39;s lack of interest in sharks, the assistant then develops an updated plan and pivots the conversation to sea lions, lions, and then cheetahs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;908&quot; data-original-width=&quot;890&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The assistant dynamically modifies its original plan to talk about sharks and shares facts about other animals.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic composition&lt;/h2>; &lt;p>; To cope with the challenge of conversational exploration, we separate the generation of assistant responses into two parts: 1) &lt;em>;content generation&lt;/em>;, which extracts relevant information from reputable sources,&lt;em>; &lt;/em>;and 2) &lt;em>;flexible composition&lt;/em>; of such content into assistant responses. We refer to this two-part approach as &lt;em>;&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;dynamic composition&lt;/a>;&lt;/em>;. Unlike LLM methods, this approach gives the assistant the ability to fully control the source, correctness, and quality of the content that it may offer. At the same time, it can achieve flexibility via a learned dialogue manager that selects and combines the most appropriate content. &lt;/p>; &lt;p>; In an earlier paper, “&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;Dynamic Composition for Conversational Domain Exploration&lt;/a>;”, we describe a novel approach which consists of: (1) a collection of content providers, which offer candidates from different sources, such as news snippets, &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; facts, and questions; (2) a dialogue manager; and (3) a sentence fusion module. Each assistant response is incrementally constructed by the dialogue manager, which selects candidates proposed by the content providers. The selected sequence of utterances is then fused into a cohesive response. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic planning using RL&lt;/h2>; &lt;p>; At the core of the assistant response composition loop is a dialogue manager trained using &lt;em>;off-policy RL&lt;/em>;, namely an algorithm that evaluates and improves a policy that is different from the policy used by the agent (in our case, the latter is based on a supervised model). Applying RL to dialogue management presents several challenges, including a large state space (as the state represents the conversation state, which needs to account for the whole conversation history) and an effectively unbounded action space (that may include all existing words or sentences in natural language). &lt;/p>; &lt;p>; We address these challenges using a novel RL construction. First, we leverage powerful supervised models — specifically, &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;>;recurrent neural networks&lt;/a>; (RNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; — to provide a succinct and effective dialogue state representation. These state encoders are fed with the dialogue history, composed of a sequence of user and assistant turns, and output a representation of the dialogue state in the form of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_space&quot;>;latent vector&lt;/a>;. &lt;/p>; &lt;p>; Second, we use the fact that a relatively small set of reasonable candidate utterances or actions can be generated by content providers at each conversation turn, and limit the action space to these. Whereas the action space is typically fixed in RL settings, because all states share the same action space, ours is a non-standard space in which the candidate actions may differ with each state, since content providers generate different actions depending on the dialogue context. This puts us in the realm of stochastic action sets, a framework that formalizes cases where the set of actions available in each state is governed by an exogenous stochastic process, which we address using &lt;a href=&quot;https://www.ijcai.org/proceedings/2018/0650.pdf&quot;>;Stochastic Action Q-Learning&lt;/a>;, a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Q-learning&quot;>;Q-learning&lt;/a>; approach. Q-learning is a popular off-policy RL algorithm, which does not require a model of the environment to evaluate and improve the policy. We trained our model on a corpus of crowd-compute–rated conversations obtained using a supervised dialogue manager. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s1500/image4.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given the current dialogue history and a new user query, content providers generate candidates from which the assistant selects one. This process runs in a loop, and at the end the selected utterances are fused into a cohesive response.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Reinforcement learning model evaluation&lt;/h2>; &lt;p>; We compared our RL dialogue manager with a launched supervised transformer model in an experiment using Google Assistant, which conversed with users about animals. A conversation starts when a user triggers the experience by asking an animal-related query (eg, “How does a lion sound?”). The experiment was conducted using an &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B testing&lt;/a>; protocol, in which a small percentage of Assistant users were randomly sampled to interact with our RL-based assistant while other users interacted with the standard assistant. &lt;/p>; &lt;p>; We found that the RL dialogue manager conducts longer, more engaging conversations. It increases conversation length by 30% while improving user engagement metrics. We see an increase of 8% in cooperative responses to the assistant&#39;s questions — eg, “Tell me about lions,” in response to “Which animal do you want to hear about next?” Although there is also a large increase in nominally “non-cooperative” responses (eg, “No,” as a reply to a question proposing additional content, such as “Do you want to hear more?”), this is expected as the RL agent takes more risks by asking pivoting questions. While a user may not be interested in the conversational direction proposed by the assistant (eg, pivoting to another animal), the user will often continue to engage in a dialogue about animals. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s552/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;534&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;From the non-cooperative user response in the 3rd turn (“No.”) and the query “Make a dog sound,” in the 5th turn, the assistant recognizes that the user is mostly interested in animal sounds and modifies its plan, providing sounds and sound quizzes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In addition, some user queries contain explicit positive (eg, “Thank you, Google,” or “I&#39;m happy.”) or negative (eg, “Shut up,” or “Stop.”) feedback. While an order of magnitude fewer than other queries, they offer a direct measure of user (dis)satisfaction. The RL model increases explicit positive feedback by 32% and reduces negative feedback by 18%. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Learned dynamic planning characteristics and strategies&lt;/h2>; &lt;p>; We observe several characteristics of the (unseen) RL plan to improve user engagement while conducting longer conversations. First, the RL-based assistant ends 20% more turns in questions, prompting the user to choose additional content. It also better harnesses content diversity, including facts, sounds, quizzes, yes/no questions, open questions, etc. On average, the RL assistant uses 26% more distinct content providers per conversation than the supervised model. &lt;/p>; &lt;p>; Two observed RL planning strategies are related to the existence of sub-dialogues with different characteristics. Sub-dialogues about animal sounds are poorer in content and exhibit entity pivoting at every turn (ie, after playing the sound of a given animal, we can either suggest the sound of a different animal or quiz the user about other animal sounds). In contrast, sub-dialogues involving animal facts typically contain richer content and have greater conversation depth. We observe that RL favors the richer experience of the latter, selecting 31% more fact-related content. Lastly, when restricting analysis to fact-related dialogues, the RL assistant exhibits 60% more focus-pivoting turns, that is, conversational turns that change the focus of the dialogue. &lt;/p>; &lt;p>; Below, we show two example conversations, one conducted by the supervised model (left) and the second by the RL model (right), in which the first three user turns are identical. With a supervised dialogue manager, after the user declined to hear about “today&#39;s animal”, the assistant pivots back to animal sounds to maximize the immediate user satisfaction. While the conversation conducted by the RL model begins identically, it exhibits a different planning strategy to optimize the overall user engagement, introducing more diverse content, such as fun facts. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s1539/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1539&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the left conversation, conducted by the supervised model, the assistant maximizes the immediate user satisfaction. The right conversation, conducted by the RL model, shows different planning strategies to optimize the overall user engagement.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future research and challenges&lt;/h2>; &lt;p>; In the past few years, LLMs trained for language understanding and generation have demonstrated impressive results across multiple tasks, including dialogue. We are now exploring the use of an RL framework to empower LLMs with the capability of dynamic planning so that they can dynamically plan ahead and delight users with a more engaging experience. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias, Idan Szpektor and Gal Elidan. We would like to thank: Roee Aharoni, Moran Ambar, John Anderson, Ido Cohn, Mohammad Ghavamzadeh, Lotem Golany, Ziv Hodak, Adva Levin, Fernando Pereira, Shimi Salant, Shachar Shimoni, Ronit Slyper, Ariel Stolovich, Hagai Taitelbaum, Noam Velan, Avital Zipori and the CrowdCompute team led by Ashwin Kakarla. We thank Sophie Allweis for her feedback on this blogpost and Tom Small for the visualization.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2478354033845809100/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Using reinforcement learning for dynamic planning in open-ended conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s72-c/rlxtalk.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-14165165832846745&lt;/id>;&lt;published>;2023-05-15T13:59:00.001-07:00&lt;/published>;&lt;updated>;2023-05-15T14:40:42.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Larger language models do in-context learning differently&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s800/SULICL.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There have recently been tremendous advances in language models, partly because they can perform tasks with strong performance via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>; (ICL), a process whereby models are prompted with a few examples of input-label pairs before performing the task on an unseen evaluation example. In general, models&#39; success at in-context learning is enabled by: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;ul>; &lt;li>;Their use of semantic prior knowledge from pre-training to predict labels while following the format of in-context examples (eg, seeing examples of movie reviews with “positive sentiment” and “negative sentiment” as labels and performing &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>; using prior knowledge). &lt;/li>;&lt;li>;Learning the input-label mappings in context from the presented examples (eg, finding a pattern that positive reviews should be mapped to one label, and negative reviews should be mapped to a different label). &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.03846&quot;>;Larger language models do in-context learning differently&lt;/a>;”, we aim to learn about how these two factors (semantic priors and input-label mappings) interact with each other in ICL settings, especially with respect to the scale of the language model that&#39;s used. We investigate two settings to study these two factors — ICL with flipped labels (flipped-label ICL) and ICL with semantically-unrelated labels (SUL-ICL). In flipped-label ICL, labels of in-context examples are flipped so that semantic priors and input-label mappings disagree with each other. In SUL-ICL, labels of in-context examples are replaced with words that are semantically unrelated to the task presented in-context. We found that overriding prior knowledge is an emergent ability of model scale, as is the ability to learn in-context with semantically-unrelated labels. We also found that instruction tuning strengthens the use of prior knowledge more than it increases the capacity to learn input-label mappings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s625/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;625&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of flipped-label ICL and semantically-unrelated label ICL (SUL-ICL), compared with regular ICL, for a sentiment analysis task. Flipped-label ICL uses flipped labels, forcing the model to override semantic priors in order to follow the in-context examples. SUL-ICL uses labels that are not semantically related to the task, which means that models must learn input-label mappings in order to perform the task because they can no longer rely on the semantics of natural language labels.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiment design&lt;/h2>; &lt;p>; For a diverse dataset mixture, we experiment on seven &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) tasks that have been widely used: &lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;sentiment analysis&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/SetFit/subj&quot;>;subjective/objective classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/trec&quot;>;question classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/glue/viewer/qqp/validation&quot;>;duplicated-question recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/super_glue/viewer/rte/test&quot;>;entailment recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/financial_phrasebank&quot;>;financial sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://huggingface.co/datasets/ethos&quot;>;hate speech detection&lt;/a>;. We test five language model families, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;>;InstructGPT&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In this experiment, labels of in-context examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override their priors. In this setting, models that are able to override prior knowledge and learn input-label mappings in-context should experience a decrease in performance (since ground-truth evaluation labels are not flipped). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s1036/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1036&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The ability to override semantic priors when presented with flipped in-context example labels emerges with model scale. Smaller models cannot flip predictions to follow flipped labels (performance only decreases slightly), while larger models can do so (performance decreases to well below 50%).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that when no labels are flipped, larger models have better performance than smaller models (as expected). But when we flip more and more labels, the performance of small models stays relatively flat, but large models experience large performance drops to well-below random guessing (eg, 90% → 22.5% for code-davinci-002). &lt;/p>; &lt;p>; These results indicate that large models can override prior knowledge from pre-training when contradicting input-label mappings are presented in-context. Small models can&#39;t do this, making this ability an emergent phenomena of model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Semantically-unrelated labels&lt;/h2>; &lt;p>; In this experiment, we replace labels with semantically-irrelevant ones (eg, for sentiment analysis, we use “foo/bar” instead of “negative/positive”), which means that the model can only perform ICL by learning from input-label mappings. If a model mostly relies on prior knowledge for ICL, then its performance should decrease after this change since it will no longer be able to use semantic meanings of labels to make predictions. A model that can learn input–label mappings in-context, on the other hand, would be able to learn these semantically-unrelated mappings and should not experience a major drop in performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s1049/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;1049&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Small models rely more on semantic priors than large models do, as indicated by the greater decrease in performance for small models than for large models when using semantically-unrelated labels (ie, targets) instead of natural language labels. For each plot, models are shown in order of increasing model size (eg, for GPT-3 models, &lt;em>;a&lt;/em>; is smaller than &lt;em>;b&lt;/em>;, which is smaller than &lt;em>;c&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Indeed, we see that using semantically-unrelated labels results in a greater performance drop for small models. This suggests that smaller models primarily rely on their semantic priors for ICL rather than learning from the presented input-label mappings. Large models, on the other hand, have the ability to learn input-label mappings in-context when the semantic nature of labels is removed. &lt;/p>; &lt;p>; We also find that including more in-context examples (ie, exemplars) results in a greater performance improvement for large models than it does for small models, indicating that large models are better at learning from in-context examples than small models are. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s825/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;417&quot; data-original-width=&quot;825&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the SUL-ICL setup, larger models benefit more from additional examples than smaller models do.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Instruction tuning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a popular technique for improving model performance, which involves tuning models on various NLP tasks that are phrased as instructions (eg, “Question: What is the sentiment of the following sentence, &#39;This movie is great.&#39; Answer: Positive”). Since the process uses natural language labels, however, an open question is whether it improves the ability to learn input-label mappings or whether it strengthens the ability to recognize and apply semantic prior knowledge. Both of these would lead to an improvement in performance on standard ICL tasks, so it&#39;s unclear which of these occur. &lt;/p>; &lt;p>; We study this question by running the same two setups as before, only this time we focus on comparing standard language models (specifically, PaLM) with their instruction-tuned variants (Flan-PaLM). &lt;/p>; &lt;p>; First, we find that Flan-PaLM is better than PaLM when we use semantically-unrelated labels. This effect is very prominent in small models, as Flan-PaLM-8B outperforms PaLM-8B by 9.6% and almost catches up to PaLM-62B. This trend suggests that instruction tuning strengthens the ability to learn input-label mappings, which isn&#39;t particularly surprising. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s573/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;301&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned language models are better at learning input–label mappings than pre-training–only language models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More interestingly, we saw that Flan-PaLM is actually worse than PaLM at following flipped labels, meaning that the instruction tuned models were unable to override their prior knowledge (Flan-PaLM models don&#39;t reach below random guessing with 100% flipped labels, but PaLM models without instruction tuning can reach 31% accuracy in the same setting). These results indicate that instruction tuning must increase the extent to which models rely on semantic priors when they&#39;re available. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s1016/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;1016&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned models are worse than pre-training–only models at learning to override semantic priors when presented with flipped labels in-context.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Combined with the previous result, we conclude that although instruction tuning improves the ability to learn input-label mappings, it strengthens the usage of semantic prior knowledge more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We examined the extent to which language models learn in-context by utilizing prior knowledge learned during pre-training versus input-label mappings presented in-context. &lt;/p>; &lt;p>; We first showed that large language models can learn to override prior knowledge when presented with enough flipped labels, and that this ability emerges with model scale. We then found that successfully doing ICL using semantically-unrelated labels is another emergent ability of model scale. Finally, we analyzed instruction-tuned language models and saw that instruction tuning improves the capacity to learn input-label mappings but also strengthens the use of semantic prior knowledge even more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; These results underscore how the ICL behavior of language models can change depending on their scale, and that larger language models have an emergent ability to map inputs to many types of labels, a form of reasoning in which input-label mappings can potentially be learned for arbitrary symbols. Future research could help provide insights on why these phenomena occur with respect to model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. We would like to thank Sewon Min and our fellow collaborators at Google Research for their advice and helpful discussions.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/14165165832846745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Larger language models do in-context learning differently&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s72-c/SULICL.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4751112643381771806&lt;/id>;&lt;published>;2023-05-15T10:16:00.003-07:00&lt;/published>;&lt;updated>;2023-05-15T10:24:09.250-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Consensus and subjectivity of skin tone annotation for ML fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Candice Schumann, Software Engineer, and Gbolahan O. Olanubi, User Experience Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s888/MST-E.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Skin tone is an observable characteristic that is subjective, perceived differently by individuals (eg, depending on their location or culture) and thus is complicated to annotate. That said, the ability to reliably and accurately annotate skin tone is highly important in computer vision. This became apparent in 2018, when the &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; study highlighted that computer vision systems struggled to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. The study highlights the importance for computer researchers and practitioners to evaluate their technologies across the full range of skin tones and at intersections of identities. Beyond evaluating model performance on skin tone, skin tone annotations enable researchers to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3375627.3375832&quot;>;measure diversity&lt;/a>; and representation in &lt;a href=&quot;https://arxiv.org/abs/1901.10265&quot;>;image retrieval systems&lt;/a>;, &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;dataset collection&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2202.04053&quot;>;image generation&lt;/a>;. For all of these applications, a collection of meaningful and inclusive skin tone annotations is key. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s1196/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Monk Skin Tone (MST) Scale See more at &lt;a href=&quot;http://skintone.google&quot;>;skintone.google&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Last year, in a step toward more inclusive computer vision systems, Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team in Research partnered with &lt;a href=&quot;https://sociology.fas.harvard.edu/people/ellis-monk&quot;>;Dr. Ellis Monk&lt;/a>; to openly release the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone&lt;/a>; (MST) Scale, a skin tone scale that captures a broad spectrum of skin tones. In comparison to an industry standard scale like the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK481857/table/chapter6.t1/&quot;>;Fitzpatrick Skin-Type Scale&lt;/a>; designed for dermatological use, the MST offers a more inclusive representation across the range of skin tones and was designed for a broad range of applications, including computer vision. &lt;/p>; &lt;p>; Today we&#39;re announcing the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples&lt;/a>; (MST-E) dataset to help practitioners understand the MST scale and train their human annotators. This dataset has been made publicly available to enable practitioners everywhere to create more consistent, inclusive, and meaningful skin tone annotations. Along with this dataset, we&#39;re providing a set of recommendations, noted below, around the MST scale and MST-E dataset so we can all create products that work well for all skin tones. &lt;/p>; &lt;p>; Since we launched the MST, we&#39;ve been using it to improve Google&#39;s computer vision systems to make &lt;a href=&quot;https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/&quot;>;equitable image tools for everyone&lt;/a>; and to &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;improve representation of skin tone in Search&lt;/a>;. Computer vision researchers and practitioners outside of Google, like the curators of &lt;a href=&quot;https://ai.facebook.com/blog/casual-conversations-v2-dataset-measure-fairness/&quot;>;MetaAI&#39;s Casual Conversations&lt;/a>; dataset, are recognizing the value of MST annotations to provide additional insight into diversity and representation in datasets. Incorporation into widely available datasets like these are essential to give everyone the ability to ensure they are building more inclusive computer vision technologies and can test the quality of their systems and products across a wide range of skin tones. &lt;/p>; &lt;p>; Our team has continued to conduct research to understand how we can continue to advance our understanding of skin tone in computer vision. One of our core areas of focus has been skin tone annotation, the process by which human annotators are asked to review images of people and select the best representation of their skin tone. MST annotations enable a better understanding of the inclusiveness and representativeness of datasets across a wide range of skin tones, thus enabling researchers and practitioners to evaluate quality and fairness of their datasets and models. To better understand the effectiveness of MST annotations, we&#39;ve asked ourselves the following questions: &lt;/p>; &lt;ul>; &lt;li>;How do people think about skin tone across geographic locations? &lt;/li>;&lt;li>;What does global consensus of skin tone look like? &lt;/li>;&lt;li>;How do we effectively annotate skin tone for use in inclusive machine learning (ML)? &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;The MST-E dataset&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>; contains 1,515 images and 31 videos of 19 subjects spanning the 10 point MST scale, where the subjects and images were sourced through &lt;a href=&quot;https://tonl.co/&quot;>;TONL&lt;/a>;, a stock photography company focusing on diversity. The 19 subjects include individuals of different ethnicities and gender identities to help human annotators decouple the concept of skin tone from race. The primary goal of this dataset is to enable practitioners to train their human annotators and test for consistent skin tone annotations across various environment capture conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/s1072/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;950&quot; data-original-width=&quot;1072&quot; height=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/w640-h568/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MST-E image set contains 1,515 images and 31 videos featuring 19 models taken under various lighting conditions and facial expressions. Images by TONL. Copyright TONL.CO 2022 ALL RIGHTS RESERVED. Used with permission.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; All images of a subject were collected in a single day to reduce variation of skin tone due to seasonal or other temporal effects. Each subject was photographed in various poses, facial expressions, and lighting conditions. In addition, Dr. Monk annotated each subject with a skin tone label and then selected a “golden” image for each subject that best represents their skin tone. In our research we compare annotations made by human annotators to those made by Dr. Monk, an academic expert in social perception and inequality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Terms of use&lt;/h3>; &lt;p>; Each model selected as a subject provided consent for their images and videos to be released. TONL has given permission for these images to be released as part of MST-E and used for research or human-annotator-training purposes only. The images are not to be used to train ML models. &lt;/p>; &lt;br />; &lt;h2>;Challenges with forming consensus of MST annotations&lt;/h2>; &lt;p>; Although skin tone is easy for a person to see, it can be challenging to systematically annotate across multiple people due to issues with technology and the complexity of human social perception. &lt;/p>; &lt;p>; On the technical side, things like the pixelation, lighting conditions of an image, or a person&#39;s monitor settings can affect how skin tone appears on a screen. You might notice this yourself the next time you change the display setting while watching a show. The hue, saturation, and brightness could all affect how skin tone is displayed on a monitor. Despite these challenges, we find that human annotators are able to learn to become invariant to lighting conditions of an image when annotating skin tone. &lt;/p>; &lt;p>; On the social perception side, aspects of a person&#39;s life like their location, culture, and lived experience may affect how they annotate various skin tones. We found some evidence for this when we asked photographers in the United States and photographers in India to annotate the same image. The photographers in the United States viewed this person as somewhere between MST-5 &amp;amp; MST-7. However, the photographers in India viewed this person as somewhere between MST-3 &amp;amp; MST-5. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s1594/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;1594&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The distribution of Monk Skin Tone Scale annotations for this image from a sample of 5 photographers in the US and 5 photographers in India.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Continuing this exploration, we asked trained annotators from five different geographical regions (India, Philippines, Brazil, Hungary, and Ghana) to annotate skin tone on the MST scale. Within each market each image had 5 annotators who were drawn from a broader pool of annotators in that region. For example, we could have 20 annotators in a market, and select 5 to review a particular image. &lt;/p>; &lt;p>; With these annotations we found two important details. First, annotators within a region had similar levels of agreement on a single image. Second, annotations between regions were, on average, significantly different from each other. (&lt;em>;p&amp;lt;0.05&lt;/em>;). This suggests that people from the same geographic region may have a similar mental model of skin tone, but this mental model is not universal. &lt;/p>; &lt;p>; However, even with these regional differences, we also find that the consensus between all five regions falls close to the MST values supplied by Dr. Monk. This suggests that a geographically diverse group of annotators can get close to the MST value annotated by an MST expert. In addition, after training, we find no significant difference between annotations on well-lit images, versus poorly-lit images, suggesting that annotators can become invariant to different lighting conditions in an image — a non-trivial task for ML models. &lt;/p>; &lt;p>; The MST-E dataset allows researchers to study annotator behavior across curated subsets controlling for potential confounders. We observed similar regional variation when annotating much larger datasets with many more subjects. &lt;/p>; &lt;br />; &lt;h2>;Skin Tone annotation recommendations&lt;/h2>; &lt;p>; Our research includes four major findings. First, annotators within a similar geographical region have a consistent and shared mental model of skin tone. Second, these mental models differ across different geographical regions. Third, the MST annotation consensus from a geographically diverse set of annotators aligns with the annotations provided by an expert in social perception and inequality. And fourth, annotators can learn to become invariant to lighting conditions when annotating MST. &lt;/p>; &lt;p>; Given our research findings, there are a few recommendations for skin tone annotation when using the MST. &lt;/p>; &lt;ol>; &lt;li>;Having a geographically diverse set of annotators is important to gain accurate, or close to ground truth, estimates of skin tone. &lt;/li>;&lt;li>;Train human annotators using the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>;, which spans the entire MST spectrum and contains images in a variety of lighting conditions. This will help annotators become invariant to lighting conditions and appreciate the nuance and differences between the MST points. &lt;/li>;&lt;li>;Given the wide range of annotations we suggest having at least two annotators in at least five different geographical regions (10 ratings per image). &lt;/li>; &lt;/ol>; &lt;p>; Skin tone annotation, like other subjective annotation tasks, is difficult but possible. These types of annotations allow for a more nuanced understanding of model performance, and ultimately help us all to create products that work well for every person across the broad and diverse spectrum of skin tones. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;em>;We wish to thank our colleagues across Google working on fairness and inclusion in computer vision for their contributions to this work, especially Marco Andreetto, Parker Barnes, Ken Burke, Benoit Corda, Tulsee Doshi, Courtney Heldreth, Rachel Hornung, David Madras, Ellis Monk, Shrikanth Narayanan, Utsav Prabhu, Susanna Ricco, Sagar Savla, Alex Siegman, Komal Singh, Biao Wang, and Auriel Wright. We also would like to thank Annie Jean-Baptiste, Florian Koenigsberger, Marc Repnyek, Maura O&#39;Brien, and Dominique Mungin and the rest of the team who help supervise, fund, and coordinate our data collection.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4751112643381771806/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot; rel=&quot;alternate&quot; title=&quot;Consensus and subjectivity of skin tone annotation for ML fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s72-c/MST-E.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6793814472941708824&lt;/id>;&lt;published>;2023-05-12T13:56:00.000-07:00&lt;/published>;&lt;updated>;2023-05-12T13:56:21.572-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;F-VLM: Open-vocabulary object detection upon frozen vision and language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Weicheng Kuo and Anelia Angelova, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s320/F-VLM%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_detection&quot;>;Detection&lt;/a>; is a fundamental vision task that aims to localize and recognize objects in an image. However, the data collection process of manually annotating bounding boxes or instance masks is tedious and costly, which limits the modern detection vocabulary size to roughly 1,000 object classes. This is orders of magnitude smaller than the vocabulary people use to describe the visual world and leaves out many categories. Recent vision and language models (VLMs), such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>;, have demonstrated improved open-vocabulary visual recognition capabilities through learning from Internet-scale image-text pairs. These VLMs are applied to zero-shot classification using frozen model weights without the need for fine-tuning, which stands in stark contrast to the existing paradigms used for retraining or fine-tuning VLMs for &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection tasks&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Intuitively, to align the image content with the text description during training, VLMs may learn region-sensitive and discriminative features that are transferable to object detection. Surprisingly, features of a frozen VLM contain rich information that are both region sensitive for describing object shapes (second column below) and discriminative for region classification (third column below). In fact, feature grouping can nicely delineate object boundaries without any supervision. This motivates us to explore the use of frozen VLMs for open-vocabulary object detection with the goal to expand detection beyond the limited set of annotated categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s1999/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1238&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We explore the potential of frozen vision and language features for open-vocabulary detection. The &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;>;K-Means&lt;/a>; feature grouping reveals rich semantic and region-sensitive information where object boundaries are nicely delineated (column 2). The same frozen features can classify groundtruth (GT) regions well without fine-tuning (column 3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2209.15639&quot;>;F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;ICLR 2023&lt;/a>;, we introduce a simple and scalable open-vocabulary detection approach built upon frozen VLMs. F-VLM reduces the training complexity of an open-vocabulary detector to below that of a standard detector, obviating the need for &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;knowledge distillation&lt;/a>;, detection-tailored pre-training, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_supervision&quot;>;weakly supervised learning&lt;/a>;. We demonstrate that by preserving the knowledge of pre-trained VLMs completely, F-VLM maintains a similar philosophy to &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;>;ViTDet&lt;/a>; and decouples detector-specific learning from the more task-agnostic vision knowledge in the detector backbone. We are also releasing the F-VLM &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm&quot;>;code&lt;/a>; along with a demo on our &lt;a href=&quot;https://sites.google.com/corp/view/f-vlm/home&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning upon frozen vision and language models&lt;/h2>; &lt;p>; We desire to retain the knowledge of pretrained VLMs as much as possible with a view to minimize effort and cost needed to adapt them for open-vocabulary detection. We use a frozen VLM image encoder as the detector backbone and a text encoder for caching the detection text embeddings of offline dataset vocabulary. We take this VLM backbone and attach a &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;detector head&lt;/a>;, which predicts object regions for localization and outputs detection scores that indicate the probability of a detected box being of a certain category. The detection scores are the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; of region features (a set of bounding boxes that the detector head outputs) and category text embeddings. The category text embeddings are obtained by feeding the category names through the text model of pretrained VLM (which has both image and text models)r. &lt;/p>; &lt;p>; The VLM image encoder consists of two parts: 1) a &lt;a href=&quot;https://github.com/openai/CLIP/blob/main/clip/model.py#L139-L151&quot;>;feature extractor&lt;/a>; and 2) a feature &lt;a href=&quot;https://github.com/openai/CLIP/blob/a9b1bf5920416aaeaec965c25dd9e8f98c864f16/clip/model.py#LL152C8-L152C8&quot;>;pooling layer&lt;/a>;. We adopt the feature extractor for detector head training, which is the only step we train (on standard detection &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;data&lt;/a>;), to allow us to directly use frozen weights, inheriting rich semantic knowledge (eg, long-tailed categories like martini, fedora hat, pennant) from the VLM backbone. The detection losses include &lt;a href=&quot;https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/&quot;>;box regression&lt;/a>; and classification losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At training time, F-VLM is simply a detector with the last classification layer replaced by base-category text embeddings.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Region-level open-vocabulary recognition&lt;/h2>; &lt;p>; The ability to perform open-vocabulary recognition at region level (ie, bounding box level as opposed to image level) is integral to F-VLM. Since the backbone features are frozen, they do not overfit to the training categories (eg, donut, zebra) and can be directly cropped for region-level classification. F-VLM performs this open-vocabulary classification only at test time. To obtain the VLM features for a region, we apply the feature pooling layer on the cropped backbone output features. Because the pooling layer requires fixed-size inputs, eg, 7x7 for &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet50&lt;/a>; (R50) &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; backbone, we crop and resize the region features with the &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;ROI-Align layer&lt;/a>; (shown below). Unlike existing open-vocabulary detection &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approaches&lt;/a>;, we do not crop and resize the RGB image regions and cache their embeddings in a separate offline process, but train the detector head in one stage. This is simpler and makes more efficient use of disk storage space.. In addition, we do not crop VLM region features during training because the backbone features are frozen. &lt;/p>; &lt;p>; Despite never being trained on regions, the cropped region features maintain good open-vocabulary recognition capability. However, we observe the cropped region features are not sensitive enough to the localization quality of the regions, ie, a loosely vs. tightly localized box both have similar features. This may be good for classification, but is problematic for detection because we need the detection scores to reflect localization quality as well. To remedy this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geometric_mean&quot;>;geometric mean&lt;/a>; to combine the VLM scores with the detection scores for each region and category. The VLM scores indicate the probability of a detection box being of a certain category according to the pretrained VLM. The detection scores indicate the class probability distribution of each box based on the similarity of region features and input text embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At test time, F-VLM uses the region proposals to crop out the top-level features of the VLM backbone and compute the VLM score per region. The trained detector head provides the detection boxes and masks, while the final detection scores are a combination of detection and VLM scores.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We apply F-VLM to the popular &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, the best F-VLM achieves 32.8 &lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>; (AP) on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;APr&lt;/a>;), which outperforms the state of the art by 6.5 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask APr&lt;/a>; and many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision. F-VLM shows strong scaling property with frozen model capacity, while the number of trainable parameters is fixed. Moreover, F-VLM generalizes and scales well in the transfer detection tasks (eg, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets) by simply replacing the vocabularies without fine-tuning the model. We test the LVIS-trained models on the popular &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; datasets and demonstrate that the model can work very well without training on in-domain detection data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM outperforms &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;the state of the art&lt;/a>; (SOTA) on LVIS open-vocabulary detection benchmark and transfer object detection. On the x-axis, we show the LVIS metric mask AP on rare categories (APr), and the Objects365 (O365) metric box AP on all categories. The sizes of the detector backbones are as follows: Small(R50), Base (R50x4), Large(R50x16), Huge(R50x64). The naming follows CLIP convention.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We visualize F-VLM on open-vocabulary detection and transfer detection tasks (shown below). On LVIS and Objects365, F-VLM correctly detects both novel and common objects. A key benefit of open-vocabulary detection is to test on out-of-distribution data with categories given by users on the fly. See the F-VLM paper for more visualization on &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>;, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1148&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM open-vocabulary and transfer detections. &lt;strong>;Top:&lt;/strong>; Open-vocabulary detection on LVIS. We only show the novel categories for clarity. &lt;strong>;Bottom:&lt;/strong>; Transfer to &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; dataset shows accurate detection of many categories. Novel categories detected: fedora, martini, pennant, football helmet (LVIS); slide (Objects365).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training efficiency&lt;/h2>; &lt;p>; We show that F-VLM can achieve top performance with much less computational resources in the table below. Compared to the state-of-the-art &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approach&lt;/a>;, F-VLM can achieve better performance with 226x fewer resources and 57x faster wall clock time. Apart from training resource savings, F-VLM has potential for substantial memory savings at training time by running the backbone in inference mode. The F-VLM system runs almost as fast as a standard detector at inference time, because the only addition is a single attention pooling layer on the detected region features. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost &lt;br />;(per-core-hour)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost Savings&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;SOTA&lt;/a>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;26.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;460 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;8,000 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;1x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;118 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;565 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;71 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;113x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;27.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;226x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We provide additional results using the shorter &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;>;Detectron2&lt;/a>; training recipes (12 and 36 epochs), and show similarly strong performance by using a frozen backbone. The default setting is marked in gray. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Backbone&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;&lt;a href=&quot;https://arxiv.org/abs/2012.07177&quot;>;Large Scale Jitter&lt;/a>;&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;#Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Batch Size &lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present F-VLM – a simple open-vocabulary detection method which harnesses the power of frozen pre-trained large vision-language models to provide detection of novel objects. This is done without a need for knowledge distillation, detection-tailored pre-training, or weakly supervised learning. Our approach offers significant compute savings and obviates the need for image-level labels. F-VLM achieves the new state-of-the-art in open-vocabulary detection on the LVIS benchmark at system level, and shows very competitive transfer detection on other datasets. We hope this study can both facilitate further research in novel-object detection and help the community explore frozen VLMs for a wider range of vision tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;i>;This work is conducted by Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. We would like to thank our colleagues at Google Research for their advice and helpful discussions. &lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6793814472941708824/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot; rel=&quot;alternate&quot; title=&quot;F-VLM: Open-vocabulary object detection upon frozen vision and language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s72-c/F-VLM%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1014951953767598848&lt;/id>;&lt;published>;2023-05-12T10:03:00.003-07:00&lt;/published>;&lt;updated>;2023-05-12T10:03:43.317-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling conversational interaction on mobile with LLMs&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bryan Wang, Student Researcher, and Yang Li, Research Scientist, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s320/LLM4Mobile%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Intelligent assistants on mobile devices have significantly advanced language-based interactions for performing simple daily tasks, such as setting a timer or turning on a flashlight. Despite the progress, these assistants still face limitations in supporting conversational interactions in mobile user interfaces (UIs), where many user tasks are performed. For example, they cannot answer a user&#39;s question about specific information displayed on a screen. An agent would need to have a computational understanding of &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_user_interface&quot;>;graphical user interfaces&lt;/a>; (GUIs) to achieve such capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Prior research has investigated several important technical building blocks to enable conversational interaction with mobile UIs, including &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3472749.3474765&quot;>;summarizing a mobile screen&lt;/a>; for users to quickly understand its purpose, &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;mapping language instructions to UI actions&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.11103&quot;>;modeling GUIs &lt;/a>;so that they are more amenable for language-based interaction. However, each of these only addresses a limited aspect of conversational interaction and requires considerable effort in curating large-scale datasets and training dedicated models. Furthermore, there is a broad spectrum of conversational interactions that can occur on mobile UIs. Therefore, it is imperative to develop a lightweight and generalizable approach to realize conversational interaction. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/pdf/2209.08655.pdf&quot;>;“Enabling Conversational Interaction with Mobile UI using Large Language Models”&lt;/a>;, presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI 2023&lt;/a>;, we investigate the viability of utilizing large language models (LLMs) to enable diverse language-based interactions with mobile UIs. Recent pre-trained LLMs, such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have demonstrated abilities to adapt themselves to various downstream language tasks when being prompted with a handful of examples of the target task. We present a set of prompting techniques that enable interaction designers and developers to quickly prototype and test novel language interactions with users, which saves time and resources before investing in dedicated datasets and models. Since LLMs only take text tokens as input, we contribute a novel algorithm that generates the text representation of mobile UIs. Our results show that this approach achieves competitive performance using only two data examples per task. More broadly, we demonstrate LLMs&#39; potential to fundamentally transform the future workflow of conversational interaction design.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s1559/LLM4Mobile%201.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;553&quot; data-original-width=&quot;1559&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s16000/LLM4Mobile%201.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing our work on enabling various conversational interactions with mobile UI using LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prompting LLMs with UIs&lt;/h2>; &lt;p>; LLMs support in-context few-shot learning via prompting — instead of fine-tuning or re-training models for each new task, one can prompt an LLM with a few input and output data exemplars from the target task. For many natural language processing tasks, such as question-answering or translation, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompting&lt;/a>; performs competitively with &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;benchmark approaches&lt;/a>; that train a model specific to each task. However, language models can only take text input, while mobile UIs are multimodal, containing text, image, and structural information in their &lt;a href=&quot;https://developer.android.com/topic/performance/rendering/optimizing-view-hierarchies&quot;>;view hierarchy&lt;/a>; data (ie, the structural data containing detailed properties of UI elements) and screenshots. Moreover, directly inputting the view hierarchy data of a mobile screen into LLMs is not feasible as it contains excessive information, such as detailed properties of each UI element, which can exceed the input length limits of LLMs. &lt;/p>; &lt;p>; To address these challenges, we developed a set of techniques to prompt LLMs with mobile UIs. We contribute an algorithm that generates the text representation of mobile UIs using &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth-first_search&quot;>;depth-first search&lt;/a>; traversal to convert the Android UI&#39;s view hierarchy into HTML syntax. We also utilize &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain of thought prompting&lt;/a>;, which involves generating intermediate results and chaining them together to arrive at the final output, to elicit the reasoning ability of the LLM.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s1486/LLM4Mobile%202.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1486&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s16000/LLM4Mobile%202.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing the process of few-shot prompting LLMs with mobile UIs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our prompt design starts with a preamble that explains the prompt&#39;s purpose. The preamble is followed by multiple exemplars consisting of the input, a chain of thought (if applicable), and the output for each task. Each exemplar&#39;s input is a mobile screen in the HTML syntax. Following the input, chains of thought can be provided to elicit logical reasoning from LLMs. This step is not shown in the animation above as it is optional. The task output is the desired outcome for the target tasks, eg, a screen summary or an answer to a user question. Few-shot prompting can be achieved with more than one exemplar included in the prompt. During prediction, we feed the model the prompt with a new input screen appended at the end. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We conducted comprehensive experiments with four pivotal modeling tasks: (1) screen question-generation, (2) screen summarization, (3) screen question-answering, and (4) mapping instruction to UI action. Experimental results show that our approach achieves competitive performance using only two data examples per task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1402&quot; data-original-width=&quot;1999&quot; height=&quot;449&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/w640-h449/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 1: Screen question generation&lt;/h3>; &lt;p>; Given a mobile UI screen, the goal of screen question-generation is to synthesize coherent, grammatically correct natural language questions relevant to the UI elements requiring user input. &lt;/p>; &lt;p>; We found that LLMs can leverage the UI context to generate questions for relevant information. LLMs significantly outperformed the heuristic approach (template-based generation) regarding question quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s1999/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;977&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example screen questions generated by the LLM. The LLM can utilize screen contexts to generate grammatically correct questions relevant to each input field on the mobile UI, while the template approach falls short.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also revealed LLMs&#39; ability to combine relevant input fields into a single question for efficient communication. For example, the filters asking for the minimum and maximum price were combined into a single question: “What&#39;s the price range? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1234&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We observed that the LLM could use its prior knowledge to combine multiple related input fields to ask a single question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In an evaluation, we solicited human ratings on whether the questions were grammatically correct (Grammar) and relevant to the input fields for which they were generated (Relevance). In addition to the human-labeled language quality, we automatically examined how well LLMs can cover all the elements that need to generate questions (Coverage &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1&lt;/a>;). We found that the questions generated by LLM had almost perfect grammar (4.98/5) and were highly relevant to the input fields displayed on the screen (92.8%). Additionally, LLM performed well in terms of covering the input fields comprehensively (95.8%). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;Template &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Grammar &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;3.6 (out of 5) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;4.98 (out of 5)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Relevance &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;84.1% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;92.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Coverage F1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;95.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 2: Screen summarization&lt;/h3>; &lt;p>; Screen summarization is the automatic generation of descriptive language overviews that cover essential functionalities of mobile screens. The task helps users quickly understand the purpose of a mobile UI, which is particularly useful when the UI is not visually accessible. &lt;/p>; &lt;p>; Our results showed that LLMs can effectively summarize the essential functionalities of a mobile UI. They can generate more accurate summaries than the &lt;a href=&quot;https://arxiv.org/abs/2108.03353&quot;>;Screen2Words&lt;/a>; benchmark model that we previously introduced using UI-specific text, as highlighted in the colored text and boxes below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s1999/image10.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1421&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example summary generated by 2-shot LLM. We found the LLM is able to use specific text on the screen to compose more accurate summaries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Interestingly, we observed LLMs using their prior knowledge to deduce information not presented in the UI when creating summaries. In the example below, the LLM inferred the subway stations belong to the London Tube system, while the input UI does not contain this information. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1450&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM uses its prior knowledge to help summarize the screens.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Human evaluation rated LLM summaries as more accurate than the benchmark, yet they scored lower on metrics like &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>;. The mismatch between perceived quality and metric scores echoes &lt;a href=&quot;https://arxiv.org/abs/2209.12356&quot;>;recent work&lt;/a>; showing LLMs write better summaries despite automatic metrics not reflecting it. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;994&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1013&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>;: Screen summarization performance on automatic metrics. &lt;strong>;Right&lt;/strong>;: Screen summarization accuracy voted by human evaluators.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 3: Screen question-answering&lt;/h3>; &lt;p>; Given a mobile UI and an open-ended question asking for information regarding the UI, the model should provide the correct answer. We focus on factual questions, which require answers based on information presented on the screen. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1689&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example results from the screen QA experiment. The LLM significantly outperforms the off-the-shelf QA baseline model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We report performance using four metrics: Exact Matches (identical predicted answer to ground truth), Contains GT (answer fully containing ground truth), Sub-String of GT (answer is a sub-string of ground truth), and the &lt;a href=&quot;https://arxiv.org/abs/1911.03347&quot;>;Micro-F1&lt;/a>; score based on shared words between the predicted answer and ground truth across the entire dataset. &lt;/p>; &lt;p>; Our results showed that LLMs can correctly answer UI-related questions, such as &quot;what&#39;s the headline?&quot;. The LLM performed significantly better than baseline QA model &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;>;DistillBERT&lt;/a>;, achieving a 66.7% fully correct answer rate. Notably, the 0-shot LLM achieved an exact match score of 30.7%, indicating the model&#39;s intrinsic question answering capability. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Exact Matches&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Contains GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Sub-String of GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Micro-F1&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;30.7% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;6.5% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;5.6% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.2% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;65.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;10.0% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;62.9% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;66.7%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;12.6%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;5.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;64.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;DistillBERT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;36.0%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;8.5%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;9.9%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;37.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 4: Mapping instruction to UI action&lt;/h3>; &lt;p>; Given a mobile UI screen and natural language instruction to control the UI, the model needs to predict the ID of the object to perform the instructed action. For example, when instructed with &quot;Open Gmail,&quot; the model should correctly identify the Gmail icon on the home screen. This task is useful for controlling mobile apps using language input such as voice access. We introduced this &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;benchmark task&lt;/a>; previously. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1131&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example using data from the &lt;a href=&quot;https://github.com/google-research-datasets/seq2act&quot;>;PixelHelp dataset&lt;/a>;. The dataset contains interaction traces for common UI tasks such as turning on wifi. Each trace contains multiple steps and corresponding instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We assessed the performance of our approach using the Partial and Complete metrics from the &lt;a href=&quot;https://arxiv.org/abs/2005.03776&quot;>;Seq2Act&lt;/a>; paper. Partial refers to the percentage of correctly predicted individual steps, while Complete measures the portion of accurately predicted entire interaction traces. Although our LLM-based method did not surpass the benchmark trained on massive datasets, it still achieved remarkable performance with just two prompted data examples. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Partial&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Complete&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.29 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.69 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;31.67 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;2-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;75.28 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;34.44 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (in-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;40.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM (in-app)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;80.36&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;45.00&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Seq2Act&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;89.21&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;70.59&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Takeaways and conclusion&lt;/h2>; &lt;p>; Our study shows that prototyping novel language interactions on mobile UIs can be as easy as designing a data exemplar. As a result, an interaction designer can rapidly create functioning mock-ups to test new ideas with end users. Moreover, developers and researchers can explore different possibilities of a target task before investing significant efforts into developing new datasets and models. &lt;/p>; &lt;p>; We investigated the feasibility of prompting LLMs to enable various conversational interactions on mobile UIs. We proposed a suite of prompting techniques for adapting LLMs to mobile UIs. We conducted extensive experiments with the four important modeling tasks to evaluate the effectiveness of our approach. The results showed that compared to traditional machine learning pipelines that consist of expensive data collection and model training, one could rapidly realize novel language-based interactions using LLMs while achieving competitive performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We thank our paper co-author Gang Li, and appreciate the discussions and feedback from our colleagues Chin-Yi Cheng, Tao Li, Yu Hsiao, Michael Terry and Minsuk Chang. Special thanks to Muqthar Mohammad and Ashwin Kakarla for their invaluable assistance in coordinating data collection. We thank John Guilyard for helping create animations and graphics in the blog.&lt;/i>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1014951953767598848/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling conversational interaction on mobile with LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s72-c/LLM4Mobile%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-538722811879850400&lt;/id>;&lt;published>;2023-05-10T08:03:00.004-07:00&lt;/published>;&lt;updated>;2023-05-12T11:51:35.963-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Building better pangenomes to improve the equity of genomics&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Andrew Carroll, Product Lead, and Kishwar Shafin, Research Scientist, Genomics &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s320/image4.png&quot; style=&quot;display: none;&quot; />; &lt;p>; For decades, researchers worked together to assemble a complete copy of the molecular instructions for a human — a map of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_genome&quot;>;human genome&lt;/a>;. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Genome_Project&quot;>;first draft&lt;/a>; was finished in 2000, but with several missing pieces. Even when a complete &lt;a href=&quot;https://en.wikipedia.org/wiki/Reference_genome&quot;>;reference genome&lt;/a>; was &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;achieved in 2022&lt;/a>;, their work was not finished. A single reference genome can&#39;t incorporate known genetic variations, such as the variants for the gene determining whether a person has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Blood_type&quot;>;blood type&lt;/a>; A, B, AB or O. Furthermore, the reference genome &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-8674(19)30231-4&quot;>;didn&#39;t represent the vast diversity of human ancestries&lt;/a>;, making it less useful for detecting disease or finding cures for people from some backgrounds than others. For the past three years, we have been part of an international collaboration with 119 scientists across 60 institutions, called the &lt;a href=&quot;https://www.genome.gov/about-genomics/new-human-pangenome-reference&quot;>;Human Pangenome Research Consortium&lt;/a>;, to address these challenges by creating a new and more representative map of the human genome, a &lt;em>;&lt;a href=&quot;https://www.youtube.com/watch?v=swNtGe9QWAQ&quot;>;pangenome&lt;/a>;&lt;/em>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to share that today, in “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05896-x&quot;>;A draft human pangenome reference&lt;/a>;”, published in &lt;em>;Nature&lt;/em>;, this group is announcing the completion of the first human pangenome reference. The pangenome combines 47 individual genome reference sequences and better represents the genomic diversity of global populations. Building on Google&#39;s deep learning technologies and &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;past advances in genomics&lt;/a>;, we used tools based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; to tackle the challenges of building accurate pangenome sequences and using them for genome analysis. These contributions helped the consortium build an information-rich resource for geneticists, researchers and clinicians around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s1600/Pangenome.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s16000/Pangenome.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Using graphs to build pangenomes &lt;/h2>; &lt;p>; In the typical analysis workflow for high-throughput DNA sequencing, a &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_sequencer&quot;>;sequencing instrument&lt;/a>; reads millions of short pieces of an individual&#39;s genome, and a program called a &lt;a href=&quot;https://en.wikibooks.org/wiki/Next_Generation_Sequencing_(NGS)/Alignment&quot;>;mapper or aligner&lt;/a>; then estimates where those pieces best fit relative to the single, linear human reference sequence. Next, variant caller software identifies the unique parts of the individual&#39;s sequence relative to the reference. &lt;/p>; &lt;p>; But because humans carry a diverse set of sequences, sections that are present in an individual&#39;s DNA but are not in the reference genome can&#39;t be analyzed. One study of 910 African individuals found that a total of &lt;a href=&quot;https://www.theatlantic.com/science/archive/2018/11/human-genome-300-million-missing-letters-dna/576481/&quot;>;300 million DNA base pairs&lt;/a>; — 10% of the roughly three billion base pair reference genome — are not present in the previous linear reference but occur in at least one of the 910 individuals. &lt;/p>; &lt;p>; To address this issue, the consortium used &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph data structures&lt;/a>;, which are powerful for genomics because they can represent the sequences of many people simultaneously, which is needed to create a pangenome. Nodes in a graph genome contain the known set of sequences in a population, and paths through those nodes compactly describe the unique sequences of an individual&#39;s DNA. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s1920/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1081&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Schematic of a graph genome. Each color represents the sequence path of a different individual. Multiple paths passing through the same node indicate multiple individuals share that sequence, but some paths also show a &lt;a href=&quot;https://www.garvan.org.au/research/kinghorn-centre-for-clinical-genomics/learn-about-genomics/for-gp/genetics-refresher-1/types-of-variants&quot;>;single nucleotide variant&lt;/a>; (SNV), insertions, or deletions. Illustration credit Darryl Leja, &lt;a href=&quot;https://www.genome.gov/&quot;>;National Human Genome Research Institute&lt;/a>; (NHGRI).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/s1172/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1172&quot; data-original-width=&quot;1158&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/w632-h640/image1.png&quot; width=&quot;632&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Actual graph genome for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Major_histocompatibility_complex&quot;>;major histocompatibility complex&lt;/a>; (MHC) region of the genome. Genes in MHC regions are essential to immune function and are associated with a person&#39;s resistance and susceptibility to infectious disease and autoimmune disorders (eg, &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fgene.2021.671682/full&quot;>;ankylosing spondylitis&lt;/a>; and &lt;a href=&quot;https://www.hindawi.com/journals/jir/2012/584374/&quot;>;lupus)&lt;/a>;. The graph shows the linear human genome reference (green) and different individual person&#39;s sequence (gray). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Using graphs creates numerous challenges. They require reference sequences to be highly accurate and the development of new methods that can use their data structure as an input. However, new sequencing technologies (such as &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/519025v1&quot;>;consensus sequencing&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2008.01237&quot;>;phased assembly methods&lt;/a>;) have driven exciting progress towards solving these problems. &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.nature.com/articles/s41592-022-01730-w&quot;>;Long-read sequencing technology&lt;/a>;, which reads larger pieces of the genome (10,000 to millions of DNA characters long) at a time, are essential to the creation of high quality reference sequences because larger pieces can be stitched together into assembled genomes more easily than the short pieces read out by earlier technologies. &lt;a href=&quot;https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/short-read-sequencing/&quot;>;Short read sequencing&lt;/a>; reads pieces of the genome that are only 100 to 300 DNA characters long, but has been the highly scalable basis for high-throughput sequencing methods developed in the 2000s. Though long-read sequencing is newer and has advantages for reference genome creation, many informatics methods for short reads hadn&#39;t been developed for long read technologies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evolving DeepVariant for error correction&lt;/h2>; &lt;p>; Google initially developed &lt;a href=&quot;https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html&quot;>;DeepVariant&lt;/a>;, an open-source CNN variant caller framework that analyzes the short-read sequencing evidence of local regions of the genome. However, we were able to re-train DeepVariant to yield &lt;a href=&quot;https://www.nature.com/articles/s41587-019-0217-9&quot;>;accurate analysis of Pacific Bioscience&#39;s long-read data&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s640/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training and evaluation schematic for DeepVariant.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We next teamed up with researchers at the University of California, Santa Cruz (UCSC) &lt;a href=&quot;https://genomics.ucsc.edu/&quot;>;Genomics Institute&lt;/a>; to participate in a &lt;a href=&quot;https://precision.fda.gov/challenges/10&quot;>;United States Food and Drug Administration competition&lt;/a>; for another &lt;a href=&quot;https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html&quot;>;long-read sequencing technology from Oxford Nanopore&lt;/a>;. Together, we won the award for highest accuracy in the &lt;a href=&quot;https://nanoporetech.com/&quot;>;nanopore&lt;/a>; category, with a single nucleotide variants (SNVs) accuracy that matched short-read sequencing. This work has been used to &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;detect and treat genetic diseases in critically ill newborns&lt;/a>;. The use of DeepVariant on long-read technologies provided the foundation for the consortium&#39;s use of DeepVariant for error correction of pangenomes. &lt;/p>; &lt;p>; DeepVariant&#39;s ability to use multiple long-read sequencing modalities proved useful for error correction in the &lt;a href=&quot;https://sites.google.com/corp/ucsc.edu/t2tworkinggroup&quot;>;Telomere-to-Telomere (T2T) Consortium&lt;/a>;&#39;s effort that generated &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;the first complete assembly of a human genome&lt;/a>;. Completing this first genome set the stage to build the multiple reference genomes required for pangenomes, and T2T was already working closely with the &lt;a href=&quot;https://humanpangenomeproject.org/&quot;>;Human Pangenome Project&lt;/a>; (with many shared members) to scale those practices. &lt;/p>; &lt;p>; With a set of high-quality human reference genomes on the horizon, developing methods that could use those assemblies grew in importance. We worked to adapt DeepVariant to use the pangenome developed by the consortium. In partnership with UCSC, we built an end-to-end analysis workflow for &lt;a href=&quot;https://genome.cshlp.org/content/32/5/893.short&quot;>;graph-based variant detection&lt;/a>;, and demonstrated &lt;a href=&quot;https://www.science.org/doi/abs/10.1126/science.abg8871&quot;>;improved accuracy across several thousand samples&lt;/a>;. The use of the pangenome allows many previously missed variants to be correctly identified. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;258&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualization of variant calls in the &lt;a href=&quot;https://medlineplus.gov/genetics/gene/kcne1/&quot;>;KCNE1 gene&lt;/a>; (a gene with variants associated with cardiac arrhythmias and &lt;a href=&quot;https://www.ahajournals.org/doi/pdf/10.1161/JAHA.117.007837&quot;>;sudden death&lt;/a>;) using a pangenome reference versus the prior linear reference. Each dot represents a variant call that is either correct (&lt;strong>;blue dot&lt;/strong>;), incorrect (&lt;strong>;green dot&lt;/strong>;) — when a variant is identified but is not really there —or a missed variant call (&lt;strong>;red dot&lt;/strong>;). The top box shows variant calls made by DeepVariant using the pangenome reference while the bottom shows variant calls made by using the linear reference. Figure adapted from &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.07.09.499321v1&quot;>;A Draft Human Pangenome Reference.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improving pangenome sequences using transformers &lt;/h2>; &lt;p>; Just as new sequencing technologies enabled new pangenome approaches, new informatics technologies enabled improvements for sequencing methods. Google adapted transformer architectures from analysis of human language to genome sequences to develop &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.08.31.458403v1&quot;>;DeepConsensus&lt;/a>;. A key enabler for this was the development of a differentiable loss function that could handle the insertions and deletions common in sequencing data. This enabled us to have high accuracy without needing a decoder, allowing the speed required to keep up with terabytes of sequencer output. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/s940/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;647&quot; data-original-width=&quot;940&quot; height=&quot;441&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/w640-h441/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer architecture for DeepConsensus. DeepConsensus takes as input the repeated sequence of the DNA molecule, measured from fluorescent light detected by the addition of each base. DeepConsensus also uses as input the more detailed information about the sequencing process, including the duration of the light pulse (referred to here as pulse width or PW), the time between pulses (IP) the signal-to-noise ratio (SN) and which side of the double helix is being measured (strand).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;794&quot; data-original-width=&quot;1999&quot; height=&quot;254&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/w640-h254/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of alignment loss function in training evaluation of model output. Better accounting of insertions and deletions by a differentiable alignment function enables the model training process to better estimate errors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; DeepConsensus &lt;a href=&quot;https://blog.google/technology/health/a-new-genome-sequencing-tool-powered-with-our-technology/&quot;>;improves the yield and accuracy of instrument&lt;/a>; data. Because PacBio sequencing provides the primary sequence information for the 47 genome assemblies, we could apply DeepConsensus to improve those assemblies. With application of DeepConsensus, consortium members &lt;a href=&quot;https://www.nature.com/articles/s41587-023-01662-6&quot;>;built a genome assembler&lt;/a>; that was able to reach 99.9997% assembly base-level accuracies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We developed multiple new approaches to improve genetic sequencing methods, which we then used to construct pangenome references that enable more robust genome analysis. &lt;/p>; &lt;p>; But this is just the beginning of the story. In the next stage, a larger, worldwide group of scientists and clinicians will use this pangenome reference to study genetic diseases and make new drugs. And future pangenomes will represent even more individuals, realizing a vision summarized this way in a recent Nature story: “&lt;a href=&quot;https://www.nature.com/articles/d41586-023-01300-w&quot;>;Every base, everywhere, all at once&lt;/a>;.” &lt;em>;Read our post on the &lt;a href=&quot;https://blog.google/technology/health/first-pangenome-reference-nature-paper-ai/&quot;>;Keyword Blog&lt;/a>; to learn more about the human pangenome reference announcement.&lt;/em>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;Many people were involved in creating the pangenome reference, including 119 authors across 60 organizations, with the Human Pangenome Reference Consortium. This blog post highlights Google&#39;s contributions to the broader work. We thank the research groups at UCSC Genomics Institute (GI) under Professors Benedict Paten and Karen Miga, genome polishing efforts of Arang Rhie at National Institute of Health (NIH), Genome Assembly and Polishing of Adam Phillipy&#39;s group, and the standards group at National Institute of Standards and Technology (NIST) of Justin Zook. We thank Google contributors: Pi-Chuan Chang, Maria Nattestad, Daniel Cook, Alexey Kolesnikov, Anastaysia Belyaeva, and Gunjan Baid. We thank John Guilyard for his illustrative animation, and Lizzie Dorfman, Elise Kleeman, Erika Hayden, Cory McLean, Shravya Shetty, Greg Corrado, Katherine Chou, and Yossi Matias for their support, coordination, and leadership. Last but not least, thanks to the research participants that provided their DNA to help build the pangenome resource.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/538722811879850400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html&quot; rel=&quot;alternate&quot; title=&quot;Building better pangenomes to improve the equity of genomics&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s72-c/image4.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7102791933497880989&lt;/id>;&lt;published>;2023-05-04T14:59:00.004-07:00&lt;/published>;&lt;updated>;2023-05-04T14:59:57.911-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s800/MAMMUT.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Vision-language foundational models are built on the premise of a single pre-training followed by subsequent adaptation to multiple downstream tasks. Two main and disjoint training scenarios are popular: a &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;-style contrastive learning and next-token prediction. Contrastive learning trains the model to predict if image-text pairs correctly match, effectively building visual and text representations for the corresponding image and text inputs, whereas next-token prediction predicts the most likely next text token in a sequence, thus learning to generate text, according to the required task. &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;Contrastive learning&lt;/a>; enables &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval tasks&lt;/a>;, such as finding the image that best matches a certain description, and next-token learning enables text-generative tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation#Image_captioning&quot;>;Image Captioning&lt;/a>; and &lt;a href=&quot;https://visualqa.org/&quot;>;Visual Question Answering&lt;/a>; (VQA). While both approaches have demonstrated powerful results, when a model is pre-trained contrastively, it typically does not fare well on text-generative tasks and vice-versa. Furthermore, adaptation to other tasks is often done with complex or inefficient methods. For example, in order to extend a vision-language model to videos, some models need to do inference for each video frame separately. This limits the size of the videos that can be processed to only a few frames and does not fully take advantage of motion information available across frames. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Motivated by this, we present “&lt;a href=&quot;https://arxiv.org/abs/2303.16839&quot;>;A Simple Architecture for Joint Learning for MultiModal Tasks&lt;/a>;”, called MaMMUT, which is able to train jointly for these competing objectives and which provides a foundation for many vision-language tasks either directly or via simple adaptation. MaMMUT is a compact, 2B-parameter multimodal model that trains across contrastive, text generative, and localization-aware objectives. It consists of a single image encoder and a text decoder, which allows for a direct reuse of both components. Furthermore, a straightforward adaptation to video-text tasks requires only using the image encoder once and can handle many more frames than prior work. In line with recent language models (eg, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;>;GLaM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;GPT3&lt;/a>;), our architecture uses a decoder-only text model and can be thought of as a simple extension of language models. While modest in size, our model outperforms the state of the art or achieves competitive performance on &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (VideoQA), &lt;a href=&quot;https://ai.googleblog.com/2022/06/end-to-end-generative-pre-training-for.html&quot;>;video captioning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>;, and &lt;a href=&quot;https://visualqa.org/&quot;>;VQA&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s1182/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;1182&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s980/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s1120/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MaMMUT model enables a wide range of tasks such as image-text/text-image retrieval (&lt;b>;top left&lt;/b>; and &lt;b>;top right&lt;/b>;), VQA (&lt;b>;middle left&lt;/b>;), open-vocabulary detection (&lt;b>;middle right&lt;/b>;), and VideoQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder-only model architecture&lt;/h2>; &lt;p>; One surprising finding is that a single language-decoder is sufficient for all these tasks, which obviates the need for both complex constructs and training procedures presented before. For example, our model (presented to the left in the figure below) consists of a single visual encoder and single text-decoder, connected via &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;cross attention&lt;/a>;, and trains simultaneously on both contrastive and text-generative types of losses. Comparatively, prior work is either not able to handle image-text retrieval tasks, or applies only some losses to only some parts of the model. To enable multimodal tasks and fully take advantage of the decoder-only model, we need to jointly train both contrastive losses and text-generative captioning-like losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s1191/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1191&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT architecture (&lt;b>;left&lt;/b>;) is a simple construct consisting of a single vision encoder and a single text decoder. Compared to other popular vision-language models — eg, &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; (&lt;b>;middle&lt;/b>;) and &lt;a href=&quot;https://arxiv.org/abs/2107.07651&quot;>;ALBEF&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot;>;CoCa&lt;/a>; (&lt;b>;right&lt;/b>;) — it trains jointly and efficiently for multiple vision-language tasks, with both contrastive and text-generative losses, fully sharing the weights between the tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder two-pass learning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1801.10198&quot;>;Decoder-only models&lt;/a>; for language learning show clear advantages in performance with smaller model size (almost half the parameters). The main challenge for applying them to multimodal settings is to unify the contrastive learning (which uses unconditional sequence-level representation) with captioning (which optimizes the likelihood of a token conditioned on the previous tokens). We propose a two-pass approach to jointly learn these two conflicting types of text representations within the decoder. During the first pass, we utilize cross attention and &lt;a href=&quot;https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer&quot;>;causal masking&lt;/a>; to learn the caption generation task — the text features can attend to the image features and predict the tokens in sequence. On the second pass, we disable the cross-attention and causal masking to learn the contrastive task. The text features will not see the image features but can attend bidirectionally to all text tokens at once to produce the final text-based representation. Completing this two-pass approach within the same decoder allows for accommodating both types of tasks that were previously hard to reconcile. While simple, we show that this model architecture is able to provide a foundation for multiple multimodal tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s670/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;670&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT decoder-only two-pass learning enables both contrastive and generative learning paths by the same model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Another advantage of our architecture is that, since it is trained for these disjoint tasks, it can be seamlessly applied to multiple applications such as image-text and text-image retrieval, VQA, and captioning. &lt;/p>; &lt;p>; Moreover, MaMMUT easily adapts to video-language tasks. Previous approaches used a vision encoder to process each frame individually, which required applying it multiple times. This is slow and restricts the number of frames the model can handle, typically to only 6–8. With MaMMUT, we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>; for lightweight adaptation directly via the spatio-temporal information from the video. Furthermore, adapting the model to &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;Open-Vocabulary Detection&lt;/a>; is done by simply training to detect bounding-boxes via an object-detection head. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s632/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;204&quot; data-original-width=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Adaptation of the MaMMUT architecture to video tasks (&lt;b>;left&lt;/b>;) is simple and fully reuses the model. This is done by generating a video “tubes” feature representation, similar to image patches, that are projected to lower dimensional tokens and run through the vision encoder. Unlike prior approaches (&lt;b>;right&lt;/b>;) that need to run multiple individual images through the vision encoder, we use it only once.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Our model achieves excellent zero-shot results on image-text and text-image retrieval without any adaptation, outperforming all previous state-of-the-art models. The results on VQA are competitive with state-of-the-art results, which are achieved by much larger models. The &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI model&lt;/a>; (17B parameters) and the &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo model&lt;/a>; (80B) have the best performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>;, but MaMMUT (2B) has the same accuracy as the 15B PaLI. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s600/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the state of the art (SOTA) on Zero-Shot Image-Text (I2T) and Text-Image (T2I) retrieval on both &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; (&lt;b>;top&lt;/b>;) and &lt;a href=&quot;https://ieeexplore.ieee.org/document/7410660&quot;>;Flickr&lt;/a>; (&lt;b>;bottom&lt;/b>;) benchmarks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>; is competitive but does not outperform large models such as Flamingo-80B and PalI-17B. Performance is evaluated in the more challenging open-ended text generation setting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MaMMUT also outperforms the state-of-the-art on VideoQA, as shown below on the &lt;a href=&quot;https://ieeexplore.ieee.org/document/7780940&quot;>;MSRVTT-QA&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3123266.3123427&quot;>;MSVD-QA&lt;/a>; datasets. Note that we outperform much bigger models such as &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo&lt;/a>;, which is specifically designed for image+video pre-training and is pre-trained with both image-text and video-text data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s600/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s600/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the SOTA models on VideoQA tasks (MSRVTT-QA dataset, &lt;b>;top&lt;/b>;, MSVD-QA dataset,&lt;b>; bottom&lt;/b>;), outperforming much larger models, eg, the 5B GIT2 or Flamingo, which uses 80B parameters and is pre-trained for both image-language and vision-language tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results outperform the state-of-the-art on open-vocabulary detection fine-tuning as is also shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MAMMUT &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>; results on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; dataset compared to state-of-the-art methods. We report the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;average precisions for rare classes&lt;/a>; (APr) as is previously adopted in the literature.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key ingredients&lt;/h2>; &lt;p>; We show that joint training of both contrastive and text-generative objectives is not an easy task, and in our ablations we find that these tasks are served better by different design choices. We see that fewer cross-attention connections are better for retrieval tasks, but more are preferred by VQA tasks. Yet, while this shows that our model&#39;s design choices might be suboptimal for individual tasks, our model is more effective than more complex, or larger, models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s600/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s16000/image11.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s600/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Ablation studies showing that fewer cross-attention connections (1-2) are better for retrieval tasks (&lt;b>;top&lt;/b>;), whereas more connections favor text-generative tasks such as VQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented MaMMUT, a simple and compact vision-encoder language-decoder model that jointly trains a number of conflicting objectives to reconcile contrastive-like and text-generative tasks. Our model also serves as a foundation for many more vision-language tasks, achieving state-of-the-art or competitive performance on image-text and text-image retrieval, videoQA, video captioning, open-vocabulary detection and VQA. We hope it can be further used for more multimodal applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, and Anelia Angelova. We would like to thank Mojtaba Seyedhosseini, Vijay Vasudevan, Priya Goyal, Jiahui Yu, Zirui Wang, Yonghui Wu, Runze Li, Jie Mei, Radu Soricut, Qingqing Huang, Andy Ly, Nan Du, Yuxin Wu, Tom Duerig, Paul Natsev, Zoubin Ghahramani for their help and support. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7102791933497880989/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot; rel=&quot;alternate&quot; title=&quot;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s72-c/MAMMUT.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3611317650594898640&lt;/id>;&lt;published>;2023-05-03T10:22:00.000-07:00&lt;/published>;&lt;updated>;2023-05-03T10:22:54.056-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Joanne Truong, Student Researcher, and Wenhao Yu, Research Scientist, Robotics at Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s320/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Teaching mobile robots to navigate in complex outdoor environments is critical to real-world applications, such as delivery or &lt;a href=&quot;https://www.nasa.gov/stem-ed-resources/robotic-search-and-rescue-challenge.html&quot;>;search and rescue&lt;/a>;. However, this is also a challenging problem as the robot needs to perceive its surroundings, and then explore to identify feasible paths towards the goal. Another common challenge is that the robot needs to overcome uneven terrains, such as stairs, curbs, or rockbed on a trail, while avoiding obstacles and pedestrians. In our prior work, we investigated the second challenge by teaching a quadruped robot to tackle challenging &lt;a href=&quot;https://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html&quot;>;uneven obstacles&lt;/a>; and &lt;a href=&quot;https://research.google/pubs/pub51639/&quot;>;various outdoor terrains&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/pdf/2305.01098.pdf&quot;>;IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience&lt;/a>;”, we present our recent work to tackle the robotic challenge of reasoning about the perceived surroundings to identify a viable navigation path in outdoor environments. We introduce a learning-based indoor-to-outdoor transfer algorithm that uses deep reinforcement learning to train a navigation policy in simulated indoor environments, and successfully transfers that same policy to real outdoor environments. We also introduce Context-Maps (maps with environment observations created by a user), which are applied to our algorithm to enable efficient long-range navigation. We demonstrate that with this policy, robots can successfully navigate hundreds of meters in novel outdoor environments, around previously unseen outdoor obstacles (trees, bushes, buildings, pedestrians, etc.), and in different weather conditions (sunny, overcast, sunset). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/teaser.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;PointGoal navigation&lt;/h2>; &lt;p>; User inputs can tell a robot where to go with commands like “go to the Android statue”, pictures showing a target location, or by simply picking a point on a map. In this work, we specify the navigation goal (a selected point on a map) as a relative coordinate to the robot&#39;s current position (ie, “go to ∆x, ∆y”), this is also known as the &lt;a href=&quot;https://arxiv.org/abs/1807.06757&quot;>;PointGoal Visual Navigation&lt;/a>; (PointNav) task. PointNav is a general formulation for navigation tasks and is one of the standard choices for indoor navigation tasks. However, due to the diverse visuals, uneven terrains and long distance goals in outdoor environments, training PointNav policies for outdoor environments is a challenging task. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Indoor-to-outdoor transfer&lt;/h2>; &lt;p>; Recent successes in training wheeled and legged robotic agents to navigate in &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf&quot;>;indoor&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1911.00357.pdf&quot;>;environments&lt;/a>; were enabled by the development of fast, scalable simulators and the availability of large-scale datasets of &lt;a href=&quot;https://aihabitat.org/datasets/hm3d/&quot;>;photorealistic&lt;/a>; &lt;a href=&quot;https://svl.stanford.edu/igibson/&quot;>;3D scans&lt;/a>; &lt;a href=&quot;https://niessner.github.io/Matterport/&quot;>;of indoor&lt;/a>; &lt;a href=&quot;https://ai2thor.allenai.org/&quot;>;environments&lt;/a>;. To leverage these successes, we develop an indoor-to-outdoor transfer technique that enables our robots to learn from simulated indoor environments and to be deployed in real outdoor environments. &lt;/p>; &lt;p>; To overcome the differences between simulated indoor environments and real outdoor environments, we apply &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;kinematic control&lt;/a>; and image augmentation techniques in our learning system. When using kinematic control, we assume the existence of a reliable low-level &lt;a href=&quot;https://dev.bostondynamics.com/docs/concepts/autonomy/graphnav_and_robot_locomotion&quot;>;locomotion controller&lt;/a>; that can control the robot to precisely reach a new location. This assumption allows us to directly move the robot to the target location during simulation training through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler_method&quot;>;forward Euler integration&lt;/a>; and relieves us from having to explicitly model the underlying robot dynamics in simulation, which drastically improves the throughput of simulation data generation. &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;Prior work&lt;/a>; has shown that kinematic control can lead to better sim-to-real transfer compared to a &lt;a href=&quot;https://arxiv.org/pdf/2008.11867.pdf&quot;>;dynamic control approach&lt;/a>;, where full robot dynamics are modeled and a low-level locomotion controller is required for moving the robot. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/kin_v_dyn.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>; Kinematic control; &lt;strong>;Right&lt;/strong>;: Dynamic control&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We created an outdoor maze-like environment using objects found indoors for initial experiments, where we used Boston Dynamics&#39; &lt;a href=&quot;https://www.bostondynamics.com/products/spot&quot;>;Spot robot&lt;/a>; for test navigation. We found that the robot could navigate around novel obstacles in the new outdoor environment. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_short_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Spot robot successfully navigates around obstacles found in indoor environments, with a policy trained entirely in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; However, when faced with unfamiliar outdoor obstacles not seen during training, such as a large slope, the robot was unable to navigate the slope. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The robot is unable to navigate up slopes, as slopes are rare in indoor environments and the robot was not trained to tackle it.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to walk up and down slopes, we apply an image augmentation technique during the simulation training. Specifically, we randomly tilt the simulated camera on the robot during training. It can be pointed up or down within 30 degrees. This augmentation effectively makes the robot perceive slopes even though the floor is level. Training on these perceived slopes enables the robot to navigate slopes in the real-world. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By randomly tilting the camera angle during training in simulation, the robot is now able to walk up and down slopes.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Since the robots were only trained in simulated indoor environments, in which they typically need to walk to a goal just a few meters away, we find that the learned network failed to process longer-range inputs — eg, the policy failed to walk forward for 100 meters in an empty space. To enable the policy network to handle long-range inputs that are common for outdoor navigation, we normalize the goal vector by using the log of the goal distance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Context-Maps for complex long-range navigation &lt;/h2>; &lt;p>; Putting everything together, the robot can navigate outdoors towards the goal, while walking on uneven terrain, and avoiding trees, pedestrians and other outdoor obstacles. However, there is still one key component missing: the robot&#39;s ability to plan an efficient long-range path. At this scale of navigation, taking a wrong turn and backtracking can be costly. For example, we find that the local exploration strategy learned by standard PointNav policies are insufficient in finding a long-range goal and usually leads to a dead end (shown below). This is because the robot is navigating without context of its environment, and the optimal path may not be visible to the robot from the start. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_long_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Navigation policies without context of the environment do not handle complex long-range navigation goals.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to take the context into consideration and purposefully plan an efficient path, we provide a Context-Map (a binary image that represents a top-down occupancy map of the region that the robot is within) as additional observations for the robot. An example Context-Map is given below, where the black region denotes areas occupied by obstacles and white region is walkable by the robot. The green and red circle denotes the start and goal location of the navigation task. Through the Context-Map, we can provide hints to the robot (eg, the narrow opening in the route below) to help it plan an efficient navigation route. In our experiments, we create the Context-Map for each route guided by &lt;a href=&quot;https://www.google.com/maps&quot;>;Google Maps satellite&lt;/a>; images. We denote this variant of PointNav with environmental context, as &lt;em>;Context-Guided PointNav&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/s1113/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1113&quot; height=&quot;350&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/w640-h350/image12.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the Context-Map (&lt;strong>;right&lt;/strong>;) for a navigation task (&lt;strong>;left&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; It is important to note that the Context-Map does not need to be accurate because it only serves as a rough outline for planning. During navigation,&lt;strong>; &lt;/strong>;the robot still needs to rely on its onboard cameras to identify and adapt its path to pedestrians, which are absent on the map. In our experiments, a human operator quickly sketches the Context-Map from the satellite image, masking out the regions to be avoided. This Context-Map, together with other onboard sensory inputs, including depth images and relative position to the goal, are fed into a neural network with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention&lt;/a>; models (ie, &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>;), which are trained using &lt;a href=&quot;https://arxiv.org/abs/1911.00357&quot;>;DD-PPO&lt;/a>;, a distributed implementation of &lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;>;proximal policy optimization&lt;/a>;, in large-scale simulations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/s782/image15.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;782&quot; height=&quot;442&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/w640-h442/image15.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Context-Guided PointNav architecture consists of a 3-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) to process depth images from the robot&#39;s camera, and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) to process the goal vector. The features are passed into a &lt;a href=&quot;https://arxiv.org/abs/1406.1078&quot;>;gated recurrent unit&lt;/a>; (GRU). We use an additional CNN encoder to process the context-map (top-down map). We compute the &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;scaled dot product attention&lt;/a>; between the map and the depth image, and use a second GRU to process the attended features (Context Attn., Depth Attn.). The output of the policy are linear and angular velocities for the Spot robot to follow. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our system across three long-range outdoor navigation tasks. The provided Context-Maps are rough, incomplete environment outlines that omit obstacles, such as cars, trees, or chairs. &lt;/p>; &lt;p>; With the proposed algorithm, our robot can successfully reach the distant goal location 100% of the time, without a single collision or human intervention. The robot was able to navigate around pedestrians and real-world clutter that are not present on the context-map, and navigate on various terrain including dirt slopes and grass. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 1&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s1999/image14.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s16000/image14.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 2&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s1999/image16.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;697&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 3&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s1999/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work opens up robotic navigation research to the less explored domain of diverse outdoor environments. Our indoor-to-outdoor transfer algorithm uses zero real-world experience and does not require the simulator to model predominantly-outdoor phenomena (terrain, ditches, sidewalks, cars, etc). The success in the approach comes from a combination of a robust locomotion control, low sim-to-real gap in depth and map sensors, and large-scale training in simulation. We demonstrate that providing robots with approximate, high-level maps can enable long-range navigation in novel outdoor environments. Our results provide compelling evidence for challenging the (admittedly reasonable) hypothesis that a new simulator must be designed for every new scenario we wish to study. For more information, please see our &lt;a href=&quot;https://indoorsim2outdoorreal.github.io/&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We would like to thank Sonia Chernova, Tingnan Zhang, April Zitkovich, Dhruv Batra, and Jie Tan for advising and contributing to the project. We would also like to thank Naoki Yokoyama, Nubby Lee, Diego Reyes, Ben Jyenis, and Gus Kouretas for help with the robot experiment setup.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3611317650594898640/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot; rel=&quot;alternate&quot; title=&quot;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s72-c/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;