<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-06-27T14:19:38.623-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="conference"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="NLP"></category><category term="CVPR"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="Computer Science"></category><category term="On-device Learning"></category><category term="MOOC"></category><category term="HCI"></category><category term="Security and Privacy"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="Self-Supervised Learning"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="accessibility"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="optimization"></category><category term="Android"></category><category term="ACL"></category><category term="Awards"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="Automatic Speech Recognition"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="video"></category><category term="Environment"></category><category term="Google Maps"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Collaboration"></category><category term="DeepMind"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Responsible AI"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Acoustic Modeling"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Systems"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Recommender Systems"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="RAI-HCT Highlights"></category><category term="Social Networks"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="Differential Privacy"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1249&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-6036162561497757163&lt;/id>;&lt;发布>;2023-06-27T14:19:00.000-07:00&lt;/发布>;&lt;更新>;2023-06- 27T14:19:04.029-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;多模式学习&quot; >;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;统一图像标题和具有前缀调节的图像分类数据集&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Kuniaki Saito（云 AI 团队学生研究员）和 Kihyuk Sohn（感知团队研究科学家） &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORW vPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3EtiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s320/Prefix%20conditioning%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 在网络规模图像字幕数据集上预训练视觉语言 (VL) 模型最近已成为图像分类数据上传统预训练的强大替代方案。图像字幕数据集被认为更“开放域”，因为它们包含更广泛的场景类型和词汇，这导致模型在以下方面具有强大的性能：少样本和零样本识别任务&lt;/a>;。然而，具有细粒度类描述的图像可能很少见，并且由于图像标题数据集不经过手动管理，类分布可能不平衡。相比之下，大规模分类数据集，例如 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>;，通常是精心策划的，因此可以提供具有平衡性的细粒度类别。标签分布。虽然听起来很有希望，但直接结合标题和分类数据集进行预训练通常是不成功的，因为它可能会导致有偏差的表示，而这些表示不能很好地推广到各种下游任务。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2206.01125&quot;>;前缀条件统一语言和标签监管&lt;/a>;” ”，在 &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>; 上提出，我们演示了一种预训练策略，该策略使用分类和描述数据集来提供互补的优势。首先，我们表明，天真地统一数据集会导致下游零样本识别任务的性能次优，因为模型受到数据集偏差的影响：每个数据集中图像域和词汇的覆盖范围不同。我们在训练过程中通过&lt;em>;前缀调节&lt;/em>;解决了这个问题，这是一种新颖的简单而有效的方法，它使用前缀标记来消除数据集偏差与视觉概念。这种方法允许语言编码器从两个数据集学习，同时还为每个数据集定制特征提取。前缀条件是一种通用方法，可以轻松集成到现有的 VL 预训练目标中，例如&lt;a href=&quot;https://openai.com/research/clip&quot;>;对比语言-图像预训练&lt;/a>; (CLIP) 或&lt;a href=&quot;https://arxiv.org/abs/2204.03610&quot;>;统一对比学习&lt;/a>; (UniCL)。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;高级思想&lt;/h2>; &lt;p>;我们注意到分类数据集往往在以下方面存在偏差：至少有两种方式：（1）图像大多包含来自受限领域的单个对象，（2）词汇量有限并且缺乏零样本学习所需的语言灵活性。例如，针对 ImageNet 优化的“狗的照片”的类嵌入通常会导致从 ImageNet 数据集拉取的图像中心有一张狗的照片，这不能很好地推广到包含多只狗的图像的其他数据集在不同的空间位置或狗与其他主体。 &lt;/p>; &lt;p>; 相比之下，字幕数据集包含更广泛的场景类型和词汇。如下所示，如果模型只是从两个数据集学习，则语言嵌入可能会纠缠图像分类和标题数据集的偏差，这会降低零样本分类的泛化能力。如果我们能够消除两个数据集的偏差，我们就可以使用为标题数据集量身定制的语言嵌入来提高泛化能力。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526 YbqifUcnvjdP4LVtk8HAETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/s1150/image1 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1150&quot; data-original-width=&quot;856&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAH8qOwZZQO32vlcA-QsPrbO6gmSrIK7LPqzJctsfYswtrhxccx-plypSfvij0_7hlbBiy3isvb526YbqifUcnvjdP4LVtk8HA ETdWgEFqgaDX_O4BJi91dJFvwshi_KEjBJ8l1HySYtP73xMUlqL-TiB9KifhVSfe8563Z00olE8-JAiHHXuesWu5tgDg/w476-h640/image1.png&quot; width=&quot;476&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;顶部&lt;/strong>;：语言嵌入纠缠了图像分类和标题数据集的偏差。 &lt;strong>;底部&lt;/strong>;：语言嵌入消除了两个数据集的偏差。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%; &quot;>; &lt;br />; &lt;/div>; &lt;h2>;前缀调节&lt;/h2>; &lt;p>; 前缀调节的部分灵感来自于&lt;a href=&quot;https://ai.googleblog.com/2022/02/guiding-frozen -language-models-with.html&quot;>;提示调整&lt;/a>;，将可学习的标记添加到输入标记序列中，以指示预训练的模型主干学习可用于解决下游任务的特定于任务的知识。前缀调节方法与提示调整有两个不同之处：（1）它旨在通过消除数据集偏差来统一图像标题和分类数据集，（2）它应用于 VL 预训练，而标准提示调整是用于微调模型。前缀条件是一种显式方法，可根据用户提供的数据集类型专门引导模型主干的行为。当提前知道不同类型数据集的数量时，这在生产中特别有用。 &lt;/p>; &lt;p>; 在训练过程中，前缀条件为每个数据集类型学习一个文本标记（前缀标记），它吸收数据集的偏差，并允许剩余的文本标记专注于学习视觉概念。具体来说，它将每个数据集类型的前缀标记添加到输入标记中，以通知输入数据类型的语言和视觉编码器（例如，分类与标题）。前缀标记经过训练可以学习数据集类型特定的偏差，这使我们能够消除语言表示中的偏差，并在测试期间利用在图像标题数据集上学习到的嵌入，即使没有输入标题。 &lt;/p>; &lt;p>; 我们使用语言和视觉编码器对 CLIP 进行前缀调节。在测试期间，我们采用用于图像标题数据集的前缀，因为该数据集应该涵盖更广泛的场景类型和词汇，从而在零样本识别中获得更好的性能。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me 5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5Ud69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s1038/image5.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;1038&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEhuU4-e7aPX98_nWDVCyecgsHcnXAK2ATQOLyUrZs4aXHK5tH6Au661b_LKrkqhnsk4JweQy7BdkguHFjelarx3Me5cwJ59Vf0sBv4TbPSNIpLc8k7Xg1fzc5U d69ltx8uYU5iSZXq1vzk1S0vUJpcnQF72KkEmyv4LdHCUG9NK9qv0UK2kxy81XBupZjq_/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;前缀调节说明。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;实验结果&lt;/h2>; &lt;p>; 我们将前缀条件应用于两种类型的对比损失，&lt;a href=&quot;http://proceedings.mlr.press/v139/radford21a&quot;>;CLIP&lt;/a >; 和 &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf&quot;>;UniCL&lt;/a>; 并评估它们在零样本识别任务上的表现使用 &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet21K&lt;/a>; (IN21K) 和 &lt;a href=&quot;https://github.com/google-research-datasets/ 训练的模型Conceptual-12m&quot;>;概念 12M&lt;/a>; (CC12M)。使用前缀调节使用两个数据集训练的 CLIP 和 UniCL 模型显示出零样本分类精度的巨大改进。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAk31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49s vL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s440/image4.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;339&quot; data-original-width=&quot;440&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgG2Q3QfWGfGILVqZwtavflFt7cRHOAK31wFR0qP75W8tgDZjGTa3SleAayInpJAj1JUPjX6kM2b9T49svL_hpMCQVfbgPSkpEIBDDLV2hOA5JNnxy23o6 mkN2flXaYyykEmBLWNXFGWqHlKvmWyuGaHR-nmqVgUDpdXJFqv_LWYfxWLuFxAijhK9QMVUeS/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;与使用前缀条件（“我们的”）这两个数据集训练的 CLIP 和 UniCL 模型相比，仅使用 IN21K 或 CC12M 训练的模型的零样本分类精度。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;测试时研究prefix&lt;/h3>; &lt;p>; 下表描述了测试期间使用的前缀带来的性能变化。我们证明，通过使用与分类数据集相同的前缀（“提示”），分类数据集上的性能 (&lt;a href=&quot;https://www.image-net.org/&quot;>;IN-1K&lt;/ a>;) 改善。当使用与图像标题数据集（“Caption”）相同的前缀时，其他数据集（零样本 AVG）的性能会提高。该分析表明，如果前缀是针对图像标题数据集定制的，则可以更好地泛化场景类型和词汇。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6T uEjo7dhe9wMNMId_GhLHNu8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/s1200/image2.png “ style=”margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEKR_NiDSFulAI_WNnugarKESL4Fn3XiOAkpxXgnfhwseeUmJEdXYDiMmit-fzeucV1J_zHyI7vuWF2fMAX6TuEjo7dhe9wMNMId_GhLHNu 8NuxGsRihvJgp-IEt8AIPVhm-s3LxAiagKoXx5M5m4_dQysTUBD5928Vz9y616ZWjg7k93dU673ze7yPVJV/w640-h396/image2.png“宽度=“640”/>;&lt;/a>;&lt;/td>;&lt;/ tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;用于测试时间的前缀分析。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;图像鲁棒性研究分布变化 &lt;/h3>; &lt;p>; 我们使用 ImageNet 变体研究图像分布的变化。我们发现“Caption”前缀在 &lt;a href=&quot;https://github.com/hendrycks/imagenet-r&quot;>;ImageNet-R&lt;/a>; (IN-R) 和 &lt;a href=&quot;https://github.com/HaohanWang/ImageNet-Sketch&quot;>;ImageNet-Sketch&lt;/a>; (IN-S)，但在 &lt;a href=&quot;https://github.com/modestyachts/ImageNetV2 中表现不佳&quot;>;ImageNet-V2&lt;/a>; (IN-V2)。这表明“Caption”前缀实现了远离分类数据集的领域的泛化。因此，最佳前缀可能因测试域距分类数据集的距离而异。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfTb-mZqtDI06KnEyrbU32FzIPStwextAYMUk5CnhS89pPMrU14TH7_nzN-lzPw11KM4FrDpXC4KVyb xgVUvT-dEt7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MUXN/s1200 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;第396章7xJ0SrqOFQM_LFvELjhiBVMYiePAdpt337_9pBZV9EWQg2zPUaxksQglNNCkgFI66X6c1-Ws1CkRGN9Bu9zrni_U-THju4E5MuxN/w640-h396/image3.png&quot;宽度=“640”/>;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;图像级分布偏移的鲁棒性分析。 IN：ImageNet，IN-V2：ImageNet-V2，IN-R：艺术，卡通风格 ImageNet，IN-S：ImageNet 草图。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论和未来工作&lt; /h2>; &lt;p>; 我们引入前缀条件，这是一种统一图像标题和分类数据集以实现更好的零样本分类的技术。我们证明这种方法可以带来更好的零样本分类精度，并且前缀可以控制语言嵌入中的偏差。一个限制是在标题数据集上学习的前缀不一定对于零样本分类来说是最佳的。确定每个测试数据集的最佳前缀是未来工作的一个有趣方向。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项研究由 Kuniaki Saito、Kihyuk Sohn 进行、张翔、李春亮、李振宇、Kate Saenko 和 Tomas Pfister。感谢 Zizhao Zhu 和 Sergey Ioffe 提供的宝贵反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6036162561497757163/comments/default&quot; rel=&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html# comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel =&quot;编辑&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6036162561497757163&quot; rel=&quot;self&quot; type=&quot;application/ atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/unifying-image-caption-and-image.html&quot; rel=&quot;alternate&quot; title=&quot;统一图像标题和具有前缀调节的图像分类数据集” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog. com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEhy_hT7xaiysVeAL6MKhQiqMfun0uCX1GJX9atNr8QSg4BlXfPPxNtC7AD21LJ0FSHjSnZnrKgttj1kIkmQGPyO4ORWvPSROjxfTvV9IlaTG5ZJom9oKEwwocUGaj3E tiuUgEmKKLHpWpQCrmHe3BzJeDGzwvI1Oqet18qsCNzIc3DsNTCXWGjZ0uMW9-Bc/s72-c/Prefix%20conditioning%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>; &lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8471212041038116532&lt;/id>;&lt;发布>;2023-06-23T12: 24:00.001-07:00&lt;/发布>;&lt;更新>;2023-06-23T12:34:21.337-07:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger.com/atom/ns# &quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;强化学习&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;Systems&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;通过缓存驱逐的自动反馈进行偏好学习&lt;/stitle>;&lt;content type=&quot;html&quot; >;&lt;span class=&quot;byline-author&quot;>;发布者：Google 软件工程师 Ramki Gummadi 和 YouTube 软件工程师 Kevin Chen&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b /R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeCdR 0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s320/Halp%20hero%20gif.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>; 缓存是计算机科学中普遍存在的想法，它根据请求模式将流行项目的子集存储在更靠近客户端的位置，从而显着提高存储和检索系统的性能。缓存管理的一个重要算法部分是用于动态更新所存储的项目集的决策策略，该决策策略在几十年来已经得到了广泛的优化，导致了一些&lt;a href=&quot;https://en.wikipedia.org/wiki /Cache_replacement_policies&quot;>;高效且强大的启发式&lt;/a>;。虽然近年来将机器学习应用于缓存策略已显示出可喜的结果（例如，&lt;a href=&quot;https://www.usenix.org/conference/nsdi20/presentation/song&quot;>;LRB&lt;/a>;，&lt;a href =&quot;https://www.usenix.org/system/files/conference/nsdi18/nsdi18-beckmann.pdf&quot;>;LHD&lt;/a>;，&lt;a href=&quot;https://www.pdl.cmu.edu/ PDL-FTP/Storage/MLSys2021-zhou.pdf&quot;>;存储应用&lt;/a>;），以一种能够可靠地超越基准到生产设置的方式超越强大的启发式方法，同时保持有竞争力的计算和内存开销，仍然是一个挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c9ebaf640152028a6cdedb684577a7c9e52b6f10. pdf&quot;>;HALP：YouTube 内容分发网络的启发式辅助学习偏好驱逐政策&lt;/a>;”，在 &lt;a href=&quot;https://www.usenix.org/conference/nsdi23&quot;>;NSDI 2023&lt;/a>; 上提出，我们引入了一个可扩展的最先进的缓存驱逐框架，该框架基于学习奖励并使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;偏好学习&lt;/a>;自动反馈。启发式辅助学习偏好 (HALP) 框架是一种元算法，它使用随机化将轻量级启发式基线驱逐规则与学习奖励模型合并。奖励模型是一个轻量级神经网络，通过对偏好比较的持续自动反馈进行持续训练，旨在模仿离线模型甲骨文&lt;/a>;。我们讨论 HALP 如何提高 YouTube &lt;a href=&quot;https://en.wikipedia.org/wiki/Content_delivery_network&quot;>;内容交付网络&lt;/a>;的基础设施效率和用户视频播放延迟。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;学习到的缓存逐出决策偏好&lt;/h2>; &lt;p>; HALP 框架根据以下内容计算缓存逐出决策：包括两个组成部分：（1）通过偏好学习通过自动反馈训练的神经奖励模型，以及（2）将学习的奖励模型与快速启发式相结合的元算法。当缓存观察传入请求时，HALP 不断训练一个小型神经网络，通过 &lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_to_rank# 将其制定为偏好学习方法，预测每个项目的标量奖励Pairwise_approach&quot;>;成对&lt;/a>;偏好反馈。 HALP 的这一方面类似于&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning_from_ human_feedback&quot;>;从人类反馈中进行强化学习&lt;/a>; (RLHF) 系统，但有两个重要的区别：&lt;/p >; &lt;ul>; &lt;li>;反馈是&lt;em>;自动化的&lt;/em>;，并利用有关离线结构的众所周知的结果&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii /S0020019006003449&quot;>;最佳&lt;/a>;缓存逐出策略。 &lt;/li>;&lt;li>;该模型是使用从自动反馈过程构建的训练示例的瞬态缓冲区来连续学习的。 &lt;/li>; &lt;/ul>; &lt;p>; 驱逐决策依赖于包含两个步骤的过滤机制。首先，使用高效但性能欠佳的启发式方法选择一小部分候选者。然后，重新排序步骤通过少量使用神经网络评分函数从基线候选中进行优化，以“提高”最终决策的质量。 &lt;/p>; &lt;p>; 作为生产就绪的缓存策略实现，HALP 不仅做出逐出决策，还包含采样成对偏好查询的端到端过程，用于有效构建相关反馈并更新模型以支持逐出决定。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;神经奖励模型&lt;/h2>; &lt;p>; HALP 使用轻量级两层 &lt; href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;多层感知器&lt;/a>;（MLP）作为其奖励模型，有选择地对缓存中的各个项目进行评分。这些功能作为仅元数据的“幽灵缓存”进行构建和管理（类似于 &lt;a href=&quot;https://en.wikipedia.org/wiki/Adaptive_replacement_cache&quot;>;ARC&lt;/a>; 等经典策略）。在任何给定的查找请求之后，除了常规的缓存操作之外，HALP 还会进行更新动态内部表示所需的簿记（例如，跟踪和更新容量受限的键值存储中的特征元数据）。这包括：（1）由用户作为输入提供的外部标记特征以及缓存查找请求，以及（2）根据观察到的查找时间构建的内部构造的动态特征（例如，自上次访问以来的时间、访问之间的平均时间）每一个项目。 &lt;/p>; &lt;p>; HALP 从随机权重初始化开始完全在线学习其奖励模型。这似乎是一个坏主意，特别是如果决策专门用于优化奖励模型的话。然而，驱逐决策依赖于学习到的奖励模型和次优但简单而强大的启发式算法，例如 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU&quot;>;LRU&lt;/a>;。当奖励模型完全泛化时，这可以实现最佳性能，同时对尚未泛化的暂时无信息的奖励模型或在赶上不断变化的环境的过程中保持鲁棒性。 &lt;/p>; &lt;p>; 在线培训的另一个优势是专业化。每个缓存服务器都在可能不同的环境（例如地理位置）中运行，这会影响本地网络条件以及本地流行的内容等。与单一的离线训练解决方案不同，在线训练会自动捕获这些信息，同时减少泛化的负担。&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2 >;从随机优先级队列中对样本进行评分&lt;/h2>; &lt;p>; 使用专门学习的目标来优化驱逐决策的质量可能不切实际，原因有两个。 &lt;/p>; &lt;ol>; &lt;li>;计算效率限制：使用学习网络进行推理可能比大规模运行的实际缓存策略中执行的计算昂贵得多。这不仅限制了网络和功能的表达能力，还限制了每次驱逐决策期间调用这些功能的频率。 &lt;/li>;&lt;li>;泛化分布外的鲁棒性：HALP 部署在涉及持续学习的设置中，其中快速变化的工作负载可能会生成相对于之前看到的暂时分布外的请求模式数据。 &lt;/li>; &lt;/ol>; &lt;p>; 为了解决这些问题，HALP 首先应用一种廉价的启发式评分规则，该规则对应于驱逐优先级来识别小型候选样本。此过程基于高效的随机抽样，&lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;近似&lt;/a>;精确&lt;a href=&quot;https://en .wikipedia.org/wiki/Priority_queue&quot;>;优先级队列&lt;/a>;。用于生成候选样本的优先级函数旨在使用现有的手动调整算法（例如LRU）快速计算。然而，这可以通过编辑简单的成本函数来配置以近似其他缓存替换启发式。与&lt;a href=&quot;http://web.stanford.edu/~balaji/papers/01arandomized.pdf&quot;>;之前的工作&lt;/a>;不同，之前的工作使用随机化来权衡近似值与效率，HALP 还依赖于&lt;/em>; 跨时间步长的采样候选者的固有随机性，以提供必要的&lt;a href=&quot;https://lilianweng.github.io/posts/2020-06-07-exploration-drl/&quot;>;探索性&lt; /a>; 用于训练和推理的样本候选者的多样性。 &lt;/p>; &lt;p>; 最终被驱逐的项目是从提供的候选项目中选择的，相当于&lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;>;best-of-n&lt; /a>; 重新排序样本，对应于根据神经奖励模型最大化预测的偏好分数。用于驱逐决策的同一候选池也用于构建自动反馈的成对偏好查询，这有助于最大限度地减少样本之间的训练和推理偏差。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEE vfAA7aieN2cZjyMIm7_MVPockLfhOAvIp7HEiF60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s1470/Halp.gif&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;523&quot; data-original-width=&quot;1470&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgRd1ot8suIGs_IQQrywOjbrF8HfHevcQQX3x2GEJjJ3nI4NmPPCs346mSFAYLIzfUWPD2tvfLyvdY2divzEEvfAA7aieN2cZjyMIm7_MVPockLfhoOAvIp7HEi F60FSFv3zJx7iWd_7koIClmh6-JYB839Giekw6y0DYcafH2DezknagObjsM6FY8cpwE3_/s16000/Halp.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;每个驱逐决策调用的两阶段流程概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;通过自动反馈进行在线偏好学习&lt;/h2>; &lt;p>;奖励模型是使用在线反馈学习的，该反馈基于自动分配的偏好标签，这些标签表明，在可行的情况下，从每个查询的项目样本之间的给定时间快照开始，接收未来重新访问所花费的时间的排名偏好排序。这类似于预言机的最优策略，在任何给定时间，从缓存中的所有项目中驱逐具有最远未来访问权的项目。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3J lTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbVHH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s1200/image2.gif&quot; style=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgHuAHFUePtnBtdnqKixxgxJwHQrkRWDJEyVJz3Eve60gvMgRLeWE2o1Ton1IaGFCUBvH6e0yhWkoLzm3JlTAuAZkhPRZacl2JnOoWU5AkEd8lFi2tNlbV HH-XSxwAmatRIVpBCAjdmFjh7kXgN1Lk2G2RYbkC8_Ods_RcHwB10yhkq_bNUjGfoX9JIjsn_/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align : center;&quot;>;生成用于学习奖励模型的自动反馈。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; 为了使此反馈过程提供丰富的信息，HALP 构建了成对偏好最有可能与驱逐决定相关的查询。与通常的缓存操作同步，HALP 在做出每个逐出决策时发出少量成对偏好查询，并将它们附加到一组待处理的比较中。这些待定比较的标签只能在未来的随机时间解析。为了在线操作，HALP 还在每个查找请求之后执行一些额外的簿记，以处理可以在当前请求之后增量标记的任何待处理比较。 HALP 使用比较中涉及的每个元素对挂起的比较缓冲区进行索引，并回收过时的比较所消耗的内存（两者都无法重新访问），以确保与反馈生成相关的内存开销随着时间的推移保持有限。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS8 9UV1sxOsECA8H4PvInlUTLeB7X5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/s1468/image3.png&quot;样式=&quot;左边距：自动；右边距：自动；&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1032&quot; data-original-width=&quot;1468&quot; height=&quot;450&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia9IxIR9ONuZtOcHejyvhWJEwEAPb1xwfTC5eY7WC829Px39Kb6IwVkQKWFqM98IlWUpbSP4Vh6oS89UV1sxOsECA8H4PvInlUTLeB7X 5myDLUU_6AO5bBLRZeMqvOEjq5kCNLdS1AGd2lsS9i9fKanr8UsRASDKmCCoh7i-MQkuCcZuGrmgXDFmCBEHZu/w640-h450/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;HALP 中所有主要组件的概述。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot; line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果：对 YouTube CDN 的影响&lt;/h2>; &lt;p>; 通过实证分析，我们表明 HALP 与现有技术相比具有优势-art 缓存策略根据缓存未命中率对公共基准进行跟踪。然而，虽然公共基准是一个有用的工具，但它们很少足以捕获一段时间内世界各地的所有使用模式，更不用说我们已经部署的各种硬件配置了。 &lt;/p>; &lt;p>; 直到最近，YouTube 服务器还使用优化的 LRU 变体来进行内存缓存驱逐。 HALP 将 YouTube 的内存出口/入口（CDN 提供的总带宽出口与由于缓存未命中而用于检索（入口）消耗的带宽之比）增加了大约 12%，内存命中率提高了 6%。这减少了用户的延迟，因为内存读取比磁盘读取更快，并且还通过屏蔽磁盘流量来提高磁盘限制机器的出口容量。 &lt;/p>; &lt;p>; 下图显示了 &lt;a href=&quot;https://www.usenix.org/legacy/publications/library/proceedings/usits97/full_papers/cao/cao_html/node12.html 中引人注目的减少。 HALP 最终在 YouTube CDN 上推出后几天内的字节丢失率&lt;/a>;，现在可以从缓存中以更低的延迟向最终用户提供更多的内容，并且无需诉诸更昂贵的检索这增加了运营成本。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1 TL2wPMFShOcCQm8rfTRwa0CrNYhvDhrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s505/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;257&quot; data-original-width=&quot;505&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijgxJi0LBqVUqJ-JB4tw4XTSile4uoCD2akELZ4pXNtdjV2eYf7uxQuIZk2YVnnEnnSHh2Snjx3G-BPGRy4T1Tl2wPMFShOcCQm8rfTRwa0CrNYhvD hrlX_U8RbeC3AlaF6Fg3qgLMw8xeQCPZwS-YnxloUjIw7oXZzYBfmqURlsFmg0o2gxe5r9d5CIyW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;推出前后全球 YouTube 字节丢失率汇总（垂直虚线）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;总体性能改进仍然可能隐藏重要的回归。除了衡量总体影响之外，我们还在本文中进行了分析，以使用机器级分析来了解其对不同机架的影响，并发现其影响是极其积极的。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们引入了一种可扩展的最先进的缓存驱逐该框架基于学习奖励，并使用带有自动反馈的&lt;a href=&quot;https://en.wikipedia.org/wiki/Preference_learning&quot;>;偏好学习&lt;/a>;。由于其设计选择，HALP 可以以类似于任何其他缓存策略的方式部署，而无需单独管理标记示例、训练过程和模型版本作为大多数机器学习系统常见的附加离线管道的操作开销。因此，与其他经典算法相比，它只产生很小的额外开销，但具有能够利用附加功能来做出驱逐决策并不断适应不断变化的访问模式的额外好处。 &lt;/p>; &lt;p>; 这是首次将学习缓存策略大规模部署到广泛使用且流量大的 CDN 上，显着提高了 CDN 基础设施效率，同时为用户提供了更好的体验质量。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;Ramki Gummadi 现已加入 Google DeepMind。我们要感谢 John Guilyard 在插图方面提供的帮助，以及 Richard Schooler 对本文的反馈。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/ 8471212041038116532/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning -with-automated.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/8471212041038116532&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8471212041038116532&quot; rel=&quot; self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/preference-learning-with-automated.html&quot; rel=&quot;alternate&quot; title=&quot;通过自动反馈进行缓存驱逐的偏好学习” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>; &lt;电子邮件>;noreply@blogger.com&lt;/电子邮件>;&lt;gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog” .com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img /b/R29vZ2xl/AVvXsEjvw85_8jzAL-q7CGVpJ-qtMLcu0Zmi6d831BAsHj-33Mw6dF6SphPpL5Bhx13fAzZFtayquwlGdwhSLIvcktVm5Iu48JpNCNBueEbu-tJrs5tlJD5eS2qV0DeC dR0z9ppfVNFafEc3B-CcXg68mhybf2z458qG9hjTGtHPQ4tLOCF15GRcL4uiHTu5vIvL/s72-c/Halp%20hero%20gif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss /&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3798161588273442525&lt;/id>; &lt;发布>;2023-06-22T11:33:00.000-07:00&lt;/发布>;&lt;更新>;2023-06-22T11:33:53.578-07:00&lt;/更新>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;声学建模&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;音频&quot;>;&lt;/category >;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SoundStorm：高效并行音频生成&lt;/stitle>;&lt; content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究中心软件工程师 Zalán Borsos 和高级研究科学家 Marco Tagliasacchi&lt;/span>; &lt;img src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZa YunzxOsmQf8vZZYzCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s2400/SoundStorm.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 生成式人工智能的最新进展释放了在多个不同领域（包括文本、视觉和音频）创建新内容的可能性。这些模型通常依赖于这样一个事实：原始数据首先转换为压缩格式作为令牌序列。对于音频，神经音频编解码器（例如，&lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;SoundStream&lt;/a >; 或 &lt;a href=&quot;https://github.com/facebookresearch/encodec&quot;>;EnCodec&lt;/a>;）可以有效地将波形压缩为紧凑的表示形式，可以将其反转以重建原始音频信号的近似值。这种表示由一系列离散音频标记组成，捕获声音的本地属性（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Phoneme&quot;>;音素&lt;/a>;）及其时间结构（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Prosody_(linguistics)&quot;>;韵律&lt;/a>;）。通过将音频表示为一系列离散标记，可以使用 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a 来生成音频基于>;的序列到序列模型——这开启了语音延续的快速进展（例如，使用&lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to .html&quot;>;AudioLM&lt;/a>;），文本转语音（例如，使用 &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/ a>;），以及一般音频和音乐生成（例如，&lt;a href=&quot;https://felixkreuk.github.io/audiogen/&quot;>;AudioGen&lt;/a>; 和 &lt;a href=&quot;https://google-research .github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;）。许多生成音频模型，包括 AudioLM，都依赖于自动回归解码，它会一一生成令牌。虽然这种方法实现了高音质，但推理（即计算输出）可能很慢，尤其是在解码长序列时。 &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 要解决此问题，请在“&lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/”中>;SoundStorm：高效并行音频生成&lt;/a>;”，我们提出了一种高效、高质量音频生成的新方法。 SoundStorm 通过依赖两个新颖元素来解决生成长音频令牌序列的问题：1) 适应 SoundStream 神经编解码器生成的音频令牌特定性质的架构，以及 2) 受 &lt;a href=&quot; 启发的解码方案https://arxiv.org/abs/2202.04200&quot;>;MaskGIT&lt;/a>;，这是最近提出的一种图像生成方法，专门针对音频令牌进行操作。与AudioLM的自回归解码方法相比，SoundStorm能够并行生成token，从而将长序列的推理时间减少100倍，并产生相同质量的音频，并且在语音和声学条件下具有更高的一致性。此外，我们还展示了 SoundStorm 以及 &lt;a href=&quot;https://google-research.github.io/seanet/speartts/examples/&quot;>;SPEAR-TTS&lt;/a>; 的文本到语义建模阶段，可以合成高质量、自然的对话，允许控制语音内容（通过文字记录）、说话者声音（通过简短的语音提示）和说话者轮流（通过文字记录注释），如下例所示：&lt;/p>; &lt;br>; &lt;videocontrols=&quot;controls&quot;playsinline=&quot;&quot;style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/SoundStorm_promo.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/ video>; &lt;br>; &lt;br>; &lt;br>; &lt;table>; &lt;tr>; &lt;td>;&lt;b>;输入：文本&lt;em>;（用于驱动粗体音频生成的脚本）&lt;/em>;&lt;/b>; &lt;/ td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical-align: top&quot;>;&lt;span style=&quot;font-size:small;&quot;>;&lt;em>;一些东西今天早上发生在我身上真的很有趣。 |哦，哇，什么？ | &lt;b>;嗯，呃，我像往常一样醒来。 |呃呃|下楼去吃早餐。 |是啊|开始吃饭了。然后呃 10 分钟后我意识到现在是半夜了。 |哦，没办法，太有趣了！&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;vertical- align: top&quot;>;&lt;span style=&quot;font-size:small;&quot;>;&lt;em>;我昨晚没睡好。 |不好了。发生了什么？ | &lt;b>;我不知道。我就是无法入睡，整个晚上我都翻来覆去。 |这太糟糕了。也许你今晚应该尝试早点睡觉，或者你可以尝试读书。 |是的，谢谢你的建议，我希望你是对的。 |没问题。希望您睡个好觉&lt;/b>;&lt;/em>;&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt; td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;输入：音频提示&lt;/b>; &lt;/td >; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet/soundstorm/examples /data/dialogue/mp_joke_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github. io/seanet/soundstorm/examples/data/dialogue/jm_sleep_prompt.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;输出：音频提示+生成的音频&lt;/b >; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;audiocontrols=&quot;controls&quot; src=&quot;https://google-research.github.io/seanet /soundstorm/examples/data/dialogue/mp_joke_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;音频控制=“控制” “ src=&quot;https://google-research.github.io/seanet/soundstorm/examples/data/dialogue/jm_sleep_1.mp3&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;/table>; &lt;br>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;SoundStorm 设计&lt;/h2>; &lt;p>; 在我们之前关于 AudioLM 的工作中，我们展示了音频生成可以分解为两个部分步骤：1) 语义建模，从先前的语义标记或条件信号（例如，SPEAR-TTS 中的转录本，或 MusicLM 中的文本提示）生成语义标记，2) 声学建模，生成声学标记来自语义标记。通过 SoundStorm，我们专门解决了第二个声学建模步骤，用更快的并行解码取代了较慢的自回归解码。 &lt;/p>; &lt;p>; SoundStorm 依赖于基于双向注意力的 &lt;a href=&quot;https://arxiv.org/abs/2005.08100&quot;>;Conformer&lt;/a>;，这是一种将 Transformer 与卷积相结合来捕获数据的模型架构标记序列的局部和全局结构。具体来说，该模型经过训练，可以在给定 AudioLM 生成的语义标记序列作为输入的情况下预测 SoundStream 生成的音频标记。执行此操作时，重要的是要考虑到这样一个事实：在每个时间步 &lt;em>;t&lt;/em>;，SoundStream 使用最多 &lt;em>;Q&lt;/em>; 个标记来使用称为的方法来表示音频&lt;em>;残差矢量量化&lt;/em>; (RVQ)，如下右图所示。关键的直觉是，随着每一步生成的令牌数量从 1 到 &lt;em>;Q&lt;/em>;，重建音频的质量逐渐提高。 &lt;/p>; &lt;p>; 在推理时，给定语义标记作为输入调节信号，SoundStorm 首先屏蔽所有音频标记，并从 RVQ 级别的粗标记开始，通过多次迭代填充屏蔽标记 &lt;em>; q = 1&lt;/em>; 并使用更精细的标记逐级进行，直到达到 &lt;em>;q = Q&lt;/em>; 级别。 &lt;/p>; &lt;p>; SoundStorm 有两个关键方面可以实现快速生成：1) 在 RVQ 级别内的单次迭代期间并行预测令牌，2) 模型架构的设计方式使得复杂性仅受级别数 &lt;em>;Q&lt;/em>; 的轻微影响。为了支持这种推理方案，在训练期间，使用精心设计的掩蔽方案来模拟推理中使用的迭代过程。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOtw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7 zSt2d985P6OC9On0xDwrLfj9OzJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s1999/image2.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;951&quot; data-original-width=&quot;1999&quot; src=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjI75okNCRdLNSwU31uccP6wuIXVDxhhhvTI9f_V0ntiAxUOTw29TZeAvNFZXNBta9GraHB1bikM6zGQrFM7zSt2d985P6OC9On0xDwrLfj9O zJjdJ8BjrcYiAj_VGi3VQsqCIbNx_B7DP75KCU9D6xB_ybgHDpBk0z-eBkXXRdu9u04zcdzNCKGJuqszrR/s16000/image2.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot;样式=&quot;text-align: center;&quot;>;SoundStorm 模型架构。 &lt;em>;T&lt;/em>; 表示时间步数，&lt;em>;Q&lt;/em>; 表示 SoundStream 使用的 RVQ 级别数。用作调节的语义标记与 SoundStream 帧时间一致。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/ div>; &lt;h2>;测量 SoundStorm 性能&lt;/h2>; &lt;p>; 我们证明 SoundStorm 与 A​​udioLM 声学发生器的质量相匹配，取代了 AudioLM 的第二阶段（粗略声学模型）和第三阶段（精细声学模型）。此外，SoundStorm 生成音频的速度比 AudioLM 的分层自回归声音发生器（下图上半部分）快 100 倍，并且在说话者身份和声学条件方面具有匹配的质量和更高的一致性（下图下半部分）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZg YL6-0bZWo8aLWAuYQrOJPf_vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s1999/image1.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1122&quot; data-original-width=&quot;1999&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyhDn0uB3n05LdErUGRfuQdT2EPH7jk4Qx3qI0bddWvJOdUFRqXjtSNARMcb17sD_w3hcE4PL3OZgYL6-0bZWo8aLWAuYQrOJPf _vfc62rkTqAJF8r7mh1CZqzw6J8W1yQzhyAXDMvT_e1dONDEnXSY2WNSAsPevZbNSrheo_mDmMYIj5UpaM5ssHi6-6r/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;TPU-v4 上 SoundStream 解码、SoundStorm 和 AudioLM 不同阶段的运行时。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign =&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C- XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cpH9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCef Uib8dFmGUm/s1312/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1312&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjsY94J4ulCi2njoPV_ITBwcCH11blyjtUBMObmVqTj7C-XqxRMLYuX61_BZxZVmTSofRkt8NQ1DxwhuZhTT0XZw8oR8c9kgLhLO0IeL9xp6cWfshf1XH9k25j4cp H9fNn8C0gYHgJ8UW74tcT_jxzsYU0FtzyQeqC2b4T5JeF7xUYaeyuCefUib8dFmGUm/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;声学提示和生成的音频之间的一致性。阴影区域表示四分位数范围。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt; br>; &lt;/div>; &lt;h2>;安全和风险缓解&lt;/h2>; &lt;p>; 我们承认模型生成的音频样本可能会受到训练数据中存在的不公平偏见的影响，例如在所代表的口音方面在我们生成的样本中，我们证明了我们可以通过提示可靠且负责任地控制说话者的特征，以避免不公平的偏见。根据我们负责任的&lt;a href=&quot;http://ai.google/principles&quot;>;AI 原则&lt;/a>;，对任何训练数据及其局限性进行彻底分析是未来工作的一个领域。 &lt;/p>; &lt;p>; 反过来，模仿声音的能力可能有许多恶意应用，包括绕过生物识别和使用该模型进行模仿。因此，制定防止潜在滥用的保护措施至关重要：为此，我们已经验证了 SoundStorm 生成的音频仍然可以被专用分类器检测到，该专用分类器使用与我们原始 AudioLM 论文中描述的相同分类器。因此，作为一个更大系统的组件，我们相信 SoundStorm 不太可能给我们之前关于 AudioLM 和 SPEAR-TTS 的论文中讨论的风险带来额外的风险。与此同时，放宽 AudioLM 的内存和计算要求将使音频生成领域的研究更容易被更广泛的社区所接受。未来，我们计划探索其他方法来检测合成语音，例如借助音频水印，以便该技术的任何潜在产品使用都严格遵循我们负责任的人工智能原则。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们引入了SoundStorm，一个可以高效合成高质量的模型来自离散调节令牌的音频。与 &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; 的声音发生器相比，SoundStorm 是两个数量级在生成长音频样本时，幅度更快并实现更高的时间一致性。通过将类似于 &lt;a href=&quot;https://google-research.github.io/seanet/spearts/examples/&quot;>;SPEAR-TTS&lt;/a>; 的文本到语义标记模型与 SoundStorm 相结合，我们可以扩展文本到语音合成更长的上​​下文，并通过多个发言者轮流生成自然对话，控制发言者的声音和生成的内容。 SoundStorm 不仅限于生成语音。例如，MusicLM 使用 SoundStorm 高效地合成较长的输出（&lt;a href=&quot;https://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot;>;如 I/O 上所示&lt;/a>;）。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;此处描述的工作由 Zalán Borsos、Matt 创作谢里菲、达米安·文森特、尤金·哈里托诺夫、尼尔·泽吉杜尔和马可·塔利亚萨基。我们感谢 Google 同事对这项工作的所有讨论和反馈。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3798161588273442525 /comments/default&quot; rel=&quot;replies&quot; title=&quot;发布评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient- Parallel-audio.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/3798161588273442525&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3798161588273442525&quot; rel=&quot;self &quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/soundstorm-efficient-parallel-audio.html&quot; rel=&quot;alternate&quot; title=&quot;SoundStorm ：高效并行音频生成” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEhjNO7RwCdlB6y_dfklFN6uJAEWuV2K-dPXjXjoNVPT_x9MsARxpLNLj5IsTj666uo_9avS2dlxmz8cvZaKPj_dLy3IR6Jn6vLRXviPZaYunzxOsmQf8vZZY zCWQwq_CSmjIv3f1OLLuI6fvayDGUlgU7jLpcivR5us6eayttLYbjFugvqP-j2pcsK2Utdy/s72-c/SoundStorm.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-3542229559799425219&lt;/id>;&lt;发布>;2023-06-21T13:57:00.004- 07:00&lt;/发布>;&lt;更新>;2023-06-22T11:02:35.392-07:00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=可访问性&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category schema=&quot;http://www .blogger.com/atom/ns#&quot; term=&quot;自动语音识别&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT 亮点&quot; >;&lt;/category>;&lt;title type=&quot;text&quot;>;Google 研究的负责任的人工智能：人工智能造福社会&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Jimmy Tobin 和Katrin Tomanek，软件工程师，Google 研究部，AI 社会公益&lt;/span>; &lt;p>; Google 的 &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI 社会公益&lt;/a>;团队由研究人员、工程师、志愿者和其他共同关注积极社会影响的人员组成。我们的使命是通过实现现实世界价值来展示人工智能的社会效益，项目涵盖&lt;a href=&quot;https://blog.google/technology/health/making-data-useful-public-health/&quot;>;公共领域健康&lt;/a>;、&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/making-android-more-accessible/&quot;>;辅助功能&lt;/a>;、&lt;a href=&quot;https:// sites.research.google/floodforecasting/&quot;>;危机应对&lt;/a>;、&lt;a href=&quot;https://sustainability.google/&quot;>;气候和能源&lt;/a>;以及&lt;a href=&quot;https:// blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;自然与社会&lt;/a>;。我们相信，在服务不足的社区推动积极变革的最佳方式是与变革者及其服务的组织合作。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在这篇博文中，我们讨论 &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;项目所做的工作Euphonia&lt;/a>; 是 &lt;a href=&quot;https://ai.google/responsibility/social-good/&quot;>;AI for Social Good&lt;/a>; 内的一个团队，旨在&lt;a href=&quot;https:/ /ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html&quot;>;改进针对言语障碍人士的自动语音识别&lt;/a>; (ASR)。对于具有典型语音的人来说，ASR 模型的单词错误率 (WER) 可以低于 10%。但对于言语模式紊乱的人，例如口吃，&lt;a href=&quot;https://www.mayoclinic.org/diseases-conditions/dysarthria/symptoms-causes/syc-20371994&quot;>;构音障碍&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Apraxia&quot;>;失用症&lt;/a>;，根据病因和严重程度，WER 可能达到 50% 甚至 90%。为了帮助解决这个问题，我们与 1,000 多名参与者合作，&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-euphonia-1000-hours-speech-recordings/&quot;>;收集了 1,000 多个数小时的无序语音样本&lt;/a>;，并使用这些数据显示 ASR &lt;a href=&quot;https://ai.googleblog.com/2021/09/personalized-asr-models-from-large-and.html&quot; >;个性化&lt;/a>;是弥补言语障碍用户性能差距的可行途径。我们已经证明，使用 &lt;a href=&quot;https://arxiv.org/abs/2110.04612&quot;>;3-4 分钟的培训演讲&lt;/a>;即可成功实现个性化https://arxiv.org/abs/1907.13511&quot;>;层冻结技术&lt;/a>;。 &lt;/p>; &lt;p>; 这项工作促成了 &lt;a href=&quot;https://sites.research.google/relate/&quot;>;Project Relate&lt;/a>; 的开发，任何具有非典型言语的人都可以从中受益个性化语音模型。 Project Relate 与 &lt;a href=&quot;https://research.google/research-areas/speech-processing/&quot;>;Google 语音团队&lt;/a>;合作构建，帮助那些难以被他人理解的人，技术来训练自己的模型。人们可以使用这些个性化模型来更有效地沟通并获得更多的独立性。为了使 ASR 更易于访问和使用，我们介绍了如何微调 Google 的 &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html &quot;>;通用语音模型&lt;/a>; (USM) 可更好地理解开箱即用的无序语音，无需个性化，可与数字助理技术、听写应用程序和对话中使用。 &lt;/p>; &lt;br />; &lt;h2>;应对挑战&lt;/h2>; &lt;p>; 通过与 Project Relate 用户密切合作，我们发现个性化模型非常有用，但对于许多用户来说，记录数十或数百个示例可能具有挑战性。此外，个性化模型在自由对话中并不总是表现良好。 &lt;/p>; &lt;p>; 为了应对这些挑战，Euphonia 的研究工作一直集中在&lt;em>;与说话人无关&lt;/em>;的 ASR (SI-ASR) 上，以使模型能够更好地为言语障碍者提供开箱即用的效果，以便无需额外培训。 &lt;/p>; &lt;br />; &lt;h2>;SI-ASR 提示语音数据集&lt;/h2>; &lt;p>; 构建稳健的 SI-ASR 模型的第一步是创建代表性数据集分割。 We created the Prompted Speech dataset by splitting the &lt;a href=&quot;https://www.isca-speech.org/archive/pdfs/interspeech_2021/macdonald21_interspeech.pdf&quot;>;Euphonia corpus&lt;/a>; into train, validation and test portions, while ensuring that each split spanned a range of speech impairment severity and underlying etiology and that no speakers or phrases appeared in multiple splits. The training portion consists of over 950k speech utterances from over 1,000 speakers with disordered speech. The test set contains around 5,700 utterances from over 350 speakers. Speech-language pathologists manually reviewed all of the utterances in the test set for transcription accuracy and audio quality. &lt;/p>; &lt;br />; &lt;h2>;Real Conversation test set&lt;/h2>; &lt;p>; Unprompted or conversational speech differs from prompted speech in several ways. In conversation, people speak faster and enunciate less. They repeat words, repair misspoken words, and use a more expansive vocabulary that is specific and personal to themselves and their community. To improve a model for this use case, we created the Real Conversation test set to benchmark performance. &lt;/p>; &lt;p>; The Real Conversation test set was created with the help of trusted testers who recorded themselves speaking during conversations. The audio was reviewed, any personally identifiable information (PII) was removed, and then that data was transcribed by speech-language pathologists. The Real Conversation test set contains over 1,500 utterances from 29 speakers. &lt;/p>; &lt;br />; &lt;h2>;Adapting USM to disordered speech&lt;/h2>; &lt;p>; We then tuned USM on the training split of the Euphonia Prompted Speech set to improve its performance on disordered speech. Instead of fine-tuning the full model, our tuning was based on &lt;a href=&quot;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&quot;>;residual adapters&lt;/a>;, a parameter-efficient tuning approach that adds tunable bottleneck layers as residuals between the transformer layers. Only these layers are tuned, while the rest of the model weights are untouched. We have &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.541/&quot;>;previously shown&lt;/a>; that this approach works very well to adapt ASR models to disordered speech. Residual adapters were only added to the encoder layers, and the bottleneck dimension was set to 64. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; To evaluate the adapted USM, we compared it to older ASR models using the two test sets described above. For each test, we compare adapted USM to the pre-USM model best suited to that task: (1) For short prompted speech, we compare to Google&#39;s production ASR model optimized for short form ASR; (2) for longer Real Conversation speech, we compare to a model &lt;a href=&quot;https://arxiv.org/abs/2005.03271&quot;>;trained for long form ASR&lt;/a>;. USM improvements over pre-USM models can be explained by USM&#39;s relative size increase, 120M to 2B parameters, and other improvements discussed in the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;USM blog post&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model word error rates (WER) for each test set (lower is better).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see that the USM adapted with disordered speech significantly outperforms the other models. The adapted USM&#39;s WER on Real Conversation is 37% better than the pre-USM model, and on the Prompted Speech test set, the adapted USM performs 53% better. &lt;/p>; &lt;p>; These findings suggest that the adapted USM is significantly more usable for an end user with disordered speech. We can demonstrate this improvement by looking at transcripts of Real Conversation test set recordings from a trusted tester of Euphonia and Project Relate (see below). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan=&quot;7&quot;>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;Audio&lt;/b>;&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;1&lt;/span>;&lt;/a>;&lt;/sup>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Ground Truth&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Pre-USM ASR&lt;/b>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;b>;Adapted USM&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample1.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I now have an Xbox adaptive controller on my lap.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now have &lt;i>;&lt;b>;a lot and that consultant&lt;/b>;&lt;/i>; on my &lt;i>;&lt;b>;mouth&lt;/b>;&lt;/i>;&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i now &lt;i>;&lt;b>;had&lt;/b>;&lt;/i>; an xbox &lt;i>;&lt;b>;adapter&lt;/b>;&lt;/i>; controller on my &lt;i>;&lt;b>;lamp&lt;/b>;&lt;/i>;.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google.github.io/euphonia/2023-06-05-audio-sample2.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;I&#39;ve been talking for quite a while now. Let&#39;s see.&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;quite a while now&lt;/span>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;span style=&quot;font-size: small;&quot;>;i&#39;ve been talking for quite a while now.&lt;/span>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; -->; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example audio and transcriptions of a trusted tester&#39;s speech from the Real Conversation test set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A comparison of the Pre-USM and adapted USM transcripts revealed some key advantages: &lt;/p>; &lt;ul>;&lt;li>; The first example shows that Adapted USM is better at recognizing disordered speech patterns. The baseline misses key words like “XBox” and “controller” that are important for a listener to understand what they are trying to say. &lt;/li>; &lt;li>; The second example is a good example of how deletions are a primary issue with ASR models that are not trained with disordered speech. Though the baseline model did transcribe a portion correctly, a large part of the utterance was not transcribed, losing the speaker&#39;s intended message. &lt;/li>;&lt;/ul>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that this work is an important step towards making speech recognition more accessible to people with disordered speech. We are continuing to work on improving the performance of our models. With the rapid advancements in ASR, we aim to ensure people with disordered speech benefit as well. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Key contributors to this project include Fadi Biadsy, Michael Brenner, Julie Cattiau, Richard Cave, Amy Chung-Yu Chou, Dotan Emanuel, Jordan Green, Rus Heywood, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Bob MacDonald, Philip Nelson, Katie Seaver, Joel Shor, Jimmy Tobin, Katrin Tomanek, and Subhashini Venugopalan. We gratefully acknowledge the support Project Euphonia received from members of the USM research team including Yu Zhang, Wei Han, Nanxin Chen, and many others. Most importantly, we wanted to say a huge thank you to the 2,200+ participants who recorded speech samples and the many &lt;a href=&quot;https://sites.research.google/euphonia/about/#thank-partners&quot;>;advocacy groups&lt;/a>; who helped us connect with these participants.&lt;/em>; &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;1&lt;/b>;&lt;/a>;&lt;/sup>;Audio volume has been adjusted for ease of listening, but the original files would be more consistent with those used in training and would have pauses, silences, variable volume, etc.&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3542229559799425219/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3542229559799425219&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/responsible-ai-at-google-research-ai.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: AI for Social Good&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHxJe3sno8uMxxn3EfeK7RHGLZArqkHyOFhvOqt00___7W-jf_KWEpN5lTyGx8pVHSvaSa5NOnWZRy-7AGQZU0A05GlAPyQF5sdqyt4kbseLwR5LNMhKX8mx3MvonGGJKAssz8rlwqiUBv7ljUi9exqbFLE3Yq7yy8eyGTQCK4iIt4BuLvm-3jrC_rUt2G/s72-w640-h396-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4041992163804186827&lt;/id>;&lt;published>;2023-06-21T10:29:00.000-07:00&lt;/published>;&lt;updated>;2023-06-21T10:29:53.804-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;The world&#39;s first braiding of non-Abelian anyons&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Trond Andersen and Yuri Lensky, Research Scientists, Google Quantum AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s320/Non-Abelian%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Imagine you&#39;re shown two identical objects and then asked to close your eyes. When you open your eyes, you see the same two objects in the same position. How can you determine if they have been swapped back and forth? Intuition and the laws of &lt;a href=&quot;https://en.wikipedia.org/wiki/Identical_particles#Quantum_mechanical_description_of_identical_particles&quot;>;quantum mechanics agree&lt;/a>;: If the objects are truly identical, there is no way to tell. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While this sounds like common sense, it only applies to our familiar three-dimensional world. Researchers have predicted that for a special type of particle, called an &lt;a href=&quot;https://en.wikipedia.org/wiki/Anyon&quot;>;anyon&lt;/a>;, that is restricted to move only in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Plane_(mathematics)&quot;>;two-dimensional&lt;/a>; (2D) plane, quantum mechanics allows for something quite different. Anyons are indistinguishable from one another and some, non-Abelian anyons, have a special property that causes observable differences in the shared quantum state under exchange, making it possible to tell when they have been exchanged, despite being fully indistinguishable from one another. While researchers have managed to detect their relatives, Abelian anyons, whose change under exchange is more subtle and impossible to directly detect, realizing “non-Abelian exchange behavior” has proven more difficult due to challenges with both control and detection. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05954-4&quot;>;Non-Abelian braiding of graph vertices in a superconducting processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we report the observation of this non-Abelian exchange behavior for the first time. Non-Abelian anyons could open a new avenue for quantum computation, in which quantum operations are achieved by swapping particles around one another like strings are swapped around one another to create braids. Realizing this new exchange behavior on our &lt;a href=&quot;https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html&quot;>;superconducting quantum processor&lt;/a>; could be an alternate route to so-called &lt;a href=&quot;https://arxiv.org/abs/quant-ph/9707021&quot;>;topological quantum computation&lt;/a>;, which benefits from being robust against environmental noise. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Exchange statistics and non-Abelian anyons&lt;/h2>; &lt;p>; In order to understand how this strange non-Abelian behavior can occur, it&#39;s helpful to consider an analogy with the braiding of two strings. Take two identical strings and lay them parallel next to one another. Swap their ends to form a double-helix shape. The strings are identical, but because they wrap around one another when the ends are exchanged, it is very clear when the two ends are swapped. &lt;/p>; &lt;p>; The exchange of non-Abelian anyons can be visualized in a similar way, where the strings are made from extending the particles&#39; positions into the time dimension to form “world-lines.” Imagine plotting two particles&#39; locations vs. time. If the particles stay put, the plot would simply be two parallel lines, representing their constant locations. But if we exchange the locations of the particles, the world lines wrap around one another. Exchange them a second time, and you&#39;ve made a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knot_theory&quot;>;knot&lt;/a>;. &lt;/p>; &lt;p>; While a bit difficult to visualize, knots in four dimensions (three spatial plus one time dimension) can always easily be undone. They are trivial — like a shoelace, simply pull one end and it unravels. But when the particles are restricted to two spatial dimensions, the knots are in three total dimensions and — as we know from our everyday 3D lives — cannot always be easily untied. The braiding of the non-Abelian anyons&#39; world lines can be used as quantum computing operations to transform the state of the particles. &lt;/p>; &lt;p>; A key aspect of non-Abelian anyons is “degeneracy”: the full state of several separated anyons is not completely specified by local information, allowing the same anyon configuration to represent superpositions of several quantum states. Winding non-Abelian anyons about each other can change the encoded state. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;How to make a non-Abelian anyon&lt;/h2>; &lt;p>; So how do we realize non-Abelian braiding with one of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sycamore_processor&quot;>;Google&#39;s quantum processors&lt;/a>;? We start with the familiar surface code, which we recently used to achieve a &lt;a href=&quot;https://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;milestone in quantum error correction&lt;/a>;, where qubits are arranged on the vertices of a checkerboard pattern. Each color square of the checkerboard represents one of two possible joint measurements that can be made of the qubits on the four corners of the square. These so-called “stabilizer measurements” can return a value of either + or – 1. The latter is referred to as a plaquette violation, and can be created and moved diagonally — just like bishops in chess — by applying single-qubit &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_logic_gate&quot;>;X- and Z-gates&lt;/a>;. Recently, we showed that these bishop-like &lt;a href=&quot;https://arxiv.org/abs/2104.01180&quot;>;plaquette violations are Abelian anyons&lt;/a>;. In contrast to non-Abelian anyons, the state of Abelian anyons changes only subtly when they are swapped — so subtly that it is impossible to directly detect. While Abelian anyons are interesting, they do not hold the same promise for topological quantum computing that non-Abelian anyons do. &lt;/p>; &lt;p>; To produce non-Abelian anyons, we need to control the degeneracy (ie, the number of &lt;a href=&quot;https://en.wikipedia.org/wiki/Wave_function&quot;>;wavefunctions&lt;/a>; that causes all &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;stabilizer&lt;/a>; measurements to be +1). Since a stabilizer measurement returns two possible values, each stabilizer cuts the degeneracy of the system in half, and with sufficiently many stabilizers, only one wave function satisfies the criterion. Hence, a simple way to increase the degeneracy is to merge two stabilizers together. In the process of doing so, we remove one edge in the stabilizer grid, giving rise to two points where only three edges intersect. These points, referred to as “&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0003491623000714&quot;>;degree-3 vertices&lt;/a>;” (D3Vs), are predicted to be non-Abelian anyons. &lt;/p>; &lt;p>; In order to braid the D3Vs, we have to move them, meaning that we have to stretch and squash the stabilizers into new shapes. We accomplish this by implementing two-qubit gates between the anyons and their neighbors (middle and right panels shown below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/s661/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;661&quot; height=&quot;529&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ99lWo6CdQ-N48W-wd2x0JV6dHLQ4OVheTvZEkxoYQlbYFarf-QdPajCIDezqKSCDw7y0jkbIxP4ZfzKP6A3I3bCocy7aWfU6ELz2XRDOCF_Kel7wzAwuMieiKhUC4IUAlScBOtuiKBKn8zjeu7UqtDK0oUD6OKpb2Zw3BkUEbxwfHI2-Sm913QYnbQIl/w640-h529/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Non-Abelian anyons in stabilizer codes.&lt;strong>; a:&lt;/strong>; Example of a knot made by braiding two anyons&#39; world lines. &lt;strong>;b: &lt;/strong>;Single-qubit gates can be used to create and move stabilizers with a value of –1 (red squares). Like bishops in chess, these can only move diagonally and are therefore constrained to one sublattice in the regular surface code. This constraint is broken when D3Vs (yellow triangles) are introduced.&lt;strong>; c: &lt;/strong>;Process to form and move D3Vs (predicted to be non-Abelian anyons). We start with the surface code, where each square corresponds to a joint measurement of the four qubits on its corners (&lt;strong>;left&lt;/strong>; panel). We remove an edge separating two neighboring squares, such that there is now a single joint measurement of all six qubits (&lt;strong>;middle&lt;/strong>; panel). This creates two D3Vs, which are non-Abelian anyons. We move the D3Vs by applying two-qubit gates between neighboring sites (right panel). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now that we have a way to create and move the non-Abelian anyons, we need to verify their anyonic behavior. For this we examine three characteristics that would be expected of non-Abelian anyons: &lt;/p>; &lt;ol>; &lt;li>;The “&lt;a href=&quot;https://en.wikipedia.org/wiki/Fusion_of_anyons&quot;>;fusion rules&lt;/a>;” — What happens when non-Abelian anyons collide with each other? &lt;/li>;&lt;li>;Exchange statistics — What happens when they are braided around one another? &lt;/li>;&lt;li>;Topological quantum computing primitives — Can we encode qubits in the non-Abelian anyons and use braiding to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_entanglement&quot;>;two-qubit entangling operations&lt;/a>;? &lt;/li>; &lt;/ol>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The fusion rules of non-Abelian anyons&lt;/h2>; &lt;p>; We investigate fusion rules by studying how a pair of D3Vs interact with the bishop-like plaquette violations introduced above. In particular, we create a pair of these and bring one of them around a D3V by applying single-qubit gates. &lt;/p>; &lt;p>; While the rules of bishops in chess dictate that the plaquette violations can never meet, the dislocation in the checkerboard lattice allows them to break this rule, meet its partner and annihilate with it. The plaquette violations have now disappeared! But bring the non-Abelian anyons back in contact with one another, and the anyons suddenly morph into the missing plaquette violations. As weird as this behavior seems, it is a manifestation of exactly the fusion rules that we expect these entities to obey. This establishes confidence that the D3Vs are, indeed, non-Abelian anyons. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/s982/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;826&quot; data-original-width=&quot;982&quot; height=&quot;538&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVr2cI-Jm0BIC0VMh6xqp1y57itfmrtbQIlWKDAcXedpOkUaw4I8wenWzibPK56yKrs_eiXeEby86hhxpk-OfAa0zF5De9nQyCKH91CwipepczUmQRQZ8URqP_GdmCposkLXF1_P_AeEoKoHZU9oEN3FDkfxygwvYxICCIVzp8eKheaOV1Vv7CygkKMorp/w640-h538/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Demonstration of anyonic fusion rules (starting with &lt;strong>;panel I&lt;/strong>;, in the &lt;strong>;lower left&lt;/strong>;). We form and separate two D3Vs (yellow triangles), then form two adjacent plaquette violations (red squares) and pass one between the D3Vs. The D3Vs deformation of the “chessboard” changes the bishop rules of the plaquette violations. While they used to lie on adjacent squares, they are now able to move along the same diagonals and collide (as shown by the red lines). When they do collide, they annihilate one another. The D3Vs are brought back together and surprisingly morph into the missing adjacent red plaquette violations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Observation of non-Abelian exchange statistics&lt;/h2>; &lt;p>; After establishing the fusion rules, we want to see the real smoking gun of non-Abelian anyons: non-Abelian exchange statistics. We create two pairs of non-Abelian anyons, then braid them by wrapping one from each pair around each other (shown below). When we fuse the two pairs back together, two pairs of plaquette violations appear. The simple act of braiding the anyons around one another changed the observables of our system. In other words, if you closed your eyes while the non-Abelian anyons were being exchanged, you would still be able to tell that they had been exchanged once you opened your eyes. This is the hallmark of non-Abelian statistics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/s1648/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1648&quot; height=&quot;437&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTusKBb00UTtegVn0QYPH0t75ydqFU_w4O0VFyolp94oU6VsZFWYuYdLkO2YmBZv8oFUX4eNnYD2LKWRdPsWfGEuio561MVdMI0aDYRV79XA-c2MpDnbYjFVWMEHAMofqJfbhO-nFqrKdciz0_syJ3PKobTaF9M30RDBSBOdbjSLBjR6K91QynZ9rIgLP9/w640-h437/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Braiding non-Abelian anyons. We make two pairs of D3Vs (&lt;strong>;panel II&lt;/strong>;), then bring one from each pair around each other (&lt;strong>;III-XI&lt;/strong>;). When fusing the two pairs together again in panel XII, two pairs of plaquette violations appear! Braiding the non-Abelian anyons changed the observables of the system from panel I to panel XII; a direct manifestation of non-Abelian exchange statistics.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Topological quantum computing&lt;/h2>; &lt;p>; Finally, after establishing their fusion rules and exchange statistics, we demonstrate how we can use these particles in quantum computations. The non-Abelian anyons can be used to encode information, represented by &lt;a href=&quot;https://ai.googleblog.com/2021/08/demonstrating-fundamentals-of-quantum.html&quot;>;logical qubits&lt;/a>;, which should be distinguished from the actual &lt;em>;physical&lt;/em>; qubits used in the experiment. The number of logical qubits encoded in &lt;em>;N&lt;/em>; D3Vs can be shown to be &lt;em>;N&lt;/em>;/2–1, so we use &lt;em>;N&lt;/em>;=8 D3Vs to encode three logical qubits, and perform braiding to entangle them. By studying the resulting state, we find that the braiding has indeed led to the formation of the desired, well-known quantum entangled state called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state&quot;>;Greenberger-Horne-Zeilinger&lt;/a>; (GHZ) state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/s750/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;544&quot; data-original-width=&quot;750&quot; height=&quot;464&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioMJAduqoUqd7EiOR6VNEBBuA8ZMgqemi3QLYCHkcBx6jqkBgwMGwReSPOdHpWqA3r2biu90nqHC5TH4_4H-6NMdn-jJG-6aFPeK-tEaVwtZKjaxOPlFQqW01jkJfVkVt6esIBHDndHIf4PI2XHeylfsgnX_cGiQPvT2TUJhGcCxpWEs3KC8KF2m2eXSTf/w640-h464/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using non-Abelian anyons as logical qubits.&lt;strong>; a, &lt;/strong>;We braid the non-Abelian anyons to entangle three qubits encoded in eight D3Vs. &lt;strong>;b, &lt;/strong>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_tomography&quot;>;Quantum state tomography&lt;/a>; allows for reconstructing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Density_matrix&quot;>;density matrix&lt;/a>;, which can be represented in a 3D bar plot and is found to be consistent with the desired highly entangled GHZ-state.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments show the first observation of non-Abelian exchange statistics, and that braiding of the D3Vs can be used to perform quantum computations. With future additions, including error correction during the braiding procedure, this could be a major step towards topological quantum computation, a long-sought method to endow qubits with intrinsic resilience against fluctuations and noise that would otherwise cause errors in computations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4041992163804186827/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4041992163804186827&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/the-worlds-first-braiding-of-non.html&quot; rel=&quot;alternate&quot; title=&quot;The world&#39;s first braiding of non-Abelian anyons&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuDMQeKcXVKtB_qxFz6L07TTk0j2ApLBEtpsiJa6uaYbztMmcj54TkI7rf3E_v1CJlouSS009__3lA0pALZadBgQTUNiflWqgEFTAg4pu9qF-aPhgxTy5yUayghhd1Yd__fJMMJBIbj6hRIEHZCeiRd9nN5qbs3-zQ_WgAGaXWuQ7OMrR_uY4GU42m1SsH/s72-c/Non-Abelian%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-9139300663122353070&lt;/id>;&lt;published>;2023-06-18T11:00:00.018-07:00&lt;/published>;&lt;updated>;2023-06-19T10:54:26.149-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at CVPR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shaina Mehta, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s1200/CVPR%20Design-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week marks the beginning of the premier annual &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;Computer Vision and Pattern Recognition&lt;/a>; conference (CVPR 2023), held in-person in Vancouver, BC (with additional virtual content). As a leader in computer vision research and a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Sponsors&quot;>;Platinum Sponsor&lt;/a>;, &lt;a href=&quot;https://research.google/&quot;>;Google Research&lt;/a>; will have a strong presence across CVPR 2023 with ~90 papers being presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers&quot;>;main conference&lt;/a>; and active involvement in over 40 conference &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/workshop-list&quot;>;workshops&lt;/a>; and &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/tutorial-list&quot;>;tutorials&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you are attending CVPR this year, please stop by our booth to chat with our researchers who are actively exploring the latest techniques for application to various areas of &lt;a href=&quot;https://research.google/pubs/?area=machine-perception&quot;>;machine perception&lt;/a>;. Our researchers will also be available to talk about and demo several recent efforts, including on-device ML applications with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;, strategies for differential privacy, neural radiance field technologies and much more. &lt;/p>; &lt;p>; You can also learn more about our research being presented at CVPR 2023 in the list below (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>; Board and organizing committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Senior area chairs include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Area chairs include: &lt;em>;&lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Rodrigo Benenson&lt;/strong>;, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Da-Cheng Jua&lt;/strong>;, &lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Stephen Lombardi&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>;, &lt;strong>;Saurabh Singh&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Pratul P. Srinivasan&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, &lt;strong>;Jasper Uijlings&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Publicity Chair: &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Demonstration Chair: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Program Advisory Board includes: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Panels&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>;&lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;History and Future of Artificial Intelligence and Computer Vision &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>;&lt;/div>; &lt;p>; &lt;/p>; &lt;a href=&quot;https://cvpr2023.thecvf.com/virtual/2023/eventlistwithbios/2023KeynotesPanels&quot;>;Scientific Discovery and the Environment &lt;/a>; &lt;div style=&quot;margin-left: 20px;&quot;>; Panelists include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;/div>; &lt;/div>; &lt;br />; &lt;h2>;Best Paper Award candidates&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.00277.pdf&quot;>;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhiqin Chen&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11082.pdf&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Qianqian Wang&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Richard Tucker&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.12242.pdf&quot;>;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/a>; &lt;br />; &lt;em>;Nataniel Ruiz*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Kfir Aberman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03142.pdf&quot;>;On Distillation of Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Chenlin Meng, Robin Rombach,&lt;strong>; Ruiqi Gao&lt;/strong>;,&lt;strong>; Diederik Kingma&lt;/strong>;, Stefano Ermon,&lt;strong>; Jonathan Ho&lt;/strong>;,&lt;strong>; Tim Salimans&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Highlight papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.11217.pdf&quot;>;Connecting Vision and Language with Video Localized Narratives&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Paul Voigtlaender&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Vittorio Ferrari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.05496.pdf&quot;>;MaskSketch: Unpaired Structure-Guided Masked Image Generation&lt;/a>; &lt;br />; &lt;em>;Dina Bashkirova*,&lt;strong>; Jose Lezama&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11738.pdf&quot;>;SPARF: Neural Radiance Fields from Sparse and Noisy Poses&lt;/a>; &lt;br />; &lt;em>;Prune Truong*,&lt;strong>; Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05199.pdf&quot;>;MAGVIT: Masked Generative Video Transformer&lt;/a>; &lt;br />; &lt;em>;Lijun Yu*,&lt;strong>; Yong Cheng&lt;/strong>;, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Alexander Hauptmann&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf&quot;>;Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Dahun Kim&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.02291.pdf&quot;>;I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification&lt;/a>; &lt;br />; &lt;em>;Muhammad Ferjad Naeem, Gul Zain Khan,&lt;strong>; Yongqin Xian&lt;/strong>;, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool,&lt;strong>; Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.12624.pdf&quot;>;Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization&lt;/a>; &lt;br />; &lt;em>;Zifan Wang*,&lt;strong>; Nan Ding&lt;/strong>;, &lt;strong>;Tomer Levinboim&lt;/strong>;, &lt;strong>;Xi Chen&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Su Wang&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Ceslee Montgomery&lt;/strong>;, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, &lt;strong>;Shai Noy&lt;/strong>;, &lt;strong>;Stefano Pellegrini&lt;/strong>;, &lt;strong>;Yasumasa Onoe&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Radu Soricut&lt;/strong>;, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;William Cha&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14306.pdf&quot;>;RUST: Latent Neural Scene Representations from Unposed Imagery&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mehdi SM Sajjadi&lt;/strong>;, &lt;strong>;Aravindh Mahendran&lt;/strong>;, &lt;strong>;Thomas Kipf&lt;/strong>;, &lt;strong>;Etienne Pot&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Mario Lučić&lt;/strong>;, &lt;strong>;Klaus Greff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05221.pdf&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-training with Multi-Source Multimodal Knowledge Memory&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Ziniu Hu*,&lt;strong>; Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Zirui Wang&lt;/strong>;, Kai-Wei Chang,&lt;strong>; Yizhou Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.00833.pdf&quot;>;RobustNeRF: Ignoring Distractors with Robust Losses&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Sara Sabour&lt;/strong>;, &lt;strong>;Suhani Vora&lt;/strong>;, &lt;strong>;Daniel Duckworth&lt;/strong>;, &lt;strong>;Ivan Krasin&lt;/strong>;, &lt;strong>;David J. Fleet&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09682.pdf&quot;>;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a>; &lt;br />; &lt;em>;Yifan Jiang*, &lt;strong>;Peter Hedman&lt;/strong>;, &lt;strong>;Ben Mildenhall&lt;/strong>;, Dejia Xu, &lt;strong>;Jonathan T. Barron&lt;/strong>;, Zhangyang Wang, Tianfan Xue*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.pdf&quot;>;BlendFields: Few-Shot Example-Driven Facial Modeling&lt;/a>; &lt;br />; &lt;em>;Kacper Kania, Stephan Garbin, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00583.pdf&quot;>;Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints&lt;/a>; &lt;br />; &lt;em>;Guilherme Potje, Felipe Cadar, &lt;strong>;Andre Araujo&lt;/strong>;, Renato Martins, Erickson Nascimento&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf&quot;>;How Can Objects Help Action Recognition?&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.12652.pdf&quot;>;Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur&lt;/a>; &lt;br />; &lt;em>;Peng Dai, &lt;strong>;Yinda Zhang&lt;/strong>;, Xin Yu, Xiaoyang Lyu, Xiaojuan Qi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14396.pdf&quot;>;IFSeg: Image-Free Semantic Segmentation via Vision-Language Model&lt;/a>; &lt;br />; &lt;em>;Sukmin Yun, Seong Park, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-Aware Saliency Modeling&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Shi Chen*, &lt;strong>;Nachiappan Valliappan&lt;/strong>;, &lt;strong>;Shaolei Shen&lt;/strong>;, &lt;strong>;Xinyu Ye&lt;/strong>;, &lt;strong>;Kai Kohlhoff&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09117.pdf&quot;>;MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis&lt;/a>; &lt;br />; &lt;em>;Tianhong Li*,&lt;strong>; Huiwen Chang&lt;/strong>;, Shlok Kumar Mishra,&lt;strong>; Han Zhang&lt;/strong>;, Dina Katabi,&lt;strong>; Dilip Krishnan&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17603.pdf&quot;>;NeRF-Supervised Deep Stereo&lt;/a>; &lt;br />; &lt;em>;Fabio Tosi, &lt;strong>;Alessio Tonioni&lt;/strong>;, Daniele Gregorio, Matteo Poggi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf&quot;>;Omnimatte3D: Associating Objects and their Effects in Unconstrained Monocular Video&lt;/a>; &lt;br />; &lt;em>;Mohammed Suhail, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, &lt;strong>;Noah Snavely&lt;/strong>;, Leon Sigal, &lt;strong>;Forrester Cole&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15654.pdf&quot;>;OpenScene: 3D Scene Understanding with Open Vocabularies&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Songyou Peng&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;, &lt;strong>;Chiyu Jiang&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Marc Pollefeys&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08504.pdf&quot;>;PersonNeRF: Personalized Reconstruction from Photo Collections&lt;/a>; &lt;br />; &lt;em>;Chung-Yi Weng,&lt;strong>; Pratul Srinivasan&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf&quot;>;Prefix Conditioning Unifies Language and Label Supervision&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*, &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, Kate Saenko, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;AJ Piergiovanni&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01194.pdf&quot;>;Burstormer: Burst Image Restoration and Enhancement Transformer&lt;/a>; &lt;br />; &lt;em>;Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.15774.pdf&quot;>;Decentralized Learning with Multi-Headed Distillation&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Andrey Zhmoginov&lt;/strong>;, &lt;strong>;Mark Sandler&lt;/strong>;, &lt;strong>;Nolan Miller&lt;/strong>;, &lt;strong>;Gus Kristiansen&lt;/strong>;, &lt;strong>;Max Vladymyrov&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02163.pdf&quot;>;GINA-3D: Learning to Generate Implicit Neural Assets in the Wild&lt;/a>; &lt;br />; &lt;em>;Bokui Shen, Xinchen Yan, Charles R. Qi, Mahyar Najibi, Boyang Deng,&lt;strong>; Leonidas Guibas&lt;/strong>;, Yin Zhou, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11846.pdf&quot;>;Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions&lt;/a>; &lt;br />; &lt;em>;Yun He, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;, Xiangyang Xue, Yanwei Fu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.11042.pdf&quot;>;Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery from Sparse Image Ensemble&lt;/a>; &lt;br />; &lt;em>;Chun-Han Yao*, Wei-Chih Hung, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.00653.pdf&quot;>;Hyperbolic Contrastive Learning for Visual Representations beyond Objects&lt;/a>; &lt;br />; &lt;em>;Songwei Ge, Shlok Mishra,&lt;strong>; Simon Kornblith&lt;/strong>;, &lt;strong>;Chun-Liang Li, &lt;/strong>;David Jacobs&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.09276.pdf&quot;>;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Bahjat Kawar*,&lt;strong>; Shiran Zada&lt;/strong>;, &lt;strong>;Oran Lang&lt;/strong>;, &lt;strong>;Omer Tov&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Tali Dekel&lt;/strong>;, &lt;strong>;Inbar Mosseri&lt;/strong>;, &lt;strong>;Michal Irani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.02743.pdf&quot;>;Incremental 3D Semantic Scene Graph Prediction from RGB Sequences&lt;/a>; &lt;br />; &lt;em>;Shun-Cheng Wu, &lt;strong>;Keisuke Tateno&lt;/strong>;, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00575.pdf&quot;>;IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction&lt;/a>; &lt;br />; &lt;em>;Dekai Zhu, Guangyao Zhai, Yan Di, &lt;strong>;Fabian Manhardt&lt;/strong>;, Hendrik Berkemeyer, Tuan Tran, Nassir Navab, &lt;strong>;Federico Tombari&lt;/strong>;, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.10844.pdf&quot;>;Learning to Generate Image Embeddings with User-Level Differential Privacy&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zheng Xu, Maxwell Collins, Yuxiao Wang, Liviu Panait, Sewoong Oh, Sean Augenstein, Ting Liu, Florian Schroff, H. Brendan McMahan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05866.pdf&quot;>;NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs&lt;/a>; &lt;br />; &lt;em>;Harsh Rangwani, Lavish Bansal, Kartik Sharma,&lt;strong>; Tejan Karmali&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, Venkatesh Babu Radhakrishnan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09794.pdf&quot;>;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Ron Mokady*, Amir Hertz*, &lt;strong>;Kfir Aberman&lt;/strong>;, &lt;strong>;Yael Pritch&lt;/strong>;, Daniel Cohen-Or*&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.14020.pdf&quot;>;SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow&lt;/a>; &lt;br />; &lt;em>;Itai Lang*,&lt;strong>; Dror Aiger&lt;/strong>;, &lt;strong>;Forrester Cole&lt;/strong>;, &lt;strong>;Shai Avidan&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11674.pdf&quot;>;Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion&lt;/a>; &lt;br />; &lt;em>;Dario Pavllo*,&lt;strong>; David Joseph Tan&lt;/strong>;, &lt;strong>;Marie-Julie Rakotosaona&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12902.pdf&quot;>;TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Hanzhi Chen, &lt;strong>;Fabian Manhardt&lt;/strong>;, Nassir Navab, Benjamin Busam&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf&quot;>;TryOnDiffusion: A Tale of Two UNets&lt;/a>; &lt;br />; &lt;em>;Luyang Zhu*, &lt;strong>;Dawei Yang&lt;/strong>;, &lt;strong>;Tyler Zhu&lt;/strong>;, &lt;strong>;Fitsum Reda&lt;/strong>;, &lt;strong>;William Chan&lt;/strong>;, &lt;strong>;Chitwan Saharia&lt;/strong>;, &lt;strong>;Mohammad Norouzi&lt;/strong>;, &lt;strong>;Ira Kemelmacher-Shlizerman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03112.pdf&quot;>;A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning&lt;/a>; &lt;br />; &lt;em>;Aishwarya Kamath*, &lt;strong>;Peter Anderson&lt;/strong>;, &lt;strong>;Su Wang&lt;/strong>;, Jing Yu Koh*, &lt;strong>;Alexander Ku&lt;/strong>;, &lt;strong>;Austin Waters&lt;/strong>;, Yinfei Yang*, &lt;strong>;Jason Baldridge&lt;/strong>;, &lt;strong>;Zarana Parekh&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08045.pdf&quot;>;CLIPPO: Image-and-Language Understanding from Pixels Only&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Basil Mustafa&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.04745.pdf&quot;>;Controllable Light Diffusion for Portraits&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;David Futschik&lt;/strong>;, &lt;strong>;Kelvin Ritland&lt;/strong>;, &lt;strong>;James Vecore&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Brian Curless&lt;/strong>;, &lt;strong>;Daniel Sýkora&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf&quot;>;CUF: Continuous Upsampling Filters&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Cristina Vasconcelos&lt;/strong>;, &lt;strong>;Cengiz Oztireli&lt;/strong>;, &lt;strong>;Mark Matthews&lt;/strong>;, &lt;strong>;Milad Hashemi&lt;/strong>;, &lt;strong>;Kevin Swersky&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01758.pdf&quot;>;Improving Zero-Shot Generalization and Robustness of Multi-modal Models&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge*, &lt;strong>;Jie Ren&lt;/strong>;, &lt;strong>;Andrew Gallagher&lt;/strong>;, &lt;strong>;Yuxiao Wang&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>;, &lt;strong>;Laurent Itti&lt;/strong>;, &lt;strong>;Balaji Lakshminarayanan&lt;/strong>;, &lt;strong>;Jiaping Zhao&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09665.pdf&quot;>;LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding&lt;/a>; &lt;br />; &lt;em>;Gen Li, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;, Laura Sevilla-Lara&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03361.pdf&quot;>;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Xiaoshuai Zhang&lt;/strong>;, &lt;strong>;Abhijit Kundu&lt;/strong>;, &lt;strong>;Thomas Funkhouser&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Hao Su&lt;/strong>;, &lt;strong>;Kyle Genova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01762.pdf&quot;>;Self-Supervised AutoFlow&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hsin-Ping Huang&lt;/strong>;, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Erika Lu&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf&quot;>;Train-Once-for-All Personalization&lt;/a>; &lt;br />; &lt;em>;Hong-You Chen*,&lt;strong>; Yandong Li&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Mingda Zhang&lt;/strong>;, Wei-Lun Chao,&lt;strong>; Li Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.14115.pdf&quot;>;Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Antoine Yang*, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Paul Hongsuck Seo&lt;/strong>;, Antoine Miech, &lt;strong>;Jordi Pont-Tuset&lt;/strong>;, Ivan Laptev, Josef Sivic, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14302.pdf&quot;>;VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Junjie Ke&lt;/strong>;, &lt;strong>;Keren Ye&lt;/strong>;, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Yonghui Wu&lt;/strong>;, &lt;strong>;Peyman Milanfar&lt;/strong>;, &lt;strong>;Feng Yang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.11152.pdf&quot;>;You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model&lt;/a>; &lt;br />; &lt;em>;Shengkun Tang, &lt;strong>;Yaqing Wang&lt;/strong>;, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, &lt;strong>;Yi Liang&lt;/strong>;, Dongkuan Xu&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.05211.pdf&quot;>;Accidental Light Probes&lt;/a>; &lt;br />; &lt;em>;Hong-Xing Yu, Samir Agarwala, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Richard Szeliski&lt;/strong>;, &lt;strong>;Noah Snavely, &lt;/strong>;Jiajun Wu, &lt;strong>;Deqing Sun&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.09653.pdf&quot;>;FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning&lt;/a>; &lt;br />; &lt;em>;Yuanhao Xiong, Ruochen Wang, Minhao Cheng,&lt;strong>; Felix Yu&lt;/strong>;, Cho-Jui Hsieh&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.08013.pdf&quot;>;FlexiViT: One Model for All Patch Sizes&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Lucas Beyer&lt;/strong>;, &lt;strong>;Pavel Izmailov&lt;/strong>;, &lt;strong>;Alexander Kolesnikov&lt;/strong>;, &lt;strong>;Mathilde Caron&lt;/strong>;, &lt;strong>;Simon Kornblith&lt;/strong>;, &lt;strong>;Xiaohua Zhai&lt;/strong>;, &lt;strong>;Matthias Minderer&lt;/strong>;, &lt;strong>;Michael Tschannen&lt;/strong>;, &lt;strong>;Ibrahim Alabdulmohsin&lt;/strong>;, &lt;strong>;Filip Pavetic&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03087.pdf&quot;>;Iterative Vision-and-Language Navigation&lt;/a>; &lt;br />; &lt;em>;Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso,&lt;strong>; Peter Anderson&lt;/strong>;, Stefan Lee, Jesse Thomason&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.08010.pdf&quot;>;MoDi: Unconditional Motion Synthesis from Diverse Data&lt;/a>; &lt;br />; &lt;em>;Sigal Raab, Inbal Leibovitch, Peizhuo Li,&lt;strong>; Kfir Aberman&lt;/strong>;, Olga Sorkine-Hornung, Daniel Cohen-Or&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.03369.pdf&quot;>;Multimodal Prompting with Missing Modalities for Visual Recognition&lt;/a>; &lt;br />; &lt;em>;Yi-Lun Lee,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>;, Wei-Chen Chiu,&lt;strong>; Chen-Yu Lee&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf&quot;>;Scene-Aware Egocentric 3D Human Pose Estimation&lt;/a>; &lt;br />; &lt;em>;Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu,&lt;strong>; Kripasindhu Sarkar&lt;/strong>;, Christian Theobalt&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06247.pdf&quot;>;ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-Based Consistency&lt;/a>; &lt;br />; &lt;em>;Zixuan Huang,&lt;strong>; Varun Jampani&lt;/strong>;, Ngoc Anh Thai,&lt;strong>; Yuanzhen Li&lt;/strong>;, Stefan Stojanov, James M. Rehg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.05173.pdf&quot;>;Improving Image Recognition by Retrieving from Web-Scale Image-Text Data&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.00341.pdf&quot;>;JacobiNeRF: NeRF Shaping with Mutual Information Gradients&lt;/a>; &lt;br />; &lt;em>;Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, Li Yi,&lt;strong>; Leonidas Guibas&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01436.pdf&quot;>;Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos&lt;/a>; &lt;br />; &lt;em>;Ziqian Bai*,&lt;strong>; Feitong Tan&lt;/strong>;, &lt;strong>;Zeng Huang&lt;/strong>;, &lt;strong>;Kripasindhu Sarkar&lt;/strong>;, &lt;strong>;Danhang Tang&lt;/strong>;, &lt;strong>;Di Qiu&lt;/strong>;, &lt;strong>;Abhimitra Meka&lt;/strong>;, &lt;strong>;Ruofei Du&lt;/strong>;, &lt;strong>;Mingsong Dou&lt;/strong>;, &lt;strong>;Sergio Orts-Escolano&lt;/strong>;, &lt;strong>;Rohit Pandey&lt;/strong>;, Ping Tan,&lt;strong>; Thabo Beeler&lt;/strong>;, &lt;strong>;Sean Fanello&lt;/strong>;, &lt;strong>;Yinda Zhang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.08556.pdf&quot;>;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis&lt;/a>; &lt;br />; &lt;em>;Allan Zhou, Mo Jin Kim, Lirui Wang,&lt;strong>; Pete Florence&lt;/strong>;, &lt;strong>;Chelsea Finn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03084.pdf&quot;>;Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval&lt;/a>; &lt;br />; &lt;em>;Kuniaki Saito*,&lt;strong>; Kihyuk Sohn&lt;/strong>;, &lt;strong>;Xiang Zhang&lt;/strong>;, &lt;strong>;Chun-Liang Li&lt;/strong>;, &lt;strong>;Chen-Yu Lee&lt;/strong>;, &lt;strong>;Kate Saenko&lt;/strong>;, &lt;strong>;Tomas Pfister&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13582.pdf&quot;>;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Mikaela Uy&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;Ke Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.06820.pdf&quot;>;Structured 3D Features for Reconstructing Controllable Avatars&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Enric Corona&lt;/strong>;, &lt;strong>;Mihai Zanfir&lt;/strong>;, &lt;strong>;Thiemo Alldieck&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Andrei Zanfir&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.09119.pdf&quot;>;Token Turing Machines&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Michael S. Ryoo&lt;/strong>;, &lt;strong>;Keerthana Gopalakrishnan&lt;/strong>;, &lt;strong>;Kumara Kahatapitiya&lt;/strong>;, &lt;strong>;Ted Xiao&lt;/strong>;, &lt;strong>;Kanishka Rao&lt;/strong>;, &lt;strong>;Austin Stone&lt;/strong>;, &lt;strong>;Yao Lu&lt;/strong>;, &lt;strong>;Julian Ibarz&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10957.pdf&quot;>;TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization&lt;/a>; &lt;br />; &lt;em>;Fabrizio Guillaro, Davide Cozzolino,&lt;strong>; Avneesh Sud&lt;/strong>;, &lt;strong>;Nicholas Dufour, &lt;/strong>;Luisa Verdoliva&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.07685.pdf&quot;>;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/a>; &lt;br />; &lt;em>;Sihyun Yu,&lt;strong>; Kihyuk Sohn, &lt;/strong>;Subin Kim, Jinwoo Shin&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.00990.pdf&quot;>;Visual Prompt Tuning for Generative Transfer Learning&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Jose Lezama&lt;/strong>;, &lt;strong>;Luisa Polania&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Han Zhang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17811.pdf&quot;>;Zero-Shot Referring Image Segmentation with Global-Local Context Features&lt;/a>; &lt;br />; &lt;em>;Seonghoon Yu,&lt;strong>; Paul Hongsuck Seo&lt;/strong>;, Jeany Son&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;&lt;strong>;Paul Hongsuck Seo&lt;/strong>;, &lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.03285.pdf&quot;>;DC2: Dual-Camera Defocus Control by Learning to Refocus&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hadi Alzayer&lt;/strong>;, &lt;strong>;Abdullah Abuolaim&lt;/strong>;, &lt;strong>;Leung Chun Chan&lt;/strong>;, &lt;strong>;Yang Yang&lt;/strong>;, &lt;strong>;Ying Chen Lou&lt;/strong>;, &lt;strong>;Jia-Bin Huang&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf&quot;>;Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision&lt;/a>; &lt;br />; &lt;em>;Aditay Tripathi*,&lt;strong>; Rishubh Singh&lt;/strong>;, Anirban Chakraborty,&lt;strong>; Pradeep Shenoy&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09898.pdf&quot;>;MetaCLUE: Towards Comprehensive Visual Metaphors Research&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arjun R. Akula&lt;/strong>;, &lt;strong>;Brendan Driscoll&lt;/strong>;, &lt;strong>;Pradyumna Narayana&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Zhiwei Jia&lt;/strong>;, &lt;strong>;Suyash Damle&lt;/strong>;, &lt;strong>;Garima Pruthi&lt;/strong>;, &lt;strong>;Sugato Basu&lt;/strong>;, &lt;strong>;Leonidas Guibas&lt;/strong>;, &lt;strong>;William T. Freeman&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.13824.pdf&quot;>;Multi-Realism Image Compression with a Conditional Generator&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Eirikur Agustsson&lt;/strong>;, &lt;strong>;David Minnen&lt;/strong>;, &lt;strong>;George Toderici&lt;/strong>;, &lt;strong>;Fabian Mentzer&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.03267.pdf&quot;>;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a>; &lt;br />; &lt;em>;Congyue Deng, Chiyu Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou,&lt;strong>; Leonidas Guibas&lt;/strong>;, Dragomir Anguelov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.12053.pdf&quot;>;On Calibrating Semantic Segmentation Models: Analyses and an Algorithm&lt;/a>; &lt;br />; &lt;em>;Dongdong Wang,&lt;strong>; Boqing Gong&lt;/strong>;, Liqiang Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13515.pdf&quot;>;Persistent Nature: A Generative Model of Unbounded 3D Worlds&lt;/a>; &lt;br />; &lt;em>;Lucy Chai,&lt;strong>; Richard Tucker&lt;/strong>;, &lt;strong>;Zhengqi Li&lt;/strong>;, Phillip Isola,&lt;strong>; Noah Snavely&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13662.pdf&quot;>;Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment&lt;/a>; &lt;br />; &lt;em>;Yiyou Sun*,&lt;strong>; Yaojie Liu&lt;/strong>;, &lt;strong>;Xiaoming Liu&lt;/strong>;, Yixuan Li,&lt;strong>; Wen-Sheng Chu&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13277.pdf&quot;>;SINE: Semantic-Driven Image-Based NeRF Editing with Prior-Guided Editing Field&lt;/a>; &lt;br />; &lt;em>;Chong Bao,&lt;strong>; Yinda Zhang&lt;/strong>;, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15533.pdf&quot;>;Sequential Training of GANs Against GAN-Classifiers Reveals Correlated &quot;Knowledge Gaps&quot; Present Among Independently Trained GAN Instances&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Arkanath Pathak&lt;/strong>;, &lt;strong>;Nicholas Dufour&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.16991.pdf&quot;>;SparsePose: Sparse-View Camera Pose Regression and Refinement&lt;/a>; &lt;br />; &lt;em>;Samarth Sinha, Jason Zhang,&lt;strong>; Andrea Tagliasacchi&lt;/strong>;, Igor Gilitschenski, David Lindell&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models&lt;/a>; &lt;br />; &lt;em>;Yushi Yao,&lt;strong>; Chang Ye&lt;/strong>;, &lt;strong>;Gamaleldin F. Elsayed&lt;/strong>;, &lt;strong>;Junfeng He&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://cv4mr.github.io/&quot;>;Computer Vision for Mixed Reality&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvpr2023.wad.vision/&quot;>;Workshop on Autonomous Driving (WAD)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Chelsea Finn&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://multimodal-content-moderation.github.io/&quot;>;Multimodal Content Moderation (MMCM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Chris Bregler&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mevan Babakar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mcv-workshop.github.io/#updates&quot;>;Medical Computer Vision (MCV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Shekoofeh Azizi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vand-cvpr23/home&quot;>;VAND: Visual Anomaly and Novelty Detection&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Yedid Hoshen&lt;/strong>;, &lt;strong>;Jie Ren&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://struco3d.github.io/cvpr2023/&quot;>;Structural and Compositional Learning on 3D Data&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Andrea Tagliasacchi&lt;/strong>;, &lt;strong>;Fei Xia&lt;/strong>;, &lt;strong>;Amir Hertz&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgvc10&quot;>;Fine-Grained Visual Categorization (FGVC10)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Kimberly Wilber&lt;/strong>;, &lt;strong>;Sara Beery&lt;/strong>;&lt;/em>; &lt;br />; Panelists include: &lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/xrnerf/&quot;>;XRNeRF: Advances in NeRF for the Metaverse&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnilabel-workshop-cvpr23/overview&quot;>;OmniLabel: Infinite Label Spaces for Semantic Understanding via Natural Language&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Long Zhao&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://holistic-video-understanding.github.io/workshops/cvpr2023.html&quot;>;Large Scale Holistic Video Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;David Ross&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nice.lgresearch.ai/&quot;>;New Frontiers for Zero-Shot Image Captioning Evaluation (NICE)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ccd2023.github.io/&quot;>;Computational Cameras and Displays (CCD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ulugbek Kamilov&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Mauricio Delbracio&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gazeworkshop.github.io/2023/&quot;>;Gaze Estimation and Prediction in the Wild (GAZE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Thabo Beele&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Erroll Wood&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/fgahi2023/home&quot;>;Face and Gesture Analysis for Health Informatics (FGAHI)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.cv4animals.com/&quot;>;Computer Vision for Animal Behavior Tracking and Modeling (CV4Animals)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;3D Vision and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Pete Florence&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-3d-vision-robotics&quot;>;End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation (E2EAD)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opendrivelab.com/e2ead/cvpr23&quot;>;End-to-End Autonomous Driving: Emerging Tasks and Challenges&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Sergey Levine&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://mula-workshop.github.io/&quot;>;Multi-modal Learning and Applications (MULA)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Aleksander Hołyński&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sdas2023/&quot;>;Synthetic Data for Autonomous Systems (SDAS)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vdu-cvpr23&quot;>;Vision Datasets Understanding&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;José Lezama&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Vijay Janapa Reddi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ieeecvf-cvpr2023-precognition/&quot;>;Precognition: Seeing Through the Future&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Utsav Prabhu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvlai.net/ntire/2023/&quot;>;New Trends in Image Restoration and Enhancement (NTIRE)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://generative-vision.github.io/workshop-CVPR-23/&quot;>;Generative Models for Computer Vision&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://robustart.github.io/&quot;>;Adversarial Machine Learning on Computer Vision: Art of Robustness&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Xinyun Chen&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/wmf2023/home&quot;>;Media Forensics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Nicholas Carlini&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://taodataset.org/workshop/cvpr23/&quot;>;Tracking and Its Many Guises: Tracking Any Object in Open-World&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Paul Voigtlaender&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://scene-understanding.com/&quot;>;3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.es.ele.tue.nl/cvpm23/&quot;>;Computer Vision for Physiological Measurement (CVPM)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/&quot;>;Affective Behaviour Analysis In-the-Wild&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision (EC3V)&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Rida Qadri&lt;/strong>;, &lt;strong>;Mohammad Havaei&lt;/strong>;, &lt;strong>;Fernando Diaz&lt;/strong>;, &lt;strong>;Emily Denton&lt;/strong>;, &lt;strong>;Sarah Laszlo&lt;/strong>;, &lt;strong>;Negar Rostamzadeh&lt;/strong>;, &lt;strong>;Pamela Peter-Agbia&lt;/strong>;, &lt;strong>;Eva Kozanecka&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vizwiz.org/workshops/2023-workshop/&quot;>;VizWiz Grand Challenge: Describing Images and Videos Taken by Blind People&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Haoran Qi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/ecv23/home&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>; (see &lt;a href=&quot;https://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot;>;blog post&lt;/a>;) &lt;br />; Organizers include: &lt;em>;&lt;strong>;Andrew Howard&lt;/strong>;, &lt;strong>;Chas Leichner&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Andrew Howard&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vcdw2023/&quot;>;Visual Copy Detection&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Priya Goyal&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dmv2023.github.io/&quot;>;Learning 3D with Multi-View Supervision (3DMV)&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://image-matching-workshop.github.io/&quot;>;Image Matching: Local Features and Beyond&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vision4allseason.net/&quot;>;Vision for All Seasons: Adverse Weather and Lightning Conditions (V4AS)&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Lukas Hoyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/t4v-cvpr23&quot;>;Transformers for Vision (T4V)&lt;/a>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/academic-cv/&quot;>;Scholars vs Big Models — How Can Academics Adapt?&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Jonathan T. Barron&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://www.scan-net.org/cvpr2023workshop/&quot;>;ScanNet Indoor Scene Understanding Challenge&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Tom Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvmi-workshop.github.io/new.html&quot;>;Computer Vision for Microscopy Image Analysis&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Po-Hsuan Cameron Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://embeddedvisionworkshop.wordpress.com/&quot;>;Embedded Vision&lt;/a>; &lt;br />; Speakers include: &lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sightsound.org/&quot;>;Sight and Sound&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Arsha Nagrani&lt;/strong>;, &lt;strong>;William Freeman&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4cc.net/&quot;>;AI for Content Creation&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Deqing Sun&lt;/strong>;, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; Speakers include: &lt;em>;&lt;strong>;Ben Mildenhall&lt;/strong>;, &lt;strong>;Tim Salimans&lt;/strong>;, &lt;strong>;Yuanzhen Li&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://computer-vision-in-the-wild.github.io/cvpr-2023/https://computer-vision-in-the-wild.github.io/cvpr-2023/&quot;>;Computer Vision in the Wild&lt;/a>; &lt;br />; Organizers include: &lt;em>;&lt;strong>;Xiuye Gu&lt;/strong>;, &lt;strong>;Neil Houlsby&lt;/strong>;&lt;/em>; &lt;br />; Speakers include: &lt;em>;&lt;strong>;Boqing Gong&lt;/strong>;, &lt;strong>;Anelia Angelova&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://vispr-workshop.github.io/&quot;>;Visual Pre-training for Robotics&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/omnicv2023/home&quot;>;Omnidirectional Computer Vision&lt;/a>; &lt;br />; Organizers include: &lt;strong>;&lt;em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://all-things-vits.github.io/atv/&quot;>;All Things ViTs: Understanding and Interpreting Attention in Vision&lt;/a>; &lt;br />; &lt;em>;&lt;strong>;Hila Chefer&lt;/strong>;, &lt;strong>;Sayak Paul&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr2023-tutorial-on-ad/&quot;>;Recent Advances in Anomaly Detection&lt;/a>; &lt;br />; &lt;em>;Guansong Pang, Joey Tianyi Zhou, Radu Tudor Ionescu, Yu Tian,&lt;strong>; Kihyuk Sohn&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cvpr-tutorial-2023/home&quot;>;Contactless Healthcare Using Cameras and Wireless Sensors&lt;/a>; &lt;br />; &lt;em>;Wenjin Wang, Xuyu Wang, Jun Luo,&lt;strong>; Daniel McDuff&lt;/strong>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://osimeoni.github.io/object-localization-for-free/&quot;>;Object Localization for Free: Going Beyond Self-Supervised Learning&lt;/a>; &lt;br />; &lt;em>;Oriane Simeoni,&lt;strong>; &lt;/strong>;Weidi Xie,&lt;strong>; Thomas Kipf&lt;/strong>;, Patrick Pérez&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://prompting-in-vision.github.io/&quot;>;Prompting in Vision&lt;/a>; &lt;br />; &lt;em>;Kaiyang Zhou, Ziwei Liu, Phillip Isola, Hyojin Bahng, Ludwig Schmidt, Sarah Pratt,&lt;strong>; Denny Zhou&lt;/strong>;&lt;/em>; &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/9139300663122353070/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/9139300663122353070&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/google-at-cvpr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CVPR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjna9ws3HEBS4wd_UsuPUR1OMmrdGD8agFbkXjiv9Y2yAKkwAGUgGOOdvqQZESTsNRLHhj0Wj8ZCtKpIGhkR0SrwielzXijpCIr57s_n6EobR8Vry_h6x2B7cAWtiB0obvEyJ098j5K2pYFdjgUdN-vhmC17aSkx-dkseTblY3VGhjWHBZ7G_D7n8pYqw/s72-c/CVPR%20Design-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2312998356111901143&lt;/id>;&lt;published>;2023-06-15T13:53:00.000-07:00&lt;/published>;&lt;updated>;2023-06-15T13:53:27.418-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Juhyun Lee and Raman Sarokin, Software Engineers, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s700/SpeedHero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The proliferation of large &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>; for &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;image generation&lt;/a>; has led to a significant increase in model size and inference workloads. On-device ML inference in mobile environments requires meticulous performance optimization and consideration of trade-offs due to resource constraints. Running inference of large diffusion models (LDMs) on-device, driven by the need for cost efficiency and user privacy, presents even greater challenges due to the substantial memory requirements and computational demands of these models. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We address this challenge in our work titled “&lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations&lt;/a>;” (to be presented at the &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; workshop for &lt;a href=&quot;https://sites.google.com/corp/view/ecv23&quot;>;Efficient Deep Learning for Computer Vision&lt;/a>;) focusing on the optimized execution of a foundational LDM model on a mobile GPU. In this blog post, we summarize the core techniques we employed to successfully execute large diffusion models like &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; at full resolution (512x512 pixels) and 20 iterations on modern smartphones with high-performing inference speed of the original model without distillation of under 12 seconds. As discussed in &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;our previous blog post&lt;/a>;, GPU-accelerated ML inference is often limited by memory performance, and execution of LDMs is no exception. Therefore, the central theme of our optimization is efficient memory input/output (I/O) even if it means choosing memory-efficient algorithms over those that prioritize arithmetic logic unit efficiency. Ultimately, our primary objective is to reduce the overall latency of the ML inference. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s512/image4.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSvrG_xr7MWV45RJH9aZr8I9a1wlYHLjVx9hJrCnLoC9xa0Y0h1N_LX0df8pa7yVKLfj6S8SH_RDx-p7obVNBu5A18uabECxdt4_14ixwjQXKE2y4jqajgAD4Okt6p48ju20n1zttBdM2D2woOdEX6gxgUzvLKQ6vie6GJBf_sSQt9UwhEMU0piYQPJQ/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A sample output of an LDM on Mobile GPU with the prompt text: “a photo realistic and high resolution image of a cute puppy with surrounding flowers”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enhanced attention module for memory efficiency&lt;/h2>; &lt;p>; An ML inference engine typically provides a variety of optimized ML operations. Despite this, achieving optimal performance can still be challenging as there is a certain amount of overhead for executing individual neural net operators on a GPU. To mitigate this overhead, ML inference engines incorporate extensive operator fusion rules that consolidate multiple operators into a single operator, thereby reducing the number of iterations across tensor elements while maximizing compute per iteration. For instance, &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; utilizes operator fusion to combine computationally expensive operations, like convolutions, with subsequent activation functions, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;>;rectified linear units&lt;/a>;, into one. &lt;/p>; &lt;p>; A clear opportunity for optimization is the heavily used attention block adopted in the &lt;a href=&quot;https://arxiv.org/pdf/2304.11267.pdf&quot;>;denoiser model&lt;/a>; in the LDM. The attention blocks allow the model to focus on specific parts of the input by assigning higher weights to important regions. There are multiple ways one can optimize the attention modules, and we selectively employ one of the two optimizations explained below depending on which optimization performs better. &lt;/p>; &lt;p>; The first optimization, which we call &lt;em>;partially fused &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;>;softmax&lt;/a>;&lt;/em>;, removes the need for extensive memory writes and reads between the softmax and the matrix multiplication in the attention module. Let the attention block be just a simple matrix multiplication of the form &lt;b>;&lt;em>;Y &lt;/em>;&lt;/b>;= softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) * &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; where &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;W&lt;/em>;&lt;/b>; are 2D matrices of shape &lt;em>;a&lt;/em>;×&lt;em>;b&lt;/em>; and &lt;em>;b&lt;/em>;×&lt;em>;c&lt;/em>;, respectively (shown below in the top half). &lt;/p>; &lt;p>; For numerical stability, &lt;b>;&lt;em>;T = &lt;/em>;&lt;/b>;softmax(&lt;b>;&lt;em>;X&lt;/em>;&lt;/b>;) is typically calculated in three passes: &lt;/p>; &lt;ol>; &lt;li>;Determine the maximum value in the list, &lt;em>;ie&lt;/em>;.,&lt;em>; &lt;/em>;for each row in matrix &lt;b>;&lt;em>;X&lt;/em>;&lt;/b>; &lt;/li>;&lt;li>;Sum up the differences of the exponential of each list item and the maximum value (from pass 1) &lt;/li>;&lt;li>;Divide the exponential of the items minus the maximum value by the sum from pass 2 &lt;/li>; &lt;/ol>; &lt;p>; Carrying out these passes naïvely would result in a huge memory write for the temporary intermediate tensor &lt;strong>;&lt;em>;T&lt;/em>;&lt;/strong>; holding the output of the entire softmax function. We bypass this large memory write if we only store the results of passes 1 and 2, labeled &lt;b>;&lt;em>;m&lt;/em>;&lt;/b>; and &lt;b>;&lt;em>;s&lt;/em>;&lt;/b>;, respectively, which are small vectors, with &lt;em>;a&lt;/em>; elements each, compared to &lt;strong>;&lt;em>;T &lt;/em>;&lt;/strong>;which has &lt;em>;a·b &lt;/em>;elements. With this technique, we are able to reduce tens or even hundreds of megabytes of memory consumption by multiple orders of magnitude (shown below in the bottom half). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s568/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;457&quot; data-original-width=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmMWkT7U6yQ7m4JegrYpJRUt6gO6dMBb16fvNQIdiaX6gRRWP7uHykb6PBNoI0LyuqFTdJc1sJgWIq1bO4j1l4x_LokTvrPVDCEnbLBAgXqd8QBAGHxCLVaWlEMYWhmW2CC4rzuKwqC06MLQ-8MVAfEea4SwSngu4-YxcMYHzEyrTF3X7lZhLHkUjmcg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Attention modules. &lt;b>;Top&lt;/b>;: A naïve attention block, composed of a SOFTMAX (with all three passes) and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;MATMUL&lt;/a>;, requires a large memory write for the big intermediate tensor &lt;em>;T&lt;/em>;. &lt;b>;Bottom&lt;/b>;: Our memory-efficient attention block with partially fused softmax in MATMUL only needs to store two small intermediate tensors for &lt;em>;m&lt;/em>; and s.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The other optimization involves employing &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;>;FlashAttention&lt;/a>;, which is an I/O-aware, exact attention algorithm. This algorithm reduces the number of GPU high-bandwidth memory accesses, making it a good fit for our memory bandwidth–limited use case. However, we found this technique to only work for &lt;a href=&quot;https://en.wikipedia.org/wiki/Static_random-access_memory&quot;>;SRAM&lt;/a>; with certain sizes and to require a large number of registers. Therefore, we only leverage this technique for attention matrices with a certain size on a select set of GPUs. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Winograd fast convolution for 3×3 convolution layers&lt;/h2>; &lt;p>; The backbone of common LDMs heavily relies on 3×3 convolution layers (convolutions with filter size 3×3), comprising over 90% of the layers in the decoder. Despite increased memory consumption and numerical errors, we found that &lt;a href=&quot;https://arxiv.org/abs/1509.09308&quot;>;Winograd fast convolution&lt;/a>; to be effective at speeding up the convolutions. Distinct from the &lt;em>;filter size&lt;/em>; 3x3 used in convolutions, &lt;em>;tile size&lt;/em>; refers to the size of a sub region of the input tensor that is processed at a time. Increasing the tile size enhances the efficiency of the convolution in terms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_logic_unit&quot;>;arithmetic logic unit&lt;/a>; (ALU) usage. However, this improvement comes at the expense of increased memory consumption. Our tests indicate that a tile size of 4×4 achieves the optimal trade-off between computational efficiency and memory utilization. &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot; style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;/td>; &lt;td>;&lt;/td>; &lt;td colspan=&quot;2&quot;>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Memory usage&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tile size&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; savings&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Intermediate tensors&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Weights&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;2×2 &lt;/td>; &lt;td>;2.25× &lt;/td>; &lt;td>;4.00× &lt;/td>; &lt;td>;1.77× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;b>;4×4&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;2.25×&lt;/b>; &lt;/td>; &lt;td>;&lt;b>;4.00×&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;6×6 &lt;/td>; &lt;td>;5.06× &lt;/td>; &lt;td>;1.80× &lt;/td>; &lt;td>;7.12× &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;8×8 &lt;/td>; &lt;td>;5.76× &lt;/td>; &lt;td>;1.56× &lt;/td>; &lt;td>;11.1× &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impact of Winograd with varying tile sizes for 3×3 convolutions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Specialized operator fusion for memory efficiency&lt;/h2>; &lt;p>; We discovered that performantly inferring LDMs on a mobile GPU requires significantly larger fusion windows for commonly employed layers and units in LDMs than current off-the-shelf on-device GPU-accelerated ML inference engines provide. Consequently, we developed specialized implementations that could execute a larger range of neural operators than typical fusion rules would permit. Specifically, we focused on two specializations: the &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;>;Gaussian Error Linear Unit&lt;/a>; (GELU) and the &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;>;group normalization&lt;/a>; layer. &lt;/p>; &lt;p>; An approximation of GELU with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions&quot;>;hyperbolic tangent&lt;/a>; function requires writing to and reading from seven auxiliary intermediate tensors (shown below as light orange rounded rectangles in the figure below), reading from the input tensor &lt;em>;x&lt;/em>; three times, and writing to the output tensor &lt;em>;y&lt;/em>; once across eight GPU programs implementing the labeled operation each (light blue rectangles). A custom GELU implementation that performs the eight operations in a single shader (shown below in the bottom) can bypass all the memory I/O for the intermediate tensors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s958/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;210&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilG64yPIiXZN5SpvnvPua9d9TFGiFcTLZhZX00gmnOMpzOoEHX5agNOyorx6uCABcYc0oRMvsWUkuIz7vK1_yzhpGb77BJ2ZLKMj640ILhSKruDK0utP5wqD5TSjF2gfB3QWLwJOkseGrwzAhjmxtriTklmsZnTuFQDx3jeF_T7wux1gR0QIpSLYtkjg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;GELU implementations. &lt;strong>;Top&lt;/strong>;: A naïve implementation with built-in operations would require 8 memory writes and 10 reads. &lt;strong>;Bottom&lt;/strong>;: Our custom GELU only requires 1 memory read (for &lt;em>;x&lt;/em>;) and 1 write (for &lt;em>;y&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; After applying all of these optimizations, we conducted tests of Stable Diffusion 1.5 (image resolution 512x512, 20 iterations) on high-end mobile devices. Running Stable Diffusion with our GPU-accelerated ML inference model uses 2,093MB for the weights and 84MB for the intermediate tensors. With latest high-end smartphones, Stable Diffusion can be run in under 12 seconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s522/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN0IRuIecTfbuq2ztTqfcahyGfGkYum_4_wvf6rfwRKkvlYZOpq7Cw2l4T5QL08ElzKIn97dpXqARuUfRp08ugvGG4SFSS6ZfzXlF3usn9J4tUEa5iXrmRZQVxY0CQjiP9VCNhxRB333HK84HdwYd58iN6JwqePj-TGqvM0WV8A4N4D7yDUZ0Adxm1Ig/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stable Diffusion runs on modern smartphones in under 12 seconds. Note that running the decoder after each iteration for displaying the intermediate output in this animated GIF results in a ~2× slowdown.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Performing on-device ML inference of large models has proven to be a substantial challenge, encompassing limitations in model file size, extensive runtime memory requirements, and protracted inference latency. By recognizing memory bandwidth usage as the primary bottleneck, we directed our efforts towards optimizing memory bandwidth utilization and striking a delicate balance between ALU efficiency and memory efficiency. As a result, we achieved state-of-the-art inference latency for large diffusion models. You can learn more about this work in &lt;a href=&quot;https://arxiv.org/abs/2304.11267&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We&#39;d like to thank Yu-Hui Chen, Jiuqiang Tang, Frank Barchard, Yang Zhao, Joe Zou, Khanh LeViet, Chuo-Ling Chang, Andrei Kulik, Lu Wang, and Matthias Grundmann.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2312998356111901143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2312998356111901143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/speed-is-all-you-need-on-device.html&quot; rel=&quot;alternate&quot; title=&quot;Speed is all you need: On-device acceleration of large diffusion models via GPU-aware optimizations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm8J7pV44tX5D9h_So9djOQ6574uRfNDoRjQDgg8kN78DFf7H3N8y7AIEnHfGJCqbYqxN9ZJVs9PaQ2B0qQ7SoXgjGEzEVfcuPQZCKQqMjqR7kDZ7rk_vjR_RFRiw-OoW7k-KjF5ecEkEVNiX2_27bePakLG6Ac805m8q6YhQPaA3IEWaWpmFbrLsWKg/s72-c/SpeedHero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3072875289355194363&lt;/id>;&lt;published>;2023-06-14T09:00:00.001-07:00&lt;/published>;&lt;updated>;2023-06-22T14:55:31.687-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Reconstructing indoor spaces with NeRF&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Marcos Seefelder, Software Engineer, and Daniel Duckworth, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s320/Immersive%20View%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; When choosing a venue, we often find ourselves with questions like the following: Does this restaurant have the right vibe for a date? Is there good outdoor seating? Are there enough screens to watch the game? While photos and videos may partially answer questions like these, they are no substitute for feeling like you&#39;re there, even when visiting in person isn&#39;t an option. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Immersive experiences that are interactive, photorealistic, and multi-dimensional stand to bridge this gap and recreate the feel and vibe of a space, empowering users to naturally and intuitively find the information they need. To help with this, Google Maps launched &lt;a href=&quot;https://blog.google/products/maps/google-maps-updates-immersive-view-trip-planning&quot;>;Immersive View&lt;/a>;, which uses advances in machine learning (ML) and computer vision to fuse billions of &lt;a href=&quot;https://www.google.com/streetview/&quot;>;Street View&lt;/a>; and aerial images to create a rich, digital model of the world. Beyond that, it layers helpful information on top, like the weather, traffic, and how busy a place is. Immersive View provides indoor views of restaurants, cafes, and other venues to give users a virtual up-close look that can help them confidently decide where to go. &lt;/p>; &lt;p>; Today we describe the work put into delivering these indoor views in Immersive View. We build on &lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;>;neural radiance fields&lt;/a>; (NeRF), a state-of-the-art approach for fusing photos to produce a realistic, multi-dimensional reconstruction within a neural network. We describe our pipeline for creation of NeRFs, which includes custom photo capture of the space using DSLR cameras, image processing and scene reproduction. We take advantage of Alphabet&#39;s &lt;a href=&quot;https://arxiv.org/abs/2008.02268&quot;>;recent&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2111.12077&quot;>;advances&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2202.05263&quot;>;in the field&lt;/a>; to design a method matching or outperforming the prior state-of-the-art in visual fidelity. These models are then embedded as interactive 360° videos following curated flight paths, enabling them to be available on smartphones. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/introduction_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The reconstruction of The Seafood Bar in Amsterdam in Immersive View.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;From photos to NeRFs&lt;/h2>; &lt;p>; At the core of our work is &lt;a href=&quot;https://www.matthewtancik.com/nerf&quot;>;NeRF&lt;/a>;, a recently-developed method for 3D reconstruction and novel view synthesis. Given a collection of photos describing a scene, NeRF distills these photos into a &lt;a href=&quot;https://arxiv.org/abs/2111.11426&quot;>;neural field&lt;/a>;, which can then be used to render photos from viewpoints not present in the original collection. &lt;/p>; &lt;p>; While NeRF largely solves the challenge of reconstruction, a user-facing product based on real-world data brings a wide variety of challenges to the table. For example, reconstruction quality and user experience should remain consistent across venues, from dimly-lit bars to sidewalk cafes to hotel restaurants. At the same time, privacy should be respected and any potentially personally identifiable information should be removed. Importantly, scenes should be captured consistently and efficiently, reliably resulting in high-quality reconstructions while minimizing the effort needed to capture the necessary photographs. Finally, the same natural experience should be available to all mobile users, regardless of the device on hand. &lt;/p>; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/overview_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Immersive View indoor reconstruction pipeline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Capture &amp;amp; preprocessing&lt;/h2>; &lt;p>; The first step to producing a high-quality NeRF is the careful capture of a scene: a dense collection of photos from which 3D geometry and color can be derived. To obtain the best possible reconstruction quality, every surface should be observed from multiple different directions. The more information a model has about an object&#39;s surface, the better it will be in discovering the object&#39;s shape and the way it interacts with lights. &lt;/p>; &lt;p>; In addition, NeRF models place further assumptions on the camera and the scene itself. For example, most of the camera&#39;s properties, such as white balance and aperture, are assumed to be fixed throughout the capture. Likewise, the scene itself is assumed to be frozen in time: lighting changes and movement should be avoided. This must be balanced with practical concerns, including the time needed for the capture, available lighting, equipment weight, and privacy. In partnership with professional photographers, we developed a strategy for quickly and reliably capturing venue photos using DSLR cameras within only an hour timeframe. This approach has been used for all of our NeRF reconstructions to date. &lt;/p>; &lt;p>; Once the capture is uploaded to our system, processing begins. As photos may inadvertently contain sensitive information, we automatically scan and blur personally identifiable content. We then apply a &lt;a href=&quot;https://en.wikipedia.org/wiki/Structure_from_motion&quot;>;structure-from-motion&lt;/a>; pipeline to solve for each photo&#39;s &lt;a href=&quot;https://www.mathworks.com/help/vision/ug/camera-calibration.html&quot;>;camera parameters&lt;/a>;: its position and orientation relative to other photos, along with lens properties like &lt;a href=&quot;https://en.wikipedia.org/wiki/Focal_length&quot;>;focal length&lt;/a>;. These parameters associate each pixel with a point and a direction in 3D space and constitute a key signal in the NeRF reconstruction process. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeRF reconstruction&lt;/h2>; &lt;p>; Unlike many ML models, a new NeRF model is trained from scratch on each captured location. To obtain the best possible reconstruction quality within a target compute budget, we incorporate features from a variety of published works on NeRF developed at Alphabet. Some of these include: &lt;/p>; &lt;ul>; &lt;li>;We build on &lt;a href=&quot;https://jonbarron.info/mipnerf360/&quot;>;mip-NeRF 360&lt;/a>;, one of the best-performing NeRF models to date. While more computationally intensive than Nvidia&#39;s widely-used &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;Instant NGP&lt;/a>;, we find the mip-NeRF 360 consistently produces fewer artifacts and higher reconstruction quality. &lt;/li>;&lt;li>;We incorporate the low-dimensional generative latent optimization (GLO) vectors introduced in &lt;a href=&quot;https://nerf-w.github.io/&quot;>;NeRF in the Wild&lt;/a>; as an auxiliary input to the model&#39;s radiance network. These are learned real-valued latent vectors that embed appearance information for each image. By assigning each image in its own latent vector, the model can capture phenomena such as lighting changes without resorting to cloudy geometry, a common artifact in casual NeRF captures. &lt;/li>;&lt;li>;We also incorporate exposure conditioning as introduced in &lt;a href=&quot;https://waymo.com/research/block-nerf/&quot;>;Block-NeRF&lt;/a>;. Unlike GLO vectors, which are uninterpretable model parameters, exposure is directly derived from a photo&#39;s &lt;a href=&quot;https://en.wikipedia.org/wiki/Exif&quot;>;metadata&lt;/a>; and fed as an additional input to the model&#39;s radiance network. This offers two major benefits: it opens up the possibility of varying &lt;a href=&quot;https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index&quot;>;ISO&lt;/a>; and provides a method for controlling an image&#39;s brightness at inference time. We find both properties invaluable for capturing and reconstructing dimly-lit venues. &lt;/li>; &lt;/ul>; &lt;p>; We train each NeRF model on TPU or GPU accelerators, which provide different trade-off points. As with all Google products, we continue to search for new ways to improve, from reducing compute requirements to improving reconstruction quality. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/side_by_side_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A side-by-side comparison of our method and a mip-NeRF 360 baseline.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A scalable user experience&lt;/h2>; &lt;p>; Once a NeRF is trained, we have the ability to produce new photos of a scene from any viewpoint and camera lens we choose. Our goal is to deliver a meaningful and helpful user experience: not only the reconstructions themselves, but guided, interactive tours that give users the freedom to naturally explore spaces from the comfort of their smartphones. &lt;/p>; &lt;p>; To this end, we designed a controllable 360° video player that emulates flying through an indoor space along a predefined path, allowing the user to freely look around and travel forward or backwards. As the first Google product exploring this new technology, 360° videos were chosen as the format to deliver the generated content for a few reasons. &lt;/p>; &lt;p>; On the technical side, &lt;a href=&quot;https://nvlabs.github.io/instant-ngp/&quot;>;real-time inference&lt;/a>; and &lt;a href=&quot;https://phog.github.io/snerg/&quot;>;baked representations&lt;/a>; are still resource intensive on a per-client basis (either on device or cloud computed), and relying on them would limit the number of users able to access this experience. By using videos, we are able to scale the storage and delivery of videos to all users by taking advantage of the same video management and serving infrastructure used by YouTube. On the operations side, videos give us clearer editorial control over the exploration experience and are easier to inspect for quality in large volumes. &lt;/p>; &lt;p>; While we had considered capturing the space with a 360° camera directly, using a NeRF to reconstruct and render the space has several advantages. A virtual camera can fly anywhere in space, including over obstacles and through windows, and can use any desired camera lens. The camera path can also be edited post-hoc for smoothness and speed, unlike a live recording. A NeRF capture also does not require the use of specialized camera hardware. &lt;/p>; &lt;p>; Our 360° videos are rendered by &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_casting&quot;>;ray casting&lt;/a>; through each pixel of a virtual, spherical camera and compositing the visible elements of the scene. Each video follows a smooth path defined by a sequence of keyframe photos taken by the photographer during capture. The position of the camera for each picture is computed during structure-from-motion, and the sequence of pictures is smoothly interpolated into a flight path. &lt;/p>; &lt;p>; To keep speed consistent across different venues, we calibrate the distances for each by capturing pairs of images, each of which is 3 meters apart. By knowing measurements in the space, we scale the generated model, and render all videos at a natural velocity. &lt;/p>; &lt;p>; The final experience is surfaced to the user within &lt;a href=&quot;https://blog.google/products/maps/three-maps-updates-io-2022/&quot;>;Immersive View&lt;/a>;: the user can seamlessly fly into restaurants and other indoor venues and discover the space by flying through the photorealistic 360° videos. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open research questions&lt;/h2>; &lt;p>; We believe that this feature is the first step of many in a journey towards universally accessible, AI-powered, immersive experiences. From a NeRF research perspective, more questions remain open. Some of these include: &lt;/p>; &lt;ol>; &lt;li>;Enhancing reconstructions with scene segmentation, adding semantic information to the scenes that could make scenes, for example, searchable and easier to navigate. &lt;/li>;&lt;li>;Adapting NeRF to outdoor photo collections, in addition to indoor. In doing so, we&#39;d unlock similar experiences to every corner of the world and change how users could experience the outdoor world. &lt;/li>;&lt;li>;Enabling real-time, interactive 3D exploration through neural-rendering on-device. &lt;/li>; &lt;/ol>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 5%; margin-right: 5%;&quot; width=&quot;90%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/jax3d-public/projects/dinnerf-blog/outdoors_resized.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reconstruction of an outdoor scene with a NeRF model trained on Street View panoramas.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; As we continue to grow, we look forward to engaging with and contributing to the community to build the next generation of immersive experiences. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Jon Barron, Julius Beres, Daniel Duckworth, Roman Dudko, Magdalena Filak, Mike Harm, Peter Hedman, Claudio Martella, Ben Mildenhall, Cardin Moffett, Etienne Pot, Konstantinos Rematas, Yves Sallat, Marcos Seefelder, Lilyana Sirakovat, Sven Tresp and Peter Zhizhin.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;Also, we&#39;d like to extend our thanks to Luke Barrington, Daniel Filip, Tom Funkhouser, Charles Goran, Pramod Gupta, Santi López, Mario Lučić, Isalo Montacute and Dan Thomasset for valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3072875289355194363/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3072875289355194363&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot; rel=&quot;alternate&quot; title=&quot;Reconstructing indoor spaces with NeRF&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxGrEnhxjRJV-5Ws2doej6GjSpHuwLJBxjBbQnqzlVhuBAyBAKRmz0LuAYLSGHqtMPEfyZf9HIxVdNGhOsvMnlZKC6gnTq0oEFw0-YjU-yoHXXUhV1ZCWNxJVZUW4_rnT6ktLhHh4TjLfS9B2A4Txv5kKF3FZcxBU4q8ktQoaNPERvfLV1S9zHzCaLQQ/s72-c/Immersive%20View%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7719238929287079732&lt;/id>;&lt;published>;2023-06-13T10:18:00.004-07:00&lt;/published>;&lt;updated>;2023-06-13T13:54:56.343-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling delightful user experiences via predictive models of human attention&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Junfeng He, Senior Research Scientist, and Kai Kohlhoff, Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s1400/HumanAttention.png&quot; style=&quot;display: none;&quot; />; &lt;p>; People have the remarkable ability to take in a tremendous amount of information (estimated to be ~10&lt;sup>;10&lt;/sup>; bits/s entering the retina) and selectively attend to a few task-relevant and interesting regions for further processing (eg, memory, comprehension, action). Modeling human attention (the result of which is often called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;saliency&lt;/a>; model) has therefore been of interest across the fields of neuroscience, psychology, &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;>;human-computer interaction&lt;/a>; (HCI) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_vision&quot;>;computer vision&lt;/a>;. The ability to predict which regions are likely to attract attention has numerous important applications in areas like graphics, photography, image compression and processing, and the measurement of visual quality. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We&#39;ve &lt;a href=&quot;https://ai.googleblog.com/2021/05/accelerating-eye-movement-research-for.html&quot;>;previously discussed&lt;/a>; the possibility of accelerating eye movement research using machine learning and smartphone-based gaze estimation, which earlier required specialized hardware costing up to $30,000 per unit. Related research includes “&lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/look-to-speak/&quot;>;Look to Speak&lt;/a>;”, which helps users with accessibility needs (eg, people with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;ALS&lt;/a>;) to communicate with their eyes, and the recently published “&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;Differentially private heatmaps&lt;/a>;” technique to compute heatmaps, like those for attention, while protecting users&#39; privacy. &lt;/p>; &lt;p>; In this blog, we present two papers (one from &lt;a href=&quot;https://cvpr2022.thecvf.com/&quot;>;CVPR 2022&lt;/a>;, and one just accepted to &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;) that highlight our recent research in the area of human attention modeling: “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” and “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, together with recent research on saliency driven progressive loading for image compression (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;2&lt;/a>;). We showcase how predictive models of human attention can enable delightful user experiences such as image editing to minimize visual clutter, distraction or artifacts, image compression for faster loading of webpages or apps, and guiding ML models towards more intuitive human-like interpretation and model performance. We focus on image editing and image compression, and discuss recent advances in modeling in the context of these applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Attention-guided image editing&lt;/h2>; &lt;p>; Human attention models usually take an image as input (eg, a natural image or a screenshot of a webpage), and &lt;a href=&quot;https://dl.acm.org/doi/10.1109/TPAMI.2019.2935715&quot;>;predict a heatmap as output&lt;/a>;. The predicted heatmap on the image is &lt;a href=&quot;https://www.computer.org/csdl/journal/tp/2019/03/08315047/17D45Vw15wU&quot;>;evaluated against ground-truth attention data&lt;/a>;, which are typically collected by an eye tracker or &lt;a href=&quot;https://bubbleview.namwkim.org/&quot;>;approximated via mouse hovering/clicking&lt;/a>;. Previous models leveraged handcrafted features for visual clues, like color/brightness contrast, edges, and shape, while more recent approaches automatically learn discriminative features based on deep neural networks, from &lt;a href=&quot;https://arxiv.org/abs/1805.01047&quot;>;convolutional&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/1610.01708.pdf&quot;>;recurrent neural networks&lt;/a>; to more recent &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231222004714&quot;>;vision transformer networks&lt;/a>;. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Aberman_Deep_Saliency_Prior_for_Reducing_Visual_Distraction_CVPR_2022_paper.pdf&quot;>;Deep Saliency Prior for Reducing Visual Distraction&lt;/a>;” (more information on this &lt;a href=&quot;https://deep-saliency-prior.github.io/&quot;>;project site&lt;/a>;), we leverage deep saliency models for dramatic yet visually realistic edits, which can significantly change an observer&#39;s attention to different image regions. For example, removing distracting objects in the background can reduce clutter in photos, leading to increased user satisfaction. Similarly, in video conferencing, reducing clutter in the background may increase focus on the main speaker (&lt;a href=&quot;https://deep-saliency-prior.github.io/supplementary/index.html&quot;>;example demo here&lt;/a>;). &lt;/p>; &lt;p>; To explore what types of editing effects can be achieved and how these affect viewers&#39; attention, we developed an optimization framework for guiding visual attention in images using a differentiable, predictive saliency model. Our method employs a state-of-the-art deep saliency model. Given an input image and a binary mask representing the distractor regions, pixels within the mask will be edited under the guidance of the predictive saliency model such that the saliency within the masked region is reduced. To make sure the edited image is natural and realistic, we carefully choose four image editing operators: two standard image editing operations, namely recolorization and image warping (shift); and two learned operators (we do not define the editing operation explicitly), namely a multi-layer convolution filter, and a generative model (&lt;a href=&quot;https://arxiv.org/abs/1912.04958&quot;>;GAN&lt;/a>;). &lt;/p>; &lt;p>; With those operators, our framework can produce a variety of powerful effects, with examples in the figure below, including recoloring, inpainting, camouflage, object editing or insertion, and facial attribute editing. Importantly, all these effects are driven solely by the single, pre-trained saliency model, without any additional supervision or training. Note that our goal is not to compete with dedicated methods for producing each effect, but rather to demonstrate how multiple editing operations can be guided by the knowledge embedded within deep saliency models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjr1wM-E9N5w73kCxoZKqPxw_J3tWwTU8CjnrOc5vjHo6ILfwHwBzvoRslIHHr105yiyuaAGAb4iwlABhC7P2SufIfcliXyTopTkNGeNAEmMbhtytLGbEwuwdfJrnFr-pWZ_4ARVgAByAe8yL0yYT868hL3eG46uyQvb7rmhD8hqpLimXvbgsS153zRBA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of reducing visual distractions, guided by the saliency model with several operators. The distractor region is marked on top of the saliency map (red border) in each example.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Enriching experiences with user-aware saliency modeling&lt;/h2>; &lt;p>; Prior research assumes a single saliency model for the whole population. However, human attention varies between individuals — while the detection of salient clues is fairly consistent, their order, interpretation, and gaze distributions can differ substantially. This offers opportunities to create personalized user experiences for individuals or groups. In “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf&quot;>;Learning from Unique Perspectives: User-aware Saliency Modeling&lt;/a>;”, we introduce a user-aware saliency model, the first that can predict attention for one user, a group of users, and the general population, with a single model. &lt;/p>; &lt;p>; As shown in the figure below, core to the model is the combination of each participant&#39;s visual preferences with a per-user attention map and adaptive user masks. This requires per-user attention annotations to be available in the training data, eg, the &lt;a href=&quot;https://www.nature.com/articles/s41467-020-18360-5&quot;>;OSIE mobile gaze dataset for natural images; FiWI&lt;/a>; and &lt;a href=&quot;http://vision.cs.stonybrook.edu/~soura/websaliency.html&quot;>;WebSaliency&lt;/a>; datasets for web pages. Instead of predicting a single saliency map representing attention of all users, this model predicts per-user attention maps to encode individuals&#39; attention patterns. Further, the model adopts a user mask (a binary vector with the size equal to the number of participants) to indicate the presence of participants in the current sample, which makes it possible to select a group of participants and combine their preferences into a single heatmap. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s1218/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;724&quot; data-original-width=&quot;1218&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C16Z2WCjGYbAUdNXg-1Mv4VB_hJSuGVtNnCkfVcVbLxAyOVnuRhJSxbq3G3M-048QOke56RI3hCKho1q3HQKEiYgnwnKQtVnCIUQ1VSJgk-Uhhr3czhCAh3hpcuRPHJ06UCLLDb4n4vkYuJUtILGceRluacg8fIE0eGrD0BqXMWOuVUocUQ7vW6TWg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of the user aware saliency model framework. The example image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During inference, the user mask allows making predictions for any combination of participants. In the following figure, the first two rows are attention predictions for two different groups of participants (with three people in each group) on an image. A &lt;a href=&quot;https://arxiv.org/pdf/1805.01047.pdf&quot;>;conventional attention prediction model&lt;/a>; will predict identical attention heatmaps. Our model can distinguish the two groups (eg, the second group pays less attention to the face and more attention to the food than the first). Similarly, the last two rows are predictions on a webpage for two distinctive participants, with our model showing different preferences (eg, the second participant pays more attention to the left region than the first). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s961/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;961&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiKRnwjwVWTPvF57HFSZBKCK6sKmL9P30bt0K1zB_z_FeLYBP1sZsI8ZpuK1xxOEpCLQX4p9vdv9VkY855PtAIfs1vYISgJb5nSx4A0gSojHmmehxyaI-UeM9TkreSVAg-r50m7PPBzujbeUuZdU5_mBXEH905kSLikhKudLYQI2DVu_J9RhbIV4jQ1kw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Predicted attention vs. ground truth (GT). EML-Net: predictions from a state-of-the-art model, which will have the same predictions for the two participants/groups. Ours: predictions from our proposed user aware saliency model, which can predict the unique preference of each participant/group correctly. The first image is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/predicting.html&quot;>;OSIE&lt;/a>; image set, and the second is from &lt;a href=&quot;https://www-users.cse.umn.edu/~qzhao/webpage_saliency.html&quot;>;FiWI&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Progressive image decoding centered on salient features&lt;/h2>; &lt;p>; Besides image editing, human attention models can also improve users&#39; browsing experience. One of the most frustrating and annoying user experiences while browsing is waiting for web pages with images to load, especially in conditions with low network connectivity. One way to improve the user experience in such cases is with progressive decoding of images, which decodes and displays increasingly higher-resolution image sections as data are downloaded, until the full-resolution image is ready. Progressive decoding usually proceeds in a sequential order (eg, left to right, top to bottom). With a predictive attention model (&lt;a href=&quot;https://github.com/google/attention-center&quot;>;1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;2&lt;/a>;), we can instead decode images based on saliency, making it possible to send the data necessary to display details of the most salient regions first. For example, in a portrait, bytes for the face can be prioritized over those for the out-of-focus background. Consequently, users perceive better image quality earlier and experience significantly reduced wait times. More details can be found in our open source blog posts (&lt;a href=&quot;https://opensource.googleblog.com/2021/09/using-saliency-in-progressive-jpeg-xl-images.html&quot;>;post 1&lt;/a>;, &lt;a href=&quot;https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html&quot;>;post 2&lt;/a>;). Thus, predictive attention models can help with image compression and faster loading of web pages with images, improve rendering for large images and streaming/VR applications. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We&#39;ve shown how predictive models of human attention can enable delightful user experiences via applications such as image editing that can reduce clutter, distractions or artifacts in images or photos for users, and progressive image decoding that can greatly reduce the perceived waiting time for users while images are fully rendered. Our user-aware saliency model can further personalize the above applications for individual users or groups, enabling richer and more unique experiences. &lt;/p>; &lt;p>; Another interesting direction for predictive attention models is whether they can help improve robustness of computer vision models in tasks such as object classification or detection. For example, in “&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf&quot;>;Teacher-generated spatial-attention labels boost robustness and accuracy of contrastive models&lt;/a>;”, we show that a predictive human attention model can guide &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;contrastive learning&lt;/a>; models to achieve better representation and improve the accuracy/robustness of classification tasks (on the &lt;a href=&quot;https://www.image-net.org/&quot;>;ImageNet&lt;/a>; and &lt;a href=&quot;https://paperswithcode.com/dataset/imagenet-c&quot;>;ImageNet-C&lt;/a>; datasets). Further research in this direction could enable applications such as using radiologist&#39;s attention on medical images to improve health screening or diagnosis, or using human attention in complex driving scenarios to guide autonomous driving systems. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work involved collaborative efforts from a multidisciplinary team of software engineers, researchers, and cross-functional contributors. We&#39;d like to thank all the co-authors of the papers/research, including Kfir Aberman, Gamaleldin F. Elsayed, Moritz Firsching, Shi Chen, Nachiappan Valliappan, Yushi Yao, Chang Ye, Yossi Gandelsman, Inbar Mosseri, David E. Jacobes, Yael Pritch, Shaolei Shen, and Xinyu Ye. We also want to thank team members Oscar Ramirez, Venky Ramachandran and Tim Fujita for their help. Finally, we thank Vidhya Navalpakkam for her technical leadership in initiating and overseeing this body of work.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7719238929287079732/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7719238929287079732&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/enabling-delightful-user-experiences.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling delightful user experiences via predictive models of human attention&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjerKPFbdl3UNBtSQUQXn94PZtgAw7zM4KC4KRdMJxwP_iQlOUkkg6FYURau3sBRJUkeNXMoiHKy-WRc_d0Ypx4Hh4x8KGUHPkeqCXM7BR7MoX6MVOWTHylNnAcQg4ogEMk7vZJDropM08gc9t1Y2HG-GwolJuWEdhFT19yaJV9gpqbr3_ZBl-geZCfcw/s72-c/HumanAttention.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2015281937814138473&lt;/id>;&lt;published>;2023-06-09T12:09:00.001-07:00&lt;/published>;&lt;updated>;2023-06-12T07:58:34.790-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Image Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Imagen Editor and EditBench: Advancing and evaluating text-guided image inpainting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Su Wang and Ceslee Montgomery, Research Engineers, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s320/Imagen%20Editor%20&amp;amp;%20EditBench%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In the last few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Text-to-image_model&quot;>;text-to-image generation&lt;/a>; research has seen an explosion of breakthroughs (notably, &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>;, &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>;, etc.) that have naturally permeated into related topics. In particular, text-guided image editing (TGIE) is a practical task that involves editing generated and photographed visuals rather than completely redoing them. Quick, automated, and controllable editing is a convenient solution when recreating visuals would be time-consuming or infeasible (eg, tweaking objects in vacation photos or perfecting fine-grained details on a cute pup generated from scratch). Further, TGIE represents a substantial opportunity to improve training of foundational models themselves. Multimodal models require diverse data to train properly, and TGIE editing can enable the generation and recombination of high-quality and scalable synthetic data that, perhaps most importantly, can provide methods to optimize the distribution of training data along any given axis. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2212.06909&quot;>;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/a>;”, to be presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;Imagen Editor&lt;/a>;, a state-of-the-art solution for the task of masked &lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;inpainting&lt;/a>; — ie, when a user provides text instructions alongside an overlay or “mask” (usually generated within a drawing-type interface) indicating the area of the image they would like to modify. We also introduce &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;EditBench&lt;/a>;, a method that gauges the quality of image editing models. EditBench goes beyond the &lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;commonly&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=bKBhQhPeKaF&quot;>;used&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/1910.13321&quot;>;coarse-grained&lt;/a>; “does this image match this text” methods, and drills down to various types of attributes, objects, and scenes for a more fine-grained understanding of model performance. In particular, it puts strong emphasis on the faithfulness of image-text alignment without losing sight of image quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s1261/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://1.bp.blogspot.com/-2ufunOk9RJY/ZINoiOxEhUI/AAAAAAAAMVw/b-PPxjdzuXQ-wz6sgTZ1Iv2bQaBhAoqNACNcBGAsYHQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an image, a user-defined mask, and a text prompt, Imagen Editor makes localized edits to the designated areas. The model meaningfully incorporates the user&#39;s intent and performs photorealistic edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Imagen Editor&lt;/h2>; &lt;p>; Imagen Editor is a &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/940392f5f32a7ade1cc201767cf83e31-Abstract.html&quot;>;diffusion-based model&lt;/a>; fine-tuned on &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>; for editing. It targets improved representations of linguistic inputs, fine-grained control and high-fidelity outputs. Imagen Editor takes three inputs from the user: 1) the image to be edited, 2) a binary mask to specify the edit region, and 3) a text prompt — all three inputs guide the output samples. &lt;/p>; &lt;p>; Imagen Editor depends on three core techniques for high-quality text-guided image inpainting. First, unlike prior inpainting models (eg, &lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1801.07892&quot;>;Context Attention&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1806.03589&quot;>;Gated Convolution&lt;/a>;) that apply random box and stroke masks, Imagen Editor employs an object detector masking policy with an &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;object detector module&lt;/a>; that produces object masks during training. Object masks are based on detected objects rather than random patches and allow for more principled alignment between edit text prompts and masked regions. Empirically, the method helps the model stave off the prevalent issue of the text prompt being ignored when masked regions are small or only partially cover an object (eg, &lt;a href=&quot;https://arxiv.org/abs/2204.14217&quot;>;CogView2&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/s648/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;332&quot; data-original-width=&quot;648&quot; height=&quot;328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzaJwAUNaa0qyyGmGb9m0eJi2w7iRnFJpJzIXMwPpj-geuz5wnwC5QOBaXYCnSOgCrqxkU8GwVkdjaH6oxBxwafDPfSWVkID53dhTkkrv_kDcXRN0vemMoT8tEMQET6v4wL0l6pRoRCvgjmhD3u6Myi9H4qbNClFOBpU4I0QbgVCALEKZLd8RUvVkWtg/w640-h328/image9.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Random masks (&lt;strong>;left&lt;/strong>;) frequently capture background or intersect object boundaries, defining regions that can be plausibly inpainted just from image context alone. Object masks (&lt;strong>;right&lt;/strong>;) are harder to inpaint from image context alone, encouraging models to rely more on text inputs during training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Next, during training and inference, Imagen Editor enhances high resolution editing by conditioning on full resolution (1024×1024 in this work), channel-wise concatenation of the input image and the mask (similar to &lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887996&quot;>;SR3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.05826&quot;>;Palette&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2112.10741&quot;>;GLIDE&lt;/a>;). For the base diffusion 64×64 model and the 64×64→256×256 super-resolution models, we apply a parameterized downsampling convolution (eg, convolution with a stride), which we empirically find to be critical for high fidelity. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s646/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;365&quot; data-original-width=&quot;646&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwttcT7FlCELkjzvBAcSHQofxyhoQg61jfOeKYeWHupQr1gIU7ai9Midirr1zU1kdgnP_2Hb47LsWBx6QQhPfmkF2m6fw-KaH4j7woDh1RgXTS3sPV31m0NnBka_1JkEdhU3b8pTO7MKuVIZvPWwy9X3myP6x17wfO4f3AZWKVy-LhEgCyQF8qMkg4hg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Imagen is fine-tuned for image editing. All of the diffusion models, ie, the base model and super-resolution (SR) models, are conditioned on high-resolution 1024×1024 image and mask inputs. To this end, new convolutional image encoders are introduced.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Finally, at inference we apply &lt;a href=&quot;https://arxiv.org/abs/2207.12598&quot;>;classifier-free guidance&lt;/a>; (CFG) to bias samples to a particular conditioning, in this case, text prompts. CFG interpolates between the text-conditioned and unconditioned model predictions to ensure strong alignment between the generated image and the input text prompt for text-guided image inpainting. We follow &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;Imagen Video&lt;/a>; and use high guidance weights with guidance oscillation (a guidance schedule that oscillates within a value range of guidance weights). In the base model (the stage-1 64x diffusion), where ensuring strong alignment with text is most critical, we use a guidance weight schedule that oscillates between 1 and 30. We observe that high guidance weights combined with oscillating guidance result in the best trade-off between sample fidelity and text-image alignment. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;EditBench&lt;/h2>; &lt;p>; The EditBench dataset for text-guided image inpainting evaluation contains 240 images, with 120 generated and 120 natural images. Generated images are synthesized by &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>; and natural images are drawn from the &lt;a href=&quot;https://arxiv.org/abs/1602.07332&quot;>;Visual Genome&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1811.00982&quot;>;Open Images&lt;/a>; datasets. EditBench captures a wide variety of language, image types, and levels of text prompt specificity (ie, simple, rich, and full captions). Each example consists of (1) a masked input image, (2) an input text prompt, and (3) a high-quality output image used as reference for automatic metrics. To provide insight into the relative strengths and weaknesses of different models, EditBench prompts are designed to test fine-grained details along three categories: (1) attributes (eg, material, color, shape, size, count); (2) object types (eg, common, rare, text rendering); and (3) scenes (eg, indoor, outdoor, realistic, or paintings). To understand how different specifications of prompts affect model performance, we provide three text prompt types: a single-attribute (Mask Simple) or a multi-attribute description of the masked object (Mask Rich) – or an entire image description (Full Image). Mask Rich, especially, probes the models&#39; ability to handle complex attribute binding and inclusion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s531/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;531&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiLRZwI03dQY3RtegS-3jMmg7xUSpcRXRkkqPdWkuuHFQeHi-EI_Uk6tGVlxdJlpquIdZkRXxIRGJAV1uau9ISKFksXjJNZwm_LrLXzEqwsi4Kkb_Q4eHRt2RZTkEsf0Tlng9MDzv0VkRq4-CzOpDAELm_xnDhzcGGbmZJYhwNYj7MNxsc0V-57SKCI7g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The full image is used as a reference for successful inpainting. The mask covers the target object with a free-form, non-hinting shape. We evaluate Mask Simple, Mask Rich and Full Image prompts, consistent with conventional text-to-image models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Due to the intrinsic weaknesses in existing automatic evaluation metrics (&lt;a href=&quot;https://arxiv.org/abs/2104.08718&quot;>;CLIPScore&lt;/a>; and &lt;a href=&quot;https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/0a09c8844ba8f0936c20bd791130d6b6-Paper-round1.pdf&quot;>;CLIP-R-Precision&lt;/a>;) for TGIE, we hold human evaluation as the gold standard for EditBench. In the section below, we demonstrate how EditBench is applied to model evaluation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation &lt;/h2>; &lt;p>; We evaluate the Imagen Editor model — with object masking (IM) and with random masking (IM-RM) — against comparable models, &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;>;Stable Diffusion&lt;/a>; (SD) and &lt;a href=&quot;https://cdn.openai.com/papers/dall-e-2.pdf&quot;>;DALL-E 2&lt;/a>; (DL2). Imagen Editor outperforms these models by substantial margins across all EditBench evaluation categories. &lt;/p>; &lt;p>; For Full Image prompts, &lt;em>;single-image human evaluation&lt;/em>; provides binary answers to confirm if the image matches the caption. For Mask Simple prompts, single-image human evaluation confirms if the object and attribute are properly rendered, and bound correctly (eg, for a red cat, a white cat on a red table would be an incorrect binding). &lt;em>;Side-by-side human evaluation&lt;/em>; uses Mask Rich prompts only for side-by-side comparisons between IM and each of the other three models (IM-RM, DL2, and SD), and indicates which image matches with the caption better for text-image alignment, and which image is most realistic. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/s800/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;703&quot; data-original-width=&quot;800&quot; height=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVL1S0kmbq-xo-aP2FJBTNaBV1z9v8rUgXoFjru__B9DqVhYKjgOwTiXFuUtFo3idjA7ZVJnNteY1VYKL-leNOiKNayiQkwzXlRjnTpJbtM-gHUeRiCOFB1VLKau0NXmCRQktQreonSujMjEtwZZbJlVSLvaBNSk_X4La8wa2i_quAsWBWCElUqEQFPA/w640-h562/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Human evaluation. Full Image prompts elicit annotators&#39; overall impression of text-image alignment; Mask Simple and Mask Rich check for the correct inclusion of particular attributes, objects and attribute binding.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For single-image human evaluation, IM receives the highest ratings across-the-board (10–13% higher than the 2nd-highest performing model). For the rest, the performance order is IM-RM &amp;gt; DL2 &amp;gt; SD (with 3–6% difference) except for with Mask Simple, where IM-RM falls 4-8% behind. As relatively more semantic content is involved in Full and Mask Rich, we conjecture IM-RM and IM are benefited by the higher performing &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 XXL&lt;/a>; text encoder. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s1117/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1117&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimKlJIq6ksgqXxJZ2aj3_6VYpD9G9YGzYiuTaeyY1wXvL0pUSz0-5WQRLRWXrLLau1nUQr-yPvSFxHJyEX698wRZGUWN3TyxR_CzR9CEQtIBJyv-2wXWqJLFJOILp6hthc7bEfXbwcMEJD1Ngu1G1eKCkvWcpfdmdWbIpAiGMDFtoCj4wU5652UzuLFA/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations of text-guided image inpainting on EditBench by prompt type. For Mask Simple and Mask Rich prompts, text-image alignment is correct if the edited image accurately includes every attribute and object specified in the prompt, including the correct attribute binding. Note that due to different evaluation designs, Full vs. Mask-only prompts, results are less directly comparable.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; EditBench focuses on fine-grained annotation, so we evaluate models for object and attribute types. For object types, IM leads in all categories, performing 10–11% better than the 2nd-highest performing model in common, rare, and text-rendering. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s1086/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_kohtwKjIGlQU_3QpbhGoXStRXgMYeuJ210wPzLhfBca3KcNJyCPE4v9ziO3WZEH3LwuDoKQEb3MBh3Qh-ea7hd1axd91Eckn-w_LbwaxHEkE0SbiJjC6dh2TqgJVliwc1kIK7Z1NbUhVO6kimeCsU4d6LCXLBHRUrRozAL_tve94Lvk-j_8DNcBoHQ/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations on EditBench Mask Simple by object type. As a cohort, models are better at object rendering than text-rendering.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For attribute types, IM is rated much higher (13–16%) than the 2nd highest performing model, except for in count, where DL2 is merely 1% behind. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s1086/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;607&quot; data-original-width=&quot;1086&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXxQK6jb6aegOKgueFIBnQn4J9J6boT1rSGk7QquOq2BqauFb0idxSzPPpr65gntOfLlA7hjfaWKaRxO-185GsZ0KPlB6gzA3lLffEwCIsC6QAb40VJ6evsGWmxPj1RY5q699eqeN453kM4P_gItkn7u5NSeHw4BPBN1tW_oW5jfEZv7Z5XHCoaS4dTQ/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Single-image human evaluations on EditBench Mask Simple by attribute type. Object masking improves adherence to prompt attributes across-the-board (IM vs. IM-RM).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Side-by-side compared with other models one-vs-one, IM leads in text alignment with a substantial margin, being preferred by annotators compared to SD, DL2, and IM-RM. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s636/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;319&quot; data-original-width=&quot;636&quot; src=&quot;https://1.bp.blogspot.com/-0U0o8UJa8jM/ZINpLHAJa6I/AAAAAAAAMWg/1ieetbzjYAsdLSvNspPcH6RtpMgWdqgwQCNcBGAsYHQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Side-by-side human evaluation of image realism &amp;amp; text-image alignment on EditBench Mask Rich prompts. For text-image alignment, Imagen Editor is preferred in all comparisons.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Finally, we illustrate a representative side-by-side comparative for all the models. See the &lt;a href=&quot;https://arxiv.org/pdf/2212.06909.pdf&quot;>;paper&lt;/a>; for more examples. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s766/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;766&quot; data-original-width=&quot;638&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxsWO-5sX1zTSDpxq5VImIxVVpmTkY4cNOixPBiwQI8RB8jcOAo7noT3Hq1MCW5aiGGF2W3GeGxB94kSu6aolr7haZS4Oggyv76YC7VwqBkg7QnLqQQ2KiSYOIXoVFJT-y2RXqLQDbbgxolKEBtDVGDDe83RLOST7foOvy3nFpBxWHQIGpqExS1ZoMvw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example model outputs for Mask Simple vs. Mask Rich prompts. Object masking improves Imagen Editor&#39;s fine-grained adherence to the prompt compared to the same model trained with random masking.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented Imagen Editor and EditBench, making significant advancements in text-guided image inpainting and the evaluation thereof. Imagen Editor is a text-guided image inpainting fine-tuned from Imagen. EditBench is a comprehensive systematic benchmark for text-guided image inpainting, evaluating performance across multiple dimensions: attributes, objects, and scenes. Note that due to concerns in relation to responsible AI, we are not releasing Imagen Editor to the public. EditBench on the other hand is &lt;a href=&quot;https://imagen.research.google/editor/&quot;>;released in full&lt;/a>; for the benefit of the research community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments &lt;/h2>; &lt;p>; &lt;em>;Thanks to Gunjan Baid, Nicole Brichtova, Sara Mahdavi, Kathy Meier-Hellstern, Zarana Parekh, Anusha Ramesh, Tris Warkentin, Austin Waters, and Vijay Vasudevan for their generous support. We give thanks to Igor Karpov, Isabel Kraus-Liang, Raghava Ram Pamidigantam, Mahesh Maddinala, and all the anonymous human annotators for their coordination to complete the human evaluation tasks. We are grateful to Huiwen Chang, Austin Tarango, and Douglas Eck for providing paper feedback. Thanks to Erica Moreira and Victor Gomes for help with resource coordination. Finally, thanks to the authors of DALL-E 2 for giving us permission to use their model outputs for research purposes.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2015281937814138473/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2015281937814138473&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html&quot; rel=&quot;alternate&quot; title=&quot;Imagen Editor and EditBench: Advancing and evaluating text-guided image inpainting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCOhgOtXQGmv1getpW3oHgD-4dCvqg9Q_Srs4qnD-FtydmHFb6XEesvUNGkf1eXRemuok7hb58ikl8tSw4xoNSwDd_YZkrdxVgj-6Fb5AeM6DkRB32URqFjpdzLYZaOtjjHcOqXzDmh7KdshdtaNtU1cVgv69UbvwW-v4-yu6h0-XDBe7vyo6PB23dyg/s72-c/Imagen%20Editor%20&amp;%20EditBench%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-779195312945173416&lt;/id>;&lt;published>;2023-06-07T11:07:00.001-07:00&lt;/published>;&lt;updated>;2023-06-07T11:07:21.399-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Evaluating speech synthesis in many languages with SQuId&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Thibault Sellam, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheQlvvXBg5s0pU2XQwb7gxDhd_sNEHcE2EifW94vv-gYgpKF8V8BG5hKqw91qF5IzbZFnp2vZr1Am0OIQLAl29XK-MAh3XpOP05CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s320/SQuId%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Previously, we presented the &lt;a href=&quot;https://blog.google/technology/ai/ways-ai-is-scaling-helpful/&quot;>;1,000 languages initiative&lt;/a>; and the &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; with the goal of making speech and language technologies available to billions of users around the world. Part of this commitment involves developing high-quality speech synthesis technologies, which build upon projects such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/vdtts-visually-driven-text-to-speech.html&quot;>;VDTTS&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>;, for users that speak many different languages. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7-f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdBjCvVENZEDd6CweDk0fKiNJjGUd7cjgtfT9ddHQqbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy21q520QkcSuA/s234/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;159&quot; data-original-width=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcw2d97jYi825Wuq60uagpE5XMlgPhb8pgmoZ7-f6tUZwYJtvcgbqdewHceeRHM1S4M64awkxgdBjCvVENZEDd6CweDk0fKiNJjGUd7cjgtfT9ddHQqbYDOZcZ_q5qdIZcoaMYFHX5lIF4cEP1L-DguwbLqvF6rsE6msyPzw1WdPCyXy21q520QkcSuA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After developing a new model, one must evaluate whether the speech it generates is accurate and natural: the content must be relevant to the task, the pronunciation correct, the tone appropriate, and there should be no acoustic artifacts such as cracks or signal-correlated noise. Such evaluation is a major bottleneck in the development of multilingual speech systems. &lt;/p>; &lt;p>; The most popular method to evaluate the quality of speech synthesis models is human evaluation: a text-to-speech (TTS) engineer produces a few thousand utterances from the latest model, sends them for human evaluation, and receives results a few days later. This evaluation phase typically involves &lt;em>;listening tests&lt;/em>;, during which dozens of annotators listen to the utterances one after the other to determine how natural they sound. While humans are still unbeaten at detecting whether a piece of text sounds natural, this process can be impractical — especially in the early stages of research projects, when engineers need rapid feedback to test and restrategize their approach. Human evaluation is expensive, time consuming, and may be limited by the availability of raters for the languages of interest. &lt;/p>; &lt;p>; Another barrier to progress is that different projects and institutions typically use various ratings, platforms and protocols, which makes apples-to-apples comparisons impossible. In this regard, speech synthesis technologies lag behind text generation, where researchers have long complemented human evaluation with automatic metrics such as &lt;a href=&quot;https://aclanthology.org/P02-1040/&quot;>;BLEU&lt;/a>; or, more recently, &lt;a href=&quot;https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html&quot;>;BLEURT&lt;/a>;. &lt;/p>; &lt;p>; In &quot;&lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;SQuId: Measuring Speech Naturalness in Many Languages&lt;/a>;&quot;, to be presented at &lt;a href=&quot;https://2023.ieeeicassp.org/&quot;>;ICASSP 2023&lt;/a>;, we introduce SQuId (Speech Quality Identification), a 600M parameter regression model that describes to what extent a piece of speech sounds natural. SQuId is based on &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>; (a pre-trained speech-text model developed by Google), fine-tuned on over a million quality ratings across 42 languages and tested in 65. We demonstrate how SQuId can be used to complement human ratings for evaluation of many languages. This is the largest published effort of this type to date. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluating TTS with SQuId&lt;/h2>; &lt;p>; The main hypothesis behind SQuId is that training a regression model on previously collected ratings can provide us with a low-cost method for assessing the quality of a TTS model. The model can therefore be a valuable addition to a TTS researcher&#39;s evaluation toolbox, providing a near-instant, albeit less accurate alternative to human evaluation. &lt;/p>; &lt;p>; SQuId takes an utterance as input and an optional locale tag (ie, a localized variant of a language, such as &quot;Brazilian Portuguese&quot; or &quot;British English&quot;). It returns a score between 1 and 5 that indicates how natural the waveform sounds, with a higher value indicating a more natural waveform. &lt;/p>; &lt;p>; Internally, the model includes three components: (1) an encoder, (2) a pooling / regression layer, and (3) a fully connected layer. First, the &lt;em>;encoder &lt;/em>;takes a spectrogram as input and embeds it into a smaller 2D matrix that contains 3,200 vectors of size 1,024, where each vector encodes a time step. The pooling / regression layer aggregates the vectors, appends the locale tag, and feeds the result into a fully connected layer that returns a score. Finally, we apply application-specific post-processing that rescales or normalizes the score so it is within the [1, 5] range, which is common for naturalness human ratings. We train the whole model end-to-end with a regression loss. &lt;/p>; &lt;p>; The encoder is by far the largest and most important piece of the model. We used &lt;a href=&quot;https://arxiv.org/abs/2202.01374&quot;>;mSLAM&lt;/a>;, a pre-existing 600M-parameter &lt;a href=&quot;http://www.interspeech2020.org/index.php?m=content&amp;amp;c=index&amp;amp;a=show&amp;amp;catid=418&amp;amp;id=1331&quot;>;Conformer&lt;/a>; pre-trained on both speech (51 languages) and text (101 languages). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s475/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;226&quot; data-original-width=&quot;475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYyDqxha8sXDsBRVypuCZzyZbhMvZ_kB3RCjbjloyxQWaimFn05RDXX5CfyRUEy5UMBM0kgB-GntnaVz989u4h4tblGfYmfdnbTkwRKfT8zA7zEHYKm0IHf82DpzdY8gGA65uGLlEk16FK88swG0QPG5fQaXpTF7TXuCOigK4PSf50sQ9NEvorWz34pw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The SQuId model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To train and evaluate the model, we created the SQuId corpus: a collection of 1.9 million rated utterances across 66 languages, collected for over 2,000 research and product TTS projects. The SQuId corpus covers a diverse array of systems, including concatenative and neural models, for a broad range of use cases, such as driving directions and virtual assistants. Manual inspection reveals that SQuId is exposed to a vast range of of TTS errors, such as acoustic artifacts (eg, cracks and pops), incorrect prosody (eg, questions without rising intonations in English), text normalization errors (eg, verbalizing &quot;7/7&quot; as &quot;seven divided by seven” rather than &quot;July seventh&quot;), or pronunciation mistakes (eg, verbalizing &quot;tough&quot; as &quot;toe&quot;). &lt;/p>; &lt;p>; A common issue that arises when training multilingual systems is that the training data may not be uniformly available for all the languages of interest. SQuId was no exception. The following figure illustrates the size of the corpus for each locale. We see that the distribution is largely dominated by US English. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQudsSVzOyqbt1cWaLKzpadU-dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s3362/locales_blog.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;3362&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikpLHSW7usXN2qsYnUbJHNcOaANJcE1LSLLeg7ykagi60xn_fBKQNMTXCv_wOcDkCCunorkrQudsSVzOyqbt1cWaLKzpadU-dAwa3Qaj41lFSg130XnyNB2I_Bt5RXHksF0BnwfBBZ9OM5yMNBtHCXX-n_1FzDrKOKqoAKDdlllazmEk1k8Nh8EJoMjA/s16000/locales_blog.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Locale distribution in the SQuId dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; How can we provide good performance for all languages when there are such variations? Inspired by &lt;a href=&quot;https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html&quot;>;previous work on machine translation&lt;/a>;&lt;strong>;, &lt;/strong>;as well as &lt;a href=&quot;https://ieeexplore.ieee.org/document/6424230&quot;>;past work&lt;/a>; from the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DNN-MultiLingual-ICASSP2013.pdf&quot;>;speech literature&lt;/a>;, we decided to train one model for all languages, rather than using separate models for each language. The hypothesis is that if the model is large enough, then &lt;em>;cross-locale transfer&lt;/em>; can occur: the model&#39;s accuracy on each locale improves as a result of jointly training on the others. As our experiments show, cross-locale proves to be a powerful driver of performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; To understand SQuId&#39;s overall performance, we compare it to a custom Big-SSL-MOS model (described in the &lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;paper&lt;/a>;), a competitive baseline inspired by &lt;a href=&quot;https://arxiv.org/abs/2110.02635&quot;>;MOS-SSL&lt;/a>;, a state-of-the-art TTS evaluation system. Big-SSL-MOS is based on &lt;a href=&quot;https://arxiv.org/abs/2108.06209&quot;>;w2v-BERT&lt;/a>; and was trained on the &lt;a href=&quot;https://arxiv.org/pdf/2203.11389.pdf&quot;>;VoiceMOS&#39;22 Challenge&lt;/a>; dataset, the most popular dataset at the time of evaluation. We experimented with several variants of the model, and found that SQuId is up to 50.0% more accurate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNfkx7z6odaf2hEJGSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/s1131/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;550&quot; data-original-width=&quot;1131&quot; height=&quot;311&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvZGKcIEzeMTewWeCTWpKpNiLRSL4CaT8tH5xU-lD1u4Qyt5UQDLn0CORlZ2QfwKMFZ5rc4jv56RRYj_YhNfkx7z6odaf2hEJGSXVQnzmeT-eJcycjB9nC6ybYlHfTB4-LIh1UmK19ca-XFRIhC36lK6oleJcnhMsp4L0K7-TmEXKr5taJOSQ0ZpMNhg/w640-h311/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId versus state-of-the-art baselines. We measure agreement with human ratings using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&quot;>;Kendall Tau&lt;/a>;, where a higher value represents better accuracy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To understand the impact of cross-locale transfer, we run a series of ablation studies. We vary the amount of locales introduced in the training set and measure the effect on SQuId&#39;s accuracy. In English, which is already over-represented in the dataset, the effect of adding locales is negligible. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXSsNmDnnM5Q51fZ7kEjPCp2Ph667_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/s949/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;881&quot; data-original-width=&quot;949&quot; height=&quot;371&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyCRuylkQK8ZseMHYwtfMkbIal3okcJX8UFmFwzzmqM0xkDfFcvISBKymDiNyG4liTH5mhGkpkBy4QAXSsNmDnnM5Q51fZ7kEjPCp2Ph667_mvPinxxeCocJVNuFM8h8JD6TCdAhYX_JzFRzEJFTags-0dVxGpzlb0GsyzZy2VBiRFv7NZHazE8Qqdmw/w400-h371/image5.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId&#39;s performance on US English, using 1, 8, and 42 locales during fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; However, cross-locale transfer is much more effective for most other locales: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbjhO-kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwibw691lLHscb277kB9RtJXd7MEL2nStMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/s1320/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;549&quot; data-original-width=&quot;1320&quot; height=&quot;266&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbjhO-kHVhhdfB8_zKKPwFrV3IqJjShgGUJSzPoTYNu7gkmPtayfR_ql4HNRGFDBLi0sh4MNORtLHbkrheLeRqBxgjKAiH7qDGa-_qaI2E7FPOdiqMOrKwibw691lLHscb277kB9RtJXd7MEL2nStMSWvbtFYaR5RCVBbNiQDAwuNgw7SFwo4e_m1wdA/w640-h266/image6.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId&#39;s performance on four selected locales (Korean, French, Thai, and Tamil), using 1, 8, and 42 locales during fine-tuning. For each locale, we also provide the training set size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To push transfer to its limit, we held 24 locales out during training and used them for testing exclusively. Thus, we measure to what extent SQuId can deal with languages that it has never seen before. The plot below shows that although the effect is not uniform, cross-locale transfer works. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizF9U8Wxz-zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGxarp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZVp0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/s823/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;340&quot; data-original-width=&quot;823&quot; height=&quot;264&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizF9U8Wxz-zsPZvqSJM1QQL7O-8TURWdbPAkzPjthZSdejs5ySvj26kysmwl9FmiFIesgzoJ5w68Zkc7_qE-KEi4SFwXY16PBTn0zukQ7w4p9pZ2RvGxarp8zZFvv-H5cEOHx5q7vibhUp_0ZdpZVp0s2NAHll0b6FrxN9z8YW-fOJyN8jtLXjK1wqUQ/w640-h264/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;SQuId&#39;s performance on four &quot;zero-shot&quot; locales; using 1, 8, and 42 locales during fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; When does cross-locale operate, and how? We present many more ablations in the &lt;a href=&quot;https://arxiv.org/abs/2210.06324&quot;>;paper&lt;/a>;, and show that while language similarity plays a role (eg, training on Brazilian Portuguese helps European Portuguese) it is surprisingly far from being the only factor that matters. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We introduce SQuId, a 600M parameter regression model that leverages the SQuId dataset and cross-locale learning to evaluate speech quality and describe how natural it sounds. We demonstrate that SQuId can complement human raters in the evaluation of many languages. Future work includes accuracy improvements, expanding the range of languages covered, and tackling new error types. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The author of this post is now part of Google DeepMind. Many thanks to all authors of the paper: Ankur Bapna, Joshua Camp, Diana Mackinnon, Ankur P. Parikh, and Jason Riesa.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/779195312945173416/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/evaluating-speech-synthesis-in-many.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/779195312945173416&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/779195312945173416&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/evaluating-speech-synthesis-in-many.html&quot; rel=&quot;alternate&quot; title=&quot;Evaluating speech synthesis in many languages with SQuId&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheQlvvXBg5s0pU2XQwb7gxDhd_sNEHcE2EifW94vv-gYgpKF8V8BG5hKqw91qF5IzbZFnp2vZr1Am0OIQLAl29XK-MAh3XpOP05CzqCSLy1arpJ5gC4mJ4OMs0CGxc0iE4I9Rn5xcZaLFitBEK0lFQF1yURehcGbPYBsnKmpqE21VQHKovgAz76av4pg/s72-c/SQuId%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1995595447829405143&lt;/id>;&lt;published>;2023-06-06T09:58:00.000-07:00&lt;/published>;&lt;updated>;2023-06-06T09:58:41.452-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Visual captions: Using large language models to augment video conferences with dynamic visuals&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ruofei Du, Research Scientist, and Alex Olwal, Senior Staff Research Scientist, Google Augmented Reality&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYYNNO0NMhCPTDsGDkVzs2FJvITsOknz8GcO4hJ7Eds57YVXdFtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s745/VisualCaptions.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Recent advances in video conferencing have significantly improved remote video communication through features like live captioning and noise cancellation. However, there are various situations where dynamic visual augmentation would be useful to better convey complex and nuanced information. For example, when discussing what to order at a Japanese restaurant, your friends could share visuals that would help you feel more confident about ordering the “Sukiyaki”. Or when talking about your recent family trip to San Francisco, you may want to show a photo from your personal album. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://research.google/pubs/pub52074/&quot;>;Visual Captions: Augmenting Verbal Communication With On-the-fly Visuals&lt;/a>;”, presented at &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/95817&quot;>;ACM CHI 2023&lt;/a>;, we introduce a system that uses verbal cues to augment synchronous video communication with real-time visuals. We fine-tuned a large language model to proactively suggest relevant visuals in open-vocabulary conversations using a &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;dataset&lt;/a>; we curated for this purpose. We open sourced Visual Captions as part of the &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; project, which is designed for rapid prototyping of augmented communication with real-time transcription. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA1HeOomCg2vQk61Flc7P58lAPXiiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s640/image11.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;277&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEieLD2jgO2DgpK5t9TrF7tg0-yHiPC0gtCdhJZcKBPx5cPo8JmWDVouffM6GK8hlkszzcR_rU9i-ym3WrA1HeOomCg2vQk61Flc7P58lAPXiiBS7ugc1S3Ah-p8GudwKJ3OJPew8xURhnesV5WP84MUAej6oF1jIY6CtXgv-gk1qLhs7GDTv7N8LYMz6A/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual Captions facilitates verbal communication with real-time visuals. The system is even robust against typical mistakes that may often appear in real-time speech-to-text transcription. For example, out of context, the transcription model misunderstood the word &quot;pier&quot; as &quot;pair&quot;, but Visual Captions still recommends images of the Santa Monica Pier.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Design space for augmenting verbal communication with dynamic visuals&lt;/h2>; &lt;p>; We invited 10 internal participants, each with various technical and non-technical backgrounds, including software engineers, researchers, UX designers, visual artists, students, etc., to discuss their particular needs and desires for a potential real-time visual augmentation service. In two sessions, we introduced low-fidelity prototypes of the envisioned system, followed by video demos of the existing text-to-image systems. These discussions informed a design space with eight dimensions for visual augmentation of real-time conversations, labeled below as D1 to D8. &lt;/p>; &lt;p>; Visual augmentations could be synchronous or asynchronous with the conversation (D1: Temporal), could be used for both expressing and understanding speech content (D2: Subject), and could be applied using a wide range of different visual content, visual types, and visual sources (D3: Visual). Such visual augmentation might vary depending on the scale of the meetings (D4: Scale) and whether a meeting is in co-located or remote settings (D5: Space). These factors also influence whether the visuals should be displayed privately, shared between participants, or public to everyone (D6: Privacy). Participants also identified different ways in which they would like to interact with the system while having conversations (D7: Initiation). For example, people proposed different levels of “proactivity”, which indicates the degree to which users would like the model to take the initiative. Finally, participants envisioned different methods of interaction, for example, using speech or gestures for input. (D8: Interaction). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hptVhSDF4gwBpPY3Ed6m6cMGzzT358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1547&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXvp9pvpgO87EEQo0nF8rMWRppqE1TKd6kGzNeiiSdpWuzH7gLKTzdj2_m6p-VP-1w2EjF2t6hptVhSDF4gwBpPY3Ed6m6cMGzzT358eZPVA9fyrGYRBpDY5aTh44G-ybHvzPn4VZyyHDkzvp0JpdG6-clKwVUlg56L6lobZjc5SZMiOcBlo4bIA8mMQ/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Design space for augmenting verbal communication with dynamic visuals.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Informed by this initial feedback, we designed Visual Captions to focus on generating &lt;em>;synchronous&lt;/em>; visuals of semantically relevant &lt;em>;visual content&lt;/em>;, &lt;em>;type&lt;/em>;, and &lt;em>;source&lt;/em>;. While participants in these initial exploratory sessions were participating in one-to-one remote conversations, deployment of Visual Captions in the wild will often be in one-to-many (eg, an individual giving a presentation to an audience) and many-to-many scenarios (eg, a discussion among multiple people in a meeting). &lt;/p>; &lt;p>; Because the visual that best complements a conversation depends strongly on the context of the discussion, we needed a training set specific to this purpose. So, we collected a dataset of 1595 quadruples of &lt;em>;language (1)&lt;/em>;, &lt;em>;visual content (2)&lt;/em>;, &lt;em>;type (3)&lt;/em>;, and &lt;em>;source (4)&lt;/em>; across a variety of contexts, including daily conversations, lectures, and travel guides. For example, “I would love to see it!” corresponds to visual content of “face smiling”, a visual type of “emoji”, and visual source of “public search”. “Did she tell you about our trip to Mexico?” corresponds to visual content of “a photo from the trip to Mexico&#39;&#39;, a visual type of “photo”, and visual source of “personal album”. We publicly released this &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K dataset&lt;/a>; for the research community. &lt;/p>; &lt;br />; &lt;h2>;Visual intent prediction model&lt;/h2>; &lt;p>; To predict what visuals could supplement a conversation, we trained a visual intent prediction model based on a large language model using the VC1.5K dataset. For training, we parsed each visual intent into the format of &quot;&lt;code style=&quot;font-size: small;&quot;>;&amp;lt;Visual Type&amp;gt; of &amp;lt;Visual Content&amp;gt; from &amp;lt;Visual Source&amp;gt;&lt;/code>;&quot;. &lt;/p>; &lt;pre class=&quot;prettyprint&quot; style=&quot;font-size: small; margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>;{&quot;prompt&quot;: &quot;&amp;lt;Previous Two Sentences&amp;gt; →&quot;, &quot;completion&quot;: &quot;&amp;lt;Visual Type 1&amp;gt; of &quot;&amp;lt;Visual Type 1&amp;gt; from &quot;&amp;lt;Visual Source 1&amp;gt;; &amp;lt;Visual Type 2&amp;gt; of &quot;&amp;lt;Visual Type 2&amp;gt; from &quot;&amp;lt;Visual Source 2&amp;gt;; ... \𝑛&quot;} &lt;/pre>; &lt;p>; Using this format, this system can handle open-vocabulary conversations and contextually predict visual content, visual source, and visual type. Anecdotally, we found that it outperforms keyword-based approaches, which fail to handle open-vocabulary examples like “Your aunt Amy will be visiting this Saturday,” and cannot suggest relevant visual types or visual sources.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWWCNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevobH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s2552/vc-examples.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1900&quot; data-original-width=&quot;2552&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtY2Jw884pCstWr6aMdcy4Ge6EmxUbMk7YIEiVX3PrDgteVh3Fhw9Ug74ygoBXCHOZUswYcNyNjf5Lckn8zWWCNxlJyvrWQTIWqGuZMYoeJnsTYiD4Anv8Wssu1nZ5dKQcevobH61kYSL8_UYBD3qF_aQoTSjD0GZc7fFS4ehf6yFcxhkUYZBy7EvOwQ/s16000/vc-examples.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of visual intent predictions by our model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We used 1276 (80%) examples from the VC1.5K dataset for fine-tuning the large language model and the remaining 319 (20%) examples as test data. We measured the performance of the fine-tuned model with the token accuracy metric, ie, the percentage of tokens in a batch that were correctly predicted by the model. During training, our model reached a training token accuracy of 97% and a validation token accuracy of 87%. &lt;/p>; &lt;br />; &lt;h2>;Performance&lt;/h2>; &lt;p>; To evaluate the utility of the trained Visual Captions model, we invited 89 participants to perform 846 tasks. They were asked to provide feedback on a scale of &quot;1 — Strongly Disagree&quot; to &quot;7 — Strongly Agree&quot; for six qualitative statements. Most participants preferred to have the visual during a conversation (Q1, 83% ≥ 5–Somewhat Agree). Moreover, they considered the displayed visuals to be useful and informative (Q2, 82% ≥ 5–Somewhat Agree), high-quality (Q3, 82% ≥ 5–Somewhat Agree), and relevant to the original speech (Q4, 84% ≥ 5–Somewhat Agree). Participants also found the predicted visual type (Q5, 87% ≥ 5–Somewhat Agree) and visual source (Q6, 86% ≥ 5–Somewhat Agree) to be accurate given the context of the corresponding conversation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsUVfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s1828/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1828&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgks8-CM1HIfvpCikBqx2EuVzdZgN0oLFZixeTcm0Fazt7vhUnR7olQqiMi12m1itphv0Pmtq0RxoPCywVBsw9OU8irUBarqTfHDJfIGpeOUrCmIeBtd5AsUVfPlbPxGXkYzgmMYu9l1JqzhHJy8Sld7rSi-DyuQeSOytHtP54rBOrbuLbCB2ndNozKLg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Technical evaluation results of the visual prediction model rated by study participants.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With this fine-tuned visual intent prediction model, we developed Visual Captions on the &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>; platform, which can add new interactive widgets directly on the camera streams of video conferencing platforms, such as &lt;a href=&quot;https://apps.google.com/meet/&quot;>;Google Meet&lt;/a>;. As shown in the system workflow below, Visual Captions automatically captures the user&#39;s speech, retrieves the last sentences, feeds them into the visual intent prediction model every 100 ms, retrieves relevant visuals, and then suggests visuals in real time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSLy_XLW3_l6NAtJJ-tkrxOYrL_kuW2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s1756/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;662&quot; data-original-width=&quot;1756&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQVHhdvhqftCX1QDtTRSfAYaYMBT_0blsN_WOaRf44C-c0R2tggMy2yddOpSnjSRfhIOQ793JAZfNwSLy_XLW3_l6NAtJJ-tkrxOYrL_kuW2D5HWYbCs3dnt0OGINHOwuYE5DIK2dp8Ia_Oq3-y4mqntpZ3hK0a44pW7umjy3KEwIhUw1z46j0NsJV7A/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;System workflow of Visual Captions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Visual Captions provides three levels of proactivity when suggesting visuals: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Auto-display&lt;/em>; (high-proactivity): The system autonomously searches and displays visuals publicly to all meeting participants. No user interaction required. &lt;/li>;&lt;li>;&lt;em>;Auto-suggest&lt;/em>; (medium-proactivity): The suggested visuals are shown in a private scrolling view. A user then clicks a visual to display it publicly. In this mode, the system is proactively recommending visuals, but the user decides when and what to display. &lt;/li>;&lt;li>;&lt;em>;On-demand-suggest &lt;/em>;(low-proactivity): The system will only suggest visuals if a user presses the spacebar. &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Quantitative and qualitative evaluation: User studies&lt;/h2>; &lt;p>; We evaluated Visual Captions in both a controlled lab study (&lt;em>;n &lt;/em>;= 26) and in-the-wild deployment studies (&lt;em>;n&lt;/em>; = 10). Participants found that real-time visuals facilitated live conversations by helping explain unfamiliar concepts, resolve language ambiguities, and make conversations more engaging. Participants also reported different preferences for interacting with the system in-situ, and that varying levels of proactivity were preferred in different social scenarios. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4Li4mjiAeqyjfqgdB0GhfkB96_sZcYjApSk9Cj9yHWyINEskK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;590&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitIGdmw1Po1EpiimJnsXctKJIVpeptyNRJYmf-fSIBkEymdJCcvI9muj5Ov_ohIKdIeScv3gvpm0RJli8V2J4Li4mjiAeqyjfqgdB0GhfkB96_sZcYjApSk9Cj9yHWyINEskK5Fic-9ET0kaZGtSGXXdzdjOJDKHcE52pzrGixYPjpZcYpNFHPzxazfw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Participants&#39; &lt;a href=&quot;https://en.wikipedia.org/wiki/NASA-TLX&quot;>;Task Load Index&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;Likert scale&lt;/a>; ratings (from 1 - Strongly Disagree to 7 - Strongly Agree) of four conversations without Visual Captions (“No VC”) and the three Visual Captions modes: auto-display, auto-suggest, and on-demand suggest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusions and future directions&lt;/h2>; &lt;p>; This work proposes a system for real-time visual augmentation of verbal communication, called Visual Captions, that was trained using a dataset of 1595 visual intents collected from 246 participants, covering 15 topic categories. We publicly release the training dataset, &lt;a href=&quot;https://github.com/google/archat/tree/main/dataset&quot;>;VC1.5K&lt;/a>; to the research community to support further research in this space. We have also deployed Visual Captions in &lt;a href=&quot;https://github.com/google/archat&quot;>;ARChat&lt;/a>;, which facilitates video conferences in Google Meet by transcribing meetings and augmenting the camera video streams. &lt;/p>; &lt;p>; Visual Captions represents a significant step towards enhancing verbal communication with on-the-fly visuals. By understanding the importance of visual cues in everyday conversations, we can create more effective communication tools and improve how people connect. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Key contributors to the project include Xingyu “Bruce” Liu, Vladimir Kirilyuk, Xiuxiu Yuan, Peggy Chi, Alex Olwal, and Ruofei Du.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We would like to extend our thanks to those on the ARChat team who provided assistance, including Jason Mayes, Max Spear, Na Li, Jun Zhang, Jing Jin, Yuan Ren, Adarsh Kowdle, Ping Yu, Darcy Philippon, and Ezgi Oztelcan. We would also like to thank the many people with whom we&#39;ve had insightful discussions and those who provided feedback on the manuscript, including Eric Turner, Yinda Zhang, Feitong Tan, Danhang Tang, and Shahram Izadi. We would also like to thank our CHI reviewers for their insightful feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1995595447829405143/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1995595447829405143&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1995595447829405143&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/visual-captions-using-large-language.html&quot; rel=&quot;alternate&quot; title=&quot;Visual captions: Using large language models to augment video conferences with dynamic visuals&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuQA-vtXoNbaUU05FVACHS1vMdvtOxK5pcusTeWW2dkG9OpPiKHLtRuCbtNlAMX4TyRYYNNO0NMhCPTDsGDkVzs2FJvITsOknz8GcO4hJ7Eds57YVXdFtQW3fEzNxiLq0MQC2G3GW0ESfVqOBogCsbIDc1YfQpIgKsJm0idXET58ia266bNjb63HHuDw/s72-c/VisualCaptions.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5739375622760902222&lt;/id>;&lt;published>;2023-06-02T10:02:00.000-07:00&lt;/published>;&lt;updated>;2023-06-02T10:02:37.777-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Automatic Speech Recognition&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AVFormer: Injecting vision into frozen speech models for zero-shot AV-ASR&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Arsha Nagrani and Paul Hongsuck Seo, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIv3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s1150/AVFormer.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;Automatic speech recognition&lt;/a>; (ASR) is a well-established technology that is widely adopted for various applications such as conference calls, streamed video transcription and voice commands. While the challenges for this technology are centered around noisy &lt;em>;audio&lt;/em>; inputs, the &lt;em>;visual&lt;/em>; stream in multimodal videos (eg, TV, online edited videos) can provide strong cues for improving the robustness of ASR systems — this is called audiovisual ASR (AV-ASR). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Although lip motion can provide strong signals for speech recognition and is the most common area of focus for AV-ASR, the mouth is often not directly visible in &lt;em>;videos in the wild&lt;/em>; (eg, due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Egocentric_vision&quot;>;egocentric viewpoints&lt;/a>;, face coverings, and low resolution) and therefore, a new emerging area of research is &lt;em>;unconstrained&lt;/em>; AV-ASR (eg, &lt;a href=&quot;https://arxiv.org/abs/2206.07684&quot;>;AVATAR&lt;/a>;), which investigates the contribution of entire visual frames, and not just the mouth region. &lt;/p>; &lt;p>; Building audiovisual datasets for training AV-ASR models, however, is challenging. Datasets such as &lt;a href=&quot;https://srvk.github.io/how2-dataset/&quot;>;How2&lt;/a>; and &lt;a href=&quot;https://gabeur.github.io/avatar-visspeech&quot;>;VisSpeech&lt;/a>; have been created from instructional videos online, but they are small in size. In contrast, the models themselves are typically large and consist of both visual and audio encoders, and so they tend to overfit on these small datasets. Nonetheless, there have been a number of recently released large-scale audio-only models that are heavily optimized via large-scale training on massive &lt;em>;audio-only&lt;/em>; data obtained from audio books, such as &lt;a href=&quot;https://github.com/facebookresearch/libri-light&quot;>;LibriLight&lt;/a>; and &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;. These models contain billions of parameters, are readily available, and show strong generalization across domains. &lt;/p>; &lt;p>; With the above challenges in mind, in “&lt;a href=&quot;https://arxiv.org/pdf/2303.16501.pdf&quot;>;AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR&lt;/a>;”, we present a simple method for augmenting existing large-scale audio-only models with visual information, at the same time performing lightweight domain adaptation. AVFormer injects visual embeddings into a frozen ASR model (similar to how Flamingo &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;injects visual information&lt;/a>; into large language models for vision-text tasks) using lightweight trainable adaptors that can be trained on a small amount of weakly labeled video data with minimum additional training time and parameters. We also introduce a simple curriculum scheme during training, which we show is crucial to enable the model to jointly process audio and visual information effectively. The resulting AVFormer model achieves state-of-the-art zero-shot performance on three different AV-ASR benchmarks (How2, VisSpeech and &lt;a href=&quot;https://ego4d-data.org/&quot;>;Ego4D&lt;/a>;), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (ie, &lt;a href=&quot;https://www.openslr.org/12&quot;>;LibriSpeech&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s958/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;286&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxMFN8ianu3Y1iJ4tB5Bt7xSnYsHXt3-oYl3LnZjpHt9bPrUHFgQ14msppOwZ7TmOFMUSXc86l5tcuB1W6SroOQiiT__IB7ZyeiHRxTf51xY-f_i3iWCRzIDmB2ciAlvjsNATTDVBt0nvuEnWBQSoKou0PNBF7UYfkI2xM0MxPZa1JlxtOSHePEgxiVw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Unconstrained audiovisual speech recognition. We inject vision into a frozen speech model (&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;BEST-RQ&lt;/a>;, in grey) for zero-shot audiovisual ASR via lightweight modules to create a parameter- and data-efficient model called AVFormer (blue). The visual context can provide helpful clues for robust speech recognition especially when the audio signal is noisy (the visual loaf of bread helps correct the audio-only mistake “clove” to “loaf” in the generated transcript).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Injecting vision using lightweight modules&lt;/h2>; &lt;p>; Our goal is to add visual understanding capabilities to an existing audio-only ASR model while maintaining its generalization performance to various domains (both AV and audio-only domains). &lt;/p>; &lt;p>; To achieve this, we augment an existing state-of-the-art ASR model (&lt;a href=&quot;https://arxiv.org/pdf/2202.01855.pdf&quot;>;Best-RQ&lt;/a>;) with the following two components: (i) linear visual projector and (ii) lightweight adapters. The former projects visual features in the audio token embedding space. This process allows the model to properly connect separately pre-trained visual feature and audio input token representations. The latter then minimally modifies the model to add understanding of multimodal inputs from videos. We then train these additional modules on unlabeled web videos from the &lt;a href=&quot;https://www.di.ens.fr/willow/research/howto100m/&quot;>;HowTo100M dataset&lt;/a>;, along with the outputs of an ASR model as pseudo ground truth, while keeping the rest of the Best-RQ model frozen. Such lightweight modules enable data-efficiency and strong generalization of performance. &lt;/p>; &lt;p>; We evaluated our extended model on AV-ASR benchmarks in a zero-shot setting, where the model is never trained on a manually annotated AV-ASR dataset. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Curriculum learning for vision injection&lt;/h2>; &lt;p>; After the initial evaluation, we discovered empirically that with a naïve single round of joint training, the model struggles to learn both the adapters and the visual projectors in one go. To mitigate this issue, we introduced a two-phase &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/8403835/&quot;>;curriculum learning strategy&lt;/a>; that decouples these two factors — domain adaptation and visual feature integration — and trains the network in a sequential manner. In the first phase, the adapter parameters are optimized without feeding visual tokens at all. Once the adapters are trained, we add the visual tokens and train the visual projection layers alone in the second phase while the trained adapters are kept frozen. &lt;/p>; &lt;p>; The first stage focuses on audio domain adaptation. By the second phase, the adapters are completely frozen and the visual projector must simply learn to generate visual prompts that project the visual tokens into the audio space. In this way, our curriculum learning strategy allows the model to incorporate visual inputs as well as adapt to new audio domains in AV-ASR benchmarks. We apply each phase just once, as an iterative application of alternating phases leads to performance degradation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJj6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s1318/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;1318&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_hr_fd1iI5JofcM_c_4rRIayzvF5dlQ7T-keHxvHuLRlqmlKhs_Yy-IOuiYrj8GrnscXNB_vN1oXqlTnxvzL0dyl9PraXfuIQ91lQttaY4-e019DmRCxRllEIJj6T3RGn5J-RyTpALh1xgpVugcYQBy20rQMbjLg2CKLTazNJLM37lAWL3kiHH8pmGA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall architecture and training procedure for AVFormer. The architecture consists of a frozen &lt;a href=&quot;https://arxiv.org/pdf/2005.08100.pdf&quot;>;Conformer&lt;/a>; encoder-decoder model, and a frozen &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; encoder (frozen layers shown in gray with a lock symbol), in conjunction with two lightweight trainable modules - (i) visual projection layer (orange) and bottleneck adapters (blue) to enable multimodal domain adaptation. We propose a two-phase curriculum learning strategy: the adapters (blue) are first trained without any visual tokens, after which the visual projection layer (orange) is tuned while all the other parts are kept frozen.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The plots below show that without curriculum learning, our AV-ASR model is worse than the audio-only baseline across all datasets, with the gap increasing as more visual tokens are added. In contrast, when the proposed two-phase curriculum is applied, our AV-ASR model performs significantly better than the baseline audio-only model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s1374/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1053&quot; data-original-width=&quot;1374&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxlm_YiD6Bdma_rFDzaZL4lnvmSamUWD5l6jLB8F4411UV1UL2Pe3Ot3iBdNhJpXhYMsa_2m_3VZ6VnZCWmK4K50h2LfGdMBP-_TqTWBAFKX-Aqq9rdpo9P1n48TV3zPTIBBwfyMEbzHMdTvU5rq1OZVkao5K-Gjl3kk7O9zsAFGk56aaMS2xrOr8T8w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effects of curriculum learning. Red and blue lines are for audiovisual models and are shown on 3 datasets in the zero-shot setting (lower &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;WER&lt;/a>; % is better). Using the curriculum helps on all 3 datasets (for How2 (a) and Ego4D (c) it is crucial for outperforming audio-only performance). Performance improves up until 4 visual tokens, at which point it saturates.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results in zero-shot AV-ASR&lt;/h2>; &lt;p>; We compare AVFormer to BEST-RQ, the audio version of our model, and AVATAR, the state of the art in AV-ASR, for zero-shot performance on the three AV-ASR benchmarks: How2, VisSpeech and Ego4D. AVFormer outperforms AVATAR and BEST-RQ on all, even outperforming both AVATAR and BEST-RQ when they are trained on LibriSpeech and the full set of HowTo100M. This is notable because for BEST-RQ, this involves training 600M parameters, while AVFormer only trains 4M parameters and therefore requires only a small fraction of the training dataset (5% of HowTo100M). Moreover, we also evaluate performance on LibriSpeech, which is audio-only, and AVFormer outperforms both baselines. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s3475/AVFormer_results%20(1).png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2129&quot; data-original-width=&quot;3475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNT2kWtP5VgTsln7LWDMpxkLacsXHRAaM8rHxnUkL3o3mh5UAt3CFZ02wX-AzllqEw7V5fDYFC5yDe-QVO9oMjFPx6b2Lw9qyCsgXUVQm4JQPqbN52V4D9u9SwaR79aF7vXsGHMzxN4StK0YZe059gxa_pUXRwea44zz8wyIs6drTrxyk8Qa48CGr1g/s16000/AVFormer_results%20(1).png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison to state-of-the-art methods for zero-shot performance across different AV-ASR datasets. We also show performances on LibriSpeech which is audio-only. Results are reported as WER % (lower is better). AVATAR and BEST-RQ are finetuned end-to-end (all parameters) on HowTo100M whereas AVFormer works effectively even with 5% of the dataset thanks to the small set of finetuned parameters.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We introduce AVFormer, a lightweight method for adapting existing, frozen state-of-the-art ASR models for AV-ASR. Our approach is practical and efficient, and achieves impressive zero-shot performance. As ASR models get larger and larger, tuning the entire parameter set of pre-trained models becomes impractical (even more so for different domains). Our method seamlessly allows both domain transfer and visual input mixing in the same, parameter efficient model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Paul Hongsuck Seo, Arsha Nagrani and Cordelia Schmid.&lt;/em>; &lt;/p>; &lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5739375622760902222/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5739375622760902222&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/avformer-injecting-vision-into-frozen.html&quot; rel=&quot;alternate&quot; title=&quot;AVFormer: Injecting vision into frozen speech models for zero-shot AV-ASR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEpnVot9nisUn-B5GierK8WfqI_mhVwKmJ1mjj_FOdOr2_74gIwvwSJgobi8K_a1uq9n5B3pO4Zi5-mXoE0mzOw32WW-ny6kBThTgHIv3wtQVrpjdMdhMYCSqustzlN7ArP3y2jSqEy_2rlz_zXfsfp05Z3svQ7bu8rYJAN6V1ellHjbL-SJOHraUHEQ/s72-c/AVFormer.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-791000644276225005&lt;/id>;&lt;published>;2023-06-01T10:25:00.000-07:00&lt;/published>;&lt;updated>;2023-06-01T10:25:21.235-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Retrieval-augmented visual-language pre-training&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s320/REVEAL%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Large-scale models, such as &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, &lt;a href=&quot;https://openai.com/research/language-models-are-few-shot-learners&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI&lt;/a>;, have demonstrated the ability to store substantial amounts of knowledge when scaled to tens of billions of parameters and trained on large text and image datasets. These models achieve state-of-the-art results on downstream tasks, such as image captioning, visual question answering and open vocabulary recognition. Despite such achievements, these models require a massive volume of data for training and end up with a tremendous number of parameters (billions in many cases), resulting in significant computational requirements. Moreover, the data used to train these models can become outdated, requiring re-training every time the world&#39;s knowledge is updated. For example, a model trained just two years ago might yield outdated information about the current president of the United States. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the fields of natural language processing (&lt;a href=&quot;https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens&quot;>;RETRO&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>;) and computer vision (&lt;a href=&quot;https://arxiv.org/abs/2112.08614&quot;>;KAT&lt;/a>;), researchers have attempted to address these challenges using retrieval-augmented models. Typically, these models use a backbone that is able to process a single modality at a time, eg, only text or only images, to encode and retrieve information from a knowledge corpus. However, these retrieval-augmented models are unable to leverage all available modalities in a query and knowledge corpora, and may not find the information that is most helpful for generating the model&#39;s output. &lt;/p>; &lt;p>; To address these issues, in “&lt;a href=&quot;https://arxiv.org/abs/2212.05221&quot;>;REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory&lt;/a>;”, to appear at &lt;a href=&quot;https://cvpr2023.thecvf.com/&quot;>;CVPR 2023&lt;/a>;, we introduce a visual-language model that learns to utilize a multi-source multi-modal “memory” to answer knowledge-intensive queries. REVEAL employs &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;>;neural representation learning&lt;/a>; to encode and convert diverse knowledge sources into a memory structure consisting of key-value pairs. The keys serve as indices for the memory items, while the corresponding values store pertinent information about those items. During training, REVEAL learns the key embeddings, value tokens, and the ability to retrieve information from this memory to address knowledge-intensive queries. This approach allows the model parameters to focus on reasoning about the query, rather than being dedicated to memorization. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s1440/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;579&quot; data-original-width=&quot;1440&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWSYZK0R2z8mvSezb9KbBJB8CFtBMQkfTamLXMUNTjOIMQJb8-4-VAeMmboZLusdXtY6MwPgAV_9B4Um23lYa9llZnYa1xLpXKrPmF_wwMPK-Orlw3t5BNJZzpHR8P6R4f7OZK46E5arqJZApeMnjD2OdBWaclvviiKVDB2yQD1YYMs5yiS9ix83NBdA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We augment a visual-language model with the ability to retrieve multiple knowledge entries from a diverse set of knowledge sources, which helps generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Memory construction from multimodal knowledge corpora&lt;/h2>; &lt;p>; Our approach is similar to &lt;a href=&quot;https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html&quot;>;REALM&lt;/a>; in that we precompute key and value embeddings of knowledge items from different sources and index them in a unified knowledge memory, where each knowledge item is encoded into a key-value pair. Each key is a &lt;em>;d&lt;/em>;-dimensional embedding vector, while each value is a sequence of token embeddings representing the knowledge item in more detail. In contrast to previous work, REVEAL leverages a diverse set of multimodal knowledge corpora, including the &lt;a href=&quot;https://research.google/pubs/pub42240/&quot;>;WikiData knowledge graph&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2103.01913&quot;>;Wikipedia passages and images&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2102.08981&quot;>;web image-text pairs&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;>;visual question answering data&lt;/a>;. Each knowledge item could be text, an image, a combination of both (eg, pages in Wikipedia) or a relationship or attribute from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; (eg, Barack Obama is 6&#39; 2” tall). During training, we continuously re-compute the memory key and value embeddings as the model parameters get updated. We update the memory asynchronously at every thousand training steps. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Scaling memory using compression &lt;/h3>; &lt;p>; A naïve solution for encoding a memory value is to keep the whole sequence of tokens for each knowledge item. Then, the model could fuse the input query and the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;>;top-k&lt;/a>; retrieved memory values by concatenating all their tokens together and feeding them into a &lt;a href=&quot;https://huggingface.co/blog/encoder-decoder&quot;>;transformer encoder-decoder&lt;/a>; pipeline. This approach has two issues: (1) storing hundreds of millions of knowledge items in memory is impractical if each memory value consists of hundreds of tokens and (2) the transformer encoder has a quadratic complexity with respect to the total number of tokens times &lt;em>;k&lt;/em>; for &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;self-attention&lt;/a>;. Therefore, we propose to use the &lt;a href=&quot;https://www.deepmind.com/publications/perceiver-general-perception-with-iterative-attention&quot;>;Perceiver architecture&lt;/a>; to encode and compress knowledge items. The Perceiver model uses a transformer decoder to compress the full token sequence into an arbitrary length. This lets us retrieve top-&lt;em>;k&lt;/em>; memory entries for &lt;em>;k&lt;/em>; as large as a hundred. &lt;/p>; &lt;p>; The following figure illustrates the procedure of constructing the memory key-value pairs. Each knowledge item is processed through a multi-modal visual-language encoder, resulting in a sequence of image and text tokens. The key head then transforms these tokens into a compact embedding vector. The value head (perceiver) condenses these tokens into fewer ones, retaining the pertinent information about the knowledge item within them. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;776&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEis_OAb1NNR8mM6vUH-Q0CU56sxDLX0FGfUI6sOGGY2STGzlwFh4Jk3O6nYyrVtc4JsxE9OJQo4EvJOoVK7ZDMiqt5BJ5VDO87CbqnOfcHJXg6czk7rGbgkwHzOApsRVgybqLUr8f8h9Lo1SGdd4eT7zMLgACejH3LHEyT_tcgLKmIdMKLv2_N4gVZRKQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We encode the knowledge entries from different corpora into unified key and value embedding pairs, where the keys are used to index the memory and values contain information about the entries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-scale pre-training on image-text pairs&lt;/h2>; &lt;p>; To train the REVEAL model, we begin with the large-scale corpus, collected from the public Web with three billion image alt-text caption pairs, introduced in &lt;a href=&quot;https://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html&quot;>;LiT&lt;/a>;. Since the dataset is noisy, we add a filter to remove data points with captions shorter than 50 characters, which yields roughly 1.3 billion image caption pairs. We then take these pairs, combined with the text generation objective used in &lt;a href=&quot;https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html&quot;>;SimVLM&lt;/a>;, to train REVEAL. Given an image-text example, we randomly sample a prefix containing the first few tokens of the text. We feed the text prefix and image to the model as input with the objective of generating the rest of the text as output. The training goal is to condition the prefix and autoregressively generate the remaining text sequence. &lt;/p>; &lt;p>; To train all components of the REVEAL model end-to-end, we need to &lt;a href=&quot;https://arxiv.org/abs/1910.08475&quot;>;warm start&lt;/a>; the model to a good state (setting initial values to model parameters). Otherwise, if we were to start with random weights (cold-start), the retriever would often return irrelevant memory items that would never generate useful training signals. To avoid this cold-start problem, we construct an initial retrieval dataset with pseudo–ground-truth knowledge to give the pre-training a reasonable head start. &lt;/p>; &lt;p>; We create a modified version of the &lt;a href=&quot;https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html&quot;>;WIT&lt;/a>; dataset for this purpose. Each image-caption pair in WIT also comes with a corresponding Wikipedia passage (words surrounding the text). We put together the surrounding passage with the query image and use it as the pseudo ground-truth knowledge that corresponds to the input query. The passage provides rich information about the image and caption, which is useful for initializing the model. &lt;/p>; &lt;p>; To prevent the model from relying on low-level image features for retrieval, we apply random data augmentation to the input query image. Given this modified dataset that contains pseudo-retrieval ground-truth, we train the query and memory key embeddings to warm start the model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;REVEAL workflow&lt;/h2>; &lt;p>; The overall workflow of REVEAL consists of four primary steps. First, REVEAL encodes a multimodal input into a sequence of token embeddings along with a condensed query embedding. Then, the model translates each multi-source knowledge entry into unified pairs of key and value embeddings, with the key being utilized for memory indexing and the value encompassing the entire information about the entry. Next, REVEAL retrieves the top-&lt;em>;k&lt;/em>; most related knowledge pieces from multiple knowledge sources, returns the pre-processed value embeddings stored in memory, and re-encodes the values. Finally, REVEAL fuses the top-&lt;em>;k&lt;/em>; knowledge pieces through an attentive knowledge fusion layer by injecting the retrieval score (dot product between query and key embeddings) as a prior during attention calculation. This structure is instrumental in enabling the memory, encoder, retriever and the generator to be concurrently trained in an end-to-end fashion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1003&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbkJJ8hgi1MUjDVf-rgpLu4DS60Mg8VZAPFw_XVL_9y3y5UGonUDSBZPygZklrVw6_K1iT7z3N27uaHIObxyWCVnVSj0InGjvHjEOtDUgg0CO79qStbibrvLtQIcY2VTM9qbVoOO0b5Cv70F-gQwuvfOr3CRWVPzqqj9tmn2VhIVV1JPJLCkPTISDaiQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall workflow of REVEAL.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate REVEAL on knowledge-based visual question answering tasks using &lt;a href=&quot;https://okvqa.allenai.org/&quot;>;OK-VQA&lt;/a>; and &lt;a href=&quot;https://allenai.org/project/a-okvqa/home&quot;>;A-OKVQA&lt;/a>; datasets. We fine-tune our pre-trained model on the VQA tasks using the same generative objective where the model takes in an image-question pair as input and generates the text answer as output. We demonstrate that REVEAL achieves better results on the A-OKVQA dataset than earlier attempts that incorporate a fixed knowledge or the works that utilize large language models (eg, GPT-3) as an implicit source of knowledge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgo8UvZJGoDe0yRpLv6qTqnwiOh3yLHBJO1oc2pZ8qqhsCcNDUkpyf95SVQo-eOU0ryPO7IGXCbr4zWG6rHqtKzz103bV8akDbegrgiVwO7y8u72UPQzfXL3Pdzb1QTTjw9lWNRYF2zrqlW5E6dr5NI3uSaB_0uYeljCErOKukwCfdsyjwAQf_14_qhcQ/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual question answering results on A-OKVQA. REVEAL achieves higher accuracy in comparison to previous works including &lt;a href=&quot;https://arxiv.org/abs/1908.02265&quot;>;ViLBERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1908.07490&quot;>;LXMERT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2111.09734&quot;>;ClipCap&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.11014&quot;>;KRISP&lt;/a>; and &lt;a href=&quot;https://prior.allenai.org/projects/gpv2&quot;>;GPV-2&lt;/a>;. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also evaluate REVEAL on the image captioning benchmarks using &lt;a href=&quot;https://cocodataset.org/#home&quot;>;MSCOCO&lt;/a>; and &lt;a href=&quot;https://nocaps.org/&quot;>;NoCaps&lt;/a>; dataset. We directly fine-tune REVEAL on the MSCOCO training split via the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross_entropy&quot;>;cross-entropy&lt;/a>; generative objective. We measure our performance on the MSCOCO test split and NoCaps evaluation set using the &lt;a href=&quot;https://arxiv.org/abs/1411.5726&quot;>;CIDEr&lt;/a>; metric, which is based on the idea that good captions should be similar to reference captions in terms of word choice, grammar, meaning, and content. Our results on MSCOCO caption and NoCaps datasets are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqCkPNUn_sWhogsBQQsYpSA0pTXcNwZriZmok6PNLmlhuYc-mxjsU1nfDthGf9CAncBg1TJ1KzMHndKe2zkiV35lL4NTqXkFuysHH2ulmT69snG0rWqlptujIvCJz3wowb3CRrSUubiIgSQZqy_ixloVE_zMof5kpWz1xcSordWkZU1LZWf8LHmY4BDA/w640-h396/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Image Captioning results on MSCOCO and NoCaps using the CIDEr metric. REVEAL achieves a higher score in comparison to &lt;a href=&quot;https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model&quot;>;Flamingo&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2101.00529&quot;>;VinVL&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2108.10904&quot;>;SimVLM&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/05/image-text-pre-training-with.html&quot;>;CoCa&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Below we show a couple of qualitative examples of how REVEAL retrieves relevant documents to answer visual questions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s931/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;338&quot; data-original-width=&quot;931&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgV11nORwmRcQrKjx2Md2T01gN_jus3BbtDlx5DZt4OXZSenEKlpjvtFTYBOC3-XusalvcpQwQEeHy3-UBUtrpYrsEMUyIS6l2SdXcGgJC4IS5KHbts9NC0x6M0a3XENFaRvME3my3yrj_nDO09yNBIuPzJZ4cZXAC4hY65VsFeEKWlBi_YYupmHNfz8Q/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REVEAL can use knowledge from different sources to correctly answer the question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present an end-to-end retrieval-augmented visual language (REVEAL) model, which contains a knowledge retriever that learns to utilize a diverse set of knowledge sources with different modalities. We train REVEAL on a massive image-text corpus with four diverse knowledge corpora, and achieve state-of-the-art results on knowledge-intensive visual question answering and image caption tasks. In the future we would like to explore the ability of this model for attribution, and apply it to a broader class of multimodal tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross and Alireza Fathi.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/791000644276225005/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/791000644276225005&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/06/retrieval-augmented-visual-language-pre.html&quot; rel=&quot;alternate&quot; title=&quot;Retrieval-augmented visual-language pre-training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrAmMW5q9tM1RpbnrDdMiw-kXeP1kkCNH0Vj_ZmqxTxoT45GYgBjIKXBZ2eF7_mtQ-ijE7h5g-O69tcbfmEoqifjP6ssE12hsRzIE-fl64ZdaFBL0f0Ruu-Ix3R-dy1jhBibQXoxZCpF0CVsdLWwK1aS-JJGu2wcPdyZkPMeRQFP-AqZq5nYUq0KTOAQ/s72-c/REVEAL%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8031728070441281505&lt;/id>;&lt;published>;2023-05-31T10:13:00.003-07:00&lt;/published>;&lt;updated>;2023-06-12T15:09:48.808-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Large sequence models for software development activities&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Petros Maniatis and Daniel Tarlow, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s1200/DIDACT-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Software isn&#39;t created in one dramatic step. It improves bit by bit, one little step at a time — editing, running unit tests, fixing build errors, addressing code reviews, editing some more, appeasing &lt;a href=&quot;https://en.wikipedia.org/wiki/Lint_(software)&quot;>;linters&lt;/a>;, and fixing more errors — until finally it becomes good enough to merge into a code repository. Software engineering isn&#39;t an isolated process, but a dialogue among human developers, code reviewers, bug reporters, software architects and tools, such as compilers, unit tests, linters and static analyzers. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we describe DIDACT (​​Dynamic Integrated Developer ACTivity), which is a methodology for training large machine learning (ML) models for software development. The novelty of DIDACT is that it uses &lt;em>;the process of software development &lt;/em>;as the source of training data for the model, rather than just &lt;em>;the polished end state &lt;/em>;of that process, the finished code. By exposing the model to the contexts that developers see as they work, paired with the actions they take in response, the model learns about the dynamics of software development and is more aligned with how developers spend their time. We leverage instrumentation of Google&#39;s software development to scale up the quantity and diversity of developer-activity data beyond previous works. Results are extremely promising along two dimensions: usefulness to professional software developers, and as a potential basis for imbuing ML models with general software development skills. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s1475/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;687&quot; data-original-width=&quot;1475&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJD9kDvfSCOudyA3cXrYr3eYIKzuqNfndwpoZVHsENIIFUTARGBFoqn_IE627Zk2iGuQg3NFOGcPw2pCA8fBSYv-iBzYNEw4yoOGghZrNvo1AJY1K0o9IvzMCqKjKPEKupP7NL8yQqBs3BUzoizgePEBgZN5vN9Ni7B0a2_eCs4GzFEJYDUR2xF4TIYg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DIDACT is a multi-task model trained on development activities that include editing, debugging, repair, and code review.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We built and deployed internally three DIDACT tools, &lt;a href=&quot;https://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot;>;Comment Resolution&lt;/a>; (which we recently announced), Build Repair, and Tip Prediction, each integrated at different stages of the development workflow. All three of these tools received enthusiastic feedback from thousands of internal developers. We see this as the ultimate test of usefulness: do professional developers, who are often experts on the code base and who have carefully honed workflows, leverage the tools to improve their productivity? &lt;/p>; &lt;p>; Perhaps most excitingly, we demonstrate how DIDACT is a first step towards a general-purpose developer-assistance agent. We show that the trained model can be used in a variety of surprising ways, via prompting with prefixes of developer activities, and by chaining together multiple predictions to roll out longer activity trajectories. We believe DIDACT paves a promising path towards developing agents that can generally assist across the software development process. &lt;/p>; &lt;br />; &lt;h2>;A treasure trove of data about the software engineering process&lt;/h2>; &lt;p>; Google&#39;s software engineering toolchains store every operation related to code as a log of interactions among tools and developers, and have done so for decades. In principle, one could use this record to replay in detail the key episodes in the “software engineering video” of how Google&#39;s codebase came to be, step-by-step — one code edit, compilation, comment, variable rename, etc., at a time. &lt;/p>; &lt;p>; Google code lives in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monorepo&quot;>;monorepo&lt;/a>;, a single repository of code for all tools and systems. A software developer typically experiments with code changes in a local &lt;a href=&quot;https://en.wikipedia.org/wiki/Copy-on-write&quot;>;copy-on-write&lt;/a>; workspace managed by a system called &lt;a href=&quot;https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext&quot;>;Clients in the Cloud&lt;/a>; (CitC). When the developer is ready to package a set of code changes together for a specific purpose (eg, fixing a bug), they create a changelist (CL) in &lt;a href=&quot;https://abseil.io/resources/swe-book/html/ch19.html&quot;>;Critique&lt;/a>;, Google&#39;s code-review system. As with other types of code-review systems, the developer engages in a dialog with a peer reviewer about functionality and style. The developer edits their CL to address reviewer comments as the dialog progresses. Eventually, the reviewer declares “LGTM!” (“looks good to me”), and the CL is merged into the code repository. &lt;/p>; &lt;p>; Of course, in addition to a dialog with the code reviewer, the developer also maintains a “dialog” of sorts with a plethora of other software engineering tools, such as the compiler, the testing framework, linters, static analyzers, fuzzers, etc. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/s1877/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1751&quot; data-original-width=&quot;1877&quot; height=&quot;597&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhdDAeePURbX0syZUQcLifMLS9uMw-ufpwg8jUXQoQMyHa0UNstR_vA7mqan_fjqzlXLuSlwVZItU-dqZne3Bk4Adsb2DLTi__wX0vmfk_JnEjRC_tmE72e7woRNCsZO6znn3OCQsZLZvZrlU55byBsm3oi5ubHBvDizqvz3N83je01r7XtZ6cuvEZZA/w640-h597/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the intricate web of activities involved in developing software: small actions by the developer, interactions with a code reviewer, and invocations of tools such as compilers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A multi-task model for software engineering&lt;/h2>; &lt;p>; DIDACT utilizes interactions among engineers and tools to power ML models that assist Google developers, by suggesting or enhancing actions developers take — in context — while pursuing their software-engineering tasks. To do that, we have defined a number of tasks about individual developer activities: repairing a broken build, predicting a code-review comment, addressing a code-review comment, renaming a variable, editing a file, etc. We use a common formalism for each activity: it takes some &lt;em>;State&lt;/em>; (a code file), some &lt;em>;Intent&lt;/em>; (annotations specific to the activity, such as code-review comments or compiler errors), and produces an &lt;em>;Action&lt;/em>; (the operation taken to address the task). This Action is like a mini programming language, and can be extended for newly added activities. It covers things like editing, adding comments, renaming variables, marking up code with errors, etc. We call this language &lt;em>;DevScript&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s1200/DIDACT.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;762&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsd4dFYSNVxcg20eqgJqdr3u9S2hHsBlqOelnix5XZd5zIhrZIemDLaF3CcaMk09Y_9kyDkhAuAq2-STjKDOP1Tb9ShsE91FpNgnLRqOxIzCKrTj2_WaAJUlRxikaLmQK2iv7YfpnQtQeaUeIXNCRE9efYju6KxGBEu4zwDA0vQ6qla6dNvpResnch2w/s16000/DIDACT.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The DIDACT model is prompted with a task, code snippets, and annotations related to that task, and produces development actions, eg, edits or comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This state-intent-action formalism enables us to capture many different tasks in a general way. What&#39;s more, DevScript is a concise way to express complex actions, without the need to output the whole state (the original code) as it would be after the action takes place; this makes the model more efficient and more interpretable. For example, a rename might touch a file in dozens of places, but a model can predict a single rename action. &lt;/p>; &lt;br />; &lt;h2>;An ML peer programmer&lt;/h2>; &lt;p>; DIDACT does a good job on individual assistive tasks. For example, below we show DIDACT doing code clean-up after functionality is mostly done. It looks at the code along with some final comments by the code reviewer (marked with “human” in the animation), and predicts edits to address those comments (rendered as a &lt;em>;diff&lt;/em>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/s897/DIDACT2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;761&quot; data-original-width=&quot;897&quot; height=&quot;543&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNMPCHlZwRpo4EKM-HZt7qVII342P6kFgSK18H2sYqftvqmrEC-SZp9FsmgvpZtafgCid3WF905_ooJrA8vEFNs7M4B9Ox5pObqzvCltaAUXouIkiVpLWKRs-VEY0Dhpg11RbH7iJfhdinkNegpK0rOhTPR7rdzleQJzpIErVIZBkPi3WjisexB0Uklw/w640-h543/DIDACT2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an initial snippet of code and the comments that a code reviewer attached to that snippet, the Pre-Submit Cleanup task of DIDACT produces edits (insertions and deletions of text) that address those comments.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The multimodal nature of DIDACT also gives rise to some surprising capabilities, reminiscent of &lt;a href=&quot;https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html&quot;>;behaviors emerging with scale&lt;/a>;. One such capability is &lt;em>;history augmentation&lt;/em>;, which can be enabled via prompting. Knowing what the developer did recently enables the model to make a better guess about what the developer should do next. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgwd-Dvh_esu_26f2rO6DKwMpGyDF0wKHzasGuD3RhXNJiJR5cM613KLSuabNp5hvKV9dF2vzHd7XkUOKD4d7l1cOyV2ovYwxWLECmudUKDk5bmVNOg5eImsqUJbkUkPWk7yOiCogafub7eHb9B3cytjGInIICcMoWFrTlsgqfN3B9IvHFPbWbwfIkZQ/s1044/CitCMonster.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;633&quot; data-original-width=&quot;1044&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgwd-Dvh_esu_26f2rO6DKwMpGyDF0wKHzasGuD3RhXNJiJR5cM613KLSuabNp5hvKV9dF2vzHd7XkUOKD4d7l1cOyV2ovYwxWLECmudUKDk5bmVNOg5eImsqUJbkUkPWk7yOiCogafub7eHb9B3cytjGInIICcMoWFrTlsgqfN3B9IvHFPbWbwfIkZQ/s16000/CitCMonster.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of history-augmented code completion in action.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A powerful such task exemplifying this capability is &lt;em>;history-augmented code completion&lt;/em>;. In the figure below, the developer adds a new function parameter (1), and moves the cursor into the documentation (2). Conditioned on the history of developer edits and the cursor position, the model completes the line (3) by correctly predicting the docstring entry for the new parameter. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s1048/DIDACT4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;602&quot; data-original-width=&quot;1048&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_ao8R6Rv9t0sz_O27BPC0Wy3dpbllP101ovPL1yklPN4SqJu8B94EZyhKumBZLsbmuiLdNVlA92wmOgnBQ-nglk2FcNRc4B2J2hFsR3x0Zy2XWcDGylowaxI2hDJH2cAyIzAEcZqCeg8PsLWCNkKzhr-jnMPr4_aGjjZguCogRalV2582jqshece5bA/s16000/DIDACT4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of edit prediction, over multiple chained iterations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In an even more powerful history-augmented task, &lt;em>;edit prediction&lt;/em>;, the model can choose where to edit next in a fashion that is historically consistent&lt;strong>;. &lt;/strong>;If the developer deletes a function parameter (1), the model can use history to correctly predict an update to the docstring (2) that removes the deleted parameter (without the human developer manually placing the cursor there) and to update a statement in the function (3) in a syntactically (and — arguably — semantically) correct way. With history, the model can unambiguously decide how to continue the “editing video” correctly. Without history, the model wouldn&#39;t know whether the missing function parameter is intentional (because the developer is in the process of a longer edit to remove it) or accidental (in which case the model should re-add it to fix the problem). &lt;/p>; &lt;p>; The model can go even further. For example, we started with a blank file and asked the model to successively predict what edits would come next until it had written a full code file. The astonishing part is that the model developed code in a step-by-step way that would seem natural&lt;strong>; &lt;/strong>;to a developer: It started by first creating a fully working skeleton with imports, flags, and a basic main function. It then incrementally added new functionality, like reading from a file and writing results, and added functionality to filter out some lines based on a user-provided regular expression, which required changes across the file, like adding new flags. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DIDACT turns Google&#39;s software development process into training demonstrations for ML developer assistants, and uses those demonstrations to train models that construct code in a step-by-step fashion, interactively with tools and code reviewers. These innovations are already powering tools enjoyed by Google developers every day. The DIDACT approach complements the great strides taken by large language models at Google and elsewhere, towards technologies that ease toil, improve productivity, and enhance the quality of work of software engineers. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a multi-year collaboration among Google Research, Google Core Systems and Experiences, and DeepMind. We would like to acknowledge our colleagues Jacob Austin, Pascal Lamblin, Pierre-Antoine Manzagol, and Daniel Zheng, who join us as the key drivers of this project. This work could not have happened without the significant and sustained contributions of our partners at Alphabet (Peter Choy, Henryk Michalewski, Subhodeep Moitra, Malgorzata Salawa, Vaibhav Tulsyan, and Manushree Vijayvergiya), as well as the many people who collected data, identified tasks, built products, strategized, evangelized, and helped us execute on the many facets of this agenda (Ankur Agarwal, Paige Bailey, Marc Brockschmidt, Rodrigo Damazio Bovendorp, Satish Chandra, Savinee Dancs,&amp;nbsp;&lt;/em>;&lt;i>;Denis Davydenko,&amp;nbsp;&lt;/i>;&lt;em>;Matt Frazier, Alexander Frömmgen, Nimesh Ghelani, Chris Gorgolewski, Chenjie Gu, Vincent Hellendoorn, Franjo Ivančić, Marko Ivanković, Emily Johnston, Luka Kalinovcic, Lera Kharatyan, Jessica Ko, Markus Kusano, Kathy Nix, Christian Perez, Sara Qu, Marc Rasi, Marcus Revaj, Ballie Sandhu, Michael Sloan, Tom Small, Gabriela Surita, Maxim Tabachnyk, David Tattersall, Sara Toth, Kevin Villela, Sara Wiltberger, and Donald Duo Zhao) and our extremely supportive leadership (Martín Abadi, Joelle Barral, Jeff Dean, Madhura Dudhgaonkar, Douglas Eck, Zoubin Ghahramani, Hugo Larochelle, Chandu Thekkath, and Niranjan Tulpule). Thank you!&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8031728070441281505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031728070441281505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot; rel=&quot;alternate&quot; title=&quot;Large sequence models for software development activities&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8_SbpjaNoBFB_ymmlPa_9YTKL4tOAPfWFHajgnphxoPc3dTXyORaOPk1kN1TP8gVgdjiyPtqGfEYmVCyYIIBdd2ICEIROmNVMCbJmcmT0wLoRWow8djNms5ejc-pfk2Bx_79P7FCnN5PGAQqnXq8YWkn5sX_oLwP0NSoq3IvaV9WyiYhYk1wYK8cTEg/s72-c/DIDACT-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4195590947404721374&lt;/id>;&lt;published>;2023-05-26T12:08:00.002-07:00&lt;/published>;&lt;updated>;2023-05-26T12:09:00.761-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Foundation models for reasoning on charts&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Julian Eisenschlos, Research Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s2200/MatCha.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Visual language is the form of communication that relies on pictorial symbols outside of text to convey information. It is ubiquitous in our digital life in the form of iconography, infographics, tables, plots, and charts, extending to the real world in street signs, comic books, food labels, etc. For that reason, having computers better understand this type of media can help with scientific communication and discovery, accessibility, and data transparency. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While computer vision models have made tremendous progress using learning-based solutions since the advent of &lt;a href=&quot;https://ieeexplore.ieee.org/document/5206848&quot;>;ImageNet&lt;/a>;, the focus has been on natural images, where all sorts of tasks, such as &lt;a href=&quot;https://www.image-net.org/&quot;>;classification&lt;/a>;, &lt;a href=&quot;https://visualqa.org/&quot;>;visual question answering&lt;/a>; (VQA), &lt;a href=&quot;https://cocodataset.org&quot;>;captioning, detection and segmentation&lt;/a>;, have been defined, studied and in some cases advanced to reach human performance. However, visual language has not garnered a similar level of attention, possibly because of the lack of large-scale training sets in this space. But over the last few years, new academic datasets have been created with the goal of evaluating question answering systems on visual language images, like &lt;a href=&quot;https://arxiv.org/abs/1909.00997&quot;>;PlotQA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicsVQA&lt;/a>;, and &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6aQ6tqQsTgI2X3TkQUK_xIK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;898&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6aQ6tqQsTgI2X3TkQUK_xIK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example from &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA&lt;/a>;. Answering the question requires reading the information and computing the sum and the difference.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Existing models built for these tasks relied on integrating &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_character_recognition&quot;>;optical character recognition&lt;/a>; (OCR) information and their coordinates into larger pipelines but the process is error prone, slow, and generalizes poorly. The prevalence of these methods was because existing end-to-end computer vision models based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) or &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; pre-trained on natural images could not be easily adapted to visual language. But existing models are ill-prepared for the challenges in answering questions on charts, including reading the relative height of bars or the angle of slices in pie charts, understanding axis scales, correctly mapping pictograms with their legend values with colors, sizes and textures, and finally performing numerical operations with the extracted numbers. &lt;/p>; &lt;p>; In light of these challenges, we propose “&lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering&lt;/a>;”. MatCha, which stands for math and charts, is a pixels-to-text foundation model (a pre-trained model with built-in inductive biases that can be fine-tuned for multiple applications) trained on two complementary tasks: (a) chart de-rendering and (b) math reasoning. In chart de-rendering, given a plot or chart, the image-to-text model is required to generate its underlying data table or the code used to render it. For math reasoning pre-training, we pick textual numerical reasoning datasets and render the input into images, which the image-to-text model needs to decode for answers. We also propose “&lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot: One-shot visual language reasoning by plot-to-table translation&lt;/a>;”, a model built on top of MatCha for one-shot reasoning on charts via translation to tables. With these methods we surpass the previous state of the art in ChartQA by more than 20% and match the best summarization systems that have 1000 times more parameters. Both papers will be presented at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Chart de-rendering&lt;/h2>; &lt;p>; Plots and charts are usually generated by an underlying data table and a piece of code. The code defines the overall layout of the figure (eg, type, direction, color/shape scheme) and the underlying data table establishes the actual numbers and their groupings. Both the data and code are sent to a compiler/rendering engine to create the final image. To understand a chart, one needs to discover the visual patterns in the image and effectively parse and group them to extract the key information. Reversing the plot rendering process demands all such capabilities and can thus serve as an ideal pre-training task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s624/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;335&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A chart created from a table in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Airbus_A380&quot;>;Airbus A380 Wikipedia page&lt;/a>; using random plotting options. The pre-training task for MatCha consists of recovering the source table or the source code from the image.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, it is challenging to simultaneously obtain charts, their underlying data tables, and their rendering code. To collect sufficient pre-training data, we independently accumulate [chart, code] and [chart, table] pairs. For [chart, code], we crawl all GitHub IPython notebooks with appropriate licenses and extract blocks with figures. A figure and the code block right before it are saved as a [chart, code] pair. For [chart, table] pairs, we explored two sources. For the first source, synthetic data, we manually write code to convert web-crawled Wikipedia tables from the &lt;a href=&quot;https://aclanthology.org/2020.acl-main.398/&quot;>;TaPas&lt;/a>; codebase to charts. We sampled from and combined several plotting options depending on the column types. In addition, we also add [chart, table] pairs generated in PlotQA to diversify the pre-training corpus. The second source is web-crawled [chart, table] pairs. We directly use the [chart, table] pairs crawled in the ChartQA training set, containing around 20k pairs in total from four websites: &lt;a href=&quot;statista.com&quot;>;Statista&lt;/a>;, &lt;a href=&quot;https://www.pewresearch.org/&quot;>;Pew&lt;/a>;, &lt;a href=&quot;https://ourworldindata.org/&quot;>;Our World in Data&lt;/a>;, and &lt;a href=&quot;https://www.oecd.org/&quot;>;OECD&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Math reasoning&lt;/h2>; &lt;p>; We incorporate numerical reasoning knowledge into MatCha by learning math reasoning skills from textual math datasets. We use two existing textual math reasoning datasets, &lt;a href=&quot;https://openreview.net/forum?id=H1gR5iR5FX&quot;>;MATH&lt;/a>; and &lt;a href=&quot;https://aclanthology.org/N19-1246/&quot;>;DROP&lt;/a>; for pre-training. MATH is synthetically created, containing two million training examples per module (type) of questions. DROP is a reading-comprehension–style QA dataset where the input is a paragraph context and a question. &lt;/p>; &lt;p>; To solve questions in DROP, the model needs to read the paragraph, extract relevant numbers and perform numerical computation. We found both datasets to be complementary. MATH contains a large number of questions across different categories, which helps us identify math operations needed to explicitly inject into the model. DROP&#39;s reading-comprehension format resembles the typical QA format wherein models simultaneously perform information extraction and reasoning. In practice, we render inputs of both datasets into images. The model is trained to decode the answer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s624/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;218&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;To improve the math reasoning skills of MatCha we incorporate examples from MATH and DROP into the pre-training objective, by rendering the input text as images.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;End-to-end results&lt;/h2>; &lt;p>; We use a &lt;a href=&quot;https://arxiv.org/abs/2210.03347&quot;>;Pix2Struct&lt;/a>; model backbone, which is an image-to-text transformer tailored for website understanding, and pre-train it with the two tasks described above. We demonstrate the strengths of MatCha by fine-tuning it on several visual language tasks — tasks involving charts and plots for question answering and summarization where no access to the underlying table is possible. MatCha surpasses previous models&#39; performance by a large margin and also outperforms the previous state of the art, which assumes access to underlying tables. &lt;/p>; &lt;p>; In the figure below, we first evaluate two baseline models that incorporate information from an &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;OCR pipeline&lt;/a>;, which until recently was the standard approach for working with charts. The first is based on &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;, the second on &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;VisionTaPas&lt;/a>;. We also compare against &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI-17B&lt;/a>;, which is a large (~1000 times larger than the other models) image plus text-to-text transformer trained on a diverse set of tasks but with limited capabilities for reading text and other forms of visual language. Finally, we report the Pix2Struct and MatCha model results. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s1646/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;696&quot; data-original-width=&quot;1646&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Experimental results on two chart QA benchmarks ChartQA &amp;amp; PlotQA (using relaxed accuracy) and a chart summarization benchmark chart-to-text (using BLEU4). Matcha surpasses the state of the art by a large margin on QA, compared to larger models, and matches these larger models on summarization.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For QA datasets, we use the &lt;a href=&quot;https://github.com/google-research/pix2struct/blob/main/pix2struct/metrics.py#L81&quot;>;official relaxed accuracy metric&lt;/a>; that allows for small relative errors in numerical outputs. For chart-to-text summarization, we report &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; scores. MatCha achieves noticeably improved results compared to baselines for question answering, and comparable results to PaLI in summarization, where large size and extensive long text/captioning generation pre-training are advantageous for this kind of long-form text generation. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Derendering plus large language model chains&lt;/h2>; &lt;p>; While extremely performant for their number of parameters, particularly on extractive tasks, we observed that fine-tuned MatCha models could still struggle with end-to-end complex reasoning (eg, mathematical operations involving large numbers or multiple steps). Thus, we also propose a two-step method to tackle this: 1) a model reads a chart, then outputs the underlying table, 2) a large language model (LLM) reads this output and then tries to answer the question solely based on the textual input. &lt;/p>; &lt;p>; For the first model, we fine-tuned MatCha solely on the chart-to-table task, increasing the output sequence length to guarantee it could recover all or most of the information in the chart. &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; is the resulting model. In the second stage, any LLM (such as &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;) can be used for the task, and we can rely on the standard methods to increase performance on LLMs, for example &lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;>;self-consistency&lt;/a>;. We also experimented with &lt;a href=&quot;https://arxiv.org/abs/2211.12588&quot;>;program-of-thoughts&lt;/a>; where the model produces executable Python code to offload complex computations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s622/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;622&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of the DePlot+LLM method. This is a real example using &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. The blue boxes are input to the LLM and the red boxes contain the answer generated by the LLMs. We highlight some of the key reasoning steps in each answer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; As shown in the example above, the DePlot model in combination with LLMs outperforms fine-tuned models by a significant margin, especially so in the human-sourced portion of ChartQA, where the questions are more natural but demand more difficult reasoning. Furthermore, DePlot+LLM can do so without access to any training data. &lt;/p>; &lt;p>; We have released the new models and code at our &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/deplot&quot;>;GitHub repo&lt;/a>;, where you can try it out yourself in colab. Checkout the papers for &lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; for more details on the experimental results. We hope that our results can benefit the research community and make the information in charts and plots more accessible to everyone. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was carried out by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen and Yasemin Altun from our &lt;a href=&quot;https://research.google/teams/language/&quot;>;Language Team&lt;/a>; as part of Fangyu&#39;s internship project. Nigel Collier from Cambridge also was a collaborator. We would like to thank Joshua Howland, Alex Polozov, Shrestha Basu Mallick, Massimo Nicosia and William Cohen for their valuable comments and suggestions.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4195590947404721374/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on.html&quot; rel=&quot;alternate&quot; title=&quot;Foundation models for reasoning on charts&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s72-c/MatCha.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3147300214577163215&lt;/id>;&lt;published>;2023-05-26T09:58:00.001-07:00&lt;/published>;&lt;updated>;2023-05-26T16:35:56.897-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Hardware&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Barkour: Benchmarking animal-level agility with quadruped robots&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ken Caluwaerts and Atil Iscen, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s320/barkour%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Creating robots that exhibit robust and dynamic locomotion capabilities, similar to animals or humans, has been a long-standing goal in the robotics community. In addition to completing tasks quickly and efficiently, agility allows legged robots to move through &lt;a href=&quot;https://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot;>;complex environments&lt;/a>; that are otherwise difficult to traverse. Researchers at Google have been pursuing agility for &lt;a href=&quot;https://arxiv.org/abs/1804.10332&quot;>;multiple years&lt;/a>; and across &lt;a href=&quot;https://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html&quot;>;various form factors&lt;/a>;. Yet, while researchers have enabled &lt;a href=&quot;https://www.science.org/doi/10.1126/scirobotics.abc5986&quot;>;robots to hike&lt;/a>; or &lt;a href=&quot;https://www.roboticsproceedings.org/rss11/p47.pdf&quot;>;jump over some obstacles&lt;/a>;, there is still no generally accepted benchmark that comprehensively measures robot agility or mobility. In contrast, benchmarks are driving forces behind the development of machine learning, such as &lt;a href=&quot;https://arxiv.org/abs/1409.0575&quot;>;ImageNet&lt;/a>; for computer vision, and &lt;a href=&quot;https://github.com/openai/gym&quot;>;OpenAI Gym&lt;/a>; for reinforcement learning (RL). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.14654&quot;>;Barkour: Benchmarking Animal-level Agility with Quadruped Robots&lt;/a>;”, we introduce the &lt;em>;Barkour &lt;/em>;agility benchmark for quadruped robots, along with a &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>;-based generalist locomotion policy. Inspired by dog agility competitions, a legged robot must sequentially display a variety of skills, including moving in different directions, traversing uneven terrains, and jumping over obstacles within a limited timeframe to successfully complete the benchmark. By providing a diverse and challenging obstacle course, the Barkour benchmark encourages researchers to develop locomotion controllers that move fast in a controllable and versatile way. Furthermore, by tying the performance metric to real dog performance, we provide an intuitive metric to understand the robot performance with respect to their animal counterparts. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/zola_vs_ap.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We invited a handful of &lt;a href=&quot;https://blog.google/inside-google/life-at-google/working-home-ruff-dooglers-make-it-little-better/&quot;>;dooglers&lt;/a>; to try the obstacle course to ensure that our agility objectives were realistic and challenging. Small dogs complete the obstacle course in approximately 10s, whereas our robot&#39;s typical performance hovers around 20s.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Barkour benchmark&lt;/h2>; &lt;p>; The Barkour scoring system uses a per obstacle and an overall course target time based on the target speed of small dogs in the &lt;a href=&quot;https://images.akc.org/pdf/rulebooks/REAGIL.pdf&quot;>;novice agility competitions&lt;/a>; (about 1.7m/s). Barkour scores range from 0 to 1, with 1 corresponding to the robot successfully traversing all the obstacles along the course within the allotted time of approximately 10 seconds, the average time needed for a similar-sized dog to traverse the course. The robot receives penalties for skipping, failing obstacles, or moving too slowly. &lt;/p>; &lt;p>; Our standard course consists of four unique obstacles in a 5m x 5m area. This is a denser and smaller setup than a typical dog competition to allow for easy deployment in a robotics lab. Beginning at the start table, the robot needs to weave through a set of poles, climb an A-frame, clear a 0.5m broad jump and then step onto the end table. We chose this subset of obstacles because they test a diverse set of skills while keeping the setup within a small footprint. As is the case for real dog agility competitions, the Barkour benchmark can be easily adapted to a larger course area and may incorporate a variable number of obstacles and course configurations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s1193/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;1193&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the Barkour benchmark&#39;s obstacle course setup, which consists of weave poles, an A-frame, a broad jump, and pause tables. The intuitive scoring mechanism, inspired by dog agility competitions, balances speed, agility and performance and can be easily modified to incorporate other types of obstacles or course configurations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning agile locomotion skills&lt;/h2>; &lt;p>; The Barkour benchmark features a diverse set of obstacles and a delayed reward system, which pose a significant challenge when training a single policy that can complete the entire obstacle course. So in order to set a strong performance baseline and demonstrate the effectiveness of the benchmark for robotic agility research, we adopt a student-teacher framework combined with a zero-shot sim-to-real approach. First, we train individual specialist locomotion skills (teacher) for different obstacles using on-policy RL methods. In particular, we leverage &lt;a href=&quot;https://github.com/google/brax&quot;>;recent advances&lt;/a>; in large-scale &lt;a href=&quot;https://arxiv.org/pdf/2108.10470.pdf&quot;>;parallel simulation&lt;/a>; to equip the robot with individual skills, including walking, slope climbing, and jumping policies. &lt;/p>; &lt;p>; Next, we train a single policy (student) that performs all the skills and transitions in between by using a student-teacher framework, based on the specialist skills we previously trained. We use simulation rollouts to create datasets of state-action pairs for each one of the specialist skills. This dataset is then distilled into a single Transformer-based generalist locomotion policy, which can handle various terrains and adjust the robot&#39;s gait based on the perceived environment and the robot&#39;s state. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s1547/Barkour%20training.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;296&quot; data-original-width=&quot;1547&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s16000/Barkour%20training.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; During deployment, we pair the locomotion transformer policy that is capable of performing multiple skills with a navigation controller that provides velocity commands based on the robot&#39;s position. Our trained policy controls the robot based on the robot&#39;s surroundings represented as an elevation map, velocity commands, and on-board sensory information provided by the robot.&lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/Generalist.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Deployment pipeline for the locomotion transformer architecture. At deployment time, a high-level navigation controller guides the real robot through the obstacle course by sending commands to the locomotion transformer policy.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Robustness and repeatability are difficult to achieve when we aim for peak performance and maximum speed. Sometimes, the robot might fail when overcoming an obstacle in an agile way. To handle failures we train a &lt;a href=&quot;https://arxiv.org/pdf/2110.05457.pdf&quot;>;recovery policy&lt;/a>; that quickly gets the robot back on its feet, allowing it to continue the episode. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We evaluate the Transformer-based generalist locomotion policy using custom-built quadruped robots and show that by optimizing for the proposed benchmark, we obtain agile, robust, and versatile skills for our robot in the real world. We further provide analysis for various design choices in our system and their impact on the system performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/s1895/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1275&quot; data-original-width=&quot;1895&quot; height=&quot;269&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/w400-h269/image4.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model of the custom-built robots used for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We deploy both the specialist and generalist policies to hardware (zero-shot sim-to-real). The robot&#39;s target trajectory is provided by a set of waypoints along the various obstacles. In the case of the specialist policies, we switch between specialist policies by using a hand-tuned policy switching mechanism that selects the most suitable policy given the robot&#39;s position. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/3obs_side.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Typical performance of our agile locomotion policies on the Barkour benchmark. Our custom-built quadruped robot robustly navigates the terrain&#39;s obstacles by leveraging various skills learned using RL in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that very often our policies can handle unexpected events or even hardware degradation resulting in good average performance, but failures are still possible. As illustrated in the image below, in case of failures, our recovery policy quickly gets the robot back on its feet, allowing it to continue the episode. By combining the recovery policy with a simple walk-back-to-start policy, we are able to run repeated experiments with minimal human intervention to measure the robustness. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/recovery_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative example of robustness and recovery behaviors. The robot trips and rolls over after heading down the A-frame. This triggers the recovery policy, which enables the robot to get back up and continue the course.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that across a large number of evaluations, the single generalist locomotion transformer policy and the specialist policies with the policy switching mechanism achieve similar performance. The locomotion transformer policy has a slightly lower average Barkour score, but exhibits smoother transitions between behaviors and gaits. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;75%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/8x8_cont_bright_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Measuring robustness of the different policies across a large number of runs on the Barkour benchmark.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/s1313/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1313&quot; height=&quot;300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/w640-h300/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Histogram of the agility scores for the locomotion transformer policy. The highest scores shown in blue (0.75 - 0.9) represent the runs where the robot successfully completes all obstacles. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that developing a benchmark for legged robotics is an important first step in quantifying progress toward animal-level agility. To establish a strong baseline, we investigated a zero-shot sim-to-real approach, taking advantage of large-scale parallel simulation and recent advancements in training Transformer-based architectures. Our findings demonstrate that Barkour is a challenging benchmark that can be easily customized, and that our learning-based method for solving the benchmark provides a quadruped robot with a single low-level policy that can perform a variety of agile low-level skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We would like to thank our co-authors at Google DeepMind and our collaborators at Google Research: Wenhao Yu, J. Chase Kew, Tingnan Zhang, Daniel Freeman, Kuang-Hei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang, Nathan Batchelor, Steven Bohez, Federico Casarini, Jose Enrique Chen, Omar Cortes, Erwin Coumans, Adil Dostmohamed, Gabriel Dulac-Arnold, Alejandro Escontrela, Erik Frey, Roland Hafner, Deepali Jain, Yuheng Kuang, Edward Lee, Linda Luu, Ofir Nachum, Ken Oslund, Jason Powell, Diego Reyes, Francesco Romano, Feresteh Sadeghi, Ron Sloat, Baruch Tabanpour, Daniel Zheng, Michael Neunert, Raia Hadsell, Nicolas Heess, Francesco Nori, Jeff Seto, Carolina Parada, Vikas Sindhwani, Vincent Vanhoucke, and Jie Tan. We would also like to thank Marissa Giustina, Ben Jyenis, Gus Kouretas, Nubby Lee, James Lubin, Sherry Moore, Thinh Nguyen, Krista Reymann, Satoshi Kataoka, Trish Blazina, and the members of the robotics team at Google DeepMind for their contributions to the project.Thanks to John Guilyard for creating the animations in this post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3147300214577163215/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html&quot; rel=&quot;alternate&quot; title=&quot;Barkour: Benchmarking animal-level agility with quadruped robots&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s72-c/barkour%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4176009881387884637&lt;/id>;&lt;published>;2023-05-25T16:09:00.005-07:00&lt;/published>;&lt;updated>;2023-06-08T14:27:41.308-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Unsupervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private clustering for large-scale datasets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Vincent Cohen-Addad and Alessandro Epasto, Research Scientists, Google Research, Graph Mining team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s900/COVID.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;Clustering&lt;/a>; is a central problem in &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;>;unsupervised&lt;/a>; machine learning (ML) with many applications across domains in both industry and academic research more broadly. At its core, clustering consists of the following problem: given a set of data elements, the goal is to partition the data elements into groups such that similar objects are in the same group, while dissimilar objects are in different groups. This problem has been studied in math, computer science, operations research and statistics for more than 60 years in its myriad variants. Two common forms of &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;clustering&lt;/a>; are metric clustering, in which the elements are points in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_space&quot;>;metric space&lt;/a>;, like in the &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; problem, and graph clustering, where the elements are nodes of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;graph&lt;/a>; whose edges represent similarity among them. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s596/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width=&quot;592&quot; height=&quot;320&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s320/image1.png&quot; width=&quot;318&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; clustering problem, we are given a set of points in a metric space with the objective to identify &lt;em>;k&lt;/em>; representative points, called centers (here depicted as triangles), so as to minimize the sum of the squared distances from each point to its closest center. &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Kmeans_toomany.PNG&quot;>;Source&lt;/a>;, rights: CC-BY-SA-4.0&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Despite the extensive literature on algorithm design for clustering, few practical works have focused on rigorously protecting the user&#39;s privacy during clustering. When clustering is applied to personal data (eg, the queries a user has made), it is necessary to consider the privacy implications of using a clustering solution in a real system and how much information the output solution reveals about the input data. &lt;/p>; &lt;p>; To ensure privacy in a rigorous sense, one solution is to develop &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differentially private&lt;/a>; (DP) clustering algorithms. These algorithms ensure that the output of the clustering does not reveal private information about a specific data element (eg, whether a user has made a given query) or sensitive data about the input graph (eg, a relationship in a social network). Given the importance of privacy protections in unsupervised machine learning, in recent years Google has invested in research on &lt;a href=&quot;https://arxiv.org/abs/2008.08007&quot;>;theory&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2021/10/practical-differentially-private.html?hl=el&amp;amp;m=1&quot;>;practice&lt;/a>; of differentially private &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;metric&lt;/a>; or &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/da645920dcd3bd35b0dae329894bad80-Abstract-Conference.html&quot;>;graph&lt;/a>; clustering, and differential privacy in a variety of contexts, eg,&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot;>;heatmaps&lt;/a>;&amp;nbsp;or &lt;a href=&quot;https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&quot;>;tools&lt;/a>; to design DP algorithms. &lt;/p>; &lt;p>; Today we are excited to announce two important updates: 1) a &lt;a href=&quot;https://arxiv.org/abs/2302.00037&quot;>;new differentially-private algorithm&lt;/a>; for hierarchical graph clustering, which we&#39;ll be presenting at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>;, and 2) the &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/hst_clustering&quot;>;open-source release&lt;/a>; of the code of a scalable differentially-private &lt;em>;k&lt;/em>;-means algorithm. This code brings differentially private &lt;em>;k&lt;/em>;-means clustering to large scale datasets using distributed computing. Here, we will also discuss our work on clustering technology for a recent launch in the health domain for informing public health authorities. &lt;/p>; &lt;br />; &lt;h2>;Differentially private hierarchical clustering&lt;/h2>; &lt;p>; Hierarchical clustering is a popular clustering approach that consists of recursively partitioning a dataset into clusters at an increasingly finer granularity. A well known example of hierarchical clustering is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Phylogenetic_tree&quot;>;phylogenetic tree&lt;/a>; in biology in which all life on Earth is partitioned into finer and finer groups (eg, kingdom, phylum, class, order, etc.). A hierarchical clustering algorithm receives as input a graph representing the similarity of entities and learns such recursive partitions in an unsupervised way. Yet at the time of our research no algorithm was known to compute hierarchical clustering of a graph with edge privacy, ie, preserving the privacy of the vertex interactions. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://doi.org/10.48550/arXiv.2302.00037&quot;>;Differentially-Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>;”, we consider how well the problem can be approximated in a DP context and establish firm upper and lower bounds on the privacy guarantee. We design an approximation algorithm (the first of its kind) with a polynomial running time that achieves both an additive error that scales with the number of nodes &lt;em>;n&lt;/em>; (of order &lt;em>;n&lt;sup>;2.5&lt;/sup>;&lt;/em>;) and a multiplicative approximation of O(log&lt;sup>;½&lt;/sup>; n), with the multiplicative error identical to the non-private setting. We further provide a new lower bound on the additive error (of order &lt;em>;n&lt;sup>;2&lt;/sup>;&lt;/em>;) for any private algorithm (irrespective of its running time) and provide an exponential-time algorithm that matches this lower bound. Moreover, our paper includes a beyond-worst-case analysis focusing on the &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html&quot;>;hierarchical stochastic block model&lt;/a>;, a standard random graph model that exhibits a natural hierarchical clustering structure, and introduces a private algorithm that returns a solution with an additive cost over the optimum that is negligible for larger and larger graphs, again matching the non-private state-of-the-art approaches. We believe this work expands the understanding of privacy preserving algorithms on graph data and will enable new applications in such settings. &lt;/p>; &lt;br />; &lt;h2>;Large-scale differentially private clustering&lt;/h2>; &lt;p>; We now switch gears and discuss our work for metric space clustering. Most prior work in DP metric clustering has focused on improving the approximation guarantees of the algorithms on the &lt;em>;k&lt;/em>;-means objective, leaving scalability questions out of the picture. Indeed, it is not clear how efficient non-private algorithms such as &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;>;k-means++&lt;/a>;&lt;/em>; or &lt;em>;&lt;a href=&quot;https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf&quot;>;k-means//&lt;/a>;&lt;/em>; can be made differentially private without sacrificing drastically either on the approximation guarantees or the scalability. On the other hand, both scalability and privacy are of primary importance at Google. For this reason, we recently published &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;multiple&lt;/a>; &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;papers&lt;/a>; that address the problem of designing efficient differentially private algorithms for clustering that can scale to massive datasets. Our goal is, moreover, to offer scalability to large scale input datasets, even when the target number of centers, &lt;em>;k&lt;/em>;, is large. &lt;/p>; &lt;p>; We work in the &lt;a href=&quot;https://www.cs.umd.edu/~gasarch/MPC/mpc.pdf&quot;>;massively parallel computation&lt;/a>; (MPC) model, which is a computation model representative of modern distributed computation architectures. The model consists of several machines, each holding only part of the input data, that work together with the goal of solving a global problem while minimizing the amount of communication between machines. We present a &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;differentially private constant factor approximation algorithm&lt;/a>; for &lt;em>;k&lt;/em>;-means that only requires a constant number of rounds of synchronization. Our algorithm builds upon our &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;previous work&lt;/a>; on the problem (with code &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/hst_clustering&quot;>;available here&lt;/a>;), which was the first differentially-private clustering algorithm with provable approximation guarantees that can work in the MPC model. &lt;/p>; &lt;p>; The DP constant factor approximation algorithm drastically improves on the previous work using a two phase approach. In an initial phase it computes a crude approximation to “seed” the second phase, which consists of a more sophisticated distributed algorithm. Equipped with the first-step approximation, the second phase relies on results from the &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3406325.3451022&quot;>;Coreset literature&lt;/a>; to subsample a relevant set of input points and find a good differentially private clustering solution for the input points. We then prove that this solution generalizes with approximately the same guarantee to the entire input. &lt;/p>; &lt;br />; &lt;h2>;Vaccination search insights via DP clustering&lt;/h2>; &lt;p>; We then apply these advances in differentially private clustering to real-world applications. One example is our application of our differentially-private clustering solution for publishing COVID vaccine-related queries, while providing strong privacy protections for the users. &lt;/p>; &lt;p>; The goal of &lt;a href=&quot;https://google-research.github.io/vaccination-search-insights/?&quot;>;Vaccination Search Insights&lt;/a>; (VSI) is to help public health decision makers (health authorities, government agencies and nonprofits) identify and respond to communities&#39; information needs regarding COVID vaccines. In order to achieve this, the tool allows users to explore at different geolocation granularities (zip-code, county and state level in the US) the top themes searched by users regarding COVID queries. In particular, the tool visualizes statistics on trending queries rising in interest in a given locale and time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s800/COVIDSearchStats.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s16000/COVIDSearchStats.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of the output of the tool. Displayed on the left, the top searches related to Covid vaccines during the period Oct 10-16 2022. On the right, the queries that have had rising importance during the same period and compared to the previous week.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To better help identifying the themes of the trending searches, the tool clusters the search queries based on their semantic similarity. This is done by applying a custom-designed &lt;em>;k&lt;/em>;-means–based algorithm run over search data that has been anonymized using the DP Gaussian mechanism to add noise and remove low-count queries (thus resulting in a differentially clustering). The method ensures strong differential privacy guarantees for the protection of the user data. &lt;/p>; &lt;p>; This tool provided fine-grained data on COVID vaccine perception in the population at unprecedented scales of granularity, something that is especially relevant to understand the needs of the marginalized communities disproportionately affected by COVID. This project highlights the impact of our investment in research in differential privacy, and unsupervised ML methods. We are looking to other important areas where we can apply these clustering techniques to help guide decision making around global health challenges, like search queries on &lt;a href=&quot;https://blog.google/technology/health/dr-von-nguyens-temperature-check-on-public-health/&quot;>;climate change–related challenges&lt;/a>; such as air quality or extreme heat. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank our co-authors Jacob Imola, Silvio Lattanzi, Jason Lee, Mohammad Mahdian, Vahab Mirrokni, Andres Munoz Medina, Shyam Narayanan, Mark Phillips, David Saulpic, Chris Schwiegelshohn, Sergei Vassilvitskii, Peilin Zhong, and the members of the&lt;/em>;&amp;nbsp;&lt;i>;Health AI&lt;/i>;&lt;em>; team that made the VSI launch possible: Shailesh Bavadekar, Adam Boulanger, Tague Griffith, Mansi Kansal, Chaitanya Kamath, Akim Kumok, Yael Mayer, Tomer Shekel, Megan Shum, Charlotte Stanton, Mimi Sun, Swapnil Vispute, and Mark Young. &lt;/em>; &lt;/p>; &lt;p>; &lt;em>;For more information on the &lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/graph-mining/&quot;>;Graph Mining team&lt;/a>; (part of &lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/&quot;>;Algorithm and Optimization&lt;/a>;) visit our pages.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4176009881387884637/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private clustering for large-scale datasets&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s72-c/COVID.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6927879920303666871&lt;/id>;&lt;published>;2023-05-25T10:03:00.008-07:00&lt;/published>;&lt;updated>;2023-06-01T08:59:51.641-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google I/O&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google Research at I/O 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by James Manyika, SVP Google Research and Technology &amp;amp; Society, and Jeff Dean, Chief Scientist, Google DeepMind and Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s1200/GoogleIO.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Wednesday, May 10th was an exciting day for the &lt;a href=&quot;https://research.google&quot;>;Google Research&lt;/a>; community as we watched the results of months and years of our foundational and applied work get announced on the &lt;a href=&quot;https://io.google/2023/&quot;>;Google I/O&lt;/a>; stage. With the quick pace of announcements on stage, it can be difficult to convey the substantial effort and unique innovations that underlie the technologies we presented. So today, we&#39;re excited to reveal more about the research efforts behind some of the many compelling announcements at &lt;a href=&quot;https://www.youtube.com/playlist?list=PL590L5WQmH8dAqv03RCMbZrbzxqCn6W3O&quot;>;this year&#39;s I/O&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;br />; &lt;h2>;PaLM 2 &lt;/h2>; &lt;p>; Our next-generation large language model (LLM), &lt;a href=&quot;https://ai.google/discover/palm2&quot;>;PaLM 2&lt;/a>;, is built on advances in &lt;a href=&quot;https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training&quot;>;compute-optimal scaling&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;scaled instruction-fine tuning&lt;/a>; and &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;improved dataset mixture&lt;/a>;. By fine-tuning and instruction-tuning the model for different purposes, we have been able to integrate state-of-the-art capabilities into over 25 Google products and features, where it is already helping to inform, assist and delight users. For example: &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; is an early experiment that lets you collaborate with generative AI and helps to boost productivity, accelerate ideas and fuel curiosity. It builds on &lt;a href=&quot;https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html&quot;>;advances in deep learning efficiency&lt;/a>; and leverages &lt;a href=&quot;https://ai.google/static/documents/google-about-bard.pdf&quot;>;reinforcement learning from human feedback&lt;/a>; to provide more relevant responses and increase the model&#39;s ability to follow instructions. Bard is now available in 180 countries, where users can interact with it in English, Japanese and Korean, and thanks to the multilingual capabilities afforded by PaLM 2, support for 40 languages is coming soon. &lt;/li>;&lt;li>;With &lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;Search Generative Experience&lt;/a>; we&#39;re taking more of the work out of searching, so you&#39;ll be able to understand a topic faster, uncover new viewpoints and insights, and get things done more easily. As part of this experiment, you&#39;ll see an AI-powered snapshot of key information to consider, with links to dig deeper. &lt;/li>;&lt;li>;&lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; is an easy-to-use prototyping environment for the &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;PaLM API&lt;/a>;, powered by PaLM 2. In fact, internal user engagement with early prototypes of MakerSuite accelerated the development of our PaLM 2 model itself. MakerSuite grew out of research focused on prompting tools, or tools explicitly designed for customizing and controlling LLMs. This line of research includes &lt;a href=&quot;https://research.google/pubs/pub51353/&quot;>;PromptMaker&lt;/a>; (precursor to MakerSuite), and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517582&quot;>;AI Chains&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3491101.3519729&quot;>;PromptChainer&lt;/a>; (one of the first research efforts demonstrating the utility of LLM chaining). &lt;/li>;&lt;li>;Project &lt;a href=&quot;https://thoughtful.withgoogle.com/about&quot;>;Tailwind&lt;/a>; also made use of early research prototypes of MakerSuite to develop features to help writers and researchers explore ideas and improve their prose; its AI-first notebook prototype used PaLM 2 to allow users to ask questions of the model grounded in documents they define. &lt;/li>;&lt;li>;&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>; is our state-of-the-art medical LLM, built on PaLM 2. Med-PaLM 2 achieved &lt;a href=&quot;https://arxiv.org/abs/2305.09617&quot;>;86.5% performance&lt;/a>; on US Medical Licensing Exam–style questions, illustrating its exciting potential for health. We&#39;re now exploring multimodal capabilities to synthesize inputs like X-rays. &lt;/li>;&lt;li>;&lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Codey&lt;/a>; is a version of PaLM 2 fine-tuned on source code to function as a developer assistant. It supports a broad range of &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;Code AI&lt;/a>; features, including code completions, code explanation, bug fixing, source code migration, error explanations, and more. Codey is available through our trusted tester program via IDEs (&lt;a href=&quot;https://blog.google/technology/developers/google-colab-ai-coding-features/&quot;>;Colab&lt;/a>;, &lt;a href=&quot;https://developer.android.com/studio/preview/studio-bot&quot;>;Android Studio&lt;/a>;, &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-ai-for-google-cloud&quot;>;Duet AI for Cloud&lt;/a>;, &lt;a href=&quot;https://techcrunch.com/2023/05/10/googles-firebase-gets-ai-extensions-opens-up-its-marketplace/&quot;>;Firebase&lt;/a>;) and via a &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;3P-facing API&lt;/a>;. &lt;/li>; &lt;/ul>; &lt;p>; Perhaps even more exciting for developers, we have opened up the &lt;a href=&quot;https://makersuite.google.com/&quot;>;PaLM APIs &amp;amp; MakerSuite&lt;/a>; to provide the community opportunities to innovate using this groundbreaking technology. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/s1002/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1002&quot; height=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/w640-h616/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PaLM 2 has advanced coding capabilities that enable it to find code errors and make suggestions in a number of different languages.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Imagen&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Imagen family of image generation and editing models&lt;/a>; builds on advances in large &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>;-based language models and &lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;diffusion models&lt;/a>;. This family of models is being incorporated into multiple Google products, including: &lt;/p>; &lt;ul>; &lt;li>;Image generation in &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>;Google Slides&lt;/a>; and Android&#39;s &lt;a href=&quot;https://blog.google/products/android/new-android-features-generative-ai/&quot;>;Generative AI wallpaper&lt;/a>; are powered by our text-to-image generation features. &lt;/li>;&lt;li>;&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-new-ai-models-opens-generative-ai-studio&quot;>;Google Cloud&#39;s Vertex AI&lt;/a>; enables image generation, image editing, image upscaling and fine-tuning to help enterprise customers meet their business needs. &lt;/li>;&lt;li>;&lt;a href=&quot;https://developers.googleblog.com/2023/05/how-its-made-io-flip-adds-twist-to.html&quot;>;I/O Flip&lt;/a>;, a digital take on a classic card game, features Google developer mascots on cards that were entirely AI generated. This game showcased a fine-tuning technique called &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; for adapting pre-trained image generation models. Using just a handful of images as &lt;a href=&quot;https://dreambooth.github.io/&quot;>;inputs for fine-tuning&lt;/a>;, it allows users to generate personalized images in minutes. With DreamBooth, users can synthesize a subject in diverse scenes, poses, views, and lighting conditions that don&#39;t appear in the reference images. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s1649/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;1649&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;I/O Flip presents custom card decks designed using DreamBooth.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Phenaki&lt;/h2>; &lt;p>; &lt;a href=&quot;https://sites.research.google/phenaki/&quot;>;Phenaki&lt;/a>;, Google&#39;s Transformer-based text-to-video generation model was featured in the I/O pre-show. Phenaki is a &lt;a href=&quot;https://openreview.net/forum?id=vOEXS39nOF&quot;>;model that can synthesize realistic videos&lt;/a>; from textual prompt sequences by leveraging two main components: an encoder-decoder model that compresses videos to discrete embeddings and a transformer model that translates text embeddings to video tokens. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOOX_c2imHsESoim3U4K0eNWE97rtrGkVWd7TZ1icscOT3qf3O_BdvBakgEk7KwAYMIbzHCb7J_xL7rXxRbe4o-m0U9wdc4CN8oxMAliornJInsRbjNe0h2c7XEXjkoDGF8oavzodCis4oH2vMwwh19du1oLle_JZvtO1KoraOOpWEUuyNHVUiVzhgGQ/s200/panda_bear_highres.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;200&quot; height=&quot;200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOOX_c2imHsESoim3U4K0eNWE97rtrGkVWd7TZ1icscOT3qf3O_BdvBakgEk7KwAYMIbzHCb7J_xL7rXxRbe4o-m0U9wdc4CN8oxMAliornJInsRbjNe0h2c7XEXjkoDGF8oavzodCis4oH2vMwwh19du1oLle_JZvtO1KoraOOpWEUuyNHVUiVzhgGQ/s1600/panda_bear_highres.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivgQpjnthO9y35MrLxLMfWGkUwNWnaqD3Pt7rDSDKg-dThTWICJL7NGW5E7PGJnoiZZA_hk6Rgw3ChWCn7YSNu5QEpgfTbgG2TBaQ7WLPs-TPZ89cVvvHEaFP-AqqRxSox4ogjZ889Cz06pbDVL7P0s8VXJdl7m4MuqNtJLsVsK89ScepRJIt94Z9_bw/s635/Screenshot%202023-06-01%2010.56.18%20AM.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;322&quot; data-original-width=&quot;635&quot; height=&quot;162&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEivgQpjnthO9y35MrLxLMfWGkUwNWnaqD3Pt7rDSDKg-dThTWICJL7NGW5E7PGJnoiZZA_hk6Rgw3ChWCn7YSNu5QEpgfTbgG2TBaQ7WLPs-TPZ89cVvvHEaFP-AqqRxSox4ogjZ889Cz06pbDVL7P0s8VXJdl7m4MuqNtJLsVsK89ScepRJIt94Z9_bw/s320/Screenshot%202023-06-01%2010.56.18%20AM.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/s128/image4.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;128&quot; data-original-width=&quot;128&quot; height=&quot;200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/w200-h200/image4.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s548/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;269&quot; data-original-width=&quot;548&quot; height=&quot;157&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s320/image6.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;br />; &lt;h2>;ARCore and the Scene Semantic API&lt;/h2>; &lt;p>; Among the new features of &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; announced by the AR team at I/O, the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;Scene Semantic API&lt;/a>; can recognize pixel-wise semantics in an outdoor scene. This helps users create custom AR experiences based on the features in the surrounding area. This API is empowered by the outdoor semantic segmentation model, leveraging our recent works around the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLab&lt;/a>; architecture and an egocentric outdoor scene understanding dataset. The latest ARCore release also includes an improved monocular depth model that provides higher accuracy in outdoor scenes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/s486/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;242&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/w199-h400/image3.gif&quot; width=&quot;199&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scene Semantics API uses DeepLab-based semantic segmentation model to provide accurate pixel-wise labels in a scene outdoors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chirp&lt;/h2>; &lt;p>; &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;Chirp&lt;/a>; is Google&#39;s family of state-of-the-art &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Models&lt;/a>; trained on 12 million hours of speech to enable automatic speech recognition (ASR) for 100+ languages. The models can perform ASR on under-resourced languages, such as Amharic, Cebuano, and Assamese, in addition to widely spoken languages like English and Mandarin. Chirp is able to cover such a wide variety of languages by leveraging &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;self-supervised learning on unlabeled multilingual dataset with fine-tuning on a smaller set of labeled data&lt;/a>;. Chirp is now available in the Google Cloud &lt;a href=&quot;https://cloud.google.com/speech-to-text&quot;>;Speech-to-Text API&lt;/a>;, allowing users to perform inference on the model through a simple interface. You can get started with Chirp &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;here&lt;/a>;. &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s591/image7.gif&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;250&quot; data-original-width=&quot;591&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s16000/image7.gif&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;MusicLM&lt;/h2>; &lt;p>; At I/O, we launched &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM, a text-to-music model&lt;/a>; that generates 20 seconds of music from a text prompt. &lt;a href=&quot;https://aitestkitchen.withgoogle.com/experiments/music-lm&quot;>;You can try it yourself on AI Test Kitchen&lt;/a>;, or see it featured during the I/O preshow, where electronic musician and composer &lt;a href=&quot;https://en.wikipedia.org/wiki/Dan_Deacon&quot;>;Dan Deacon&lt;/a>; used MusicLM in his performance. &lt;/p>; &lt;p>; MusicLM, which consists of models powered by &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2208.12415&quot;>;MuLAN&lt;/a>;, can make music (from text, humming, images or video) and musical accompaniments to singing. AudioLM generates high quality audio with long-term consistency. It maps audio to a sequence of discrete tokens and casts audio generation as a language modeling task. To synthesize longer outputs efficiently, it used a novel approach we&#39;ve developed called &lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm&lt;/a>;. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s1927/keywordhero.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1927&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s16000/keywordhero.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;Universal Translator dubbing&lt;/h2>; &lt;p>; Our dubbing efforts leverage dozens of ML technologies to translate the full expressive range of video content, making videos accessible to audiences across the world. These technologies have been used to &lt;a href=&quot;https://www.youtube.com/watch?v=S-iIV5Oo0n0&quot;>;dub videos&lt;/a>; across a variety of products and content types, including educational content, advertising campaigns, and creator content, with more to come. We use deep learning technology to achieve &lt;a href=&quot;https://developers.googleblog.com/2022/12/improving-video-voice-dubbing-through-deep-learning.html&quot;>;voice preservation and lip matching&lt;/a>; and enable high-quality video translation. We&#39;ve built this product to include human review for quality, safety checks to help prevent misuse, and we make it accessible only to authorized partners. &lt;/p>; &lt;br />; &lt;h2>;AI for global societal good&lt;/h2>; &lt;p>; We are applying our AI technologies to solve some of the biggest global challenges, like mitigating climate change, adapting to a warming planet and improving human health and wellbeing. For example: &lt;/p>; &lt;ul>; &lt;li>;Traffic engineers use our Green Light recommendations to reduce stop-and-go traffic at intersections and improve the flow of traffic in cities from Bangalore to Rio de Janeiro and Hamburg. Green Light models each intersection, analyzing traffic patterns to develop recommendations that make traffic lights more efficient — for example, by better synchronizing timing between adjacent lights, or adjusting the “green time” for a given street and direction. &lt;/li>;&lt;li>;We&#39;ve also expanded global coverage on the &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>; to 80 countries, as part of our efforts to predict riverine floods and alert people who are about to be impacted before disaster strikes. Our &lt;a href=&quot;https://sites.research.google/floodforecasting&quot;>;flood forecasting efforts&lt;/a>; rely on &lt;a href=&quot;https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;hydrological models&lt;/a>; informed by satellite observations, weather forecasts and in-situ measurements. &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s1672/image5.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;713&quot; data-original-width=&quot;1672&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s16000/image5.png&quot; />;&lt;/a>;&lt;/div>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Technologies for inclusive and fair ML applications&lt;/h2>; &lt;p>; With our continued investment in AI technologies, we are emphasizing responsible AI development with the goal of making our models and tools useful and impactful while also ensuring fairness, safety and alignment with our &lt;a href=&quot;https://ai.google/principles/&quot;>;AI Principles&lt;/a>;. Some of these efforts were highlighted at I/O, including: &lt;/p>; &lt;ul>; &lt;li>;The release of the &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;Monk Skin Tone Examples (MST-E) Dataset&lt;/a>; to help practitioners gain a deeper understanding of the MST scale and train human annotators for more consistent, inclusive, and meaningful skin tone annotations. You can read more about this and other developments on our &lt;a href=&quot;https://skintone.google/&quot;>;website&lt;/a>;. This is an advancement on the open source release of the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone (MST) Scale&lt;/a>; we launched last year to enable developers to build products that are more inclusive and that better represent their diverse users. &lt;/li>;&lt;li>;A &lt;a href=&quot;https://www.kaggle.com/competitions/asl-fingerspelling/overview/description&quot;>;new Kaggle competition&lt;/a>; (open until August 10th) in which the ML community is tasked with creating a model that can quickly and accurately identify American Sign Language (ASL) fingerspelling — where each letter of a word is spelled out in ASL rapidly using a single hand, rather than using the specific signs for entire words — and translate it into written text. Learn more about the &lt;a href=&quot;https://www.youtube.com/watch?v=q3xKB3dfvtA&quot;>;fingerspelling Kaggle competition&lt;/a>;, which features a song from &lt;a href=&quot;https://www.deafandloud.com/&quot;>;Sean Forbes&lt;/a>;, a deaf musician and rapper. We also &lt;a href=&quot;https://www.youtube.com/watch?v=WC9x3jp_nV8&quot;>;showcased at I/O&lt;/a>; the winning algorithm from the prior year&#39;s competition powers &lt;a href=&quot;https://play.google.com/store/apps/details?id=edu.gatech.popsignai&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;PopSign&lt;/a>;, an ASL learning app for parents of deaf or hard of hearing children created by Georgia Tech and Rochester Institute of Technology (RIT). &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Building the future of AI together&lt;/h2>; &lt;p>; It&#39;s inspiring to be part of a community of so many talented individuals who are leading the way in developing state-of-the-art technologies, responsible AI approaches and exciting user experiences. We are in the midst of a period of incredible and transformative change for AI. Stay tuned for more updates about the ways in which the Google Research community is boldly exploring the frontiers of these technologies and using them responsibly to benefit people&#39;s lives around the world. We hope you&#39;re as excited as we are about the future of AI technologies and we invite you to engage with our teams through the references, sites and tools that we&#39;ve highlighted here. &lt;/p>;&lt;p>;&lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6927879920303666871/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research at I/O 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s72-c/GoogleIO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5648346519353613408&lt;/id>;&lt;published>;2023-05-23T10:51:00.007-07:00&lt;/published>;&lt;updated>;2023-05-31T12:19:41.513-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Semantic Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Resolving code review comments with ML&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alexander Frömmgen, Staff Software Engineer, and Lera Kharatyan, Senior Software Engineer, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s2000/comments2code.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Code-change reviews are a critical part of the software development process at scale, &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;taking&lt;/a>; a significant amount of the code authors&#39; and the code reviewers&#39; time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;millions of reviewer comments&lt;/a>; per year, and authors require an average of ~60 minutes &lt;a href=&quot;https://research.google/pubs/pub49446/&quot;>;active shepherding time&lt;/a>; between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must do to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code review process, eg, by proposing code changes based on a comment&#39;s text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe our application of recent advances in large sequence models (using the &lt;a href=&quot;https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html&quot;>;DIDACT methodology&lt;/a>;) in a real-world setting to automatically resolve code review comments in the day-to-day development workflow at Google. As of today, code-change authors at Google address a substantial amount of reviewer comments by applying an ML-suggested edit. We expect that to reduce time spent on code reviews by hundreds of thousands of hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers&#39; productivity and allows them to focus on more creative and complex tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Predicting the code edit&lt;/h2>; &lt;p>; We started by training a model that predicts code edits needed to address reviewer comments. The model is pre-trained on various coding tasks and related developer activities (eg, renaming a variable, repairing a broken build, editing a file). It&#39;s then fine-tuned for this specific task with reviewed code changes, the reviewer comments, and the edits the author performed to address those comments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s1523/figure1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1523&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s16000/figure1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of an ML-suggested edit of refactorings that are spread within the code.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Google uses a &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2854146&quot;>;monorepo&lt;/a>;, a single repository for all of its software artifacts, which allows our training dataset to include all unrestricted code used to build Google&#39;s most recent software, as well as previous versions. &lt;/p>; &lt;p>; To improve the model quality, we iterated on the training dataset. For example, we compared the model performance for datasets with a single reviewer comment per file to datasets with multiple comments per file, and experimented with classifiers to clean up the training data based on a small, curated dataset to choose the model with the best offline precision and recall metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Serving infrastructure and user experience&lt;/h2>; &lt;p>; We designed and implemented the feature on top of the trained model, focusing on the overall user experience and developer efficiency. As part of this, we explored different user experience (UX) alternatives through a series of user studies. We then refined the feature based on insights from an internal beta (ie, a test of the feature in development) including user feedback (eg, a “Was this helpful?” button next to the suggested edit). &lt;/p>; &lt;p>; The final model was calibrated for a target &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&quot;>;precision&lt;/a>; of 50%. That is, we tuned the model and the suggestions filtering, so that 50% of suggested edits on our evaluation dataset are correct. In general, increasing the target precision reduces the number of shown suggested edits, and decreasing the target precision leads to more incorrect suggested edits. Incorrect suggested edits take the developers time and reduce the developers&#39; trust in the feature. We found that a target precision of 50% provides a good balance. &lt;/p>; &lt;p>; At a high level, for every new reviewer comment, we generate the model input in the same format that is used for training, query the model, and generate the suggested code edit. If the model is confident in the prediction and a few additional heuristics are satisfied, we send the suggested edit to downstream systems. The downstream systems, ie, the code review frontend and the integrated development environment (IDE), expose the suggested edits to the user and log user interactions, such as preview and apply events. A dedicated pipeline collects these logs and generates aggregate insights, eg, the overall acceptance rates as reported in this blog post. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s1250/figure%202c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1250&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s16000/figure%202c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Architecture of the ML-suggested edits infrastructure. We process code and infrastructure from multiple services, get the model predictions and surface the predictions in the code review tool and IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The developer interacts with the ML-suggested edits in the code review tool and the IDE. Based on insights from the user studies, the integration into the code review tool is most suitable for a streamlined review experience. The IDE integration provides additional functionality and supports 3-way merging of the ML-suggested edits (left in the figure below) in case of conflicting local changes on top of the reviewed code state (right) into the merge result (center). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s958/figure%203%20(25%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s16000/figure%203%20(25%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;3-way-merge UX in IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Offline evaluations indicate that the model addresses 52% of comments with a target precision of 50%. The online metrics of the beta and the full internal launch confirm these offline metrics, ie, we see model suggestions above our target model confidence for around 50% of all relevant reviewer comments. 40% to 50% of all previewed suggested edits are applied by code authors. &lt;/p>; &lt;p>; We used the “not helpful” feedback during the beta to identify recurring failure patterns of the model. We implemented serving-time heuristics to filter these and, thus, reduce the number of shown incorrect predictions. With these changes, we traded quantity for quality and observed an increased real-world acceptance rate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s1753/figure%204%20(50%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1753&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s16000/figure%204%20(50%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Code review tool UX. The suggestion is shown as part of the comment and can be previewed, applied and rated as helpful or not helpful.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our beta launch showed a &lt;em>;discoverability challenge&lt;/em>;: code authors only previewed ~20% of all generated suggested edits. We modified the UX and introduced a prominent “Show ML-edit” button (see the figure above) next to the reviewer comment, leading to an overall preview rate of ~40% at launch. We additionally found that suggested edits in the code review tool are often not applicable due to conflicting changes that the author did during the review process. We addressed this with a button in the code review tool that opens the IDE in a merge view for the suggested edit. We now observe that more than 70% of these are applied in the code review tool and fewer than 30% are applied in the IDE. All these changes allowed us to increase the overall fraction of reviewer comments that are addressed with an ML-suggested edit by a factor of 2 from beta to the full internal launch. At Google scale, these results help automate the resolution of hundreds of thousands of comments each year. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s1920/figure%207.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;887&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s16000/figure%207.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Suggestions filtering funnel.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see ML-suggested edits addressing a wide range of reviewer comments in production. This includes simple localized refactorings and refactorings that are spread within the code, as shown in the examples throughout the blog post above. The feature addresses longer and less formally-worded comments that require code generation, refactorings and imports. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s1479/figure%205c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;651&quot; data-original-width=&quot;1479&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s16000/figure%205c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of a suggestion for a longer and less formally worded comment that requires code generation, refactorings and imports.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The model can also respond to complex comments and produce extensive code edits (shown below). The generated test case follows the existing unit test pattern, while changing the details as described in the comment. Additionally, the edit suggests a comprehensive name for the test reflecting the test semantics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s1497/figure%206c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1497&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s16000/figure%206c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the model&#39;s ability to respond to complex comments and produce extensive code edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this post, we introduced an ML-assistance feature to reduce the time spent on code review related changes. At the moment, a substantial amount of all actionable code review comments on supported languages are addressed with applied ML-suggested edits at Google. A 12-week A/B experiment across all Google developers will further measure the impact of the feature on the overall developer productivity. &lt;/p>; &lt;p>; We are working on improvements throughout the whole stack. This includes increasing the quality and recall of the model and building a more streamlined experience for the developer with improved discoverability throughout the review process. As part of this, we are investigating the option of showing suggested edits to the reviewer while they draft comments and expanding the feature into the IDE to enable code-change authors to get suggested code edits for natural-language commands. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This is the work of many people in Google Core Systems &amp;amp; Experiences team, Google Research, and DeepMind. We&#39;d like to specifically thank Peter Choy for bringing the collaboration together, and all of our team members for their key contributions and useful advice, including Marcus Revaj, Gabriela Surita, Maxim Tabachnyk, Jacob Austin, Nimesh Ghelani, Dan Zheng, Peter Josling, Mariana Stariolo, Chris Gorgolewski, Sascha Varkevisser, Katja Grünwedel, Alberto Elizondo, Tobias Welp, Paige Bailey, Pierre-Antoine Manzagol, Pascal Lamblin, Chenjie Gu, Petros Maniatis, Henryk Michalewski, Sara Wiltberger, Ambar Murillo, Satish Chandra, Madhura Dudhgaonkar, Niranjan Tulpule, Zoubin Ghahramani, Juanjo Carin, Danny Tarlow, Kevin Villela, Stoyan Nikolov, David Tattersall, Boris Bokowski, Kathy Nix, Mehdi Ghissassi, Luis C. Cobo, Yujia Li, David Choi, Kristóf Molnár, Vahid Meimand, Amit Patel, Brett Wiltshire, Laurent Le Brun, Mingpan Guo, Hermann Loose, Jonas Mattes, Savinee Dancs. Thanks to John Guilyard for creating the graphics in this post.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5648346519353613408/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot; rel=&quot;alternate&quot; title=&quot;Resolving code review comments with ML&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s72-c/comments2code.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3261601760457492933&lt;/id>;&lt;published>;2023-05-19T09:59:00.002-07:00&lt;/published>;&lt;updated>;2023-05-19T10:01:19.784-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Making ML models differentially private: Best practices and open challenges&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Natalia Ponomareva and Alex Kurakin, Staff Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s320/DPfy%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large machine learning (ML) models are ubiquitous in modern applications: from &lt;a href=&quot;https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters&quot;>;spam filters&lt;/a>; to &lt;a href=&quot;https://developers.google.com/machine-learning/recommendation/overview/types&quot;>;recommender systems&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;virtual assistants&lt;/a>;. These models achieve remarkable performance partially due to the abundance of available training data. However, these data can sometimes contain private information, including personal identifiable information, copyright material, etc. Therefore, protecting the privacy of the training data is critical to practical, applied ML. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential Privacy&lt;/a>; (DP) is one of the most widely accepted technologies that allows reasoning about data anonymization in a formal way. In the context of an ML model, DP can guarantee that each individual user&#39;s contribution will not result in a significantly different model. A model&#39;s privacy guarantees are characterized by a tuple (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;), where smaller values of both represent stronger DP guarantees and better privacy. &lt;/p>; &lt;p>; While there are successful &lt;a href=&quot;https://research.google/pubs/pub52351/&quot;>;examples&lt;/a>; of &lt;a href=&quot;https://ai.googleblog.com/2022/02/federated-learning-with-formal.html&quot;>;protecting training data&lt;/a>; using DP, obtaining good utility with differentially private ML (DP-ML) techniques can be challenging. First, there are inherent privacy/computation tradeoffs that may limit a model&#39;s utility. Further, DP-ML models often require architectural and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter&lt;/a>; tuning, and guidelines on how to do this effectively are limited or difficult to find. Finally, non-rigorous privacy reporting makes it challenging to compare and choose the best DP methods. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy&lt;/a>;”, to appear in the &lt;em>;&lt;a href=&quot;https://www.jair.org/index.php/jair&quot;>;Journal of Artificial Intelligence Research&lt;/a>;&lt;/em>;, we discuss the current state of DP-ML research. We provide an overview of common techniques for obtaining DP-ML models and discuss research, engineering challenges, mitigation techniques and current open questions. We will present tutorials based on this work at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>; and &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-ML methods&lt;/h2>; &lt;p>; DP can be introduced during the ML model development process in three places: (1) at the input data level, (2) during training, or (3) at inference. Each option provides privacy protections at different stages of the ML development process, with the weakest being when DP is introduced at the prediction level and the strongest being when introduced at the input level. Making the input data differentially private means that any model that is trained on this data will also have DP guarantees. When introducing DP during the training, only that particular model has DP guarantees. DP at the prediction level means that only the model&#39;s predictions are protected, but the model itself is not differentially private. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s1821/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;1821&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The task of introducing DP gets progressively easier from the left to right.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; DP is commonly introduced during training (DP-training). &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;Gradient noise injection&lt;/a>; methods, like &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, and their extensions are currently the most practical methods for achieving DP guarantees in complex models like large deep neural networks. &lt;/p>; &lt;p>; DP-SGD builds off of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; (SGD) optimizer with two modifications: (1) per-example gradients are clipped to a certain norm to limit sensitivity (the influence of an individual example on the overall model), which is a slow and computationally intensive process, and (2) a noisy gradient update is formed by taking aggregated gradients and adding noise that is proportional to the sensitivity and the strength of privacy guarantees. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s1439/DP-ML.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;494&quot; data-original-width=&quot;1439&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s16000/DP-ML.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP-SGD is a modification of SGD that involves a) clipping per-example gradients to limit the sensitivity and b) adding the noise, calibrated to the sensitivity and privacy guarantees, to the aggregated gradients, before the gradient update step. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Existing DP-training challenges&lt;/h2>; &lt;p>; Gradient noise injection methods usually exhibit: (1) loss of utility, (2) slower training, and (3) an increased &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_footprint&quot;>;memory footprint&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Loss of utility&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The best method for reducing utility drop is to use more computation. Using larger batch sizes and/or more iterations is one of the most prominent and practical ways of improving a model&#39;s performance. Hyperparameter tuning is also extremely important but often overlooked. The utility of DP-trained models is sensitive to the total amount of noise added, which depends on hyperparameters, like the clipping norm and batch size. Additionally, other hyperparameters like the learning rate should be re-tuned to account for noisy gradient updates. &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Another option is to obtain more data or use public data of similar distribution. This can be done by leveraging publicly available checkpoints, like &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/1910.10683.pdf&quot;>;T5&lt;/a>;, and fine-tuning them using private data. &lt;/p>; &lt;p>; &lt;strong>;Slower training&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Most gradient noise injection methods limit sensitivity via clipping per-example gradients, considerably slowing down &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;>;backpropagation&lt;/a>;. This can be addressed by choosing an efficient DP framework that efficiently implements per-example clipping. &lt;/p>; &lt;p>; &lt;strong>;Increased memory footprint&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;DP-training requires significant memory for computing and storing per-example gradients. Additionally, it requires significantly larger batches to obtain better utility. Increasing the computation resources (eg, the number and size of accelerators) is the simplest solution for extra memory requirements. Alternatively, &lt;a href=&quot;https://arxiv.org/abs/2109.12298&quot;>;several&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2201.12328&quot;>;works&lt;/a>; advocate for gradient accumulation where smaller batches are combined to simulate a larger batch before the gradient update is applied. Further, some algorithms (eg, &lt;a href=&quot;https://arxiv.org/pdf/2110.05679.pdf&quot;>;ghost clipping&lt;/a>;, which is based on &lt;a href=&quot;https://arxiv.org/abs/1510.01799&quot;>;this paper&lt;/a>;) avoid per-example gradient clipping altogether. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Best practices&lt;/h2>; &lt;p>; The following best practices can attain rigorous DP guarantees with the best model utility possible. &lt;/p>; &lt;p>; &lt;strong>;Choosing the right privacy unit:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;First, we should be clear about a model&#39;s privacy guarantees. This is encoded by selecting the “privacy unit,” which represents the neighboring dataset concept (ie, datasets where only one row is different). Example-level protection is a common choice in the research literature, but may not be ideal, however, for user-generated data if individual users contributed multiple records to the training dataset. For such a case, user-level protection might be more appropriate. For text and sequence data, the choice of the unit is harder since in most applications individual training examples are not aligned to the semantic meaning embedded in the text. &lt;/p>; &lt;p>; &lt;strong>;Choosing privacy guarantees:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;We outline three broad tiers of privacy guarantees and encourage practitioners to choose the lowest possible tier below: &lt;/p>; &lt;ul style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;Tier 1 — Strong privacy guarantees:&lt;/em>; Choosing &lt;em>;ε&lt;/em>; ≤ 1 provides a strong privacy guarantee, but frequently results in a significant utility drop for large models and thus may only be feasible for smaller models. &lt;/li>;&lt;li>;&lt;em>;Tier 2 — Reasonable privacy guarantees:&lt;/em>; We advocate for the currently undocumented, but still widely used, goal for DP-ML models to achieve an &lt;em>;ε&lt;/em>; ≤ 10. &lt;/li>;&lt;li>;&lt;em>;Tier 3 — Weak privacy guarantees:&lt;/em>; Any finite &lt;em>;ε&lt;/em>; is an improvement over a model with no formal privacy guarantee. However, for &lt;em>;ε&lt;/em>; &amp;gt; 10, the DP guarantee alone cannot be taken as sufficient evidence of data anonymization, and additional measures (eg, empirical privacy auditing) may be necessary to ensure the model protects user data. &lt;/li>; &lt;/ul>; &lt;p>; &lt;strong>;Hyperparameter tuning&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Choosing hyperparameters requires optimizing over three inter-dependent objectives: 1) model utility, 2) privacy cost &lt;em>;ε&lt;/em>;, and 3) computation cost. Common strategies take two of the three as constraints, and focus on optimizing the third. We provide methods that will maximize the utility with a limited number of trials, eg, tuning with privacy and computation constraints. &lt;/p>; &lt;p>; &lt;strong>;Reporting privacy guarantees: &lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;A lot of works on DP for ML report only &lt;em>;ε&lt;/em>; and possibly &lt;em>;δ &lt;/em>;values for their training procedure. However, we believe that practitioners should provide a comprehensive overview of model guarantees that includes: &lt;/p>; &lt;ol style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;DP setting:&lt;/em>; Are the results assuming central DP with a trusted service provider, &lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;local DP&lt;/a>;, or some other setting? &lt;/li>;&lt;li>;&lt;em>;Instantiating the DP definition:&lt;/em>; &lt;ol type=&quot;a&quot;>; &lt;li>;&lt;em>;Data accesses covered:&lt;/em>; Whether the DP guarantee applies (only) to a single training run or also covers hyperparameter tuning etc. &lt;/li>;&lt;li>;&lt;em>;Final mechanism&#39;s output&lt;/em>;: What is covered by the privacy guarantees and can be released publicly (eg, model checkpoints, the full sequence of privatized gradients, etc.) &lt;/li>;&lt;li>;&lt;em>;Unit of privacy&lt;/em>;: The selected “privacy unit” (example-level, user-level, etc.) &lt;/li>;&lt;li>;&lt;em>;Adjacency definition&lt;/em>; for DP “neighboring” datasets: A description of how neighboring datasets differ (eg, add-or-remove, replace-one, zero-out-one). &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;Privacy accounting details:&lt;/em>; Providing accounting details, eg, composition and amplification, are important for proper comparison between methods and should include: &lt;ol type=&quot;a&quot;>; &lt;li>;Type of accounting used, eg, &lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;-based accounting, PLD accounting, etc. &lt;/li>;&lt;li>;Accounting assumptions and whether they hold (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_regression#Maximum_likelihood-based_parameter_estimation&quot;>;Poisson&lt;/a>; sampling was assumed for privacy amplification but data shuffling was used in training). &lt;/li>;&lt;li>;Formal DP statement for the model and tuning process (eg, the specific &lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;-DP or &lt;em>;&lt;a href=&quot;https://arxiv.org/pdf/1605.02065.pdf&quot;>;ρ-zCDP&lt;/a>;&lt;/em>; values). &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;Transparency and verifiability:&lt;/em>; When possible, complete open-source code using standard DP libraries for the key mechanism implementation and accounting components. &lt;/li>; &lt;/ol>; &lt;p>; &lt;strong>;Paying attention to all the components used:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Usually, DP-training is a straightforward application of DP-SGD or other algorithms. However, some components or losses that are often used in ML models (eg, &lt;a href=&quot;https://www.baeldung.com/cs/contrastive-learning#:~:text=Contrastive%20Loss,and%20dissimilar%20samples%20far%20apart.&quot;>;contrastive losses&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;graph neural network&lt;/a>; layers) should be examined to ensure privacy guarantees are not violated. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Open questions&lt;/h2>; &lt;p>; While DP-ML is an active research area, we highlight the broad areas where there is room for improvement. &lt;/p>; &lt;p>; &lt;strong>;Developing better accounting methods&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Our current understanding of DP-training &lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;strong>; &lt;/strong>;&lt;/em>;guarantees&lt;strong>; &lt;/strong>;relies on a number of techniques, like Rényi DP composition and privacy amplification. We believe that better accounting methods for existing algorithms will demonstrate that DP guarantees for ML models are actually better than expected. &lt;/p>; &lt;p>; &lt;strong>;Developing better algorithms:&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The computational burden of using gradient noise injection for DP-training comes from the need to use larger batches and limit per-example sensitivity. Developing methods that can use smaller batches or identifying other ways (apart from per-example clipping) to limit the sensitivity would be a breakthrough for DP-ML. &lt;/p>; &lt;p>; &lt;strong>;Better optimization techniques&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Directly applying the same DP-SGD recipe is believed to be suboptimal for adaptive optimizers because the noise added to privatize the gradient may accumulate in learning rate computation. Designing theoretically grounded DP adaptive optimizers remains an active research topic. Another potential direction is to better understand the surface of DP loss, since for standard (non-DP) ML models flatter regions have been shown to &lt;a href=&quot;https://arxiv.org/abs/2010.01412&quot;>;generalize&lt;/a>; &lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;>;better&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Identifying architectures that are more robust to noise&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;There&#39;s an opportunity to better understand whether we need to adjust the architecture of an existing model when introducing DP. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;survey paper&lt;/a>; summarizes the current research related to making ML models DP, and provides practical tips on how to achieve the best privacy-utility trade offs. Our hope is that this work will serve as a reference point for the practitioners who want to effectively apply DP to complex ML models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank Hussein Hazimeh, Zheng Xu , Carson Denison , H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien and Abhradeep Thakurta, Badih Ghazi, Chiyuan Zhang for the help preparing this blog post, paper and tutorials content. Thanks to John Guilyard for creating the graphics in this post, and Ravi Kumar for comments.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3261601760457492933/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot; rel=&quot;alternate&quot; title=&quot;Making ML models differentially private: Best practices and open challenges&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s72-c/DPfy%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-234098392689663352&lt;/id>;&lt;published>;2023-05-18T14:08:00.000-07:00&lt;/published>;&lt;updated>;2023-05-18T14:08:39.899-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparse video tubes for joint video and image vision transformers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s320/TubeViT%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Video understanding is a challenging problem that requires reasoning about both spatial information (eg, for objects in a scene, including their locations and relations) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Temporal_information_retrieval#:~:text=Temporal%20information%20retrieval%20(T%2DIR,of%20the%20user%20information%20needs.&quot;>;temporal information&lt;/a>; for activities or events shown in a video. There are many video understanding applications and tasks, such as &lt;a href=&quot;https://cloud.google.com/video-intelligence&quot;>;understanding the semantic content of web videos&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/02/robot-see-robot-do.html&quot;>;robot perception&lt;/a>;. However, current works, such as &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;ViViT&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2102.05095&quot;>;TimeSFormer&lt;/a>;, densely process the video and require significant compute, especially as model size plus video length and resolution increase. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning&lt;/a>;”, to be presented at &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>;, we introduce a simple technique that turns a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer&lt;/a>; (ViT) model image encoder into an efficient video backbone using sparse video tubes (learnable visual representations of samples from the video) to reduce the model&#39;s compute needs. This approach can seamlessly process both images and videos, which allows it to leverage both image and video data sources during training. This training further enables our sparse tubes ViT model to coalesce image and video backbones together to serve a dual role as either an image or video backbone (or both), depending on the input. We demonstrate that this model is scalable, can be adapted to large pre-trained ViTs without requiring full fine-tuning, and achieves state-of-the-art results across many video classification benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s860/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;219&quot; data-original-width=&quot;860&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using sparse video tubes to sample a video, combined with a standard ViT encoder, leads to an efficient visual representation that can be seamlessly shared with image inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Building a joint image-video backbone&lt;/h2>; &lt;p>; Our sparse tube ViT uses a standard ViT backbone, consisting of a stack of &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; layers, that processes video information. Previous methods, such as ViViT, densely tokenize the video and then apply &lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;factorized attention&lt;/a>;, ie, the attention weights for each token are computed separately for the temporal and spatial dimensions. In the standard ViT architecture, self-attention is computed over the whole token sequence. When using videos as input, token sequences become quite long, which can make this computation slow. Instead, in the method we propose, the video is sparsely sampled using &lt;em>;video tubes&lt;/em>;, which are 3D learnable visual representations of various shapes and sizes (described in more detail below) from the video. These tubes are used to sparsely sample the video using a &lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/strided-convolutions-wfUhx&quot;>;large temporal stride&lt;/a>;, ie, when a tube kernel is only applied to a few locations in the video, rather than every pixel. &lt;/p>; &lt;p>; By sparsely sampling the video tubes, we can use the same global self-attention module, rather than factorized attention like ViViT. We experimentally show that the addition of factorized attention layers can harm the performance due to the uninitialized weights. This single stack of transformer layers in the ViT backbone also enables better sharing of the weights and improves performance. Sparse video tube sampling is done by using a large spatial and temporal stride that selects tokens on a fixed grid. The large stride reduces the number of tokens in the full network, while still capturing both spatial and temporal information and enabling the efficient processing of all tokens. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Sparse video tubes&lt;/h2>; &lt;p>; Video tubes are 3D grid-based cuboids that can have different shapes or categories and capture different information with strides and starting locations that can overlap. In the model, we use three distinct tube shapes that capture: (1) only spatial information (resulting in a set of 2D image patches), (2) long temporal information (over a small spatial area), and (3) both spatial and temporal information equally. Tubes that capture only spatial information can be applied to both image and video inputs. Tubes that capture long temporal information or both temporal and spatial information equally are only applied to video inputs. Depending on the input video size, the three tube shapes are applied to the model multiple times to generate tokens. &lt;/p>; &lt;p>; A fixed position embedding, which captures the global location of each tube (including any strides, offsets, etc.) relative to all the other tubes, is applied to the video tubes. Different from the previous learned position embeddings, this fixed one better enables sparse, overlapping sampling. Capturing the global location of the tube helps the model know where each came from, which is especially helpful when tubes overlap or are sampled from distant video locations. Next, the tube features are concatenated together to form a set of &lt;em>;N&lt;/em>; tokens. These tokens are processed by a standard ViT encoder. Finally, we apply an attention pooling to compress all the tokens into a single representation and input to a fully connected (FC) layer to make the classification (eg, playing soccer, swimming, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s474/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;474&quot; data-original-width=&quot;346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our video ViT model works by sampling sparse video tubes from the video (shown at the bottom) to enable either or both image or video inputs to be seamlessly processed. These tubes have different shapes and capture different video features. Tube 1 (&lt;strong>;yellow&lt;/strong>;) only captures spatial information, resulting in a set of 2D patches that can be applied to image inputs. Tube 2 (&lt;strong>;red&lt;/strong>;) captures temporal information and some spatial information and tube 3 (&lt;strong>;green&lt;/strong>;) equally captures both temporal and spatial information (ie, the spatial size of the tube &lt;em>;x&lt;/em>; and &lt;em>;y&lt;/em>; are the same as the number of frames &lt;em>;t&lt;/em>;). Tubes 2 and 3 can only be applied to video inputs. The position embedding is added to all the tube features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaling video ViTs&lt;/h2>; &lt;p>; The process of building video backbones is computationally intensive, but our sparse tube ViT model enables computationally efficient scaling of video models, leveraging previously trained image backbones. Since image backbones can be adapted to a video backbone, large image backbones can be turned into large video backbones. More specifically, one can transfer the learned video feature representations from a small tube ViT to a large pre-trained image ViT and train the resulting model with video data for only a few steps, as opposed to a full training from scratch. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s899/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;899&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our approach enables scaling a sparse tube ViT in a more efficient way. Specifically, the video features from a small video ViT (&lt;strong>;top network&lt;/strong>;) can be transferred to a large, pre-trained image ViT (&lt;strong>;bottom network&lt;/strong>;), and further fine-tuned. This requires fewer training steps to achieve strong performance with the large model. This is beneficial as large video models might be prohibitively expensive to train from scratch.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our sparse tube ViT approach using &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;>;Kinetics-400&lt;/a>; (shown below), Kinetics-600 and Kinetics-700 datasets and compare its performance to a long list of prior methods. We find that our approach outperforms all prior methods. Importantly, it outperforms all state-of-the-art methods trained jointly on image+video datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBVbXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBVbXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance compared to several prior works on the popular Kinetics-400 video dataset. Our sparse tube ViT outperforms state-of-the-art methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore, we test our sparse tube ViT model on the &lt;a href=&quot;https://developer.qualcomm.com/software/ai-datasets/something-something&quot;>;Something-Something V2&lt;/a>; dataset, which is commonly used to evaluate more dynamic activities, and also report that it outperforms all prior state-of-the-art approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the Something-Something V2 video dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Visualizing some learned kernels&lt;/h2>; &lt;p>; It is interesting to understand what kind of rudimentary features are being learned by the proposed model. We visualize them below, showing both the 2D patches, which are shared for both images and videos, and video tubes. These visualizations show the 2D or 3D information being captured by the projection layer. For example, in the 2D patches, various common features, like edges and colors, are detected, while the 3D tubes capture basic shapes and how they may change over time. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s549/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;285&quot; data-original-width=&quot;549&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualizations of patches and tubes learned the sparse tube ViT model. Top row are the 2D patches and the remaining two rows are snapshots from the learned video tubes. The tubes show each patch for the 8 or 4 frames to which they are applied.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusions&lt;/h2>; &lt;p>; We have presented a new sparse tube ViT, which can turn a ViT encoder into an efficient video model, and can seamlessly work with both image and video inputs. We also showed that large video encoders can be bootstrapped from small video encoders and image-only ViTs. Our approach outperforms prior methods across several popular video understanding benchmarks. We believe that this simple representation can facilitate much more efficient learning with input videos, seamlessly incorporate either image or video inputs and effectively eliminate the bifurcation of image and video models for future multimodal understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is conducted by AJ Piergiovanni, Weicheng Kuo and Anelia Angelova, who are now at Google DeepMind. We thank Abhijit Ogale, Luowei Zhou, Claire Cui and our colleagues in Google Research for their helpful discussions, comments, and support.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/234098392689663352/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot; rel=&quot;alternate&quot; title=&quot;Sparse video tubes for joint video and image vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s72-c/TubeViT%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2749680625311121514&lt;/id>;&lt;published>;2023-05-18T10:12:00.006-07:00&lt;/published>;&lt;updated>;2023-05-18T16:14:50.453-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: PAIR&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Lucas Dixon and Michael Terry, co-leads, PAIR, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; PAIR (People + AI Research) first &lt;a href=&quot;https://blog.google/technology/ai/pair-people-ai-research-initiative/&quot;>;launched&lt;/a>; in 2017 with the belief that “AI can go much further — and be more useful to all of us — if we build systems with people in mind at the start of the process.” We continue to focus on making AI more understandable, interpretable, fun, and usable by more people around the world. It&#39;s a mission that is particularly timely given the emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chatbot&quot;>;chatbots&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, PAIR is part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team within Google Research, and our work spans this larger research space: We advance &lt;a href=&quot;https://pair.withgoogle.com/research/&quot;>;foundational research&lt;/a>; on human-AI interaction (HAI) and machine learning (ML); we publish educational materials, including the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;PAIR Guidebook&lt;/a>; and &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;Explorables&lt;/a>; (such as the recent Explorable looking at &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;); and we develop software tools like the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; to help people understand and debug ML behaviors. Our inspiration this year is &quot;changing the way people think about what &lt;em>;THEY&lt;/em>; can do with AI.” This vision is inspired by the rapid emergence of generative AI technologies, such as large language models (LLMs) that power chatbots like &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>;, and new generative media models like Google&#39;s &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;, and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;. In this blog post, we review recent PAIR work that is changing the way we engage with AI. &lt;/p>; &lt;br />; &lt;h2>;Generative AI research&lt;/h2>; &lt;p>; Generative AI is creating a lot of excitement, and PAIR is involved in a range of related research, from &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;>;using language models to create generative agents&lt;/a>;&amp;nbsp;to studying how artists adopted generative image models like &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; and &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;. These latter &quot;text-to-image&quot; models let a person input a text-based description of an image for the model to generate (eg, &quot;a gingerbread house in a forest in a cartoony style&quot;). In a forthcoming paper titled “&lt;a href=&quot;https://arxiv.org/abs/2303.12253&quot;>;The Prompt Artists&lt;/a>;” (to appear in &lt;a href=&quot;https://cc.acm.org/2023/&quot;>;Creativity and Cognition 2023&lt;/a>;), we found that users of generative image models strive not only to create beautiful images, but also to create unique, innovative styles. To help achieve these styles, some would even seek unique vocabulary to help develop their visual style. For example, they may visit architectural blogs to learn what domain-specific vocabulary they can adopt to help produce distinctive images of buildings. &lt;/p>; &lt;p>; We are also researching solutions to challenges faced by prompt creators who, with generative AI, are essentially programming without using a programming language. As an example, we developed &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;new methods&lt;/a>; for extracting semantically meaningful structure from natural language prompts. We have applied these structures to prompt editors to provide features similar to those found in other programming environments, such as semantic highlighting, autosuggest, and structured data views. &lt;/p>; &lt;p>; The growth of generative LLMs has also opened up new techniques to solve important long-standing problems. &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;Agile classifiers&lt;/a>; are one approach we&#39;re taking to leverage the semantic and syntactic strengths of LLMs to solve classification problems related to safer online discourse, such as nimbly blocking newer types of toxic language as quickly as it may evolve online. The big advance here is the ability to develop high quality classifiers from very small datasets — as small as 80 examples. This suggests a positive future for online discourse and better moderation of it: instead of collecting millions of examples to attempt to create universal safety classifiers for all use cases over months or years, more agile classifiers might be created by individuals or small organizations and tailored for their specific use cases, and iterated on and adapted in the time-span of a day (eg, to block a new kind of harassment being received or to correct unintended biases in models). As an example of their utility, these methods recently &lt;a href=&quot;https://www.aclweb.org/portal/content/semeval-2023-task-10-explainable-detection-online-sexism-edos&quot;>;won a SemEval competition&lt;/a>; to identify and explain sexism. &lt;/p>; &lt;p>; We&#39;ve also developed &lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>;new state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors and misbehaviours. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we also found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements on model accuracy. &lt;/p>; &lt;p>; Collectively, these methods are critical to help the scientific community improve generative models. They provide techniques for fast and effective content moderation and dialogue safety methods that help support creators whose content is the basis for generative models&#39; amazing outcomes. In addition, they provide direct tools to help debug model misbehavior which leads to better generation. &lt;/p>; &lt;br />; &lt;h2>;Visualization and education&lt;/h2>; &lt;p>; To lower barriers in understanding ML-related work, we regularly design and publish highly visual, interactive online essays, called &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, that provide accessible, hands-on ways to learn about key ideas in ML. For example, we recently published new AI Explorables on the topics of model confidence and unintended biases. In our latest Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” we discuss the problem with model confidence: models can sometimes be &lt;em>;very&lt;/em>; confident in their predictions… and yet completely incorrect. Why does this happen and what can be done about it? Our Explorable walks through these issues with interactive examples and shows how we can build models that have more appropriate confidence in their predictions by using a technique called &lt;a href=&quot;https://ai.googleblog.com/2021/11/model-ensembles-are-faster-than-you.html&quot;>;ensembling&lt;/a>;, which works by averaging the outputs of multiple models. Another Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/saliency/&quot;>;Searching for Unintended Biases with Saliency&lt;/a>;”, shows how spurious correlations can lead to unintended biases — and how techniques such as saliency maps can detect some biases in datasets, with the caveat that it can be difficult to see bias when it&#39;s more subtle and sporadic in a training set. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;543&quot; data-original-width=&quot;724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR designs and publishes AI Explorables, interactive essays on timely topics and new methods in ML research, such as “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” which looks at how and why models offer incorrect predictions with high confidence, and how “ensembling” the outputs of many models can help avoid this.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Transparency and the Data Cards Playbook&lt;/h2>; &lt;p>; Continuing to advance our goal of helping people to understand ML, we promote transparent documentation. In the past, PAIR and Google Cloud developed &lt;a href=&quot;http://modelcards.withgoogle.com&quot;>;model cards&lt;/a>;. Most recently, we presented &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3531146.3533231&quot;>;our work on Data Cards&lt;/a>; at &lt;a href=&quot;https://facctconference.org/2022/&quot;>;ACM FAccT&#39;22&lt;/a>; and open-sourced the &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;, a joint effort with the &lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;Technology, AI, Society, and Culture team&lt;/a>; (TASC). &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;The Data Cards Playbook&lt;/a>; is a toolkit of participatory activities and frameworks to help teams and organizations overcome obstacles when setting up a transparency effort. It was created using an iterative, multidisciplinary approach rooted in the experiences of over 20 teams at Google, and comes with four modules: Ask, Inspect, Answer and Audit. These modules contain a variety of resources that can help you customize Data Cards to your organization&#39;s needs: &lt;/p>; &lt;ul>; &lt;li>;18 Foundations: Scalable frameworks that anyone can use on any dataset type &lt;/li>;&lt;li>;19 Transparency Patterns: Evidence-based guidance to produce high-quality Data Cards at scale &lt;/li>;&lt;li>;33 Participatory Activities: Cross-functional workshops to navigate transparency challenges for teams &lt;/li>;&lt;li>;Interactive Lab: Generate interactive Data Cards from markdown in the browser &lt;/li>; &lt;/ul>; &lt;p>; The Data Cards Playbook is accessible as a learning pathway for startups, universities, and other research groups. &lt;/p>; &lt;br />; &lt;h2>;Software Tools&lt;/h2>; &lt;p>; Our team thrives on creating tools, toolkits, libraries, and visualizations that expand access and improve understanding of ML models. One such resource is &lt;a href=&quot;https://knowyourdata.withgoogle.com/&quot;>;Know Your Data&lt;/a>;, which allows researchers to test a model&#39;s performance for various scenarios through interactive qualitative exploration of datasets that they can use to find and fix unintended dataset biases. &lt;/p>; &lt;p>; Recently, PAIR released a new version of the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT) for model debugging and understanding. LIT v0.5 provides support for image and tabular data, new interpreters for tabular feature attribution, a &quot;Dive&quot; visualization for faceted data exploration, and performance improvements that allow LIT to scale to 100k dataset entries. You can find the &lt;a href=&quot;https://github.com/PAIR-code/lit/blob/main/RELEASE.md&quot;>;release notes&lt;/a>; and &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;code&lt;/a>; on GitHub. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s1920/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR&#39;s &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT), an open-source platform for visualization and understanding of ML models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; PAIR has also contributed to &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a tool for rapid prototyping with LLMs using prompt programming. MakerSuite builds on our earlier research on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;PromptMaker&lt;/a>;, which won an honorable mention at &lt;a href=&quot;https://chi2022.acm.org/&quot;>;CHI 2022. &lt;/a>;MakerSuite lowers the barrier to prototyping ML applications by broadening the types of people who can author these prototypes and by shortening the time spent prototyping models from months to minutes.&amp;nbsp;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s1216/reversedictionary.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;918&quot; data-original-width=&quot;1216&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s16000/reversedictionary.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A screenshot of MakerSuite, a tool for rapidly prototyping new ML models using prompt-based programming, which grew out of PAIR&#39;s prompt programming research.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Ongoing work&lt;/h2>; &lt;p>; As the world of AI moves quickly ahead, PAIR is excited to continue to develop new tools, research, and educational materials to help change the way people think about what THEY can do with AI. &lt;/p>; &lt;p>; For example, we recently conducted &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;an exploratory study&lt;/a>; with five designers (presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI&lt;/a>; this year) that looks at how people with no ML programming experience or training can use prompt programming to quickly prototype functional user interface mock-ups. This prototyping speed can help inform designers on how to integrate ML models into products, and enables them to conduct user research sooner in the product design process. &lt;/p>; &lt;p>; Based on this study, PAIR&#39;s researchers built &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser&lt;/a>;, a design tool plugin for authoring LLM-infused mock-ups. The plug-in introduces two novel LLM-interactions: input-output, which makes content interactive and dynamic, and frame-change, which directs users to different frames depending on their natural language input. The result is more tightly integrated UI and ML prototyping, all within a single interface. &lt;/p>; &lt;p>; Recent advances in AI represent a significant shift in how easy it is for researchers to customize and control models for their research objectives and goals.These capabilities are transforming the way we think about interacting with AI, and they create lots of new opportunities for the research community. PAIR is excited about how we can leverage these capabilities to make AI easier to use for more people. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Thanks to everyone in PAIR, to Reena Jana and to all of our collaborators. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2749680625311121514/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: PAIR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2478354033845809100&lt;/id>;&lt;published>;2023-05-16T12:22:00.001-07:00&lt;/published>;&lt;updated>;2023-05-16T12:23:42.794-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using reinforcement learning for dynamic planning in open-ended conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Deborah Cohen, Staff Research Scientist, and Craig Boutilier, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s1650/rlxtalk.png&quot; style=&quot;display: none;&quot; />; &lt;p>; As virtual assistants become ubiquitous, users increasingly interact with them to learn about new topics or obtain recommendations and expect them to deliver capabilities beyond narrow dialogues of one or two turns. Dynamic planning, namely the capability to look ahead and replan based on the flow of the conversation, is an essential ingredient for the making of engaging conversations with the deeper, open-ended interactions that users expect. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While large language models (LLMs) are now beating state-of-the-art approaches in many natural language processing benchmarks, they are typically trained to output the next best response, rather than planning ahead, which is required for multi-turn interactions. However, in the past few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;>;reinforcement learning&lt;/a>; (RL) has delivered incredible results addressing specific problems that involve dynamic planning, such as winning games and protein folding. &lt;/p>; &lt;p>; Today, we are sharing our recent advances in &lt;a href=&quot;https://arxiv.org/abs/2208.02294&quot;>;dynamic planning for human-to-assistant conversations&lt;/a>;, in which we enable an assistant to plan a multi-turn conversation towards a goal and adapt that plan in real-time by adopting an RL-based approach. Here we look at how to improve long interactions by applying RL to compose answers based on information extracted from reputable sources, rather than relying on content generated by a language model. We expect that future versions of this work could combine LLMs and RL in multi-turn dialogues. The deployment of RL “in the wild” in a large-scale dialogue system proved a formidable challenge due to the modeling complexity, tremendously large state and action spaces, and significant subtlety in designing reward functions. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What is dynamic planning?&lt;/h2>; &lt;p>; Many types of conversations, from gathering information to offering recommendations, require a flexible approach and the ability to modify the original plan for the conversation based on its flow. This ability to shift gears in the middle of a conversation is known as &lt;em>;dynamic planning&lt;/em>;, as opposed to &lt;em>;static planning&lt;/em>;, which refers to a more fixed approach. In the conversation below, for example, the goal is to engage the user by sharing interesting facts about cool animals. To begin, the assistant steers the conversation to sharks via a sound quiz. Given the user&#39;s lack of interest in sharks, the assistant then develops an updated plan and pivots the conversation to sea lions, lions, and then cheetahs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;908&quot; data-original-width=&quot;890&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The assistant dynamically modifies its original plan to talk about sharks and shares facts about other animals.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic composition&lt;/h2>; &lt;p>; To cope with the challenge of conversational exploration, we separate the generation of assistant responses into two parts: 1) &lt;em>;content generation&lt;/em>;, which extracts relevant information from reputable sources,&lt;em>; &lt;/em>;and 2) &lt;em>;flexible composition&lt;/em>; of such content into assistant responses. We refer to this two-part approach as &lt;em>;&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;dynamic composition&lt;/a>;&lt;/em>;. Unlike LLM methods, this approach gives the assistant the ability to fully control the source, correctness, and quality of the content that it may offer. At the same time, it can achieve flexibility via a learned dialogue manager that selects and combines the most appropriate content. &lt;/p>; &lt;p>; In an earlier paper, “&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;Dynamic Composition for Conversational Domain Exploration&lt;/a>;”, we describe a novel approach which consists of: (1) a collection of content providers, which offer candidates from different sources, such as news snippets, &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; facts, and questions; (2) a dialogue manager; and (3) a sentence fusion module. Each assistant response is incrementally constructed by the dialogue manager, which selects candidates proposed by the content providers. The selected sequence of utterances is then fused into a cohesive response. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic planning using RL&lt;/h2>; &lt;p>; At the core of the assistant response composition loop is a dialogue manager trained using &lt;em>;off-policy RL&lt;/em>;, namely an algorithm that evaluates and improves a policy that is different from the policy used by the agent (in our case, the latter is based on a supervised model). Applying RL to dialogue management presents several challenges, including a large state space (as the state represents the conversation state, which needs to account for the whole conversation history) and an effectively unbounded action space (that may include all existing words or sentences in natural language). &lt;/p>; &lt;p>; We address these challenges using a novel RL construction. First, we leverage powerful supervised models — specifically, &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;>;recurrent neural networks&lt;/a>; (RNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; — to provide a succinct and effective dialogue state representation. These state encoders are fed with the dialogue history, composed of a sequence of user and assistant turns, and output a representation of the dialogue state in the form of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_space&quot;>;latent vector&lt;/a>;. &lt;/p>; &lt;p>; Second, we use the fact that a relatively small set of reasonable candidate utterances or actions can be generated by content providers at each conversation turn, and limit the action space to these. Whereas the action space is typically fixed in RL settings, because all states share the same action space, ours is a non-standard space in which the candidate actions may differ with each state, since content providers generate different actions depending on the dialogue context. This puts us in the realm of stochastic action sets, a framework that formalizes cases where the set of actions available in each state is governed by an exogenous stochastic process, which we address using &lt;a href=&quot;https://www.ijcai.org/proceedings/2018/0650.pdf&quot;>;Stochastic Action Q-Learning&lt;/a>;, a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Q-learning&quot;>;Q-learning&lt;/a>; approach. Q-learning is a popular off-policy RL algorithm, which does not require a model of the environment to evaluate and improve the policy. We trained our model on a corpus of crowd-compute–rated conversations obtained using a supervised dialogue manager. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s1500/image4.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given the current dialogue history and a new user query, content providers generate candidates from which the assistant selects one. This process runs in a loop, and at the end the selected utterances are fused into a cohesive response.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Reinforcement learning model evaluation&lt;/h2>; &lt;p>; We compared our RL dialogue manager with a launched supervised transformer model in an experiment using Google Assistant, which conversed with users about animals. A conversation starts when a user triggers the experience by asking an animal-related query (eg, “How does a lion sound?”). The experiment was conducted using an &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B testing&lt;/a>; protocol, in which a small percentage of Assistant users were randomly sampled to interact with our RL-based assistant while other users interacted with the standard assistant. &lt;/p>; &lt;p>; We found that the RL dialogue manager conducts longer, more engaging conversations. It increases conversation length by 30% while improving user engagement metrics. We see an increase of 8% in cooperative responses to the assistant&#39;s questions — eg, “Tell me about lions,” in response to “Which animal do you want to hear about next?” Although there is also a large increase in nominally “non-cooperative” responses (eg, “No,” as a reply to a question proposing additional content, such as “Do you want to hear more?”), this is expected as the RL agent takes more risks by asking pivoting questions. While a user may not be interested in the conversational direction proposed by the assistant (eg, pivoting to another animal), the user will often continue to engage in a dialogue about animals. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s552/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;534&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;From the non-cooperative user response in the 3rd turn (“No.”) and the query “Make a dog sound,” in the 5th turn, the assistant recognizes that the user is mostly interested in animal sounds and modifies its plan, providing sounds and sound quizzes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In addition, some user queries contain explicit positive (eg, “Thank you, Google,” or “I&#39;m happy.”) or negative (eg, “Shut up,” or “Stop.”) feedback. While an order of magnitude fewer than other queries, they offer a direct measure of user (dis)satisfaction. The RL model increases explicit positive feedback by 32% and reduces negative feedback by 18%. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Learned dynamic planning characteristics and strategies&lt;/h2>; &lt;p>; We observe several characteristics of the (unseen) RL plan to improve user engagement while conducting longer conversations. First, the RL-based assistant ends 20% more turns in questions, prompting the user to choose additional content. It also better harnesses content diversity, including facts, sounds, quizzes, yes/no questions, open questions, etc. On average, the RL assistant uses 26% more distinct content providers per conversation than the supervised model. &lt;/p>; &lt;p>; Two observed RL planning strategies are related to the existence of sub-dialogues with different characteristics. Sub-dialogues about animal sounds are poorer in content and exhibit entity pivoting at every turn (ie, after playing the sound of a given animal, we can either suggest the sound of a different animal or quiz the user about other animal sounds). In contrast, sub-dialogues involving animal facts typically contain richer content and have greater conversation depth. We observe that RL favors the richer experience of the latter, selecting 31% more fact-related content. Lastly, when restricting analysis to fact-related dialogues, the RL assistant exhibits 60% more focus-pivoting turns, that is, conversational turns that change the focus of the dialogue. &lt;/p>; &lt;p>; Below, we show two example conversations, one conducted by the supervised model (left) and the second by the RL model (right), in which the first three user turns are identical. With a supervised dialogue manager, after the user declined to hear about “today&#39;s animal”, the assistant pivots back to animal sounds to maximize the immediate user satisfaction. While the conversation conducted by the RL model begins identically, it exhibits a different planning strategy to optimize the overall user engagement, introducing more diverse content, such as fun facts. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s1539/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1539&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the left conversation, conducted by the supervised model, the assistant maximizes the immediate user satisfaction. The right conversation, conducted by the RL model, shows different planning strategies to optimize the overall user engagement.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future research and challenges&lt;/h2>; &lt;p>; In the past few years, LLMs trained for language understanding and generation have demonstrated impressive results across multiple tasks, including dialogue. We are now exploring the use of an RL framework to empower LLMs with the capability of dynamic planning so that they can dynamically plan ahead and delight users with a more engaging experience. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias, Idan Szpektor and Gal Elidan. We would like to thank: Roee Aharoni, Moran Ambar, John Anderson, Ido Cohn, Mohammad Ghavamzadeh, Lotem Golany, Ziv Hodak, Adva Levin, Fernando Pereira, Shimi Salant, Shachar Shimoni, Ronit Slyper, Ariel Stolovich, Hagai Taitelbaum, Noam Velan, Avital Zipori and the CrowdCompute team led by Ashwin Kakarla. We thank Sophie Allweis for her feedback on this blogpost and Tom Small for the visualization.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2478354033845809100/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Using reinforcement learning for dynamic planning in open-ended conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s72-c/rlxtalk.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-14165165832846745&lt;/id>;&lt;published>;2023-05-15T13:59:00.001-07:00&lt;/published>;&lt;updated>;2023-05-15T14:40:42.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Larger language models do in-context learning differently&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s800/SULICL.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There have recently been tremendous advances in language models, partly because they can perform tasks with strong performance via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>; (ICL), a process whereby models are prompted with a few examples of input-label pairs before performing the task on an unseen evaluation example. In general, models&#39; success at in-context learning is enabled by: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;ul>; &lt;li>;Their use of semantic prior knowledge from pre-training to predict labels while following the format of in-context examples (eg, seeing examples of movie reviews with “positive sentiment” and “negative sentiment” as labels and performing &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>; using prior knowledge). &lt;/li>;&lt;li>;Learning the input-label mappings in context from the presented examples (eg, finding a pattern that positive reviews should be mapped to one label, and negative reviews should be mapped to a different label). &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.03846&quot;>;Larger language models do in-context learning differently&lt;/a>;”, we aim to learn about how these two factors (semantic priors and input-label mappings) interact with each other in ICL settings, especially with respect to the scale of the language model that&#39;s used. We investigate two settings to study these two factors — ICL with flipped labels (flipped-label ICL) and ICL with semantically-unrelated labels (SUL-ICL). In flipped-label ICL, labels of in-context examples are flipped so that semantic priors and input-label mappings disagree with each other. In SUL-ICL, labels of in-context examples are replaced with words that are semantically unrelated to the task presented in-context. We found that overriding prior knowledge is an emergent ability of model scale, as is the ability to learn in-context with semantically-unrelated labels. We also found that instruction tuning strengthens the use of prior knowledge more than it increases the capacity to learn input-label mappings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s625/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;625&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of flipped-label ICL and semantically-unrelated label ICL (SUL-ICL), compared with regular ICL, for a sentiment analysis task. Flipped-label ICL uses flipped labels, forcing the model to override semantic priors in order to follow the in-context examples. SUL-ICL uses labels that are not semantically related to the task, which means that models must learn input-label mappings in order to perform the task because they can no longer rely on the semantics of natural language labels.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiment design&lt;/h2>; &lt;p>; For a diverse dataset mixture, we experiment on seven &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) tasks that have been widely used: &lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;sentiment analysis&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/SetFit/subj&quot;>;subjective/objective classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/trec&quot;>;question classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/glue/viewer/qqp/validation&quot;>;duplicated-question recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/super_glue/viewer/rte/test&quot;>;entailment recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/financial_phrasebank&quot;>;financial sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://huggingface.co/datasets/ethos&quot;>;hate speech detection&lt;/a>;. We test five language model families, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;>;InstructGPT&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In this experiment, labels of in-context examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override their priors. In this setting, models that are able to override prior knowledge and learn input-label mappings in-context should experience a decrease in performance (since ground-truth evaluation labels are not flipped). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s1036/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1036&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The ability to override semantic priors when presented with flipped in-context example labels emerges with model scale. Smaller models cannot flip predictions to follow flipped labels (performance only decreases slightly), while larger models can do so (performance decreases to well below 50%).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that when no labels are flipped, larger models have better performance than smaller models (as expected). But when we flip more and more labels, the performance of small models stays relatively flat, but large models experience large performance drops to well-below random guessing (eg, 90% → 22.5% for code-davinci-002). &lt;/p>; &lt;p>; These results indicate that large models can override prior knowledge from pre-training when contradicting input-label mappings are presented in-context. Small models can&#39;t do this, making this ability an emergent phenomena of model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Semantically-unrelated labels&lt;/h2>; &lt;p>; In this experiment, we replace labels with semantically-irrelevant ones (eg, for sentiment analysis, we use “foo/bar” instead of “negative/positive”), which means that the model can only perform ICL by learning from input-label mappings. If a model mostly relies on prior knowledge for ICL, then its performance should decrease after this change since it will no longer be able to use semantic meanings of labels to make predictions. A model that can learn input–label mappings in-context, on the other hand, would be able to learn these semantically-unrelated mappings and should not experience a major drop in performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s1049/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;1049&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Small models rely more on semantic priors than large models do, as indicated by the greater decrease in performance for small models than for large models when using semantically-unrelated labels (ie, targets) instead of natural language labels. For each plot, models are shown in order of increasing model size (eg, for GPT-3 models, &lt;em>;a&lt;/em>; is smaller than &lt;em>;b&lt;/em>;, which is smaller than &lt;em>;c&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Indeed, we see that using semantically-unrelated labels results in a greater performance drop for small models. This suggests that smaller models primarily rely on their semantic priors for ICL rather than learning from the presented input-label mappings. Large models, on the other hand, have the ability to learn input-label mappings in-context when the semantic nature of labels is removed. &lt;/p>; &lt;p>; We also find that including more in-context examples (ie, exemplars) results in a greater performance improvement for large models than it does for small models, indicating that large models are better at learning from in-context examples than small models are. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s825/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;417&quot; data-original-width=&quot;825&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the SUL-ICL setup, larger models benefit more from additional examples than smaller models do.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Instruction tuning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a popular technique for improving model performance, which involves tuning models on various NLP tasks that are phrased as instructions (eg, “Question: What is the sentiment of the following sentence, &#39;This movie is great.&#39; Answer: Positive”). Since the process uses natural language labels, however, an open question is whether it improves the ability to learn input-label mappings or whether it strengthens the ability to recognize and apply semantic prior knowledge. Both of these would lead to an improvement in performance on standard ICL tasks, so it&#39;s unclear which of these occur. &lt;/p>; &lt;p>; We study this question by running the same two setups as before, only this time we focus on comparing standard language models (specifically, PaLM) with their instruction-tuned variants (Flan-PaLM). &lt;/p>; &lt;p>; First, we find that Flan-PaLM is better than PaLM when we use semantically-unrelated labels. This effect is very prominent in small models, as Flan-PaLM-8B outperforms PaLM-8B by 9.6% and almost catches up to PaLM-62B. This trend suggests that instruction tuning strengthens the ability to learn input-label mappings, which isn&#39;t particularly surprising. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s573/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;301&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned language models are better at learning input–label mappings than pre-training–only language models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More interestingly, we saw that Flan-PaLM is actually worse than PaLM at following flipped labels, meaning that the instruction tuned models were unable to override their prior knowledge (Flan-PaLM models don&#39;t reach below random guessing with 100% flipped labels, but PaLM models without instruction tuning can reach 31% accuracy in the same setting). These results indicate that instruction tuning must increase the extent to which models rely on semantic priors when they&#39;re available. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s1016/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;1016&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned models are worse than pre-training–only models at learning to override semantic priors when presented with flipped labels in-context.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Combined with the previous result, we conclude that although instruction tuning improves the ability to learn input-label mappings, it strengthens the usage of semantic prior knowledge more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We examined the extent to which language models learn in-context by utilizing prior knowledge learned during pre-training versus input-label mappings presented in-context. &lt;/p>; &lt;p>; We first showed that large language models can learn to override prior knowledge when presented with enough flipped labels, and that this ability emerges with model scale. We then found that successfully doing ICL using semantically-unrelated labels is another emergent ability of model scale. Finally, we analyzed instruction-tuned language models and saw that instruction tuning improves the capacity to learn input-label mappings but also strengthens the use of semantic prior knowledge even more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; These results underscore how the ICL behavior of language models can change depending on their scale, and that larger language models have an emergent ability to map inputs to many types of labels, a form of reasoning in which input-label mappings can potentially be learned for arbitrary symbols. Future research could help provide insights on why these phenomena occur with respect to model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. We would like to thank Sewon Min and our fellow collaborators at Google Research for their advice and helpful discussions.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/14165165832846745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Larger language models do in-context learning differently&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s72-c/SULICL.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;