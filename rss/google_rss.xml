<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com,1999:blog-8474926331452026626</id><updated> 2023-05-26T12:09:36.572-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Publications"></category><category term="Research"></category><category term="TensorFlow"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="conference"></category><category term="Education"></category><category term="conferences"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="NLP"></category><category term="CVPR"></category><category term="Algorithms"></category><category term="Quantum Computing"></category><category term="Research Awards"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="Multimodal Learning"></category><category term="Computer Science"></category><category term="On-device Learning"></category><category term="MOOC"></category><category term="Security and Privacy"></category><category term="HCI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="AI for Social Good"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Visualization"></category><category term="YouTube"></category><category term="Self-Supervised Learning"></category><category term="AutoML"></category><category term="Hardware"></category><category term="Quantum AI"></category><category term="NeurIPS"></category><category term="accessibility"></category><category term="optimization"></category><category term="Audio"></category><category term="ACL"></category><category term="Android"></category><category term="Awards"></category><category term="Structured Data"></category><category term="TPU"></category><category term="EMNLP"></category><category term="ICML"></category><category term="Information Retrieval"></category><category term="ML"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Image Processing"></category><category term="Search"></category><category term="TTS"></category><category term="User Experience"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="distributed systems"></category><category term="video"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="DeepMind"></category><category term="Earth Engine"></category><category term="Google Maps"></category><category term="K-12"></category><category term="Responsible AI"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Chemistry"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="Interspeech"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Acoustic Modeling"></category><category term="Faculty Summit"></category><category term="ICCV"></category><category term="Semantic Models"></category><category term="Systems"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Augmented Reality"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Recommender Systems"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Social Networks"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="RAI-HCT Highlights"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="Android Wear"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="Differential Privacy"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新消息。&lt;/substitle>;&lt;link href=&quot;http://ai.googleblog.com/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1234&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4195590947404721374&lt;/id>;&lt;published>;2023-05-26T12:08:00.002-07:00&lt;/published>;&lt;updated>;2023-05- 26T12:09:00.761-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;用于推理的基础模型图表&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google Research 研究软件工程师 Julian Eisenschlos 发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTXbEXxSohAub jvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s2200/MatCha.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 视觉语言是一种交流形式，它依靠文本之外的图形符号来传达信息。它以图标、信息图、表格、绘图和图表的形式在我们的数字生活中无处不在，并以路牌、漫画书、食品标签等形式延伸到现实世界。因此，让计算机更好地理解这种类型媒体可以帮助科学交流和发现、可访问性和数据透明度。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 自 &lt;a href=&quot;https://ieeeexplore.ieee.org 出现以来，计算机视觉模型使用基于学习的解决方案取得了巨大进步/document/5206848&quot;>;ImageNet&lt;/a>;，重点一直放在自然图像上，其中有各种任务，例如&lt;a href=&quot;https://www.image-net.org/&quot;>;分类&lt;/ a>;、&lt;a href=&quot;https://visualqa.org/&quot;>;视觉问答&lt;/a>; (VQA)、&lt;a href=&quot;https://cocodataset.org&quot;>;字幕、检测和分割&lt;/ a>;，已经被定义、研究并在某些情况下被改进以达到人类的表现。然而，视觉语言并没有获得同等程度的关注，这可能是因为该领域缺乏大规模的训练集。但在过去几年中，已经创建了新的学术数据集，其目标是评估视觉语言图像上的问答系统，例如 &lt;a href=&quot;https://arxiv.org/abs/1909.00997&quot;>;PlotQA&lt;/a>; 、&lt;a href=&quot;https://arxiv.org/abs/2104.12756&quot;>;InfographicsVQA&lt;/a>; 和 &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA &lt;/a>;。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6a Q6tqQsTgI2X3TkQUK_xIK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s1999/image5.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;898&quot; data-original-width=&quot;1999&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJN-BR8Zy4_5B10U4EeSFd9fU7jfvw_QeA1YvEzRK7NDt8oaGEqU1JesVnP2SIsaPbglFE1Qku8c6aQ6tqQsTgI2X3TkQUK_x IK8Pck7-8zNrbXShLr1Z44QAfBd1uVFVeuvR4f6Iuy_7r-bZoQwnj4xI2_iFhTg4XDEfvQMdgKydPrGfOfWMPMldBtg/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;ChartQA&lt;/a>; 示例。回答问题需要阅读信息并计算和与差。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 为这些任务构建的现有模型依赖于集成 &lt;a href=&quot;https: //en.wikipedia.org/wiki/Optical_character_recognition&quot;>;光学字符识别&lt;/a>; (OCR) 信息及其坐标进入更大的管道，但该过程容易出错、速度慢且泛化能力差。这些方法的流行是因为现有的端到端计算机视觉模型基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;卷积神经网络&lt;/a>; (CNN) 或 &lt;在自然图像上预先训练的 href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; 无法轻易适应视觉语言。但现有模型在回答图表问题时准备不足，包括阅读条形图的相对高度或饼图中切片的角度、理解轴刻度、正确映射象形图及其图例值、颜色、大小和纹理，最后对提取的数字进行数值运算。 &lt;/p>; &lt;p>; 鉴于这些挑战，我们提出“&lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha：通过数学推理和图表去渲染增强视觉语言预训练&lt;/a>; ”。 MatCha 代表数学和图表，是一种像素到文本的基础模型（一种预训练模型，具有内置的归纳偏差，可以针对多种应用进行微调），在两个互补任务上进行训练：(a) 图表去渲染和（b）数学推理。在图表反渲染中，给定一个绘图或图表，图像到文本模型需要生成其底层数据表或用于渲染它的代码。对于数学推理预训练，我们选择文本数字推理数据集并将输入渲染为图像，图像到文本模型需要对其进行解码以获得答案。我们还提出了“&lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot：通过绘图到表格翻译的一次性视觉语言推理&lt;/a>;”，一个建立在 MatCha 之上的模型通过转换为表格在图表上进行一次性推理。通过这些方法，我们超越了 ChartQA 之前的最先进技术 20% 以上，并且与具有 1000 倍以上参数的最佳摘要系统相匹配。这两篇论文都将在 &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>; 上发表。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;图表反渲染&lt;/h2>; &lt;p>; 图表通常由底层数据表生成和一段代码。代码定义图形的总体布局（例如，类型、方向、颜色/形状方案），基础数据表建立实际数字及其分组。数据和代码都被发送到编译器/渲染引擎以创建最终图像。要理解图表，需要发现图像中的视觉模式，并有效地解析和分组它们以提取关键信息。逆转情节渲染过程需要所有这些能力，因此可以作为理想的预训练任务。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buEx Ip0qziz1AwUXf6ukGUM-_mufPWWpuEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s624/image2.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;335&quot; data-original-width=&quot;624&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0uJvVVTEGwHvrPqrbWpHw5YPJGU41c8zdH5FvVRUGnUqSSYn7Un3BFnOaDCBglhNEzbk5xeUzmBDCS8buExIp0qziz1AwUXf6ukGUM-_mufPWW puEMD89LWUrx1XxVJ8o0lJBpJ8503WyLqMuSG8iBvddycooDvlzVSXkjwuOygW21Xmzf55sskzk_g/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;使用随机绘图选项根据&lt;a href=&quot;https://en.wikipedia.org/wiki/Airbus_A380&quot;>;Airbus A380 维基百科页面&lt;/a>; 中的表格创建的图表. MatCha 的预训练任务包括从图像中恢复源表或源代码。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在实践中，同时获取图表具有挑战性，它们的底层数据表，以及它们的渲染代码。为了收集足够的预训练数据，我们独立地积累了 [chart, code] 和 [chart, table] 对。对于 [图表、代码]，我们使用适当的许可证抓取所有 GitHub IPython 笔记本并提取带有数字的块。图形和它之前的代码块被保存为 [图表，代码] 对。对于 [图表、表格] 对，我们探索了两个来源。对于第一个来源，即合成数据，我们手动编写代码以从 &lt;a href=&quot;https://aclanthology.org/2020.acl-main.398/&quot;>;TaPas&lt;/a>; 代码库转换网络抓取的维基百科表格到图表。我们根据列类型从多个绘图选项中抽取并组合了这些选项。此外，我们还添加了 PlotQA 中生成的 [chart, table] 对，以丰富预训练语料库。第二个来源是网络抓取的 [图表、表格] 对。我们直接使用在 ChartQA 训练集中抓取的 [chart, table] 对，总共包含来自四个网站的大约 20k 对：&lt;a href=&quot;statista.com&quot;>;Statista&lt;/a>;，&lt;a href=&quot;https: //www.pewresearch.org/&quot;>;皮尤研究中心&lt;/a>;，&lt;a href=&quot;https://ourworldindata.org/&quot;>;我们的数据世界&lt;/a>;，以及&lt;a href=&quot;https:// www.oecd.org/&quot;>;经合组织&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;数学推理&lt;/h2>; &lt;p>; 我们通过学习数学推理技能，将数字推理知识融入MatCha文本数学数据集。我们使用两个现有的文本数学推理数据集，&lt;a href=&quot;https://openreview.net/forum?id=H1gR5iR5FX&quot;>;MATH&lt;/a>; 和 &lt;a href=&quot;https://aclanthology.org/N19- 1246/&quot;>;DROP&lt;/a>; 用于预训练。 MATH 是综合创建的，每个模块（类型）的问题包含 200 万个训练示例。 DROP 是一个阅读理解式的 QA 数据集，其中输入是段落上下文和问题。 &lt;/p>; &lt;p>; 为了解决DROP中的问题，模型需要阅读段落，提取相关数字并进行数值计算。我们发现这两个数据集是互补的。 MATH 包含大量不同类别的问题，这有助于我们识别显式注入模型所需的数学运算。 DROP 的阅读理解格式类似于典型的 QA 格式，其中模型同时执行信息提取和推理。在实践中，我们将两个数据集的输入渲染成图像。该模型被训练来解码答案。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVv gENyKKMTeSXjB39KlsOwC8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s624/image1.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;218&quot; data-original-width=&quot;624&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyaSRY-BeSZ0qxOy3DqRqChADOFXkvYd5iI-sich3d5CZ7qSUh4gdGNoKj-XzCoOWCcGkTPOjlWPTUyrjFJVvgENyKKMTeSXjB39KlsOwC 8lJJKHe_bNhjmPk6ewNJWObwENwPiyy_cCFK529eCRxqpXOeStkf2KlLLsn9zALUbJIBiOsTlwK06HlvNw/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;为了提高 MatCha 的数学推理能力，我们通过将输入文本渲染为图像，将 MATH 和 DROP 中的示例合并到预训练目标中。&lt;/ td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;端到端结果&lt;/h2>; &lt;p >; 我们使用 &lt;a href=&quot;https://arxiv.org/abs/2210.03347&quot;>;Pix2Struct&lt;/a>; 模型主干，这是一种为网站理解量身定制的图像到文本转换器，并使用上面描述的两个任务。我们通过在几个视觉语言任务上对其进行微调来展示 MatCha 的优势——这些任务涉及用于问答和总结的图表和绘图，在这些任务中无法访问基础表格。 MatCha 大大超越了之前模型的性能，也优于之前假设访问底层表的最新技术水平。 &lt;/p>; &lt;p>; 在下图中，我们首先评估了两个包含来自&lt;a href=&quot;https://aclanthology.org/2022.findings-acl.177/&quot;>;OCR 管道&lt;/的信息的基线模型a>;，直到最近它还是处理图表的标准方法。第一个基于 &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5&lt;/a>;，第二个基于 &lt;a href= “https://aclanthology.org/2022.findings-acl.177/”>;VisionTaPas&lt;/a>;。我们还与 &lt;a href=&quot;https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html&quot;>;PaLI-17B&lt;/a>; 进行了比较，这是一个大型（大约是其他模型的 1000 倍）图像加文本到文本转换器接受了一系列不同任务的训练，但阅读文本和其他形式的视觉语言的能力有限。最后，我们报告 Pix2Struct 和 MatCha 模型结果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6C ysizEpk4jccOI57U4hsxXQPO4aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s1646/image3.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;696&quot; data-original-width=&quot;1646&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2FufIovBgB3bz0ve8Zh3vVwONF4gkTNXQjPUX8wmYYS8N3M2opoQcn9aM5d8pjCNkF7puXds_qesakd6CysizEpk4jccOI57U4hsxXQPO4 aR8ehU1MvQCiFr3rrcViJJTXu8o1aElkMuw5VBKi3R6OVoRHt25SNP-pPrxkPZv_36kEQDCimsvFek3zQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;在两个图表 QA 基准 ChartQA &amp;amp; 上的实验结果PlotQA（使用宽松的准确性）和图表摘要基准图表到文本（使用 BLEU4）。与更大的模型相比，Matcha 在 QA 方面大大超越了现有技术水平，并且在总结方面与这些更大的模型相匹配。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 对于 QA 数据集，我们使用&lt;a href=&quot;https://github.com/google-research/pix2struct/blob/main/pix2struct/metrics.py#L81&quot;>;官方宽松的准确性指标&lt;/a>;，它允许在数字输出。对于图表到文本的摘要，我们报告 &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; 分数。与问答的基线相比，MatCha 取得了显着改善的结果，并且在摘要方面获得了与 PaLI 相当的结果，其中大尺寸和广泛的长文本/字幕生成预训练有利于这种长文本生成。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;去渲染加上大型语言模型链&lt;/h2>; &lt;p>; 虽然它们的参数数量非常高效，特别是在提取任务上，我们观察到经过微调的 MatCha 模型仍然可能难以处理端到端的复杂推理（例如，涉及大量或多个步骤的数学运算）。因此，我们还提出了一种两步法来解决这个问题：1) 模型读取图表，然后输出基础表，2) 大型语言模型 (LLM) 读取此输出，然后尝试仅基于文本输入。 &lt;/p>; &lt;p>; 对于第一个模型，我们仅在图表到表格任务上对 MatCha 进行了微调，增加了输出序列长度以保证它可以恢复图表中的全部或大部分信息。 &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; 是生成的模型。在第二阶段，任何 LLM（例如 &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; 或 &lt;a href=&quot;https://arxiv.org/abs/2107.03374 &quot;>;Codex&lt;/a>;) 可以用于该任务，我们可以依靠标准方法来提高 LLM 的性能，例如 &lt;a href=&quot;https://ai.googleblog.com/2022/05/ language-models-perform-reasoning-via.html&quot;>;思想链&lt;/a>;和&lt;a href=&quot;https://arxiv.org/abs/2203.11171&quot;>;自洽&lt;/a>;。我们还试验了&lt;a href=&quot;https://arxiv.org/abs/2211.12588&quot;>;program-of-thoughts&lt;/a>;，其中模型生成可执行的 Python 代码以卸载复杂的计算。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0t IOhpTfNqEFNEwETF3cvm5LbVooJ2sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s622/image4.png&quot; imageanchor=&quot; 1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;622&quot; src=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilCkMJE3ftohAt9BeIoISbNLl6W8qXMizYxbR-P4gusArAZI0WYCZgT3z_GwDE9e54V8SE0Hxob1lDr0tIOhpTfNqEFNEwETF3cvm5LbVooJ2 sSFmx5QtLlVjNVjPT0n_WYk6d1gb1PKU87siVbNZFFs34UICnymyWxzkgtPt9HtRr7dLB_wa86wJNWA/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; 样式=&quot;text-align: center;&quot;>;DePlot+LLM 方法的图示。这是使用 &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;FlanPaLM&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt; 的真实示例/a>;。蓝色框是 LLM 的输入，红色框包含 LLM 生成的答案。我们在每个答案中强调了一些关键的推理步骤。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 如上例所示，结合 LLM 的 DePlot 模型优于微调模型很大的差距，尤其是在 ChartQA 的人工部分，问题更自然但需要更困难的推理。此外，DePlot+LLM 无需访问任何训练数据即可执行此操作。 &lt;/p>; &lt;p>; 我们已经在我们的 &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/deplot&quot;>;GitHub 存储库&lt;/a 中发布了新模型和代码>;，您可以在 colab 中自行试用。查看关于 &lt;a href=&quot;https://arxiv.org/abs/2212.09662&quot;>;MatCha&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2212.10505&quot;>;DePlot&lt;/a>; 的论文>; 有关实验结果的更多详细信息。我们希望我们的结果可以使研究界受益，并使每个人都可以更轻松地访问图表和绘图中的信息。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作由 Fangyu Liu、Julian Martin 完成Eisenschlos、Francesco Piccinno、Syrine Krichene、Chenxi Pang、Kenton Lee、Mandar Joshi、Wenhu Chen 和 Yasemin Altun，来自我们的&lt;a href=&quot;https://research.google/teams/language/&quot;>;语言团队&lt;/a>;方宇实习项目的一部分。来自剑桥的 Nigel Collier 也是合作者。我们要感谢 Joshua Howland、Alex Polozov、Shrestha Basu Mallick、Massimo Nicosia 和 William Cohen 提出的宝贵意见和建议。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai. googleblog.com/feeds/4195590947404721374/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023 /05/foundation-models-for-reasoning-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www. blogger.com/feeds/8474926331452026626/posts/default/4195590947404721374&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/4195590947404721374&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/foundation-models-for-reasoning-on. html&quot; rel=&quot;alternate&quot; title=&quot;图表推理的基础模型&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:/ /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifdjDjOARh4hb7ejkosuWwJDb1xEhPVkFiJQpa9Go1uhkRMy58esTSN9DXgWN5N74ZS5P4dyvG6FnZ_F-EzxNl0FcwFycDqEBOvabOoudTX bEXxSohAUbjvKYu_GKu3XNCumZLxBpzeUbjIZ1pgXtXZofVrBdR19g2dGBZ1gIYOCr6h9wYPQY-sXdhbQ/s72-c/MatCha.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3147300214577163215&lt;/id>;&lt;已发布>;2023-05-26T09:58:00.000-07:00&lt;/published>;&lt;更新>;2023-05-26T09:58:34.233-07:00&lt;/更新>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;硬件&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Barkour：用四足机器人对动物级敏捷性进行基准测试&lt;/stitle >;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google 研究科学家 Ken Caluwaerts 和 Atil Iscen 发表&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU 1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s320/barkour%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; 创造具有强大和动态运动能力的机器人，类似于动物或人类，一直是机器人社区的一个长期目标。除了快速高效地完成任务外，敏捷性还允许有腿机器人在&lt;a href=&quot;https://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot;>;复杂环境中移动&lt;/a>; 否则很难遍历。 Google 的研究人员&lt;a href=&quot;https://arxiv.org/abs/1804.10332&quot;>;多年&lt;/a>;一直在追求敏捷性，并跨越 &lt;a href=&quot;https://ai.googleblog.com/2022 /10/table-tennis-research-platform-for.html&quot;>;各种形状因素&lt;/a>;。然而，尽管研究人员已启用&lt;a href=&quot;https://www.science.org/doi/10.1126/scirobotics.abc5986&quot;>;机器人远足&lt;/a>;或&lt;a href=&quot;https://www.roboticsproceedings .org/rss11/p47.pdf&quot;>;jump over some obstacles&lt;/a>;，仍然没有普遍接受的综合衡量机器人敏捷性或机动性的基准。相比之下，基准测试是机器学习发展背后的推动力，例如用于计算机视觉的 &lt;a href=&quot;https://arxiv.org/abs/1409.0575&quot;>;ImageNet&lt;/a>; 和 &lt;a href=&quot;https ://github.com/openai/gym&quot;>;OpenAI Gym&lt;/a>; 用于强化学习 (RL)。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2305.14654&quot;>;Barkour：使用四足机器人对动物级敏捷性进行基准测试&lt; /a>;”，我们引入了四足机器人的&lt;em>;Barkour &lt;/em>;敏捷性基准，以及基于&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>; 的通才运动政策。受狗类敏捷比赛的启发，有腿机器人必须在有限的时间内依次展示各种技能，包括向不同方向移动、穿越崎岖不平的地形以及跳过障碍物才能成功完成基准测试。通过提供多样化且具有挑战性的障碍课程，Barkour 基准测试鼓励研究人员开发以可控和多功能方式快速移动的运动控制器。此外，通过将性能指标与真实狗的性能联系起来，我们提供了一个直观的指标来了解机器人相对于动物对应物的性能。 &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/ blob/main/zola_vs_ap.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; 类=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;我们邀请了一些&lt;a href=&quot;https://blog.google/inside-google/life-at-google/working-home-ruff-dooglers-make-it-little-better /&quot;>;dooglers&lt;/a>; 尝试障碍课程，以确保我们的敏捷目标是现实和具有挑战性的。小型狗在大约 10 秒内完成障碍训练，而我们的机器人的典型性能徘徊在 20 秒左右。&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40% ;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Barkour 基准&lt;/h2>; &lt;p>; Barkour 计分系统使用每个障碍物和基于小狗在 &lt;a href 中的目标速度的总体路线目标时间=&quot;https://images.akc.org/pdf/rulebooks/REAGIL.pdf&quot;>;新手敏捷竞赛&lt;/a>;（约 1.7m/s）。 Barkour 分数范围从 0 到 1，其中 1 对应于机器人在大约 10 秒的指定时间内成功穿越了路线上的所有障碍物，这是一只体型相似的狗穿越路线所需的平均时间。机器人会因跳过、越过障碍物或移动速度太慢而受到处罚。 &lt;/p>; &lt;p>; 我们的标准路线包括 5 米 x 5 米区域内的四个独特障碍。这是一个比典型的狗比赛更密集和更小的设置，以便在机器人实验室中轻松部署。从起始台开始，机器人需要穿过一组杆子，爬上 A 型架，完成 0.5 米的跳远，然后踏上终点台。我们之所以选择这部分障碍，是因为它们测试了多种技能，同时将设置保持在较小的空间内。与真正的狗敏捷比赛一样，Barkour 基准可以很容易地适应更大的球场区域，并且可以包含可变数量的障碍物和球场配置。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1 R5B1kBNkfBVCgUmaxa96F-2WyRk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s1193/image7 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;542&quot; data-original-width=&quot;1193&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1ExmuR_LT8UzVANv1nLsfQGLEA04KYuwTtxCeP0-UnCmIT1ID0tCrxcNqhR8qmCKbQIdCmaGlgtsmIgim1R5B1kBNkfBVCgUmaxa96F-2Wy Rk-hMnHlKPYBlRnZ8aT02xIGRweZGaRLjHMzQjS5QjX9erHh_h2IHtyVGywOfRw9J0UhHu6-1oWjgaAg/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Barkour 基准测试的障碍训练设置概览，包括编织杆、A 字架、跳远和暂停表。直观的计分机制受到狗的敏捷性比赛的启发，平衡了速度、敏捷性和性能，并且可以轻松修改以包含其他类型的障碍或路线配置。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;学习敏捷运动技能&lt;/h2>; &lt;p>; Barkour 基准测试具有多种障碍和延迟奖励系统，这在训练可以完成整个障碍课程的单一策略时提出了重大挑战。因此，为了设置强大的性能基准并展示机器人敏捷性研究基准的有效性，我们采用了学生-教师框架，结合了零样本模拟到真实的方法。首先，我们使用 on-policy RL 方法针对不同的障碍训练个人专业运动技能（教师）。特别是，我们在大规模 &lt;a href=&quot;https://arxiv.org/pdf/2108.10470. pdf&quot;>;并行模拟&lt;/a>; 为机器人配备个人技能，包括行走、爬坡和跳跃策略。 &lt;/p>; &lt;p>; 接下来，我们基于我们之前训练的专业技能，通过使用学生-教师框架来训练一个单一的策略（学生）来执行所有技能和之间的转换。我们使用模拟部署为每一种专业技能创建状态-动作对数据集。然后将该数据集提炼成一个单一的基于 Transformer 的通才运动策略，该策略可以处理各种地形并根据感知环境和机器人的状态调整机器人的步态。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSk Y33Lwtr0fJXe6tNskpT-uSgG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s1547/巴库%20training.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;296&quot; data-original-width=&quot;1547&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifG3krWfLwod1U-km9joIoiyxc_nUZo6Us0uv1dBTzvcb6Q_4Bz2fO2tYUz1v_CHmcda8wnnOHbpa3ZSkY33Lwtr0fJXe6tNskpT-uS gG8_vZSu-cxtUlLNd4M8QtIkSzhkFjaCkXLWjDJ_aWN67xIUJCBiQoMsV2-NvRIHtV7eeZWY19ppQ88qcp7Vg/s16000/Barkour%20training.gif”/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 在部署期间，我们将运动转换器策略配对能够使用导航控制器执行多种技能，导航控制器根据机器人的位置提供速度命令。 Our trained policy controls the robot based on the robot&#39;s surroundings represented as an elevation map, velocity commands, and on-board sensory information provided by the robot.&lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/Generalist.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Deployment pipeline for the locomotion transformer architecture. At deployment time, a high-level navigation controller guides the real robot through the obstacle course by sending commands to the locomotion transformer policy.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Robustness and repeatability are difficult to achieve when we aim for peak performance and maximum speed. Sometimes, the robot might fail when overcoming an obstacle in an agile way. To handle failures we train a &lt;a href=&quot;https://arxiv.org/pdf/2110.05457.pdf&quot;>;recovery policy&lt;/a>; that quickly gets the robot back on its feet, allowing it to continue the episode. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We evaluate the Transformer-based generalist locomotion policy using custom-built quadruped robots and show that by optimizing for the proposed benchmark, we obtain agile, robust, and versatile skills for our robot in the real world. We further provide analysis for various design choices in our system and their impact on the system performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/s1895/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1275&quot; data-original-width=&quot;1895&quot; height=&quot;269&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoNo6kxG9PGgP4Rl8t0GWS1O6bgWtJuA2wMtY95eZQQinIJ-54mJhaWxC_pWLFGNS7wI9ZVUhv24Eoix1o7T-KKHb1aNces1vp9I_3otqttMh0Y8Y0j_O4qe2kX5oNRtfX5pQWh-0sFBG8zn6qfbBx3mOd6HPcF3nYembjm1i1Vun1YPBfjvpJhvkxIQ/w400-h269/image4.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model of the custom-built robots used for evaluation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We deploy both the specialist and generalist policies to hardware (zero-shot sim-to-real). The robot&#39;s target trajectory is provided by a set of waypoints along the various obstacles. In the case of the specialist policies, we switch between specialist policies by using a hand-tuned policy switching mechanism that selects the most suitable policy given the robot&#39;s position. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/3obs_side.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Typical performance of our agile locomotion policies on the Barkour benchmark. Our custom-built quadruped robot robustly navigates the terrain&#39;s obstacles by leveraging various skills learned using RL in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that very often our policies can handle unexpected events or even hardware degradation resulting in good average performance, but failures are still possible. As illustrated in the image below, in case of failures, our recovery policy quickly gets the robot back on its feet, allowing it to continue the episode. By combining the recovery policy with a simple walk-back-to-start policy, we are able to run repeated experiments with minimal human intervention to measure the robustness. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/recovery_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Qualitative example of robustness and recovery behaviors. The robot trips and rolls over after heading down the A-frame. This triggers the recovery policy, which enables the robot to get back up and continue the course.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We find that across a large number of evaluations, the single generalist locomotion transformer policy and the specialist policies with the policy switching mechanism achieve similar performance. The locomotion transformer policy has a slightly lower average Barkour score, but exhibits smoother transitions between behaviors and gaits. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%;&quot; width=&quot;75%&quot;>; &lt;source src=&quot;https://github.com/ai134/obstaclecourse/blob/main/8x8_cont_bright_small.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>;&lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Measuring robustness of the different policies across a large number of runs on the Barkour benchmark.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/s1313/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;616&quot; data-original-width=&quot;1313&quot; height=&quot;300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrYpGuGZTId6CNusbroNBZkgGFi9-8V9sAPoGmjIWWyHqppAqkDnD3FyWEDKWlbIDqMAjtmSo5Xwq7_Prwr-ajuGNeejKn8eIrcFmBIm560mlt5ogQh2hxCGTKWzHx48oPdgPN54dgY7q03SHaCXARYcRkTKVrZbbStg9WjLGP3TGVu-8PLA0jRBjxdw/w640-h300/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Histogram of the agility scores for the locomotion transformer policy. The highest scores shown in blue (0.75 - 0.9) represent the runs where the robot successfully completes all obstacles. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We believe that developing a benchmark for legged robotics is an important first step in quantifying progress toward animal-level agility. To establish a strong baseline, we investigated a zero-shot sim-to-real approach, taking advantage of large-scale parallel simulation and recent advancements in training Transformer-based architectures. Our findings demonstrate that Barkour is a challenging benchmark that can be easily customized, and that our learning-based method for solving the benchmark provides a quadruped robot with a single low-level policy that can perform a variety of agile low-level skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The authors of this post are now part of Google DeepMind. We would like to thank our co-authors at Google DeepMind and our collaborators at Google Research: Wenhao Yu, J. Chase Kew, Tingnan Zhang, Daniel Freeman, Kuang-Hei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang, Nathan Batchelor, Steven Bohez, Federico Casarini, Jose Enrique Chen, Omar Cortes, Erwin Coumans, Adil Dostmohamed, Gabriel Dulac-Arnold, Alejandro Escontrela, Erik Frey, Roland Hafner, Deepali Jain, Yuheng Kuang, Edward Lee, Linda Luu, Ofir Nachum, Ken Oslund, Jason Powell, Diego Reyes, Francesco Romano, Feresteh Sadeghi, Ron Sloat, Baruch Tabanpour, Daniel Zheng, Michael Neunert, Raia Hadsell, Nicolas Heess, Francesco Nori, Jeff Seto, Carolina Parada, Vikas Sindhwani, Vincent Vanhoucke, and Jie Tan. We would also like to thank Marissa Giustina, Ben Jyenis, Gus Kouretas, Nubby Lee, James Lubin, Sherry Moore, Thinh Nguyen, Krista Reymann, Satoshi Kataoka, Trish Blazina, and the members of the robotics team at Google DeepMind for their contributions to the project.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3147300214577163215/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3147300214577163215&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html&quot; rel=&quot;alternate&quot; title=&quot;Barkour: Benchmarking animal-level agility with quadruped robots&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrBp7HZPrMFbbwb27gWZsRozP9LIfcn-Jc-MRqipA7Wi93OOwcpuYFDA_wJVepiwlAHT8cbtKn_IatNGhbX0qBHodjCWrluPI9_56aGOsScLWhGkvQ9jNaQUwU1uZNgg0U4-dWFwVyK3aCZPl5Z6frQimBaIZuyZHnQBY-KOUHmdOJNpkDCW1I8Fl6Bw/s72-c/barkour%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4176009881387884637&lt;/id>;&lt;published>;2023-05-25T16:09:00.003-07:00&lt;/published>;&lt;updated>;2023-05-25T16:12:35.269-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Unsupervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private clustering for large-scale datasets&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Vincent Cohen-Addad and Alessandro Epasto, Research Scientists, Google Research, Graph Mining team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQWQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s900/COVID.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;聚类&lt;/a>;是&lt;a href=&quot;https://en.wikipedia.org/ wiki/Unsupervised_learning&quot;>;无监督&lt;/a>; 机器学习 (ML)，在行业和学术研究领域有许多跨领域的应用。聚类的核心包括以下问题：给定一组数据元素，目标是将数据元素分成几组，使得相似的对象在同一组中，而不同的对象在不同的​​组中。这个问题已经在数学、计算机科学、运筹学和统计学中以其无数变体进行了 60 多年的研究。 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;>;聚类&lt;/a>;的两种常见形式是度量聚类，其中元素是 &lt;a href=&quot;https:// en.wikipedia.org/wiki/Metric_space&quot;>;度量空间&lt;/a>;，例如&lt;em>;&lt;a href=&quot;https://ieeeexplore.ieee.org/document/1056489&quot;>;k-means&lt;/a >;&lt;/em>; 问题和图聚类，其中元素是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;图&lt;/a>; 的节点，其边代表相似性他们之中。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto ; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP -iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4 CDfrQ/s596/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;596&quot; data-original-width= &quot;592&quot; height=&quot;320&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaLHvb0dg44mUKzExJZDrM_OgIHx80OnDaguHi6ZyysbfP-iwBEFndK6UfO_y3hvbZKWwJREJTltz6qDr0k5 PNqsy-k0WSu4f863w2aKqencPuE7ZKNdqOgckRITkuDDwwEEcvL588GQKjS8Td8Bgz4lQYYgLw7VVMd46DdduACj6iJzbhwmVG4CDfrQ/s320/image1.png&quot; width=&quot;318&quot; />;&lt;/a>; &lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;在 &lt;em>;&lt;a href=&quot;https://ieeexplore.ieee.org /document/1056489&quot;>;k-means&lt;/a>;&lt;/em>; 聚类问题，我们在度量空间中得到一组点，目的是识别&lt;em>;k&lt;/em>; 个代表点，称为中心（此处描绘为三角形），以最小化从每个点到其最近中心的距离平方和。 &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Kmeans_toomany.PNG&quot;>;来源&lt;/a>;，版权：CC-BY-SA-4.0&lt;/td>;&lt;/tr>;&lt;/tbody >;&lt;/table>; &lt;p>; 尽管有大量关于聚类算法设计的文献，但很少有实际工作着重于在聚类过程中严格保护用户的隐私。将聚类应用于个人数据（例如，用户进行的查询）时，有必要考虑在真实系统中使用聚类解决方案对隐私的影响，以及输出解决方案揭示了多少有关输入数据的信息。 &lt;/p>; &lt;p>; 为了确保严格意义上的隐私，一种解决方案是开发&lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;差分隐私&lt;/a>; (DP) 集群算法。这些算法确保聚类的输出不会泄露有关特定数据元素的私人信息（例如，用户是否进行了给定查询）或有关输入图的敏感数据（例如，社交网络中的关系）。鉴于隐私保护在无监督机器学习中的重要性，近年来 Google 投入了&lt;a href=&quot;https://arxiv.org/abs/2008.08007&quot;>;理论&lt;/a>;和&lt;a href=&quot;https ://ai.googleblog.com/2021/10/practical-differentially-private.html?hl=el&amp;amp;m=1&quot;>;差异隐私的实践&lt;/a>; &lt;a href=&quot;https://proceedings.neurips .cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;metric&lt;/a>; 或329894坏80 -Abstract-Conference.html&quot;>;graph&lt;/a>; 聚类，以及各种上下文中的差异隐私，例如，&amp;nbsp;&lt;a href=&quot;https://ai.googleblog.com/2023/04/differentially-private -heatmaps.html&quot;>;热图&lt;/a>;&amp;nbsp;或&lt;a href=&quot;https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&quot;>;工具&lt;/a>;设计DP算法。 &lt;/p>; &lt;p>; 今天，我们很高兴地宣布两项重要更新：1) &lt;a href=&quot;https://arxiv.org/abs/2302.00037&quot;>;用于分层图的新差分私有算法&lt;/a>;聚类，我们将在 &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>; 上展示，以及 2) &lt;a href=&quot;https://github.com /google-research/google-research/tree/master/hst_clustering&quot;>;开源发布&lt;/a>;可扩展的差分私有&lt;em>;k&lt;/em>;均值算法的代码。此代码使用分布式计算将差异私有的 &lt;em>;k&lt;/em>; 表示聚类到大规模数据集。在这里，我们还将讨论我们最近在卫生领域发布的集群技术工作，以便通知公共卫生当局。 &lt;/p>; &lt;br />; &lt;h2>;差分私有层次聚类&lt;/h2>; &lt;p>; 层次聚类是一种流行的聚类方法，它包括以越来越细的粒度递归地将数据集划分为聚类。层次聚类的一个众所周知的例子是生物学中的&lt;a href=&quot;https://en.wikipedia.org/wiki/Phylogenetic_tree&quot;>;系统发育树&lt;/a>;，其中地球上的所有生命都被分成越来越细的组（例如，界、门、类、目等）。层次聚类算法接收表示实体相似性的图作为输入，并以无监督方式学习此类递归分区。然而，在我们研究的时候，还没有已知的算法来计算具有边缘隐私的图的层次聚类，即，保留顶点交互的隐私。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://doi.org/10.48550/arXiv.2302.00037&quot;>;Differentially-Private Hierarchical Clustering with Provable Approximation Guarantees&lt;/a>;”中，我们考虑问题的好坏程度可以在 DP 上下文中进行近似，并在隐私保证上建立牢固的上限和下限。我们设计了一种具有多项式运行时间的近似算法（同类中的第一个），该算法实现了与节点数&lt;em>;n&lt;/em>;（&lt;em>;n&lt;sup>;2.5&lt; /sup>;&lt;/em>;) 和 O(log&lt;sup>;½&lt;/sup>; n) 的乘法近似值，乘法误差与非私有设置相同。我们进一步为任何私有算法（不考虑其运行时间）提供了一个新的加性误差下限（&lt;em>;n&lt;sup>;2&lt;/sup>;&lt;/em>;），并提供了一个指数时间算法匹配这个下界。此外，我们的论文包括超越最坏情况的分析，重点关注&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/hash/e8bf0f27d70d480d3ab793bb7619aaa5-Abstract.html&quot;>;分层随机块模型&lt;/a >;，一个标准的随机图模型，展示了一个自然的层次聚类结构，并引入了一个私有算法，该算法返回一个解决方案，该算法的附加成本超过最优值，对于越来越大的图来说可以忽略不计，再次匹配非私有状态-最先进的方法。我们相信这项工作扩展了对图形数据隐私保护算法的理解，并将在此类设置中启用新的应用程序。 &lt;/p>; &lt;br />; &lt;h2>;大规模差异私有集群&lt;/h2>; &lt;p>; 我们现在换个话题，讨论我们在度量空间集群方面的工作。 DP 度量聚类中的大多数先前工作都集中在改进算法在 &lt;em>;k&lt;/em>; 均值目标上的近似保证，而将可扩展性问题排除在外。事实上，目前还不清楚非私有算法的效率如何，例如 &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;>;k-means++&lt;/a>; &lt;/em>; 或 &lt;em>;&lt;a href=&quot;https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf&quot;>;k-means//&lt;/a>;&lt;/em>; 可以是在不显着牺牲近似保证或可扩展性的情况下实现差异化私有化。另一方面，可扩展性和隐私在谷歌是最重要的。为此，我们最近发布了&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;多个&lt;/a>; &lt;a href=&quot;https://proceedings.neurips.cc/paper_files /paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;论文&lt;/a>; 解决了为可扩展到大规模数据集的聚类设计高效差分隐私算法的问题。此外，我们的目标是为大规模输入数据集提供可扩展性，即使中心的目标数量 &lt;em>;k&lt;/em>; 很大。 &lt;/p>; &lt;p>; 我们在&lt;a href=&quot;https://www.cs.umd.edu/~gasarch/MPC/mpc.pdf&quot;>;大规模并行计算&lt;/a>; (MPC) 模型中工作，是代表现代分布式计算架构的计算模型。该模型由多台机器组成，每台机器仅包含部分输入数据，它们协同工作以解决全局问题，同时最大限度地减少机器之间的通信量。我们为 &lt;em>;k 提出了&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/43f55776896a2e33239c2954519f605e-Abstract-Conference.html&quot;>;差分私有常数因子近似算法&lt;/a>; &lt;/em>;——意思是只需要恒定的同步轮数。我们的算法建立在我们&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3534678.3539409&quot;>;之前对该问题的研究&lt;/a>;之上（代码&lt;a href=&quot;https://github. com/google-research/google-research/tree/master/hst_clustering&quot;>;在此处可用&lt;/a>;），这是第一个具有可证明的近似保证的差异私有聚类算法，可以在 MPC 模型中工作。 &lt;/p>; &lt;p>; DP 常数因子近似算法大大改进了以前使用两阶段方法的工作。在初始阶段，它会计算粗略的近似值以“播种”第二阶段，该阶段由更复杂的分布式算法组成。配备第一步近似值后，第二阶段依赖于 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3406325.3451022&quot;>;Coreset 文献&lt;/a>;的结果对相关的一组子样本进行子采样输入点，并为输入点找到一个好的差异私有聚类解决方案。然后，我们证明该解决方案对整个输入具有大致相同的保证。 &lt;/p>; &lt;br />; &lt;h2>;通过 DP 聚类的疫苗接种搜索见解&lt;/h2>; &lt;p>; 然后，我们将差异隐私聚类中的这些进步应用到实际应用中。一个例子是我们应用差分私有集群解决方案来发布与 COVID 疫苗相关的查询，同时为用户提供强大的隐私保护。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://google-research.github.io/vaccination-search-insights/?&quot;>;Vaccination Search Insights&lt;/a>; (VSI) 的目标是帮助公众卫生决策者（卫生当局、政府机构和非营利组织）确定并响应社区关于 COVID 疫苗的信息需求。为了实现这一点，该工具允许用户在不同的地理位置粒度（美国的邮政编码、县和州级别）探索用户搜索的有关 COVID 查询的热门主题。特别是，该工具可视化有关在给定地点和时间兴趣上升的趋势查询的统计数据。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhU umdAQ7IHpEHNsECa53M98cXTvucOquUwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s800/COVIDSearchStats.png”样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLB2pSTmTxFG_h03ZLJf22m2DhRwjo31ksW7yxde1TLwhqT1nUqhG4ryiEWs8SGwhBAJvGY9Urt6BuhUumdAQ7IHpEHNsECa53M98cXTvucOqu UwGlKnq-cXfdryCJB7U3zR9859vZrrnh5ufwfWSE0OIlNUzN_0e0g7dggUHY-AxwtlIx5AF4Risviw/s16000/COVIDSearchStats.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;工具输出的屏幕截图。左侧显示的是 2022 年 10 月 10 日至 16 日期间与 Covid 疫苗相关的热门搜索。右侧显示的是在同一时期以及与前一周相比重要性上升的查询。&lt;/td>;&lt;/tr >;&lt;/tbody>;&lt;/table>; &lt;p>; 为了更好地帮助识别趋势搜索的主题，该工具根据语义相似性对搜索查询进行聚类。这是通过应用自定义设计的基于 &lt;em>;k&lt;/em>; 均值的算法来完成的，该算法运行在已使用 DP 高斯机制匿名化的搜索数据上以添加噪声并删除低计数查询（从而导致差异化聚类）。该方法为保护用户数据提供了强有力的差分隐私保证。 &lt;/p>; &lt;p>; 该工具以前所未有的粒度提供了有关人群对 COVID 疫苗认知的细粒度数据，这对于了解受 COVID 不成比例影响的边缘化社区的需求特别重要。该项目突出了我们在差分隐私和无监督 ML 方法研究方面投资的影响。我们正在寻找其他重要领域，我们可以在这些领域应用这些聚类技术来帮助指导围绕全球健康挑战的决策，例如 &lt;a href=&quot;https://blog.google/technology/health/dr-von-nguyens -temperature-check-on-public-health/&quot;>;与气候变化相关的挑战&lt;/a>;，例如空气质量或极端高温。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢我们的共同作者 Silvio Lattanzi、Vahab Mirrokni、Andres Munoz Medina、Shyam Narayanan、David Saulpic、Chris Schwiegelshohn、Sergei Vassilvitskii、 Peilin Zhong 和我们来自 &lt;/em>; Health AI&lt;em>; 团队的同事使 VSI 的发布成为可能Charlotte Stanton、Mimi Sun、Swapnil Vispute 和 Mark Young。 &lt;/em>; &lt;/p>; &lt;p>; &lt;em>;有关&lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/graph-mining/&quot;>;图挖掘团队&lt;的更多信息/a>;（&lt;a href=&quot;https://ai.google/research/teams/algorithms-optimization/&quot;>;算法和优化&lt;/a>;的一部分）访问我们的页面。&lt;/em>; &lt;/p>;&lt; /content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4176009881387884637/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/differentially-private-clustering-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/ >;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http:/ /www.blogger.com/feeds/8474926331452026626/posts/default/4176009881387884637&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/ 05/differentially-private-clustering-for.html&quot; rel=&quot;alternate&quot; title=&quot;大规模数据集的差异私有聚类&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>; &lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas. google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:缩略图高度=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjG6jKdvoUMqPI-WRS9KnK6Clj8W-uXfR_Pd3BfLIeSQGMb7sJtp1dTNlITKnF5CTtw4c-JtRIi9r_ySKXmKLeIGBTeLozFnQ WQhp6aiXMHVctZjqQfcl2LGDywb3SktCtPwQV9OgAJZ9PyMsAOxeUxxjdRzTf3CIApROfx6hSFBv7NItHKjB8LbFgiTQ/s72-c/COVID.jpg&quot; width=&quot;72&quot; xmlns:media= &quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999 :blog-8474926331452026626.post-6927879920303666871&lt;/id>;&lt;published>;2023-05-25T10:03:00.006-07:00&lt;/published>;&lt;updated>;2023-05-25T13:13:43.829-07:00&lt;/已更新>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google I/O&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/ atom/ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;开源&quot;>;&lt;/category>;&lt;标题类型=&quot;text&quot;>;Google Research at I/O 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 James Manyika 发布，谷歌研究与技术高级副总裁 &amp;amp;社会，以及 Google DeepMind 和 Google Research 的首席科学家 Jeff Dean&lt;/span>; sFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s1200/GoogleIO.jpg &quot; style=&quot;显示：无；&quot; />; &lt;p>; 5 月 10 日，星期三，对于 &lt;a href=&quot;https://research.google&quot;>;Google Research&lt;/a>; 社区来说是激动人心的一天，因为我们看到了数月乃至数年的基础和应用研究成果工作在 &lt;a href=&quot;https://io.google/2023/&quot;>;Google I/O&lt;/a>; 阶段公布。由于舞台上的公告节奏很快，因此很难传达我们展示的技术所付出的巨大努力和独特的创新。所以今天，我们很高兴在 &lt;a href=&quot;https://www.youtube.com/playlist?list=PL590L5WQmH8dAqv03RCMbZrbzxqCn6W3O&quot;>;今年的 I/O&lt; /a>;。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;br />; &lt;h2>;PaLM 2 &lt;/h2>; &lt;p>; 我们的下一代大型语言模型 (LLM)，&lt;a href=&quot;https: //ai.google/discover/palm2&quot;>;PaLM 2&lt;/a>;，建立在&lt;a href=&quot;https://www.deepmind.com/publications/an-empirical-analysis-of-compute- optimal-large-language-model-training&quot;>;计算优化缩放&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;缩放指令微调&lt;/a>;和&lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;改进的数据集混合&lt;/a>;。通过针对不同目的对模型进行微调和指令调整，我们已经能够将最先进的功能集成到超过 25 种 Google 产品和功能中，它已经在帮助告知、协助和取悦用户。例如：&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; 是一项早期实验，可让您与生成式 AI 协作并帮助提升生产力，加速创意并激发好奇心。它以&lt;a href=&quot;https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html&quot;>;深度学习效率的进步&lt;/a>;为基础，并利用&lt;a href= &quot;https://ai.google/static/documents/google-about-bard.pdf&quot;>;从人类反馈中强化学习&lt;/a>;，以提供更相关的响应并提高模型遵循指令的能力。 Bard 现在在 180 个国家/地区可用，用户可以使用英语、日语和韩语与其进行交互，并且由于 PaLM 2 提供的多语言功能，即将支持 40 种语言。 &lt;/li>;&lt;li>;借助&lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;搜索生成体验&lt;/a>;，我们正在从搜索，因此您将能够更快地理解一个主题，发现新的观点和见解，并更轻松地完成工作。作为此实验的一部分，您将看到要考虑的关键信息的 AI 驱动快照，以及可深入挖掘的链接。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; 是一个易于使用的原型制作环境，适用于 &lt;a href=&quot;https://developers .generativeai.google/products/palm&quot;>;PaLM API&lt;/a>;，由 PaLM 2 提供支持。事实上，内部用户对 MakerSuite 早期原型的参与加速了我们 PaLM 2 模型本身的开发。 MakerSuite 源于专注于提示工具或明确设计用于定制和控制 LLM 的工具的研究。这一研究系列包括 &lt;a href=&quot;https://research.google/pubs/pub51353/&quot;>;PromptMaker&lt;/a>;（MakerSuite 的前身）和 &lt;a href=&quot;https://dl.acm.org /doi/abs/10.1145/3491102.3517582&quot;>;AI Chains&lt;/a>; 和 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3491101.3519729&quot;>;PromptChainer&lt;/a>;（最早的研究之一努力证明 LLM 链接的效用）。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://thoughtful.withgoogle.com/about&quot;>;Tailwind&lt;/a>; 项目还利用 MakerSuite 的早期研究原型来开发功能，以帮助作家和研究人员探索想法并改进他们的散文；它的 AI-first 笔记本原型使用 PaLM 2 允许用户根据他们定义的文档对模型提出问题。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>; 是我们最先进的医学 LLM，建立在PaLM 2。Med-PaLM 2 在美国医疗执照考试类问题上取得了 &lt;a href=&quot;https://arxiv.org/abs/2305.09617&quot;>;86.5% 的性能&lt;/a>;，说明了其令人兴奋的健康潜力。我们现在正在探索多模式功能来合成像 X 射线这样的输入。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;Codey&lt;/a>; 是 PaLM 2 的一个版本，在源代码上进行了微调，以充当开发人员助手。它支持广泛的&lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;Code AI&lt;/a>;功能，包括代码补全、代码解释、错误修复、源代码迁移、错误解释等。 Codey 可通过我们受信任的测试程序通过 IDE（&lt;a href=&quot;https://blog.google/technology/developers/google-colab-ai-coding-features/&quot;>;Colab&lt;/a>;，&lt;a href= “https://developer.android.com/studio/preview/studio-bot”>;Android Studio&lt;/a>;，&lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/ introducing-duet-ai-for-google-cloud&quot;>;Duet AI for Cloud&lt;/a>;，&lt;a href=&quot;https://techcrunch.com/2023/05/10/googles-firebase-gets-ai-extensions -opens-up-its-marketplace/&quot;>;Firebase&lt;/a>;) 并通过 &lt;a href=&quot;https://developers.generativeai.google/products/palm&quot;>;面向 3P 的 API&lt;/a>;。 &lt;/li>; &lt;/ul>; &lt;p>; 对于开发人员来说，也许更令人兴奋的是，我们已经开放了 &lt;a href=&quot;https://makersuite.google.com/&quot;>;PaLM API 和MakerSuite&lt;/a>; 为社区提供使用这项突破性技术进行创新的机会。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_c KSQXa_I1vf73PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/s1002 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;965&quot; data-original-width=&quot;1002&quot; height=&quot; 616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_lO0-6ZHGutFue3c03SU6PSvFhCpUhe8zbAOA9DGqsluZztwmFEDmOZmh6PyB_5keo1VDI5plUqy_cKSQXa_I1vf7 3PwTR7kP5npgdzH9WT5WXkzEi3Lz0U6Vd1Pot93nNqLj_HPkbzRN9I29XWtQV-FjMd-_UZUnx1m6yP5s9-5yGQuKOE50MJ-YRQ/w640-h616/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PaLM 2 具有先进的编码功能，可以发现代码错误并提出建议多种不同的语言。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Imagen&lt;/h2>; &lt;p>; 我们的 &lt;a href=&quot;https://ai.google /discover/foundation-models/&quot;>;Imagen 图像生成和编辑模型系列&lt;/a>;建立在大型&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural -network.html&quot;>;Transformer&lt;/a>;-based 语言模型和&lt;a href=&quot;https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html&quot;>;扩散模型&lt;/a>;。该系列模型正被整合到多个 Google 产品中，包括：&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet- ai&quot;>;Google Slides&lt;/a>; 和 Android 的 &lt;a href=&quot;https://blog.google/products/android/new-android-features-generative-ai/&quot;>;Generative AI 壁纸&lt;/a>;由我们的文本到图像生成功能。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-new-ai-models-opens-generative-ai- studio&quot;>;Google Cloud 的 Vertex AI&lt;/a>; 支持图像生成、图像编辑、图像放大和微调，以帮助企业客户满足其业务需求。 &lt;/li>;&lt;li>;&lt;a href=&quot;https://developers.googleblog.com/2023/05/how-its-made-io-flip-adds-twist-to.html&quot;>;I/O 翻转&lt; /a>; 是一款经典纸牌游戏的数字版本，在完全由 AI 生成的纸牌上展示了 Google 开发者吉祥物。该游戏展示了一种名为 &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; 的微调技术，用于调整预先训练的图像生成模型。仅使用少量图像作为&lt;a href=&quot;https://dreambooth.github.io/&quot;>;用于微调的输入&lt;/a>;，它允许用户在几分钟内生成个性化图像。借助 DreamBooth，用户可以在参考图像中未出现的各种场景、姿势、视图和光照条件下合成主体。 &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt; td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq3 2ym-zoY6jQqI1r7t69GCre9PKtBcoBOrtB7Qytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s1649/image2.jpg&quot; style=&quot;margin-左：自动；右边距：自动；”>;&lt;img border=&quot;0&quot; data-original-height=&quot;799&quot; data-original-width=&quot;1649&quot; src=&quot;https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEjMKclgX98fR7aSyZEsk62yhnw26GWZAEqlj9PVcyH7ImBlwd06UlWeok4FHHfwoouc9Zj71kpY1tULtyRCGq32ym-zoY6jQqI1r7t69GCre9PKtBcoBORTB7Q ytj3jGsbjsOh55FSq6fgzBWC8Nwne3n602IWDpE-t6qjV66PekyBL4G3zUd9i4DeZ4A/s16000/image2.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;I/O Flip 展示了使用 DreamBooth 设计的定制卡片组。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Phenaki&lt; /h2>; &lt;p>; &lt;a href=&quot;https://sites.research.google/phenaki/&quot;>;Phenaki&lt;/a>;，谷歌基于 Transformer 的文本到视频生成模型在 I/O 预展中得到了展示-展示。 Phenaki 是一个&lt;a href=&quot;https://openreview.net/forum?id=vOEXS39nOF&quot;>;模型，可以通过利用两个主要组件从文本提示序列合成逼真的视频&lt;/a>;：编码器-解码器模型，压缩视频到离散嵌入和将文本嵌入转换为视频标记的转换器模型。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmW wpa_pzkjIXo68cAK8SfQosCdpoYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/s128/image4.gif”样式=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;128&quot; data-original-width=&quot;128&quot; height=&quot;200&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFZEAHb9MfuxVSr3ia9e4NoqvUvu71YoNGqHoZ2-Wc4N-Z3SjUMrSULwfI9p1uTJWRZufio1jl4cLmWwpa_pzkjIXo68cAK8SfQosCd poYei0LKl4Gm5tDvamc2MU2ot9LwN2dG_Pw9MQJgb9b2iH3ZntTjwIlWu0CFDH5mAf3WbMmKCoYAZGypC7tHw/w200-h200/image4.gif&quot; width=&quot;200&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href =&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4 -GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s548/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;269&quot; data-original-width=&quot;548&quot; height=&quot;157&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8goXhaXmHZV_weg4vi1NLXDA8iIwfHlue9R0RVYCKT4PnQPJ8WR2OjfWK-WuQcHB-plbxnV75GxLHzOU7nXSCpeUVyIbSZAFBix4-GOpmwh6UFRzV_7QTQoRzcf-88ythNaHdlvdl-z7RfbX7WWmkEg9oO5S2fWwfOXi3UM4qGkcVmw_jVDWQIm17oQ/s320/image6.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;ARCore and the Scene Semantic API&lt;/h2>; &lt;p>; Among the new features of &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; announced by the AR team at I/O, the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;Scene Semantic API&lt;/a>; can recognize pixel-wise semantics in an outdoor scene. This helps users create custom AR experiences based on the features in the surrounding area. This API is empowered by the outdoor semantic segmentation model, leveraging our recent works around the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLab&lt;/a>; architecture and an egocentric outdoor scene understanding dataset. The latest ARCore release also includes an improved monocular depth model that provides higher accuracy in outdoor scenes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/s486/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;486&quot; data-original-width=&quot;242&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGnl1lYcfkaoqmwIOqb1R8bojq_bbIgiGYC9cMuTGO5hqwsrdXHhGaHIBlfAPwTob-KpyVXUFt-8_NPu1GPhZgmbyfT-ILRDmn980P_tkb0jfwzNNw0cannsvSNTiMyRcntVJ9AyjfnGT5Q4ZBBotOx4MJPOOCF57Ejs0VN1evKjubZQYpyX_NfMtx4A/w199-h400/image3.gif&quot; width=&quot;199&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scene Semantics API uses DeepLab-based semantic segmentation model to provide accurate pixel-wise labels in a scene outdoors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Chirp&lt;/h2>; &lt;p>; &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;Chirp&lt;/a>; is Google&#39;s family of state-of-the-art &lt;a href=&quot;https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Models&lt;/a>; trained on 12 million hours of speech to enable automatic speech recognition (ASR) for 100+ languages. The models can perform ASR on under-resourced languages, such as Amharic, Cebuano, and Assamese, in addition to widely spoken languages like English and Mandarin. Chirp is able to cover such a wide variety of languages by leveraging &lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;self-supervised learning on unlabeled multilingual dataset with fine-tuning on a smaller set of labeled data&lt;/a>;. Chirp is now available in the Google Cloud &lt;a href=&quot;https://cloud.google.com/speech-to-text&quot;>;Speech-to-Text API&lt;/a>;, allowing users to perform inference on the model through a simple interface. You can get started with Chirp &lt;a href=&quot;https://cloud.google.com/speech-to-text/v2/docs/chirp-model&quot;>;here&lt;/a>;. &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s591/image7.gif&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;250&quot; data-original-width=&quot;591&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhtWnj_lXrnnMX5wnXJoWwpRg34v-MyHuSepuvVK4MZfWgbBhGgcMZXPFoSBvS8RIccX9Fes4ASD4mhjrQYyLzZRoaGZjqBrNSfFRChVh1rQ3Disr2q9iO0tiPkHwf3JiReP_0E1nfsS4MWbQwOzxwBHGCX29IhxooTSqw8qhXvQAyoIG9v_SVX6YOHPQ/s16000/image7.gif&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;MusicLM&lt;/h2>; &lt;p>; At I/O, we launched &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM, a text-to-music model&lt;/a>; that generates 20 seconds of music from a text prompt. &lt;a href=&quot;https://aitestkitchen.withgoogle.com/experiments/music-lm&quot;>;You can try it yourself on AI Test Kitchen&lt;/a>;, or see it featured during the I/O preshow, where electronic musician and composer &lt;a href=&quot;https://en.wikipedia.org/wiki/Dan_Deacon&quot;>;Dan Deacon&lt;/a>; used MusicLM in his performance. &lt;/p>; &lt;p>; MusicLM, which consists of models powered by &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2208.12415&quot;>;MuLAN&lt;/a>;, can make music (from text, humming, images or video) and musical accompaniments to singing. AudioLM generates high quality audio with long-term consistency. It maps audio to a sequence of discrete tokens and casts audio generation as a language modeling task. To synthesize longer outputs efficiently, it used a novel approach we&#39;ve developed called &lt;a href=&quot;https://google-research.github.io/seanet/soundstorm/examples/&quot;>;SoundStorm&lt;/a>;. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s1927/keywordhero.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;964&quot; data-original-width=&quot;1927&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiTUOKJuyb19L8ukNhdv51q7M4cMXSJbSnp1n9pd_-vRHzOAW3pENKYWDyBoPKYIN-K9UpduTbXjqQZ3IX5jwtrp0YNu13dZuYC2uIplzl_oSyLDXAhfO6L1kAaXdJXIWWhsGTqJQb-O_0JZ18E4lVNDaT23gMBg9Jcu2r4N_ofkUckaIMSTW0vEJkMQ/s16000/keywordhero.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;Universal Translator dubbing&lt;/h2>; &lt;p>; Our dubbing efforts leverage dozens of ML technologies to translate the full expressive range of video content, making videos accessible to audiences across the world. These technologies have been used to &lt;a href=&quot;https://www.youtube.com/watch?v=S-iIV5Oo0n0&quot;>;dub videos&lt;/a>; across a variety of products and content types, including educational content, advertising campaigns, and creator content, with more to come. We use deep learning technology to achieve &lt;a href=&quot;https://developers.googleblog.com/2022/12/improving-video-voice-dubbing-through-deep-learning.html&quot;>;voice preservation and lip matching&lt;/a>; and enable high-quality video translation. We&#39;ve built this product to include human review for quality, safety checks to help prevent misuse, and we make it accessible only to authorized partners. &lt;/p>; &lt;br />; &lt;h2>;AI for global societal good&lt;/h2>; &lt;p>; We are applying our AI technologies to solve some of the biggest global challenges, like mitigating climate change, adapting to a warming planet and improving human health and wellbeing. For example: &lt;/p>; &lt;ul>; &lt;li>;Traffic engineers use our Green Light recommendations to reduce stop-and-go traffic at intersections and improve the flow of traffic in cities from Bangalore to Rio de Janeiro and Hamburg. Green Light models each intersection, analyzing traffic patterns to develop recommendations that make traffic lights more efficient — for example, by better synchronizing timing between adjacent lights, or adjusting the “green time” for a given street and direction. &lt;/li>;&lt;li>;We&#39;ve also expanded global coverage on the &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>; to 80 countries, as part of our efforts to predict riverine floods and alert people who are about to be impacted before disaster strikes. Our &lt;a href=&quot;https://sites.research.google/floodforecasting&quot;>;flood forecasting efforts&lt;/a>; rely on &lt;a href=&quot;https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html&quot;>;hydrological models&lt;/a>; informed by satellite observations, weather forecasts and in-situ measurements. &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s1672/image5.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;713&quot; data-original-width=&quot;1672&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqlBPlwDoD8Fa-Kmh1PfWb2St5CytGcSHNNNuPjvmACdbXZA7xdWV1cQm_ucjvazfm091eVtZcyFteSrXMLrYWDEAjB7tcDY4xoxu3CMSAK0e4J2d6FG1uIBj_I2udbWNwFHHAoeLwkyco4rdCey-0adK5rZBLw1tjAtFzkhheifFsjXRBMxGF6to10w/s16000/image5.png&quot; />;&lt;/a>;&lt;/div>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Technologies for inclusive and fair ML applications&lt;/h2>; &lt;p>; With our continued investment in AI technologies, we are emphasizing responsible AI development with the goal of making our models and tools useful and impactful while also ensuring fairness, safety and alignment with our &lt;a href=&quot;https://ai.google/principles/&quot;>;AI Principles&lt;/a>;. Some of these efforts were highlighted at I/O, including: &lt;/p>; &lt;ul>; &lt;li>;The release of the &lt;a href=&quot;https://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;Monk Skin Tone Examples (MST-E) Dataset&lt;/a>; to help practitioners gain a deeper understanding of the MST scale and train human annotators for more consistent, inclusive, and meaningful skin tone annotations. You can read more about this and other developments on our &lt;a href=&quot;https://skintone.google/&quot;>;website&lt;/a>;. This is an advancement on the open source release of the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone (MST) Scale&lt;/a>; we launched last year to enable developers to build products that are more inclusive and that better represent their diverse users. &lt;/li>;&lt;li>;A &lt;a href=&quot;https://www.kaggle.com/competitions/asl-fingerspelling/overview/description&quot;>;new Kaggle competition&lt;/a>; (open until August 10th) in which the ML community is tasked with creating a model that can quickly and accurately identify American Sign Language (ASL) fingerspelling — where each letter of a word is spelled out in ASL rapidly using a single hand, rather than using the specific signs for entire words — and translate it into written text. Learn more about the &lt;a href=&quot;https://www.youtube.com/watch?v=q3xKB3dfvtA&quot;>;fingerspelling Kaggle competition&lt;/a>;, which features a song from &lt;a href=&quot;https://www.deafandloud.com/&quot;>;Sean Forbes&lt;/a>;, a deaf musician and rapper. We also &lt;a href=&quot;https://www.youtube.com/watch?v=WC9x3jp_nV8&quot;>;showcased at I/O&lt;/a>; the winning algorithm from the prior year&#39;s competition powers &lt;a href=&quot;https://play.google.com/store/apps/details?id=edu.gatech.popsignai&amp;amp;hl=en_US&amp;amp;gl=US&quot;>;PopSign&lt;/a>;, an ASL learning app for parents of deaf or hard of hearing children created by Georgia Tech and Rochester Institute of Technology (RIT). &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;Building the future of AI together&lt;/h2>; &lt;p>; It&#39;s inspiring to be part of a community of so many talented individuals who are leading the way in developing state-of-the-art technologies, responsible AI approaches and exciting user experiences. We are in the midst of a period of incredible and transformative change for AI. Stay tuned for more updates about the ways in which the Google Research community is boldly exploring the frontiers of these technologies and using them responsibly to benefit people&#39;s lives around the world. We hope you&#39;re as excited as we are about the future of AI technologies and we invite you to engage with our teams through the references, sites and tools that we&#39;ve highlighted here. &lt;/p>;&lt;p>;&lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6927879920303666871/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6927879920303666871&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/google-research-at-io-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research at I/O 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUde5cQAPm8frw1OO3Ub-QIx91GGpY3crqacseMxvbkJ55GMmjma-dqjEmxg8XtAJpSEYvyVWsFagmNVLzugwLWJiQ6OHPm0c1yDgIgXzTHRRF4NpoOJRl5p75u3O11uYuOmPNJW97Xyciox0OJni48f3MrMGmAPqVudm9mtsUUtGaCvt9vIQrc6h3NA/s72-c/GoogleIO.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5648346519353613408&lt;/id>;&lt;published>;2023-05-23T10:51:00.004-07:00&lt;/published>;&lt;updated>;2023-05-24T14:28:39.289-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;DeepMind&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Semantic Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;User Experience&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Resolving code review comments with ML&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alexander Frömmgen, Staff Software Engineer, and Lera Kharatyan, Senior Software Engineer, Core Systems &amp;amp; Experiences&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s2000/comments2code.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Code-change reviews are a critical part of the software development process at scale, &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;taking&lt;/a>; a significant amount of the code authors&#39; and the code reviewers&#39; time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see &lt;a href=&quot;https://sback.it/publications/icse2018seip.pdf&quot;>;millions of reviewer comments&lt;/a>; per year, and authors require an average of ~60 minutes &lt;a href=&quot;https://research.google/pubs/pub49446/&quot;>;active shepherding time&lt;/a>; between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must do to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code review process, eg, by proposing code changes based on a comment&#39;s text. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe applying recent advances of large sequence models in a real-world setting to automatically resolve code review comments in the day-to-day development workflow at Google (publication forthcoming). As of today, code-change authors at Google address a substantial amount of reviewer comments by applying an ML-suggested edit. We expect that to reduce time spent on code reviews by hundreds of thousands of hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers&#39; productivity and allows them to focus on more creative and complex tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Predicting the code edit&lt;/h2>; &lt;p>; We started by training a model that predicts code edits needed to address reviewer comments. The model is pre-trained on various coding tasks and related developer activities (eg, renaming a variable, repairing a broken build, editing a file). It&#39;s then fine-tuned for this specific task with reviewed code changes, the reviewer comments, and the edits the author performed to address those comments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s1523/figure1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1523&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRR4C9paw916JEuwW9jTXjFBTt9ii4KEnnXBw2HgxY5MR9muFW3IdJLloAlRV-7AP3Nd8Jg1q633KQS6zfXDlPZlIV8c7KIp9iXgsM2GHJV_iv4e5mCrUhRrT2LFNBpuSFGTLzKDk6w56Km_jOOkWMMdEZlMadlwBDYVGnztFOPk1mo828sWEpUF1O-g/s16000/figure1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of an ML-suggested edit of refactorings that are spread within the code.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Google uses a &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2854146&quot;>;monorepo&lt;/a>;, a single repository for all of its software artifacts, which allows our training dataset to include all unrestricted code used to build Google&#39;s most recent software, as well as previous versions. &lt;/p>; &lt;p>; To improve the model quality, we iterated on the training dataset. For example, we compared the model performance for datasets with a single reviewer comment per file to datasets with multiple comments per file, and experimented with classifiers to clean up the training data based on a small, curated dataset to choose the model with the best offline precision and recall metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Serving infrastructure and user experience&lt;/h2>; &lt;p>; We designed and implemented the feature on top of the trained model, focusing on the overall user experience and developer efficiency. As part of this, we explored different user experience (UX) alternatives through a series of user studies. We then refined the feature based on insights from an internal beta (ie, a test of the feature in development) including user feedback (eg, a “Was this helpful?” button next to the suggested edit). &lt;/p>; &lt;p>; The final model was calibrated for a target &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&quot;>;precision&lt;/a>; of 50%. That is, we tuned the model and the suggestions filtering, so that 50% of suggested edits on our evaluation dataset are correct. In general, increasing the target precision reduces the number of shown suggested edits, and decreasing the target precision leads to more incorrect suggested edits. Incorrect suggested edits take the developers time and reduce the developers&#39; trust in the feature. We found that a target precision of 50% provides a good balance. &lt;/p>; &lt;p>; At a high level, for every new reviewer comment, we generate the model input in the same format that is used for training, query the model, and generate the suggested code edit. If the model is confident in the prediction and a few additional heuristics are satisfied, we send the suggested edit to downstream systems. The downstream systems, ie, the code review frontend and the integrated development environment (IDE), expose the suggested edits to the user and log user interactions, such as preview and apply events. A dedicated pipeline collects these logs and generates aggregate insights, eg, the overall acceptance rates as reported in this blog post. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s1250/figure%202c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1250&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5csoDiwO-5vBxy3C4hFSW1urE791VDmpjuM2XyuVQvgMzjwrfqdwIKyJaOffDGEI4X3Y1cjTwP0kiRudNVHB6IKbkzo7znabCW16vsmhyOYyeXFOC24RYXIr406expAnjSRiid883LWa1hRiEXjMa2j5SqvTWFUeucRZ4Bjw9moOo7D_zg1pmGE02-g/s16000/figure%202c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Architecture of the ML-suggested edits infrastructure. We process code and infrastructure from multiple services, get the model predictions and surface the predictions in the code review tool and IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The developer interacts with the ML-suggested edits in the code review tool and the IDE. Based on insights from the user studies, the integration into the code review tool is most suitable for a streamlined review experience. The IDE integration provides additional functionality and supports 3-way merging of the ML-suggested edits (left in the figure below) in case of conflicting local changes on top of the reviewed code state (right) into the merge result (center). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s958/figure%203%20(25%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;436&quot; data-original-width=&quot;958&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgceQpRBQ0ooZGNUvGgc1j1qnF8YwJaVGmhUF2KUD8F_KlLJVgzoo-VGcJsIRykGR-flujvuXB83a61cVz88tZHKsJPb8h9tLmoI4LDizmHvaSzRhwcQpxmQZtLI12ckyWJmxl9yhqiFYmHF7oKoBEYSJhqiyhVRtYp-nFBB5RH2VLH3Jtauls4ti1Tmw/s16000/figure%203%20(25%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;3-way-merge UX in IDE.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Offline evaluations indicate that the model addresses 52% of comments with a target precision of 50%. The online metrics of the beta and the full internal launch confirm these offline metrics, ie, we see model suggestions above our target model confidence for around 50% of all relevant reviewer comments. 40% to 50% of all previewed suggested edits are applied by code authors. &lt;/p>; &lt;p>; We used the “not helpful” feedback during the beta to identify recurring failure patterns of the model. We implemented serving-time heuristics to filter these and, thus, reduce the number of shown incorrect predictions. With these changes, we traded quantity for quality and observed an increased real-world acceptance rate. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s1753/figure%204%20(50%25).gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1753&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyN4DWxW8l5pkxQs0zvbrINGkJSUWnLizahvVbmanqdh0KlnxH4mTHVtKDjNn29fq8oJ35BQNYeeF8p_YURKz2qwS7LwAVoBZflochjmcTVJlag1UrSsaY-DLTDB9H0tJTi1emA6nWstqomACKOeVS4DSANGDl-UWpgXfu-PgliUIMFtUOgi7OyUNg2A/s16000/figure%204%20(50%25).gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Code review tool UX. The suggestion is shown as part of the comment and can be previewed, applied and rated as helpful or not helpful.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our beta launch showed a &lt;em>;discoverability challenge&lt;/em>;: code authors only previewed ~20% of all generated suggested edits. We modified the UX and introduced a prominent “Show ML-edit” button (see the figure above) next to the reviewer comment, leading to an overall preview rate of ~40% at launch. We additionally found that suggested edits in the code review tool are often not applicable due to conflicting changes that the author did during the review process. We addressed this with a button in the code review tool that opens the IDE in a merge view for the suggested edit. We now observe that more than 70% of these are applied in the code review tool and fewer than 30% are applied in the IDE. All these changes allowed us to increase the overall fraction of reviewer comments that are addressed with an ML-suggested edit by a factor of 2 from beta to the full internal launch. At Google scale, these results help automate the resolution of hundreds of thousands of comments each year. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s1920/figure%207.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;887&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbb5VXmcCaqT7-jtdVBB57Jzb41KhulYqquTk1OAG35-hM-ztMZvQAzh69E-IA-fJMJCkPJ9v7altTmAU9IzkfJrhxqtZdYT7sF-swZ9ThsiOpK5sD2B1bMfxaQlwFKy7dEqEEgDxnF2FOnnMteUgilDPQcvxciDGpaqh4bRfkds5XIQq9-qaZEZkZyw/s16000/figure%207.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Suggestions filtering funnel.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We see ML-suggested edits addressing a wide range of reviewer comments in production. This includes simple localized refactorings and refactorings that are spread within the code, as shown in the examples throughout the blog post above. The feature addresses longer and less formally-worded comments that require code generation, refactorings and imports. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s1479/figure%205c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;651&quot; data-original-width=&quot;1479&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8d6n1irF1I8_8vkOGByJYA_S5K1kxMguPNr9Z212V0yJvTX46rfj7HZT0ggOfcpJPHobT7UQttP_86gxRsPCbBbiuwMKfrzktVCccBTQXWe8l5a_ezmZJMQQ0yl8tXPThkxQR_yV2JejcpFXwhZ0U_ssbKyVcfaLKk_KErzWJ1Dszu0gasvsuTS0gVQ/s16000/figure%205c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of a suggestion for a longer and less formally worded comment that requires code generation, refactorings and imports.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The model can also respond to complex comments and produce extensive code edits (shown below). The generated test case follows the existing unit test pattern, while changing the details as described in the comment. Additionally, the edit suggests a comprehensive name for the test reflecting the test semantics. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s1497/figure%206c.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;648&quot; data-original-width=&quot;1497&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPJQaVUZNW7KVuLtF1n_Gxhe1wg8QcElAABAp4AQH-B6mnt5N2-xZqEnK5VnEoy5eqXrdwR__xICAHDUYxb3wC25M_XA7gdJCN0l4mavhLNSxzC4lUqXL-7x3FD4M9SNEvgdmLzN5jcX727V4QEgAxn36BN_R7l4HQmjzwG6e9v6Vq1HzFKy1s8H01SA/s16000/figure%206c.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the model&#39;s ability to respond to complex comments and produce extensive code edits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; In this post, we introduced an ML-assistance feature to reduce the time spent on code review related changes. At the moment, a substantial amount of all actionable code review comments on supported languages are addressed with applied ML-suggested edits at Google. A 12-week A/B experiment across all Google developers will further measure the impact of the feature on the overall developer productivity. &lt;/p>; &lt;p>; We are working on improvements throughout the whole stack. This includes increasing the quality and recall of the model and building a more streamlined experience for the developer with improved discoverability throughout the review process. As part of this, we are investigating the option of showing suggested edits to the reviewer while they draft comments and expanding the feature into the IDE to enable code-change authors to get suggested code edits for natural-language commands. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This is the work of many people in Google Core Systems &amp;amp; Experiences team, Google Research, and DeepMind.我们要特别感谢 Peter Choy 召集合作，感谢我们所有团队成员的重要贡献和有用的建议，包括 Marcus Revaj、Gabriela Surita、Maxim Tabachnyk、Jacob Austin、Nimesh Ghelani、Dan Zheng、Peter Josling , Mariana Stariolo, Chris Gorgolewski, Sascha Varkevisser, Katja Grünwedel, Alberto Elizondo, Tobias Welp, Paige Bailey, Pierre-Antoine Manzagol, Pascal Lamblin, Chenjie Gu, Petros Maniatis, Henryk Michalewski, Sara Wiltberger, Ambar Murillo, Satish Chandra, Madhura Dudhgaonkar , Niranjan Tulpule Zoubin Ghahramani , Brett Wiltshire, Laurent Le Brun, Mingpan Guo, Hermann Loose, Jonas Mattes, Savinee Dancs。感谢 John Guilyard 在这篇博文中创建图形。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/5648346519353613408/comments/default&quot; rel=&quot;回复&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml. html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408 &quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5648346519353613408&quot; rel=&quot;self&quot; type=&quot; application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html&quot; rel=&quot;alternate&quot; title=&quot;Resolving使用 ML 进行代码审查评论&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>; noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/ img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjoK3FH1StJnuqrj0gTExqLUxNkVfwRVmiG9tbCNUDV2I5295yqrPlw0L4hRHJmsruvZBIa_FqAFvJfwW7VU4XvxDU4ZweaW6ehENeU7-BLJaLVGPPtn-25cme5qTUldd11 YigkAj6Ks9Tif6J3MePey_Gn3cAp3nQf9LMmVy4Eg3yF0qyBZPUKfYJYLw/s72-c/comments2code.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total >;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3261601760457492933&lt;/id>;&lt;published>;2023-05-19T09:59:00.002- 07:00&lt;/published>;&lt;updated>;2023-05-19T10:01:19.784-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;安全和隐私&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;制作 ML差分隐私模型：最佳实践和开放挑战&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google Research 高级软件工程师 Natalia Ponomareva 和 Alex Kurakin 发布&lt;/span>; &lt; img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35 T8OrL_HmNr4YIKzBS5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s320/DPfy%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Large machine learning (ML) models are ubiquitous in modern applications: from &lt;a href=&quot;https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters&quot;>;spam filters&lt;/a>; to &lt;a href=&quot;https://developers.google.com/machine-learning/recommendation/overview/types&quot;>;recommender systems&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2022/07/look-and-talk-natural-conversations.html&quot;>;virtual assistants&lt;/a>;. These models achieve remarkable performance partially due to the abundance of available training data. However, these data can sometimes contain private information, including personal identifiable information, copyright material, etc. Therefore, protecting the privacy of the training data is critical to practical, applied ML. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential Privacy&lt;/a>; (DP) is one of the most widely accepted technologies that allows reasoning about data anonymization in a formal way. In the context of an ML model, DP can guarantee that each individual user&#39;s contribution will not result in a significantly different model. A model&#39;s privacy guarantees are characterized by a tuple (&lt;em>;ε&lt;/em>;, &lt;em>;δ&lt;/em>;), where smaller values of both represent stronger DP guarantees and better privacy. &lt;/p>; &lt;p>; While there are successful &lt;a href=&quot;https://research.google/pubs/pub52351/&quot;>;examples&lt;/a>; of &lt;a href=&quot;https://ai.googleblog.com/2022/02/federated-learning-with-formal.html&quot;>;protecting training data&lt;/a>; using DP, obtaining good utility with differentially private ML (DP-ML) techniques can be challenging. First, there are inherent privacy/computation tradeoffs that may limit a model&#39;s utility. Further, DP-ML models often require architectural and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;>;hyperparameter&lt;/a>; tuning, and guidelines on how to do this effectively are limited or difficult to find. Finally, non-rigorous privacy reporting makes it challenging to compare and choose the best DP methods. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.00654&quot;>;How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy&lt;/a>;”, to appear in the &lt;em>;&lt;a href=&quot;https://www.jair.org/index.php/jair&quot;>;Journal of Artificial Intelligence Research&lt;/a>;&lt;/em>;, we discuss the current state of DP-ML research. We provide an overview of common techniques for obtaining DP-ML models and discuss research, engineering challenges, mitigation techniques and current open questions. We will present tutorials based on this work at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>; and &lt;a href=&quot;https://kdd.org/kdd2023/&quot;>;KDD 2023&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DP-ML methods&lt;/h2>; &lt;p>; DP can be introduced during the ML model development process in three places: (1) at the input data level, (2) during training, or (3) at inference. Each option provides privacy protections at different stages of the ML development process, with the weakest being when DP is introduced at the prediction level and the strongest being when introduced at the input level. Making the input data differentially private means that any model that is trained on this data will also have DP guarantees. When introducing DP during the training, only that particular model has DP guarantees. DP at the prediction level means that only the model&#39;s predictions are protected, but the model itself is not differentially private. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s1821/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;1821&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm6JbnVKbvGHMOesXYhP2AWZqVWeYqZGL4Hp4xruQmLzGZNv_Pb0tweOOmSznicm06Fb0Dk6G4HpE5hp2hOeAY0BWCO04rL1w_tdE5JC3jBbmPzMRpGQgf9z1jYYszQyltM3rvXaHLG7__JnePH9MEVG_YTSFYEQdYHQUTJXxVjzyCjREw0Bj4I4It3w/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The task of introducing DP gets progressively easier from the left to right.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; DP is commonly introduced during training (DP-training). &lt;a href=&quot;https://arxiv.org/pdf/2303.00654.pdf&quot;>;Gradient noise injection&lt;/a>; methods, like &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, and their extensions are currently the most practical methods for achieving DP guarantees in complex models like large deep neural networks. &lt;/p>; &lt;p>; DP-SGD builds off of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; (SGD) optimizer with two modifications: (1) per-example gradients are clipped to a certain norm to limit sensitivity (the influence of an individual example on the overall model), which is a slow and computationally intensive process, and (2) a noisy gradient update is formed by taking aggregated gradients and adding noise that is proportional to the sensitivity and the strength of privacy guarantees. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s1439/DP-ML.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;494&quot; data-original-width=&quot;1439&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQqxkabmVXlVWrPH4rEop_ZFxMo_DtkzllZq3JIpstNNxAxutrZe-kS24Vqi1b2r4BvF1zEP3hpZGEvoExnJHKqgnREGcgTJWSjmfglItZAa_ZHLrsTTRvLBxY4z6nmeJoT1ptEnM9dkM82Iq8_FCA2lmUZclzmLsqfDZ2czAcHQW_8PnabZvy1ZYBXw/s16000/DP-ML.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DP-SGD is a modification of SGD that involves a) clipping per-example gradients to limit the sensitivity and b) adding the noise, calibrated to the sensitivity and privacy guarantees, to the aggregated gradients, before the gradient update step. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Existing DP-training challenges&lt;/h2>; &lt;p>; Gradient noise injection methods usually exhibit: (1) loss of utility, (2) slower training, and (3) an increased &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory_footprint&quot;>;memory footprint&lt;/a>;. &lt;/p>; &lt;p>; &lt;strong>;Loss of utility&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;The best method for reducing utility drop is to use more computation. Using larger batch sizes and/or more iterations is one of the most prominent and practical ways of improving a model&#39;s performance. Hyperparameter tuning is also extremely important but often overlooked. The utility of DP-trained models is sensitive to the total amount of noise added, which depends on hyperparameters, like the clipping norm and batch size. Additionally, other hyperparameters like the learning rate should be re-tuned to account for noisy gradient updates. &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Another option is to obtain more data or use public data of similar distribution. This can be done by leveraging publicly available checkpoints, like &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>; or &lt;a href=&quot;https://arxiv.org/pdf/1910.10683.pdf&quot;>;T5&lt;/a>;, and fine-tuning them using private data. &lt;/p>; &lt;p>; &lt;strong>;Slower training&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;Most gradient noise injection methods limit sensitivity via clipping per-example gradients, considerably slowing down &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;>;backpropagation&lt;/a>;. This can be addressed by choosing an efficient DP framework that efficiently implements per-example clipping. &lt;/p>; &lt;p>; &lt;strong>;Increased memory footprint&lt;/strong>;: &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;DP-training requires significant memory for computing and storing per-example gradients. Additionally, it requires significantly larger batches to obtain better utility. Increasing the computation resources (eg, the number and size of accelerators) is the simplest solution for extra memory requirements. Alternatively, &lt;a href=&quot;https://arxiv.org/abs/2109.12298&quot;>;several&lt;/a>; &lt;a href=&quot;https://arxiv.org/abs/2201.12328&quot;>;works&lt;/a>; advocate for gradient accumulation where smaller batches are combined to simulate a larger batch before the gradient update is applied.此外，一些算法（例如，&lt;a href=&quot;https://arxiv.org/pdf/2110.05679.pdf&quot;>;ghost clipping&lt;/a>;，它基于 &lt;a href=&quot;https://arxiv.org /abs/1510.01799&quot;>;本文&lt;/a>;) 完全避免每个示例的梯度裁剪。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;最佳实践&lt;/h2>; &lt;p>; 以下最佳实践可以达到严格的DP保证，最好模型效用可能。 &lt;/p>; &lt;p>; &lt;strong>;选择正确的隐私单元：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;首先，我们应该清楚模型的隐私保证。这是通过选择代表相邻数据集概念（即只有一行不同的数据集）的“隐私单元”来编码的。示例级保护是研究文献中的常见选择，但如果单个用户向训练数据集贡献了多个记录，则对于用户生成的数据可能并不理想。对于这种情况，用户级保护可能更合适。对于文本和序列数据，单元的选择更加困难，因为在大多数应用中，单个训练示例与文本中嵌入的语义不一致。 &lt;/p>; &lt;p>; &lt;strong>;选择隐私保障：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;我们概述了隐私保障的三大层级，并鼓励从业者选择最低的以下可能的层级：&lt;/p>; &lt;ul style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;层级 1 — 强大的隐私保证：&lt;/em>; 选择 &lt;em>;ε&lt;/em>; ≤ 1 提供一个强大的隐私保证，但经常导致大型模型的效用显着下降，因此可能只适用于较小的模型。 &lt;/li>;&lt;li>;&lt;em>;第 2 层 — 合理的隐私保证&lt;/em>;：我们提倡当前未记录但仍被广泛使用的 DP-ML 模型目标，以实现 &lt;em>;ε&lt;/em>; ≤ 10。&lt;/li>;&lt;li>;&lt;em>;第 3 层 — 弱隐私保证&lt;/em>;：任何有限 &lt;em>;ε&lt;/em>; 都是对没有正式隐私保证的模型的改进。然而，对于 &lt;em>;ε&lt;/em>; >; 10，单独的 DP 保证不能作为数据匿名化的充分证据，可能需要额外的措施（例如，经验隐私审计）来确保模型保护用户数据。 &lt;/li>; &lt;/ul>; &lt;p>; &lt;strong>;超参数调整&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;选择超参数需要优化三个​​相互依赖的目标：1 ) 模型效用，2) 隐私成本 &lt;em>;ε&lt;/em>;，以及 3) 计算成本。常见的策略以三者中的两个为约束，重点优化第三个。我们提供的方法可以通过有限次数的试验来最大化效用，例如，根据隐私和计算约束进行调整。 &lt;/p>; &lt;p>; &lt;strong>;报告隐私保证：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;许多关于 DP 的工作仅用于 ML 报告&lt;em>;ε&lt; /em>; 和可能的 &lt;em>;δ &lt;/em>; 训练过程的值。但是，我们认为从业者应该提供模型保证的全面概述，包括：&lt;/p>; &lt;ol style=&quot;margin-left: 4em;&quot;>; &lt;li>;&lt;em>;DP 设置：&lt;/em>; 结果是假设中央 DP 与受信任的服务提供商、&lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;本地 DP&lt;/a>; 或其他设置？ &lt;/li>;&lt;li>;&lt;em>;实例化 DP 定义：&lt;/em>; &lt;ol type=&quot;a&quot;>; &lt;li>;&lt;em>;涵盖的数据访问：&lt;/em>;DP 保证是否（仅）适用于单个训练运行或还涵盖超参数调整等。&lt;/li>;&lt;li>;&lt;em>;最终机制的输出&lt;/em>;：隐私保证涵盖的内容可以公开发布（例如，模型检查点、完整序列私有化梯度等）&lt;/li>;&lt;li>;&lt;em>;隐私单元&lt;/em>;：选择的“隐私单元”（示例级、用户级等）&lt;/li>;&lt;li>; DP“相邻”数据集的&lt;em>;邻接定义&lt;/em>;：描述相邻数据集的不同之处（例如，添加或删除、替换一、零出一）。 &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;隐私核算细节：&lt;/em>;提供核算细节，例如组成和放大，对于方法之间的正确比较很重要，应包括：&lt;ol 类型=&quot;a&quot;>; &lt;li>;使用的会计类型，例如&lt;a href=&quot;https://arxiv.org/abs/1702.07476&quot;>;Rényi DP&lt;/a>;会计、PLD会计等&lt;/ li>;&lt;li>;会计假设及其是否成立（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_regression#Maximum_likelihood-based_pa​​rameter_estimation&quot;>;Poisson&lt;/a>; 抽样假设用于隐私放大，但训练中使用了数据改组）。 &lt;/li>;&lt;li>;模型和调整过程的正式 DP 声明（例如，特定的&lt;em>;ε&lt;/em>;、&lt;em>;δ&lt;/em>;-DP 或&lt;em>;&lt;a href=&quot;https ://arxiv.org/pdf/1605.02065.pdf&quot;>;ρ-zCDP&lt;/a>;&lt;/em>; 值）。 &lt;/li>; &lt;/ol>; &lt;/li>;&lt;li>;&lt;em>;透明度和可验证性：&lt;/em>;如果可能，使用标准 DP 库的关键机制实现和会计组件的完整开源代码。 &lt;/li>; &lt;/ol>; &lt;p>; &lt;strong>;注意所有使用的组件：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;通常，DP-training 是一个直接应用 DP-SGD 或其他算法。然而，ML 模型中经常使用的一些组件或损失（例如，&lt;a href=&quot;https://www.baeldung.com/cs/contrastive-learning#:~:text=Contrastive%20Loss,and%20dissimilar% 20samples%20far%20apart.&quot;>;contrastive losses&lt;/a>;，&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;图形神经网络&lt;/a>;层）应该被检查以确保隐私不违反保证。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;开放式问题&lt;/h2>; &lt;p>; 虽然 DP-ML 是一个活跃的研究领域，但我们强调有改进空间的广泛领域。 &lt;/p>; &lt;p>; &lt;strong>;开发更好的会计方法&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;我们目前对 DP 训练的理解 &lt;em>;ε&lt;/em >;，&lt;em>;δ&lt;strong>; &lt;/strong>;&lt;/em>;保证&lt;strong>; &lt;/strong>;依赖于多种技术，例如 Rényi DP 组合和隐私放大。我们相信，现有算法的更好计算方法将证明 ML 模型的 DP 保证实际上比预期的要好。 &lt;/p>; &lt;p>; &lt;strong>;开发更好的算法：&lt;/strong>; &lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;使用梯度噪声注入进行 DP 训练的计算负担来自需要使用更大的批次并限制每个示例的灵敏度。开发可以使用较小批次或确定其他方式（除了每个示例裁剪）来限制灵敏度的方法将是 DP-ML 的突破。 &lt;/p>; &lt;p>; &lt;strong>;更好的优化技术&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;直接应用相同的 DP-SGD 配方被认为不是自适应的最佳选择优化器，因为为梯度私有化而添加的噪声可能会在学习率计算中累积。设计基于理论的 DP 自适应优化器仍然是一个活跃的研究课题。另一个潜在方向是更好地理解 DP 损失的表面，因为对于标准（非 DP）ML 模型，平坦区域已被证明可以&lt;a href=&quot;https://arxiv.org/abs/2010.01412&quot;>;泛化&lt;/ a>; &lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;>;更好&lt;/a>;。 &lt;/p>; &lt;p>; &lt;strong>;识别对噪声更稳健的架构&lt;/strong>;：&lt;/p>; &lt;p style=&quot;margin-left: 3em;&quot;>;有机会更好地了解我们是否需要在引入 DP 时调整现有模型的架构。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结论&lt;/h2>; &lt;p>; 我们的&lt;a href=&quot;https://arxiv.org /abs/2303.00654&quot;>;调查报告&lt;/a>; 总结了与 ML 模型 DP 相关的当前研究，并提供了有关如何实现最佳隐私与实用程序权衡的实用技巧。我们希望这项工作能够为想要有效地将 DP 应用于复杂 ML 模型的从业者提供参考点。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢 Hussein Hazimeh、Zheng Xu 和 Carson Denison 、H. Brendan McMahan、Sergei Vassilvitskii、Steve Chien 和 Abhradeep Thakurta、Badih Ghazi、Chiyuan Zhang 帮助准备这篇博文、论文和教程内容。感谢 John Guilyard 在这篇博文中制作图片，感谢 Ravi Kumar 的评论。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3261601760457492933/comments /default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models- differentially-private.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts /default/3261601760457492933&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3261601760457492933&quot; rel=&quot;self &quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/making-ml-models-differentially-private.html&quot; rel=&quot;alternate&quot; title= “使 ML 模型具有差异化隐私：最佳实践和公开挑战” type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161 &lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https:/ /img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent .com/img/b/R29vZ2xl/AVvXsEhqMCb-nNfoCncADD29b0YyxB4wN8bOhRph7pJBGzKyYc1bvdZ10VC3RkyUCJBopmvukjXJnZiJyiZ5ZIPyIW66ZfrYmBoJFrxx35T8OrL_HmNr4YIKzBS 5y6Ej24f1Mjsu-IqH5ECyzPBRKDRtoGgQhoKoOpILElqHDIGsn9SiI-7x58qc_nbtQ1JZ1A/s72-c/DPfy%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-234098392689663352&lt;/id>;&lt;已发布>;2023-05-18T14:08:00.000-07:00&lt;/published>;&lt;更新>;2023-05-18T14:08:39.899-07:00&lt;/更新>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;计算机视觉&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt; category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;联合视频和图像视觉转换器的稀疏视频管&lt;/ stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;由 Google 研究科学家 AJ Piergiovanni 和 Anelia Angelova 发布&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANDX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDej tp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s320/TubeViT%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; 视频理解是一个具有挑战性的问题，需要对空间信息（例如，对于场景中的对象，包括它们的位置和关系）和 &lt;a href=&quot;https://en.wikipedia.org/wiki /Temporal_information_retrieval#:~:text=Temporal%20information%20retrieval%20(T%2DIR,of%20the%20user%20information%20needs.&quot;>;视频中显示的活动或事件的时间信息&lt;/a>;。有很多视频理解应用程序和任务，例如&lt;a href=&quot;https://cloud.google.com/video-intelligence&quot;>;理解网络视频的语义内容&lt;/a>;和&lt;a href=&quot;https://ai .googleblog.com/2022/02/robot-see-robot-do.html&quot;>;机器人感知&lt;/a>;。但是，当前的工作，例如 &lt;a href=&quot;https://arxiv.org/abs/2103.15691 &quot;>;ViViT&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2102.05095&quot;>;TimeSFormer&lt;/a>;，密集处理视频并需要大量计算，尤其是模型大小加上视频长度和分辨率&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;重新思考视频 ViT：用于联合的稀疏视频管图像和视频学习&lt;/a>;”，将在 &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>; 上展示，我们介绍了一种简单的技术，可以将 &lt; href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer&lt;/a>; (ViT) 将图像编码器模型转换为使用稀疏视频管（视频样本的可学习视觉表示）的高效视频主干，以减少模型的计算需求。这种方法可以无缝地处理图像和视频，这使得它可以在训练期间利用图像和视频数据源。这种训练进一步使我们的稀疏管 ViT 模型能够将图像和视频骨干结合在一起，根据输入充当图像或视频骨干（或两者）的双重角色。我们证明该模型具有可扩展性，无需完全微调即可适应大型预训练 ViT，并在许多视频分类基准测试中取得了最先进的结果。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o 7xhi3SIiM0ir4fGyzbZH5PJ823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s860/image5.png &quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;219&quot; data-original-width=&quot;860&quot; src= “https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMB86kM4i14vA4RTy6ZcFbxosHQJ-jtMmsFrIsg32wAwrwkeeewMzC1tpTP-i5ukgLVq93yLq4ELx6o7xhi3SIiM0ir4fGyzbZH5P J823gaC3EHb3AdRgtM3SGkTrAnrDPnDc5qow_RVopG3H4xTjV7UmdRwqipD5tjRcPW3s9fwJVGJW7cb3Ybb-44A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;使用稀疏视频管对视频进行采样，并结合标准的 ViT 编码器，产生可以与图像输入无缝共享的高效视觉表示。&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;构建联合图像-视频主干&lt;/h2>; &lt;p>; 我们的稀疏管 ViT 使用标准的 ViT 主干，由一堆 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network. html&quot;>;Transformer&lt;/a>; 层，处理视频信息。以前的方法，例如 ViViT，对视频进行密集标记，然后应用&lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;>;分解注意力&lt;/a>;，即单独计算每个标记的注意力权重对于时间和空间维度。在标准的 ViT 架构中，self-attention 是在整个 token 序列上计算的。当使用视频作为输入时，令牌序列变得很长，这会使计算变慢。相反，在我们提出的方法中，视频是使用 &lt;em>;视频管&lt;/em>; 进行稀疏采样的，视频管是视频中各种形状和大小的 3D 可学习视觉表示（在下面有更详细的描述）。这些管用于使用&lt;a href=&quot;https://www.coursera.org/lecture/convolutional-neural-networks/strided-convolutions-wfUhx&quot;>;大时间跨度&lt;/a>;对视频进行稀疏采样，即，当管内核仅应用于视频中的几个位置，而不是每个像素。 &lt;/p>; &lt;p>; 通过对视频管进行稀疏采样，我们可以使用相同的全局自注意力模块，而不是像 ViViT 那样分解注意力。我们通过实验表明，由于未初始化的权重，添加分解注意力层可能会损害性能。 ViT 骨干网中的这种单一变压器层堆栈还可以更好地共享权重并提高性能。稀疏视频管采样是通过使用在固定网格上选择标记的大空间和时间步长来完成的。大步幅减少了整个网络中的令牌数量，同时仍然捕获空间和时间信息并实现所有令牌的高效处理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;稀疏视频管&lt;/h2>; &lt;p>; 视频管是基于 3D 网格的长方体，可以有不同的形状或类别，并以可以重叠的步幅和起始位置捕获不同的信息。在模型中，我们使用三种不同的管状来捕获：(1) 仅空间信息（产生一组 2D 图像块），(2) 长时间信息（在小空间区域），以及 (3) 空间信息和时间信息一样。仅捕获空间信息的管可以应用于图像和视频输入。捕获长时间信息或同时捕获时间和空间信息的管仅适用于视频输入。根据输入视频的大小，将三个管形多次应用于模型以生成标记。 &lt;/p>; &lt;p>; 固定位置嵌入，它捕获每个管相对于所有其他管的全局位置（包括任何步幅、偏移量等），应用于视频管。与之前学习的位置嵌入不同，这个固定的嵌入更好地实现了稀疏、重叠的采样。捕获管子的全局位置有助于模型了解每个管子的来源，这在管子重叠或从远处的视频位置采样时特别有用。接下来，将管特征连接在一起以形成一组 &lt;em>;N&lt;/em>; 标记。这些令牌由标准的 ViT 编码器处理。最后，我们应用注意力池将所有标记压缩为单个表示，并输入到全连接 (FC) 层以进行分类（例如，踢足球、游泳等）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP 1ef9IC3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr- Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s474/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;474&quot; data-original-width=&quot;346&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEaxZgk0mko5p-W6Tk3puhe9jq7seDBa8KAyxBYZZFaipGGx0kNmCq7mvrW-A--zFu-lz49-09utLfkP1ef9IC 3qh_wReFrSfYqhJgfbVwBefVlR_GVrvr-Xvsn4y_ib53SbigVnTHa2m8gw-4i0Ez9g1jqn03Gpv0JJPJTh4G3iSO3O-mX9JY8Z6HfQ/s16000/image2 .png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的视频 ViT 模型通过对稀疏视频管进行采样来工作从视频（显示在底部），使图像或视频输入中的一个或两个能够被无缝处理。这些管子具有不同的形状并捕捉不同的视频特征。管 1（&lt;strong>;黄色&lt;/strong>;）仅捕获空间信息，从而生成一组可应用于图像输入的 2D 色块。管 2（&lt;strong>;红色&lt;/strong>;）捕获时间信息和一些空间信息，管 3（&lt;strong>;绿色&lt;/strong>;）同样捕获时间和空间信息（即管的空间大小 &lt;em >;x&lt;/em>; 和&lt;em>;y&lt;/em>; 与帧数相同&lt;em>;t&lt;/em>;）。管 2 和 3 只能应用于视频输入。位置嵌入被添加到所有管特征中。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt; /div>; &lt;h2>;Scaling video ViTs&lt;/h2>; &lt;p>; 构建视频主干的过程需要大量计算，但我们的稀疏管 ViT 模型可以利用先前训练的图像主干，实现视频模型的计算高效缩放。由于图像主干可以适应视频主干，因此大型图像主干可以变成大型视频主干。更具体地说，可以将学习到的视频特征表示从一个小管 ViT 转移到一个大的预训练图像 ViT，并且只需几个步骤就可以用视频数据训练生成的模型，而不是从头开始进行完整的训练。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27 B35UE0sV0pYkcsIW0XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s899/image6 .png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;500&quot; data-original-width=&quot;899&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXSbl0uJ2k_vpBU1Ll0PF93KVqUFB_NAk_Wy4pCyUCF8-LYUDDB209aneR4c5Nx_1lCMk74sBNZthIDM2G27B35UE0sV0pYkcsIW0 XBtSdMagBkcdIrbELMxWCLurV2o-DqyeMyuYlKkc__KNQNvCasGCBEelQ5jmb5dMKpG-bhQ4xAPaLGi-8CP6kYw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;我们的方法能够以更有效的方式缩放稀疏管 ViT。具体来说，可以将来自小型视频 ViT（&lt;strong>;top 网络&lt;/strong>;）的视频特征转移到大型预训练图像 ViT（&lt;strong>;bottom 网络&lt;/strong>;），并进一步微调.这需要更少的训练步骤来实现大型模型的强大性能。这是有益的，因为从头开始训练大型视频模型可能会非常昂贵。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>; 我们使用&lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;>;动力学评估我们的稀疏管 ViT 方法-400&lt;/a>;（如下所示）、Kinetics-600 和 Kinetics-700 数据集，并将其性能与一长串现有方法进行比较。我们发现我们的方法优于所有先前的方法。重要的是，它优于所有在图像+视频数据集上联合训练的最先进方法。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhw style=&quot; margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgDFFJWQ_xEQcssI_Z0A4AbsuIzVDvttvYLhiVvt0QAivawWM27oUJNOo766KXnbf8vpcQvE2WKWhwvm00nJ10XWwjYxiFKRZEyQaJAgjEOfYBV bXO0Mio7Z1NgEczOR1fF36r-nBonPI7dfWnRQRREcu7a6XJ4lVWcY3T90jgruakIR75OlnmmOGhzBQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align&quot; : center;&quot;>;与流行的 Kinetics-400 视频数据集上的几个先前作品相比的性能。我们的稀疏管 ViT 优于最先进的方法。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 此外，我们在 &lt;a href=&quot;https ://developer.qualcomm.com/software/ai-datasets/something-something&quot;>;Something-Something V2&lt;/a>; 数据集，通常用于评估更多动态活动，并且还报告说它优于所有先前的状态-最先进的方法。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBX Lwc3b9fV0wTunsjBBQbEnNBVFwiD20X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s600/image4.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEusff29OZ6p1q03d-WXctJ86ceGEZKTz931L81Ah8MbVUqH8hk_jZBQ_Ob0lnR9fxZABknMJilDBXLwc3b9fV0wTunsjBBQbEnNBVFwiD2 0X25RUB5KJknjQtiwzMl2lTrga8XY-HDhhw6wDfdpNufEF4QIV-9FbTquOLebaO4P7YqSEa7fg0N6EqGw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;在 Something-Something V2 视频数据集上的表现。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line- height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;可视化一些已学习的内核&lt;/h2>; &lt;p>; 了解所提出的模型正在学习什么样的基本特征很有趣。我们在下面将它们可视化，显示了为图像和视频以及视频管共享的 2D 补丁。这些可视化显示投影层捕获的 2D 或 3D 信息。例如，在 2D 贴片中，检测到各种常见特征，如边缘和颜色，而 3D 管捕捉基本形状以及它们如何随时间变化。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s549/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;285&quot; data-original-width=&quot;549&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjThXmwg6c2miOzHrFodzuyhZ2inlHe3SeUfXfV-xpwYNsVj0bQD4QcLrRNM7MRZy4gqJ97PCeABtc-JioR5kkJXkEtd3KiealEKgmvwRu33j6xHVvvd9ylKPMUit6em59orJw4YHlMVMT-gfro4MXp1ESFclsZn3vZVkEqxxkSVsucgQZ-f5NXbImv6w/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualizations of patches and tubes learned the sparse tube ViT model. Top row are the 2D patches and the remaining two rows are snapshots from the learned video tubes. The tubes show each patch for the 8 or 4 frames to which they are applied.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusions&lt;/h2>; &lt;p>; We have presented a new sparse tube ViT, which can turn a ViT encoder into an efficient video model, and can seamlessly work with both image and video inputs. We also showed that large video encoders can be bootstrapped from small video encoders and image-only ViTs. Our approach outperforms prior methods across several popular video understanding benchmarks. We believe that this simple representation can facilitate much more efficient learning with input videos, seamlessly incorporate either image or video inputs and effectively eliminate the bifurcation of image and video models for future multimodal understanding. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is conducted by AJ Piergiovanni, Weicheng Kuo and Anelia Angelova, who are now at Google DeepMind. We thank Abhijit Ogale, Luowei Zhou, Claire Cui and our colleagues in Google Research for their helpful discussions, comments, and support.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/234098392689663352/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/234098392689663352&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/sparse-video-tubes-for-joint-video-and.html&quot; rel=&quot;alternate&quot; title=&quot;Sparse video tubes for joint video and image vision transformers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5vGQDyw66Z4hevP1JdKXnn8OCKudaDk2bacIPBkxtSYH1BgyNqXhK2QXOr-ANdX6MfoZp-jzVsETmH6kgy_Tvnii7ylDJEA-u5pJXareZRDyIBsRqguw2-AcDejtp3HXImtSLXKs_wUfMf8plDnpWzk1KGCgmm0U_j31qGh3rU4Cyv58HTpiocpyZiA/s72-c/TubeViT%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2749680625311121514&lt;/id>;&lt;published>;2023-05-18T10:12:00.006-07:00&lt;/published>;&lt;updated>;2023-05-18T16:14:50.453-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: PAIR&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Lucas Dixon and Michael Terry, co-leads, PAIR, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; PAIR (People + AI Research) first &lt;a href=&quot;https://blog.google/technology/ai/pair-people-ai-research-initiative/&quot;>;launched&lt;/a>; in 2017 with the belief that “AI can go much further — and be more useful to all of us — if we build systems with people in mind at the start of the process.” We continue to focus on making AI more understandable, interpretable, fun, and usable by more people around the world. It&#39;s a mission that is particularly timely given the emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chatbot&quot;>;chatbots&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, PAIR is part of the &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team within Google Research, and our work spans this larger research space: We advance &lt;a href=&quot;https://pair.withgoogle.com/research/&quot;>;foundational research&lt;/a>; on human-AI interaction (HAI) and machine learning (ML); we publish educational materials, including the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;PAIR Guidebook&lt;/a>; and &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;Explorables&lt;/a>; (such as the recent Explorable looking at &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;); and we develop software tools like the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; to help people understand and debug ML behaviors. Our inspiration this year is &quot;changing the way people think about what &lt;em>;THEY&lt;/em>; can do with AI.” This vision is inspired by the rapid emergence of generative AI technologies, such as large language models (LLMs) that power chatbots like &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>;, and new generative media models like Google&#39;s &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;, and &lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;. In this blog post, we review recent PAIR work that is changing the way we engage with AI. &lt;/p>; &lt;br />; &lt;h2>;Generative AI research&lt;/h2>; &lt;p>; Generative AI is creating a lot of excitement, and PAIR is involved in a range of related research, from &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;>;using language models to create generative agents&lt;/a>;&amp;nbsp;to studying how artists adopted generative image models like &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; and &lt;a href=&quot;https://sites.research.google/parti/&quot;>;Parti&lt;/a>;. These latter &quot;text-to-image&quot; models let a person input a text-based description of an image for the model to generate (eg, &quot;a gingerbread house in a forest in a cartoony style&quot;). In a forthcoming paper titled “&lt;a href=&quot;https://arxiv.org/abs/2303.12253&quot;>;The Prompt Artists&lt;/a>;” (to appear in &lt;a href=&quot;https://cc.acm.org/2023/&quot;>;Creativity and Cognition 2023&lt;/a>;), we found that users of generative image models strive not only to create beautiful images, but also to create unique, innovative styles. To help achieve these styles, some would even seek unique vocabulary to help develop their visual style. For example, they may visit architectural blogs to learn what domain-specific vocabulary they can adopt to help produce distinctive images of buildings. &lt;/p>; &lt;p>; We are also researching solutions to challenges faced by prompt creators who, with generative AI, are essentially programming without using a programming language. As an example, we developed &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;new methods&lt;/a>; for extracting semantically meaningful structure from natural language prompts. We have applied these structures to prompt editors to provide features similar to those found in other programming environments, such as semantic highlighting, autosuggest, and structured data views. &lt;/p>; &lt;p>; The growth of generative LLMs has also opened up new techniques to solve important long-standing problems. &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;Agile classifiers&lt;/a>; are one approach we&#39;re taking to leverage the semantic and syntactic strengths of LLMs to solve classification problems related to safer online discourse, such as nimbly blocking newer types of toxic language as quickly as it may evolve online. The big advance here is the ability to develop high quality classifiers from very small datasets — as small as 80 examples. This suggests a positive future for online discourse and better moderation of it: instead of collecting millions of examples to attempt to create universal safety classifiers for all use cases over months or years, more agile classifiers might be created by individuals or small organizations and tailored for their specific use cases, and iterated on and adapted in the time-span of a day (eg, to block a new kind of harassment being received or to correct unintended biases in models). As an example of their utility, these methods recently &lt;a href=&quot;https://www.aclweb.org/portal/content/semeval-2023-task-10-explainable-detection-online-sexism-edos&quot;>;won a SemEval competition&lt;/a>; to identify and explain sexism. &lt;/p>; &lt;p>; We&#39;ve also developed &lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>;new state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors and misbehaviours. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we also found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements on model accuracy. &lt;/p>; &lt;p>; Collectively, these methods are critical to help the scientific community improve generative models. They provide techniques for fast and effective content moderation and dialogue safety methods that help support creators whose content is the basis for generative models&#39; amazing outcomes. In addition, they provide direct tools to help debug model misbehavior which leads to better generation. &lt;/p>; &lt;br />; &lt;h2>;Visualization and education&lt;/h2>; &lt;p>; To lower barriers in understanding ML-related work, we regularly design and publish highly visual, interactive online essays, called &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, that provide accessible, hands-on ways to learn about key ideas in ML. For example, we recently published new AI Explorables on the topics of model confidence and unintended biases. In our latest Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” we discuss the problem with model confidence: models can sometimes be &lt;em>;very&lt;/em>; confident in their predictions… and yet completely incorrect. Why does this happen and what can be done about it? Our Explorable walks through these issues with interactive examples and shows how we can build models that have more appropriate confidence in their predictions by using a technique called &lt;a href=&quot;https://ai.googleblog.com/2021/11/model-ensembles-are-faster-than-you.html&quot;>;ensembling&lt;/a>;, which works by averaging the outputs of multiple models. Another Explorable, “&lt;a href=&quot;https://pair.withgoogle.com/explorables/saliency/&quot;>;Searching for Unintended Biases with Saliency&lt;/a>;”, shows how spurious correlations can lead to unintended biases — and how techniques such as saliency maps can detect some biases in datasets, with the caveat that it can be difficult to see bias when it&#39;s more subtle and sporadic in a training set. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s724/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;543&quot; data-original-width=&quot;724&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR designs and publishes AI Explorables, interactive essays on timely topics and new methods in ML research, such as “&lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;From Confidently Incorrect Models to Humble Ensembles&lt;/a>;,” which looks at how and why models offer incorrect predictions with high confidence, and how “ensembling” the outputs of many models can help avoid this.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Transparency and the Data Cards Playbook&lt;/h2>; &lt;p>; Continuing to advance our goal of helping people to understand ML, we promote transparent documentation. In the past, PAIR and Google Cloud developed &lt;a href=&quot;http://modelcards.withgoogle.com&quot;>;model cards&lt;/a>;. Most recently, we presented &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3531146.3533231&quot;>;our work on Data Cards&lt;/a>; at &lt;a href=&quot;https://facctconference.org/2022/&quot;>;ACM FAccT&#39;22&lt;/a>; and open-sourced the &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;, a joint effort with the &lt;a href=&quot;https://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot;>;Technology, AI, Society, and Culture team&lt;/a>; (TASC). &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;The Data Cards Playbook&lt;/a>; is a toolkit of participatory activities and frameworks to help teams and organizations overcome obstacles when setting up a transparency effort. It was created using an iterative, multidisciplinary approach rooted in the experiences of over 20 teams at Google, and comes with four modules: Ask, Inspect, Answer and Audit. These modules contain a variety of resources that can help you customize Data Cards to your organization&#39;s needs: &lt;/p>; &lt;ul>; &lt;li>;18 Foundations: Scalable frameworks that anyone can use on any dataset type &lt;/li>;&lt;li>;19 Transparency Patterns: Evidence-based guidance to produce high-quality Data Cards at scale &lt;/li>;&lt;li>;33 Participatory Activities: Cross-functional workshops to navigate transparency challenges for teams &lt;/li>;&lt;li>;Interactive Lab: Generate interactive Data Cards from markdown in the browser &lt;/li>; &lt;/ul>; &lt;p>; The Data Cards Playbook is accessible as a learning pathway for startups, universities, and other research groups. &lt;/p>; &lt;br />; &lt;h2>;Software Tools&lt;/h2>; &lt;p>; Our team thrives on creating tools, toolkits, libraries, and visualizations that expand access and improve understanding of ML models. One such resource is &lt;a href=&quot;https://knowyourdata.withgoogle.com/&quot;>;Know Your Data&lt;/a>;, which allows researchers to test a model&#39;s performance for various scenarios through interactive qualitative exploration of datasets that they can use to find and fix unintended dataset biases. &lt;/p>; &lt;p>; Recently, PAIR released a new version of the &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT) for model debugging and understanding. LIT v0.5 provides support for image and tabular data, new interpreters for tabular feature attribution, a &quot;Dive&quot; visualization for faceted data exploration, and performance improvements that allow LIT to scale to 100k dataset entries. You can find the &lt;a href=&quot;https://github.com/PAIR-code/lit/blob/main/RELEASE.md&quot;>;release notes&lt;/a>; and &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;code&lt;/a>; on GitHub. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s1920/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh2Go-gEqDQXrfnabvj4UR8GIDP958pe1NR3nGIrbAYphWvAWhh0fKYeGMEWnVGLyu-oE6qIDQ5SYTOdYZ3DAaGpKX475DhXci7YYTf-lToOZ82aZwDy2GRldR2UZf-vhS0O6jUQFtUPrR_3um17I_y9Q0L5z3-Q8p671xWfzl67krImGdRfh9loWsw2Q/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;PAIR&#39;s &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;Learning Interpretability Tool&lt;/a>; (LIT), an open-source platform for visualization and understanding of ML models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; PAIR has also contributed to &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a tool for rapid prototyping with LLMs using prompt programming. MakerSuite builds on our earlier research on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;PromptMaker&lt;/a>;, which won an honorable mention at &lt;a href=&quot;https://chi2022.acm.org/&quot;>;CHI 2022. &lt;/a>;MakerSuite lowers the barrier to prototyping ML applications by broadening the types of people who can author these prototypes and by shortening the time spent prototyping models from months to minutes.&amp;nbsp;&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s1216/reversedictionary.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;918&quot; data-original-width=&quot;1216&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT-pO6cN7vWIGTcUpC8mSbitlIn8zz0sYOyikxZbwU4-lNXoQw8FGP-LnN2wdJiTzpZsY2rl9Pw-CrZw7N2ZOZqwIM_BAWIQ0tWP_7FI88krRedQUXQ1cz_Jx_WBottMukv-9rIzUJFlFMzgL9dVTsDycSd7L2TbYSsmUCp-xIMif0rhtASefVZZXOSQ/s16000/reversedictionary.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A screenshot of MakerSuite, a tool for rapidly prototyping new ML models using prompt-based programming, which grew out of PAIR&#39;s prompt programming research.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Ongoing work&lt;/h2>; &lt;p>; As the world of AI moves quickly ahead, PAIR is excited to continue to develop new tools, research, and educational materials to help change the way people think about what THEY can do with AI. &lt;/p>; &lt;p>; For example, we recently conducted &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;an exploratory study&lt;/a>; with five designers (presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI&lt;/a>; this year) that looks at how people with no ML programming experience or training can use prompt programming to quickly prototype functional user interface mock-ups. This prototyping speed can help inform designers on how to integrate ML models into products, and enables them to conduct user research sooner in the product design process. &lt;/p>; &lt;p>; Based on this study, PAIR&#39;s researchers built &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser&lt;/a>;, a design tool plugin for authoring LLM-infused mock-ups. The plug-in introduces two novel LLM-interactions: input-output, which makes content interactive and dynamic, and frame-change, which directs users to different frames depending on their natural language input. The result is more tightly integrated UI and ML prototyping, all within a single interface. &lt;/p>; &lt;p>; Recent advances in AI represent a significant shift in how easy it is for researchers to customize and control models for their research objectives and goals.These capabilities are transforming the way we think about interacting with AI, and they create lots of new opportunities for the research community. PAIR is excited about how we can leverage these capabilities to make AI easier to use for more people. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;em>;Thanks to everyone in PAIR, to Reena Jana and to all of our collaborators. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2749680625311121514/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2749680625311121514&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: PAIR&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjDUj2YMMcVzCHuAQ9TYS3IkauQbh9Tti-nZl6LZOMwwieYdyJM7DHShetIblRhW0hvTaPvDmK2f0SDu9XcLCVg0780I3GnYU0FrA27s0-MiQXQ5xPUCGqTSx-7KNJLwrlczkco2Ql5KzMeVHNXBl2oeaN0gMCPi7A2Bpc6eDpzjwZIHoNMiWLfQqL2vQ/s72-c/image3.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2478354033845809100&lt;/id>;&lt;published>;2023-05-16T12:22:00.001-07:00&lt;/published>;&lt;updated>;2023-05-16T12:23:42.794-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Using reinforcement learning for dynamic planning in open-ended conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Deborah Cohen, Staff Research Scientist, and Craig Boutilier, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s1650/rlxtalk.png&quot; style=&quot;display: none;&quot; />; &lt;p>; As virtual assistants become ubiquitous, users increasingly interact with them to learn about new topics or obtain recommendations and expect them to deliver capabilities beyond narrow dialogues of one or two turns. Dynamic planning, namely the capability to look ahead and replan based on the flow of the conversation, is an essential ingredient for the making of engaging conversations with the deeper, open-ended interactions that users expect. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; While large language models (LLMs) are now beating state-of-the-art approaches in many natural language processing benchmarks, they are typically trained to output the next best response, rather than planning ahead, which is required for multi-turn interactions. However, in the past few years, &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;>;reinforcement learning&lt;/a>; (RL) has delivered incredible results addressing specific problems that involve dynamic planning, such as winning games and protein folding. &lt;/p>; &lt;p>; Today, we are sharing our recent advances in &lt;a href=&quot;https://arxiv.org/abs/2208.02294&quot;>;dynamic planning for human-to-assistant conversations&lt;/a>;, in which we enable an assistant to plan a multi-turn conversation towards a goal and adapt that plan in real-time by adopting an RL-based approach. Here we look at how to improve long interactions by applying RL to compose answers based on information extracted from reputable sources, rather than relying on content generated by a language model. We expect that future versions of this work could combine LLMs and RL in multi-turn dialogues. The deployment of RL “in the wild” in a large-scale dialogue system proved a formidable challenge due to the modeling complexity, tremendously large state and action spaces, and significant subtlety in designing reward functions. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;What is dynamic planning?&lt;/h2>; &lt;p>; Many types of conversations, from gathering information to offering recommendations, require a flexible approach and the ability to modify the original plan for the conversation based on its flow. This ability to shift gears in the middle of a conversation is known as &lt;em>;dynamic planning&lt;/em>;, as opposed to &lt;em>;static planning&lt;/em>;, which refers to a more fixed approach. In the conversation below, for example, the goal is to engage the user by sharing interesting facts about cool animals. To begin, the assistant steers the conversation to sharks via a sound quiz. Given the user&#39;s lack of interest in sharks, the assistant then develops an updated plan and pivots the conversation to sea lions, lions, and then cheetahs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s908/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;908&quot; data-original-width=&quot;890&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3M0EGm5oBCvMasXPP1Pf6CJ4pnFdJOGFLZcrwSb_A3XDCAOtfhMA1-80J3sohkwBMcqRUgstHS6E8NRLzfr3W4Wf34tTbfpt1VRUGBFKGvQ0yBjOWrq9XVhmJH7PxMc5SH3MOyTopbJyztKs83EHw06Ou84fObUMf5U78KRbivZSLOGJDxT4WJIhVdg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The assistant dynamically modifies its original plan to talk about sharks and shares facts about other animals.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic composition&lt;/h2>; &lt;p>; To cope with the challenge of conversational exploration, we separate the generation of assistant responses into two parts: 1) &lt;em>;content generation&lt;/em>;, which extracts relevant information from reputable sources,&lt;em>; &lt;/em>;and 2) &lt;em>;flexible composition&lt;/em>; of such content into assistant responses. We refer to this two-part approach as &lt;em>;&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;dynamic composition&lt;/a>;&lt;/em>;. Unlike LLM methods, this approach gives the assistant the ability to fully control the source, correctness, and quality of the content that it may offer. At the same time, it can achieve flexibility via a learned dialogue manager that selects and combines the most appropriate content. &lt;/p>; &lt;p>; In an earlier paper, “&lt;a href=&quot;https://research.google/pubs/pub48892/&quot;>;Dynamic Composition for Conversational Domain Exploration&lt;/a>;”, we describe a novel approach which consists of: (1) a collection of content providers, which offer candidates from different sources, such as news snippets, &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_graph&quot;>;knowledge graph&lt;/a>; facts, and questions; (2) a dialogue manager; and (3) a sentence fusion module. Each assistant response is incrementally constructed by the dialogue manager, which selects candidates proposed by the content providers. The selected sequence of utterances is then fused into a cohesive response. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Dynamic planning using RL&lt;/h2>; &lt;p>; At the core of the assistant response composition loop is a dialogue manager trained using &lt;em>;off-policy RL&lt;/em>;, namely an algorithm that evaluates and improves a policy that is different from the policy used by the agent (in our case, the latter is based on a supervised model). Applying RL to dialogue management presents several challenges, including a large state space (as the state represents the conversation state, which needs to account for the whole conversation history) and an effectively unbounded action space (that may include all existing words or sentences in natural language). &lt;/p>; &lt;p>; We address these challenges using a novel RL construction. First, we leverage powerful supervised models — specifically, &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;>;recurrent neural networks&lt;/a>; (RNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; — to provide a succinct and effective dialogue state representation. These state encoders are fed with the dialogue history, composed of a sequence of user and assistant turns, and output a representation of the dialogue state in the form of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_space&quot;>;latent vector&lt;/a>;. &lt;/p>; &lt;p>; Second, we use the fact that a relatively small set of reasonable candidate utterances or actions can be generated by content providers at each conversation turn, and limit the action space to these. Whereas the action space is typically fixed in RL settings, because all states share the same action space, ours is a non-standard space in which the candidate actions may differ with each state, since content providers generate different actions depending on the dialogue context. This puts us in the realm of stochastic action sets, a framework that formalizes cases where the set of actions available in each state is governed by an exogenous stochastic process, which we address using &lt;a href=&quot;https://www.ijcai.org/proceedings/2018/0650.pdf&quot;>;Stochastic Action Q-Learning&lt;/a>;, a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Q-learning&quot;>;Q-learning&lt;/a>; approach. Q-learning is a popular off-policy RL algorithm, which does not require a model of the environment to evaluate and improve the policy. We trained our model on a corpus of crowd-compute–rated conversations obtained using a supervised dialogue manager. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s1500/image4.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCgBVQFNQRY6kMBleSNB0ySV458BM9UIn9uECbmOvYLTg30sLaBkYhHAsEgw-UI3jMuTowM31ox2Anh0SLB6I8hJLwPNCZR0bDInV-f2g9jr-EbAKH2KeEocq8iy-9nMVsQd1iZ8Dkxk9Ne6d8Yidw9VSRGN4EzvWXHwMcsr6H6Oau1ZVqAI5Ouhe1jQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given the current dialogue history and a new user query, content providers generate candidates from which the assistant selects one. This process runs in a loop, and at the end the selected utterances are fused into a cohesive response.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Reinforcement learning model evaluation&lt;/h2>; &lt;p>; We compared our RL dialogue manager with a launched supervised transformer model in an experiment using Google Assistant, which conversed with users about animals. A conversation starts when a user triggers the experience by asking an animal-related query (eg, “How does a lion sound?”). The experiment was conducted using an &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B testing&lt;/a>; protocol, in which a small percentage of Assistant users were randomly sampled to interact with our RL-based assistant while other users interacted with the standard assistant. &lt;/p>; &lt;p>; We found that the RL dialogue manager conducts longer, more engaging conversations. It increases conversation length by 30% while improving user engagement metrics. We see an increase of 8% in cooperative responses to the assistant&#39;s questions — eg, “Tell me about lions,” in response to “Which animal do you want to hear about next?” Although there is also a large increase in nominally “non-cooperative” responses (eg, “No,” as a reply to a question proposing additional content, such as “Do you want to hear more?”), this is expected as the RL agent takes more risks by asking pivoting questions. While a user may not be interested in the conversational direction proposed by the assistant (eg, pivoting to another animal), the user will often continue to engage in a dialogue about animals. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s552/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;552&quot; data-original-width=&quot;534&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLPBoC8S32dyGFimV_WFUxSmnxlby5K86Mphcsk7hdxfP32_5s-LK5vljPgSjCC_e_nMu5YUh8LPgk5s6Gd5PVSOE8Bw9-WZHMZEYLUNsdArCSEpMTW1ACUrmxIezcrSwMiOYvt_6QplYDorVBEscLzDGBkNhusTaXYstyhPpE6xpAyK2Bj6c0deGCOQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;From the non-cooperative user response in the 3rd turn (“No.”) and the query “Make a dog sound,” in the 5th turn, the assistant recognizes that the user is mostly interested in animal sounds and modifies its plan, providing sounds and sound quizzes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In addition, some user queries contain explicit positive (eg, “Thank you, Google,” or “I&#39;m happy.”) or negative (eg, “Shut up,” or “Stop.”) feedback. While an order of magnitude fewer than other queries, they offer a direct measure of user (dis)satisfaction. The RL model increases explicit positive feedback by 32% and reduces negative feedback by 18%. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Learned dynamic planning characteristics and strategies&lt;/h2>; &lt;p>; We observe several characteristics of the (unseen) RL plan to improve user engagement while conducting longer conversations. First, the RL-based assistant ends 20% more turns in questions, prompting the user to choose additional content. It also better harnesses content diversity, including facts, sounds, quizzes, yes/no questions, open questions, etc. On average, the RL assistant uses 26% more distinct content providers per conversation than the supervised model. &lt;/p>; &lt;p>; Two observed RL planning strategies are related to the existence of sub-dialogues with different characteristics. Sub-dialogues about animal sounds are poorer in content and exhibit entity pivoting at every turn (ie, after playing the sound of a given animal, we can either suggest the sound of a different animal or quiz the user about other animal sounds). In contrast, sub-dialogues involving animal facts typically contain richer content and have greater conversation depth. We observe that RL favors the richer experience of the latter, selecting 31% more fact-related content. Lastly, when restricting analysis to fact-related dialogues, the RL assistant exhibits 60% more focus-pivoting turns, that is, conversational turns that change the focus of the dialogue. &lt;/p>; &lt;p>; Below, we show two example conversations, one conducted by the supervised model (left) and the second by the RL model (right), in which the first three user turns are identical. With a supervised dialogue manager, after the user declined to hear about “today&#39;s animal”, the assistant pivots back to animal sounds to maximize the immediate user satisfaction. While the conversation conducted by the RL model begins identically, it exhibits a different planning strategy to optimize the overall user engagement, introducing more diverse content, such as fun facts. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s1539/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1539&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9TvsvZrBGNK14oSUk8CHlXv0Yh9wqQOjPlVAfSF4KsdB_3_Y5Y_YShxyL7sr6NZe5A1eITfhUahUNsTl3SZHRabpIfQorSWYUeOqXGqkbCvUriCLsoZqFCfwX9wpgrlanLp_bzDE9tN6d4_HbHH_CiGuOxL55q_CiyMoWetsLY5Ow7GHLJM554i-fkw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the left conversation, conducted by the supervised model, the assistant maximizes the immediate user satisfaction. The right conversation, conducted by the RL model, shows different planning strategies to optimize the overall user engagement.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Future research and challenges&lt;/h2>; &lt;p>; In the past few years, LLMs trained for language understanding and generation have demonstrated impressive results across multiple tasks, including dialogue. We are now exploring the use of an RL framework to empower LLMs with the capability of dynamic planning so that they can dynamically plan ahead and delight users with a more engaging experience. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias, Idan Szpektor and Gal Elidan. We would like to thank: Roee Aharoni, Moran Ambar, John Anderson, Ido Cohn, Mohammad Ghavamzadeh, Lotem Golany, Ziv Hodak, Adva Levin, Fernando Pereira, Shimi Salant, Shachar Shimoni, Ronit Slyper, Ariel Stolovich, Hagai Taitelbaum, Noam Velan, Avital Zipori and the CrowdCompute team led by Ashwin Kakarla. We thank Sophie Allweis for her feedback on this blogpost and Tom Small for the visualization.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/2478354033845809100/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2478354033845809100&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/using-reinforcement-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Using reinforcement learning for dynamic planning in open-ended conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1pRX6ly5Tgckfk47u8_KxBasWJhuFoQ4SzbSmiK3SrQOKn8jXr3RHvb32lGlBksKl3-wXCwU8UnEhRp6YztIREY874N4i1289xQKHlO64QZqD9tQBiBQHMW-S5B4u5mSaBw6lgbuTEVplIrBfaOIcX-7-YG6D0lGzSOMN7r9umjZwMoE_1t1gcsEOZA/s72-c/rlxtalk.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-14165165832846745&lt;/id>;&lt;published>;2023-05-15T13:59:00.001-07:00&lt;/published>;&lt;updated>;2023-05-15T14:40:42.386-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Larger language models do in-context learning differently&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jerry Wei, Student Researcher, and Denny Zhou, Principal Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s800/SULICL.png&quot; style=&quot;display: none;&quot; />; &lt;p>; There have recently been tremendous advances in language models, partly because they can perform tasks with strong performance via &lt;a href=&quot;https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)&quot;>;in-context learning&lt;/a>; (ICL), a process whereby models are prompted with a few examples of input-label pairs before performing the task on an unseen evaluation example. In general, models&#39; success at in-context learning is enabled by: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;ul>; &lt;li>;Their use of semantic prior knowledge from pre-training to predict labels while following the format of in-context examples (eg, seeing examples of movie reviews with “positive sentiment” and “negative sentiment” as labels and performing &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>; using prior knowledge). &lt;/li>;&lt;li>;Learning the input-label mappings in context from the presented examples (eg, finding a pattern that positive reviews should be mapped to one label, and negative reviews should be mapped to a different label). &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.03846&quot;>;Larger language models do in-context learning differently&lt;/a>;”, we aim to learn about how these two factors (semantic priors and input-label mappings) interact with each other in ICL settings, especially with respect to the scale of the language model that&#39;s used. We investigate two settings to study these two factors — ICL with flipped labels (flipped-label ICL) and ICL with semantically-unrelated labels (SUL-ICL). In flipped-label ICL, labels of in-context examples are flipped so that semantic priors and input-label mappings disagree with each other. In SUL-ICL, labels of in-context examples are replaced with words that are semantically unrelated to the task presented in-context. We found that overriding prior knowledge is an emergent ability of model scale, as is the ability to learn in-context with semantically-unrelated labels. We also found that instruction tuning strengthens the use of prior knowledge more than it increases the capacity to learn input-label mappings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s625/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;625&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJUQZqnsXprXhMMpPVS3eqvH57D4U4G9xvmXH3rQi1KmIQ45f3a5621hbc2T_gW6ELMUv29ZxqKxfZLVlt8rMRUDfLK2hxPoIpRR-H2D7n_ZUXX7uKunqXFb_x2GOgQdbPsl0JeiSagA0VjP4N9hT-RLHDZbVz7lg-prtnONvkShF7uHGrmSAVURsveA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of flipped-label ICL and semantically-unrelated label ICL (SUL-ICL), compared with regular ICL, for a sentiment analysis task. Flipped-label ICL uses flipped labels, forcing the model to override semantic priors in order to follow the in-context examples. SUL-ICL uses labels that are not semantically related to the task, which means that models must learn input-label mappings in order to perform the task because they can no longer rely on the semantics of natural language labels.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiment design&lt;/h2>; &lt;p>; For a diverse dataset mixture, we experiment on seven &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP) tasks that have been widely used: &lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;sentiment analysis&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/SetFit/subj&quot;>;subjective/objective classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/trec&quot;>;question classification&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/glue/viewer/qqp/validation&quot;>;duplicated-question recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/super_glue/viewer/rte/test&quot;>;entailment recognition&lt;/a>;, &lt;a href=&quot;https://huggingface.co/datasets/financial_phrasebank&quot;>;financial sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://huggingface.co/datasets/ethos&quot;>;hate speech detection&lt;/a>;. We test five language model families, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;>;Flan-PaLM&lt;/a>;, &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html&quot;>;GPT-3&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;>;InstructGPT&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2107.03374&quot;>;Codex&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Flipped labels&lt;/h2>; &lt;p>; In this experiment, labels of in-context examples are flipped, meaning that prior knowledge and input-label mappings disagree (eg, sentences containing positive sentiment labeled as “negative sentiment”), thereby allowing us to study whether models can override their priors. In this setting, models that are able to override prior knowledge and learn input-label mappings in-context should experience a decrease in performance (since ground-truth evaluation labels are not flipped). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s1036/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1036&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSlRhZycuNyJePf45hjI8TYSDe5Xbn1Uj9R0DobtZLRy8nTScnl-V7f-Zti6qPpprSHLOac5HqavO8JWg1fy6_0VisA40LVyXAv9MzHQm3Xvkr9WyuktlOqbfga3uaVOCVlhoxGTOZ1qWWznWIvf6NcMC1UgmnDUVsgy9qgu6ncGbUV8J22AacWHiKXg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The ability to override semantic priors when presented with flipped in-context example labels emerges with model scale. Smaller models cannot flip predictions to follow flipped labels (performance only decreases slightly), while larger models can do so (performance decreases to well below 50%).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We found that when no labels are flipped, larger models have better performance than smaller models (as expected). But when we flip more and more labels, the performance of small models stays relatively flat, but large models experience large performance drops to well-below random guessing (eg, 90% → 22.5% for code-davinci-002). &lt;/p>; &lt;p>; These results indicate that large models can override prior knowledge from pre-training when contradicting input-label mappings are presented in-context. Small models can&#39;t do this, making this ability an emergent phenomena of model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Semantically-unrelated labels&lt;/h2>; &lt;p>; In this experiment, we replace labels with semantically-irrelevant ones (eg, for sentiment analysis, we use “foo/bar” instead of “negative/positive”), which means that the model can only perform ICL by learning from input-label mappings. If a model mostly relies on prior knowledge for ICL, then its performance should decrease after this change since it will no longer be able to use semantic meanings of labels to make predictions. A model that can learn input–label mappings in-context, on the other hand, would be able to learn these semantically-unrelated mappings and should not experience a major drop in performance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s1049/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;430&quot; data-original-width=&quot;1049&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEisGHKYzD0d4B8ynJIhqpU6wedtvu37pMJY7Dd02xWELTodKiDCcXWfu0kQ926XJJWheXRbA7XMMAYP1C3PpY7b9X0N-yfLzVcKwI5nSE4rjOdH1UKBLs_e_e4wF8KXQ7ogywNXn5htE-bWeSde7FbJ9JYLSbLVrll3YTfgIQMTKUtdKAMtZ-Zo2WR1hQ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Small models rely more on semantic priors than large models do, as indicated by the greater decrease in performance for small models than for large models when using semantically-unrelated labels (ie, targets) instead of natural language labels. For each plot, models are shown in order of increasing model size (eg, for GPT-3 models, &lt;em>;a&lt;/em>; is smaller than &lt;em>;b&lt;/em>;, which is smaller than &lt;em>;c&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Indeed, we see that using semantically-unrelated labels results in a greater performance drop for small models. This suggests that smaller models primarily rely on their semantic priors for ICL rather than learning from the presented input-label mappings. Large models, on the other hand, have the ability to learn input-label mappings in-context when the semantic nature of labels is removed. &lt;/p>; &lt;p>; We also find that including more in-context examples (ie, exemplars) results in a greater performance improvement for large models than it does for small models, indicating that large models are better at learning from in-context examples than small models are. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s825/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;417&quot; data-original-width=&quot;825&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj747MlBNxtirznhOm0RR4P1kEpk97WOclz3N3TMMTBUBgcciB3MuHopHH0UT4I5qOQPUJ1O8n0M0zRr9sePxpecLSnoyb9foa6Ho5qh_xWtkSzeXyTg-VTRL1hTAc_EIXWewymn6vsOCR96SOidHMHxsu-AjsPTFEVwpNT5HZ954zE2AVIuL0qCXmrvw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the SUL-ICL setup, larger models benefit more from additional examples than smaller models do.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Instruction tuning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;>;Instruction tuning&lt;/a>; is a popular technique for improving model performance, which involves tuning models on various NLP tasks that are phrased as instructions (eg, “Question: What is the sentiment of the following sentence, &#39;This movie is great.&#39; Answer: Positive”). Since the process uses natural language labels, however, an open question is whether it improves the ability to learn input-label mappings or whether it strengthens the ability to recognize and apply semantic prior knowledge. Both of these would lead to an improvement in performance on standard ICL tasks, so it&#39;s unclear which of these occur. &lt;/p>; &lt;p>; We study this question by running the same two setups as before, only this time we focus on comparing standard language models (specifically, PaLM) with their instruction-tuned variants (Flan-PaLM). &lt;/p>; &lt;p>; First, we find that Flan-PaLM is better than PaLM when we use semantically-unrelated labels. This effect is very prominent in small models, as Flan-PaLM-8B outperforms PaLM-8B by 9.6% and almost catches up to PaLM-62B. This trend suggests that instruction tuning strengthens the ability to learn input-label mappings, which isn&#39;t particularly surprising. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s573/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;301&quot; data-original-width=&quot;573&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgeFBd7p-muxUg1CTDEkDx4VzH4IG1wtn2z-qW3P0dwSiUu_GS-BQW0RSG-WveJl89MwovvYMQL5UN6Ldze6laCzCxSHhkn3uxf5kuzLmFgtU9hPvstPq-4YmdWWoxuHMnazQCOs-F9faQd-AMtxg6zZsxTD6ZGCm42iV8JZrbAWKrA526dHyppLOQR_A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned language models are better at learning input–label mappings than pre-training–only language models are.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More interestingly, we saw that Flan-PaLM is actually worse than PaLM at following flipped labels, meaning that the instruction tuned models were unable to override their prior knowledge (Flan-PaLM models don&#39;t reach below random guessing with 100% flipped labels, but PaLM models without instruction tuning can reach 31% accuracy in the same setting). These results indicate that instruction tuning must increase the extent to which models rely on semantic priors when they&#39;re available. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s1016/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;1016&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUB8OahGEvoe32aO4ZRaTg0rFSsC69cso4eJS1FIV4BOTTLJi93HQ3e4lUnPDJcea98tBsVJnjh8-CgZ10lBtSl1UlNiY6-hHotYq_ow2TEUmcb1tj9NaAFRWxaDTYO1_K0y6bgTg5BvNdihcvHsd78zk3Mn4jwFic5gdEvYn5Ol-JIRmYehgoHtfrg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Instruction-tuned models are worse than pre-training–only models at learning to override semantic priors when presented with flipped labels in-context.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Combined with the previous result, we conclude that although instruction tuning improves the ability to learn input-label mappings, it strengthens the usage of semantic prior knowledge more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We examined the extent to which language models learn in-context by utilizing prior knowledge learned during pre-training versus input-label mappings presented in-context. &lt;/p>; &lt;p>; We first showed that large language models can learn to override prior knowledge when presented with enough flipped labels, and that this ability emerges with model scale. We then found that successfully doing ICL using semantically-unrelated labels is another emergent ability of model scale. Finally, we analyzed instruction-tuned language models and saw that instruction tuning improves the capacity to learn input-label mappings but also strengthens the use of semantic prior knowledge even more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; These results underscore how the ICL behavior of language models can change depending on their scale, and that larger language models have an emergent ability to map inputs to many types of labels, a form of reasoning in which input-label mappings can potentially be learned for arbitrary symbols. Future research could help provide insights on why these phenomena occur with respect to model scale. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. We would like to thank Sewon Min and our fellow collaborators at Google Research for their advice and helpful discussions.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/14165165832846745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/14165165832846745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html&quot; rel=&quot;alternate&quot; title=&quot;Larger language models do in-context learning differently&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQS0-Pkd_JiN7brkU6zV0-FEisuUcnKs0I56t37HVYKMgKStmwNgLREYn5CfURvW-lMvKbHU4cEV9elAu9qe4-M_FvveTlvBQHezbksTlH3YfOAk4TyJiXYiGBW_95RGKIW-JyjAQiC0Zd4VIjZrCSIm1PEBqrIAqbiEklluNunTOMhX_7CU9Degbwqg/s72-c/SULICL.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-4751112643381771806&lt;/id>;&lt;published>;2023-05-15T10:16:00.003-07:00&lt;/published>;&lt;updated>;2023-05-15T10:24:09.250-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Consensus and subjectivity of skin tone annotation for ML fairness&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Candice Schumann, Software Engineer, and Gbolahan O. Olanubi, User Experience Researcher, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s888/MST-E.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Skin tone is an observable characteristic that is subjective, perceived differently by individuals (eg, depending on their location or culture) and thus is complicated to annotate. That said, the ability to reliably and accurately annotate skin tone is highly important in computer vision. This became apparent in 2018, when the &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; study highlighted that computer vision systems struggled to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. The study highlights the importance for computer researchers and practitioners to evaluate their technologies across the full range of skin tones and at intersections of identities. Beyond evaluating model performance on skin tone, skin tone annotations enable researchers to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3375627.3375832&quot;>;measure diversity&lt;/a>; and representation in &lt;a href=&quot;https://arxiv.org/abs/1901.10265&quot;>;image retrieval systems&lt;/a>;, &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;dataset collection&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2202.04053&quot;>;image generation&lt;/a>;. For all of these applications, a collection of meaningful and inclusive skin tone annotations is key. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s1196/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLK0qZxbNK0Yaxg6SkLiFYUo6dQCaxqrQcM0WPKW0FuI3o-DvKwH4uyyGA1QxDHP4q572W_J0cPg5aojjbsLV8yskNPeOwhIb7_c_5LsDfOqa0hUuMUuKPKFyuHNOPzPXuZTNHBdZ27AwA6UDRoIq1ehFpx0nn7jgIeKh1KRuCkePg9HSgwYbYC7Vp/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Monk Skin Tone (MST) Scale See more at &lt;a href=&quot;http://skintone.google&quot;>;skintone.google&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Last year, in a step toward more inclusive computer vision systems, Google&#39;s &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; team in Research partnered with &lt;a href=&quot;https://sociology.fas.harvard.edu/people/ellis-monk&quot;>;Dr. Ellis Monk&lt;/a>; to openly release the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone&lt;/a>; (MST) Scale, a skin tone scale that captures a broad spectrum of skin tones. In comparison to an industry standard scale like the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK481857/table/chapter6.t1/&quot;>;Fitzpatrick Skin-Type Scale&lt;/a>; designed for dermatological use, the MST offers a more inclusive representation across the range of skin tones and was designed for a broad range of applications, including computer vision. &lt;/p>; &lt;p>; Today we&#39;re announcing the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;Monk Skin Tone Examples&lt;/a>; (MST-E) dataset to help practitioners understand the MST scale and train their human annotators. This dataset has been made publicly available to enable practitioners everywhere to create more consistent, inclusive, and meaningful skin tone annotations. Along with this dataset, we&#39;re providing a set of recommendations, noted below, around the MST scale and MST-E dataset so we can all create products that work well for all skin tones. &lt;/p>; &lt;p>; Since we launched the MST, we&#39;ve been using it to improve Google&#39;s computer vision systems to make &lt;a href=&quot;https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/&quot;>;equitable image tools for everyone&lt;/a>; and to &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;improve representation of skin tone in Search&lt;/a>;. Computer vision researchers and practitioners outside of Google, like the curators of &lt;a href=&quot;https://ai.facebook.com/blog/casual-conversations-v2-dataset-measure-fairness/&quot;>;MetaAI&#39;s Casual Conversations&lt;/a>; dataset, are recognizing the value of MST annotations to provide additional insight into diversity and representation in datasets. Incorporation into widely available datasets like these are essential to give everyone the ability to ensure they are building more inclusive computer vision technologies and can test the quality of their systems and products across a wide range of skin tones. &lt;/p>; &lt;p>; Our team has continued to conduct research to understand how we can continue to advance our understanding of skin tone in computer vision. One of our core areas of focus has been skin tone annotation, the process by which human annotators are asked to review images of people and select the best representation of their skin tone. MST annotations enable a better understanding of the inclusiveness and representativeness of datasets across a wide range of skin tones, thus enabling researchers and practitioners to evaluate quality and fairness of their datasets and models. To better understand the effectiveness of MST annotations, we&#39;ve asked ourselves the following questions: &lt;/p>; &lt;ul>; &lt;li>;How do people think about skin tone across geographic locations? &lt;/li>;&lt;li>;What does global consensus of skin tone look like? &lt;/li>;&lt;li>;How do we effectively annotate skin tone for use in inclusive machine learning (ML)? &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;The MST-E dataset&lt;/h2>; &lt;p>; The &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>; contains 1,515 images and 31 videos of 19 subjects spanning the 10 point MST scale, where the subjects and images were sourced through &lt;a href=&quot;https://tonl.co/&quot;>;TONL&lt;/a>;, a stock photography company focusing on diversity. The 19 subjects include individuals of different ethnicities and gender identities to help human annotators decouple the concept of skin tone from race. The primary goal of this dataset is to enable practitioners to train their human annotators and test for consistent skin tone annotations across various environment capture conditions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/s1072/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;950&quot; data-original-width=&quot;1072&quot; height=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2A4I4TKB-NEDSXI_aK4QoQaqenwJCaPb6BfDYXkPDaW9YR_QUuLH8kyLsAk8jVnoOtOT5bhsey_4oORYrUqZH5UOUKx8uQYyJlnfx1YVT6XrHDmvl9p1dYoCBPeAGrj99mFYqmKRG6PxwxKAMjFiagoVVyeUKjbfaed33zmSX69-xkZ9SND-YWUUF/w640-h568/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MST-E image set contains 1,515 images and 31 videos featuring 19 models taken under various lighting conditions and facial expressions. Images by TONL. Copyright TONL.CO 2022 ALL RIGHTS RESERVED. Used with permission.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; All images of a subject were collected in a single day to reduce variation of skin tone due to seasonal or other temporal effects. Each subject was photographed in various poses, facial expressions, and lighting conditions. In addition, Dr. Monk annotated each subject with a skin tone label and then selected a “golden” image for each subject that best represents their skin tone. In our research we compare annotations made by human annotators to those made by Dr. Monk, an academic expert in social perception and inequality. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Terms of use&lt;/h3>; &lt;p>; Each model selected as a subject provided consent for their images and videos to be released. TONL has given permission for these images to be released as part of MST-E and used for research or human-annotator-training purposes only. The images are not to be used to train ML models. &lt;/p>; &lt;br />; &lt;h2>;Challenges with forming consensus of MST annotations&lt;/h2>; &lt;p>; Although skin tone is easy for a person to see, it can be challenging to systematically annotate across multiple people due to issues with technology and the complexity of human social perception. &lt;/p>; &lt;p>; On the technical side, things like the pixelation, lighting conditions of an image, or a person&#39;s monitor settings can affect how skin tone appears on a screen. You might notice this yourself the next time you change the display setting while watching a show. The hue, saturation, and brightness could all affect how skin tone is displayed on a monitor. Despite these challenges, we find that human annotators are able to learn to become invariant to lighting conditions of an image when annotating skin tone. &lt;/p>; &lt;p>; On the social perception side, aspects of a person&#39;s life like their location, culture, and lived experience may affect how they annotate various skin tones. We found some evidence for this when we asked photographers in the United States and photographers in India to annotate the same image. The photographers in the United States viewed this person as somewhere between MST-5 &amp;amp; MST-7. However, the photographers in India viewed this person as somewhere between MST-3 &amp;amp; MST-5. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s1594/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;1594&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhAVsRGhmFspTV3U1lwxroz6P-9Z76-WQiNWuTzhlMLd9kb6t8LvlrzecxHXHzCCT4ix_UePKr2OApOIRnVIiFcda7hck48ar2pmSanZd9YTj2QkLCC1TQN7XFYLFcILDMLITKZY4lFnPNwvOX79Q-H6QTzlL1cyxt_0hBucZmCFodt83mlK-3xOcy4/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The distribution of Monk Skin Tone Scale annotations for this image from a sample of 5 photographers in the US and 5 photographers in India.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Continuing this exploration, we asked trained annotators from five different geographical regions (India, Philippines, Brazil, Hungary, and Ghana) to annotate skin tone on the MST scale. Within each market each image had 5 annotators who were drawn from a broader pool of annotators in that region. For example, we could have 20 annotators in a market, and select 5 to review a particular image. &lt;/p>; &lt;p>; With these annotations we found two important details. First, annotators within a region had similar levels of agreement on a single image. Second, annotations between regions were, on average, significantly different from each other. (&lt;em>;p&amp;lt;0.05&lt;/em>;). This suggests that people from the same geographic region may have a similar mental model of skin tone, but this mental model is not universal. &lt;/p>; &lt;p>; However, even with these regional differences, we also find that the consensus between all five regions falls close to the MST values supplied by Dr. Monk. This suggests that a geographically diverse group of annotators can get close to the MST value annotated by an MST expert. In addition, after training, we find no significant difference between annotations on well-lit images, versus poorly-lit images, suggesting that annotators can become invariant to different lighting conditions in an image — a non-trivial task for ML models. &lt;/p>; &lt;p>; The MST-E dataset allows researchers to study annotator behavior across curated subsets controlling for potential confounders. We observed similar regional variation when annotating much larger datasets with many more subjects. &lt;/p>; &lt;br />; &lt;h2>;Skin Tone annotation recommendations&lt;/h2>; &lt;p>; Our research includes four major findings. First, annotators within a similar geographical region have a consistent and shared mental model of skin tone. Second, these mental models differ across different geographical regions. Third, the MST annotation consensus from a geographically diverse set of annotators aligns with the annotations provided by an expert in social perception and inequality. And fourth, annotators can learn to become invariant to lighting conditions when annotating MST. &lt;/p>; &lt;p>; Given our research findings, there are a few recommendations for skin tone annotation when using the MST. &lt;/p>; &lt;ol>; &lt;li>;Having a geographically diverse set of annotators is important to gain accurate, or close to ground truth, estimates of skin tone. &lt;/li>;&lt;li>;Train human annotators using the &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;MST-E dataset&lt;/a>;, which spans the entire MST spectrum and contains images in a variety of lighting conditions. This will help annotators become invariant to lighting conditions and appreciate the nuance and differences between the MST points. &lt;/li>;&lt;li>;Given the wide range of annotations we suggest having at least two annotators in at least five different geographical regions (10 ratings per image). &lt;/li>; &lt;/ol>; &lt;p>; Skin tone annotation, like other subjective annotation tasks, is difficult but possible. These types of annotations allow for a more nuanced understanding of model performance, and ultimately help us all to create products that work well for every person across the broad and diverse spectrum of skin tones. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;em>;We wish to thank our colleagues across Google working on fairness and inclusion in computer vision for their contributions to this work, especially Marco Andreetto, Parker Barnes, Ken Burke, Benoit Corda, Tulsee Doshi, Courtney Heldreth, Rachel Hornung, David Madras, Ellis Monk, Shrikanth Narayanan, Utsav Prabhu, Susanna Ricco, Sagar Savla, Alex Siegman, Komal Singh, Biao Wang, and Auriel Wright. We also would like to thank Annie Jean-Baptiste, Florian Koenigsberger, Marc Repnyek, Maura O&#39;Brien, and Dominique Mungin and the rest of the team who help supervise, fund, and coordinate our data collection.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/4751112643381771806/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4751112643381771806&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot; rel=&quot;alternate&quot; title=&quot;Consensus and subjectivity of skin tone annotation for ML fairness&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZx6lCt5n99GkA_Q8Jd3S0msBQNxZpoFWH9Jc2vwcnyxLUhWn-s8f7zn2u5YRNrCbdGki6sRB9pcJ-n_0dGgqD47BM72DVy3zCykkRgJjP1zohvB9l7KPgetFJiLebxm9EvtWBAjRM59eAUbVUPfmuUWAeKc6ljtiMOfiAnOttUBtkYNcXm2HieMod2Q/s72-c/MST-E.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6793814472941708824&lt;/id>;&lt;published>;2023-05-12T13:56:00.000-07:00&lt;/published>;&lt;updated>;2023-05-12T13:56:21.572-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;F-VLM: Open-vocabulary object detection upon frozen vision and language models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Weicheng Kuo and Anelia Angelova, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s320/F-VLM%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Object_detection&quot;>;Detection&lt;/a>; is a fundamental vision task that aims to localize and recognize objects in an image. However, the data collection process of manually annotating bounding boxes or instance masks is tedious and costly, which limits the modern detection vocabulary size to roughly 1,000 object classes. This is orders of magnitude smaller than the vocabulary people use to describe the visual world and leaves out many categories. Recent vision and language models (VLMs), such as &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>;, have demonstrated improved open-vocabulary visual recognition capabilities through learning from Internet-scale image-text pairs. These VLMs are applied to zero-shot classification using frozen model weights without the need for fine-tuning, which stands in stark contrast to the existing paradigms used for retraining or fine-tuning VLMs for &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection tasks&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Intuitively, to align the image content with the text description during training, VLMs may learn region-sensitive and discriminative features that are transferable to object detection. Surprisingly, features of a frozen VLM contain rich information that are both region sensitive for describing object shapes (second column below) and discriminative for region classification (third column below). In fact, feature grouping can nicely delineate object boundaries without any supervision. This motivates us to explore the use of frozen VLMs for open-vocabulary object detection with the goal to expand detection beyond the limited set of annotated categories. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s1999/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1238&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2n5NICh4of0FA5QULX11NvBei_HpoNucScGWUdoSrtBssc8D1QkQ5K9amKSUf6E1qcgCTpgvMBcvQ_BDr0mGbISPo_azclMqQS2A3zbucXFA7goBy3Z30Z_UMGY9RlxAR7EATM9VVf5mlJybFywuy_4JFIhrtW5lhT2BTN5YR0aPaybEmt2najf5Ow/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We explore the potential of frozen vision and language features for open-vocabulary detection. The &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;>;K-Means&lt;/a>; feature grouping reveals rich semantic and region-sensitive information where object boundaries are nicely delineated (column 2). The same frozen features can classify groundtruth (GT) regions well without fine-tuning (column 3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2209.15639&quot;>;F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;”, presented at &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;ICLR 2023&lt;/a>;, we introduce a simple and scalable open-vocabulary detection approach built upon frozen VLMs. F-VLM reduces the training complexity of an open-vocabulary detector to below that of a standard detector, obviating the need for &lt;a href=&quot;https://en.wikipedia.org/wiki/Knowledge_distillation&quot;>;knowledge distillation&lt;/a>;, detection-tailored pre-training, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_supervision&quot;>;weakly supervised learning&lt;/a>;. We demonstrate that by preserving the knowledge of pre-trained VLMs completely, F-VLM maintains a similar philosophy to &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;>;ViTDet&lt;/a>; and decouples detector-specific learning from the more task-agnostic vision knowledge in the detector backbone. We are also releasing the F-VLM &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/fvlm&quot;>;code&lt;/a>; along with a demo on our &lt;a href=&quot;https://sites.google.com/corp/view/f-vlm/home&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Learning upon frozen vision and language models&lt;/h2>; &lt;p>; We desire to retain the knowledge of pretrained VLMs as much as possible with a view to minimize effort and cost needed to adapt them for open-vocabulary detection. We use a frozen VLM image encoder as the detector backbone and a text encoder for caching the detection text embeddings of offline dataset vocabulary. We take this VLM backbone and attach a &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;detector head&lt;/a>;, which predicts object regions for localization and outputs detection scores that indicate the probability of a detected box being of a certain category. The detection scores are the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;cosine similarity&lt;/a>; of region features (a set of bounding boxes that the detector head outputs) and category text embeddings. The category text embeddings are obtained by feeding the category names through the text model of pretrained VLM (which has both image and text models)r. &lt;/p>; &lt;p>; The VLM image encoder consists of two parts: 1) a &lt;a href=&quot;https://github.com/openai/CLIP/blob/main/clip/model.py#L139-L151&quot;>;feature extractor&lt;/a>; and 2) a feature &lt;a href=&quot;https://github.com/openai/CLIP/blob/a9b1bf5920416aaeaec965c25dd9e8f98c864f16/clip/model.py#LL152C8-L152C8&quot;>;pooling layer&lt;/a>;. We adopt the feature extractor for detector head training, which is the only step we train (on standard detection &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;data&lt;/a>;), to allow us to directly use frozen weights, inheriting rich semantic knowledge (eg, long-tailed categories like martini, fedora hat, pennant) from the VLM backbone. The detection losses include &lt;a href=&quot;https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/&quot;>;box regression&lt;/a>; and classification losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;592&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC048xhQHV4Or37EgRogK92Fl32e6Hz4NqWGBZfK60NNCqMWDG4IWKWRNeZozPsGRAk8lxTnhpB74j3Ul5aIc6ETsUZI1kY8moSa0ctl_jscqfr9XFpL7yet4Tvz4urLa3CI4k29_2NqDh7wBJcs_MfLIQypv5-coo8bY3fX4iRzqRn6_vvDy9ZV2qvA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At training time, F-VLM is simply a detector with the last classification layer replaced by base-category text embeddings.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Region-level open-vocabulary recognition&lt;/h2>; &lt;p>; The ability to perform open-vocabulary recognition at region level (ie, bounding box level as opposed to image level) is integral to F-VLM. Since the backbone features are frozen, they do not overfit to the training categories (eg, donut, zebra) and can be directly cropped for region-level classification. F-VLM performs this open-vocabulary classification only at test time. To obtain the VLM features for a region, we apply the feature pooling layer on the cropped backbone output features. Because the pooling layer requires fixed-size inputs, eg, 7x7 for &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet50&lt;/a>; (R50) &lt;a href=&quot;https://openai.com/research/clip&quot;>;CLIP&lt;/a>; backbone, we crop and resize the region features with the &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;ROI-Align layer&lt;/a>; (shown below). Unlike existing open-vocabulary detection &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approaches&lt;/a>;, we do not crop and resize the RGB image regions and cache their embeddings in a separate offline process, but train the detector head in one stage. This is simpler and makes more efficient use of disk storage space.. In addition, we do not crop VLM region features during training because the backbone features are frozen. &lt;/p>; &lt;p>; Despite never being trained on regions, the cropped region features maintain good open-vocabulary recognition capability. However, we observe the cropped region features are not sensitive enough to the localization quality of the regions, ie, a loosely vs. tightly localized box both have similar features. This may be good for classification, but is problematic for detection because we need the detection scores to reflect localization quality as well. To remedy this, we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Geometric_mean&quot;>;geometric mean&lt;/a>; to combine the VLM scores with the detection scores for each region and category. The VLM scores indicate the probability of a detection box being of a certain category according to the pretrained VLM. The detection scores indicate the class probability distribution of each box based on the similarity of region features and input text embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s1999/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghlbBSG10W6R7qheWV8lgd1AtxAMZf5ejPl8IlLPmUTffAoGOJ3OAibToxGB3qFEBelhlLOL25PBYoYZCvXj96pe_YHkkJH5dX9CzLeoYkaNKzYwXPc4-cVisHiSoNaMAYh3dBi_7tFFQK_4exvEUFqRSEWsOn0FysYPJsT6xQuVxIGvbrkZkz6oDXJQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;At test time, F-VLM uses the region proposals to crop out the top-level features of the VLM backbone and compute the VLM score per region. The trained detector head provides the detection boxes and masks, while the final detection scores are a combination of detection and VLM scores.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation&lt;/h2>; &lt;p>; We apply F-VLM to the popular &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>; open-vocabulary detection benchmark. At the system-level, the best F-VLM achieves 32.8 &lt;a href=&quot;https://blog.paperspace.com/mean-average-precision/&quot;>;average precision&lt;/a>; (AP) on rare categories (&lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;APr&lt;/a>;), which outperforms the state of the art by 6.5 &lt;a href=&quot;https://cocodataset.org/#home&quot;>;mask APr&lt;/a>; and many other approaches based on knowledge distillation, pre-training, or joint training with weak supervision. F-VLM shows strong scaling property with frozen model capacity, while the number of trainable parameters is fixed. Moreover, F-VLM generalizes and scales well in the transfer detection tasks (eg, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets) by simply replacing the vocabularies without fine-tuning the model. We test the LVIS-trained models on the popular &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; datasets and demonstrate that the model can work very well without training on in-domain detection data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgblGVF2S1yxqyAT6QDU_Rpf9PO1tXdlDY40Z8fcIB1X8zmi5hsyhD2bTvsVU6w5So2nSayOdlTF-DA2s0A_TYcqa4i8owXGmG7yMw588WfwzQpyU_y3a9XQTLjGjJ7wHZl--VoIgbbwAJk8iHFX_YhjdjDxDOiGVwyZtbymcBpd3J9qP-eUqiV2OG1Gg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM outperforms &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;the state of the art&lt;/a>; (SOTA) on LVIS open-vocabulary detection benchmark and transfer object detection. On the x-axis, we show the LVIS metric mask AP on rare categories (APr), and the Objects365 (O365) metric box AP on all categories. The sizes of the detector backbones are as follows: Small(R50), Base (R50x4), Large(R50x16), Huge(R50x64). The naming follows CLIP convention.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We visualize F-VLM on open-vocabulary detection and transfer detection tasks (shown below). On LVIS and Objects365, F-VLM correctly detects both novel and common objects. A key benefit of open-vocabulary detection is to test on out-of-distribution data with categories given by users on the fly. See the F-VLM paper for more visualization on &lt;a href=&quot;https://www.lvisdataset.org/&quot;>;LVIS&lt;/a>;, &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; and &lt;a href=&quot;https://ego4d-data.org/index.html&quot;>;Ego4D&lt;/a>; datasets. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1148&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-MnlvFqxuPGutMen6ZgF93pR56doCEIeQeoK7YfIF6KTyk75MGH3TgCzwBbcgIqFi68wEFVxuAAZCQDKZflAA6Qs1GXflQs38XazGDr8g7uQNMhQx9tZGFIxgCjNVBBp2jP-Qa-sxeVIbnKM3tEjnrGTEKxhmcEDdsH15_zeHrKaw4n8M6qq92DdJXw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;F-VLM open-vocabulary and transfer detections. &lt;strong>;Top:&lt;/strong>; Open-vocabulary detection on LVIS. We only show the novel categories for clarity. &lt;strong>;Bottom:&lt;/strong>; Transfer to &lt;a href=&quot;https://www.objects365.org/overview.html&quot;>;Objects365&lt;/a>; dataset shows accurate detection of many categories. Novel categories detected: fedora, martini, pennant, football helmet (LVIS); slide (Objects365).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training efficiency&lt;/h2>; &lt;p>; We show that F-VLM can achieve top performance with much less computational resources in the table below. Compared to the state-of-the-art &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;approach&lt;/a>;, F-VLM can achieve better performance with 226x fewer resources and 57x faster wall clock time. Apart from training resource savings, F-VLM has potential for substantial memory savings at training time by running the backbone in inference mode. The F-VLM system runs almost as fast as a standard detector at inference time, because the only addition is a single attention pooling layer on the detected region features. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Method&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost &lt;br />;(per-core-hour)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Training Cost Savings&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;SOTA&lt;/a>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;26.3 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;460 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;8,000 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;1x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;118 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;565 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.0 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;14.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;71 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;113x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;F-VLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;27.7 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.4 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;226x &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We provide additional results using the shorter &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;>;Detectron2&lt;/a>; training recipes (12 and 36 epochs), and show similarly strong performance by using a frozen backbone. The default setting is marked in gray. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Backbone&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;&lt;a href=&quot;https://arxiv.org/abs/2012.07177&quot;>;Large Scale Jitter&lt;/a>;&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;#Epochs&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Batch Size &lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;APr&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.5 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;18.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;12 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;16 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.9 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;36 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.6 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;R50x64 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;✓ &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;256 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;32.8 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present F-VLM – a simple open-vocabulary detection method which harnesses the power of frozen pre-trained large vision-language models to provide detection of novel objects. This is done without a need for knowledge distillation, detection-tailored pre-training, or weakly supervised learning. Our approach offers significant compute savings and obviates the need for image-level labels. F-VLM achieves the new state-of-the-art in open-vocabulary detection on the LVIS benchmark at system level, and shows very competitive transfer detection on other datasets. We hope this study can both facilitate further research in novel-object detection and help the community explore frozen VLMs for a wider range of vision tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements &lt;/h2>; &lt;p>; &lt;i>;This work is conducted by Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. We would like to thank our colleagues at Google Research for their advice and helpful discussions. &lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6793814472941708824/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6793814472941708824&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot; rel=&quot;alternate&quot; title=&quot;F-VLM: Open-vocabulary object detection upon frozen vision and language models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd5HnIvLfcA1rgwAR8Jg3M9p1kkVhDC74oaHdUuRS5MPXqyWYoZURIj6Ibllzk2GKfFMNzB-fKxI3J0NNxyLZimLcGEC6GBzX7FG3pUGC-vRECHDaCXkorH0BL9kIBsOO4EAa3tF3HE3eH0QdzKOLwQQYI_NiToJBmBatxTVNOmYctagvi1ml2YLbgtA/s72-c/F-VLM%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1014951953767598848&lt;/id>;&lt;published>;2023-05-12T10:03:00.003-07:00&lt;/published>;&lt;updated>;2023-05-12T10:03:43.317-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;UI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling conversational interaction on mobile with LLMs&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Bryan Wang, Student Researcher, and Yang Li, Research Scientist, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s320/LLM4Mobile%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Intelligent assistants on mobile devices have significantly advanced language-based interactions for performing simple daily tasks, such as setting a timer or turning on a flashlight. Despite the progress, these assistants still face limitations in supporting conversational interactions in mobile user interfaces (UIs), where many user tasks are performed. For example, they cannot answer a user&#39;s question about specific information displayed on a screen. An agent would need to have a computational understanding of &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_user_interface&quot;>;graphical user interfaces&lt;/a>; (GUIs) to achieve such capabilities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Prior research has investigated several important technical building blocks to enable conversational interaction with mobile UIs, including &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3472749.3474765&quot;>;summarizing a mobile screen&lt;/a>; for users to quickly understand its purpose, &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;mapping language instructions to UI actions&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2101.11103&quot;>;modeling GUIs &lt;/a>;so that they are more amenable for language-based interaction. However, each of these only addresses a limited aspect of conversational interaction and requires considerable effort in curating large-scale datasets and training dedicated models. Furthermore, there is a broad spectrum of conversational interactions that can occur on mobile UIs. Therefore, it is imperative to develop a lightweight and generalizable approach to realize conversational interaction. &lt;/p>; &lt;p>; In &lt;a href=&quot;https://arxiv.org/pdf/2209.08655.pdf&quot;>;“Enabling Conversational Interaction with Mobile UI using Large Language Models”&lt;/a>;, presented at &lt;a href=&quot;https://chi2023.acm.org/&quot;>;CHI 2023&lt;/a>;, we investigate the viability of utilizing large language models (LLMs) to enable diverse language-based interactions with mobile UIs. Recent pre-trained LLMs, such as &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, have demonstrated abilities to adapt themselves to various downstream language tasks when being prompted with a handful of examples of the target task. We present a set of prompting techniques that enable interaction designers and developers to quickly prototype and test novel language interactions with users, which saves time and resources before investing in dedicated datasets and models. Since LLMs only take text tokens as input, we contribute a novel algorithm that generates the text representation of mobile UIs. Our results show that this approach achieves competitive performance using only two data examples per task. More broadly, we demonstrate LLMs&#39; potential to fundamentally transform the future workflow of conversational interaction design.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s1559/LLM4Mobile%201.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;553&quot; data-original-width=&quot;1559&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgQXWoE6YAxVVZtPhmUUj2TDGSOdKYYX9nMWUrBGVmaTtjQ_oqChBnrtemyHCYZKKE6svxZchRJlLj3m1s31NAHoWstSFYL2tQbYQM4UODKN6c7PXXXFUQM45K-FPHbXzagLRrevieIceDUuhGBtULDHbFFO04AIQTbWiWGC6-NAJZHVf9be-2PdvEeg/s16000/LLM4Mobile%201.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing our work on enabling various conversational interactions with mobile UI using LLMs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Prompting LLMs with UIs&lt;/h2>; &lt;p>; LLMs support in-context few-shot learning via prompting — instead of fine-tuning or re-training models for each new task, one can prompt an LLM with a few input and output data exemplars from the target task. For many natural language processing tasks, such as question-answering or translation, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompting&lt;/a>; performs competitively with &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;benchmark approaches&lt;/a>; that train a model specific to each task. However, language models can only take text input, while mobile UIs are multimodal, containing text, image, and structural information in their &lt;a href=&quot;https://developer.android.com/topic/performance/rendering/optimizing-view-hierarchies&quot;>;view hierarchy&lt;/a>; data (ie, the structural data containing detailed properties of UI elements) and screenshots. Moreover, directly inputting the view hierarchy data of a mobile screen into LLMs is not feasible as it contains excessive information, such as detailed properties of each UI element, which can exceed the input length limits of LLMs. &lt;/p>; &lt;p>; To address these challenges, we developed a set of techniques to prompt LLMs with mobile UIs. We contribute an algorithm that generates the text representation of mobile UIs using &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth-first_search&quot;>;depth-first search&lt;/a>; traversal to convert the Android UI&#39;s view hierarchy into HTML syntax. We also utilize &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;chain of thought prompting&lt;/a>;, which involves generating intermediate results and chaining them together to arrive at the final output, to elicit the reasoning ability of the LLM.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s1486/LLM4Mobile%202.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1486&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjW5i8ce8Fgml9BhrSIQvYjnog2FrWKK57oyt4vQAmn0hGtFf2la5EjfGxREsRY0Y78ZD4jVvjYq1nr8kuou3rdcDTCQVMN2ROYkdIfsbKTI-QWkjGspm7zSNRrPdci1yH4t9t8MpGh7L55qqCj7Xrno6rIekp7gVu2CMKfZGyTaeoVLP8KwVaqehdF5w/s16000/LLM4Mobile%202.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animation showing the process of few-shot prompting LLMs with mobile UIs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Our prompt design starts with a preamble that explains the prompt&#39;s purpose. The preamble is followed by multiple exemplars consisting of the input, a chain of thought (if applicable), and the output for each task. Each exemplar&#39;s input is a mobile screen in the HTML syntax. Following the input, chains of thought can be provided to elicit logical reasoning from LLMs. This step is not shown in the animation above as it is optional. The task output is the desired outcome for the target tasks, eg, a screen summary or an answer to a user question. Few-shot prompting can be achieved with more than one exemplar included in the prompt. During prediction, we feed the model the prompt with a new input screen appended at the end. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We conducted comprehensive experiments with four pivotal modeling tasks: (1) screen question-generation, (2) screen summarization, (3) screen question-answering, and (4) mapping instruction to UI action. Experimental results show that our approach achieves competitive performance using only two data examples per task. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1402&quot; data-original-width=&quot;1999&quot; height=&quot;449&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy-vub2oputqbKOQnQnE5_w3aF0O60mkiQSWzN-MYmgSgIqmRUQqbz5t1jN4ZNpjty4g86NXttJwGM3ZD1VnQQNyNW8R6vJTeFqUGAFzOi96hfL_FJNmoWn3AegiaeC583g8otIUkdxk8Al6HJL3XA7Q5pCuyX3CKDAFwof0pPLjuHkbBAs2wm89jk1Q/w640-h449/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 1: Screen question generation&lt;/h3>; &lt;p>; Given a mobile UI screen, the goal of screen question-generation is to synthesize coherent, grammatically correct natural language questions relevant to the UI elements requiring user input. &lt;/p>; &lt;p>; We found that LLMs can leverage the UI context to generate questions for relevant information. LLMs significantly outperformed the heuristic approach (template-based generation) regarding question quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s1999/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;977&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGnlzGAv4XcI_WqlfRmQ0ni0tyhf9VKHkTLyDRnEPi3AdHrn4g_-z7DxLaUr-ip_dzJg9KdnkCg9043BsecejFz3bLDDW2oGYZCmTNDf4uaZlaHOMfRfx0Iz-lP1OiD1a8ieHwHDEmA5K1g3ObHDFJJbF7V_8oILlFQ2jw-aVL8MG-kFBamjp3J5LBOA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example screen questions generated by the LLM. The LLM can utilize screen contexts to generate grammatically correct questions relevant to each input field on the mobile UI, while the template approach falls short.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We also revealed LLMs&#39; ability to combine relevant input fields into a single question for efficient communication. For example, the filters asking for the minimum and maximum price were combined into a single question: “What&#39;s the price range? &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1234&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNuxVqhk31zOIpFkwOW-qhQsuGWZmsf79u_ViOU4T7hZ85RfmT_dgECvjjw5DXDuzgNdbqZpRIYRDCiIkiu9lTuoBOIxz8WnQBhgfRRQzyfOCFUZRfw_S4O91beQHXzWZoCR18gwOTk7YoAsZumYTT5H1pfAiLiW-FbvbuBioAO-CMbucTh8DF4c4hNg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We observed that the LLM could use its prior knowledge to combine multiple related input fields to ask a single question.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; In an evaluation, we solicited human ratings on whether the questions were grammatically correct (Grammar) and relevant to the input fields for which they were generated (Relevance). In addition to the human-labeled language quality, we automatically examined how well LLMs can cover all the elements that need to generate questions (Coverage &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1&lt;/a>;). We found that the questions generated by LLM had almost perfect grammar (4.98/5) and were highly relevant to the input fields displayed on the screen (92.8%). Additionally, LLM performed well in terms of covering the input fields comprehensively (95.8%). &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;Template &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Grammar &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;3.6 (out of 5) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;4.98 (out of 5)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Relevance &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;84.1% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;92.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;Coverage F1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;100% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;95.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 2: Screen summarization&lt;/h3>; &lt;p>; Screen summarization is the automatic generation of descriptive language overviews that cover essential functionalities of mobile screens. The task helps users quickly understand the purpose of a mobile UI, which is particularly useful when the UI is not visually accessible. &lt;/p>; &lt;p>; Our results showed that LLMs can effectively summarize the essential functionalities of a mobile UI. They can generate more accurate summaries than the &lt;a href=&quot;https://arxiv.org/abs/2108.03353&quot;>;Screen2Words&lt;/a>; benchmark model that we previously introduced using UI-specific text, as highlighted in the colored text and boxes below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s1999/image10.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1421&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTadczJdtSUCt-TmkjQM0tuqOjVGVIjU99Dq7B2IITFMemsrduIpskHaEmV_uxxNB0ORdu5LP9J7O583ZbRKlEO0Oax3cVun3ve3pna3zlq6xmKMiTmmekDxGsRgfLbGhm-ORDiISl3q_ObOLaCs4ke5iqPl5XOGio6So56SOMkYsGPuILYXvz8biWFg/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example summary generated by 2-shot LLM. We found the LLM is able to use specific text on the screen to compose more accurate summaries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Interestingly, we observed LLMs using their prior knowledge to deduce information not presented in the UI when creating summaries. In the example below, the LLM inferred the subway stations belong to the London Tube system, while the input UI does not contain this information. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1450&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuNnkoDKv84_G6y-SXbZWxPqO8A93OY3d752GM3JCDX-3OMeGUwcQ0A4KoldEaDJH9mvIrgUKxKJkgNBbDH3CNntfNMwS7yK3FGXS2r8szFJoJyqzvq5OvZx3eHbD7scoVJyrbKMArEPPS80uBfX043g_Rk8Y0Rqadsd_59ID7QdbxHYZXzE7czybEPg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;LLM uses its prior knowledge to help summarize the screens.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Human evaluation rated LLM summaries as more accurate than the benchmark, yet they scored lower on metrics like &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>;. The mismatch between perceived quality and metric scores echoes &lt;a href=&quot;https://arxiv.org/abs/2209.12356&quot;>;recent work&lt;/a>; showing LLMs write better summaries despite automatic metrics not reflecting it. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;994&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTK_lA_dTvKUA74-scaC5ERuMSyQjgyS2qlSrneNTF1P7230FSmeHw5uI0nEvbkvJLd04Djxyd7mZwAskCd6fDOmHMFz-H5euwRLWjk3_uOqxeWaU7dU05z8CnmgbSi31xxkGBo0NE-SKAturH7CNKFGOvm4ZINW-I30QeS6Xvv_8Ke72T8p69v-IoJA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1013&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbd9j67uoyo5VNT7jgi7ILKtE_ixBYimwPGITEZ33U8FzR9WWNCZR7dbw-wY7PHITqZ8O9u7S53I1_Vzy5AuFf9gjUltZuMUtdpaYqlRB7kUM68hpwPwKSs5KCVR51vD9OfDTyOi_WDk_nPPiOgxJbGoirGmriMzhpufNRyCsJtiv3h8jXdeKLJ6WU1g/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>;: Screen summarization performance on automatic metrics. &lt;strong>;Right&lt;/strong>;: Screen summarization accuracy voted by human evaluators.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 3: Screen question-answering&lt;/h3>; &lt;p>; Given a mobile UI and an open-ended question asking for information regarding the UI, the model should provide the correct answer. We focus on factual questions, which require answers based on information presented on the screen. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1689&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqzwdWK44a9Ksqw1d13jITNwLqtyR8455FZZiIhZPo7uKbFGiA-QzfyGwejfRg5Ma3c0eGVu_Fuzry15miV7HmHDf_YvdvrEGsefVhCT54N0551rUeeGux-MxkH5E9ZM2KUpUsDSExa38j3LtIGv0E-D6HRLmkHqK-kKkhzb-ktYrUz5MVq7zuEHOqDw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example results from the screen QA experiment. The LLM significantly outperforms the off-the-shelf QA baseline model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We report performance using four metrics: Exact Matches (identical predicted answer to ground truth), Contains GT (answer fully containing ground truth), Sub-String of GT (answer is a sub-string of ground truth), and the &lt;a href=&quot;https://arxiv.org/abs/1911.03347&quot;>;Micro-F1&lt;/a>; score based on shared words between the predicted answer and ground truth across the entire dataset. &lt;/p>; &lt;p>; Our results showed that LLMs can correctly answer UI-related questions, such as &quot;what&#39;s the headline?&quot;. The LLM performed significantly better than baseline QA model &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;>;DistillBERT&lt;/a>;, achieving a 66.7% fully correct answer rate. Notably, the 0-shot LLM achieved an exact match score of 30.7%, indicating the model&#39;s intrinsic question answering capability. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Exact Matches&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Contains GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Sub-String of GT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;Micro-F1&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;30.7% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;6.5% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;5.6% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;31.2% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;65.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;10.0% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;7.8% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;62.9% &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;66.7%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;12.6%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;5.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;64.8%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;DistillBERT&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;36.0%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;8.5%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;9.9%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;strong>;37.2%&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Task 4: Mapping instruction to UI action&lt;/h3>; &lt;p>; Given a mobile UI screen and natural language instruction to control the UI, the model needs to predict the ID of the object to perform the instructed action. For example, when instructed with &quot;Open Gmail,&quot; the model should correctly identify the Gmail icon on the home screen. This task is useful for controlling mobile apps using language input such as voice access. We introduced this &lt;a href=&quot;https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html&quot;>;benchmark task&lt;/a>; previously. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1131&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCxN6P9wyIeZAhHYMmMofKkjLAP6rhn7eoG0H7dzQW956YjsPc5WYWctqUsS_NzRleFoWqMh_Rw6jCo47InLcXUvmeorRu3VJ34Wz4BvnikJaryg5VyG3jrITWAuGigw2l27DjHanBpQw6Twb53rqaVor7YqE0m-EnjZtBvLSbRmii7Xh1_TbE2wWckA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example using data from the &lt;a href=&quot;https://github.com/google-research-datasets/seq2act&quot;>;PixelHelp dataset&lt;/a>;. The dataset contains interaction traces for common UI tasks such as turning on wifi. Each trace contains multiple steps and corresponding instructions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We assessed the performance of our approach using the Partial and Complete metrics from the &lt;a href=&quot;https://arxiv.org/abs/2005.03776&quot;>;Seq2Act&lt;/a>; paper. Partial refers to the percentage of correctly predicted individual steps, while Complete measures the portion of accurately predicted entire interaction traces. Although our LLM-based method did not surpass the benchmark trained on massive datasets, it still achieved remarkable performance with just two prompted data examples. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>; &lt;tbody>;&lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Models&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Partial&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Complete&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;0-shot LLM &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.29 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.69 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;31.67 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;2-shot LLM (cross-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;75.28 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;34.44 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;1-shot LLM (in-app) &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.35 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;40.00 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;2-shot LLM (in-app)&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;80.36&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;45.00&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;strong>;Seq2Act&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;89.21&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;70.59&lt;/strong>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Takeaways and conclusion&lt;/h2>; &lt;p>; Our study shows that prototyping novel language interactions on mobile UIs can be as easy as designing a data exemplar. As a result, an interaction designer can rapidly create functioning mock-ups to test new ideas with end users. Moreover, developers and researchers can explore different possibilities of a target task before investing significant efforts into developing new datasets and models. &lt;/p>; &lt;p>; We investigated the feasibility of prompting LLMs to enable various conversational interactions on mobile UIs. We proposed a suite of prompting techniques for adapting LLMs to mobile UIs. We conducted extensive experiments with the four important modeling tasks to evaluate the effectiveness of our approach. The results showed that compared to traditional machine learning pipelines that consist of expensive data collection and model training, one could rapidly realize novel language-based interactions using LLMs while achieving competitive performance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We thank our paper co-author Gang Li, and appreciate the discussions and feedback from our colleagues Chin-Yi Cheng, Tao Li, Yu Hsiao, Michael Terry and Minsuk Chang. Special thanks to Muqthar Mohammad and Ashwin Kakarla for their invaluable assistance in coordinating data collection. We thank John Guilyard for helping create animations and graphics in the blog.&lt;/i>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1014951953767598848/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1014951953767598848&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/enabling-conversational-interaction-on.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling conversational interaction on mobile with LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBtmVasIEAQHIKlt2az_MQbF13URtJ9LYTtcACwT54elVT1Jr-l-o7MvsHntd4HnN3bxO-rniQdlt3pM1ty7SOpMhXaEsrD0Y8azCU-zAhs3xHR1OE2hsYs_PMysluHt7QItT2klQyU5xGEKIg8JaMNwGsGRGc1axpXBMWUL1KGLqWtSm2bDGDw4B0Pg/s72-c/LLM4Mobile%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-538722811879850400&lt;/id>;&lt;published>;2023-05-10T08:03:00.004-07:00&lt;/published>;&lt;updated>;2023-05-12T11:51:35.963-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Building better pangenomes to improve the equity of genomics&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Andrew Carroll, Product Lead, and Kishwar Shafin, Research Scientist, Genomics &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s320/image4.png&quot; style=&quot;display: none;&quot; />; &lt;p>; For decades, researchers worked together to assemble a complete copy of the molecular instructions for a human — a map of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_genome&quot;>;human genome&lt;/a>;. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Genome_Project&quot;>;first draft&lt;/a>; was finished in 2000, but with several missing pieces. Even when a complete &lt;a href=&quot;https://en.wikipedia.org/wiki/Reference_genome&quot;>;reference genome&lt;/a>; was &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;achieved in 2022&lt;/a>;, their work was not finished. A single reference genome can&#39;t incorporate known genetic variations, such as the variants for the gene determining whether a person has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Blood_type&quot;>;blood type&lt;/a>; A, B, AB or O. Furthermore, the reference genome &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-8674(19)30231-4&quot;>;didn&#39;t represent the vast diversity of human ancestries&lt;/a>;, making it less useful for detecting disease or finding cures for people from some backgrounds than others. For the past three years, we have been part of an international collaboration with 119 scientists across 60 institutions, called the &lt;a href=&quot;https://www.genome.gov/about-genomics/new-human-pangenome-reference&quot;>;Human Pangenome Research Consortium&lt;/a>;, to address these challenges by creating a new and more representative map of the human genome, a &lt;em>;&lt;a href=&quot;https://www.youtube.com/watch?v=swNtGe9QWAQ&quot;>;pangenome&lt;/a>;&lt;/em>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to share that today, in “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-05896-x&quot;>;A draft human pangenome reference&lt;/a>;”, published in &lt;em>;Nature&lt;/em>;, this group is announcing the completion of the first human pangenome reference. The pangenome combines 47 individual genome reference sequences and better represents the genomic diversity of global populations. Building on Google&#39;s deep learning technologies and &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;past advances in genomics&lt;/a>;, we used tools based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) and &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>; to tackle the challenges of building accurate pangenome sequences and using them for genome analysis. These contributions helped the consortium build an information-rich resource for geneticists, researchers and clinicians around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s1600/Pangenome.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheMl1zu7IcLtfrzl4-r4Dn22mhelcLxcyX45ofXGp2YMHVH5fdQs3cG4bfSJhDCGOHDULi20qbGl7Qm9wcbpFLW4OJXhb4NuFGRhx3Q7Ek7RmEKAAruOmjfoQAUOK2pWwYnFfYZC_hpwPB8seZBCYEzDIY8O0WWpgghaLm0s_Xb_HlJqVPExpzxBnIdw/s16000/Pangenome.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Using graphs to build pangenomes &lt;/h2>; &lt;p>; In the typical analysis workflow for high-throughput DNA sequencing, a &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_sequencer&quot;>;sequencing instrument&lt;/a>; reads millions of short pieces of an individual&#39;s genome, and a program called a &lt;a href=&quot;https://en.wikibooks.org/wiki/Next_Generation_Sequencing_(NGS)/Alignment&quot;>;mapper or aligner&lt;/a>; then estimates where those pieces best fit relative to the single, linear human reference sequence. Next, variant caller software identifies the unique parts of the individual&#39;s sequence relative to the reference. &lt;/p>; &lt;p>; But because humans carry a diverse set of sequences, sections that are present in an individual&#39;s DNA but are not in the reference genome can&#39;t be analyzed. One study of 910 African individuals found that a total of &lt;a href=&quot;https://www.theatlantic.com/science/archive/2018/11/human-genome-300-million-missing-letters-dna/576481/&quot;>;300 million DNA base pairs&lt;/a>; — 10% of the roughly three billion base pair reference genome — are not present in the previous linear reference but occur in at least one of the 910 individuals. &lt;/p>; &lt;p>; To address this issue, the consortium used &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph data structures&lt;/a>;, which are powerful for genomics because they can represent the sequences of many people simultaneously, which is needed to create a pangenome. Nodes in a graph genome contain the known set of sequences in a population, and paths through those nodes compactly describe the unique sequences of an individual&#39;s DNA. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s1920/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1081&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkK20wjYAM3shEwnvL5NXL5tyYaVvlYZLYdsWja5W3pmdKUnY7M9B_RcnBdbiEQGvMqx0Whw4ULcxL8ju8n4YSCfvvRMbnvSzVPKv_E779Gn7AUlLqXxVs52qjwnJJ7EqI5Hf6hHAxsLzd3q0oz6N5iDiFV1irgH0ABRyeklX6uc7v6JJl-JqXGD4wtQ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Schematic of a graph genome. Each color represents the sequence path of a different individual. Multiple paths passing through the same node indicate multiple individuals share that sequence, but some paths also show a &lt;a href=&quot;https://www.garvan.org.au/research/kinghorn-centre-for-clinical-genomics/learn-about-genomics/for-gp/genetics-refresher-1/types-of-variants&quot;>;single nucleotide variant&lt;/a>; (SNV), insertions, or deletions. Illustration credit Darryl Leja, &lt;a href=&quot;https://www.genome.gov/&quot;>;National Human Genome Research Institute&lt;/a>; (NHGRI).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/s1172/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1172&quot; data-original-width=&quot;1158&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfn0R7RS41lAYhTB8N0pofqFhQKgpoaNAuMhwVwkHRbyMKQTzWGN5VQ_03LnFmPW3YlPSo4MqU1cScLTgMoUDWXwuwBUly8VvafDc761n-yD_P4d5rGhD-HnJZQMf28KIqIGd_o8ABYH9LHTwUnr8p8RgwgGFW-arNswGRSH6WPFZf7zpeqeYd6nwFcQ/w632-h640/image1.png&quot; width=&quot;632&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Actual graph genome for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Major_histocompatibility_complex&quot;>;major histocompatibility complex&lt;/a>; (MHC) region of the genome. Genes in MHC regions are essential to immune function and are associated with a person&#39;s resistance and susceptibility to infectious disease and autoimmune disorders (eg, &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fgene.2021.671682/full&quot;>;ankylosing spondylitis&lt;/a>; and &lt;a href=&quot;https://www.hindawi.com/journals/jir/2012/584374/&quot;>;lupus)&lt;/a>;. The graph shows the linear human genome reference (green) and different individual person&#39;s sequence (gray). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; Using graphs creates numerous challenges. They require reference sequences to be highly accurate and the development of new methods that can use their data structure as an input. However, new sequencing technologies (such as &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/519025v1&quot;>;consensus sequencing&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2008.01237&quot;>;phased assembly methods&lt;/a>;) have driven exciting progress towards solving these problems. &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.nature.com/articles/s41592-022-01730-w&quot;>;Long-read sequencing technology&lt;/a>;, which reads larger pieces of the genome (10,000 to millions of DNA characters long) at a time, are essential to the creation of high quality reference sequences because larger pieces can be stitched together into assembled genomes more easily than the short pieces read out by earlier technologies. &lt;a href=&quot;https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/short-read-sequencing/&quot;>;Short read sequencing&lt;/a>; reads pieces of the genome that are only 100 to 300 DNA characters long, but has been the highly scalable basis for high-throughput sequencing methods developed in the 2000s. Though long-read sequencing is newer and has advantages for reference genome creation, many informatics methods for short reads hadn&#39;t been developed for long read technologies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evolving DeepVariant for error correction&lt;/h2>; &lt;p>; Google initially developed &lt;a href=&quot;https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html&quot;>;DeepVariant&lt;/a>;, an open-source CNN variant caller framework that analyzes the short-read sequencing evidence of local regions of the genome. However, we were able to re-train DeepVariant to yield &lt;a href=&quot;https://www.nature.com/articles/s41587-019-0217-9&quot;>;accurate analysis of Pacific Bioscience&#39;s long-read data&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s640/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzFeDmSK8-VZn8cURoq3yxecWrBDMGQxyhx9RgSmwCTDZmmQDPhM1tI3kKxBRAatoUppSSKuDSapl5h2XXE7_KV_7e-OTerQNqeI-a30z2ydBc4xxFWaSdUKfc0sDyzEBSAPM9HJxnuWdWhL7rzebMJyUdHf38LSmqrWc5O8IHB5_9TEZ3fZC6CTWzvg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Training and evaluation schematic for DeepVariant.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We next teamed up with researchers at the University of California, Santa Cruz (UCSC) &lt;a href=&quot;https://genomics.ucsc.edu/&quot;>;Genomics Institute&lt;/a>; to participate in a &lt;a href=&quot;https://precision.fda.gov/challenges/10&quot;>;United States Food and Drug Administration competition&lt;/a>; for another &lt;a href=&quot;https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html&quot;>;long-read sequencing technology from Oxford Nanopore&lt;/a>;. Together, we won the award for highest accuracy in the &lt;a href=&quot;https://nanoporetech.com/&quot;>;nanopore&lt;/a>; category, with a single nucleotide variants (SNVs) accuracy that matched short-read sequencing. This work has been used to &lt;a href=&quot;https://blog.google/technology/health/advancing-genomics-better-understand-and-treat-disease/&quot;>;detect and treat genetic diseases in critically ill newborns&lt;/a>;. The use of DeepVariant on long-read technologies provided the foundation for the consortium&#39;s use of DeepVariant for error correction of pangenomes. &lt;/p>; &lt;p>; DeepVariant&#39;s ability to use multiple long-read sequencing modalities proved useful for error correction in the &lt;a href=&quot;https://sites.google.com/corp/ucsc.edu/t2tworkinggroup&quot;>;Telomere-to-Telomere (T2T) Consortium&lt;/a>;&#39;s effort that generated &lt;a href=&quot;https://www.science.org/doi/10.1126/science.abj6987&quot;>;the first complete assembly of a human genome&lt;/a>;. Completing this first genome set the stage to build the multiple reference genomes required for pangenomes, and T2T was already working closely with the &lt;a href=&quot;https://humanpangenomeproject.org/&quot;>;Human Pangenome Project&lt;/a>; (with many shared members) to scale those practices. &lt;/p>; &lt;p>; With a set of high-quality human reference genomes on the horizon, developing methods that could use those assemblies grew in importance. We worked to adapt DeepVariant to use the pangenome developed by the consortium. In partnership with UCSC, we built an end-to-end analysis workflow for &lt;a href=&quot;https://genome.cshlp.org/content/32/5/893.short&quot;>;graph-based variant detection&lt;/a>;, and demonstrated &lt;a href=&quot;https://www.science.org/doi/abs/10.1126/science.abg8871&quot;>;improved accuracy across several thousand samples&lt;/a>;. The use of the pangenome allows many previously missed variants to be correctly identified. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;258&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikUrR1aDwPDm3QMz6-L5-njSqo75klSNgThCDFOtcrQlegf5Q5ImDdomGY2LEpmOJ-pj4sv8oBxIOpwshwK4yVEaNeKiL9UeGmABMaQOEptkj9IXsOhnJF9n9gkMsR0ThhVa-7_VwV8q2cj_vQH7I3kIHub1qxCcjrSJwVbYnFj9x1E_TZkwWzicT36Q/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualization of variant calls in the &lt;a href=&quot;https://medlineplus.gov/genetics/gene/kcne1/&quot;>;KCNE1 gene&lt;/a>; (a gene with variants associated with cardiac arrhythmias and &lt;a href=&quot;https://www.ahajournals.org/doi/pdf/10.1161/JAHA.117.007837&quot;>;sudden death&lt;/a>;) using a pangenome reference versus the prior linear reference. Each dot represents a variant call that is either correct (&lt;strong>;blue dot&lt;/strong>;), incorrect (&lt;strong>;green dot&lt;/strong>;) — when a variant is identified but is not really there —or a missed variant call (&lt;strong>;red dot&lt;/strong>;). The top box shows variant calls made by DeepVariant using the pangenome reference while the bottom shows variant calls made by using the linear reference. Figure adapted from &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2022.07.09.499321v1&quot;>;A Draft Human Pangenome Reference.&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improving pangenome sequences using transformers &lt;/h2>; &lt;p>; Just as new sequencing technologies enabled new pangenome approaches, new informatics technologies enabled improvements for sequencing methods. Google adapted transformer architectures from analysis of human language to genome sequences to develop &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.08.31.458403v1&quot;>;DeepConsensus&lt;/a>;. A key enabler for this was the development of a differentiable loss function that could handle the insertions and deletions common in sequencing data. This enabled us to have high accuracy without needing a decoder, allowing the speed required to keep up with terabytes of sequencer output. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/s940/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;647&quot; data-original-width=&quot;940&quot; height=&quot;441&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigPdxtD2BOftLBQbcdEolg3yD4_c2o0_JOzbHJLcy2nbAkPibpYXWEgwzYueoBo9gd90bAPjty-9zq8imNd1ivU-XfOQwzg_ozVGslZ_HtgIfk1vLEzzM6h_r8Ys_YfjpBF_KfAyYdSzBegJJZhDYWbSr47Na3KI2kCqUGwOfHUaAKyQxlovTEY8xc7w/w640-h441/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer architecture for DeepConsensus. DeepConsensus takes as input the repeated sequence of the DNA molecule, measured from fluorescent light detected by the addition of each base. DeepConsensus also uses as input the more detailed information about the sequencing process, including the duration of the light pulse (referred to here as pulse width or PW), the time between pulses (IP) the signal-to-noise ratio (SN) and which side of the double helix is being measured (strand).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;794&quot; data-original-width=&quot;1999&quot; height=&quot;254&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPEsfy8M9Grhzh2b4ivMiE6itqY7e0-kbfOzFc0ja3AeNLnAQD1_inzl_YZKJbmB9TACw3cbAVkF1dV0ElnB1R6CGJlwvFzcLeH7HBJ8SqL91ABPZgYiuF0S_-8v-U1utfJ3iSbSSfTprnP0uFzj1ib3NkJi8HyMMNFReIEJ2Am44iI_7LLrbvVOplxA/w640-h254/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of alignment loss function in training evaluation of model output. Better accounting of insertions and deletions by a differentiable alignment function enables the model training process to better estimate errors.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;p>; DeepConsensus &lt;a href=&quot;https://blog.google/technology/health/a-new-genome-sequencing-tool-powered-with-our-technology/&quot;>;improves the yield and accuracy of instrument&lt;/a>; data. Because PacBio sequencing provides the primary sequence information for the 47 genome assemblies, we could apply DeepConsensus to improve those assemblies. With application of DeepConsensus, consortium members &lt;a href=&quot;https://www.nature.com/articles/s41587-023-01662-6&quot;>;built a genome assembler&lt;/a>; that was able to reach 99.9997% assembly base-level accuracies. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We developed multiple new approaches to improve genetic sequencing methods, which we then used to construct pangenome references that enable more robust genome analysis. &lt;/p>; &lt;p>; But this is just the beginning of the story. In the next stage, a larger, worldwide group of scientists and clinicians will use this pangenome reference to study genetic diseases and make new drugs. And future pangenomes will represent even more individuals, realizing a vision summarized this way in a recent Nature story: “&lt;a href=&quot;https://www.nature.com/articles/d41586-023-01300-w&quot;>;Every base, everywhere, all at once&lt;/a>;.” &lt;em>;Read our post on the &lt;a href=&quot;https://blog.google/technology/health/first-pangenome-reference-nature-paper-ai/&quot;>;Keyword Blog&lt;/a>; to learn more about the human pangenome reference announcement.&lt;/em>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;Many people were involved in creating the pangenome reference, including 119 authors across 60 organizations, with the Human Pangenome Reference Consortium. This blog post highlights Google&#39;s contributions to the broader work. We thank the research groups at UCSC Genomics Institute (GI) under Professors Benedict Paten and Karen Miga, genome polishing efforts of Arang Rhie at National Institute of Health (NIH), Genome Assembly and Polishing of Adam Phillipy&#39;s group, and the standards group at National Institute of Standards and Technology (NIST) of Justin Zook. We thank Google contributors: Pi-Chuan Chang, Maria Nattestad, Daniel Cook, Alexey Kolesnikov, Anastaysia Belyaeva, and Gunjan Baid. We thank John Guilyard for his illustrative animation, and Lizzie Dorfman, Elise Kleeman, Erika Hayden, Cory McLean, Shravya Shetty, Greg Corrado, Katherine Chou, and Yossi Matias for their support, coordination, and leadership. Last but not least, thanks to the research participants that provided their DNA to help build the pangenome resource.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/538722811879850400/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/538722811879850400&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/building-better-pangenomes-to-improve.html&quot; rel=&quot;alternate&quot; title=&quot;Building better pangenomes to improve the equity of genomics&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDfvV8C3uALQpVcvYLeqdbLBXoX0ZvSQC76l4SVp4xHq7xGKRDRLuxKYKsZzc5e3gVPp2yyhO4R4YgoXNQreJH4RYSQNlIy7cG57e0bviTgMKeEtubq0kt7q4Ei2uF4fjgqo9qjiT8kXjo9LamEZ4iWHkq6bV80Vu0LYFQ1dTwJCnzSdfpoeZfZuVZzw/s72-c/image4.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7102791933497880989&lt;/id>;&lt;published>;2023-05-04T14:59:00.004-07:00&lt;/published>;&lt;updated>;2023-05-04T14:59:57.911-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by AJ Piergiovanni and Anelia Angelova, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s800/MAMMUT.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Vision-language foundational models are built on the premise of a single pre-training followed by subsequent adaptation to multiple downstream tasks. Two main and disjoint training scenarios are popular: a &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;-style contrastive learning and next-token prediction. Contrastive learning trains the model to predict if image-text pairs correctly match, effectively building visual and text representations for the corresponding image and text inputs, whereas next-token prediction predicts the most likely next text token in a sequence, thus learning to generate text, according to the required task. &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;>;Contrastive learning&lt;/a>; enables &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval tasks&lt;/a>;, such as finding the image that best matches a certain description, and next-token learning enables text-generative tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_generation#Image_captioning&quot;>;Image Captioning&lt;/a>; and &lt;a href=&quot;https://visualqa.org/&quot;>;Visual Question Answering&lt;/a>; (VQA). While both approaches have demonstrated powerful results, when a model is pre-trained contrastively, it typically does not fare well on text-generative tasks and vice-versa. Furthermore, adaptation to other tasks is often done with complex or inefficient methods. For example, in order to extend a vision-language model to videos, some models need to do inference for each video frame separately. This limits the size of the videos that can be processed to only a few frames and does not fully take advantage of motion information available across frames. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Motivated by this, we present “&lt;a href=&quot;https://arxiv.org/abs/2303.16839&quot;>;A Simple Architecture for Joint Learning for MultiModal Tasks&lt;/a>;”, called MaMMUT, which is able to train jointly for these competing objectives and which provides a foundation for many vision-language tasks either directly or via simple adaptation. MaMMUT is a compact, 2B-parameter multimodal model that trains across contrastive, text generative, and localization-aware objectives. It consists of a single image encoder and a text decoder, which allows for a direct reuse of both components. Furthermore, a straightforward adaptation to video-text tasks requires only using the image encoder once and can handle many more frames than prior work. In line with recent language models (eg, &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html&quot;>;GLaM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;GPT3&lt;/a>;), our architecture uses a decoder-only text model and can be thought of as a simple extension of language models. While modest in size, our model outperforms the state of the art or achieves competitive performance on &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_retrieval&quot;>;image-text and text-image retrieval&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (VideoQA), &lt;a href=&quot;https://ai.googleblog.com/2022/06/end-to-end-generative-pre-training-for.html&quot;>;video captioning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>;, and &lt;a href=&quot;https://visualqa.org/&quot;>;VQA&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s1182/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;321&quot; data-original-width=&quot;1182&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfXoqtrQ_BRyHf1n2D4dU-bUQZTrTBBDfLt3fSlhuQy1pAFOXANnp6WUsJlEQLdlOsOPG2y3iqJUDH0fB1lOUaxjGv3L9BoFCL4Y8vqC2Cya0iXM3sjhiYM_6rzGjSnsvfvTc8qqjPi8BIjppq2xswR3-gk4x6ysfu_FsN7G5_VCFr0kjx4ttYcn3LYA/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s980/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGAXjkLnL2NXPDE4zPRjFmPblDtP_vR5G9s7ox3xZFgGUIVm5YROXJpaSGb1MJR3QHajoIwslBnEeTuSPhVT9FwxqvvUVmjBXUS838J3G5jmicy7Y2DYF_u-J8x0Avv9TJuq6yFFZc0Yi3sqCrclKGbLiMEuFUJxVHL6ij8fgXFOuKR2WOaeWYxST8VA/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s1120/image10.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;390&quot; data-original-width=&quot;1120&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQErbPcXI1OwDK2NK155nBq67jBnBcYb5pCGJrupehsnE-WT6MpVWbMmcSZ7rwFBGsCIKQiILMzSxGmIgLMrdi9ULbHN7X_6y6Z3bpOActzoqziSIfee6L-vppDifAF3uVh6M61MtsQ-LM-gG4g5S4-k9Sga7IYVNMC5pGB8KxLRYjXDVZACAF_S4Okw/s16000/image10.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The MaMMUT model enables a wide range of tasks such as image-text/text-image retrieval (&lt;b>;top left&lt;/b>; and &lt;b>;top right&lt;/b>;), VQA (&lt;b>;middle left&lt;/b>;), open-vocabulary detection (&lt;b>;middle right&lt;/b>;), and VideoQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder-only model architecture&lt;/h2>; &lt;p>; One surprising finding is that a single language-decoder is sufficient for all these tasks, which obviates the need for both complex constructs and training procedures presented before. For example, our model (presented to the left in the figure below) consists of a single visual encoder and single text-decoder, connected via &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;cross attention&lt;/a>;, and trains simultaneously on both contrastive and text-generative types of losses. Comparatively, prior work is either not able to handle image-text retrieval tasks, or applies only some losses to only some parts of the model. To enable multimodal tasks and fully take advantage of the decoder-only model, we need to jointly train both contrastive losses and text-generative captioning-like losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s1191/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;532&quot; data-original-width=&quot;1191&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrJfuTxTaxiMFIo1c34KaN-CRsNQcUV4WSaxNTCV-y0CSQodJECx4k9QT0Ww6xjF1hVgJ7zioahWWiBWvMYoIiVKsDFF5p78ACe7f7j-M9DtfOwmnNznwt7pfCBmsO-jU-QiOWCbL4IiLgSXnYMa23b9LSk6Hyb25_ZVeRwPj86LSOvyQqpA1pugzFaw/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT architecture (&lt;b>;left&lt;/b>;) is a simple construct consisting of a single vision encoder and a single text decoder. Compared to other popular vision-language models — eg, &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI&lt;/a>; (&lt;b>;middle&lt;/b>;) and &lt;a href=&quot;https://arxiv.org/abs/2107.07651&quot;>;ALBEF&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2205.01917&quot;>;CoCa&lt;/a>; (&lt;b>;right&lt;/b>;) — it trains jointly and efficiently for multiple vision-language tasks, with both contrastive and text-generative losses, fully sharing the weights between the tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Decoder two-pass learning&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1801.10198&quot;>;Decoder-only models&lt;/a>; for language learning show clear advantages in performance with smaller model size (almost half the parameters). The main challenge for applying them to multimodal settings is to unify the contrastive learning (which uses unconditional sequence-level representation) with captioning (which optimizes the likelihood of a token conditioned on the previous tokens). We propose a two-pass approach to jointly learn these two conflicting types of text representations within the decoder. During the first pass, we utilize cross attention and &lt;a href=&quot;https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer&quot;>;causal masking&lt;/a>; to learn the caption generation task — the text features can attend to the image features and predict the tokens in sequence. On the second pass, we disable the cross-attention and causal masking to learn the contrastive task. The text features will not see the image features but can attend bidirectionally to all text tokens at once to produce the final text-based representation. Completing this two-pass approach within the same decoder allows for accommodating both types of tasks that were previously hard to reconcile. While simple, we show that this model architecture is able to provide a foundation for multiple multimodal tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s670/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;314&quot; data-original-width=&quot;670&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRePkirehczoN54jZwC69K_aoj7BraBtL3rHmX2Q6FDfkwauOF-ODzu-TzhMv5c2PdTZln40s_AImgBHz01ASHMN4ZybMA0gVmq6A2GSP9L9b3uslrnE3256A6v-hp-ZEy2H6Az1HCFhlnKt6h-YjN_BwS3rGMUnq_YzrKOWPoNGL31hcPJ6vpbBnNIg/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT decoder-only two-pass learning enables both contrastive and generative learning paths by the same model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Another advantage of our architecture is that, since it is trained for these disjoint tasks, it can be seamlessly applied to multiple applications such as image-text and text-image retrieval, VQA, and captioning. &lt;/p>; &lt;p>; Moreover, MaMMUT easily adapts to video-language tasks. Previous approaches used a vision encoder to process each frame individually, which required applying it multiple times. This is slow and restricts the number of frames the model can handle, typically to only 6–8. With MaMMUT, we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>; for lightweight adaptation directly via the spatio-temporal information from the video. Furthermore, adapting the model to &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;Open-Vocabulary Detection&lt;/a>; is done by simply training to detect bounding-boxes via an object-detection head. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s632/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;204&quot; data-original-width=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihF47QWAtA-rAe5KLd6H5SlOjsT5SRS7GhLbujvzg0xpRTrNgsuttkJIOjFlMSoQGxKDkBL5kGVX0waDNr3-4ewW1pkZEHk8RcA39HX7w3j7lAcwOIw0OBcAiSSQRA1NERhMp2GMDpaHgkAlByZJ1uVcp-MWU4KFQJMBXt1Sr_duxGa3rN_X6GYIHh7w/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Adaptation of the MaMMUT architecture to video tasks (&lt;b>;left&lt;/b>;) is simple and fully reuses the model. This is done by generating a video “tubes” feature representation, similar to image patches, that are projected to lower dimensional tokens and run through the vision encoder. Unlike prior approaches (&lt;b>;right&lt;/b>;) that need to run multiple individual images through the vision encoder, we use it only once.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Our model achieves excellent zero-shot results on image-text and text-image retrieval without any adaptation, outperforming all previous state-of-the-art models. The results on VQA are competitive with state-of-the-art results, which are achieved by much larger models. The &lt;a href=&quot;https://arxiv.org/abs/2209.06794&quot;>;PaLI model&lt;/a>; (17B parameters) and the &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo model&lt;/a>; (80B) have the best performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>;, but MaMMUT (2B) has the same accuracy as the 15B PaLI. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s600/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrJFan32k1qe9UYjl8S0wX9PyYc4myFwpjtL70v4w6FcZYkwZ_ruQd5NADBFfQrVJwO5I55Zl6_MhQSQsA2xojPH4CnBdNlWuz8zPAfxP8j54YQUC5252Hs3jN5ErpnkfiiILuXL5GBHBh4EKI9rETHhQmvXLGmN7lo5EoV_30kpHQFGnNufz3YX7yw/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s600/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIZMvhSGCzClyc92aahNZzS8ru6T0cMh6jY0JHwUVUMTXzcmZUmPj3E3Yq-j1Xy0AG_VFNmE5B--CFQACoQaJmPt8-bbo6FeA9oroJgGP-PUhNJGnwq4_J_C9virlIu30c7FGhLNGpzdYc7-H5dVUSAkJE3z9BMbev_ZL4T14gYxRJSPkcYDZu9Y8Qag/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the state of the art (SOTA) on Zero-Shot Image-Text (I2T) and Text-Image (T2I) retrieval on both &lt;a href=&quot;https://arxiv.org/abs/1504.00325&quot;>;MS-COCO&lt;/a>; (&lt;b>;top&lt;/b>;) and &lt;a href=&quot;https://ieeexplore.ieee.org/document/7410660&quot;>;Flickr&lt;/a>; (&lt;b>;bottom&lt;/b>;) benchmarks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s600/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioruxeWxIKiiAjCLOb2_AT-DrSBI0El7BDO_hO2U-LLDnJXhu2N6c2lfVEj7bIp2GJ2oF2UPnuXUBQwRy_z_Kvfu_2PqShjKi1Ak3kvxRecmHHIcnZGhD6cyEnMy72PAf2izaEa_DMSBhlfjtjLQyjSptFuHtNHry6MEnL5LbYMOi9vqtat70WpN3pbA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance on the &lt;a href=&quot;https://visualqa.org/&quot;>;VQA2.0 dataset&lt;/a>; is competitive but does not outperform large models such as Flamingo-80B and PalI-17B. Performance is evaluated in the more challenging open-ended text generation setting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; MaMMUT also outperforms the state-of-the-art on VideoQA, as shown below on the &lt;a href=&quot;https://ieeexplore.ieee.org/document/7780940&quot;>;MSRVTT-QA&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3123266.3123427&quot;>;MSVD-QA&lt;/a>; datasets. Note that we outperform much bigger models such as &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&quot;>;Flamingo&lt;/a>;, which is specifically designed for image+video pre-training and is pre-trained with both image-text and video-text data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s600/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQIu-j_ORVRUXKeMjJNnLbvTScNct-Fv14_jy-y2kvMbu0UcuNOm_pQTe1fqA8XblGMS4qNbbCP0wk3EwpXBwwpBmjOqIm0WznRqKPSGiAkjtoOkG3K6hHB5Sa-FpeAdkaF8_KaXdz-Hext1_SBlMFeY8TRSFMXwtl1xR6sITQ6xMm7XkMrd9XrIYhbg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s600/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwn4A39cTqXGhA-HgDPSERXNOI6hggrFSmWGtCSIbCbOnAilYKlhlDa_H4X6QuU7DiVNaya6uITKlFFod33Vlm5M631dp73u5zsZVQBu8AyPpu25JZMDnyiXmqgnr_z3KEiQ-BoUhjPcYWweM01mJzdKLs6i0-30fVkjqK2bvGg2ppkzBjaI_Tw1YPWQ/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MaMMUT outperforms the SOTA models on VideoQA tasks (MSRVTT-QA dataset, &lt;b>;top&lt;/b>;, MSVD-QA dataset,&lt;b>; bottom&lt;/b>;), outperforming much larger models, eg, the 5B GIT2 or Flamingo, which uses 80B parameters and is pre-trained for both image-language and vision-language tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results outperform the state-of-the-art on open-vocabulary detection fine-tuning as is also shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s600/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh20_80l5_7HbEfLKtkl0a8AwUqgOPgHva2en4nOUbqMyjANIMBp-RwWZamt7GQfSnEzyXPnADFbcyIhUkQVfY44rqRiTnEv0l2rjZ7FJYHVhIjE24EIUf9DhpQGkgoDjn3o1K_m-mrcFMfA6ycOSI2MclXmNyr1RYEmjCI694BxzngTGXVL9bcl35W6g/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;MAMMUT &lt;a href=&quot;https://arxiv.org/abs/2104.13921&quot;>;open-vocabulary detection&lt;/a>; results on the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;LVIS&lt;/a>; dataset compared to state-of-the-art methods. We report the &lt;a href=&quot;https://arxiv.org/abs/1908.03195&quot;>;average precisions for rare classes&lt;/a>; (APr) as is previously adopted in the literature.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Key ingredients&lt;/h2>; &lt;p>; We show that joint training of both contrastive and text-generative objectives is not an easy task, and in our ablations we find that these tasks are served better by different design choices. We see that fewer cross-attention connections are better for retrieval tasks, but more are preferred by VQA tasks. Yet, while this shows that our model&#39;s design choices might be suboptimal for individual tasks, our model is more effective than more complex, or larger, models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s600/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBEkGSnnhxudBxkKF6j9y2VDIsHw6_TVfB2e3a0zxuI70LR7BS36oWUXoBG13tOqVMUygmi7uLHh6rnOWEALSdujHNh5iSUfOEMp2hTw0L4I-t5Jp2O8Sj0hpoYoC50asshxoaQgrdMEci-c7zPOaS9znnWy8WJqdJveWlDlN1k5UKLIJDcS0-Ipru-Q/s16000/image11.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s600/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;371&quot; data-original-width=&quot;600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgraznHk4GwR-5XkHHCVzBqq-F-XC11rdag5OlGe57beIUIc_yDZsZkcmALVd7ed_x-Cy_YNBxPUuu7xj3K_rXDrmBC_jvm8VxgZBSSSZ04TRe5NkXN5fsRHAuleUXZdFe6O0JiVFpa5U24bz2xAfQqqvDqTz5IN5wanVwDYRsQ9SJGYO09U4E3bCr5w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Ablation studies showing that fewer cross-attention connections (1-2) are better for retrieval tasks (&lt;b>;top&lt;/b>;), whereas more connections favor text-generative tasks such as VQA (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented MaMMUT, a simple and compact vision-encoder language-decoder model that jointly trains a number of conflicting objectives to reconcile contrastive-like and text-generative tasks. Our model also serves as a foundation for many more vision-language tasks, achieving state-of-the-art or competitive performance on image-text and text-image retrieval, videoQA, video captioning, open-vocabulary detection and VQA. We hope it can be further used for more multimodal applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The work described is co-authored by: Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, and Anelia Angelova. We would like to thank Mojtaba Seyedhosseini, Vijay Vasudevan, Priya Goyal, Jiahui Yu, Zirui Wang, Yonghui Wu, Runze Li, Jie Mei, Radu Soricut, Qingqing Huang, Andy Ly, Nan Du, Yuxin Wu, Tom Duerig, Paul Natsev, Zoubin Ghahramani for their help and support. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/7102791933497880989/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7102791933497880989&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html&quot; rel=&quot;alternate&quot; title=&quot;MaMMUT: A simple vision-encoder text-decoder architecture for multimodal tasks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh53KlJZUXTHEd1ZhRav_9Hwl-MzCVTzans8VhEzushmfeKHUBfNDKTIPpVEbrDhtxlZWeBgLYsIsi6krB_GefP0SrNX-92H3eunTcCwjAH_t2KBW8wVMzZlvYbiltJM5xMFhy9Euclq7q33HgKgdvmsoXnOIbL-RkGMDeHn_ocy2puVKIqfkJ05REmuA/s72-c/MAMMUT.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3611317650594898640&lt;/id>;&lt;published>;2023-05-03T10:22:00.000-07:00&lt;/published>;&lt;updated>;2023-05-03T10:22:54.056-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Robotics&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Joanne Truong, Student Researcher, and Wenhao Yu, Research Scientist, Robotics at Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s320/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; style=&quot;display: none;&quot; />; &lt;p>; Teaching mobile robots to navigate in complex outdoor environments is critical to real-world applications, such as delivery or &lt;a href=&quot;https://www.nasa.gov/stem-ed-resources/robotic-search-and-rescue-challenge.html&quot;>;search and rescue&lt;/a>;. However, this is also a challenging problem as the robot needs to perceive its surroundings, and then explore to identify feasible paths towards the goal. Another common challenge is that the robot needs to overcome uneven terrains, such as stairs, curbs, or rockbed on a trail, while avoiding obstacles and pedestrians. In our prior work, we investigated the second challenge by teaching a quadruped robot to tackle challenging &lt;a href=&quot;https://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html&quot;>;uneven obstacles&lt;/a>; and &lt;a href=&quot;https://research.google/pubs/pub51639/&quot;>;various outdoor terrains&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/pdf/2305.01098.pdf&quot;>;IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience&lt;/a>;”, we present our recent work to tackle the robotic challenge of reasoning about the perceived surroundings to identify a viable navigation path in outdoor environments. We introduce a learning-based indoor-to-outdoor transfer algorithm that uses deep reinforcement learning to train a navigation policy in simulated indoor environments, and successfully transfers that same policy to real outdoor environments. We also introduce Context-Maps (maps with environment observations created by a user), which are applied to our algorithm to enable efficient long-range navigation. We demonstrate that with this policy, robots can successfully navigate hundreds of meters in novel outdoor environments, around previously unseen outdoor obstacles (trees, bushes, buildings, pedestrians, etc.), and in different weather conditions (sunny, overcast, sunset). &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/teaser.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;PointGoal navigation&lt;/h2>; &lt;p>; User inputs can tell a robot where to go with commands like “go to the Android statue”, pictures showing a target location, or by simply picking a point on a map. In this work, we specify the navigation goal (a selected point on a map) as a relative coordinate to the robot&#39;s current position (ie, “go to ∆x, ∆y”), this is also known as the &lt;a href=&quot;https://arxiv.org/abs/1807.06757&quot;>;PointGoal Visual Navigation&lt;/a>; (PointNav) task. PointNav is a general formulation for navigation tasks and is one of the standard choices for indoor navigation tasks. However, due to the diverse visuals, uneven terrains and long distance goals in outdoor environments, training PointNav policies for outdoor environments is a challenging task. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Indoor-to-outdoor transfer&lt;/h2>; &lt;p>; Recent successes in training wheeled and legged robotic agents to navigate in &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf&quot;>;indoor&lt;/a>; &lt;a href=&quot;https://arxiv.org/pdf/1911.00357.pdf&quot;>;environments&lt;/a>; were enabled by the development of fast, scalable simulators and the availability of large-scale datasets of &lt;a href=&quot;https://aihabitat.org/datasets/hm3d/&quot;>;photorealistic&lt;/a>; &lt;a href=&quot;https://svl.stanford.edu/igibson/&quot;>;3D scans&lt;/a>; &lt;a href=&quot;https://niessner.github.io/Matterport/&quot;>;of indoor&lt;/a>; &lt;a href=&quot;https://ai2thor.allenai.org/&quot;>;environments&lt;/a>;. To leverage these successes, we develop an indoor-to-outdoor transfer technique that enables our robots to learn from simulated indoor environments and to be deployed in real outdoor environments. &lt;/p>; &lt;p>; To overcome the differences between simulated indoor environments and real outdoor environments, we apply &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;kinematic control&lt;/a>; and image augmentation techniques in our learning system. When using kinematic control, we assume the existence of a reliable low-level &lt;a href=&quot;https://dev.bostondynamics.com/docs/concepts/autonomy/graphnav_and_robot_locomotion&quot;>;locomotion controller&lt;/a>; that can control the robot to precisely reach a new location. This assumption allows us to directly move the robot to the target location during simulation training through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler_method&quot;>;forward Euler integration&lt;/a>; and relieves us from having to explicitly model the underlying robot dynamics in simulation, which drastically improves the throughput of simulation data generation. &lt;a href=&quot;https://arxiv.org/abs/2207.10821&quot;>;Prior work&lt;/a>; has shown that kinematic control can lead to better sim-to-real transfer compared to a &lt;a href=&quot;https://arxiv.org/pdf/2008.11867.pdf&quot;>;dynamic control approach&lt;/a>;, where full robot dynamics are modeled and a low-level locomotion controller is required for moving the robot. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/kin_v_dyn.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;strong>;Left&lt;/strong>; Kinematic control; &lt;strong>;Right&lt;/strong>;: Dynamic control&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We created an outdoor maze-like environment using objects found indoors for initial experiments, where we used Boston Dynamics&#39; &lt;a href=&quot;https://www.bostondynamics.com/products/spot&quot;>;Spot robot&lt;/a>; for test navigation. We found that the robot could navigate around novel obstacles in the new outdoor environment. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_short_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Spot robot successfully navigates around obstacles found in indoor environments, with a policy trained entirely in simulation.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; However, when faced with unfamiliar outdoor obstacles not seen during training, such as a large slope, the robot was unable to navigate the slope. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The robot is unable to navigate up slopes, as slopes are rare in indoor environments and the robot was not trained to tackle it.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to walk up and down slopes, we apply an image augmentation technique during the simulation training. Specifically, we randomly tilt the simulated camera on the robot during training. It can be pointed up or down within 30 degrees. This augmentation effectively makes the robot perceive slopes even though the floor is level. Training on these perceived slopes enables the robot to navigate slopes in the real-world. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/slope_success.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;By randomly tilting the camera angle during training in simulation, the robot is now able to walk up and down slopes.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Since the robots were only trained in simulated indoor environments, in which they typically need to walk to a goal just a few meters away, we find that the learned network failed to process longer-range inputs — eg, the policy failed to walk forward for 100 meters in an empty space. To enable the policy network to handle long-range inputs that are common for outdoor navigation, we normalize the goal vector by using the log of the goal distance. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Context-Maps for complex long-range navigation &lt;/h2>; &lt;p>; Putting everything together, the robot can navigate outdoors towards the goal, while walking on uneven terrain, and avoiding trees, pedestrians and other outdoor obstacles. However, there is still one key component missing: the robot&#39;s ability to plan an efficient long-range path. At this scale of navigation, taking a wrong turn and backtracking can be costly. For example, we find that the local exploration strategy learned by standard PointNav policies are insufficient in finding a long-range goal and usually leads to a dead end (shown below). This is because the robot is navigating without context of its environment, and the optimal path may not be visible to the robot from the start. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; style=&quot;margin-left: 10%; margin-right: 10%;&quot; width=&quot;80%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/pointnav_long_fail.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Navigation policies without context of the environment do not handle complex long-range navigation goals.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; To enable the robot to take the context into consideration and purposefully plan an efficient path, we provide a Context-Map (a binary image that represents a top-down occupancy map of the region that the robot is within) as additional observations for the robot. An example Context-Map is given below, where the black region denotes areas occupied by obstacles and white region is walkable by the robot. The green and red circle denotes the start and goal location of the navigation task. Through the Context-Map, we can provide hints to the robot (eg, the narrow opening in the route below) to help it plan an efficient navigation route. In our experiments, we create the Context-Map for each route guided by &lt;a href=&quot;https://www.google.com/maps&quot;>;Google Maps satellite&lt;/a>; images. We denote this variant of PointNav with environmental context, as &lt;em>;Context-Guided PointNav&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/s1113/image12.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;608&quot; data-original-width=&quot;1113&quot; height=&quot;350&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLMeRnkXK13rksKz4NKZ-eB1wt9N6HsAM723XJihrvcV2PQLV4JwGB5Q1pWudaxXnAVdw5eK1_szIn43kIl8uLQ2H-Hk1gL-EsiRHwIZTXr1LH_Bq8mOG3vVsKfuUZHcUc0ddJlrkhnp41KVUDkcwOHZBCznZQ7IDX_e0DJvO1qusa8gb_ACrGP3hFuA/w640-h350/image12.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the Context-Map (&lt;strong>;right&lt;/strong>;) for a navigation task (&lt;strong>;left&lt;/strong>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; It is important to note that the Context-Map does not need to be accurate because it only serves as a rough outline for planning. During navigation,&lt;strong>; &lt;/strong>;the robot still needs to rely on its onboard cameras to identify and adapt its path to pedestrians, which are absent on the map. In our experiments, a human operator quickly sketches the Context-Map from the satellite image, masking out the regions to be avoided. This Context-Map, together with other onboard sensory inputs, including depth images and relative position to the goal, are fed into a neural network with &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention&lt;/a>; models (ie, &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;transformers&lt;/a>;), which are trained using &lt;a href=&quot;https://arxiv.org/abs/1911.00357&quot;>;DD-PPO&lt;/a>;, a distributed implementation of &lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;>;proximal policy optimization&lt;/a>;, in large-scale simulations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/s782/image15.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;782&quot; height=&quot;442&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpnovuZC9I0U3OEk1vTb7afyeVG-Kh10yHQWANbgVCZK6cMhbSLYrcA5bppocAxAJUpmV6O8Fh5Ix9pxxwcjeb4U7GVeyasNAUmVYnnl3DS04EjFLxLMn8QtyWuVYQWC_Lj0VBSRQz1w-kl0bw0QDIhemcZXwNOzRsh990vdsafciIeHtbGUECSNzKTQ/w640-h442/image15.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Context-Guided PointNav architecture consists of a 3-layer &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural network&lt;/a>; (CNN) to process depth images from the robot&#39;s camera, and a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) to process the goal vector. The features are passed into a &lt;a href=&quot;https://arxiv.org/abs/1406.1078&quot;>;gated recurrent unit&lt;/a>; (GRU). We use an additional CNN encoder to process the context-map (top-down map). We compute the &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;scaled dot product attention&lt;/a>; between the map and the depth image, and use a second GRU to process the attended features (Context Attn., Depth Attn.). The output of the policy are linear and angular velocities for the Spot robot to follow. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our system across three long-range outdoor navigation tasks. The provided Context-Maps are rough, incomplete environment outlines that omit obstacles, such as cars, trees, or chairs. &lt;/p>; &lt;p>; With the proposed algorithm, our robot can successfully reach the distant goal location 100% of the time, without a single collision or human intervention. The robot was able to navigate around pedestrians and real-world clutter that are not present on the context-map, and navigate on various terrain including dirt slopes and grass. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 1&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s1999/image14.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;700&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDnVLTyUxrCK_2JqEYt7aMmjLHet9YMEgsJYayFevK9abDYy8qh9DYCCWB8NUO5VIPrzh7CX1QxnSmCmWSZQq-rD7sxv4k_EyZRyB-jb0VtNObh6pDTPQRg-mNzzdFPONZx05lDfbraoUMwbKRToPYLZFgDMSgiehPDj_4JGRMSflJJFuUdo7P6qb6mA/s16000/image14.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route1_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 2&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s1999/image16.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;697&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQMH1-yhVELFWTo13ZAth593Q24BwNBU-3HrzBztOc1g5cu27sCBZQJDs9UPY_G0wXJQFAIbLf_sUWGAf_TMssTNQLBIpZQ1MtS7hWDq64ftv0uAB2sIqjHiTLLoOGsXrPyhn6Js2mzP2Im2B-34W6CoGzifvoA6UUA4ERYOPMf5VmqYh5bJeQgKjvTA/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route2_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Route 3&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s1999/image13.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;702&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_IZrPCB1CqRJvCf8emAbFDmsTKDGSbvYMcePepXhuL2CsApXGS0ZeJZr-l0mq4TRjqcSEuxZjkYrNjqY7C4dtce2LdAyImw2aXjHwoPHk8_lJPCYkLiTWoOhYqphGDNmviKz347ZdyHntDX_-w-jiA-zNib2yw2WenqDrBrKy-3MrdNrtcQBYagCofg/s16000/image13.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;4&quot; cellspacing=&quot;4&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_context.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://github.com/joannetruong/website_media/blob/master/projects/i2o/blog/route3_nocontext.mp4?raw=true&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work opens up robotic navigation research to the less explored domain of diverse outdoor environments. Our indoor-to-outdoor transfer algorithm uses zero real-world experience and does not require the simulator to model predominantly-outdoor phenomena (terrain, ditches, sidewalks, cars, etc). The success in the approach comes from a combination of a robust locomotion control, low sim-to-real gap in depth and map sensors, and large-scale training in simulation. We demonstrate that providing robots with approximate, high-level maps can enable long-range navigation in novel outdoor environments. Our results provide compelling evidence for challenging the (admittedly reasonable) hypothesis that a new simulator must be designed for every new scenario we wish to study. For more information, please see our &lt;a href=&quot;https://indoorsim2outdoorreal.github.io/&quot;>;project page&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;We would like to thank Sonia Chernova, Tingnan Zhang, April Zitkovich, Dhruv Batra, and Jie Tan for advising and contributing to the project. We would also like to thank Naoki Yokoyama, Nubby Lee, Diego Reyes, Ben Jyenis, and Gus Kouretas for help with the robot experiment setup.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/3611317650594898640/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3611317650594898640&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/05/indoorsim-to-outdoorreal-learning-to.html&quot; rel=&quot;alternate&quot; title=&quot;IndoorSim-to-OutdoorReal: Learning to navigate outdoors without any outdoor experience&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmX_nD3A8PWm8-BTB-ACmoSoiFt16URvxjvqYmkYFTTDdB8ykipmO9QH3mUTi5fWgoPe3A-LEfG4McZfZPmxj2WxI_g2TMr5qTDxYXjgqgGDd_CBBcjyWJIpAE_pHlFWtxvmkHM68E3TeodFyF_YvLC1Gtw1cGQaERfhIcq1jelMX-73KtlyqXzJ1D8g/s72-c/IndoorSim-to-OutdoorReal%20hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6812586756904863062&lt;/id>;&lt;published>;2023-04-30T23:37:00.004-07:00&lt;/published>;&lt;updated>;2023-05-17T15:37:33.732-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICLR 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaXU0Qnf7lWC70SIqFUj3KcinepmKmqVwu369RYcajoFZI0UGtej6ZGJD7c0GsXJk_Wjg08CnBLtE7lLlz__azqzUNCgflaTp3F9mZyIxX7HBzJEdTN19WMlfVsu4mTww-LLhvZ4hhWqdVCQqjQbrOk6VumBCH6dA2YwdATT6I_QLKLHSUz_Upnw6zlg/s1200/Google%20Rwanda%20Logo-02.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;Eleventh International Conference on Learning Representations&lt;/a>; (ICLR 2023) is being held this week as a hybrid event in Kigali, Rwanda. We are proud to be a &lt;a href=&quot;https://iclr.cc/Conferences/2023/Sponsors&quot;>;Diamond Sponsor&lt;/a>; of ICLR 2023, a premier conference on deep learning, where Google researchers contribute at all levels. This year we are presenting over 100 papers and are actively involved in organizing and hosting a number of different events, including workshops and interactive sessions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you&#39;re registered for ICLR 2023, we hope you&#39;ll visit the Google booth to learn more about the exciting work we&#39;re doing across topics spanning representation and reinforcement learning, theory and optimization, social impact, safety and privacy, and applications from generative AI to speech and robotics. Continue below to find the many ways in which Google researchers are engaged at ICLR 2023, including workshops, papers, posters and talks (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;p>; Board Members include: &lt;strong>;&lt;em>;Tara Sainath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Senior Program Chairs include: &lt;strong>;&lt;em>;Been Kim&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Workshop Chairs include: &lt;strong>;&lt;em>;Aisha Walcott-Bryant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rose Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; Diversity, Equity &amp;amp; Inclusion Chairs include: &lt;strong>;&lt;em>;Rosanne Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outstanding Paper awards&lt;/h2>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lTt4KjHSsyl&quot;>;Emergence of Maps in the Memories of Blind Navigation Agents&lt;/a>; &lt;br />; &lt;em>;Erik Wijmans&lt;/em>;, &lt;em>;Manolis Savva&lt;/em>;, &lt;strong>;&lt;em>;Irfan Essa&lt;/em>;&lt;/strong>;, &lt;em>;Stefan Lee&lt;/em>;, &lt;em>;Ari S. Morcos&lt;/em>;, &lt;em>;Dhruv Batra&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FjNys5c7VyY&quot;>;DreamFusion: Text-to-3D Using 2D Diffusion&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ben Poole&lt;/em>;&lt;/strong>;, &lt;em>;Ajay Jain&lt;/em>;, &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ben Mildenhall&lt;/em>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Keynote speaker&lt;/h2>; &lt;p>; &lt;a href=&quot;https://iclr.cc/virtual/2023/invited-talk/14236&quot;>;Learned Optimizers: Why They&#39;re the Future, Why They&#39;re Hard, and What They Can Do Now&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jascha Sohl-Dickstein &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://www.kaggle.com/iclr-2023&quot;>;Kaggle@ICLR 2023: ML Solutions in Africa&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Julia Elliott&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Phil Culliton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ray Harvey &lt;/i>;&lt;/b>;&lt;br />; Facilitators: &lt;b>;&lt;i>;Julia Elliot&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Walter Reade &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://reincarnating-rl.github.io/&quot;>;Reincarnating Reinforcement Learning&lt;/a>; (Reincarnating RL) &lt;br />; Organizers include: &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ted Xiao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Max Schwarzer &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;br />; Panelists include: &lt;b>;&lt;i>;Marc G. Bellemare&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://rtml-iclr2023.github.io/organizers.html&quot;>;Trustworthy and Reliable Large-Scale Machine Learning Models&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Sanmi Koyejo &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Nicholas Carlini &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://physics4ml.github.io/organizers&quot;>;Physics for Machine Learning&lt;/a>; (Physics4ML) &lt;br />; Speakers include: &lt;b>;&lt;i>;Yasaman Bahri &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://ai4abm.org/workshop_iclr2023/&quot;>;AI for Agent-Based Modelling Community&lt;/a>; (AI4ABM) &lt;br />; Organizers include: &lt;b>;&lt;i>;Pablo Samuel Castro &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/me-fomo2023&quot;>;Mathematical and Empirical Understanding of Foundation Models&lt;/a>; (ME-FoMo) &lt;br />; Organizers include: &lt;b>;&lt;i>;Mathilde Caron&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tengyu Ma&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanie Sedghi &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Yasaman Bahri&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yann Dauphin &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://nesygems.github.io/&quot;>;Neurosymbolic Generative Models 2023&lt;/a>; (NeSy-GeMs) &lt;br />; Organizers include: &lt;b>;&lt;i>;Kevin Ellis &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Daniel Tarlow&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tuan Anh Le &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://domaingen.github.io/&quot;>;What Do We Need for Successful Domain Generalization?&lt;/a>; &lt;br />; Panelists include: &lt;b>;&lt;i>;Boqing Gong &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://pml4dc.github.io/iclr2023/&quot;>;The 4th Workshop on Practical ML for Developing Countries: Learning Under Limited/Low Resource Settings&lt;/a>; &lt;br />; Keynote Speaker: &lt;b>;&lt;i>;Adji Bousso Dieng &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://nasaharvest.github.io/ml-for-remote-sensing/iclr2023/#schedule&quot;>;Machine Learning for Remote Sensing&lt;/a>; &lt;br />; Speakers include: &lt;b>;&lt;i>;Abigail Annkah &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://mrl-workshop.github.io/iclr-2023/&quot;>;Multimodal Representation Learning (MRL): Perks and Pitfalls&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Petra Poklukar &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Arsha Nagrani &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/trustml-unlimited/home&quot;>;Pitfalls of Limited Data and Computation for Trustworthy ML&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Praneeth Netrapalli &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/trustml-unlimited/home&quot;>;Sparsity in Neural Networks: On Practical Limitations and Tradeoffs Between Sustainability and Efficiency&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Trevor Gale&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Utku Evci &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Aakanksha Chowdhery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeff Dean &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/tsrl4h-iclr2023/home&quot;>;Time Series Representation Learning for Health&lt;/a>; &lt;br />; Speakers include: &lt;b>;&lt;i>;Katherine Heller &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl4c.github.io/&quot;>;Deep Learning for Code&lt;/a>; (DL4C) &lt;br />; Organizers include: &lt;b>;&lt;i>;Gabriel Orlanski &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Alex Polozov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Tarlow &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://mlgh-2023.netlify.app/&quot;>;Machine Learning and Global Health&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Mercy Asiedu &lt;/i>;&lt;/b>;&lt;br />; Speakers include: &lt;b>;&lt;i>;Joelle Barral&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Affinity Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://iclr.cc/virtual/2023/affinity-workshop/13838&quot;>;Tiny Papers Showcase Day&lt;/a>; (a DEI initiative) &lt;br />; Organizers include: &lt;b>;&lt;i>;Rosanne Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Papers&lt;/h2>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Z4s7sJYQM&quot;>;Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics for Advection-Dominated Systems&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhong Yi Wan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Leonardo Zepeda-Nunez&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anudhyan Boral&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fei Sha &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TatRHT_1cK&quot;>;Quantifying Memorization Across Neural Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daphne Ippolito&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Jagielski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katherine Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Florian Tramer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lTt4KjHSsyl&quot;>;Emergence of Maps in the Memories of Blind Navigation Agents&lt;/a>; (&lt;em>;Outstanding Paper Award&lt;/em>;) &lt;br />;&lt;i>; Erik Wijmans&lt;/i>;, &lt;i>;Manolis Savva&lt;/i>;, &lt;b>;&lt;i>;Irfan Essa&lt;/i>;&lt;/b>;, &lt;i>;Stefan Lee&lt;/i>;, &lt;i>;Ari S. Morcos&lt;/i>;, &lt;i>;Dhruv Batra &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4-k7kUavAj&quot;>;Offline Q-Learning on Diverse Multi-task Data Both Scales and Generalizes&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Aviral Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;i>;Xingyang Geng&lt;/i>;, &lt;b>;&lt;i>;George Tucker&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Levine &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=WE_vluYUL-X&quot;>;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;i>; Shunyu Yao&lt;/i>;*, &lt;b>;&lt;i>;Jeffrey Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dian Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nan Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Izhak Shafran&lt;/i>;&lt;/b>;, &lt;i>;Karthik R. Narasimhan&lt;/i>;, &lt;b>;&lt;i>;Yuan Cao &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=_CDixzkzeyb&quot;>;Prompt-to-Prompt Image Editing with Cross-Attention Control&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Amir Hertz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ron Mokady&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jay Tenenbaum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Kfir Aberman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yael Pritch&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Cohen-Or &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FjNys5c7VyY&quot;>;DreamFusion: Text-to-3D Using 2D Diffusion&lt;/a>; (&lt;em>;Outstanding Paper Award&lt;/em>;) &lt;br />;&lt;i>;&lt;b>; Ben Poole&lt;/b>;&lt;/i>;, &lt;i>;Ajay Jain&lt;/i>;, &lt;b>;&lt;i>;Jonathan T. Barron&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ben Mildenhall &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HcUf-QwZeFh&quot;>;A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Hiroki Furuta&lt;/i>;&lt;/b>;, &lt;i>;Yusuke Iwasawa&lt;/i>;, &lt;i>;Yutaka Matsuo&lt;/i>;, &lt;b>;&lt;i>;Shixiang Shane Gu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OpC-9aBBVJe&quot;>;Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier&lt;/a>; &lt;br />;&lt;i>; Pierluca D&#39;Oro&lt;/i>;, &lt;b>;&lt;i>;Max Schwarzer&lt;/i>;&lt;/b>;, &lt;i>;Evgenii Nikishin&lt;/i>;, &lt;i>;Pierre-Luc Bacon&lt;/i>;, &lt;b>;&lt;i>;Marc G Bellemare&lt;/i>;&lt;/b>;, &lt;i>;Aaron Courville &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=DEGjDDV22pI&quot;>;Dichotomy of Control: Separating What You Can Control from What You Cannot&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Sherry Yang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Pieter Abbeel&lt;/i>;, &lt;b>;&lt;i>;Ofir Nachum &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=7JsGYvjE88d&quot;>;Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search&lt;/a>; &lt;br />;&lt;i>; Michał Zawalski&lt;/i>;, &lt;i>;Michał Tyrolski&lt;/i>;, &lt;i>;Konrad Czechowski&lt;/i>;, &lt;i>;Tomasz Odrzygóźdź&lt;/i>;, &lt;i>;Damian Stachura&lt;/i>;, &lt;i>;Piotr Piekos&lt;/i>;, &lt;b>;&lt;i>;Yuhuai Wu&lt;/i>;&lt;/b>;, &lt;i>;Łukasz Kucinski&lt;/i>;, &lt;i>;Piotr Miłos &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rvsbw2YthH_&quot;>;The Trade-Off Between Universality and Label Efficiency of Representations from Contrastive Learning&lt;/a>; &lt;br />;&lt;i>; Zhenmei Shi&lt;/i>;, &lt;i>;Jiefeng Chen&lt;/i>;, &lt;i>;Kunyang Li&lt;/i>;, &lt;i>;Jayaram Raghuram&lt;/i>;, &lt;b>;&lt;i>;Xi Wu&lt;/i>;&lt;/b>;, &lt;i>;Yingyu Liang&lt;/i>;, &lt;i>;Somesh Jha &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=yHY9NbQJ5BP&quot;>;Sparsity-Constrained Optimal Transport&lt;/a>; &lt;br />;&lt;i>; Tianlin Liu&lt;/i>;*, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mathieu Blondel &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=xSsW2Am-ukZ&quot;>;Unmasking the Lottery Ticket Hypothesis: What&#39;s Encoded in a Winning Ticket&#39;s Mask?&lt;/a>; &lt;br />;&lt;i>; Mansheej Paul&lt;/i>;, &lt;i>;Feng Chen&lt;/i>;, &lt;i>;Brett W. Larsen&lt;/i>;, &lt;i>;Jonathan Frankle&lt;/i>;, &lt;i>;Surya Ganguli&lt;/i>;, &lt;b>;&lt;i>;Gintare Karolina Dziugaite &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SJ0Lde3tRL&quot;>;Extreme Q-Learning: MaxEnt RL without Entropy&lt;/a>; &lt;br />;&lt;i>; Divyansh Garg&lt;/i>;, &lt;i>;Joey Hejna&lt;/i>;, &lt;b>;&lt;i>;Matthieu Geist,&lt;/i>;&lt;/b>; &lt;i>;Stefano Ermon &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SMa9EAovKMC&quot;>;Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs&lt;/a>; &lt;br />;&lt;i>; Albert Qiaochu Jiang&lt;/i>;, &lt;i>;Sean Welleck&lt;/i>;, &lt;b>;&lt;i>;Jin Peng Zhou&lt;/i>;&lt;/b>;, &lt;i>;Timothee Lacroix&lt;/i>;, &lt;i>;Jiacheng Liu&lt;/i>;, &lt;i>;Wenda Li&lt;/i>;, &lt;i>;Mateja Jamnik&lt;/i>;, &lt;i>;Guillaume Lample&lt;/i>;, &lt;b>;&lt;i>;Yuhuai Wu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=EKpMeEV0hOo&quot;>;SimPer: Simple Self-Supervised Learning of Periodic Targets&lt;/a>; &lt;br />;&lt;i>; Yuzhe Yang&lt;/i>;, &lt;i>;Xin Liu&lt;/i>;, &lt;i>;&lt;b>;Jiang Wu&lt;/b>;&lt;/i>;, &lt;i>;&lt;b>;Silviu Borac&lt;/b>;&lt;/i>;, &lt;i>;Dina Katabi&lt;/i>;, &lt;b>;&lt;i>;Ming-Zher Poh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel McDuff &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=G2Q2Mh3avow&quot;>;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Andy Zeng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Maria Attarian&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Brian Ichter&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krzysztof Marcin Choromanski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adrian Wong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Stefan Welker&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Federico Tombari&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aveek Purohit&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael S. Ryoo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vikas Sindhwani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Johnny Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vincent Vanhoucke&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pete Florence &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=0g0X4H8yN4I&quot;>;What Learning Algorithm Is In-Context Learning? Investigations with Linear Models&lt;/a>; &lt;br />;&lt;i>; Ekin Akyurek&lt;/i>;*, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Jacob Andreas&lt;/i>;, &lt;i>;Tengyu Ma&lt;/i>;*, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Peot1SFDX0&quot;>;Preference Transformer: Modeling Human Preferences Using Transformers for RL&lt;/a>; &lt;br />;&lt;i>; Changyeon Kim&lt;/i>;, &lt;i>;Jongjin Park&lt;/i>;, &lt;i>;Jinwoo Shin&lt;/i>;, &lt;i>;Honglak Lee&lt;/i>;, &lt;i>;Pieter Abbeel&lt;/i>;, &lt;b>;&lt;i>;Kimin Lee &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=QCrw0u9LQ7&quot;>;Iterative Patch Selection for High-Resolution Image Recognition&lt;/a>; &lt;br />;&lt;i>; Benjamin Bergner&lt;/i>;, &lt;i>;Christoph Lippert&lt;/i>;, &lt;b>;&lt;i>;Aravindh Mahendran &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=MIMwy4kh9lf&quot;>;Open-Vocabulary Object Detection upon Frozen Vision and Language Models&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/05/f-vlm-open-vocabulary-object-detection.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Weicheng Kuo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yin Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuye Gu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;AJ Piergiovanni&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Anelia Angelova &lt;/b>;&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JLg5aHHv7j&quot;>;(Certified!!) Adversarial Robustness for Free!&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Florian Tramér&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krishnamurthy (Dj) Dvijotham&lt;/i>;&lt;/b>;, &lt;i>;Leslie Rice&lt;/i>;, &lt;i>;Mingjie Sun&lt;/i>;, &lt;i>;J. Zico Kolter &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gU5sJ6ZggcX&quot;>;REPAIR: REnormalizing Permuted Activations for Interpolation Repair&lt;/a>; &lt;br />;&lt;i>; Keller Jordan&lt;/i>;, &lt;b>;&lt;i>;Hanie Sedghi&lt;/i>;&lt;/b>;, &lt;i>;Olga Saukh&lt;/i>;, &lt;i>;Rahim Entezari&lt;/i>;, &lt;b>;&lt;i>;Behnam Neyshabur &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=VM8batVBWvg&quot;>;Discrete Predictor-Corrector Diffusion Models for Image Synthesis&lt;/a>; &lt;br />;&lt;b>;&lt;i>; José Lezama&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tim Salimans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lu Jiang&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Huiwen Chang&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Jonathan Ho&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Irfan Essa &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=zH9GcZ3ZGXu&quot;>;Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Sravanti Addepalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anshul Nasery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Praneeth Netrapalli&lt;/i>;&lt;/b>;, &lt;i>;Venkatesh Babu R.&lt;/i>;, &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=-CoNloheTs&quot;>;An Exact Poly-time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Amit Daniely&lt;/i>;&lt;/b>;, &lt;i>;Elad Granot &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=fR3wGCk-IXp&quot;>;Language Models Are Multilingual Chain-of-Thought Reasoners&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Freda Shi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mirac Suzgun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Markus Freitag&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;i>;Suraj Srivats&lt;/i>;, &lt;i>;Soroush Vosoughi&lt;/i>;, &lt;b>;&lt;i>;Hyung Won Chung&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sebastian Ruder&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dipanjan Das&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JxpBP1JM15-&quot;>;Scaling Forward Gradient with Local Losses&lt;/a>; &lt;br />;&lt;i>; Mengye Ren&lt;/i>;*, &lt;b>;&lt;i>;Simon Kornblith&lt;/i>;&lt;/b>;, &lt;i>;Renjie Liao&lt;/i>;, &lt;b>;&lt;i>;Geoffrey Hinton &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=DWn1TEb2fK&quot;>;Treeformer: Dense Gradient Trees for Efficient Attention Computation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Lovish Madaan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srinadh Bhojanapalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Himanshu Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Prateek Jain &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=NVZvalzCLg&quot;>;LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification&lt;/a>; &lt;br />;&lt;i>; Sharath Girish&lt;/i>;, &lt;i>;Kamal Gupta&lt;/i>;, &lt;b>;&lt;i>;Saurabh Singh&lt;/i>;&lt;/b>;, &lt;i>;Abhinav Shrivastava &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=nG9RF9z1yy3&quot;>;DiffusER: Diffusion via Edit-Based Reconstruction&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Machel Reid&lt;/i>;&lt;/b>;, &lt;i>;Vincent J. Hellendoorn&lt;/i>;, &lt;i>;Graham Neubig &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ORp91sAbzI&quot;>;Leveraging Unlabeled Data to Track Memorization&lt;/a>; &lt;br />;&lt;i>; Mahsa Forouzesh&lt;/i>;, &lt;b>;&lt;i>;Hanie Sedghi&lt;/i>;&lt;/b>;, &lt;i>;Patrick Thiran &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4FBUihxz5nm&quot;>;A Mixture-of-Expert Approach to RL-Based Dialogue Management&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yinlam Chow&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aza Tulepbergenov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ofir Nachum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dhawal Gupta&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Moonkyung Ryu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Ghavamzadeh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Craig Boutilier &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rSUCajhLsQ&quot;>;Easy Differentially Private Linear Regression&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Kareem Amin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Joseph&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Monica Ribero&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergei Vassilvitskii &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=p0JSSa1AuV&quot;>;KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals&lt;/a>; &lt;br />;&lt;i>; Sandeep Silwal&lt;/i>;*, &lt;b>;&lt;i>;Sara Ahmadian&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew Nystrom&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew McCallum&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Deepak Ramachandran&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mehran Kazemi &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=sIoED-yPK9l&quot;>;Massively Scaling Heteroscedastic Classifiers&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Mark Collier&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rodolphe Jenatton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jesse Berent&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Effrosyni Kokiopoulou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TJ2nxciYCk-&quot;>;The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zonglin Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chong You&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srinadh Bhojanapalli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daliang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sashank J. Reddi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ke Ye&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Chern&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruiqi Guo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gJW8hSGBys8&quot;>;Compositional Semantic Parsing with Large Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Andrew Drozdov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathanael Scharli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ekin Akyurek&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathan Scales&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xinying Song&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xinyun Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Bousquet&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ndYXTEL6cZz&quot;>;Extremely Simple Activation Shaping for Out-of-Distribution Detection&lt;/a>; &lt;br />;&lt;i>; Andrija Djurisic&lt;/i>;, &lt;i>;Nebojsa Bozanic&lt;/i>;, &lt;i>;Arjun Ashok&lt;/i>;, &lt;b>;&lt;i>;Rosanne Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=5MkYIYCbva&quot;>;Long Range Language Modeling via Gated State Spaces&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Harsh Mehta&lt;/i>;&lt;/b>;, &lt;i>;Ankit Gupta&lt;/i>;, &lt;i>;Ashok Cutkosky&lt;/i>;, &lt;i>;Behnam Neyshabur &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=sSt9fROSZRO&quot;>;Investigating Multi-task Pretraining and Generalization in Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Adrien Ali Taiga&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jesse Farebrother&lt;/i>;&lt;/b>;, &lt;i>;Aaron Courville&lt;/i>;, &lt;b>;&lt;i>;Marc G. Bellemare &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=k9CF4h3muD&quot;>;Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets&lt;/a>; &lt;br />;&lt;i>; Edo Cohen-Karlik&lt;/i>;, &lt;i>;Itamar Menuhin-Gruman&lt;/i>;, &lt;i>;Raja Giryes&lt;/i>;, &lt;i>;Nadav Cohen&lt;/i>;, &lt;b>;&lt;i>;Amir Globerson &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=CL-sVR9pvF&quot;>;Weighted Ensemble Self-Supervised Learning&lt;/a>; &lt;br />;&lt;i>; Yangjun Ruan&lt;/i>;*, &lt;b>;&lt;i>;Saurabh Singh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Warren Morningstar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alexander A. Alemi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergey Ioffe&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ian Fischer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joshua V. Dillon &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=0qSOodKmJaN&quot;>;Calibrating Sequence Likelihood Improves Conditional Language Generation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yao Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Misha Khalman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Joshi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shashi Narayan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Saleh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OIe3kpwl40D&quot;>;SMART: Sentences as Basic Units for Text Evaluation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Reinald Kim Amplayo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yao Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shashi Narayan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=9Nj_gNdvqYf&quot;>;Leveraging Importance Weights in Subset Selection&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Gui Citovsky&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Giulia DeSalvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Srikumar Ramalingam&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Afshin Rostamizadeh&lt;/i>;&lt;/b>;, &lt;i>;Yunjuan Wang&lt;/i>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=oGDKSt9JrZi&quot;>;Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks&lt;/a>; &lt;br />;&lt;i>;&lt;b>;Jesse Farebrother&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Joshua Greaves&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Rishabh Agarwal&lt;/i>;&lt;/b>;, &lt;i>;Charline Le Lan&lt;/i>;, &lt;b>;&lt;i>;Ross Goroshin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pablo Samuel Castro&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Marc G. Bellemare &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=n70oyIlS4g&quot;>;An Extensible Multi-modal Multi-task Object Dataset with Materials&lt;/a>; &lt;br />;&lt;i>; Trevor Standley&lt;/i>;, &lt;i>;Ruohan Gao&lt;/i>;, &lt;b>;&lt;i>;Dawn Chen&lt;/i>;&lt;/b>;, &lt;i>;Jiajun Wu&lt;/i>;, &lt;i>;Silvio Savarese &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=7bJizxLKrR&quot;>;Measuring Forgetting of Memorized Training Examples&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Matthew Jagielski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Om Thakkar&lt;/i>;&lt;/b>;, &lt;i>;Florian Tramér&lt;/i>;, &lt;b>;&lt;i>;Daphne Ippolito&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katherine Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;i>;Eric Wallace&lt;/i>;, &lt;b>;&lt;i>;Shuang Song&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Abhradeep Thakurta&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nicolas Papernot&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=wCFB37bzud4&quot;>;Bidirectional Language Models Are Also Few-Shot Learners&lt;/a>; &lt;br />;&lt;i>; Ajay Patel&lt;/i>;, &lt;i>;Bryan Li&lt;/i>;, &lt;i>;Mohammad Sadegh Rasooli&lt;/i>;, &lt;b>;&lt;i>;Noah Constant&lt;/i>;&lt;/b>;, &lt;i>;Colin Raffel&lt;/i>;, &lt;i>;Chris Callison-Burch &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=xE-LtsE-xx&quot;>;Is Attention All That NeRF Needs?&lt;/a>; &lt;br />;&lt;i>; Mukund Varma T.&lt;/i>;, &lt;i>;Peihao Wang&lt;/i>;, &lt;i>;Xuxi Chen&lt;/i>;, &lt;i>;Tianlong Chen&lt;/i>;, &lt;b>;&lt;i>;Subhashini Venugopalan&lt;/i>;&lt;/b>;, &lt;i>;Zhangyang Wang &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=KfptQCEKVW4&quot;>;Automating Nearest Neighbor Search Configuration with Constrained Optimization&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Philip Sun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruiqi Guo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lLp-C5nTdJG&quot;>;Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; David Bieber&lt;/i>;&lt;/b>;, &lt;i>;Rishab Goel&lt;/i>;, &lt;b>;&lt;i>;Daniel Zheng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hugo Larochelle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Tarlow &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gmwDKo-4cY&quot;>;Composing Ensembles of Pre-trained Models via Iterative Consensus&lt;/a>; &lt;br />;&lt;i>; Shuang Li&lt;/i>;, &lt;i>;Yilun Du&lt;/i>;, &lt;i>;Joshua B. Tenenbaum&lt;/i>;, &lt;i>;Antonio Torralba&lt;/i>;, &lt;b>;&lt;i>;Igor Mordatch &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=oztkQizr3kk&quot;>;Λ-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection Among Cells&lt;/a>; &lt;br />;&lt;i>; Sajad Movahedi&lt;/i>;, &lt;i>;Melika Adabinejad&lt;/i>;, &lt;i>;Ayyoob Imani&lt;/i>;, &lt;i>;Arezou Keshavarz&lt;/i>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;i>;Azadeh Shakery&lt;/i>;, &lt;i>;Babak N. Araabi &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=OjDkC57x5sz&quot;>;Blurring Diffusion Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Emiel Hoogeboom&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tim Salimans &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=bAMTaeqluh4&quot;>;Part-Based Models Improve Adversarial Robustness&lt;/a>; &lt;br />;&lt;i>; Chawin Sitawarin&lt;/i>;, &lt;i>;Kornrapat Pongmala&lt;/i>;, &lt;i>;Yizheng Chen&lt;/i>;, &lt;b>;&lt;i>;Nicholas Carlini&lt;/i>;&lt;/b>;, &lt;i>;David Wagner &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=z0_V5O9cmNw&quot;>;Learning in Temporally Structured Environments&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Matt Jones&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tyler R. Scott&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mengye Ren&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gamaleldin ElSayed&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Katherine Hermann&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;David Mayo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael C. Mozer &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TFbwV6I0VLg&quot;>;SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models&lt;/a>; &lt;br />;&lt;i>; Ziyi Wu&lt;/i>;, &lt;i>;Nikita Dvornik&lt;/i>;, &lt;b>;&lt;i>;Klaus Greff&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Thomas Kipf&lt;/i>;&lt;/b>;, &lt;i>;Animesh Garg &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=I29Kt0RwChs&quot;>;Robust Algorithms on Adaptive Inputs from Bounded Adversaries&lt;/a>; &lt;br />;&lt;i>; Yeshwanth Cherapanamjeri&lt;/i>;, &lt;i>;Sandeep Silwal&lt;/i>;, &lt;i>;David P. Woodruff&lt;/i>;, &lt;i>;Fred Zhang&lt;/i>;, &lt;b>;&lt;i>;Qiuyi (Richard) Zhang&lt;/i>;&lt;/b>;, &lt;i>;Samson Zhou &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=EnrY5TOrbQ&quot;>;Agnostic Learning of General ReLU Activation Using Gradient Descent&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Pranjal Awasthi&lt;/i>;&lt;/b>;, &lt;i>;Alex Tang&lt;/i>;, &lt;i>;Aravindan Vijayaraghavan &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=3itjR9QxFw&quot;>;Analog Bits: Generating Discrete Data Using Diffusion Models with Self-Conditioning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ting Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruixiang Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Geoffrey Hinton &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=lEkl0jdSb7B&quot;>;Any-Scale Balanced Samplers for Discrete Space&lt;/a>; &lt;br />;&lt;i>; Haoran Sun&lt;/i>;*, &lt;b>;&lt;i>;Bo Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Charles Sutton&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanjun Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kPPVmUF6bM_&quot;>;Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation&lt;/a>; &lt;br />;&lt;i>; Ziqi Wang&lt;/i>;*, &lt;b>;&lt;i>;Yuexin Wu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frederick Liu&lt;/i>;&lt;/b>;, &lt;i>;Daogao Liu&lt;/i>;, &lt;b>;&lt;i>;Le Hou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hongkun Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Li&lt;/i>;&lt;/b>;, &lt;i>;Heng Ji &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=pOyi9KqE56b&quot;>;Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD&lt;/a>; &lt;br />;&lt;i>; Konstantinos E. Nikolakakis&lt;/i>;, &lt;i>;Farzin Haddadpour&lt;/i>;, &lt;b>;&lt;i>;Amin Karbasi&lt;/i>;&lt;/b>;, &lt;i>;Dionysios S. Kalogerias &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Ha2MnQM9Ph&quot;>;Causal Estimation for Text Data with (Apparent) Overlap Violations&lt;/a>; &lt;br />;&lt;i>; Lin Gui&lt;/i>;, &lt;b>;&lt;i>;Victor Veitch &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=AjC0KBjiMu&quot;>;Contrastive Learning Can Find an Optimal Basis for Approximately View-Invariant Functions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Daniel D. Johnson&lt;/i>;&lt;/b>;, &lt;i>;Ayoub El Hanchi&lt;/i>;, &lt;i>;Chris J. Maddison &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=j1zQGmQQOX1&quot;>;Differentially Private Adaptive Optimization with Delayed Preconditioners&lt;/a>; &lt;br />;&lt;i>; Tian Li&lt;/i>;, &lt;i>;Manzil Zaheer&lt;/i>;, &lt;i>;Ziyu Liu&lt;/i>;, &lt;b>;&lt;i>;Sashank Reddi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Brendan McMahan&lt;/i>;&lt;/b>;, &lt;i>;Virginia Smith &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=3KUfbI9_DQE&quot;>;Distributionally Robust Post-hoc Classifiers Under Prior Shifts&lt;/a>; &lt;br />;&lt;i>; Jiaheng Wei&lt;/i>;*, &lt;b>;&lt;i>;Harikrishna Narasimhan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ehsan Amid&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Wen-Sheng Chu&lt;/i>;&lt;/b>;, &lt;i>;Yang Liu&lt;/i>;, &lt;b>;&lt;i>;Abhishek Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ReDQ1OUQR0X&quot;>;Human Alignment of Neural Network Representations&lt;/a>; &lt;br />;&lt;i>; Lukas Muttenthaler&lt;/i>;, &lt;i>;Jonas Dippel&lt;/i>;, &lt;i>;Lorenz Linhardt&lt;/i>;, &lt;i>;Robert A. Vandermeulen&lt;/i>;, &lt;b>;&lt;i>;Simon Kornblith &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=JpbLyEI5EwW&quot;>;Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data&lt;/a>; &lt;br />;&lt;i>; Spencer Frei&lt;/i>;, &lt;i>;Gal Vardi&lt;/i>;, &lt;b>;&lt;i>;Peter Bartlett&lt;/i>;&lt;/b>;, &lt;i>;Nathan Srebro&lt;/i>;, &lt;i>;Wei Hu &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kUmdmHxK5N&quot;>;Koopman Neural Operator Forecaster for Time-Series with Temporal Distributional Shifts&lt;/a>; &lt;br />;&lt;i>; Rui Wang&lt;/i>;*, &lt;b>;&lt;i>;Yihe Dong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sercan Ö. Arik&lt;/i>;&lt;/b>;, &lt;i>;Rose Yu &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=mQpmZVzXK1h&quot;>;Latent Variable Representation for Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tongzheng Ren&lt;/i>;&lt;/b>;, &lt;i>;Chenjun Xiao&lt;/i>;, &lt;b>;&lt;i>;Tianjun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;i>;Zhaoran Wang&lt;/i>;, &lt;i>;Sujay Sanghavi&lt;/i>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=WZH7099tgfM&quot;>;Least-to-Most Prompting Enables Complex Reasoning in Large Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathanael Scharli&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Le Hou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nathan Scales&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Claire Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Bousquet&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quoc Le&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ed Chi &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=4rXMRuoJlai&quot;>;Mind&#39;s Eye: Grounded Language Model Reasoning Through Simulation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ruibo Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shixiang Shane Gu&lt;/i>;&lt;/b>;, &lt;i>;Te-Yen Wu&lt;/i>;, &lt;i>;Soroush Vosoughi&lt;/i>;, &lt;b>;&lt;i>;Claire Cui&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew M. Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=H0HGljkxQFN&quot;>;MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models&lt;/a>; &lt;br />;&lt;i>; Chenglin Yang&lt;/i>;*, &lt;b>;&lt;i>;Siyuan Qiao&lt;/i>;&lt;/b>;, &lt;i>;Qihang Yu&lt;/i>;, &lt;i>;Xiaoding Yuan&lt;/i>;, &lt;b>;&lt;i>;Yukun Zhu&lt;/i>;&lt;/b>;, &lt;i>;Alan Yuille&lt;/i>;, &lt;b>;&lt;i>;Hartwig Adam&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Liang-Chieh Chen &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=HtoA0oT30jC&quot;>;Novel View Synthesis with Diffusion Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Daniel Watson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;William Chan&lt;/i>;&lt;/b>;,&lt;b>;&lt;i>;&amp;nbsp;Ricardo&lt;/i>;&lt;/b>;&amp;nbsp;&lt;b>;&lt;i>;Martin-Brualla&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jonathan Ho&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrea Tagliasacchi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Norouzi&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=fYzLpCsGZVf&quot;>;On Accelerated Perceptrons and Beyond&lt;/a>; &lt;br />;&lt;i>; Guanghui Wang&lt;/i>;, &lt;i>;Rafael Hanashiro&lt;/i>;, &lt;i>;Etash Guha&lt;/i>;, &lt;b>;&lt;i>;Jacob Abernethy &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=rJcLocAJpA6&quot;>;On Compositional Uncertainty Quantification for Seq2seq Graph Parsing&lt;/a>; &lt;br />;&lt;i>; Zi Lin&lt;/i>;*, &lt;b>;&lt;i>;Du Phan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Panupong Pasupat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeremiah Liu&lt;/i>;&lt;/b>;, &lt;i>;Jingbo Shang &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=jbIYfq4Tr-&quot;>;On the Robustness of Safe Reinforcement Learning Under Observational Perturbations&lt;/a>; &lt;br />;&lt;i>; Zuxin Liu&lt;/i>;, &lt;i>;Zijian Guo&lt;/i>;, &lt;i>;Zhepeng Cen&lt;/i>;, &lt;i>;Huan Zhang&lt;/i>;, &lt;b>;&lt;i>;Jie Tan&lt;/i>;&lt;/b>;, &lt;i>;Bo Li&lt;/i>;, &lt;i>;Ding Zhao &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=47KG_AvNqeZ&quot;>;Online Low Rank Matrix Completion&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Prateek Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Soumyabrata Pal &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=kJUS5nD0vPB&quot;>;Out-of-Distribution Detection and Selective Generation for Conditional Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jie Ren&lt;/i>;&lt;/b>;, &lt;i>;&lt;b>;Jiaming Luo&lt;/b>;&lt;/i>;, &lt;b>;&lt;i>;Yao Zhao&lt;/i>;&lt;/b>;, &lt;i>;Kundan Krishna&lt;/i>;*, &lt;b>;&lt;i>;Mohammad Saleh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Balaji Lakshminarayanan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peter J. Liu &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=mWVoBz4W0u&quot;>;PaLI: A Jointly-Scaled Multilingual Language-Image Model&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Xi Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiao Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Soravit Changpinyo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;AJ Piergiovanni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Piotr Padlewski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel Salz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sebastian Goodman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adam Grycner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lucas Beyer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alexander Kolesnikov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nan Ding&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Keran Rong&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hassan Akbari&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gaurav Mishra&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Linting Xue&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ashish V. Thapliyal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;James Bradbury&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Weicheng Kuo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mojtaba Seyedhosseini&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chao Jia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Burcu Karagol Ayan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carlos Riquelme Ruiz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andreas Peter Steiner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anelia Angelova&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiaohua Zhai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Radu Soricut &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=vOEXS39nOF&quot;>;Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Ruben Villegas&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Babaeizadeh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pieter-Jan Kindermans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hernan Moraldo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Han Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammad Taghi Saffar&lt;/i>;&lt;/b>;, &lt;i>;Santiago Castro&lt;/i>;*, &lt;i>;Julius Kunze&lt;/i>;*, &lt;b>;&lt;i>;Dumitru Erhan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gmL46YMpu2J&quot;>;Promptagator: Few-Shot Dense Retrieval from 8 Examples&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhuyun Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vincent Y. Zhao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ji Ma&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Luan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jianmo Ni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Lu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anton Bakalov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Kelvin Guu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Keith B. Hall&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ming-Wei Chang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=MofT9KEF0kw&quot;>;Pushing the Accuracy-Group Robustness Frontier with Introspective Self-Play&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Jeremiah Zhe Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Krishnamurthy Dj Dvijotham&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jihyeon Lee&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quan Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Balaji Lakshminarayanan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Deepak Ramachandran &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=XSEBx0iSjFQ&quot;>;Re-Imagen: Retrieval-Augmented Text-to-Image Generator&lt;/a>; &lt;brp>;&lt;b>;&lt;i>; Wenhu Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hexiang Hu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chitwan Saharia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;William W. Cohen &lt;/i>;&lt;/b>;&lt;/brp>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.01296.pdf&quot;>;Recitation-Augmented Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Zhiqing Sun&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;i>;Yiming Yang&lt;/i>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=h9O0wsmL-cT&quot;>;Regression with Label Differential Privacy&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Badih Ghazi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pritish Kamath&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ravi Kumar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ethan Leeman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Pasin Manurangsi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Avinash Varadarajan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chiyuan Zhang &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=SNgLnzFQeiD&quot;>;Revisiting the Entropy Semiring for Neural Speech Recognition&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Oscar Chang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dongseong Hwang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Olivier Siohan &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=ALDM5SN2r7M&quot;>;Robust Active Distillation&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Cenk Baykal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Khoa Trinh&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fotis Iliopoulos&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gaurav Menghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Erik Vee &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=BYWWwSY2G5s&quot;>;Score-Based Continuous-Time Discrete Diffusion Models&lt;/a>; &lt;br />;&lt;i>; Haoran Sun&lt;/i>;*, &lt;b>;&lt;i>;Lijun Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hanjun Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=1PL1NIMMrw&quot;>;Self-Consistency Improves Chain of Thought Reasoning in Language Models&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Quoc Le&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ed H. Chi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sharan Narang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aakanksha Chowdhery&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=Ubc74gTVo3&quot;>;Self-Supervision Through Random Segments with Autoregressive Coding (RandSAC)&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tianyu Hua&lt;/i>;&lt;/b>;, &lt;i>;Yonglong Tian&lt;/i>;, &lt;i>;Sucheng Ren&lt;/i>;, &lt;b>;&lt;i>;Michalis Raptis&lt;/i>;&lt;/b>;, &lt;i>;Hang Zhao&lt;/i>;, &lt;i>;Leonid Sigal &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=T-qVtA3pAxG&quot;>;Serving Graph Compression for Graph Neural Networks&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Si Si&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Felix Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;i>;Cho-Jui Hsieh&lt;/i>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=TTLLGx3eet&quot;>;Sequential Attention for Feature Selection&lt;/a>; &lt;br />;&lt;i>; Taisuke Yasuda&lt;/i>;*, &lt;b>;&lt;i>;MohammadHossein Bateni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lin Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Matthew Fahrbach&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Gang Fu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vahab Mirrokni &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=T5nUQDrM4u&quot;>;Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints&lt;/a>; &lt;br />;&lt;i>; Aran Komatsuzaki&lt;/i>;*, &lt;b>;&lt;i>;Joan Puigcerver&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;James Lee-Thorp&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carlos Riquelme&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Basil Mustafa&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Joshua Ainslie&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=FBMLeaXpZN&quot;>;Spectral Decomposition Representation for Reinforcement Learning&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Tongzheng Ren&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tianjun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lisa Lee&lt;/i>;&lt;/b>;, &lt;i>;Joseph Gonzalez&lt;/i>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Bo Dai &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=9yE2xEj0BH7&quot;>;Spotlight: Mobile UI Understanding Using Vision-Language Models with a Focus&lt;/a>;&amp;nbsp;(see &lt;a href=&quot;https://ai.googleblog.com/2023/02/a-vision-language-approach-for.html&quot;>;blog post&lt;/a>;)&lt;br />;&lt;b>;&lt;i>; Gang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yang Li &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=8jU7wy7N7mA&quot;>;Supervision Complexity and Its Role in Knowledge Distillation&lt;/a>; &lt;br />;&lt;i>; Hrayr Harutyunyan&lt;/i>;*, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Aditya Krishna Menon&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Seungyeon Kim&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=GVSf7Z7DbYL&quot;>;Teacher Guided Training: An Efficient Framework for Knowledge Transfer&lt;/a>; &lt;br />;&lt;i>; Manzil Zaheer&lt;/i>;, &lt;b>;&lt;i>;Ankit Singh Rawat&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Seungyeon Kim&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chong You&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Himanshu Jain&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andreas Veit&lt;/i>;&lt;/b>;, &lt;i>;Rob Fergus&lt;/i>;, &lt;b>;&lt;i>;Sanjiv Kumar &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=gSHyqBijPFO&quot;>;TEMPERA: Test-Time Prompt Editing via Reinforcement Learning&lt;/a>; &lt;br />;&lt;i>; Tianjun Zhang&lt;/i>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dale Schuurmans&lt;/i>;&lt;/b>;, &lt;i>;Joseph E. Gonzalez &lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf?id=6ruVLB727MC&quot;>;UL2: Unifying Language Learning Paradigms&lt;/a>; &lt;br />;&lt;b>;&lt;i>; Yi Tay&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mostafa Dehghani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vinh Q. Tran&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xavier Garcia&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jason Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Hyung Won Chung&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dara Bahri&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Tal Schuster&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Steven Zheng&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Denny Zhou&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Neil Houlsby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Donald Metzler &lt;/i>;&lt;/b>;&lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; * Work done while at Google&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6812586756904863062/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-iclr-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6812586756904863062&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6812586756904863062&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-iclr-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICLR 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaXU0Qnf7lWC70SIqFUj3KcinepmKmqVwu369RYcajoFZI0UGtej6ZGJD7c0GsXJk_Wjg08CnBLtE7lLlz__azqzUNCgflaTp3F9mZyIxX7HBzJEdTN19WMlfVsu4mTww-LLhvZ4hhWqdVCQqjQbrOk6VumBCH6dA2YwdATT6I_QLKLHSUz_Upnw6zlg/s72-c/Google%20Rwanda%20Logo-02.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1064513940498249945&lt;/id>;&lt;published>;2023-04-27T13:24:00.001-07:00&lt;/published>;&lt;updated>;2023-04-27T13:27:36.601-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Biology&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Genomics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;An ML-based approach to better characterize lung diseases&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Babak Behsaz, Software Engineer, and Andrew Carroll, Product Lead, Genomics&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjukGQ3XqcJ6pKHbbMe80-OO_hrGogO7FUPtRlCGq7GH5ba83-6_MnzX5CS7MecI2rz4fbFuY1sHp4hb9mpV_zU95zSxA4mfucMRj_d-LtPNNKUAEEHwuE_Cqj1nvVDJN86RLZU2byBsemz-GiVEDIHVusSQHTerZVr0dPytu4HwGA5_9IYXIeiQ3pprg/s1400/copdgenomics.png&quot; style=&quot;display: none;&quot; />; &lt;p>; The combination of the environment an individual experiences and their genetic predispositions determines the majority of their risk &lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/nejmsa073350&quot;>;for various diseases&lt;/a>;. Large national efforts, such as &lt;a href=&quot;https://www.ukbiobank.ac.uk/&quot;>;the UK Biobank&lt;/a>;, have created large, public resources to better understand the links between environment, genetics, and disease. This has the potential to help individuals better understand how to stay healthy, clinicians to treat illnesses, and scientists to develop new medicines. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One challenge in this process is how we make sense of the vast amount of clinical measurements — the UK Biobank has many petabytes of imaging, metabolic tests, and medical records spanning 500,000 individuals. To best use this data, we need to be able to represent the information present as succinct, informative labels about meaningful diseases and traits, a process called &lt;a href=&quot;https://en.wikipedia.org/wiki/Phenotype&quot;>;phenotyping&lt;/a>;. That is where we can use the ability of ML models to pick up on subtle intricate patterns in large amounts of data. &lt;/p>; &lt;p>; We&#39;ve previously demonstrated the ability to use ML models to &lt;a href=&quot;https://www.cell.com/ajhg/fulltext/S0002-9297(21)00188-9&quot;>;quickly phenotype&lt;/a>; at scale for retinal diseases. Nonetheless, these models were trained using labels from clinician judgment, and access to clinical-grade labels is a limiting factor due to the time and expense needed to create them. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.nature.com/articles/s41588-023-01372-4&quot;>;Inference of chronic obstructive pulmonary disease with deep learning on raw spirograms identifies new genetic loci and improves risk models&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/ng/&quot;>;Nature Genetics&lt;/a>;&lt;/em>;, we&#39;re excited to highlight a method for training accurate ML models for genetic discovery of diseases, even when using noisy and unreliable labels. We demonstrate the ability to train ML models that can phenotype directly from raw clinical measurement and unreliable medical record information. This reduced reliance on medical domain experts for labeling greatly expands the range of applications for our technique to a panoply of diseases and has the potential to improve their prevention, diagnosis, and treatment. We showcase this method with ML models that can better characterize lung function and &lt;a href=&quot;https://en.wikipedia.org/wiki/Chronic_obstructive_pulmonary_disease&quot;>;chronic obstructive pulmonary disease&lt;/a>; (COPD). Additionally, we show the usefulness of these models by demonstrating a better ability to identify genetic variants associated with COPD, improved understanding of the biology behind the disease, and successful prediction of outcomes associated with COPD. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;ML for deeper understanding of exhalation &lt;/h2>; &lt;p>; For this demonstration, we focused on COPD, the &lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death&quot;>;third leading cause of worldwide death in 2019&lt;/a>;, in which airway inflammation and impeded airflow can progressively reduce lung function. Lung function for COPD and other diseases is measured by recording an individual&#39;s exhalation volume over time (the record is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spirometer&quot;>;spirogram&lt;/a>;; see an example below). Although there are guidelines (called &lt;a href=&quot;https://bestpractice.bmj.com/topics/en-us/7/criteria&quot;>;GOLD&lt;/a>;) for determining COPD status from exhalation, these use only a few, specific data points in the curve and apply fixed thresholds to those values. Much of the rich data from these spirograms is discarded in this analysis of lung function. &lt;/p>; &lt;p>; We reasoned that ML models trained to classify spirograms would be able to use the rich data present more completely and result in more accurate and comprehensive measures of lung function and disease, similar to what we have seen in other classification tasks like &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6&quot;>;mammography&lt;/a>; or &lt;a href=&quot;https://ai.googleblog.com/2023/03/learning-from-deep-learning-case-study.html&quot;>;histology&lt;/a>;. We trained ML models to predict whether an individual has COPD using the full spirograms as inputs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw_vNOsI6b6xOptAAEdrfeY1biM7ZgLO6-bGD7ICW2681H_4foD5Gms6AHGDlS1cmWjx2iR8paxAvY8lLAT29uwrijzI-W7uQVmRtHSts0yBA46zJhw8mYRz_H4NZ7rwvMQWOQBGUAvpfsBHHj9kBlVzCc3fU2rUsh7sM_Dn5wNVgPeu2XmR7ERMbJGw/s1999/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;664&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw_vNOsI6b6xOptAAEdrfeY1biM7ZgLO6-bGD7ICW2681H_4foD5Gms6AHGDlS1cmWjx2iR8paxAvY8lLAT29uwrijzI-W7uQVmRtHSts0yBA46zJhw8mYRz_H4NZ7rwvMQWOQBGUAvpfsBHHj9kBlVzCc3fU2rUsh7sM_Dn5wNVgPeu2XmR7ERMbJGw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spirometry and COPD status overview. Spirograms from lung function test showing a forced expiratory volume-time spirogram (&lt;b>;left&lt;/b>;), a forced expiratory flow-time spirogram (&lt;b>;middle&lt;/b>;), and an interpolated forced expiratory flow-volume spirogram (&lt;b>;right&lt;/b>;). The profile of individuals w/o COPD is different.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The common method of training models for this problem, &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;supervised learning&lt;/a>;, requires samples to be associated with labels. Determining those labels can require the effort of very time-constrained experts. For this work, to show that we do not necessarily need medically graded labels, we decided to use a variety of widely available sources of medical record information to create those labels without medical expert review. These labels are &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233028/&quot;>;less reliable and noisy&lt;/a>; for two reasons. First, there are gaps in the medical records of individuals because they use multiple health services. Second, COPD is often undiagnosed, meaning many with the disease will not be labeled as having it even if we compile the complete medical records. Nonetheless, we trained a model to predict these noisy labels from the spirogram curves and treat the model predictions as a quantitative COPD liability or risk score. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlWGiBTaBqut9XGz_wip5AXFwRVX2gLetKEdaPY0L75-Kz8eow-NIZdwJmjqbZ7bZMlpTzHgWqMya9HVQHlS2HnPaVWnl8vh0NiIlYM5IyEkM6tKLfSsKpSVCBDEXXue6UoHgMvz8ZMAE0_wQcK78tFbTDWmY3RWKhKi7phODzQU1aZ6wJUjIidLOlvQ/s1787/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;499&quot; data-original-width=&quot;1787&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlWGiBTaBqut9XGz_wip5AXFwRVX2gLetKEdaPY0L75-Kz8eow-NIZdwJmjqbZ7bZMlpTzHgWqMya9HVQHlS2HnPaVWnl8vh0NiIlYM5IyEkM6tKLfSsKpSVCBDEXXue6UoHgMvz8ZMAE0_wQcK78tFbTDWmY3RWKhKi7phODzQU1aZ6wJUjIidLOlvQ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Noisy COPD status labels were derived using various medical record sources (clinical data). A COPD liability model is then trained to predict COPD status from raw flow-volume spirograms.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Predicting COPD outcomes&lt;/h2>; &lt;p>; We then investigated whether the risk scores produced by our model could better predict a variety of binary COPD outcomes (for example, an individual&#39;s COPD status, whether they were hospitalized for COPD or died from it). For comparison, we benchmarked the model relative to expert-defined measurements required to diagnose COPD, specifically &lt;a href=&quot;https://goldcopd.org/gold-spirometry-guide/&quot;>;FEV1/FVC&lt;/a>;, which compares specific points on the spirogram curve with a simple mathematical ratio. We observed an improvement in the ability to predict these outcomes as seen in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision-recall&lt;/a>; curves below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyIWs-H_se4QVdvrrr-8iJ3x2FmWcaGId2LeRr30Sw9trq_03KB94o05zDYAg8l0OW7idCf7G8sbs5PldkZI6WnaDJH3X6z7yv9IEjxLCM9T1Od8HKOtDraAAsOWfZyXCrs860q7iqrVbk4-N6bKniXSjQd64RKIJ_-5_eZKsmivu0qCYhBq1Z0b6xOA/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;728&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgyIWs-H_se4QVdvrrr-8iJ3x2FmWcaGId2LeRr30Sw9trq_03KB94o05zDYAg8l0OW7idCf7G8sbs5PldkZI6WnaDJH3X6z7yv9IEjxLCM9T1Od8HKOtDraAAsOWfZyXCrs860q7iqrVbk4-N6bKniXSjQd64RKIJ_-5_eZKsmivu0qCYhBq1Z0b6xOA/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Precision-recall curves for COPD status and outcomes for our ML model (green) compared to traditional measures. Confidence intervals are shown by lighter shading.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also observed that separating populations by their COPD model score was predictive of all-cause mortality. This plot suggests that individuals with higher COPD risk are more likely to die earlier from any causes and the risk probably has implications beyond just COPD. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4e3BTXxn3jtqNdnW3xh9mNLiMXRBqCWMfgKdM_TsWOPTbeY4AXm71cVOhvFDY7YR0wvIn2AzvtXnsyn94LFYqY35vVdPIZkF6aVa5zvvRxidRTTHAfe0S3KryH27fWS1VUyKRETqZfRsWwsBHP7c2khartsF3zWyBVQ7AtTK7_6X4TzgrM4M001ttQg/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;809&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4e3BTXxn3jtqNdnW3xh9mNLiMXRBqCWMfgKdM_TsWOPTbeY4AXm71cVOhvFDY7YR0wvIn2AzvtXnsyn94LFYqY35vVdPIZkF6aVa5zvvRxidRTTHAfe0S3KryH27fWS1VUyKRETqZfRsWwsBHP7c2khartsF3zWyBVQ7AtTK7_6X4TzgrM4M001ttQg/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Survival analysis of a cohort of UK Biobank individuals stratified by their COPD model&#39;s predicted risk &lt;a href=&quot;https://en.wikipedia.org/wiki/Quartile&quot;>;quartile&lt;/a>;. The decrease of the curve indicates individuals in the cohort dying over time. For example, p100 represents the 25% of the cohort with greatest predicted risk, while p50 represents the 2nd quartile.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Identifying the genetic links with COPD&lt;/h2>; &lt;p>; Since the goal of large scale biobanks is to bring together large amounts of both phenotype and genetic data, we also performed a test called a &lt;a href=&quot;https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies&quot;>;genome-wide association study&lt;/a>; (GWAS) to identify the genetic links with COPD and genetic predisposition. A GWAS measures the strength of the statistical association between a given genetic variant — a change in a specific position of DNA — and the observations (eg, COPD) across a cohort of cases and controls. Genetic associations discovered in this manner can inform drug development that modifies the activity or products of a gene, as well as expand our understanding of the biology for a disease. &lt;/p>; &lt;p>; We showed with our ML-phenotyping method that not only do we rediscover almost all known COPD variants found by manual phenotyping, but we also find many novel genetic variants significantly associated with COPD. In addition, we see good agreement on the effect sizes for the variants discovered by both our ML approach and the manual one (&lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;>;R&lt;sup>;2&lt;/sup>;&lt;/a>;=0.93), which provides strong evidence for validity of the newly found variants. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvSvAcIMwQ-L_1esTkw6ChFzo_sAbd1ESskmHs0i_9si-Gh91P1IVVnl8kRGUyTtLRSjw6jPbE4bPjUzUpyXawimkXhayASxac-ywLjcPIXOVTKf1fNnO9XZZsCX_a_1BmFFYrtH9eOcAHodEaiS51my3kpol0TvTHK7_Ov7ACF5jOyNbionY1nUVxmw/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;883&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjvSvAcIMwQ-L_1esTkw6ChFzo_sAbd1ESskmHs0i_9si-Gh91P1IVVnl8kRGUyTtLRSjw6jPbE4bPjUzUpyXawimkXhayASxac-ywLjcPIXOVTKf1fNnO9XZZsCX_a_1BmFFYrtH9eOcAHodEaiS51my3kpol0TvTHK7_Ov7ACF5jOyNbionY1nUVxmw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left&lt;/b>;: A plot comparing the statistical power of genetic discovery using the &lt;em>;labels for our ML model&lt;/em>; (y-axis) with the statistical power of the &lt;em>;manual labels from a traditional study&lt;/em>; (&lt;em>;x&lt;/em>;-axis). A value above the &lt;em>;y &lt;/em>;= &lt;em>;x&lt;/em>; line indicates greater statistical power in our method. Green points indicate significant findings in our method that are not found using the traditional approach. Orange points are significant in the traditional approach but not ours. Blue points are significant in both. &lt;b>;Right&lt;/b>;: Estimates of the association effect between our method (&lt;em>;y&lt;/em>;-axis) and traditional method (&lt;em>;x&lt;/em>;-axis). Note that the relative values between studies are comparable but the absolute numbers are not.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Finally, our collaborators at Harvard Medical School and Brigham and Women&#39;s Hospital further examined the plausibility of these findings by providing insights into the possible biological role of the novel variants in development and progression of COPD (you can see more discussion on these insights in the &lt;a href=&quot;https://www.nature.com/articles/s41588-023-01372-4&quot;>;paper&lt;/a>;). &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We demonstrated that our earlier methods for phenotyping with ML can be expanded to a wide range of diseases and can provide novel and valuable insights. We made two key observations by using this to predict COPD from spirograms and discovering new genetic insights. First, domain knowledge was not necessary to make predictions from raw medical data. Interestingly, we showed the raw medical data is probably underutilized and the ML model can find patterns in it that are not captured by expert-defined measurements. Second, we do not need medically graded labels; instead, noisy labels defined from widely available medical records can be used to generate clinically predictive and genetically informative risk scores. We hope that this work will broadly expand the ability of the field to use noisy labels and will improve our collective understanding of lung function and disease. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;br />;&lt;/h2>; &lt;p>; &lt;em>;This work is the combined output of multiple contributors and institutions. We thank all contributors: Justin Cosentino, Babak Alipanahi, Zachary R. McCaw, Cory Y. McLean, Farhad Hormozdiari (Google), Davin Hill (Northeastern University), Tae-Hwi Schwantes-An and Dongbing Lai (Indiana University), Brian D. Hobbs and Michael H. Cho (Brigham and Women&#39;s Hospital, and Harvard Medical School). We also thank Ted Yun and Nick Furlotte for reviewing the manuscript, Greg Corrado and Shravya Shetty for support, and Howard Yang, Kavita Kulkarni, and Tammi Huynh for helping with publication logistics.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1064513940498249945/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1064513940498249945&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1064513940498249945&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html&quot; rel=&quot;alternate&quot; title=&quot;An ML-based approach to better characterize lung diseases&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjukGQ3XqcJ6pKHbbMe80-OO_hrGogO7FUPtRlCGq7GH5ba83-6_MnzX5CS7MecI2rz4fbFuY1sHp4hb9mpV_zU95zSxA4mfucMRj_d-LtPNNKUAEEHwuE_Cqj1nvVDJN86RLZU2byBsemz-GiVEDIHVusSQHTerZVr0dPytu4HwGA5_9IYXIeiQ3pprg/s72-c/copdgenomics.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8846807972710427001&lt;/id>;&lt;published>;2023-04-26T13:32:00.000-07:00&lt;/published>;&lt;updated>;2023-04-26T13:32:51.664-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Self-Supervised Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Robust and efficient medical imaging with self-supervision&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shekoofeh Azizi, Senior Research Scientist, and Laura Culp, Senior Research Engineer, Google Research&lt;/span>;&lt;div>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvMtAszRzkUmNz3DliRRKOPMA0hi-EplY9vfJ9cWCZhFBUgnblKjVoObTYAfybreL7HEKh11g2unV5oR_KOWfkPxpe3Hzo6qWSWq4YxA9RuhkeGXdDJST2XvW2Nfl3L6kDpV1hA2b_vpBMVaw2_pxt2vWD2n92S9fDGAZp0a4c2dV2Ylf3XrUSLQxAoQ/w640-h640/REMEDIS%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;Despite recent progress in the field of &lt;a href=&quot;https://www.nature.com/articles/s41746-020-00376-2&quot;>;medical artificial intelligence&lt;/a>; (AI), most existing models are &lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6.epdf?author_access_token=V_LKV2xpSv9G1dhANYeWM9RgN0jAjWel9jnR3ZoTv0M5zwPVx5jT4z_z-YkUZTBT6_1AtRXi8QouJM7xB-oSN-cVBoH7f_QTgx-yQN3UBEVfkvO1_5urNT-CZHGCEQNGlCuO69tMQYak4SmdoDqyzg%3D%3D&quot;>;narrow&lt;/a>;, &lt;a href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2588763&quot;>;single-task&lt;/a>; systems that require large quantities of labeled data to train. Moreover, these models cannot be easily reused in new clinical contexts as they often require the collection, &lt;a href=&quot;https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html&quot;>;de-identification&lt;/a>; and annotation of site-specific data for every new deployment environment, which is both &lt;a href=&quot;https://pubs.rsna.org/doi/full/10.1148/radiol.2020192224&quot;>;laborious and expensive&lt;/a>;. This problem of &lt;em>;data-efficient generalization &lt;/em>;(a model&#39;s ability to generalize to new settings using minimal new data) continues to be a key translational challenge for medical machine learning (ML) models and has in turn, prevented their broad uptake in real world healthcare settings. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The emergence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;>;foundation models&lt;/a>; offers a significant opportunity to rethink development of medical AI to make it more performant, safer, and equitable. These models are trained using data at scale, often by self-supervised learning. This process results in generalist models that can rapidly be adapted to new tasks and environments with less need for supervised data. With foundation models, it may be possible to safely and efficiently deploy models across various clinical contexts and environments. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2205.09723&quot;>;Robust and Efficient MEDical Imaging with Self-supervision&lt;/a>;” (REMEDIS), to be published in &lt;em>;&lt;a href=&quot;https://www.nature.com/natbiomedeng/&quot;>;Nature Biomedical Engineering&lt;/a>;&lt;/em>;, we introduce a unified large-scale self-supervised learning framework for building foundation medical imaging models. This strategy combines large scale &lt;a href=&quot;https://arxiv.org/abs/2101.05913&quot;>;supervised transfer learning&lt;/a>; with &lt;a href=&quot;https://ai.googleblog.com/2021/10/self-supervised-learning-advances.html&quot;>;self-supervised learning&lt;/a>; and requires minimal task-specific customization. REMEDIS shows significant improvement in data-efficient generalization across medical imaging tasks and modalities with a 3–100x reduction in site-specific data for adapting models to new clinical contexts and environments. Building on this, we are excited to announce &lt;a href=&quot;https://github.com/google-research/medical-ai-research-foundations&quot;>;Medical AI Research Foundations&lt;/a>; (hosted by &lt;a href=&quot;https://doi.org/10.13026/psq3-vj24&quot;>;PhysioNet&lt;/a>;), an expansion of the public release of &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;chest X-ray Foundations&lt;/a>; in 2022. Medical AI Research Foundations is a collection of open-source non-diagnostic models (starting with REMEDIS models), APIs, and resources to help researchers and developers accelerate medical AI research. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large scale self-supervision for medical imaging&lt;/h2>; &lt;p>; REMEDIS uses a combination of natural (non-medical) images and unlabeled medical images to develop strong medical imaging foundation models. Its pre-training strategy consists of two steps. The first involves supervised representation learning on a large-scale dataset of labeled natural images (pulled from &lt;a href=&quot;https://arxiv.org/abs/2104.10972&quot;>;Imagenet 21k&lt;/a>; or &lt;a href=&quot;http://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html&quot;>;JFT&lt;/a>;) using the &lt;a href=&quot;https://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html&quot;>;Big Transfer&lt;/a>; (BiT) method. &lt;/p>; &lt;p>; The second step involves intermediate self-supervised learning, which does not require any labels and instead, trains a model to learn medical data representations independently of labels. The specific approach used for pre-training and learning representations is &lt;a href=&quot;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&quot;>;SimCLR&lt;/a>;. The method works by maximizing agreement between differently augmented views of the same training example via a contrastive loss in a hidden layer of a feed-forward neural network with &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) outputs. However, REMEDIS is equally compatible with other contrastive self-supervised learning methods. This training method is applicable for healthcare environments as many hospitals acquire raw data (images) as a routine practice. While processes would have to be implemented to make this data usable within models (ie, patient consent prior to gathering the data, de-identification, etc.), the costly, time-consuming, and difficult task of labeling that data could be avoided using REMEDIS. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4_m0T9kVxqldLFRTdlqSqYTieFr_4z2BIwAmjDqk3tDO9keGFVIMWMFgJmEGdHT4boqPCUYsi66ekSGKQXeY_3X-s95_bjsgugMquMjfMzRg5TcsO35SBzCv9AleHuEYYCS_gYNHsvRZw07oI_ZPtRynbqxYNF_F8iTLSesAZMHWbhB54Nlhoz9nPvA/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;368&quot; data-original-width=&quot;1600&quot; height=&quot;147&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4_m0T9kVxqldLFRTdlqSqYTieFr_4z2BIwAmjDqk3tDO9keGFVIMWMFgJmEGdHT4boqPCUYsi66ekSGKQXeY_3X-s95_bjsgugMquMjfMzRg5TcsO35SBzCv9AleHuEYYCS_gYNHsvRZw07oI_ZPtRynbqxYNF_F8iTLSesAZMHWbhB54Nlhoz9nPvA/w640-h147/image1.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS leverages large-scale supervised learning using natural images and self-supervised learning using unlabeled medical data to create strong foundation models for medical imaging.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Given ML model parameter constraints, it is important that our proposed approach works when using both small and large model architecture sizes. To study this in detail, we considered two ResNet architectures with commonly used depth and width multipliers, &lt;a href=&quot;https://colab.research.google.com/github/yashclone999/ResNet_MODEL/blob/master/ResNet50.ipynb&quot;>;ResNet-50&lt;/a>; (1×) and &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152&quot;>;ResNet-152&lt;/a>; (2×) as the backbone encoder networks. &lt;/p>; &lt;p>; After pre-training, the model was fine-tuned using labeled task-specific medical data and evaluated for in-distribution task performance. In addition, to evaluate the data-efficient generalization, the model was also optionally fine-tuned using small amounts of out-of-distribution (OOD) data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ895lMLLchi5kiP38Gi16aBaaLNEgOxOkKft4ZnMVthFW_NRS_6ZcU8mIgMO2QuvJ3jU0TTH_yE-UjPhO64MX2sMLCvo9qt-reXkYY5zQtYkgunAEittRQru0Zvzi8BT_MhzBnSF9PHqEmD0EEmMErJiw-dXbfX2uWWCEyDVq0pGjSuK3EhVqh9y-gQ/s2209/REMEDIS%201200x2000.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2209&quot; data-original-width=&quot;1215&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ895lMLLchi5kiP38Gi16aBaaLNEgOxOkKft4ZnMVthFW_NRS_6ZcU8mIgMO2QuvJ3jU0TTH_yE-UjPhO64MX2sMLCvo9qt-reXkYY5zQtYkgunAEittRQru0Zvzi8BT_MhzBnSF9PHqEmD0EEmMErJiw-dXbfX2uWWCEyDVq0pGjSuK3EhVqh9y-gQ/w352-h640/REMEDIS%201200x2000.png&quot; width=&quot;352&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS starts with representations initialized using large-scale natural image pretraining following the Big Transfer (BiT) method. We then adapt the model to the medical domain using intermediate contrastive self-supervised learning without using any labeled medical data. Finally, we fine-tune the model to specific downstream medical imaging tasks. We evaluate the ML model both in an in-distribution (ID) setting and in an out-of-distribution (OOD) setting to establish the data-efficient generalization performance of the model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation and results&lt;/h2>; &lt;p>; To evaluate the REMEDIS model&#39;s performance, we simulate realistic scenarios using retrospective de-identified data across a broad range of medical imaging tasks and modalities, including &lt;a href=&quot;https://ai.googleblog.com/2019/09/using-deep-learning-to-inform.html&quot;>;dermatology&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2018/12/improving-effectiveness-of-diabetic.html&quot;>;retinal imaging&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html&quot;>;chest X-ray interpretation&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2023/03/learning-from-deep-learning-case-study.html&quot;>;pathology&lt;/a>; and &lt;a href=&quot;https://blog.google/technology/health/improving-breast-cancer-screening&quot;>;mammography&lt;/a>;. We further introduce the notion of data-efficient generalization, capturing the model&#39;s ability to generalize to new deployment distributions with a significantly reduced need for expert annotated data from the new clinical setting. In-distribution performance is measured as (1) improvement in zero-shot generalization to OOD settings (assessing performance in an OOD evaluation set, with zero access to training data from the OOD dataset) and (2) significant reduction in the need for annotated data from the OOD settings to reach performance equivalent to clinical experts (or threshold demonstrating clinical utility). REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strongly supervised baseline. &lt;/p>; &lt;p>; More importantly, our strategy leads to data-efficient generalization of medical imaging models, matching strong supervised baselines resulting in a 3–100x reduction in the need for retraining data. While SimCLR is the primary self-supervised learning approach used in the study, we also show that REMEDIS is compatible with other approaches, such as &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo-V2&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2010.07922&quot;>;RELIC&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.03230&quot;>;Barlow Twins&lt;/a>;. Furthermore, the approach works across model architecture sizes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIyKlK_o3J1b62jj1RPy7HS2MvM4qrcXCHWI0iqckQt2yYgLRzqM4K2s3Xz3qqH4u6Mle1E5Ixvw9RDVuBLIQDNT9a3XJYKZAdrNoTjhn_FkVakBVVxLmDUlLBJ7Ap3IHzzOVtUTfQt3hyUCF9Y0ntbFKFHSfy5n8op0d2kYYszsv7DhnX3HxHQxWwg/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIyKlK_o3J1b62jj1RPy7HS2MvM4qrcXCHWI0iqckQt2yYgLRzqM4K2s3Xz3qqH4u6Mle1E5Ixvw9RDVuBLIQDNT9a3XJYKZAdrNoTjhn_FkVakBVVxLmDUlLBJ7Ap3IHzzOVtUTfQt3hyUCF9Y0ntbFKFHSfy5n8op0d2kYYszsv7DhnX3HxHQxWwg/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS outperformed the supervised baseline pre-trained on JFT-300M for various medical tasks and demonstrated improved data-efficient generalization, reducing data needs by 3–100x for adapting models to new clinical settings. This could potentially translate to significant reduction in clinician hours saved annotating data and cost of developing robust medical imaging systems.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1Q1VS1l1Xd4SqEqJySE7xjxr1L9X_g0lWXY3c8IHBVTsOWQ1X2gwVMbFeAKXHGo6kH3J1PUDsG8OPWKV1zC0dtGNwa6jETWEl9G2affZxyEtzmwOkwxoiO6MntRwrMxRo425fKMT4Q775O-YzAQmW-7k3McZmFq1qmW1i0UwA1vygRADWOJP62psERA/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1683&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1Q1VS1l1Xd4SqEqJySE7xjxr1L9X_g0lWXY3c8IHBVTsOWQ1X2gwVMbFeAKXHGo6kH3J1PUDsG8OPWKV1zC0dtGNwa6jETWEl9G2affZxyEtzmwOkwxoiO6MntRwrMxRo425fKMT4Q775O-YzAQmW-7k3McZmFq1qmW1i0UwA1vygRADWOJP62psERA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;REMEDIS is compatible with &lt;a href=&quot;https://arxiv.org/abs/2003.04297&quot;>;MoCo-V2&lt;/a>;,&lt;a href=&quot;https://arxiv.org/pdf/2010.07922.pdf&quot;>; RELIC&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.03230&quot;>;Barlow Twins&lt;/a>; as alternate self-supervised learning strategies. All the REMEDIS variants lead to data-efficient generalization improvements over the strong supervised baseline for dermatology condition classification (&lt;strong>;T1&lt;/strong>;), diabetic macular edema classification (&lt;strong>;T2&lt;/strong>;), and chest X-ray condition classification (&lt;strong>;T3&lt;/strong>;). The gray shaded area indicates the performance of the strong supervised baseline pre-trained on JFT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Medical AI Research Foundations&lt;/h2>; &lt;p>; Building on REMEDIS, we are excited to announce &lt;a href=&quot;https://doi.org/10.13026/psq3-vj24&quot;>;Medical AI Research Foundations&lt;/a>;, an expansion of the public release of &lt;a href=&quot;https://ai.googleblog.com/2022/07/simplified-transfer-learning-for-chest.html&quot;>;chest X-ray Foundations&lt;/a>; in 2022. Medical AI Research Foundations is a repository of open-source medical foundation models hosted by PhysioNet. This expands the previous API-based approach to also encompass non-diagnostic models, to help researchers and developers accelerate their medical AI research. We believe that REMEDIS and the release of the Medical AI Research Foundations are a step toward building medical models that can generalize across healthcare settings and tasks. &lt;/p>; &lt;p>; We are seeding Medical AI Research Foundations with REMEDIS models for chest X-ray and pathology (with related&lt;a href=&quot;https://github.com/google-research/medical-ai-research-foundationshttps://github.com/google-research/medical-ai-research-foundations&quot;>; code&lt;/a>;). Whereas the existing chest X-ray Foundation approach focuses on providing frozen embeddings for application-specific fine tuning from a model trained on several large private datasets, the REMEDIS models (trained on public datasets) enable users to fine-tune end-to-end for their application, and to run on local devices. We recommend users test different approaches based on their unique needs for their desired application. We expect to add more models and resources for training medical foundation models such as datasets and benchmarks in the future. We also welcome the medical AI research community to contribute to this. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; These results suggest that REMEDIS has the potential to significantly accelerate the development of ML systems for medical imaging, which can preserve their strong performance when deployed in a variety of changing contexts. We believe this is an important step forward for medical imaging AI to deliver a broad impact. Beyond the experimental results presented, the approach and insights described here have been integrated into several of &lt;a href=&quot;https://health.google/health-research/&quot;>;Google&#39;s medical imaging research projects&lt;/a>;, such as dermatology, mammography and radiology among others. We&#39;re using a similar self-supervised learning approach with our non-imaging foundation model efforts, such as &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM&lt;/a>; and &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;Med-PaLM 2&lt;/a>;. &lt;/p>; &lt;p>; With REMEDIS, we demonstrated the potential of foundation models for medical imaging applications. Such models hold exciting possibilities in medical applications with the opportunity of multimodal representation learning. The practice of medicine is inherently multimodal and incorporates information from images, electronic health records, sensors, wearables, genomics and more. We believe ML systems that leverage these data at scale using self-supervised learning with careful consideration of privacy, safety, fairness and ethics will help lay the groundwork for the next generation of learning health systems that scale world-class healthcare to everyone. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;This work involved extensive collaborative efforts from a multidisciplinary team of researchers, software engineers, clinicians, and cross-functional contributors across Google Health AI and Google Brain. In particular, we would like to thank our first co-author Jan Freyberg and our lead senior authors of these projects, Vivek Natarajan, Alan Karthikesalingam, Mohammad Norouzi and Neil Houlsby for their invaluable contributions and support. We also thank Lauren Winer, Sami Lachgar, Yun Liu and Karan Singhal for their feedback on this post and Tom Small for support in creating the visuals. Finally, we also thank the PhysioNet team for their support on hosting Medical AI Research Foundations. Users with questions can reach out to medical-ai-research-foundations at google.com.&lt;/i>; &lt;/p>; &lt;/div>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8846807972710427001/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8846807972710427001&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8846807972710427001&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html&quot; rel=&quot;alternate&quot; title=&quot;Robust and efficient medical imaging with self-supervision&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvMtAszRzkUmNz3DliRRKOPMA0hi-EplY9vfJ9cWCZhFBUgnblKjVoObTYAfybreL7HEKh11g2unV5oR_KOWfkPxpe3Hzo6qWSWq4YxA9RuhkeGXdDJST2XvW2Nfl3L6kDpV1hA2b_vpBMVaw2_pxt2vWD2n92S9fDGAZp0a4c2dV2Ylf3XrUSLQxAoQ/s72-w640-h640-c/REMEDIS%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1427927967438262189&lt;/id>;&lt;published>;2023-04-25T10:20:00.002-07:00&lt;/published>;&lt;updated>;2023-04-25T15:21:23.219-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AutoML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Compression&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Neural Networks&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;LayerNAS: Neural architecture search in polynomial complexity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yicheng Fan and Dana Alon, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s1200/image4.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Every byte and every operation matters when trying to build a faster model, especially if the model is to run on-device. &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;>;Neural architecture search&lt;/a>; (NAS) algorithms design sophisticated model architectures by searching through a larger model-space than what is possible manually. Different NAS algorithms, such as &lt;a href=&quot;https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html&quot;>;MNasNet&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2008.06120&quot;>;TuNAS&lt;/a>;, have been proposed and have discovered several efficient model architectures, including &lt;a href=&quot;https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html&quot;>;MobileNetV3&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html&quot;>;EfficientNet&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Here we present &lt;a href=&quot;https://arxiv.org/abs/2304.11517&quot;>;LayerNAS&lt;/a>;, an approach that reformulates the multi-objective NAS problem within the framework of &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization&lt;/a>; to greatly reduce the complexity, which results in an order of magnitude reduction in the number of model candidates that must be searched, less computation required for multi-trial searches, and the discovery of model architectures that perform better overall. Using a search space built on backbones taken from &lt;a href=&quot;https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html&quot;>;MobileNetV2&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html&quot;>;MobileNetV3&lt;/a>;, we find models with top-1 accuracy on &lt;a href=&quot;https://image-net.org/&quot;>;ImageNet&lt;/a>; up to 4.9% better than current state-of-the-art alternatives. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Problem formulation&lt;/h2>; &lt;p>; NAS tackles a variety of different problems on different search spaces. To understand what LayerNAS is solving, let&#39;s start with a simple example: You are the owner of GBurger and are designing the flagship burger, which is made up with three layers, each of which has four options with different costs. Burgers taste differently with different mixtures of options. You want to make the most delicious burger you can that comes in under a certain budget. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGEuvaD4YhxpcPiekUL6LnVFXI7h6V_iq5geFR6O5N3zv5SjRu36Vn4pxvI2WJM5cZgolJM1MOasbbe6HtsTf6TIfjtmDZrNGCP5Q946iaLygvyj8RIxbLVq3l9EiwmzfitnubP34jsuybOjiUxZ0ptJ6H_rKfIZa0rOYqE0pOew2yWYymObM7IPy0gg/s1194/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;780&quot; data-original-width=&quot;1194&quot; height=&quot;261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjGEuvaD4YhxpcPiekUL6LnVFXI7h6V_iq5geFR6O5N3zv5SjRu36Vn4pxvI2WJM5cZgolJM1MOasbbe6HtsTf6TIfjtmDZrNGCP5Q946iaLygvyj8RIxbLVq3l9EiwmzfitnubP34jsuybOjiUxZ0ptJ6H_rKfIZa0rOYqE0pOew2yWYymObM7IPy0gg/w400-h261/image3.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Make up your burger with different options available for each layer, each of which has different costs and provides different benefits.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Just like the architecture for a neural network, the search space for the perfect burger follows a layerwise pattern, where each layer has several options with different changes to costs and performance. This simplified model illustrates a common approach for setting up search spaces. For example, for models based on convolutional neural networks (CNNs), like &lt;a href=&quot;https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html&quot;>;MobileNet&lt;/a>;, the NAS algorithm can select between a different number of options — filters, strides, or kernel sizes, etc. — for the convolution layer. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Method&lt;/h2>; &lt;p>; We base our approach on search spaces that satisfy two conditions: &lt;/p>; &lt;ul>; &lt;li>;An optimal model can be constructed using one of the model candidates generated from searching the previous layer and applying those search options to the current layer. &lt;/li>;&lt;li>;If we set a &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOP&lt;/a>; constraint on the current layer, we can set constraints on the previous layer by reducing the FLOPs of the current layer. &lt;/li>; &lt;/ul>; &lt;p>; Under these conditions it is possible to search linearly, from layer 1 to layer &lt;em>;n&lt;/em>; knowing that when searching for the best option for layer &lt;em>;i&lt;/em>;, a change in any previous layer will not improve the performance of the model. We can then bucket candidates by their cost, so that only a limited number of candidates are stored per layer. If two models have the same FLOPs, but one has better accuracy, we only keep the better one, and assume this won&#39;t affect the architecture of following layers. Whereas the search space of a full treatment would expand exponentially with layers since the full range of options are available at each layer, our layerwise cost-based approach allows us to significantly reduce the search space, while being able to rigorously reason over the polynomial complexity of the algorithm. Our experimental evaluation shows that within these constraints we are able to discover top-performance models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NAS as a combinatorial optimization problem&lt;/h2>; &lt;p>; By applying a layerwise-cost approach, we reduce NAS to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Combinatorial_optimization&quot;>;combinatorial optimization problem&lt;/a>;. Ie, for layer &lt;em>;i&lt;/em>;, we can compute the cost and reward after training with a given component &lt;em>;S&lt;sub>;i&lt;/sub>;&lt;/em>; . This implies the following combinatorial problem: &lt;em>;How can we get the best reward if we select one choice per layer within a cost budget?&lt;/em>; This problem can be solved with many different methods, one of the most straightforward of which is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming&quot;>;dynamic programming&lt;/a>;, as described in the following pseudo code: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;font-size: small; text-align: left;&quot;>; &lt;pre class=&quot;”prettyprint”&quot; margin-right:40px=&quot;&quot; pre-wrap=&quot;&quot; style=&quot;margin-left: 40px;&quot; white-space:=&quot;&quot;>;&lt;span style=&quot;color: blue;&quot;>;while True&lt;/span>;: &lt;span style=&quot;color: mediumorchid;&quot;>;# select a candidate to search in Layer i&lt;/span>; candidate = select_candidate(layeri) &lt;span style=&quot;color: blue;&quot;>;if&lt;/span>; searchable(candidate): &lt;span style=&quot;color: mediumorchid;&quot;>;# Use the layerwise structural information to generate the children.&lt;/span>; children = generate_children(candidate) reward = train(children) bucket = bucketize(children) &lt;span style=&quot;color: blue;&quot;>;if&lt;/span>; memorial_table[i][bucket] &amp;lt; reward: memorial_table[i][bucket] = children move to &lt;span style=&quot;color: blue;&quot;>;next&lt;/span>; layer &lt;/pre>; &lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Pseudocode of LayerNAS.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s1200/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;940&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the LayerNAS approach for the example of trying to create the best burger within a budget of $7–$9. We have four options for the first layer, which results in four burger candidates. By applying four options on the second layer, we have 16 candidates in total. We then bucket them into ranges from $1–$2, $3–$4, $5–$6, and $7–$8, and only keep the most delicious burger within each of the buckets, ie, four candidates. Then, for those four candidates, we build 16 candidates using the pre-selected options for the first two layers and four options for each candidate for the third layer. We bucket them again, select the burgers within the budget range, and keep the best one.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; When comparing NAS algorithms, we evaluate the following metrics: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Quality&lt;/em>;: What is the most accurate model that the algorithm can find? &lt;/li>;&lt;li>;&lt;em>;Stability&lt;/em>;: How stable is the selection of a good model? Can high-accuracy models be consistently discovered in consecutive trials of the algorithm? &lt;/li>;&lt;li>;&lt;em>;Efficiency&lt;/em>;: How long does it take for the algorithm to find a high-accuracy model? &lt;/li>; &lt;/ul>; &lt;p>; We evaluate our algorithm on the standard benchmark &lt;a href=&quot;https://arxiv.org/abs/2009.00437&quot;>;NATS-Bench&lt;/a>; using 100 NAS runs, and we compare against other NAS algorithms, previously described in the NATS-Bench paper: random search, &lt;a href=&quot;https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html&quot;>;regularized evolution&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;>;proximal policy optimization&lt;/a>;. Below, we visualize the differences between these search algorithms for the metrics described above. For each comparison, we record the average accuracy and variation in accuracy (variation is noted by a shaded region corresponding to the 25% to 75% &lt;a href=&quot;https://en.wikipedia.org/wiki/Interquartile_range&quot;>;interquartile range&lt;/a>;). &lt;/p>; &lt;p>; NATS-Bench size search defines a 5-layer CNN model, where each layer can choose from eight different options, each with different channels on the convolution layers. Our goal is to find the best model with 50% of the FLOPs required by the largest model. LayerNAS performance stands apart because it formulates the problem in a different way, separating the cost and reward to avoid searching a significant number of irrelevant model architectures. We found that model candidates with fewer channels in earlier layers tend to yield better performance, which explains how LayerNAS discovers better models much faster than other algorithms, as it avoids spending time on models outside the desired cost range. Note that the accuracy curve drops slightly after searching longer due to the lack of correlation between validation accuracy and test accuracy, ie, some model architectures with higher validation accuracy have a lower test accuracy in NATS-Bench size search. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;262&quot; data-original-width=&quot;393&quot; height=&quot;266&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbQb-b4kQHp_93-t0brOV-7GNKfTv4ZDQ5XAxJOrmWwsRHkgsl3u2H14Dp0nRwiNoHoiR5Fd5CBozSj85iQ27OniH5BtW_AIhA4VDRcSVP8BGp6rJfPaNJfOKgQ6T-lvWMY5eHCVUqYyspU0i7imUUIGaZA6pO2RbnLxQ5IDeDOjtQSDANcSoWa9DFBw/w400-h266/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot; width=&quot;400&quot; />;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFDH_4dHxfYjdc-8AfWxfYSdzXHxesQzUF3qm4G17PONWVmxGKH9qcxL7dc0Ql7-sHvAhllUXXic3Xea3wOMmmKR0XbV364VpHZE7LTQAzDQ18jeD0JWj970Sj4hOecUqezf128NGm6IQkbQXfSHpYnDBTa5qBOpIUunr7z9DW1_wZa8FZqY59E9PrxA/s393/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;266&quot; data-original-width=&quot;393&quot; height=&quot;271&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFDH_4dHxfYjdc-8AfWxfYSdzXHxesQzUF3qm4G17PONWVmxGKH9qcxL7dc0Ql7-sHvAhllUXXic3Xea3wOMmmKR0XbV364VpHZE7LTQAzDQ18jeD0JWj970Sj4hOecUqezf128NGm6IQkbQXfSHpYnDBTa5qBOpIUunr7z9DW1_wZa8FZqY59E9PrxA/w400-h271/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9xD2WsoSrTSEyxCRd15iFc5I16zYF55rotV0URUcULNzl1GUkOh4b89vV_r147CP8xK3b_0GH5BfgGW24cGDVaiXTXjCrH4qsnsZ8jXNl3LEZR9cNrMocEHvuovNTeQI4OJaUQXocxxRhbeJspl9X7Bgp5AFCXtSqf5bz1ejCgWGGnhpPKHr_ysMQkQ/s393/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;266&quot; data-original-width=&quot;393&quot; height=&quot;271&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9xD2WsoSrTSEyxCRd15iFc5I16zYF55rotV0URUcULNzl1GUkOh4b89vV_r147CP8xK3b_0GH5BfgGW24cGDVaiXTXjCrH4qsnsZ8jXNl3LEZR9cNrMocEHvuovNTeQI4OJaUQXocxxRhbeJspl9X7Bgp5AFCXtSqf5bz1ejCgWGGnhpPKHr_ysMQkQ/w400-h271/image6.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Top:&lt;/b>; NATS-Bench size search test accuracy on &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;>;Cifar10&lt;/a>;; &lt;b>;Middle:&lt;/b>; On &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;>;Cifar100&lt;/a>;; &lt;b>;Bottom:&lt;/b>; On &lt;a href=&quot;https://image-net.org/&quot;>;ImageNet16-120&lt;/a>;. Average on 100 runs compared with random search (random), &lt;a href=&quot;https://arxiv.org/abs/1802.01548&quot;>;Regularized Evolution&lt;/a>; (evolution), and &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;>;Proximal Policy Optimization&lt;/a>; (PPO). &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We construct search spaces based on MobileNetV2, MobileNetV2 1.4x, MobileNetV3 Small, and MobileNetV3 Large and search for an optimal model architecture under different #MADDs (number of multiply-additions per image) constraints. Among all settings, LayerNAS finds a model with better accuracy on ImageNet. See the &lt;a href=&quot;https://arxiv.org/abs/2304.11517&quot;>;paper&lt;/a>; for details. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsxOl7N7q8-V8N4obUKVTDRjjIkmArrMUF_r4kZUF6-Cl0aJ5CUVgf78Bx4w3XfuGaevnZktn6zJ3EITTZF-vDRp5Yln_tYTIJcFqlK0JrSy2JDcORstzU7ipYeaKJgkNdtNXhpgWgViuYTYtc8jSPeCvlw7GL06MqgcbYDxxzSEb6QCM0pOiw3ociyw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1333&quot; data-original-width=&quot;1999&quot; height=&quot;426&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsxOl7N7q8-V8N4obUKVTDRjjIkmArrMUF_r4kZUF6-Cl0aJ5CUVgf78Bx4w3XfuGaevnZktn6zJ3EITTZF-vDRp5Yln_tYTIJcFqlK0JrSy2JDcORstzU7ipYeaKJgkNdtNXhpgWgViuYTYtc8jSPeCvlw7GL06MqgcbYDxxzSEb6QCM0pOiw3ociyw/w640-h426/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison on models under different #MAdds.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this post, we demonstrated how to reformulate NAS into a combinatorial optimization problem, and proposed LayerNAS as a solution that requires only polynomial search complexity. We compared LayerNAS with existing popular NAS algorithms and showed that it can find improved models on NATS-Bench. We also use the method to find better architectures based on MobileNetV2, and MobileNetV3. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Jingyue Shen, Keshav Kumar, Daiyi Peng, Mingxing Tan, Esteban Real, Peter Young, Weijun Wang, Qifei Wang, Xuanyi Dong, Xin Wang, Yingjie Miao, Yun Long, Zhuo Wang, Da-Cheng Juan, Deqiang Chen, Fotis Iliopoulos, Han-Byul Kim, Rino Lee, Andrew Howard, Erik Vee, Rina Panigrahy, Ravi Kumar and Andrew Tomkins for their contribution, collaboration and advice.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/1427927967438262189/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/layernas-neural-architecture-search-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1427927967438262189&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1427927967438262189&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/layernas-neural-architecture-search-in.html&quot; rel=&quot;alternate&quot; title=&quot;LayerNAS: Neural architecture search in polynomial complexity&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGm-s8kGO9BZTCn7UvdWUO2rNDUdXPGjXkT7C1RVq2sqx1qBWa72PUs5U8vcoP0inIaqubw9YtvkHODTZZDEryFCvj0PUV4bYCTgPD5b_sswQk1XG9ZcqHdU0jYQ879o-7TdBbpPa0HRklbB2BgZh0VlSRS-VhA3ovIxQPN4V1pmuSa6NrsQfzU0diiQ/s72-c/image4.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-190478125834380745&lt;/id>;&lt;published>;2023-04-23T19:26:00.001-07:00&lt;/published>;&lt;updated>;2023-04-23T19:30:30.342-07:00&lt;/updated>;&lt;title type=&quot;text&quot;>;Google at CHI 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWsabyBZRJDH9o64PiYE4YHaRa21wHoI4lgm0SGzcL5dqNK132QoCw0K-rUSrxAG2lfIO2Z8F8rR8OfotsxiJ-hXUAt3G2mtZ_jOsI_84r59EYcW_q6wZ-kR3Hi3H8yH1mZA8ReGI_jfkr7wGUc1A4BrMcWB7D7X8GjSUEc818RoY3X7QZ3G7wCVWeFQ/w640-h640/Google%20Germany%20Sticker%20Art%20Vecto%20PO%2013031.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week, the &lt;a href=&quot;https://chi2023.acm.org/&quot;>;Conference on Human Factors in Computing Systems&lt;/a>; (CHI 2023) is being held in Hamburg, Germany. We are proud to be a &lt;a href=&quot;https://chi2023.acm.org/for-sponsors-exhibitors-and-recruiters/sponsoring/chi-2023-sponsors/&quot;>;Hero Sponsor&lt;/a>; of CHI 2023, a premier conference on human-computer interaction, where Google researchers contribute at all levels. This year we are presenting over 30 papers and are actively involved in organizing and hosting a number of different events across workshops, courses, and interactive sessions. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; If you&#39;re registered for CHI 2023, we hope you&#39;ll visit the Google booth to learn more about the exciting work across various topics, including language interactions, causal inference, question answering and more. Take a look below to learn more about the Google research being presented at CHI 2023 (Google affiliations in bold). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;p>; Technical Program Chairs include: &lt;strong>;&lt;i>;Tesh Goyal&lt;/i>;&lt;/strong>; &lt;/p>; &lt;p>; Case Studies Chairs include: &lt;strong>;&lt;i>;Frank Bentley&lt;/i>;&lt;/strong>; &lt;/p>; &lt;p>; Keynotes Chairs include: &lt;strong>;&lt;i>;Elizabeth Churchill&lt;/i>;&lt;/strong>; &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Best Paper Award&lt;/h2>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/98030d5f96d6f383c90e40fcbf79425919f178d3.pdf&quot;>;Infrastructuring Care: How Trans and Non-Binary People Meet Health and Well-Being Needs through Technology&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Renee Shelby&lt;/i>;&lt;/b>;, &lt;i>;Rajesh Veeraraghavan&lt;/i>;, &lt;i>;Oliver Haimson&lt;/i>;, &lt;b>;&lt;i>;Gabriela Erickson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Turken&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Beka Gulotta &lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.04219.pdf&quot;>;NewsComp: Facilitating Diverse News Reading through Comparative Annotation&lt;/a>; &lt;br />; &lt;i>;Md Momen Bhuiyan&lt;/i>;, &lt;i>;Sang Won Lee&lt;/i>;, &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;, &lt;i>;Tanushree Mitra&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://www.shuminzhai.com/_files/ugd/62dd15_d0a29020b5e5488f9b9adb1e43b470b9.pdf&quot;>;WordGesture-GAN: Modeling Word-Gesture Movement with Generative Adversarial Network&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Jeremy Chu&lt;/i>;, &lt;i>;Dongsheng An&lt;/i>;, &lt;i>;Yan Ma&lt;/i>;, &lt;i>;Wenzhe Cui&lt;/i>;, &lt;b>;&lt;i>;Shumin Zhai&lt;/i>;&lt;/b>;, &lt;i>;Xianfeng David Gu&lt;/i>;, &lt;i>;Xiaojun Bi&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c02ad27506f5ce120cedfcd26246f782aa525625.pdf&quot;>;“The less I type, the better”: How AI Language Models can Enhance or Impede Communication for AAC Users&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Stephanie Valencia&lt;/i>;&lt;/b>;, &lt;i>;Richard Cave&lt;/i>;, &lt;b>;&lt;i>;Krystal Kallarackal&lt;/i>;&lt;/b>;, &lt;i>;Katie Seaver&lt;/i>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shaun Kane&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.00164v2.pdf&quot;>;A Mixed-Methods Approach to Understanding User Trust after Voice Assistant Failures&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Amanda Baughan&lt;/i>;*, &lt;b>;&lt;i>;Xuezhi Wang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ariel Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Allison Mercurio&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jilin Chen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiao Ma&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.08057.pdf&quot;>;&quot;There&#39;s so much responsibility on users right now:&quot; Expert Advice for Staying Safer From Hate and Harassment&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Miranda Wei&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sunny Consolvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Patrick Gage Kelley&lt;/i>;&lt;/b>;, &lt;i>;Tadayoshi Kohno&lt;/i>;, &lt;i>;Franziska Roesner&lt;/i>;, &lt;b>;&lt;i>;Kurt Thomas&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/20779824f83ae742db71b4711af67e389c189768.pdf&quot;>;ThingShare: Ad-Hoc Digital Copies of Physical Objects for Sharing Things in Video Meetings&lt;/a>; &lt;br />; &lt;i>;Erzhen Hu&lt;/i>;, &lt;i>;Jens Emil Sloth Grønbæk&lt;/i>;, &lt;i>;Wen Ying&lt;/i>;, &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;, &lt;i>;Seongkook Heo&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dfreed.me/wp-content/uploads/2023/02/Digital_Experiences_of_Teens__CHI__23_-18.pdf&quot;>;Understanding Digital-Safety Experiences of Youth in the US&lt;/a>; &lt;br />; &lt;i>;Diana Freed&lt;/i>;, &lt;i>;Natalie N. Bazarova&lt;/i>;, &lt;i>;Sunny Consolvo&lt;/i>;, &lt;i>;Eunice Han&lt;/i>;, &lt;b>;&lt;i>;Patrick Gage Kelley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>; Kurt Thomas&lt;/i>;&lt;/b>;, &lt;i>;Dan Cosley&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/87b73c53996c05a5f9cc67b1f67e3c9e2a49b28a.pdf&quot;>;Slide Gestalt: Automatic Structure Extraction in Slide Decks for Non-Visual Access&lt;/a>; &lt;br />; &lt;i>;Yi-Hao Peng&lt;/i>;*, &lt;b>;&lt;i>;Peggy Chi&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anjuli Kannan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Meredith Ringel Morris&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Irfan Essa&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.04711.pdf&quot;>;Using Logs Data to Identify When Engineers Experience Flow or Focused Work&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Adam Brown&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sarah D&#39;Angelo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ben Holtz&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ciera Jaspan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Collin Green&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.08655.pdf&quot;>;Enabling Conversational Interaction with Mobile UI Using Large Language Models&lt;/a>; &lt;br />; &lt;i>;Bryan Wang&lt;/i>;*, &lt;b>;&lt;i>;Gang Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yang Li&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.07184.pdf&quot;>;Practicing Information Sensibility: How Gen Z Engages with Online Information&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Amelia Hassoun&lt;/i>;, &lt;i>;Ian Beacock&lt;/i>;, &lt;b>;&lt;i>;Sunny Consolvo&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Beth Goldberg&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Patrick Gage Kelley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Daniel M. Russell&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/CHI23/857/959cee87-9a41-11ed-a76e-16bb50361d1f/OUT/chi23-857.pdf&quot;>;How Bold Can We Be? The Impact of Adjusting Font Grade on Readability in Light and Dark Polarities&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Hilary Palmen&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Gilbert&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Dave Crossland&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.12243.pdf&quot;>;Investigating How Practitioners Use Human-AI Guidelines: A Case Study on the People + AI Guidebook&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;i>;Nur Yildirim&lt;/i>;*, &lt;b>;&lt;i>;Mahima Pushkarna&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Martin Wattenberg&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Fernanda Viegas &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03535.pdf&quot;>;From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Shalaleh Rismani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Renee Shelby&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrew Smart&lt;/i>;&lt;/b>;, &lt;i>;Edgar W. Jatho&lt;/i>;, &lt;i>;Joshua A. Kroll&lt;/i>;, &lt;i>;AJung Moon&lt;/i>;, &lt;i>;&lt;b>;Negar Rostamzadeh&lt;/b>;&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4045f0e0e61d89b6b1eacad4b861e86631d5e660.pdf&quot;>;Designing Responsible AI: Adaptations of UX Practice to Meet Responsible AI Challenges&lt;/a>; &lt;br />; &lt;i>;Qiaosi Wang&lt;/i>;*, &lt;b>;&lt;i>;Michael Madaio&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shaun Kane&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shivani Kapania&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://www.researchgate.net/publication/368685360_It_is_currently_hodgepodge_Examining_AIML_Practitioners&#39;_Challenges_during_Co-production_of_Responsible_AI_Values&quot;>;“It is currently hodgepodge”: Examining AI/ML Practitioners&#39; Challenges during Co-production of Responsible AI Values&lt;/a>; &lt;br />; &lt;i>;Rama Adithya Varanasi&lt;/i>;, &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c5dbc2c146b7443447d43a344b4a22a359b32b16.pdf&quot;>;A Hunt for the Snark: Annotator Diversity in Data Practices&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;) &lt;br />; &lt;b>;&lt;i>;Shivani Kapania&lt;/i>;&lt;/b>;, &lt;i>;Alex S. Taylor&lt;/i>;, &lt;b>;&lt;i>;Ding Wang&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6dc693d389c226e5c70de5b40633c4ec2ddb6b43.pdf&quot;>;Visual Captions: Augmenting Verbal Communication with On-the-Fly Visuals&lt;/a>; &lt;br />; &lt;i>;Xingyu &quot;Bruce&quot; Liu&lt;/i>;, &lt;b>;&lt;i>;Vladimir Kirilyuk&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuxiu Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Peggy Chi&lt;/i>;&lt;/b>;, &lt;i>; Xiang &quot;Anthony&quot; Chen&lt;/i>;, &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/98030d5f96d6f383c90e40fcbf79425919f178d3.pdf&quot;>;Infrastructuring Care: How Trans and Non-Binary People Meet Health and Well-Being Needs through Technology&lt;/a>; (&lt;em>;Best Paper Award&lt;/em>;) &lt;br />; &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Renee Shelby&lt;/i>;&lt;/b>;, &lt;i>;Rajesh Veeraraghavan&lt;/i>;, &lt;i>;Oliver Haimson&lt;/i>;, &lt;b>;&lt;i>;Gabriela Erickson&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Turken&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Beka Gulotta&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://drive.google.com/file/d/1meAA_arK9rsR5H_UJBlZYsyh9eunyX14/view&quot;>;Kaleidoscope: Semantically-Grounded, Context-Specific ML Model Evaluation&lt;/a>; &lt;br />; &lt;i>;Harini Suresh&lt;/i>;, &lt;i>;Divya Shanmugam&lt;/i>;, &lt;i>;Tiffany Chen&lt;/i>;, &lt;i>;Annie G. Bryan&lt;/i>;, &lt;b>;&lt;i>;Alexander D&#39;Amour&lt;/i>;&lt;/b>;, &lt;i>;John Guttag&lt;/i>;, &lt;i>;Arvind Satyanarayan&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/405beeea55e7ae39d815e9b23fb6623cabb59ef5.pdf&quot;>;Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming&lt;/a>; (&lt;em>;Honorable Mention&lt;/em>;; see &lt;a href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Jin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michelle Carney&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Scott Miles&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Maria Kleiner&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuxiu Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yinda Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anuva Kulkarni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xingyu &quot;Bruce&quot; Liu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ahmed Sabie&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sergio Orts-Escolano&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Abhishek Kar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ping Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ram Iyengar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adarsh Kowdle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://frankbentleycom.files.wordpress.com/2023/04/chi23m-sub6725-cam-i61.pdf&quot;>;Exploring Users&#39; Perceptions and Expectations of Shapes for Dialog Designs&lt;/a>; &lt;br />; &lt;i>;Xinghui &quot;Erica&quot; Yan&lt;/i>;, &lt;b>;&lt;i>;Julia Feldman&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frank Bentley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammed Khwaja&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Gilbert &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://frankbentleycom.files.wordpress.com/2023/04/chi23m-sub2538-cam-i61.pdf&quot;>;Exploring the Future of Design Tooling: The Role of Artificial Intelligence in Tools for User Experience Professionals&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Tiffany Knearem&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Mohammed Khwaja&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Yuling Gao&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frank Bentley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Clara E. Kliman-Silver&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/a6855e786038b546c11b6bbc4e55fc8f876ad45d.pdf&quot;>;SpeakFaster Observer: Long-Term Instrumentation of Eye-Gaze Typing for Measuring AAC Communication&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Shanqing Cai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Subhashini Venugopalan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Katrin Tomanek&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shaun Kane&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Meredith Ringel Morris&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Richard Cave&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Robert MacDonald&lt;/i>;&lt;/b>;, &lt;i>;Jon Campbell&lt;/i>;, &lt;i>;Blair Casey&lt;/i>;, &lt;i>;Emily Kornman&lt;/i>;, &lt;i>;Daniel E. Vance&lt;/i>;, &lt;i>;Jay Beavers&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3506698&quot;>;Designerly Tele-Experiences: A New Approach to Remote Yet Still Situated Co-design&lt;/a>; &lt;br />; &lt;i>;Ferran Altarriba Bertran&lt;/i>;, &lt;i>;Alexandra Pometko&lt;/i>;, &lt;i>;Muskan Gupta&lt;/i>;, &lt;b>;&lt;i>;Lauren Wilcox&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Reeta Banerjee&lt;/i>;&lt;/b>;, &lt;i>;Katherine Isbister&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3506698&quot;>;“I Just Wanted to Triple Check . . . They Were All Vaccinated”: Supporting Risk Negotiation in the Context of COVID-19&lt;/a>; &lt;br />; &lt;i>;Margaret E. Morris&lt;/i>;, &lt;i>;Jennifer Brown&lt;/i>;, &lt;i>;Paula Nurius&lt;/i>;, &lt;i>;Savanna Yee&lt;/i>;, &lt;i>;Jennifer C. Mankoff&lt;/i>;, &lt;b>;&lt;i>;Sunny Consolvo &lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3544549.3585763&quot;>;Expectation vs Reality in Users&#39; Willingness to Delegate to Digital Assistants&lt;/a>; &lt;br />; &lt;i>;Ekaterina Svikhnushina&lt;/i>;*, &lt;b>;&lt;i>;Marcel Schellenberg&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Anna K. Niedbala&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Iva Barisic&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jeremy N. Miles&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3544549.3585596&quot;>;Interactive Visual Exploration of Knowledge Graphs with Embedding-Based Guidance&lt;/a>; &lt;br />; &lt;i>;Chao-Wen Hsuan Yuan&lt;/i>;, &lt;i>;Tzu-Wei Yu&lt;/i>;, &lt;b>;&lt;i>;Jia-Yu Pan&lt;/i>;&lt;/b>;, &lt;i>;Wen-Chieh Lin&lt;/i>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09498.pdf&quot;>;Measuring the Impact of Explanation Bias: A Study of Natural Language Justifications for Recommender Systems&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Krisztian Balog&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Filip Radlinski&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Andrey Petrov&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://duruofei.com/papers/Liu_ModelingAndImprovingTextStabilityInLiveCaptions_CHIEA2023.pdf&quot;>;Modeling and Improving Text Stability in Live Captions&lt;/a>; &lt;br />; &lt;i>;Xingyu &quot;Bruce&quot; Liu&lt;/i>;, &lt;b>;&lt;i>;Jun Zhang&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Leonardo Ferrer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Susan Xu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Vikas Bahirwani&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Boris Smus&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/be5d0f3efae124b5eefe0ff3f32c7ffcf1daf421.pdf&quot;>;Programming without a Programming Language: Challenges and Opportunities for Designing Developer Tools for Prompt Programming&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Alexander J. Fiannaca&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Chinmay Kulkarni&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carrie J. Cai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://savvaspetridis.github.io/papers/promptinfuser.pdf&quot;>;PromptInfuser: Bringing User Interface Mock-ups to Life with Large Language Models&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Savvas Petridis&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Carrie J. Cai&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://frankbentleycom.files.wordpress.com/2023/04/chiea23-529.pdf&quot;>;Prototypes, Platforms and Protocols: Identifying Common Issues with Remote, Unmoderated Studies and Their Impact on Research Participants&lt;/a>; &lt;br />; &lt;b>;&lt;i>;Steven Schirra&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Sasha Volkov&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Frank Bentley&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Shraddhaa Narasimha&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/99315&quot;>;Human-Centered Responsible Artificial Intelligence: Current &amp;amp; Future Trends&lt;/a>; &lt;br />; &lt;i>;Mohammad Tahaei&lt;/i>;, &lt;i>;Marios Constantinides&lt;/i>;, &lt;i>;Daniele Quercia&lt;/i>;, &lt;i>;Sean Kennedy&lt;/i>;, &lt;i>;Michael Muller&lt;/i>;, &lt;i>;Simone Stumpf&lt;/i>;, &lt;i>;Q. Vera Liao&lt;/i>;, &lt;i>;Ricardo Baeza-Yates&lt;/i>;, &lt;b>;&lt;i>;Lora Aroyo&lt;/i>;&lt;/b>;, &lt;i>;Jess Holbrook&lt;/i>;, &lt;i>;Ewa Luger&lt;/i>;, &lt;i>;&lt;b>;Michael Madaio&lt;/b>;&lt;/i>;, &lt;i>;Ilana Golbin Blumenfeld&lt;/i>;, &lt;i>;Maria De-Arteaga&lt;/i>;, &lt;i>;Jessica Vitak&lt;/i>;, &lt;i>;Alexandra Olteanu&lt;/i>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Interactive sessions&lt;/h2>; &lt;p>; &lt;a href=&quot;https://duruofei.com/papers/Du_ExperiencingRapidPrototypingOfMachineLearningBasedMultimediaApplicationsInRapsai_CHIEA2023.pdf&quot;>;Experiencing Rapid Prototyping of Machine Learning Based Multimedia Applications in Rapsai&lt;/a>; (see &lt;a href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;b>;&lt;i>;Ruofei Du&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Na Li&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Jing Jin&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michelle Carney&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Xiuxiu Yuan&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ram Iyengar&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Ping Yu&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Adarsh Kowdle&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Alex Olwal&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;p>; &lt;a href=&quot;https://in2writing.glitch.me/&quot;>;The Second Workshop on Intelligent and Interactive Writing Assistants&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Minsuk Chang&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://combatingonlinetoxicity.sites.uu.nl/organizers/&quot;>;Combating Toxicity, Harassment, and Abuse in Online Social Spaces: A Workshop at CHI 2023&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Nitesh Goyal&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/nd.edu/computational-uichi23/&quot;>;The Future of Computational Approaches for Understanding and Adapting User Interfaces&lt;/a>; &lt;br />; Keynote Speaker: &lt;b>;&lt;i>;Yang Li&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://empathich.com/&quot;>;The EmpathiCH Workshop: Unraveling Empathy-Centric Design&lt;/a>; &lt;br />; Panelists include: &lt;b>;&lt;i>;Cindy Bennett&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://chi-trait.github.io/#/&quot;>;Workshop on Trust and Reliance in AI-Human Teams (TRAIT)&lt;/a>; &lt;br />; Keynote Speakers: &lt;b>;&lt;i>;Carrie J. Cai&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>; &lt;br />; Program committee includes: &lt;b>;&lt;i>;Aaron Springer&lt;/i>;&lt;/b>;, &lt;b>;&lt;i>;Michael Terry&lt;/i>;&lt;/b>;&lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/view/sar-decision-making/&quot;>;Socially Assistive Robots as Decision Makers: Transparency, Motivations, and Intentions&lt;/a>; &lt;br />; Organizers include: &lt;b>;&lt;i>;Maja Matarić&lt;/i>;&lt;/b>;&lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Courses&lt;/h2>; &lt;p>; Human-Computer Interaction and AI: What Practitioners Need to Know to Design and Build Effective AI Systems from a Human Perspective (&lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/session/99328&quot;>;Part I&lt;/a>;; &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/session/99350&quot;>;Part II&lt;/a>;) &lt;br />; &lt;i>;Daniel M. Russell&lt;/i>;, &lt;i>;Q. Vera Liao&lt;/i>;, &lt;b>;&lt;i>;Chinmay Kulkarni&lt;/i>;&lt;/b>;, &lt;i>;Elena L. Glassman&lt;/i>;, &lt;i>;Nikolas Martelaro&lt;/i>;&lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; * Work done while at Google&lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/190478125834380745/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-chi-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/190478125834380745&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/190478125834380745&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/google-at-chi-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at CHI 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWsabyBZRJDH9o64PiYE4YHaRa21wHoI4lgm0SGzcL5dqNK132QoCw0K-rUSrxAG2lfIO2Z8F8rR8OfotsxiJ-hXUAt3G2mtZ_jOsI_84r59EYcW_q6wZ-kR3Hi3H8yH1mZA8ReGI_jfkr7wGUc1A4BrMcWB7D7X8GjSUEc818RoY3X7QZ3G7wCVWeFQ/s72-w640-h640-c/Google%20Germany%20Sticker%20Art%20Vecto%20PO%2013031.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6318039050629886850&lt;/id>;&lt;published>;2023-04-21T12:01:00.004-07:00&lt;/published>;&lt;updated>;2023-05-08T15:06:16.289-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TensorFlow&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Visual Blocks for ML: Accelerating machine learning prototyping with interactive tools&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ruofei Du, Interactive Perception &amp;amp; Graphics Lead, Google Augmented Reality, and Na Li, Tech Lead Manager, Google CoreML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl7alDg3lrJ1OofDgtgrcGJlHpjkoU_DThiHYOUepNt4mEUfRJckufpZD8wExahLSJC1S7XQZrznp0WkApOI8cq4-hPwxolT65igv5zx6wG0o1avhmKE2HQuz_aHAz2mld3ViB5Rc0eZnLK75EEILqsfjqhnLw7m22oK0fGVZadBWG-BRObu8oJpWKiQ/s320/Visual%20blocks%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;span style=&quot;font-size: small;&quot;>;&lt;i>;&lt;b>;Update — 2023/05/08:&lt;/b>; This post has been updated to include open-source details for the &lt;a href=&quot;https://visualblocks.withgoogle.com/#/&quot;>;Visual Blocks framework&lt;/a>;.&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>; Recent deep learning advances have enabled a plethora of high-performance, real-time multimedia applications based on machine learning (ML), such as &lt;a href=&quot;https://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html&quot;>;human body segmentation&lt;/a>; for video and teleconferencing, &lt;a href=&quot;https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793&quot;>;depth estimation&lt;/a>; for 3D reconstruction, &lt;a href=&quot;https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html&quot;>;hand and body tracking&lt;/a>; for interaction, and &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;>;audio processing&lt;/a>; for remote communication. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; However, developing and iterating on these ML-based multimedia prototypes can be challenging and costly. It usually involves a cross-functional team of ML practitioners who fine-tune the models, evaluate robustness, characterize strengths and weaknesses, inspect performance in the end-use context, and develop the applications. Moreover, models are frequently updated and require repeated integration efforts before evaluation can occur, which makes the workflow ill-suited to design and experiment. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://duruofei.com/papers/Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf&quot;>;Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming&lt;/a>;”, presented at &lt;a href=&quot;https://programs.sigchi.org/chi/2023/program/content/95967&quot;>;CHI 2023&lt;/a>;, we describe a visual programming platform for rapid and iterative development of end-to-end ML-based multimedia applications. Visual Blocks for ML, formerly called Rapsai, provides a no-code graph building experience through its &lt;a href=&quot;https://en.wikipedia.org/wiki/Node_graph_architecture&quot;>;node-graph editor&lt;/a>;. Users can create and connect different components (nodes) to rapidly build an ML pipeline, and see the results in real-time without writing any code. We demonstrate how this platform enables a better model evaluation experience through interactive characterization and visualization of ML model performance and interactive data augmentation and comparison. We have released the &lt;a href=&quot;https://visualblocks.withgoogle.com/#/&quot;>;Visual Blocks for ML&lt;/a>; framework, along with a &lt;a href=&quot;https://visualblocks.withgoogle.com/#/demo&quot;>;demo&lt;/a>; and &lt;a href=&quot;https://github.com/google/visualblocks&quot;>;Colab examples&lt;/a>;. Try it out yourself today. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0LJIJBaw9SlfR-PPhTerCqPiXkJ_6EakiQKk6LEVPs5fedNI8-OcZBXlHjmlAyo6Io-chL2MQb4tDMu4MzZtvxB3FjIbhJHw70OZ793aIhEFl6oXfOkI_NMENc-9HjlxC11k5zO_h4YPrTWJVE8l990AJXBXvitHPxxoNKa8XQch7-7JcMYojCFtGZg/s800/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;443&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0LJIJBaw9SlfR-PPhTerCqPiXkJ_6EakiQKk6LEVPs5fedNI8-OcZBXlHjmlAyo6Io-chL2MQb4tDMu4MzZtvxB3FjIbhJHw70OZ793aIhEFl6oXfOkI_NMENc-9HjlxC11k5zO_h4YPrTWJVE8l990AJXBXvitHPxxoNKa8XQch7-7JcMYojCFtGZg/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual Blocks uses a &lt;em>;node-graph editor that facilitates rapid prototyping of ML-based multimedia applications.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Formative study: Design goals for rapid ML prototyping&lt;/h2>; &lt;p>; To better understand the challenges of existing rapid prototyping ML solutions (&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2939672.2939778&quot;>;LIME&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/document/9751204&quot;>;VAC-CNN&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1518701.1518895&quot;>;EnsembleMatrix&lt;/a>;), we conducted a &lt;a href=&quot;https://psycnet.apa.org/record/2006-21649-000&quot;>;formative study&lt;/a>; (ie, the process of gathering feedback from potential users early in the design process of a technology product or system) using a conceptual mock-up interface. Study participants included seven computer vision researchers, audio ML researchers, and engineers across three ML teams. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRS0W-mt37RO8Af_UPkDsjg_XQyWjTTXkU1YViz-FpGIx4TqLONO9VQJftjo5dB0JBn0vIuqm8ZOFXTOTGHf8SPjnZoFe6y-wUErxXTz82-VPvyyUtT1OBWNnsThyE-Of-6yurGHUiRBkQiWuegVMi8rbLjE9FE6133XsD7aJJyFWpTCi0CKl7-nmAFg/s1039/mockup.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;690&quot; data-original-width=&quot;1039&quot; height=&quot;425&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRS0W-mt37RO8Af_UPkDsjg_XQyWjTTXkU1YViz-FpGIx4TqLONO9VQJftjo5dB0JBn0vIuqm8ZOFXTOTGHf8SPjnZoFe6y-wUErxXTz82-VPvyyUtT1OBWNnsThyE-Of-6yurGHUiRBkQiWuegVMi8rbLjE9FE6133XsD7aJJyFWpTCi0CKl7-nmAFg/w640-h425/mockup.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The formative study used a conceptual mock-up interface to gather early insights.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Through this formative study, we identified six challenges commonly found in existing prototyping solutions: &lt;/p>; &lt;ol>; &lt;li>;The input used to evaluate models typically differs from in-the-wild input with actual users in terms of resolution, aspect ratio, or sampling rate. &lt;/li>;&lt;li>;Participants could not quickly and interactively alter the input data or tune the model. &lt;/li>;&lt;li>;Researchers optimize the model with quantitative metrics on a fixed set of data, but real-world performance requires human reviewers to evaluate in the application context. &lt;/li>;&lt;li>;It is difficult to compare versions of the model, and cumbersome to share the best version with other team members to try it. &lt;/li>;&lt;li>;Once the model is selected, it can be time-consuming for a team to make a bespoke prototype that showcases the model. &lt;/li>;&lt;li>;Ultimately, the model is just part of a larger real-time pipeline, in which participants desire to examine intermediate results to understand the bottleneck. &lt;/li>; &lt;/ol>; &lt;p>; These identified challenges informed the development of the Visual Blocks system, which included six design goals: (1) develop a visual programming platform for rapidly building ML prototypes, (2) support real-time multimedia user input in-the-wild, (3) provide interactive data augmentation, (4) compare model outputs with side-by-side results, (5) share visualizations with minimum effort, and (6) provide off-the-shelf models and datasets. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Node-graph editor for visually programming ML pipelines&lt;/h2>; &lt;p>; Visual Blocks is mainly written in JavaScript and leverages &lt;a href=&quot;https://www.tensorflow.org/js&quot;>;TensorFlow.js&lt;/a>; and &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; for ML capabilities and &lt;a href=&quot;https://threejs.org/&quot;>;three.js&lt;/a>; for graphics rendering. The interface enables users to rapidly build and interact with ML models using three coordinated views: (1) a &lt;strong>;Nodes Library&lt;/strong>; that contains over 30 nodes (eg, Image Processing, Body Segmentation, Image Comparison) and a search bar for filtering, (2) a &lt;strong>;Node-graph Editor&lt;/strong>; that allows users to build and adjust a multimedia pipeline by dragging and adding nodes from the Nodes Library, and (3) a &lt;strong>;Preview Panel&lt;/strong>; that visualizes the pipeline&#39;s input and output, alters the input and intermediate results, and visually compares different models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYXWn2G2_Ra1f1gwxbk5GPlkpG5wTnNS0qm-GnfdxsLhc477T_Ig2kbeLI_tOI9Ad0QoAAPc-wCrcebPxjhw0k9L2JlWq2U2Y2aWEcbw95NP2mBu1pCVGmXodS0-fLqDiy4787gZM2REfOmBmfoz1pMQ51iurqksSg-WtvVYaclfpa8X69V22Fq09h6A/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1126&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYXWn2G2_Ra1f1gwxbk5GPlkpG5wTnNS0qm-GnfdxsLhc477T_Ig2kbeLI_tOI9Ad0QoAAPc-wCrcebPxjhw0k9L2JlWq2U2Y2aWEcbw95NP2mBu1pCVGmXodS0-fLqDiy4787gZM2REfOmBmfoz1pMQ51iurqksSg-WtvVYaclfpa8X69V22Fq09h6A/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The visual programming interface allows users to quickly develop and evaluate ML models by composing and previewing node-graphs with real-time results.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Iterative design, development, and evaluation of unique rapid prototyping capabilities&lt;/h2>; &lt;p>; Over the last year, we&#39;ve been iteratively designing and improving the Visual Blocks platform. Weekly feedback sessions with the three ML teams from the formative study showed appreciation for the platform&#39;s unique capabilities and its potential to accelerate ML prototyping through: &lt;/p>; &lt;ul>; &lt;li>;Support for various types of input data (image, video, audio) and output modalities (graphics, sound). &lt;/li>;&lt;li>;A library of pre-trained ML models for common tasks (body segmentation, landmark detection, portrait depth estimation) and custom model import options. &lt;/li>;&lt;li>;Interactive data augmentation and manipulation with drag-and-drop operations and parameter sliders. &lt;/li>;&lt;li>;Side-by-side comparison of multiple models and inspection of their outputs at different stages of the pipeline. &lt;/li>;&lt;li>;Quick publishing and sharing of multimedia pipelines directly to the web. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation: Four case studies&lt;/h2>; &lt;p>; To evaluate the usability and effectiveness of Visual Blocks, we conducted four case studies with 15 ML practitioners. They used the platform to prototype different multimedia applications: &lt;a href=&quot;https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html&quot;>;portrait depth with relighting effects&lt;/a>;, &lt;a href=&quot;https://augmentedperception.github.io/depthlab/&quot;>;scene depth with visual effects&lt;/a>;, &lt;a href=&quot;https://ai.googleblog.com/2022/01/accurate-alpha-matting-for-portrait.html&quot;>;alpha matting for virtual conferences&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2203.15578&quot;>;audio denoising for communication&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJTZjauOp5u2CP6ZCWD7V-E3okn17rDbcHLRRUNAZiygGbZfsTH0KaH5QRYJBj_HoLAKkLrX0Cw6wHrjRR2vTju-xC5GZzCEQp21w8Odwpo7IOyQQUHWusYvr4fzgc5RU1xP388AZuPr0LSQfSXaNl56fMCATZGydrmh6TwrUYC2ZmPRFa9zocWJFL_A/s1890/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1282&quot; data-original-width=&quot;1890&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJTZjauOp5u2CP6ZCWD7V-E3okn17rDbcHLRRUNAZiygGbZfsTH0KaH5QRYJBj_HoLAKkLrX0Cw6wHrjRR2vTju-xC5GZzCEQp21w8Odwpo7IOyQQUHWusYvr4fzgc5RU1xP388AZuPr0LSQfSXaNl56fMCATZGydrmh6TwrUYC2ZmPRFa9zocWJFL_A/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The system streamlining comparison of two &lt;a href=&quot;https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793&quot;>;Portrait Depth&lt;/a>; models, including customized visualization and effects.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With a short introduction and video tutorial, participants were able to quickly identify differences between the models and select a better model for their use case. We found that Visual Blocks helped facilitate rapid and deeper understanding of model benefits and trade-offs: &lt;/p>; &lt;blockquote>;&lt;span style=&quot;font-size: small;&quot;>;&lt;i>;“It gives me intuition about which data augmentation operations that my model is more sensitive [to], then I can go back to my training pipeline, maybe increase the amount of data augmentation for those specific steps that are making my model more sensitive.” (Participant 13)&lt;/i>;&lt;/span>;&lt;/blockquote>; &lt;blockquote>;&lt;span style=&quot;font-size: small;&quot;>; &lt;i>; “It&#39;s a fair amount of work to add some background noise, I have a script, but then every time I have to find that script and modify it. I&#39;ve always done this in a one-off way. It&#39;s simple but also very time consuming. This is very convenient.” (Participant 15)&lt;/i>;&lt;/span>;&lt;/blockquote>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDZJpy0nnMh4O6RuoRbWQ78LzcYWA57nQXwUMy9UtB4mu0pKgnZ4Uo2FDixcgEx6OorTPKBpquX1HGdWL-kPUNSxVpQSwb0PcbX2wMWpghnNbnIFYTSfIVT1oS5LEqmz1inahGAcFNDpb3Jyohu107tuQdLVLNmZ-4kX4CBU25WrlYjV-c_sQC21v-0A/s720/depth.mov.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;389&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDZJpy0nnMh4O6RuoRbWQ78LzcYWA57nQXwUMy9UtB4mu0pKgnZ4Uo2FDixcgEx6OorTPKBpquX1HGdWL-kPUNSxVpQSwb0PcbX2wMWpghnNbnIFYTSfIVT1oS5LEqmz1inahGAcFNDpb3Jyohu107tuQdLVLNmZ-4kX4CBU25WrlYjV-c_sQC21v-0A/s16000/depth.mov.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The system allows researchers to compare multiple &lt;a href=&quot;https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html?linkId=8063793&quot;>;Portrait Depth&lt;/a>; models at different noise levels, helping ML practitioners identify the strengths and weaknesses of each.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In a post-hoc survey using a seven-point &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale#:~:text=Likert%20scaling%20is%20a%20bipolar,the%20neutral%20option%20is%20removed.&quot;>;Likert scale&lt;/a>;, participants reported Visual Blocks to be more transparent about how it arrives at its final results than Colab (Visual Blocks 6.13 ± 0.88 vs. &lt;a href=&quot;https://colab.sandbox.google.com/github/google-research/vision_transformer/blob/main/lit.ipynb&quot;>;Colab&lt;/a>; 5.0 ± 0.88, 𝑝 &amp;lt; .005) and more collaborative with users to come up with the outputs (Visual Blocks 5.73 ± 1.23 vs. Colab 4.15 ± 1.43, 𝑝 &amp;lt; .005). Although Colab assisted users in thinking through the task and controlling the pipeline more effectively through programming, Users reported that they were able to complete tasks in Visual Blocks in just a few minutes that could normally take up to an hour or more. For example, after watching a 4-minute tutorial video, all participants were able to build a custom pipeline in Visual Blocks from scratch within 15 minutes (10.72 ± 2.14). Participants usually spent less than five minutes (3.98 ± 1.95) getting the initial results, then were trying out different input and output for the pipeline. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaPnqjJDwShP4G-S-QplGrbGpdkevN8ZivhcP8h_8APwFGP0VWLnkwH5MB57qGLVOpE_cyiwJKMHqDWJBp5gr2fryTvO2HZtdRvFINYQfOC1CrsCRo2gUn1zrkXDHOCrLsXLJgpqYhj1b0sFuAmgWND5HOZV_V9HpzOJwES8ecvMWgpmvCTKskgSkvnw/s1511/rapsai-eval.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;483&quot; data-original-width=&quot;1511&quot; height=&quot;205&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaPnqjJDwShP4G-S-QplGrbGpdkevN8ZivhcP8h_8APwFGP0VWLnkwH5MB57qGLVOpE_cyiwJKMHqDWJBp5gr2fryTvO2HZtdRvFINYQfOC1CrsCRo2gUn1zrkXDHOCrLsXLJgpqYhj1b0sFuAmgWND5HOZV_V9HpzOJwES8ecvMWgpmvCTKskgSkvnw/w640-h205/rapsai-eval.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User ratings between Rapsai (initial prototype of Visual Blocks) and Colab across five dimensions.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; More results in &lt;a href=&quot;https://duruofei.com/papers/Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf&quot;>;our paper&lt;/a>; showed that Visual Blocks helped participants accelerate their workflow, make more informed decisions about model selection and tuning, analyze strengths and weaknesses of different models, and holistically evaluate model behavior with real-world input. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusions and future directions&lt;/h2>; &lt;p>;&lt;a href=&quot;https://visualblocks.withgoogle.com/#/&quot;>;Visual Blocks&lt;/a>; lowers development barriers for ML-based multimedia applications. It empowers users to experiment without worrying about coding or technical details. It also facilitates collaboration between designers and developers by providing a common language for describing ML pipelines. In the future, we plan to open this framework up for the community to contribute their own nodes and integrate it into many different platforms. We expect visual programming for machine learning to be a common interface across ML tooling going forward. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;i>;This work is a collaboration across multiple teams at Google. Key contributors to the project include Ruofei Du, Na Li, Jing Jin, Michelle Carney, Xiuxiu Yuan, Kristen Wright, Mark Sherwood, Jason Mayes, Lin Chen, Jun Jiang, Scott Miles, Maria Kleiner, Yinda Zhang, Anuva Kulkarni, Xingyu &quot;Bruce&quot; Liu, Ahmed Sabie, Sergio Escolano, Abhishek Kar, Ping Yu, Ram Iyengar, Adarsh Kowdle, and Alex Olwal.&lt;/i>; &lt;/p>; &lt;p>; &lt;i>;We would like to extend our thanks to Jun Zhang, Satya Amarapalli and Sarah Heimlich for a few early-stage prototypes, Sean Fanello, Danhang Tang, Stephanie Debats, Walter Korman, Anne Menini, Joe Moran, Eric Turner, and Shahram Izadi for providing initial feedback for the manuscript and the blog post. We would also like to thank our CHI 2023 reviewers for their insightful feedback.&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6318039050629886850/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6318039050629886850&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6318039050629886850&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html&quot; rel=&quot;alternate&quot; title=&quot;Visual Blocks for ML: Accelerating machine learning prototyping with interactive tools&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl7alDg3lrJ1OofDgtgrcGJlHpjkoU_DThiHYOUepNt4mEUfRJckufpZD8wExahLSJC1S7XQZrznp0WkApOI8cq4-hPwxolT65igv5zx6wG0o1avhmKE2HQuz_aHAz2mld3ViB5Rc0eZnLK75EEILqsfjqhnLw7m22oK0fGVZadBWG-BRObu8oJpWKiQ/s72-c/Visual%20blocks%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8031022821156747802&lt;/id>;&lt;published>;2023-04-20T13:26:00.001-07:00&lt;/published>;&lt;updated>;2023-04-20T13:30:15.380-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Cloud Platform&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICLR&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Recent advances in deep long-horizon forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Rajat Sen and Abhimanyu Das, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjazCNV8O9s-ZwT9g5cBDsMQ_gv2SMf5enK_cMdtOamv-WJGXlB3qzhZme8xVfXhA2xFS0HwaDBlh3d6z3R2FS_qW05ouD2z9jQ5sgIizM4VcE6-i6BSf4MJmin1KmfEhkkrVxfZUj-H2ePsGYjqlnCrMqBtk68uHplGODDSmeu8FABJ_ldthUpR0WF_A/s1600/scalar.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time-series&lt;/a>; forecasting is an important research area that is critical to several scientific and industrial applications, like retail supply chain optimization, energy and traffic prediction, and weather forecasting. In retail use cases, for example, it has been observed that &lt;a href=&quot;https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&quot;>;improving demand forecasting accuracy&lt;/a>; can meaningfully reduce inventory costs and increase revenue. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Modern time-series applications can involve forecasting hundreds of thousands of correlated time-series (eg, demands of different products for a retailer) over long horizons (eg, a quarter or year away at daily granularity). As such, time-series forecasting models need to satisfy the following key criterias: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Ability to handle auxiliary features or covariates:&lt;/em>; Most use-cases can benefit tremendously from effectively using covariates, for instance, in retail forecasting, holidays and product specific attributes or promotions can affect demand. &lt;/li>;&lt;li>;&lt;em>;Suitable for different data modalities:&lt;/em>; It should be able to handle sparse count data, eg, intermittent demand for a product with low volume of sales while also being able to model robust continuous seasonal patterns in traffic forecasting. &lt;/li>; &lt;/ol>; &lt;p>; A number of neural network–based solutions have been able to show good performance on benchmarks and also support the above criterion. However, these methods are typically slow to train and can be expensive for inference, especially for longer horizons. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2304.08424&quot;>;Long-term Forecasting with TiDE: Time-series Dense Encoder&lt;/a>;”, we present an all &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) encoder-decoder architecture for time-series forecasting that achieves superior performance on long horizon time-series forecasting benchmarks when compared to &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformer&lt;/a>;-based solutions, while being 5–10x faster. Then in “&lt;a href=&quot;https://openreview.net/forum?id=zrW-LVXj2k1&quot;>;On the benefits of maximum likelihood estimation for Regression and Forecasting&lt;/a>;”, we demonstrate that using a carefully designed training loss function based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;>;maximum likelihood estimation&lt;/a>; (MLE) can be effective in handling different data modalities. These two works are complementary and can be applied as a part of the same model. In fact, they will be available soon in &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python&quot;>;Google Cloud AI&#39;s Vertex AutoML Forecasting&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;TiDE: A simple MLP architecture for fast and accurate forecasting&lt;/h2>; &lt;p>; Deep learning has shown promise in time-series forecasting, &lt;a href=&quot;https://proceedings.mlr.press/v162/zhou22g.html&quot;>;outperforming traditional statistical methods, especially for large multivariate datasets&lt;/a>;. After the success of &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;transformers&lt;/a>; in &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_processing&quot;>;natural language processing&lt;/a>; (NLP), there have been several works evaluating variants of the Transformer architecture for long horizon (the amount of time into the future) forecasting, such as &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;FEDformer&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;. However, &lt;a href=&quot;https://arxiv.org/pdf/2205.13504.pdf&quot;>;other work&lt;/a>; has suggested that even linear models can outperform these transformer variants on time-series benchmarks. Nonetheless, simple linear models are not expressive enough to handle auxiliary features (eg, holiday features and promotions for retail demand forecasting) and non-&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_function&quot;>;linear&lt;/a>; dependencies on the past. &lt;/p>; &lt;p>; We present a scalable MLP-based encoder-decoder model for fast and accurate multi-step forecasting. Our model encodes the past of a time-series and all available features using an MLP encoder. Subsequently, the encoding is combined with future features using an MLP decoder to yield future predictions. The architecture is illustrated below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAvTJSswOJMJL_hACqexTMAxEV9faTAYLT_rxH_0mT9AlsBC8neR1iumIVwi30vlZK7US4BpLRBgRu-5uzjdhKwpizYWrO_3Cwwh0xjkKkpwkdF0He9CRyidSva8lgznEB7X-e36hEE0E22eXrWf2c2nqbGZPGE0YPFZ8a-5GR2daV4gwBb8dstVdu-g/s850/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;850&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAvTJSswOJMJL_hACqexTMAxEV9faTAYLT_rxH_0mT9AlsBC8neR1iumIVwi30vlZK7US4BpLRBgRu-5uzjdhKwpizYWrO_3Cwwh0xjkKkpwkdF0He9CRyidSva8lgznEB7X-e36hEE0E22eXrWf2c2nqbGZPGE0YPFZ8a-5GR2daV4gwBb8dstVdu-g/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;TiDE model architecture for multi-step forecasting.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; TiDE is more than 10x faster in training compared to transformer-based baselines while being more accurate on benchmarks. Similar gains can be observed in inference as it only scales linearly with the length of the context (the number of time-steps the model looks back) and the prediction horizon. Below on the left, we show that our model can be 10.6% better than the best transformer-based baseline (PatchTST) on a popular traffic forecasting benchmark, in terms of test &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;mean squared error (MSE)&lt;/a>;. On the right, we show that at the same time our model can have much faster inference latency than PatchTST. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbSEnIDH_ZmJgpxkPTqvTYpaHmNeTOSryb7lgBxWk8zj7OTVeg8NGRkDy70AW3CQBKBPeVO9K7h8DgbylbRcUBZomd1W53L1ZCVrjN7OVcWykXYisEDvlHLZVdF6Rx_3e257_UYFu81xRdrXstStF-4x_VIDiyiRgvhztHpiW_Ts_qPwSO3MMgAI44lw/s1999/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;584&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbSEnIDH_ZmJgpxkPTqvTYpaHmNeTOSryb7lgBxWk8zj7OTVeg8NGRkDy70AW3CQBKBPeVO9K7h8DgbylbRcUBZomd1W53L1ZCVrjN7OVcWykXYisEDvlHLZVdF6Rx_3e257_UYFu81xRdrXstStF-4x_VIDiyiRgvhztHpiW_Ts_qPwSO3MMgAI44lw/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left:&lt;/b>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;MSE&lt;/a>; on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets&quot;>;test set&lt;/a>; of a popular traffic forecasting benchmark. &lt;b>;Right:&lt;/b>; inference time of TiDE and PatchTST as a function of the look-back length.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our research demonstrates that we can take advantage of MLP&#39;s linear computational scaling with look-back and horizon sizes without sacrificing accuracy, while transformers scale quadratically in this situation. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Probabilistic loss functions&lt;/h2>; &lt;p>; In most forecasting applications the end user is interested in popular target metrics like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_percentage_error&quot;>;mean absolute percentage error&lt;/a>; (MAPE), &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_percentage_error#WMAPE&quot;>;weighted absolute percentage error&lt;/a>; (WAPE), etc. In such scenarios, the standard approach is to use the same target metric as the loss function while training. In “&lt;a href=&quot;https://openreview.net/forum?id=zrW-LVXj2k1&quot;>;On the benefits of maximum likelihood estimation for Regression and Forecasting&lt;/a>;”, accepted at &lt;a href=&quot;https://iclr.cc/Conferences/2023&quot;>;ICLR&lt;/a>;, we show that this approach might not always be the best. Instead, we advocate using the maximum likelihood loss for a carefully chosen family of distributions (discussed more below) that can capture inductive biases of the dataset during training. In other words, instead of directly outputting point predictions that minimize the target metric, the forecasting neural network predicts the parameters of a distribution in the chosen family that best explains the target data. At inference time, we can predict the statistic from the learned predictive distribution that minimizes the target metric of interest (eg, the mean minimizes the MSE target metric while the median minimizes the WAPE). Further, we can also easily obtain uncertainty estimates of our forecasts, ie, we can provide quantile forecasts by estimating the quantiles of the predictive distribution. In several use cases, accurate quantiles are vital, for instance, in demand forecasting a retailer might want to stock for the 90th &lt;a href=&quot;https://en.wikipedia.org/wiki/Percentile&quot;>;percentile&lt;/a>; to guard against worst-case scenarios and avoid lost revenue. &lt;/p>; &lt;p>; The choice of the distribution family is crucial in such cases. For example, in the context of sparse count data, we might want to have a distribution family that can put more probability on zero, which is commonly known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-inflated_model&quot;>;zero-inflation&lt;/a>;. We propose a mixture of different distributions with learned mixture weights that can adapt to different data modalities. In the paper, we show that using a mixture of zero and multiple negative binomial distributions works well in a variety of settings as it can adapt to sparsity, multiple modalities, count data, and data with &lt;a href=&quot;https://en.wikipedia.org/wiki/Heavy-tailed_distribution&quot;>;sub-exponential tails&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpER0cWQ65hFEvCvFvsD-PrOxgw-6WBOSPzDxM0lZOft7j8KiKfugaBJc2l4kvpI6_6udTYjPdgdRgELPSOhwnwviPImdBY4R6Y8bVpfhkqtY9boDMHqQLiJ0yEiEtWI4Z6QnHy3uSVVltBcoiEfw7SqQEtnF9tqFqj7puNKZmO-BaRuTubpxsD-l4Kw/s960/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;720&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpER0cWQ65hFEvCvFvsD-PrOxgw-6WBOSPzDxM0lZOft7j8KiKfugaBJc2l4kvpI6_6udTYjPdgdRgELPSOhwnwviPImdBY4R6Y8bVpfhkqtY9boDMHqQLiJ0yEiEtWI4Z6QnHy3uSVVltBcoiEfw7SqQEtnF9tqFqj7puNKZmO-BaRuTubpxsD-l4Kw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A mixture of zero and two negative binomial distributions. The weights of the three components, a&lt;sub>;1&lt;/sub>;, a&lt;sub>;2&lt;/sub>; and a&lt;sub>;3&lt;/sub>;, can be learned during training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We use this loss function for training Vertex AutoML models on the &lt;a href=&quot;https://www.kaggle.com/c/m5-forecasting-accuracy&quot;>;M5 forecasting competition&lt;/a>; dataset and show that this simple change can lead to a 6% gain and outperform other benchmarks in the competition metric, &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy/overview/evaluation&quot;>;weighted root mean squared scaled error&lt;/a>; (WRMSSE). &lt;/p>; &lt;br>; &lt;table align=&quot;center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>;&lt;b>;M5 Forecasting&lt;/b>; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;b>;WRMSSE&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Vertex AutoML &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.639 +/- 0.007 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Vertex AutoML with probabilistic loss&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;b>;0.581 +/- 0.007&lt;/b>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;DeepAR &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.789 +/- 0.025 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;FEDFormer &lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.804 +/- 0.033 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:140%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have shown how TiDE, together with probabilistic loss functions, enables fast and accurate forecasting that automatically adapts to different data distributions and modalities and also provides uncertainty estimates for its predictions. It provides state-of-the-art accuracy among neural network–based solutions at a fraction of the cost of previous transformer-based forecasting architectures, for large-scale enterprise forecasting applications. We hope this work will also spur interest in revisiting (both theoretically and empirically) MLP-based deep time-series forecasting models. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Pranjal Awasthi, Dawei Jia, Weihao Kong, Andrew Leach, Shaan Mathur, Petros Mol, Shuxin Nie, Ananda Theertha Suresh, and Rose Yu. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/8031022821156747802/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031022821156747802&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8031022821156747802&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html&quot; rel=&quot;alternate&quot; title=&quot;Recent advances in deep long-horizon forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjazCNV8O9s-ZwT9g5cBDsMQ_gv2SMf5enK_cMdtOamv-WJGXlB3qzhZme8xVfXhA2xFS0HwaDBlh3d6z3R2FS_qW05ouD2z9jQ5sgIizM4VcE6-i6BSf4MJmin1KmfEhkkrVxfZUj-H2ePsGYjqlnCrMqBtk68uHplGODDSmeu8FABJ_ldthUpR0WF_A/s72-c/scalar.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6000199110548559167&lt;/id>;&lt;published>;2023-04-19T11:12:00.002-07:00&lt;/published>;&lt;updated>;2023-04-19T14:11:04.992-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Technology, AI, Society and Culture&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Lauren Wilcox, Senior Staff Research Scientist, on behalf of the Technology, AI, Society, and Culture Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjojA8Eq3h0gUbJQ6ttpMVcy6mzFucxjbGvEpKL2jnxLoQaAqSYYTwrw4q6GLtCGOeNlThZw4-uj9oQjmYlJyb_vNgEWHoE1jUfHP-bUslCjN6eeJGZvMcfQkaZf-E4RRg06yhmC0giabaqlRupcYOmWi46uZC3VnLmpX1ZQn3C_wtw8gp8zVubx_o0dw/s620/TASC-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google sees &lt;a href=&quot;https://blog.google/technology/ai/why-we-focus-on-ai-and-to-what-end/&quot;>;AI as a foundational and transformational technology&lt;/a>;, with recent advances in generative AI technologies, such as &lt;a href=&quot;https://arxiv.org/abs/2201.08239&quot;>;LaMDA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot;>;PaLM&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;>;Imagen&lt;/a>;, &lt;a href=&quot;https://parti.research.google/&quot;>;Parti&lt;/a>;, &lt;a href=&quot;https://research.google/pubs/pub52118/&quot;>;MusicLM&lt;/a>;, and similar machine learning (ML) models, some of which are now being incorporated into &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/generative-ai&quot;>;our products&lt;/a>;. This transformative potential requires us to be responsible not only in how we advance our technology, but also in how we envision which technologies to build, and how we assess the social impact AI and ML-enabled technologies have on the world. This endeavor necessitates fundamental and applied research with an &lt;a href=&quot;https://doi.org/10.17226/26507&quot;>;interdisciplinary lens&lt;/a>; that engages with — and accounts for — the social, cultural, economic, and other contextual dimensions that shape the development and deployment of AI systems. We must also understand the range of possible impacts that ongoing use of such technologies may have on vulnerable communities and broader social systems.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Our team, Technology, AI, Society, and Culture (TASC), is addressing this critical need. Research on the societal impacts of AI is complex and multi-faceted; no one disciplinary or methodological perspective can alone provide the diverse insights needed to grapple with the social and cultural implications of ML technologies. TASC thus leverages the strengths of an interdisciplinary team, with backgrounds ranging from computer science to social science, digital media and urban science. We use a multi-method approach with qualitative, quantitative, and mixed methods to critically examine and shape the social and technical processes that underpin and surround AI technologies. We focus on participatory, culturally-inclusive, and intersectional equity-oriented research that brings to the foreground impacted communities. Our work advances Responsible AI (RAI) in areas such as &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://sites.google.com/corp/view/c3nlp/home&quot;>;natural language processing&lt;/a>;, &lt;a href=&quot;https://aaas.confex.com/aaas/2023/meetingapp.cgi/Session/29974&quot;>;health&lt;/a>;, and general purpose ML models and applications. Below, we share examples of our approach to &lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI&lt;/a>; and where we are headed in 2023. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7CrSVLFHgjk2TP-7oA0-suXvnDKTBxOBQjmgUz6QHn9nhiPcPZ19GFXsaU2kl9vlc1j8H6lm2oz0_bcor5vOaMrB3TK2-hKFnTps2yyo_m9QLlNnc75KgiYuj52O1U6bEeuv57eswy3SF1rPsyer3y42KrHOpN8Puzvn-fC_mloWJNh9Pfg2QnOwMHA/s1392/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;675&quot; data-original-width=&quot;1392&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh7CrSVLFHgjk2TP-7oA0-suXvnDKTBxOBQjmgUz6QHn9nhiPcPZ19GFXsaU2kl9vlc1j8H6lm2oz0_bcor5vOaMrB3TK2-hKFnTps2yyo_m9QLlNnc75KgiYuj52O1U6bEeuv57eswy3SF1rPsyer3y42KrHOpN8Puzvn-fC_mloWJNh9Pfg2QnOwMHA/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual diagram of the various social, technical, and equity-oriented research areas that TASC studies to progress Responsible AI in a way that respects the complex relationships between AI and society.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Theme 1: Culture, communities, &amp;amp; AI&lt;/h2>; &lt;p>; One of our key areas of research is the advancement of methods to make generative AI technologies more inclusive of and valuable to people globally, through &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517716&quot;>;community-engaged&lt;/a>;, and &lt;a href=&quot;https://ai-cultures.github.io/&quot;>;culturally-inclusive&lt;/a>; approaches. Toward this aim, we see communities as experts in their context, recognizing their deep knowledge of how technologies can and should impact their own lives. Our research champions the importance of embedding &lt;a href=&quot;https://arxiv.org/pdf/2211.13069.pdf&quot;>;cross-cultural considerations&lt;/a>; throughout the ML development pipeline. Community engagement enables us to shift how we incorporate knowledge of what&#39;s most important throughout this pipeline, from dataset curation to evaluation. This also enables us to understand and account for the ways in which technologies fail and how specific communities might experience harm. Based on this understanding we have created &lt;a href=&quot;https://aclanthology.org/2022.aacl-main.55/&quot;>;responsible AI evaluation strategies&lt;/a>; that are effective in recognizing and mitigating biases along multiple dimensions. &lt;/p>; &lt;p>; Our work in this area is vital to ensuring that Google&#39;s technologies are safe for, work for, and are useful to a diverse set of stakeholders around the world. For example, our research on &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491102.3517533&quot;>;user attitudes towards AI&lt;/a>;, &lt;a href=&quot;https://research.google/pubs/pub52061/&quot;>;responsible interaction design&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/pdf/2209.12226.pdf&quot;>;fairness evaluations&lt;/a>; with a focus on the global south demonstrated the cross-cultural differences in the impact of AI and contributed resources that enable culturally-situated evaluations. We are also building cross-disciplinary research communities to examine the relationship between AI, culture, and society, through our recent and upcoming workshops on &lt;a href=&quot;https://ai-cultures.github.io/&quot;>;Cultures in AI/AI in Culture&lt;/a>;, &lt;a href=&quot;https://sites.google.com/corp/view/ec3v-cvpr2023/home&quot;>;Ethical Considerations in Creative Applications of Computer Vision&lt;/a>;, and &lt;a href=&quot;https://sites.google.com/corp/view/c3nlp&quot;>;Cross-Cultural Considerations in NLP&lt;/a>;. &lt;/p>; &lt;p>; Our recent research has also sought out perspectives of particular communities who are known to be less represented in ML development and applications. For example, we have investigated gender bias, both in &lt;a href=&quot;https://blog.google/technology/ai/reducing-gender-based-harms-in-ai-with-sunipa-dev/&quot;>;natural language&lt;/a>; and in contexts such as &lt;a href=&quot;https://research.google/pubs/pub52060/&quot;>;gender-inclusive health&lt;/a>;, drawing on our research to develop more accurate evaluations of bias so that anyone developing these technologies can identify and mitigate harms for people with &lt;a href=&quot;https://facctconference.org/2022/acceptedcraft.html#colab&quot;>;queer and non-binary identities&lt;/a>;. &lt;br />; &lt;/p>; &lt;br />; &lt;h2>;Theme 2: Enabling Responsible AI throughout the development lifecycle&lt;/h2>; &lt;p>; We work to enable RAI at scale, by establishing industry-wide best practices for RAI across the development pipeline, and ensuring our technologies verifiably incorporate that best practice by default. This applied research includes responsible data production and analysis for ML development, and systematically advancing tools and practices that support practitioners in meeting key RAI goals like transparency, fairness, and accountability. Extending earlier work on &lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533231&quot;>;Data Cards&lt;/a>;, &lt;a href=&quot;https://modelcards.withgoogle.com/model-reports&quot;>;Model Cards&lt;/a>; and the &lt;a href=&quot;https://www.tensorflow.org/responsible_ai/model_card_toolkit/guide&quot;>;Model Card Toolkit&lt;/a>;, we released the &lt;a href=&quot;https://sites.research.google/datacardsplaybook/&quot;>;Data Cards Playbook&lt;/a>;, providing developers with methods and tools to document appropriate uses and essential facts related to a dataset. Because ML models are often trained and evaluated on human-annotated data, we also advance human-centric research on data annotation. We have developed &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3531146.3534647&quot;>;frameworks to document annotation processes&lt;/a>; and methods to account for &lt;a href=&quot;https://arxiv.org/abs/2110.05719&quot;>;rater disagreement&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2301.09406&quot;>;rater diversity&lt;/a>;. These methods enable ML practitioners to better ensure &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c5dbc2c146b7443447d43a344b4a22a359b32b16.pdf&quot;>;diversity in annotation of datasets&lt;/a>; used to train models, by identifying current barriers and re-envisioning data work practices. &lt;/p>; &lt;br />; &lt;h2>;Future directions&lt;/h2>; &lt;p>; We are now working to further broaden participation in ML model development, through approaches that embed a diversity of cultural contexts and voices into technology design, development, and impact assessment to ensure that AI achieves societal goals. We are also redefining responsible practices that can handle the scale at which ML technologies operate in today&#39;s world. For example, we are developing frameworks and structures that can enable community engagement within industry AI research and development, including community-centered evaluation frameworks, benchmarks, and dataset curation and sharing. &lt;/p>; &lt;p>; In particular, we are furthering our prior work on understanding how &lt;a href=&quot;https://aclanthology.org/2020.acl-main.487.pdf&quot;>;NLP language models &lt;span>;may perpetuate bias against people with disabilities&lt;/span>;&lt;/a>;, extending this research to address other marginalized communities and cultures and including image, video, and other multimodal models. Such models may contain tropes and stereotypes about particular groups or may erase the experiences of specific individuals or communities. Our efforts to identify sources of bias within ML models will lead to better detection of these representational harms and will support the creation of more fair and inclusive systems. &lt;/p>; &lt;p>; TASC is about studying all the touchpoints between AI and people — from individuals and communities, to cultures and society. For AI to be culturally-inclusive, equitable, accessible, and reflective of the needs of impacted communities, we must take on these challenges with inter- and multidisciplinary research that centers the needs of impacted communities. Our research studies will continue to explore the interactions between society and AI, furthering the discovery of new ways to develop and evaluate AI in order for us to develop more robust and culturally-situated AI technologies. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank everyone on the team that contributed to this blog post. In alphabetical order by last name: Cynthia Bennett, Eric Corbett, Aida Mostafazadeh Davani, Emily Denton, Sunipa Dev, Fernando Diaz, Mark Díaz, Shaun Kane, Shivani Kapania, Michael Madaio, Vinodkumar Prabhakaran, Rida Qadri, Renee Shelby, Ding Wang, and Andrew Zaldivar. Also, we would like to thank Toju Duke and Marian Croak for their valuable feedback and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/6000199110548559167/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6000199110548559167&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6000199110548559167&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Technology, AI, Society and Culture&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjojA8Eq3h0gUbJQ6ttpMVcy6mzFucxjbGvEpKL2jnxLoQaAqSYYTwrw4q6GLtCGOeNlThZw4-uj9oQjmYlJyb_vNgEWHoE1jUfHP-bUslCjN6eeJGZvMcfQkaZf-E4RRg06yhmC0giabaqlRupcYOmWi46uZC3VnLmpX1ZQn3C_wtw8gp8zVubx_o0dw/s72-c/TASC-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-331432973525797076&lt;/id>;&lt;published>;2023-04-18T13:19:00.000-07:00&lt;/published>;&lt;updated>;2023-04-18T13:19:38.487-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private heatmaps&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Badih Ghazi, Staff Research Scientist, and Nachiappan Valliappan, Staff Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgS24x7QsXnFRdTdq_PJYQOr4XqJSpsFBWcb630Ujz2vOcGLL0FzXcvaDq_xwd72wCVXs3U5oNPYz0JgLJYNDrfyqf_0Fzuri6CJg2s8OzNVwsWRzzlU23NkGtre3XMlhoxf9egBUJRyEfEeIjdsRJ8m2L3gFJYCbNefqY2XNNGotPNJF6lMOM1DrlbPA/s1100/DPheatmaps.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Recently, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) has emerged as a mathematically robust notion of user privacy for data aggregation and machine learning (ML), with practical deployments including the &lt;a href=&quot;https://www.census.gov/about/policies/privacy/statistical_safeguards.html&quot;>;2022 US Census&lt;/a>; and in industry. Over the last few years, we have open-sourced &lt;a href=&quot;https://github.com/google/differential-privacy/&quot;>;libraries&lt;/a>; for privacy-preserving &lt;a href=&quot;https://developers.googleblog.com/2019/09/enabling-developers-and-organizations.html&quot;>;analytics&lt;/a>; and &lt;a href=&quot;https://blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.html&quot;>;ML&lt;/a>; and have been &lt;a href=&quot;https://ai.googleblog.com/2022/12/differential-privacy-accounting-by.html&quot;>;constantly enhancing&lt;/a>; their capabilities. Meanwhile, new algorithms have been developed by the research community for several analytic tasks involving private aggregation of data. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; One such important data aggregation method is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Heat_map&quot;>;heatmap&lt;/a>;. Heatmaps are popular for visualizing aggregated data in two or more dimensions. They are widely used in many fields including computer vision, image processing, spatial data analysis, bioinformatics, and more. Protecting the privacy of user data is critical for many applications of heatmaps. For example, heatmaps for &lt;a href=&quot;https://en.wikipedia.org/wiki/Gene_expression_profiling&quot;>;gene microdata&lt;/a>; are based on private data from individuals. Similarly, a heatmap of popular locations in a geographic area are based on user location check-ins that need to be kept private. &lt;/p>; &lt;p>; Motivated by such applications, in “&lt;a href=&quot;https://arxiv.org/pdf/2211.13454.pdf&quot;>;Differentially Private Heatmaps&lt;/a>;” (presented at &lt;a href=&quot;https://aaai-23.aaai.org/&quot;>;AAAI 2023&lt;/a>;), we describe an efficient DP algorithm for computing heatmaps with provable guarantees and evaluate it empirically. At the core of our DP algorithm for heatmaps is a solution to the basic problem of how to privately aggregate sparse input vectors (ie, input vectors with a small number of non-zero coordinates) with a small error as measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&quot;>;Earth Mover&#39;s Distance&lt;/a>; (EMD). Using a hierarchical partitioning procedure, our algorithm views each input vector, as well as the output heatmap, as a probability distribution over a number of items equal to the dimension of the data. For the problem of sparse aggregation under EMD, we give an efficient algorithm with error asymptotically close to the best possible. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Algorithm description&lt;/h2>; &lt;p>; Our algorithm works by privatizing the aggregated distribution (obtained by averaging over all user inputs), which is sufficient for computing a final heatmap that is private due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Robustness_to_post-processing&quot;>;the post-processing property of DP&lt;/a>;. This property ensures that any transformation of the output of a DP algorithm remains differentially private. Our main contribution is a new privatization algorithm for the aggregated distribution, which we will describe next. &lt;/p>; &lt;p>; The EMD measure, which is a distance-like measure of dissimilarity between two probability distributions originally proposed for computer vision tasks, is well-suited for heatmaps since it takes the underlying metric space into account and considers &quot;neighboring&quot; bins. EMD is used in a variety of applications including &lt;a href=&quot;https://ai.googleblog.com/2021/06/using-variational-transformer-networks.html&quot;>;deep learning&lt;/a>;, spatial analysis, human mobility, image retrieval, face recognition, visual tracking, shape matching, and more. &lt;/p>; &lt;p>; To achieve DP, we need to add noise to the aggregated distribution. We would also like to preserve statistics at different scales of the grid to minimize the EMD error. So, we create a hierarchical partitioning of the grid, add noise at each level, and then recombine into the final DP aggregated distribution. In particular, the algorithm has the following steps: &lt;/p>; &lt;ol>; &lt;li>;&lt;b>;Quadtree construction:&lt;/b>; Our hierarchical partitioning procedure first divides the grid into four cells, then divides each cell into four subcells; it recursively continues this process until each cell is a single pixel. This procedure creates a &lt;a href=&quot;https://en.wikipedia.org/wiki/Quadtree&quot;>;quadtree&lt;/a>; over the subcells where the root represents the entire grid and each leaf represents a pixel. The algorithm then calculates the total probability mass for each tree node (obtained by adding up the aggregated distribution&#39;s probabilities of all leaves in the subtree rooted at this node). This step is illustrated below. &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgf3kAHyEgmTCQfW3jRdmr9cVQ4j0FH2eqxwrVlx27nPaKV4n-AUzugBfqaj0mAhSbSNOoq6H_Ta9DRTfIjTx9zYOhkHrE4f65pqqhNOq-JOFRXMPIa9SGuCDTgB0PSu3saaCV1C7YQ1bukiwBveyI9Y_gAgwCHEUVIlVkIZIY_py90aAuBuR8svzXzRQ/s960/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgf3kAHyEgmTCQfW3jRdmr9cVQ4j0FH2eqxwrVlx27nPaKV4n-AUzugBfqaj0mAhSbSNOoq6H_Ta9DRTfIjTx9zYOhkHrE4f65pqqhNOq-JOFRXMPIa9SGuCDTgB0PSu3saaCV1C7YQ1bukiwBveyI9Y_gAgwCHEUVIlVkIZIY_py90aAuBuR8svzXzRQ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In the first step, we take the (non-private) aggregated distribution (&lt;b>;top left&lt;/b>;) and repeatedly divide it to create a quadtree. Then, we compute the total probability mass is each cell (&lt;b>;bottom&lt;/b>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;li>;&lt;b>;Noise addition:&lt;/b>; To each tree node&#39;s mass we then add &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_noise_mechanisms#Laplace_Mechanism&quot;>;Laplace noise&lt;/a>; calibrated to the use case. &lt;/li>;&lt;li>;&lt;b>;Truncation:&lt;/b>; To help reduce the final amount of noise in our DP aggregated distribution, the algorithm traverses the tree starting from the root and, at each level, it discards all but the top &lt;em>;w&lt;/em>; nodes with highest (noisy) masses together with their descendants. &lt;/li>;&lt;li>;&lt;b>;Reconstruction:&lt;/b>; Finally, the algorithm solves a &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_programming&quot;>;linear program&lt;/a>; to recover the aggregated distribution. This linear program is inspired by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Compressed_sensing&quot;>;sparse recovery &lt;/a>;literature where the noisy masses are viewed as (noisy) measurements of the data. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizD4PvQcqzf9UyCIF7m2DYgA14QJ7TewHebY0rZSZrpDPHTNdGPKJpNc-aqS4y8MzMQuXisMJxy8nXRdgX7frNu9Xqh9G-YhZMHNet4Df1hz_VM17v3TTFHUCPEzIJH1_WYZ9SYrsUocoeZpqZIqOlq8LGUlyCmPBNybnyrlVBIPL8dBvxI6Aw2LQ33A/s960/image5.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizD4PvQcqzf9UyCIF7m2DYgA14QJ7TewHebY0rZSZrpDPHTNdGPKJpNc-aqS4y8MzMQuXisMJxy8nXRdgX7frNu9Xqh9G-YhZMHNet4Df1hz_VM17v3TTFHUCPEzIJH1_WYZ9SYrsUocoeZpqZIqOlq8LGUlyCmPBNybnyrlVBIPL8dBvxI6Aw2LQ33A/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In step 2, noise is added to each cell&#39;s probability mass. Then in step 3, only top-w cells are kept (&lt;b>;green&lt;/b>;) whereas the remaining cells are truncated (&lt;b>;red&lt;/b>;). Finally, in the last step, we write a linear program on these top cells to reconstruct the aggregation distribution, which is now differentially private.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experimental results&lt;/h2>; &lt;p>; We evaluate the performance of our algorithm in two different domains: real-world location check-in data and image saliency data. We consider as a baseline the ubiquitous &lt;a href=&quot;https://doi.org/10.1007/11681878_14&quot;>;Laplace mechanism&lt;/a>;, where we add Laplace noise to each cell, zero out any negative cells, and produce the heatmap from this noisy aggregate. We also consider a “thresholding” variant of this baseline that is more suited to sparse data: only keep top &lt;em>;t&lt;/em>;% of the cell values (based on the probability mass in each cell) after noising while zeroing out the rest. To evaluate the quality of an output heatmap compared to the true heatmap, we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;>;Pearson coefficient&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;KL-divergence&lt;/a>;, and EMD. Note that when the heatmaps are more similar, the first metric increases but the latter two decrease. &lt;/p>; &lt;p>; The locations dataset is obtained by combining two datasets, &lt;a href=&quot;http://snap.stanford.edu/data/loc-Gowalla.html&quot;>;Gowalla&lt;/a>; and &lt;a href=&quot;http://snap.stanford.edu/data/loc-Brightkite.html&quot;>;Brightkite&lt;/a>;, both of which contain check-ins by users of location-based social networks. We pre-processed this dataset to consider only check-ins in the continental US resulting in a final dataset consisting of ~500,000 check-ins by ~20,000 users. Considering the top cells (from an initial partitioning of the entire space into a 300 x 300 grid) that have check-ins from at least 200 unique users, we partition each such cell into subgrids with a resolution of ∆ × ∆ and assign each check-in to one of these subgrids. &lt;/p>; &lt;p>; In the first set of experiments, we fix ∆ = 256. We test the performance of our algorithm for different values of ε (the &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Definition_of_%CE%B5-differential_privacy&quot;>;privacy parameter&lt;/a>;, where smaller ε means stronger DP guarantees), ranging from 0.1 to 10, by running our algorithms together with the baseline and its variants on all cells, randomly sampling a set of 200 users in each trial, and then computing the distance metrics between the true heatmap and the DP heatmap. The average of these metrics is presented below. Our algorithm (the red line) performs better than all versions of the baseline across all metrics, with improvements that are especially significant when ε is not too large or small (ie, 0.2 ≤ ε ≤ 5). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaJ4ZoMvzmgh-1RtxelE3BPryTE_OsHzQBzO4JAA6gli0XO0AZa_yTocYwqQXP1PY5Jt7nF77PF6j6LqVCvxAhAljvfKfZHb-9WxqYwTLepWEVrnM7bqdE__GBuSc35jGil5G5PrVs0qEUuWEgA7lQy19ok5cmrgKvbezrBMB197dthwGZ9iZkf0tolQ/s1999/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;647&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaJ4ZoMvzmgh-1RtxelE3BPryTE_OsHzQBzO4JAA6gli0XO0AZa_yTocYwqQXP1PY5Jt7nF77PF6j6LqVCvxAhAljvfKfZHb-9WxqYwTLepWEVrnM7bqdE__GBuSc35jGil5G5PrVs0qEUuWEgA7lQy19ok5cmrgKvbezrBMB197dthwGZ9iZkf0tolQ/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Metrics averaged over 60 runs when varying ε for the location dataset. Shaded areas indicate 95% confidence interval.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Next, we study the effect of varying the number &lt;em>;n&lt;/em>; of users. By fixing a single cell (with &amp;gt; 500 users) and ε, we vary &lt;em>;n&lt;/em>; from 50 to 500 users. As predicted by theory, our algorithms and the baseline perform better as &lt;em>;n&lt;/em>; increases. However, the behavior of the thresholding variants of the baseline are less predictable. &lt;/p>; &lt;p>; We also run another experiment where we fix a single cell and ε, and vary the resolution ∆ from 64 to 256. In agreement with theory, our algorithm&#39;s performance remains nearly constant for the entire range of ∆. However, the baseline suffers across all metrics as ∆ increases while the thresholding variants occasionally improve as ∆ increases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlu-390vSrt7mMXlCdyVTKEahZohQKC7U_P7C7poSvE23bODHZLlj9CtjeF3kvF8UxCJDf7O8s9TcT49QH4vbmRhc7An8b8GjhBVMh1tVPM7upoS9dYqtGoPruom5Cd6uKUch-Hhs7fpYdSx17dfF1cYYmDCBFj1U8krV-OCmkwZT-INKWAPrmmWCBcw/s1999/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlu-390vSrt7mMXlCdyVTKEahZohQKC7U_P7C7poSvE23bODHZLlj9CtjeF3kvF8UxCJDf7O8s9TcT49QH4vbmRhc7An8b8GjhBVMh1tVPM7upoS9dYqtGoPruom5Cd6uKUch-Hhs7fpYdSx17dfF1cYYmDCBFj1U8krV-OCmkwZT-INKWAPrmmWCBcw/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Effect of the number of users and grid resolution on EMD.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also experiment on the &lt;a href=&quot;http://salicon.net/&quot;>;Salicon image saliency dataset (SALICON&lt;/a>;). This dataset is a collection of &lt;a href=&quot;https://en.wikipedia.org/wiki/Saliency_map&quot;>;saliency annotations&lt;/a>; on the Microsoft &lt;a href=&quot;https://cocodataset.org/#home&quot;>;Common Objects in Context&lt;/a>; image database. We downsized the images to a fixed resolution of 320 × 240 and each [user, image] pair consists of a sequence of coordinates in the image where the user looked. We repeat the experiments described previously on 38 randomly sampled images (with ≥ 50 users each) from SALICON. As we can see from the examples below, the heatmap obtained by our algorithm is very close to the ground truth. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGgnVoDZw3huIVPMGOAeOHGggcZcnd2IWcrlSyMnDS7RixKyBV12dtGF3ofDR8mxYy6CEElatdAyNIcxkyk91jZ2c-7_kSnnZ1omPLuUwWjnl019yQdj1NlmCf0844xwNeFH9uvTbwnmjOEpUszF8nztKpt_TBMiZcaX4f3_rIBZDpxJ1uCYLyO1JJGQ/s930/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;930&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGgnVoDZw3huIVPMGOAeOHGggcZcnd2IWcrlSyMnDS7RixKyBV12dtGF3ofDR8mxYy6CEElatdAyNIcxkyk91jZ2c-7_kSnnZ1omPLuUwWjnl019yQdj1NlmCf0844xwNeFH9uvTbwnmjOEpUszF8nztKpt_TBMiZcaX4f3_rIBZDpxJ1uCYLyO1JJGQ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example visualization of different algorithms for two different natural images from SALICON for ε = 10 and &lt;em>;n&lt;/em>; = 50 users. The algorithms from left to right are: original heatmap (no privacy), baseline, and ours.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Additional experimental results, including those on other datasets, metrics, privacy parameters and DP models, can be found in the paper. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We presented a privatization algorithm for sparse distribution aggregation under the EMD metric, which in turn yields an algorithm for producing privacy-preserving heatmaps. Our algorithm extends naturally to distributed models that can implement the Laplace mechanism, including the secure aggregation model and the &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3132747.3132769&quot;>;shuffle model&lt;/a>;. This does not apply to the more stringent &lt;a href=&quot;https://en.wikipedia.org/wiki/Local_differential_privacy&quot;>;local DP model&lt;/a>;, and it remains an interesting open question to devise practical local DP heatmap/EMD aggregation algorithms for “moderate” number of users and privacy parameters. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;This work was done jointly with Junfeng He, Kai Kohlhoff, Ravi Kumar, Pasin Manurangsi, and Vidhya Navalpakkam.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://ai.googleblog.com/feeds/331432973525797076/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/differentially-private-heatmaps.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/331432973525797076&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/331432973525797076&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://ai.googleblog.com/2023/04/differentially-private-heatmaps.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private heatmaps&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgS24x7QsXnFRdTdq_PJYQOr4XqJSpsFBWcb630Ujz2vOcGLL0FzXcvaDq_xwd72wCVXs3U5oNPYz0JgLJYNDrfyqf_0Fzuri6CJg2s8OzNVwsWRzzlU23NkGtre3XMlhoxf9egBUJRyEfEeIjdsRJ8m2L3gFJYCbNefqY2XNNGotPNJF6lMOM1DrlbPA/s72-c/DPheatmaps.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;