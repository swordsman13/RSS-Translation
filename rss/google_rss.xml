<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2024-01-31T10:09:39.992-08:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="conference"></category><category term="Machine Perception"></category><category term="Natural Language Understanding"></category><category term="TensorFlow"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Health"></category><category term="Robotics"></category><category term="AI"></category><category term="Algorithms"></category><category term="CVPR"></category><category term="NLP"></category><category term="Quantum Computing"></category><category term="Multimodal Learning"></category><category term="Speech"></category><category term="Research Awards"></category><category term="Computational Photography"></category><category term="Machine Intelligence"></category><category term="AI for Social Good"></category><category term="On-device Learning"></category><category term="Security and Privacy"></category><category term="Computer Science"></category><category term="HCI"></category><category term="MOOC"></category><category term="Quantum AI"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="optimization"></category><category term="accessibility"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="AutoML"></category><category term="Hardware"></category><category term="NeurIPS"></category><category term="ACL"></category><category term="Audio"></category><category term="TPU"></category><category term="Android"></category><category term="EMNLP"></category><category term="ICML"></category><category term="ML"></category><category term="Physics"></category><category term="video"></category><category term="Awards"></category><category term="ML Fairness"></category><category term="Responsible AI"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="Google Maps"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="Video Analysis"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="RAI-HCT Highlights"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="ph.d. fellowship"></category><category term="Augmented Reality"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="ICCV"></category><category term="Large Language Models"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Translate"></category><category term="Unsupervised Learning"></category><category term="grants"></category><category term="market algorithms"></category><category term="Faculty Summit"></category><category term="Google Cloud Platform"></category><category term="Google Genomics"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Differential Privacy"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="ads"></category><category term="renewable energy"></category><category term="Climate"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Kaggle"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Virtual Reality"></category><category term="Year in Review"></category><category term="schema.org"></category><category term="API"></category><category term="Africa"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="NAACL"></category><category term="Networks"></category><category term="Style Transfer"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Generative AI"></category><category term="Genomics"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Graph"></category><category term="Graphs"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="Weather"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="CHI"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google Cloud"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="High-Performance Computing"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1329&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-5144906729109253495&lt;/id>;&lt;发布>;2024-01-26T11:56:00.000-08:00&lt;/发布>;&lt;更新>;2024-01- 26T11:56:23.553-08:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category schema=&quot;http: //www.blogger.com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>; &lt;/category>;&lt;title type=&quot;text&quot;>;混合输入矩阵乘法性能优化&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Manish Gupta，资深软件工程师， Google 研究&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6 k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 人工智能驱动的技术正在融入我们的日常生活，有可能增强我们获取知识的能力并提高我们的整体生产力。这些应用程序的支柱在于大型语言模型（LLM）。 LLM 需要占用大量内存，通常需要专门的硬件加速器来高效交付&lt;a href=&quot;https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on -tpu-v5e&quot;>;数十亿亿次浮点运算&lt;/a>;的计算能力。这篇博文展示了我们如何通过更有效地利用内存来开始解决计算挑战。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; LLM 的大部分内存和计算均由 &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;>; 消耗&lt;a href=&quot;https://arxiv.org/pdf/2006.16668.pdf&quot;>;矩阵乘法&lt;/a>;运算中的权重&lt;/a>;。使用较窄的&lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Primitive_data_type&quot;>;数据类型&lt;/a>;&lt;/em>;可以减少内存消耗。例如，以 8 位&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_(computer_science)&quot;>;整数&lt;/a>;（即 U8 或 S8）数据类型存储权重会减少内存相对于&lt;a href=&quot;https://en.wikipedia.org/wiki/Single- precision_floating-point_format&quot;>;单精度&lt;/a>; (F32) 占用空间为 4 倍，相对于 &lt;a href=&quot; 占用空间为 2 倍https://en.wikipedia.org/wiki/Half- precision_floating-point_format&quot;>;半精度&lt;/a>; (F16) 或 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bfloat16_floating-point_format &quot;>;bfloat16&lt;/a>; (BF16)。此外，&lt;a href=&quot;https://arxiv.org/pdf/2206.01861.pdf&quot;>;之前的工作&lt;/a>;表明，LLM 模型在 S8 和 &lt;em>; 中运行带有&lt;em>;权重&lt;/em>;的矩阵乘法>;F16 中的输入（保留用户输入的较高精度）是一种提高效率的有效方法，同时在精度方面进行了可接受的权衡。这种技术被称为&lt;em>;仅权量化&lt;/em>;，并且需要有效地实现与&lt;em>;混合输入&lt;/em>;的矩阵乘法，例如，半精度输入乘以8位整数。包括 GPU 在内的硬件加速器支持一组固定的数据类型，因此混合输入矩阵乘法需要软件转换来映射到硬件运算。 &lt;/p>; &lt;p>; 为此，在本博客中，我们重点关注将混合输入矩阵乘法映射到 &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-深度/&quot;>;NVIDIA Ampere 架构&lt;/a>;。我们提出了解决数据类型转换和布局一致性的软件技术，以将混合输入矩阵乘法有效地映射到硬件支持的数据类型和布局上。我们的结果表明，软件中额外工作的开销是最小的，并且使性能接近峰值硬件能力。此处描述的软件技术已在开源 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/pull/1084&quot;>;NVIDIA/CUTLASS&lt;/a>; 存储库中发布。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXgh Ez-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999 /image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1159&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm _K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;具有各种数据类型格式的 175B 参数 LLM 模型的内存占用。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table >; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;矩阵乘法累加运算&lt;/h2>; &lt;p>; 现代人工智能硬件加速器，例如&lt;a href= &quot;https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works&quot;>;Google 的 TPU&lt;/a>; 和 &lt;a href=&quot;https://www.nvidia.com/en-us/ data-center/tensor-cores/&quot;>;NVIDIA 的 GPU&lt;/a>; 通过针对 Tensor Core 在硬件中本地乘法矩阵，Tensor Core 是用于加速矩阵运算的专用处理元素，特别是对于 AI 工作负载。在本博客中，我们重点关注 NVIDIA Ampere Tensor Core，它提供&lt;em>;矩阵乘法累加&lt;/em>; (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel- thread-execution/index.html#warp-level-matrix-instructions-mma&quot;>;mma&lt;/a>;&lt;/code>;）操作。在博客的其余部分中，对 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 的引用适用于 Ampere Tensor Core。 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算的两个输入矩阵（称为操作数）支持的数据类型、形状和数据布局在硬件中是固定的。这意味着通过将问题平铺到硬件支持的数据类型、形状和布局上，可以在软件中实现各种数据类型和更大形状的矩阵乘法。 &lt;/p>; &lt;p>; Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算通过指定两个输入矩阵来定义（例如，&lt;em>;A&lt; /em>; &amp;amp; &lt;em>;B&lt;/em>;，如下所示）以生成结果矩阵 &lt;em>;C&lt;/em>;。 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作本身支持混合精度。 &lt;em>;&lt;a href=&quot;https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/&quot;>;混合精度张量核心&lt;/a>;&lt;/em>;允许混合输入（&lt; em>;A&lt;/em>; 和 &lt;em>;B&lt;/em>;）数据类型与结果（&lt;em>;C&lt;/em>;）数据类型。相比之下，混合输入矩阵乘法涉及混合输入数据类型，硬件不支持，因此需要在软件中实现。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxn VdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTISZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;668&quot; data-original-width=&quot;1039&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ 68Yr5wve9uLTISZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;M×N-by-K 对 M×K 的输入矩阵 A 和 K×N 的矩阵 B 进行张量核心运算，产生输出矩阵 C M-by-N。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;混合的挑战-输入矩阵乘法&lt;/h2>; &lt;p>; 为了简化讨论，我们只讨论混合输入矩阵乘法的一个具体示例：F16 表示用户输入，U8 表示模型权重（写为 F16 * U8）。这里描述的技术适用于混合输入数据类型的各种组合。 &lt;/p>; &lt;p>; GPU 程序员可以访问&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy&quot;>;内存层次结构&lt;/a>;，包括全局内存、共享内存、寄存器，按照容量递减、速度递增的顺序排列。 NVIDIA Ampere Tensor Core &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算消耗寄存器中的输入矩阵。此外，输入和输出矩阵需要符合称为&lt;em>;扭曲&lt;/em>;的一组32个线程内的数据布局。对于 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作，扭曲中支持的数据类型&lt;em>;和&lt;/em>;布局是固定的，因此要实现混合-高效输入乘法，需要解决软件中数据类型转换和布局一致性的挑战。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;数据类型转换&lt;/h3>; &lt;p>; &lt;span style=&quot;color: #54863f; &quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算需要两个具有相同数据类型的输入矩阵。因此，混合输入矩阵乘法（其中一个操作数存储在全局内存中的 U8 中，其他操作数存储在 F16 中）需要从 U8 到 F16 的数据类型转换。该转换将为 F16 带来两个操作数，将&lt;em>;混合输入&lt;/em>;矩阵乘法映射到硬件支持的&lt;em>;混合精度&lt;/em>;张量核心。鉴于权重数量很大，因此存在大量此类操作，我们的技术展示了如何减少其延迟并提高性能。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;布局一致性&lt;/h3>; &lt;p>; &lt;span style=&quot;color: #54863f;&quot; >;&lt;code>;mma&lt;/code>;&lt;/span>; 操作还要求扭曲寄存器内的两个输入矩阵的布局符合硬件规范。混合输入矩阵乘法（F16 * U8）中U8数据类型的输入矩阵&lt;em>;B&lt;/em>;的布局需要符合转换后的F16数据类型。这称为布局一致性，需要在软件中实现。 &lt;/p>; &lt;p>; 下图显示了一个 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 运算，消耗了矩阵 &lt;em>;A&lt;/em>; 和矩阵 &lt; em>;B&lt;/em>; 从寄存器生成寄存器中的矩阵 &lt;em>;C&lt;/em>;，分布在一个扭曲上。线程 &lt;em>;T0&lt;/em>; 突出显示并放大，以显示权重矩阵 &lt;em>;B&lt;/em>; 经过数据类型转换，并且需要布局一致性才能映射到硬件支持的 Tensor Core手术。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-X M6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1240&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi 27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;软件中的混合输入 (F32 = F16 * U8) 操作到硬件中原生支持的扭曲级张量核心 (F32 = F16 * F16) 的映射。 （原始图来源&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;>;开发 CUDA 内核以将 Tensor Core 推向 NVIDIA A100 的绝对极限&lt; /a>;.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;应对挑战的软件策略&lt;/h2>; &lt;p>; 典型的数据类型转换涉及 32 位寄存器上的一系列操作，如下所示。每个矩形块代表一个寄存器，相邻的文本是操作。整个序列显示了从 4xU8 到 2x(2xF16) 的转换。该序列大约涉及 10 个操作。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3 KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;947&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q 4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760&quot;>;NumericArrayConvertor&lt;/a>;&lt; /code>; 在 32 位寄存器中从 4xU8 到 2x(2xF16)。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 实现布局一致性的方法有很多。现有的两个解决方案是： &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;窄位宽共享内存加载&lt;/em>;：在这种方法中，线程发出窄位宽内存加载，将 U8 数据从共享内存移动到寄存器。这会产生&lt;em>;两个&lt;/em>; 32 位寄存器，每个寄存器包含 2xF16 值（如上所示矩阵 &lt;em>;B&lt;/em>; 的线程 &lt;em>;T0&lt;/em>;）。较窄的共享内存负载直接实现寄存器的布局一致性，而不需要任何洗牌；但是，它没有利用全部共享内存带宽。 &lt;/li>;&lt;li>;&lt;em>;全局内存中的预处理&lt;/em>;：&lt;a href=&quot;https://arxiv.org/pdf/2211.10017.pdf&quot;>;替代策略&lt;/a>;涉及重新排列全局内存中的数据（内存层次结构中共享内存的上一级） &lt;/a>;），允许更广泛的共享内存负载。这种方法最大限度地提高了共享内存带宽利用率，并确保数据以一致的布局直接加载到寄存器中。虽然重新排列过程可以在 LLM 部署之前离线执行，确保不会影响应用程序性能，但它引入了一个额外的、重要的特定于硬件的预处理步骤，需要额外的程序来重新排列数据。 &lt;a href=&quot;https://github.com/NVIDIA/FasterTransformer&quot;>;NVIDIA/FasterTransformer&lt;/a>; 采用这种方法来有效解决布局一致性挑战。 &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;优化软件策略&lt;/h2>; &lt;p>;进一步优化并降低开销为了实现数据类型转换和布局一致性，我们实现了 &lt;code>;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;FastNumericArrayConvertor&lt;/a >;&lt;/code>; 和 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120&quot;>;FragmentShuffler&lt;/a>;分别。 &lt;/p>;&lt;p>; &lt;code>;FastNumericArrayConvertor&lt;/code>; 在 32 位寄存器中的 4xU8 上运行，无需解包各个 1xU8 值。此外，它使用成本较低的算术运算，从而减少了指令数量并提高了转换速度。 &lt;/p>; &lt;p>; &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot;>;U8-to-F16&lt;/的转换顺序a>; 如下所示。这些操作使用打包的 32b 寄存器，避免显式解包和打包。 &lt;code>;FastNumericArrayConvertor&lt;/code>; 使用 &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions- prmt&quot;>;permute byte&lt;/a>;&lt;/code>; 将 4xU8 的字节重新排列到两个寄存器中。此外，&lt;code>;FastNumericArrayConvertor&lt;/code>; 不使用昂贵的整数到浮点转换指令，而是采用向量化运算来获取包含 2x(2xF16) 值的&lt;em>;两个&lt;/em>; 32 位寄存器中的打包结果。 U8 到 F16 的 &lt;code>;FastNumericArrayConvertor&lt;/code>; 大约使用六次操作，相对于上面所示的方法减少了 1.6 倍。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGU cpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png&quot; imageanchor =&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;733&quot; data-original-width=&quot;1392&quot; src=&quot;https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1 yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr -caption&quot; style=&quot;text-align: center;&quot;>;&lt;code>;FastNumericArrayConvertor&lt;/code>; 利用 &lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index .html#data-movement-and-conversion-instructions-prmt&quot;>;置换字节&lt;/a>;&lt;/code>;和压缩算术，减少数据类型转换中的指令数量。&lt;/td>;&lt;/tr>;&lt; /tbody>;&lt;/table>; &lt;p>; &lt;code>;FragmentShuffler&lt;/code>; 通过以允​​许使用更宽位宽的加载操作的方式混洗数据来处理布局一致性，从而提高共享内存带宽利用率并减少操作总数。 &lt;/p>; &lt;p>; NVIDIA Ampere 架构提供加载矩阵指令 (&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-矩阵指令-ldmatrix&quot;>;ldmatrix&lt;/a>;&lt;/code>;）。 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; 是一个 warp 级别的操作，其中一个 warp 的 32 个线程将数据从共享内存移动到 &lt;em >;形状&lt;/em>;和&lt;em>;布局&lt;/em>;，&lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>;矩阵&lt;em>;A&lt;/em>;和&lt;em>;B&lt;/em>;消耗。使用&lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>;&lt;em>;减少&lt;/em>;加载指令的数量并&lt;em>;增加&lt;/em>;内存带宽利用率。由于 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;ldmatrix&lt;/code>;&lt;/span>; 指令将 U8 数据移至寄存器，因此加载后的布局符合 U8*U8 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作，而不是 F16*F16 &lt;span style=&quot;color: #54863f;&quot;>;&lt;code>;mma&lt;/code>;&lt;/span>; 操作。我们实现了 FragmentShuffler，以使用 shuffle 重新排列寄存器内的数据（&lt;code>;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html #warp-shuffle-functions&quot;>;shfl.sync&lt;/a>;)&lt;/code>; 操作以实现布局一致性。 &lt;/p>;&lt;p>; 这项工作最重要的贡献是通过寄存器洗牌实现布局一致性，避免全局内存中的离线预处理或较窄位宽的共享内存负载。此外，我们还提供了 &lt;code>;FastNumericArrayConvertor&lt;/code>; 的实现，涵盖了 &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514&quot; 的数据类型转换>;U8 到 F16&lt;/a>;、&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448&quot;>;S8 到 F16&lt;/ a>;、&lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546&quot;>;U8 到 BF16&lt;/a>; 和 &lt;a href= “https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588&quot;>;S8 到 BF16&lt;/a>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;性能结果&lt;/h2>; &lt;p>; 我们测量了 &lt; 的八种混合输入变体的性能em>;我们的方法&lt;/em>;（如下图蓝色和红色所示；改变矩阵&lt;em>;A&lt;/em>;和&lt;em>;B&lt;/em>;的数据类型）和两个&lt;em>;混合精度&lt;/em>; em>; NVIDIA A100 SXM 芯片上的数据类型（以绿色显示）。性能结果以 &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPS&lt;/a>; 为单位（越高越好）。值得注意的是，前八个矩阵乘法相对于后两个矩阵乘法需要额外的操作，因为混合精度变体直接针对硬件加速的 Tensor Core 操作，不需要数据类型转换和布局一致性。即便如此，我们的方法证明了混合输入矩阵乘法性能仅略低于或与混合精度相当。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4 fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1180&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6 IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;NVIDIA A100 40GB SMX4 芯片上计算受限矩阵问题形状的混合输入矩阵乘法性能&lt;code>;m=3456, n=4096, k=2048。&lt;/ code>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p >; &lt;em>;我们想提及几位通过技术头脑风暴和改进博客文章做出贡献的人，包括 Quentin Colombet、Jacques Pienaar、Allie Culp、Calin Cascaval、Ashish Gondimalla、Matt Walsh、Marek Kolodziej 和 Aman Bhatia。我们要感谢 NVIDIA 合作伙伴 Rawn Henry、Pradeep Ramani、Vijay Thakkar、Hai Cheng Wu、Andrew Kerr、Matthew Nicely 和 Vartika Singh。&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http:// /blog.research.google/feeds/5144906729109253495/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research. google/2024/01/mixed-input-matrix-multiplication.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www .blogger.com/feeds/8474926331452026626/posts/default/5144906729109253495&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/ posts/default/5144906729109253495&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html &quot; rel=&quot;alternate&quot; title=&quot;混合输入矩阵乘法性能优化&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com /profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src= &quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https:// /blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG 2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png”宽度=“72”xmlns：媒体=“http://search.yahoo.com/mrss/” >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076&lt;/id>;&lt;已发布>;2024-01-23T14:27:00.000-08:00&lt;/已发布>;&lt;更新>;2024-01-23T14:27:09.785-08:00&lt;/更新>;&lt;category schema=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;深度学习&quot;>;&lt;/category>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Graphs&quot;>;&lt;/category>;&lt; title type=&quot;text&quot;>;Exphormer：图结构数据的缩放转换器&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Ameya Velingker，Google 研究院研究科学家Balaji Venkatachalam，Google 软件工程师&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRn N8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif&quot;样式=“显示：无；” />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)&quot;>;图&lt;/a>;，其中对象及其关系表示为节点（或顶点）和边节点对之间的连接（或链接）在计算和机器学习 (ML) 中无处不在。例如，社交网络、道路网络以及分子结构和相互作用都是底层数据集具有自然图结构的领域。机器学习可用于学习节点、边或整个图的属性。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 图学习的常见方法是&lt;a href=&quot;https://distill.pub/2021/gnn-intro/&quot;>;图神经网络网络（GNN），通过对节点、边缘和全局属性应用可优化的转换来对图数据进行操作。最典型的 GNN 类别通过 &lt;a href=&quot;https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2 进行操作&quot;>;消息传递框架，其中每一层将节点的表示与其直接邻居的表示聚合。 &lt;/p>; &lt;p>; 最近，&lt;a href=&quot;https://arxiv.org/abs/2012.09699&quot;>;图转换器模型&lt;/a>;已成为消息传递 GNN 的流行替代方案。这些模型建立在自然语言处理 (NLP) 领域&lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)&quot;>;Transformer 架构&lt;/a>;的成功之上，使它们适应图形- 结构化数据。图转换器中的注意力机制可以通过交互图来建模，其中边代表相互关注的节点对。与消息传递架构不同，图转换器具有与输入图分离的交互图。典型的交互图是一个完整的图，这意味着一个完整的注意力机制，可以模拟所有节点对之间的直接交互。&lt;em>; &lt;/em>;然而，这会产生二次计算和内存瓶颈，限制了图转换器对最多具有数千个节点的小图上的数据集的适用性。使图转换器可扩展被认为是该领域最重要的研究方向之一（参见&lt;a href=&quot;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&quot; >;这里是第一个开放问题&lt;/a>;）。 &lt;/p>; &lt;p>; 一种自然的补救措施是使用边数较少的&lt;em>;稀疏&lt;/em>;交互图。 &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3530811&quot;>;人们提出了许多稀疏且高效的变换器&lt;/a>;来消除序列的二次瓶颈，但是，它们通常不会扩展到有原则地绘制图表。 &lt;/p>; &lt;p>; 在“&lt;a href=&quot;https://arxiv.org/abs/2303.06147&quot;>;Exphormer：图的稀疏变换器&lt;/a>;”中，介绍于 &lt;a href=&quot;https:// icml.cc/Conferences/2023/Dates&quot;>;ICML 2023&lt;/a>;，我们通过引入专为图数据设计的变压器稀疏注意力框架来解决可扩展性挑战。 Exphormer 框架利用了扩展图，这是&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_graph_theory&quot;>;谱图理论&lt;/a>;中的一个强大工具，并且能够在以下方面取得强有力的实证结果：各种各样的数据集。我们的 Exphormer 实现现已在 &lt;a href=&quot;https://github.com/hamed1375/Exphormer&quot;>;GitHub&lt;/a>; 上提供。 &lt;/p>; &lt;br />; &lt;h2>;扩展图&lt;/h2>; &lt;p>; Exphormer 核心的一个关键思想是使用 &lt;a href=&quot;https://en.wikipedia.org/wiki/Expander_graph &quot;>;扩展图&lt;/a>;，它们是稀疏但连接良好的图，具有一些有用的属性 - 1）图的矩阵表示具有与完整图类似的线性代数属性，2）它们表现出快速混合随机游走，即从任何起始节点开始的随机游走中的少量步骤足以确保收敛到图的节点上的“稳定”分布。扩展器已应用于不同领域，例如算法、伪随机性、复杂性理论和纠错码。 &lt;/p>; &lt;p>;一个通用类的扩展器图是&lt;em>; d &lt;/em>;  - 常规扩展器，其中每个节点都有&lt;em>; d &lt;/em>;的边缘（即，每个节点都有度&lt; em>; d &lt;/em>;）。扩展器图的质量通过其&lt;em>;频谱差距&lt;/em>;来衡量，&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/adjacency_matrix&quot;>; uschacency Matrix &lt;/a >;（该图的矩阵表示，其中行和列由节点和条目索引索引表示节点对是否通过边缘连接）。那些最大化光谱差距的人称为&lt;a href=&quot;https://en.wikipedia.org/wiki/ramanujan_graph&quot;>; ramanujan图&lt;/a>;  - 他们实现了&lt;em>; d &lt;/em>;  -   -   - 2*√（&lt;em>; d &lt;/em>; -1），这在&lt;em>; d &lt;/em>;  - 规范图中实质上是最好的。多年来，针对&lt;em>; d &lt;/em>;的各种值，已经提出了许多Ramanujan图的确定性和随机结构。我们使用&lt;a href=&quot;https://arxiv.org/abs/cs/0405020&quot;>; friedman的随机扩展器构建&lt;/a>;，它产生了近ramanujan图。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif&quot; style =“边距 - 左：自动;边缘右：自动;”>; &lt;img border =“ 0” data-Original-height =“ 843” data-Original-width =“ 800” height =“ 320” src =“ https ://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif&quot; width=&quot;304&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt; td class =“ tr-caption”样式=“ text-align：center;”>; &lt;span ID =“ docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968”>; &lt;span face =“ serif“ style =”字体大小：10pt;字体样式：斜体；字体变化 - 替代：正常；字体变化 - 东亚：正常；字体变体数字：正常；字体变化位置：正常；垂直平衡：基线；白色空间溢出：保存；“>;扩展器图是Exponmer的核心。良好的扩展器稀疏，但显示了随机步行的快速混合，使其全局连通性适用于图形变压器模型中的相互作用图。&lt;/span span。 >; &lt;/span>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;解释器替换了具有稀疏&lt;em>; d &lt;/em的边缘的标准变压器的密集，完全连接的相互作用图>;  - 常规扩展器图。直观地，频谱近似和混合特性允许远距离节点在一个图形变压器体系结构中堆叠多个注意力层后彼此相互通信，即使节点可能无法直接彼此聚集。此外，通过确保&lt;em>; d &lt;/em>;是恒定的（与节点数的大小无关），我们在所得的相互作用图中获得了线性的边数。&lt;/p>; &lt;br />; &lt;h2 >; expostormer：构建稀疏交互图&lt;/h2>; &lt;p>;解释器将扩展器边缘与输入图和虚拟节点结合在一起。更具体地说，Expostormer的稀疏注意机制构建了一个由三种类型的边缘组成的交互图：&lt;/p>; &lt;ul>; &lt;li>;来自输入图的边缘（&lt;em>; local Coative &lt;/em>;）&lt;/em>;）&lt;/em>;）&lt;/li>; &lt;li>;从恒定度扩展器图（&lt;em>;扩展器注意&lt;/em>;）&lt;/li>; &lt;li>;从每个节点到一组虚拟节点的边缘（&lt;em>;全局注意&lt;/em>;） ）&lt;/li>; &lt;/ul>; &lt;table align =“中心” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container”样式=“ margin-Left：auto; auto; margin-right：auto; auto; &quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq- hdayx8/s800/image1.gif“ style =”边距 - 左：自动; margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 430” data-original witth =“ 800” src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt; tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;span ID =“ docs-internal-guid-ac11d16d-ac11d16d-7fff-62da-cf18-7ba830f6777d3”>; &lt;span face = &lt;span face =“ ，sans-serif“ style =”字体大小：10pt;字体样式：斜体；字体变化 - 替代：正常；字体变化 - 东亚：正常；字体变体数字：正常；字体变化位置：正常；垂直平衡：基线；白空间偏离：保存;“>; exposerveer通过组合三种类型的边缘来构建相互作用图。结果图具有良好的连通性属性，并保留了输入数据集图的电感偏置，同时仍然保持稀疏。&lt;/span>; &lt;/span>; &lt;/span>; &lt;/跨度>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;每个组件都具有特定目的：输入图的边缘保留了输入图结构的归纳偏置同时，膨胀边缘允许良好的全局连接性和随机步行混合属性（在频谱上近似完整的图的边缘较少）。最后，虚拟节点用作全局“内存下沉”，可以直接与每个人通信节点。尽管这会导致每个虚拟节点的其他边缘等于输入图中的节点的数量，但结果图仍然很稀疏。扩展器图的程度和虚拟节点的数量是超级参数，可以调整质量以调整质量指标。 &lt;/p>; &lt;p>;此外，由于我们使用恒定度的扩展器图和少量恒定数量的虚拟节点来全局注意，因此所得的稀疏注意机制在原始输入图的大小上是线性的，即在节点和边缘总数的顺序上模拟许多直接相互作用。 &lt;/p>; &lt;p>;我们还表明，解释者与致密变压器一样表达，并服从通用近似特性。特别是，当稀疏的注意力图被自循环增强（将节点连接到自身的边缘）时，它可以普遍近似连续函数[&lt;a href=&quot;https://arxiv.org/abs/1912.10077QU &lt;/a>;，&lt;a href=&quot;https://arxiv.org/abs/2006.04862&quot;>; 2 &lt;/a>;]。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/>;序列的注意方法。也许在概念上与我们的方法最相似的架构是&lt;a href=&quot;https://blog.research.google/2021/03/constructing-transformers-for-longer.html&quot;>; bigbird &lt;/a>;通过组合不同的组件来图。 BigBird还使用虚拟节点，但是与Expymer不同，它使用窗口的关注和随机关注，来自&lt;a href =” ％a9nyi_model“>;erdős-rényi&lt;/a>;剩余组件的随机图模型。 &lt;/p>; &lt;p>; bigbird中的窗户注意力介绍了一个序列的令牌周围的令牌 - 可以将Exponmer中的当地邻里注意力视为窗口对图的关注的概括。 &lt;/p>; &lt;p>; &lt;em>; n &lt;/em>; nodes，&lt;em>; g（n，p）&lt;em>; g（n，p）&lt;/em>;上的erdős-rényi图，该图与概率&lt;em>; p独立连接每个节点&lt;/em>;，还可以充当适当高&lt;em>; p &lt;/em>;的扩展器图。但是，需要一个超线性数（ω（&lt;em>; n &lt;/em>; log &lt;em>; n &lt;/em>;）），以确保连接Erdős-rényi图，更不用说良好的扩展器了。另一方面，在解释器中使用的扩展器仅具有&lt;em>;线性&lt;/em>;的边缘数。 &lt;/p>; &lt;br />; &lt;h2>;实验结果&lt;/h2>; &lt;p>;早期的作品显示了在数据集中使用完整的基于图形变压器的模型，该模型的图形大小高达5,000个节点。为了评估Expomper的表现，我们以著名的&lt;a href=&quot;https://github.com/rampasek/graphgps&quot;>; GraphGps Framework &lt;/a>; [&lt;a href =“ https://arxiv.org/ ABS/2205.12454“>; 3 &lt;/a>;]，它结合了消息传递和图形变压器，并在许多数据集中实现最先进的性能。我们表明，用GraphGPS框架中图形注意力组件的Exponmer代替密集的注意力，使人们可以实现具有可比或更好的性能的模型，通常具有更少的可训练参数。 &lt;/p>; &lt;p>;此外，Expontormer特别允许Graph Transformer体系结构远远超出上面提到的通常的图形尺寸限制。解释器可以扩展到10,000多个节点图的数据集，例如&lt;a href=&quot;https://arxiv.org/abs/1811.05868&quot;>; cooauthor dataset &lt;/a>;，甚至是更大的图像-nown &lt;a href=&quot;https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv&quot;>; ogbn-arxiv数据集&lt;/a>;，一个引用网络，该网络由170k节点和110万个Edges组成。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance 。 height=&quot;190&quot; data-original-width=&quot;1522&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png&quot; / >; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;结果将expontormer与五个&lt;a href =&#39;的标准graphgps进行比较与标准graphgps进行比较https://arxiv.org/abs/2206.08164&quot;>;长范围Graph Benchmark &lt;/a>;数据集。我们注意到，在本文出版时，五个数据集（Pascalvoc-sp，Coco-SP，肽结构，PCQM-contact）中的四个在五个数据集中实现了最新的结果。&lt;/td>; &lt;/ tr>; &lt;/tbody>; &lt;/table>; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-Left：auto; margin; margin; margin -right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B -suztayk0dx8udq2Sysx4Teep5erw021KZY8L4FEVNV3Boxal/s1600/exphormerPerformance.png“ style =”示例=“ display：block; margin-left; margin-left; auto; auto; margin-right; margin-right：auto; auto; auto; padding; padding：1em 0px; pade; 1em 0px; text-dext-centrign; 0&quot; data-original-height=&quot;202&quot; data-original-width=&quot;1655&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png “/>; &lt;/a>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td class =“ tr-caption” style =“ text-align：center;”>;结果将exponmer与五&lt;a href上的标准graphgps进行比较=“ https://arxiv.org/abs/2206.08164”>;远程图基准&lt;/a>;数据集。我们注意到，发行时的五个数据集中的四个（Pascalvoc-SP，Coco-SP，肽结构，PCQM-contact）中的四个实现了疏散器。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>;  - >; &lt;！ -  &lt;table align =“ center” cellpadding =“ 0” cellSpacing =“ 0” class =“ tr-caption-container” style =“ margin-Left：auto; auto;边缘右：自动;“>; &lt;tbody>; &lt;tr>; &lt;td align =“ left”>; &lt;strong>;型号＆nbsp; &lt;/strong>; &lt;/stron>; &lt;/td>; &lt;td align =“ center”>; &lt;strong>; &lt;strong>; &lt;strong>;＆nbsp; pascalvoc- sp＆nbsp; &lt;/strong>; &lt;br>;＆nbsp; &lt;font size =“  -  1”>; f1分数&lt;/font>; &lt;strong>;↑&lt;/strong>;＆nbsp; &lt;/td>; &lt;td align =“中心”>; &lt;strong>;＆nbsp; coco-sp＆nbsp; &lt;/strong>; &lt;br>; &lt;br>; &lt;br>; &lt;font size =“  -  1”>; f1 score &lt;/f1 score &lt;/fand>; &lt;strong>; /strong>;＆nbsp;态强>;＆nbsp;态强>;＆nbsp;在>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td colspan =“ 6”>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;/td>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;标准Graphgps＆nbsp; &lt;/td>; &lt;td align =“ center”>;＆nbsp; 0.375±0.011＆nbsp; &lt;/td>; &lt;td align =“ center”>;＆nbsp; 0.341±0.004＆nbsp; &lt;/td>; &lt;td align =“中心”>;＆nbsp; &lt;strong>; 0.654±0.004 &lt;/strong>;＆nbsp; &lt;/td>; &lt;td align =“ center”>;＆nbsp; 0.250±0.001＆nbsp; &lt;/td>; &lt;td align =“中心”>;＆nbsp; 0.334±0.001 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;tr>; &lt;td>; &lt;em>; exponmer（我们的） Align =“ Center”>; &lt;strong>; &lt;em>;＆nbsp; 0.398±0.004＆nbsp; &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/td>; &lt;td>; &lt;td align =“ center”>; &lt;strong>; &lt;strong>; &lt;mont>; &lt;em>; &lt;em>; 0.346±0.001 ” >; &lt;em>;＆nbsp; 0.248±0.001＆nbsp; &lt;/em>; &lt;/em>; &lt;/strong>; &lt;/td>; &lt;td align =“ center”>; &lt;strong>; &lt;strong>; &lt;em>; &lt;em>;＆nbsp; 0.364±0.002 &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>;  - >; &lt;p>;最后，我们观察到通过扩张器创建小直径的覆盖层的Expomper，具有有效学习长期依赖性的能力。 &lt;a href=&quot;https://arxiv.org/abs/2206.08164&quot;>;长距离图基准&lt;/a>;＆nbsp;是五个图形学习数据集的套件。结果表明，基于Expostemer的模型的表现优于标准GraphGPS模型（在发布时，在五个数据集中的四分之四是最先进的模型）。 &lt;/p>; &lt;br />; &lt;h2>;结论&lt;/h2>; &lt;p>;图形变压器已成为ML的重要体系结构，该体系结构适应了NLP中使用的非常成功的基于序列的变压器对图形结构化数据。然而，事实证明，可伸缩性在启用具有大图的数据集上使用图形变压器是一个重大挑战。在这篇文章中，我们提出了Exponmer，这是一个稀疏的注意框架，它使用扩展器图来提高图形变压器的可扩展性。被表明表现为具有重要的理论特性并具有强大的经验表现，尤其是在学习长距离依赖性至关重要的数据集上。有关更多信息，我们将读者指向简短的演示&lt;a href=&quot;https://icml.cc/virtual/2023/poster/23782&quot;>;视频&lt;/a>;来自ICML2023。&lt;/p>; &lt;br/ >; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;我们感谢我们的研究合作者Hamed Shirzad和Danica J. Sutherland的不列颠哥伦比亚大学以及Google Research的Ali Kemal Sinop。特别感谢Tom Small创建了本文中使用的动画。&lt;/em>; &lt;/p>; &lt;/content>; &lt;link href =” =“回复” title =“ post注释” type =“ application/atom+xml”/>; &lt;link href =“ http://blog.research.google/2024/01/exphormer-scaling-transformer-transformers-for.html# comment-form“ rel =”回复“ title =” 0注释“ type =” text/html“/>; &lt;link href =” http://wwwww.blogger.com/feeds/847492633145202626/ =“ edit” type =“ application/atom+xml”/>; &lt;link href =“ http://www.blogger.com/feeds/feeds/847492633145202626/posts/posts/posts/default/14187365826582601940076 ATOM+XML“/>; &lt;link href =” http://blog.research.google/2024/01/exphormer-scaling-transformers-forers-for.html“ rel =” rel =“替代” title =“ exphormer结构化数据“ type =” text/html”/>; &lt;under>; &lt;name>; google ai &lt;/name>; &lt;uri>; http://www.blogger.com/profile/120986265147752666161 &lt;/uri>; .com &lt;/email>; &lt;gd：image Height =“ 16” rel =“ http://schemas.google.com/g/g/2005#thumbnail” -rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd -7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt; THR：THR>; 0 &lt;/thr：Total>; &lt;/entry>; &lt;entry>; &lt;id>;标签：blogger.com，1999年：Blog-8474926331452026626.POST-220849549986709693 &lt;/id>; ：00.000-08：00 &lt;/publined>; &lt;更新>; 2024-01-18T10：03：43.608-08：00 &lt;/updateed>; &lt;category scheme =&#39; term =“大语言模型”>; &lt;/category>; &lt;title type =“ text”>;在llms &lt;/stitle &lt;/stitle>; &lt;content type =“ html”>; &lt;span class =“ byline-author”>;发布by Jiefeng Chen, Student Researcher, and Jinsung Yoon, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_- o4spk8wye/s320/aspire％20HERO.jpg“ style =” display：none;” />; &lt;p>;在人工智能的快速发展景观中，大语言模型（LLM）彻底改变了我们与机器互动的方式，将自然语言理解和发电的界限推向了前所未有的高度。然而，高风险决策应用程序的飞跃仍然太宽了，这主要是由于模型预测的固有不确定性。传统的LLM递归产生响应，但它们缺乏内在的机制来为这些反应分配置信度。尽管可以通过在序列中概括单个令牌的概率来得出置信度得分，但传统方法通常在可靠地区分正确和错误的答案方面缺乏。但是，如果LLM可以衡量自己的信心，并且只能在确定的情况下做出预测怎么办？在预测&lt;/a>;旨在通过使LLMS和选择分数一起输出答案来做到这一点，这表明答案是正确的概率。通过选择性预测，可以更好地了解部署在各种应用程序中的LLM的可靠性。先前的研究，例如&lt;a href=&quot;https://openreview.net/pdf?id=vd-ytp0dve&quot;>;语义不确定性&lt;/a>;和&lt;a href =“ https://arxiv.org/arxiv.org/pdf/pdf/2207.052211.052211.0522111 .pdf“>;自我评估&lt;/a>;，试图在LLMS中启用选择性预测。一种典型的方法是使用启发式提示，例如“提议的答案是对还是错？”触发LLM中的自我评估。但是，这种方法在挑战&lt;a href=&quot;https://en.wikipedia.org/wiki/question_answering&quot;>;问题答案&lt;/a>;（QA）任务方面可能不佳。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s1999/image2 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIwlr34r2t085uhaOx2IbBulTJX2FB2g8LkhTgrKgycgb8cDZaRuht0cFPqmlgSkT5jHOx-rrWywmYAEEfJ0FxlC7ammU8ewrZaVo_My7cCpBNYlfgERRKgFYnF-8LhsWhcyS3KTFdBnhyphenhyphencrenwQxBkbjM8UriPKzji8zDkXYv-5rhRXiE0SlvGmXUV-jt/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ tr-caption” style =“ text-align：center;”>; &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>; opt-2.7b &lt;/a>;模型错误地回答了一个问题&lt;a href=&quot;https://aclanthology.org/p17-1147/&quot;>; triviaqa &lt;/a>;数据集：“哪种维生素有助于调节血液凝结？”用“维生素C”。如果没有选择性预测，LLMS可能会输出错误的答案，在这种情况下，可能会导致用户服用错误的维生素。通过选择性预测，LLMS将输出答案以及选择分数。如果选择分数较低（0.1），LLMS将进一步输出“我不知道！”要警告用户不要信任它或使用其他来源验证它。 org/2023.findings-emnlp.345.pdf“>;自我评估改编以改善LLMS中的选择性预测&lt;/a>;”，在&lt;a href =“ /“>; EMNLP 2023的发现&lt;/a>;，我们介绍了Aspire，这是一个精心设计的新型框架，旨在增强LLM的选择性预测能力。通过&lt;a href=&quot;https://huggingface.co/blog/peft&quot;>;参数 -  parameter-fordic-field-tuning，&lt;/a>;通过&lt;a href=&quot;https://huggingface.co/blog/peft &quot;>; &lt;a https://a>; &lt;a https://a>;，ASPIRE微型tunes llms &lt;/a>; &lt;/a>;并培训他们以评估他们生成的答案是否正确。 Aspire允许LLMS输出答案以及该答案的信心分数。我们的实验结果表明，Aspire在各种QA数据集上的最先进的选择性预测方法明显胜过，例如&lt;a href=&quot;https://aclanthology.org/q19-1016.pdf&quot;>; coQA Benchmark &lt;/a>;。 &lt;/p>; &lt;div style =“ line-height：40％;”>; &lt;br />; &lt;/>; &lt;/div>; &lt;h2>; Aspire &lt;/h2>; &lt;p>;的机制想象一下教授LLM不仅要回答问题，而且还要回答问题还评估这些答案 - 类似于学生在教科书的背面验证答案。这就是Aspire的本质，涉及三个阶段：（1）特定于任务的调整，（2）回答抽样和（3）自评估学习。 &lt;/p>; &lt;p>; &lt;strong>;特定于任务的调整&lt;/strong>;：Aspire在冷冻LLM时执行特定于任务的调整以训练适应性参数（θ&lt;ub>; p &lt;/sub>;）。鉴于针对生成任务的培训数据集，它可以微调预培训的LLM，以提高其预测性能。为此，参数效率调音技术（例如，&lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;软提示调谐&lt;/a>;和&lt;a href =“ href =” https：https： //openreview.net/forum?id=nzevkeefyf9&quot;>; lora &lt;/a>;）可以用来适应预训练的LLM对任务的适应，因为它们在获得少量目标任务数据的强大概括方面有效。具体而言，添加了LLM参数（θ）是冷冻的，并且可以添加可自适应参数（θ&lt;sub>; p &lt;/sub>;）以进行微调。只需更新θ&lt;sub>; p &lt;/sub>;才能最大程度地减少标准LLM训练损失（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/wiki/cross-entropropy#cross-entropopy_minimization_minimizationd熵&lt;/a>;）。这样的微调可以提高选择性预测性能，因为它不仅提高了预测准确性，还可以提高正确的输出序列的可能性。 &lt;/p>; &lt;p>; &lt;strong>;答案采样&lt;/strong>;：在特定任务调整后，Aspire将LLM与学习的θ&lt;ub>; p &lt;/sub>;一起为每个培训问题生成不同的答案，并创建一个用于自我评估学习的数据集。我们旨在生成可能性很高的输出序列。我们使用&lt;a href=&quot;https://en.wikipedia.org/wiki/wiki/beam_search&quot;>; beam search &lt;/a>;作为解码算法来生成高likeLihienhiphoodhoodhoodhoodhoodhoodhoodhoodhoodhoodhoodhoodhoodhiphiehiehiahienhiahienhial ofupt sequences us &lt;a href =“ https：// https：// https：// https：// aclanthology.org/p04-1077/&quot;>; rouge-l &lt;/a>;衡量标准，以确定生成的输出序列是否正确。 &lt;/p>; &lt;p>; &lt;strong>;自评估学习&lt;/strong>;：在每个查询中对高易头输出进行采样后，Aspire添加了适应性参数（θ&lt;ub>; s &lt;/sub>;），只有微调θθ &lt;sub>; s &lt;/sub>;用于学习自我评估。由于输出序列的生成仅取决于θ和θ&lt;ub>; p &lt;/sub>;，因此冻结θ和学习的θ&lt;ub>; p &lt;/sub>;可以避免在学习自我评估时改变LLM的预测行为。我们优化了θ&lt;sub>; s &lt;/sub>;，以便适应的LLM可以自行区分正确和错误的答案。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s1999 /image5.png“ style =”边缘左左右：自动; margin-right：auto;“>; &lt;img border =“ 0” data-eriginal-height =“ 933” data-eriginal-width =“ 1999” src =“ src =” https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKSO3s9PgcBr1MpkJR_PYI-ogQ79JD4A4-fJ_OgT8reSKEqIWSUPD7QUVSqIUuAhNfbgEA-XVrOne8S1oJSFaE6YIH4z43bn8jzsfG768qynW-G6lG7dwOvu15UCH6tdlIXEoe2dCUAHmT2bmNijwUvigF50W8vBCsCrjBA_FGYlnsmizHiyutHYZ1A-A2/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >; &lt;td class =“ tr-caption” style =“ text-align：center;”>; Aspire Framework的三个阶段。 &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;b>;在拟议的框架中，可以训练θ&lt;sub>; p &lt;/sub>;和θ&lt;sub>; s &lt;/sub>;使用任何参数有效的调整方法。在这项工作中，我们使用&lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243/&quot;>;软提示调音&lt;/a>;，一种简单而有效的学习机制，用于学习“ &lt;a href =” https://blog.research.google/2022/02/guiding-frozen-language-models-with.html&quot;>;提示。这种方法背后的推动力在于人们认识到，如果我们能够开发有效刺激自我评估的提示，那么应该可以通过与目标培训目标结合使用软提示来发现这些提示。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTErsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s800/image1 。 //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCX9MjJ_KqSauAvXOvcXCDWR1H-hoD3e6EaSCEbG5-EJoJYLmekytCRSXaXrhNGS5BH7DfwbZW7FWzUaTErsGxKSWWM8lLAOxxDX3M5U4Zv8gERXBk_uCY7OVshLexrKt5GTSwrkRdFW0dAcMaALHvrLIosv7Tn4pRd7Rh35-HGWQ13SAeHtJ5-wsNgMNt/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class =“ TR-CAPTION”样式=“ Text-Align：Center;”>;通过软提示调整实现Aspire框架。我们首先使用第一个软提示来生成问题的答案，然后使用第二个软提示来计算学习的自我评估得分。&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;/table>; &lt;br />; &lt;b />; &lt;p >;训练θ&lt;sub>; p &lt;/sub>;和θ&lt;sub>; s &lt;/sub>;后，我们通过梁搜索解码获得查询的预测。然后，我们定义了一个选择分数，将生成答案的可能性与学习的自我评估得分（即，对查询正确的预测是正确的）以做出选择性预测的可能性。 &lt;/p>; &lt;div style =“线路高：40％;”>; &lt;br />; &lt;/>; &lt;/>; &lt;/div>; &lt;h2>;结果&lt;/h2>; &lt;p>;为了证明Aspire的效率，我们在三个问题索问题中对其进行了评估数据集 -  &lt;a href=&quot;https://aclanthology.org/q19-1016/&quot;>; coqa &lt;/a>;，&lt;a href=&quot;https://aclanthology.org/p17-1147/&quot;>; triviaqa &lt;/a &lt;/a &lt;/a &lt;/a &lt;/a >;和&lt;a href=&quot;https://aclanthology.org/d16-1264/&quot;>; squead &lt;/a>;  - 使用各种&lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>; - 训练的变压器&lt;/a>;（OPT）模型。通过训练θ&lt;sub>; p &lt;/sub>;，通过软提示调整，我们观察到LLMS的精度有实质性的升高。例如，&lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;>; opt-2.7b &lt;/a>;与Aspire相应的模型与使用使用该模型相比，使用Aspire的模型提高了性能，使用了使用该模型的改进COQA和小队数据集。这些结果表明，通过适当的适应，较小的LLM在某些情况下具有匹配或可能超过较大模型的准确性的能力。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz -ei4brrw-e9wzc/s1999/image4.png“ style =” Margin-Left：auto; Margin-right：auto;“>; &lt;img border =” 0“ data-original-height =” 1500“ data-original-width =” data-original-width = &quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiT3H50FS2Ml9VoWJCuNnInzRQqtNEpyUXuwRnogIG-pDIlRduV0DmvyI9iQIHTziGdqkugV9SDjIcV8WPfnb8QyzQ3ACcusD7O1FQvyVe9U9C5iCbX-uS8xHNhbvg2uv_CPwe4UJASF_dPe8s-c-xz-1hplqXYYxw4wsLOw9dJ-vWrLz-Ei4BrrW-e9Wzc/s16000/image4.png&quot; />;&lt;/ a>; &lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;p>;在探究使用固定模型预测的选择分数计算时，Aspire收到了更高的&lt;a href =“ https://openreview.net /pdf？id = vd-aytp0dve“>; auroc &lt;/a>;得分（随机选择的正确输出序列的选择得分高于所有数据集中的基线方法的概率比随机选择的不正确输出序列更高的概率）。例如，与基准相比，在COQA基准上，Aspire将AUROC从51.3％提高到80.3％。 &lt;/p>; &lt;p>;从triviaqa数据集评估中出现了一种有趣的模式。虽然预训练的OPT-30B模型表现出较高的基线准确性，但当传统的自我评估方法 -  &lt;a href=&quot;https://arxiv.org/abs/2207.052222当传统预测中的性能并没有显着提高。应用和p（true）&lt;/a>;  - 应用。相比之下，较小的Opt-2.7b模型在用Aspire增强时，在这方面表现出色。这种差异强调了一个重要的见解：使用常规自我评估技术的较大的LLM在选择性预测中可能不如较小的，靠近的增强模型。 &lt;/p>; &lt;table align =“ center” cellpadding =“ 0” cellspacing =“ 0” class =“ tr-caption-container”样式=“ margin-left：auto; auto; margin-right：auto; auto;>; &lt;tbody>;>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s1999/image3.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1599&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQLSLnyNPe8LW9W3KLt7Tkp3gmLSLHkTbICIi5j__yWNt7aG9PmWyW5bE4vSs8q2iFqE5dlb0KOdtKzcmg1JuKdzZWFUxYXiatDPB7N-q1NhkkH9hEcgqlw33BFUh_v_8DQJnY5lMrXexv0HUvTPYRS_Gb-M75Rx_TBhxyvLrI-AhZJV243sUzo2gIPoh/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our experimental journey with ASPIRE underscores a pivotal shift in the landscape of LLMs: The capacity of a language model is not the be-all and end-all of its performance. Instead, the effectiveness of models can be drastically improved through strategic adaptations, allowing for more precise, confident predictions even in smaller models. As a result, ASPIRE stands as a testament to the potential of LLMs that can judiciously ascertain their own certainty and decisively outperform larger counterparts in selective prediction tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In conclusion, ASPIRE is not just another framework; it&#39;s a vision of a future where LLMs can be trusted partners in decision-making. By honing the selective prediction performance, we&#39;re inching closer to realizing the full potential of AI in critical applications. &lt;/p>; &lt;p>; Our research has opened new doors, and we invite the community to build upon this foundation. We&#39;re excited to see how ASPIRE will inspire the next generation of LLMs and beyond. To learn more about our findings, we encourage you to read our paper and join us in this thrilling journey towards creating a more reliable and self-aware AI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the contributions of Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, and Somesh Jha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/220849549986709693/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/220849549986709693&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/introducing-aspire-for-selective.html&quot; rel=&quot;alternate&quot; title=&quot;Introducing ASPIRE for selective prediction in LLMs&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMaqP9dd9YDXh04PEWHSFquEToz6U5M4YUxzfExokjfteUfuGAKhqs1LV5DUMJOBoiF3GjGxg7NqezNazuTeWePMsuH_OW7NM4z4ooMPhWnR22iyzENgpmG2-xJDbRbeeyyLbG-3dIdgYjl2IxX0K-bFvpbrAJsQA7Mu70MqxEuVFJXvwnP_-o4sPK8wYe/s72-c/ASPIRE%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1800430129205268706&lt;/id>;&lt;published>;2024-01-12T09:04:00.000-08:00&lt;/published>;&lt;updated>;2024-01-16T10:55:15.626-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;AMIE: A research AI system for diagnostic medical reasoning and conversations&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Alan Karthikesalingam and Vivek Natarajan, Research Leads, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s16000/AMIE.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; The physician-patient conversation is a cornerstone of medicine, in which skilled and intentional communication drives diagnosis, management, empathy and trust. AI systems capable of such diagnostic dialogues could increase availability, accessibility, quality and consistency of care by being useful conversational partners to clinicians and patients alike. But approximating clinicians&#39; considerable expertise is a significant challenge. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Recent progress in large language models (LLMs) outside the medical domain has shown that they can plan, reason, and use relevant context to hold rich conversations. However, there are many aspects of good diagnostic dialogue that are unique to the medical domain. An effective clinician takes a complete “clinical history” and asks intelligent questions that help to derive a differential diagnosis. They wield considerable skill to foster an effective relationship, provide information clearly, make joint and informed decisions with the patient, respond empathically to their emotions, and support them in the next steps of care. While LLMs can accurately perform tasks such as medical summarization or answering medical questions, there has been little work specifically aimed towards developing these kinds of conversational diagnostic capabilities. &lt;/p>; &lt;p>; Inspired by this challenge, we developed &lt;a href=&quot;https://arxiv.org/abs/2401.05654&quot;>;Articulate Medical Intelligence Explorer (AMIE)&lt;/a>;, a research AI system based on a LLM and optimized for diagnostic reasoning and conversations. We trained and evaluated AMIE along many dimensions that reflect quality in real-world clinical consultations from the perspective of both clinicians and patients. To scale AMIE across a multitude of disease conditions, specialties and scenarios, we developed a novel self-play based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE&#39;s diagnostic accuracy and conversation quality. Finally, we tested AMIE prospectively in real examples of multi-turn dialogue by simulating consultations with trained actors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s1200/AMIE%20GIF%201%20v2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;512&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh1TB1GjBHpX7Kazzao7l_ysdB7rXdxZQG7CodkfM4A7cYSJRKUEfhZL4iFJ4BI0ipp9o4rPam4ARcp0v98V_1CtcYPb9fCalxW3Y_vekZl1iDtkdfshLbAi_OSbwuaecYtMosCRUtgvAMYWSoASj7A7OgAPfzVEQbwMOmkfzNnWKot2dtyAmQmFDtYKy4/s16000/AMIE%20GIF%201%20v2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE was optimized for diagnostic conversations, asking questions that help to reduce its uncertainty and improve diagnostic accuracy, while also balancing this with other requirements of effective clinical communication, such as empathy, fostering a relationship, and providing information clearly.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Evaluation of conversational diagnostic AI&lt;/h2>; &lt;p>; Besides developing and optimizing AI systems themselves for diagnostic conversations, how to assess such systems is also an open question. Inspired by accepted tools used to measure consultation quality and clinical communication skills in real-world settings, we constructed a pilot evaluation rubric to assess diagnostic conversations along axes pertaining to history-taking, diagnostic accuracy, clinical management, clinical communication skills, relationship fostering and共情。 &lt;/p>; &lt;p>; We then designed a randomized, double-blind crossover study of text-based consultations with validated patient actors interacting either with board-certified primary care physicians (PCPs) or the AI system optimized for diagnostic dialogue. We set up our consultations in the style of an &lt;a href=&quot;https://en.wikipedia.org/wiki/Objective_structured_clinical_examination&quot;>;objective structured clinical examination&lt;/a>; (OSCE), a practical assessment commonly used in the real world to examine clinicians&#39; skills and competencies in a standardized and objective way. In a typical OSCE, clinicians might rotate through multiple stations, each simulating a real-life clinical scenario where they perform tasks such as conducting a consultation with a standardized patient actor (trained carefully to emulate a patient with a particular condition). Consultations were performed using a synchronous text-chat tool, mimicking the interface familiar to most consumers using LLMs today. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/s1350/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1350&quot; height=&quot;568&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBTzb4Nx-dNLSwgDjJka4tYA29gFEnPna3N68cdriUmaybI1IGhqxPMLzObLg1nRh3S7pd21G7CHczMvy7tUHyETClVRu8Hv3J9gQRd_WGYwORLiylKUgNViILvFO068daetL3MzJAa2rGU7Yzjg2BkfUas0hQgP9yBwmH_Wx3wmCpGfuZ-75yejQTAg8/w640-h568/image4.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE is a research AI system based on LLMs for diagnostic reasoning and dialogue.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;AMIE: an LLM-based conversational diagnostic research AI system &lt;/h2>; &lt;p>; We trained AMIE on real-world datasets comprising medical reasoning, medical summarization and real-world clinical conversations. &lt;/p>; &lt;p>; It is feasible to train LLMs using real-world dialogues developed by passively collecting and transcribing in-person clinical visits, however, two substantial challenges limit their effectiveness in training LLMs for medical conversations. First, existing real-world data often fails to capture the vast range of medical conditions and scenarios, hindering the scalability and comprehensiveness. Second, the data derived from real-world dialogue transcripts tends to be noisy, containing ambiguous language (including slang, jargon, humor and sarcasm), interruptions, ungrammatical utterances, and implicit references. &lt;/p>; &lt;p>; To address these limitations, we designed a self-play based simulated learning environment with automated feedback mechanisms for diagnostic medical dialogue in a virtual care setting, enabling us to scale AMIE&#39;s knowledge and capabilities across many medical conditions and contexts 。 We used this environment to iteratively fine-tune AMIE with an evolving set of simulated dialogues in addition to the static corpus of real-world data described. &lt;/p>; &lt;p>; This process consisted of two self-play loops: (1) an “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient simulator; and (2) an “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a virtuous continuous learning cycle. &lt;/p>; &lt;p>; Further, we also employed an inference time chain-of-reasoning strategy which enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s1400/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiASnZ5p1olZNNL1e-bzqA6s1WprbenNRebHvq2sXuRhYHtHBMw5s1sgAR6SXS4lSDkBD_WgsY6mepBCoLojtes3GOU3yCoOPRGLoGpkMV99TM1Ru0xpNSNWee-5xfkUGBeE9fnp_rY8t_0Dv3NIxVnj9iGPNSyoGtJ6N9MSsjFsIqDpJVhsAQbCBoyJ1-N/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/s1834/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDZCFvlZe9612iZ0gY2ov193z0iDcGQI0I9I1o0KRullBPhVIxyHerVflj5dm24vxyRqlNiPyLnfHW1yc9gE88KNekb_WQ4as6qO5BExL9K_aDXc9Ypx6DYNqWmvP5QoJ2sVitVVfVMvyLY2DJ7Ck92fwFAaHEeF2JmSMnftbXpAAUjS6ADM4F3dctOz4/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;br />; &lt;p>; We tested performance in consultations with simulated patients (played by trained actors), compared to those performed by 20 real PCPs using the randomized approach described above. AMIE and PCPs were assessed from the perspectives of both specialist attending physicians and our simulated patients in a randomized, blinded crossover study that included 149 case scenarios from OSCE providers in Canada, the UK and India in a diverse range of specialties and diseases. &lt;/p>; &lt;p>; Notably, our study was not designed to emulate either traditional in-person OSCE evaluations or the ways clinicians usually use text, email, chat or telemedicine. Instead, our experiment mirrored the most common way consumers interact with LLMs today, a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1296&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSuFcXXr8lxIA1-p428y31PqVkpEMlQAdKj-vdTmtVYqeuogSQAjWFM3Gj8akNVG-6Cyd9xZKbLKz0jUFABDU_JjwcM35qLVsSi5zlB3frgei5NAwhTQ7PyEbipXJK8gPvb0vY_VKGrZ-GFAmwtDf-pcJbKtY5DCEBb43N5rtnSa8gYdLxl5aQxpAZ1qo/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the randomized study design to perform a virtual remote OSCE with simulated patients via online multi-turn synchronous text chat.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Performance of AMIE&lt;/h2>; &lt;p>; In this setting, we observed that AMIE performed simulated diagnostic conversations at least as well as PCPs when both were evaluated along multiple clinically-meaningful axes of consultation quality. AMIE had greater diagnostic accuracy and superior performance for 28 of 32 axes from the perspective of specialist physicians, and 24 of 26 axes from the perspective of patient actors. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/s1834/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1284&quot; data-original-width=&quot;1834&quot; height=&quot;448&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvP4CCxNKLIBLIBfPnJZ-7mRYZqxINGJ8_Uos8K3Gd8PJrFURwBYYjJePGqHpa63nFQR2aahi3HcwPos9NCV-fknrdVRsrwJCI6qFub84f5g5gNo_SvuosZt7Rjm5LXOQuVvG0n_GmzL6jNhihROxls9ZQBA5aVPod_onwurffiTI12F6d4wwfbeNMdxQ/w640-h448/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;AMIE outperformed PCPs on multiple evaluation axes for diagnostic dialogue in our evaluations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s1999/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZvkY54tqvLCbfTrEFR5e_T1eEXJZvn3V__lBts2bukKDwuJkLmDo5w-ilA8B44JwDPUv5v5hzCN9WRWttPEZ2qN1wQaGQR0SRjjVhapLDxg6Te5YLjPqgUwoDCot2sBujGLVHgIrKFXUkT3bKzL1MLHCMxEMs0pC5ZMoi-PTAhPFLgW7bsjsi5jy3pL0/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Specialist-rated top-k diagnostic accuracy. AMIE and PCPs top-k differential diagnosis (DDx) accuracy are compared across 149 scenarios with respect to the ground truth diagnosis (a) and all diagnoses listed within the accepted differential diagnoses (b). Bootstrapping (n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with p &amp;lt;0.05 after&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/False_discovery_rate&quot;>;false discovery rate&lt;/a>;&amp;nbsp;(FDR) correction.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1864&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3FLScvpxSucelwHpfxEC_pMR4cXG3ioiw1lRJs1XgWbuGM8mLS635ryiJJOF7ZOuuA4t0rkj1OXWXB57GW-FQcNcYq_TKfTPyCLm-EV3Ivk5yPgYdjYKxT8-yxQnDz4mNJwKop4yS3XvyNpcUzVOrhm0MrJKs5DVfl-u8hgwhwkI6kZAvCto4Z4JGcnTb/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic conversation and reasoning qualities as assessed by specialist physicians. On 28 out of 32 axes, AMIE outperformed PCPs while being comparable on the rest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s1892/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1714&quot; data-original-width=&quot;1892&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNa2OOpTrDPJWdNMWqfl0Mp3Yjn6d0DbFLTTwnpcAmjxddEt6rr6ryOBE_KNWbtce0bRFRYJYOVqdA9eetEptfRWgoWJ4-4LEka8RJMZ7p3qcjqGWtA2PKRxyMZKzbtRXCfvQqMQrpVgGrefJB0QwzO1GxTkNAPwhrQ1HXHFbUMZpR7fFhhBKUylktESg/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Diagnostic conversation and reasoning qualities as assessed by specialist physicians. On 28 out of 32 axes, AMIE outperformed PCPs while being comparable on the rest.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;br />; &lt;h2>;Limitations&lt;/h2>; &lt;p>; Our research has several limitations and should be interpreted with appropriate caution. Firstly, our evaluation technique likely underestimates the real-world value of human conversations, as the clinicians in our study were limited to an unfamiliar text-chat interface, which permits large-scale LLM–patient interactions but is not representative of usual clinical practice. Secondly, any research of this type must be seen as only a first exploratory step on a long journey. Transitioning from a LLM research prototype that we evaluated in this study to a safe and robust tool that could be used by people and those who provide care for them will require significant additional research. There are many important limitations to be addressed, including experimental performance under real-world constraints and dedicated exploration of such important topics as health equity and fairness, privacy, robustness, and many more, to ensure the safety and reliability of the technology. &lt;/p>; &lt;br />; &lt;h2>;AMIE as an aid to clinicians&lt;/h2>; &lt;p>; In a &lt;a href=&quot;https://arxiv.org/abs/2312.00164&quot;>;recently released preprint&lt;/a>;, we evaluated the ability of an earlier iteration of the AMIE system to generate a DDx alone or as an aid to clinicians. Twenty (20) generalist clinicians evaluated 303 challenging, real-world medical cases sourced from the &lt;em>;&lt;a href=&quot;https://www.nejm.org/&quot;>;New England Journal of Medicine&lt;/a>;&lt;/em>; (NEJM) &lt;a href=&quot;https://www.nejm.org/case-challenges&quot;>;ClinicoPathologic Conferences&lt;/a>; (CPCs). Each case report was read by two clinicians randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or AMIE assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1022&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEia-nKW28SyAk0DHT21L2CsB18JKUmmt2sPtafGvRtJWrOEgfn1v_hXDtSIJsFP2m66tBA33MwMHXKQSL-nGKfvMTKASXUVZ5n_I4VytKfa0S3EN5vf2TeMHfmOtMLCJtfD3PCvMMc8PJsbIYu-iikFu4atfCOBa-a5yHTM2Tok1wjZpkmBbvioUhXz4Dc/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Assisted randomized reader study setup to investigate the assistive effect of AMIE to clinicians in solving complex diagnostic case challenges from the New England Journal of Medicine.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% vs. 33.6%, p= 0.04). Comparing the two assisted study arms, the top-10 accuracy was higher for clinicians assisted by AMIE, compared to clinicians without AMIE assistance (24.6%, p&amp;lt;0.01) and clinicians with search (5.45%, p=0.02). Further, clinicians assisted by AMIE arrived at more comprehensive differential lists than those without AMIE assistance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1424&quot; data-original-width=&quot;1999&quot; height=&quot;456&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiO1YlFJdQMCt1ZMTQN5neWeYoljjA7Y13FP_2c7q85hSKbLCdNLJtUt1VtBFlCUBlGTIviqdr4XWnnandULaKfGlyPh89QzzaHXmb-wFxYfkwbRv5OO9Wni6Hr04jVO_W1w2cs7RQcCRWCWrW9lxM3t61BI3ZPK6hdsv7RAQAn8TN6s80nP9nwjia0xig/w640-h456/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;In addition to strong standalone performance, using the AMIE system led to significant assistive effect and improvements in diagnostic accuracy of the clinicians in solving these complex case challenges.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; It&#39;s worth noting that NEJM CPCs are not representative of everyday clinical practice. They are unusual case reports in only a few hundred individuals so offer limited scope for probing important issues like equity or fairness. &lt;/p>; &lt;br />; &lt;h2>;Bold and responsible research in healthcare — the art of the possible &lt;/h2>; &lt;p>; Access to clinical expertise remains scarce around the world. While AI has shown great promise in specific clinical applications, engagement in the dynamic, conversational diagnostic journeys of clinical practice requires many capabilities not yet demonstrated by AI systems. Doctors wield not only knowledge and skill but a dedication to myriad principles, including safety and quality, communication, partnership and teamwork, trust, and professionalism. Realizing these attributes in AI systems is an inspiring challenge that should be approached responsibly and with care. AMIE is our exploration of the “art of the possible”, a research-only system for safely exploring a vision of the future where AI systems might be better aligned with attributes of the skilled clinicians entrusted with our care. It is early experimental-only work, not a product, and has several limitations that we believe merit rigorous and extensive further scientific studies in order to envision a future in which conversational, empathic and diagnostic AI systems might become safe, helpful and accessible. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The research described here is joint work across many teams at Google Research and Google Deepmind. We are grateful to all our co-authors - Tao Tu, Mike Schaekermann, Anil Palepu, Daniel McDuff, Jake Sunshine, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Sara Mahdavi, Karan Sighal, Shekoofeh Azizi, Nenad Tomasev, Yun Liu, Yong Cheng, Le Hou, Albert Webson, Jake Garrison, Yash Sharma, Anupam Pathak, Sushant Prakash, Philip Mansfield, Shwetak Patel, Bradley Green, Ewa Dominowska, Renee Wong, Juraj Gottweis, Dale Webster, Katherine Chou, Christopher Semturs, Joelle Barral, Greg Corrado and Yossi Matias. We also thank Sami Lachgar, Lauren Winer and John Guilyard for their support with narratives and the visuals. Finally, we are grateful to Michael Howell, James Manyika, Jeff Dean, Karen DeSalvo, Zoubin Ghahramani&amp;nbsp;and Demis Hassabis for their support during the course of this project&lt;/em>;. &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1800430129205268706/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1800430129205268706&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/amie-research-ai-system-for-diagnostic_12.html&quot; rel=&quot;alternate&quot; title=&quot;AMIE: A research AI system for diagnostic medical reasoning and conversations&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr_wfWpw2CVBYDc3Mlk879CUecv4uGlq36Fxe0GEnVcK2kJnzAyRkPRb8vO5jJVqrd_zvQ6W8suHyp1xhFhNFJuUj8nTNRp3TsZP7Z5uWlqw22hZZKVJJ33X5NWmT0UTkOdC4raONlnSbR8E616Mi_lJVE3DvbWYB-19eR2wpCgwAaykkquUV3DOLRY6c/s72-c/AMIE.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7998118785777164574&lt;/id>;&lt;published>;2024-01-11T14:42:00.000-08:00&lt;/published>;&lt;updated>;2024-01-11T14:42:51.944-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Understanding&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Can large language models identify and correct their mistakes?&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Gladys Tyen, Intern, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s1600/Backtracking.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; LLMs are increasingly popular for reasoning tasks, such as &lt;a href=&quot;https://hotpotqa.github.io/&quot;>;multi-turn QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2207.01206&quot;>;task completion&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2304.05128&quot;>;code generation&lt;/a>;, or &lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>;mathematics&lt;/a>;. Yet much like people, they do not always solve problems correctly on the first try, especially on tasks for which they were not trained. Therefore, for such systems to be most useful, they should be able to 1) identify where their reasoning went wrong and 2) backtrack to find another solution. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; This has led to a surge in methods related to &lt;em>;self-correction&lt;/em>;, where an LLM is used to identify problems in its own output, and then produce improved results based on the feedback. Self-correction is generally thought of as a single process, but we decided to break it down into two components, &lt;em>;mistake finding&lt;strong>; &lt;/strong>;&lt;/em>;and &lt;em>;output correction&lt;/em>;. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.08516#:~:text=While%20self%2Dcorrection%20has%20shown,et%20al.%2C%202023).&quot;>;LLMs cannot find reasoning errors, but can correct them!&lt;/a>;”, we test state-of-the-art LLMs on mistake finding and output correction separately. We present &lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake&quot;>;BIG-Bench Mistake&lt;/a>;, an evaluation benchmark dataset for mistake identification, which we use to address the following questions: &lt;/p>; &lt;ol>; &lt;li>;Can LLMs find logical mistakes in &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;Chain-of-Thought&lt;/a>; (CoT) style reasoning? &lt;/li>;&lt;li>;Can mistake-finding be used as a proxy for correctness? &lt;/li>;&lt;li>;Knowing where the mistake is, can LLMs then be prompted to backtrack and arrive at the correct answer? &lt;/li>;&lt;li>;Can mistake finding as a skill generalize to tasks the LLMs have never seen? &lt;/li>; &lt;/ol>; &lt;br />; &lt;h2>;About our dataset&lt;/h2>; &lt;p>; Mistake finding is an underexplored problem in natural language processing, with a particular lack of evaluation tasks in this domain. To best assess the ability of LLMs to find mistakes, evaluation tasks should exhibit mistakes that are non-ambiguous. To our knowledge, most current mistake-finding datasets do not go beyond the realm of &lt;a href=&quot;https://github.com/openai/grade-school-math&quot;>;mathematics&lt;/a>; for this reason. &lt;/p>; &lt;p>; To assess the ability of LLMs to reason about mistakes outside of the math domain, we produce a new dataset for use by the research community, called&lt;strong>; &lt;/strong>;&lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake&quot;>;BIG-Bench Mistake&lt;/a>;. This dataset consists of Chain-of-Thought traces generated using &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; on five tasks in &lt;a href=&quot;https://github.com/suzgunmirac/BIG-Bench-Hard&quot;>;BIG-Bench&lt;/a>;. Each trace is annotated with the location of the first logical mistake. &lt;/p>; &lt;p>; To maximize the number of mistakes in our dataset, we sample 255 traces where the answer is incorrect (so we know there is definitely a mistake), and 45 traces where the answer is correct (so there may or may not be a mistake). We then ask human labelers to go through each trace and identify the first mistake step. Each trace has been annotated by at least three labelers, whose answers had &lt;a href=&quot;https://en.wikipedia.org/wiki/Inter-rater_reliability&quot;>;inter-rater reliability&lt;/a>; levels of &amp;gt;0.98 (using &lt;a href=&quot;https://en.wikipedia.org/wiki/Krippendorff%27s_alpha&quot;>;Krippendorff&#39;s α&lt;/a>;). The labeling was done for all tasks except the &lt;a href=&quot;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/dyck_languages&quot;>;Dyck Languages task&lt;/a>;, which involves predicting the sequence of closing parentheses for a given input sequence. This task we labeled algorithmically. &lt;/p>; &lt;p>; The logical errors made in this dataset are simple and unambiguous, providing a good benchmark for testing an LLM&#39;s ability to find its own mistakes before using them on harder, more ambiguous tasks.&lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s1354/BBMistakes2.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;410&quot; data-original-width=&quot;1354&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvk5zKLBvc2Ou6RpJc9l-lLqwHW6nWARuc2IAckSQ2SPYX6-UQj9Z8FyOB5emaBvXPta4MWqR1gis9FMEXeafffprNpyPmF_XaBOQ7tQpRpEylbnSlbwytNv1BFXlz5I-ulNM0ZBC7kBhx2KkdCT5MIejwdsHKpHu6rrJ4LBVd-Na_XUn5DCy0EKtj1Uy6/s16000/BBMistakes2.png&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;h2>;Core questions about mistake identification&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;1. Can LLMs find logical mistakes in Chain-of-Thought style reasoning?&lt;/h3>; &lt;p>; First, we want to find out if LLMs can identify mistakes independently of their ability to correct them. We attempt multiple prompting methods to test &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_pre-trained_transformer&quot;>;GPT&lt;/a>; series models for their ability to locate mistakes (prompts &lt;a href=&quot;https://github.com/WHGTyen/BIG-Bench-Mistake/tree/main/mistake_finding_prompts&quot;>;here&lt;/a>;) under the assumption that they are generally representative of modern LLM performance. &lt;/p>; &lt;p>; Generally, we found these state-of-the-art models perform poorly, with the best model achieving 52.9% accuracy overall. Hence, there is a need to improve LLMs&#39; ability in this area of reasoning. &lt;/p>; &lt;p>; In our experiments, we try three different prompting methods: direct (trace), direct (step) and CoT (step). In direct (trace), we provide the LLM with the trace and ask for the location step of the mistake or &lt;em>;no mistake&lt;/em>;. In direct (step), we prompt the LLM to ask itself this question for each step it takes. In CoT (step), we prompt the LLM to give its reasoning for whether each step is a mistake or not a mistake. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s1061/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;466&quot; data-original-width=&quot;1061&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYeNEXWh6vhy4SvTgSE5kOYYdRV3vFdUGs9zorH7bI010gD4gFziPwtig3bvlJFnzEpOgcQZZbn_2_KDEiqwFgdtimB-IYhhROTmtTKoxmmWF0jzI1IKfU3ZSeAhqEDJgLBwkmdUrbMDd9uYo3kLvK5uhygNRU2mkuRhnW3ZofDkYw-CsjKzFUQdplpFfe/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing the three prompting methods direct (trace), direct (step) and CoT (step).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our finding is in line and builds upon &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;prior results&lt;/a>;, but goes further in showing that LLMs struggle with even simple and unambiguous mistakes (for comparison, our human raters without prior expertise solve the problem with a high degree of agreement). We hypothesize that this is a big reason why LLMs are unable to self-correct reasoning errors. See &lt;a href=&quot;https://arxiv.org/abs/2311.08516&quot;>;the paper&lt;/a>; for the full results. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;2. Can mistake-finding be used as a proxy for correctness of the answer?&lt;/h3>; &lt;p>; When people are confronted with a problem where we are unsure of the answer, we can work through our solutions step-by-step. If no error is found, we can make the assumption that we did the right thing. &lt;/p>; &lt;p>; While we hypothesized that this would work similarly for LLMs, we discovered that this is a poor strategy. On our dataset of 85% incorrect traces and 15% correct traces, using this method is not much better than the naïve strategy of always labeling traces as incorrect, which gives a weighted average &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1&lt;/a>; of 78. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s698/Self-correcting-LLMs-Tasks.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;427&quot; data-original-width=&quot;698&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi07Eh2ZJrPHzVPJkom0V-tc51me104pXKoAmHrVaNwNgL8CW4QCIv4js4_aZzabllySdTx5vpHv_5T0NwKDB7nDcfHaNpx7C-fkoWKArltSWSWoXSTB5_4IPr2uOdjpsZKVMBfqVJUejyEuvy5SFC0y8933eBb6hxuvtbyoa-CcWfyQdDtBwBdMadk04JU/s16000/Self-correcting-LLMs-Tasks.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing how well mistake-finding with LLMs can be used as a proxy for correctness of the answer on each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;3. Can LLMs backtrack knowing where the error is?&lt;/h3>; &lt;p>; Since we&#39;ve shown that LLMs exhibit poor performance in finding reasoning errors in CoT traces, we want to know whether LLMs can even correct errors &lt;em>;at all&lt;/em>;, even if they know where the error is. &lt;/p>; &lt;p>; Note that knowing the &lt;em>;mistake location&lt;/em>; is different from knowing &lt;em>;the right answer&lt;/em>;: CoT traces can contain logical mistakes even if the final answer is correct, or反之亦然。 In most real-world situations, we won&#39;t know what the right answer is, but we might be able to identify logical errors in intermediate steps. &lt;/p>; &lt;p>; We propose the following backtracking method: &lt;/p>; &lt;ol>; &lt;li>;Generate CoT traces as usual, at temperature = 0. (Temperature is a parameter that controls the randomness of generated responses, with higher values producing more diverse and creative outputs, usually at the expense of quality.) &lt;/li>;&lt;li>;Identify the location of the first logical mistake (for example with a classifier, or here we just use labels from our dataset). &lt;/li>;&lt;li>;Re-generate the mistake step at temperature = 1 and produce a set of eight outputs. Since the original output is known to lead to incorrect results, the goal is to find an alternative generation at this step that is significantly different from the original. &lt;/li>;&lt;li>;From these eight outputs, select one that is different from the original mistake step. (We just use exact matching here, but in the future this can be something more sophisticated.) &lt;/li>;&lt;li>;Using the new step, generate the rest of the trace as normal at temperature = 0. &lt;/li>; &lt;/ol>; &lt;p>; It&#39;s a very simple method that does not require any additional prompt crafting and avoids having to re-generate the entire trace. We test it using the mistake location data from BIG-Bench Mistake, and we find that it can correct CoT errors. &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2310.01798&quot;>;Recent work&lt;/a>; showed that self-correction methods, like &lt;a href=&quot;https://arxiv.org/abs/2303.11366&quot;>;Reflexion&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2303.17491&quot;>;RCI&lt;/a>;, cause deterioration in accuracy scores because there are more correct answers becoming incorrect than vice versa. Our method, on the other hand, produces more gains (by correcting wrong answers) than losses (by changing right answers to wrong answers). &lt;/p>; &lt;p>; We also compare our method with a random baseline, where we randomly assume a step to be a mistake. Our results show that this random baseline does produce some gains, but not as much as backtracking with the correct mistake location, and with more losses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s744/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;744&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4CQ3amwJgMJ7OZToizp04vYIOj7F4kTdVO5DgthHi_HQQNa2FrkKjBA7LB249yN44kXFs0lZVuX1W4n3GLQI51Fy95ls-gK_rMLQQETYNmvqI7OS7U6xHgXx2cUnhTcwmZrpFWrS1vd01G14gWOexxZDTcpOIbGTHfXRgLWj22OteqMh_iTK2Rg_fC7xt/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing the gains and losses in accuracy for our method as well as a random baseline on each dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;4. Can mistake finding generalize to tasks the LLMs have never seen?&lt;/h3>; &lt;p>; To answer this question, we fine-tuned a small model on four of the BIG-Bench tasks and tested it on the fifth, held-out task 。 We do this for every task, producing five fine-tuned models in total. Then we compare the results with just zero-shot prompting &lt;a href=&quot;https://blog.google/technology/ai/google-palm-2-ai-large-language-model/&quot;>;PaLM 2-L-Unicorn&lt;/a>;, a much larger model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s759/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;759&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0wDD7i6x7jVZpzCE3jQDkun0ros6zlSxrHP9wMEZAky3WiWaVB1U8ffguLlcl1vIrDy-8AxyZhxPlymeUas4FJaCqDQdQFW7YAXTGH6MaXwZs9SyrkE4Q4h1zlgFgblXwDmxTTR0uQugbOXK93s7uAE-Q4GbOBO5z94uby9KtOgc0rMBEU1qq4hQVYmuV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Bar chart showing the accuracy improvement of the fine-tuned small model compared to zero-shot prompting with PaLM 2-L-Unicorn.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Our results show that the much smaller fine-tuned reward model generally performs better than zero-shot prompting a large model, even though the reward model has never seen data from the task in the test set. The only exception is logical deduction, where it performs on par with zero-shot prompting. &lt;/p>; &lt;p>; This is a very promising result as we can potentially just use a small fine-tuned reward model to perform backtracking and improve accuracy on any task, even if we don&#39;t have the data for it. This smaller reward model is completely independent of the generator LLM, and can be updated and further fine-tuned for individual use cases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s867/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;446&quot; data-original-width=&quot;867&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2HmNdzNKjZYLC_aPbRVHZmFs6_ko4Bs5CjljgTEeXipJsW0_H3HlbE-8TLCQK9GtYuUBT-liCpaZ5zi2dcbF1GkvhjouJbJBE9mVl1yUCJEZAVY8Gk8d-P_HlmeqxcPIpsKwSQeSE93LV1aimd_GuLA5VrWOYtfeLkLpEXXkrgJW5R6fV06_OBxJi6CI/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration showing how our backtracking method works.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; In this work, we created an evaluation benchmark dataset that the wider academic community can use to evaluate future LLMs. We further showed that LLMs currently struggle to find logical errors. However, if they could, we show the effectiveness of backtracking as a strategy that can provide gains on tasks. Finally, a smaller reward model can be trained on general mistake-finding tasks and be used to improve out-of-domain mistake finding, showing that mistake-finding can generalize. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Thank you to Peter Chen, Tony Mak, Hassan Mansoor and Victor Cărbune for contributing ideas and helping with the experiments and data collection. We would also like to thank Sian Gooding and Vicky Zayats for their comments and suggestions on the paper.&lt;/em>; &lt;/p>;&lt;br />;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7998118785777164574/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7998118785777164574&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/can-large-language-models-identify-and.html&quot; rel=&quot;alternate&quot; title=&quot;Can large language models identify and correct their mistakes?&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRCK2QC8HmH0lzm2lPjqVOxFPZDoyTAPq9icazR1vrsFUTDr5OJdTIZNMDgut_ylOOmeZmA4n0BTIgFCsksZ_xATbJDnQegxWMpdqv2kyGBWKMTV9E2k3WybhBzhL3-oQnpNUWWTKURxt5y8f7gGwUkPTExml1QD2U-UqW0hglZ-cXXCkznmufJIfyPAR/s72-c/Backtracking.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7197275876457161088&lt;/id>;&lt;published>;2024-01-08T14:07:00.000-08:00&lt;/published>;&lt;updated>;2024-01-19T09:59:57.076-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: User Experience Team&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Ayça Çakmakli, UX Lead, Google Research, Responsible AI and Human Centered Technology Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s16000/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google&#39;s Responsible AI User Experience (Responsible AI UX) team is a product-minded team embedded within Google Research. This unique positioning requires us to apply responsible AI development practices to our user-centered user experience (UX) design process. In this post, we describe the importance of UX design and responsible AI in product development, and share a few examples of how our team&#39;s capabilities and cross-functional collaborations have led to responsible development across Google. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; First, the UX part. We are a multi-disciplinary team of product design experts: designers, engineers, researchers, and strategists who manage the user-centered UX design process from early-phase ideation and problem framing to later-phase user-interface (UI) design, prototyping and refinement. We believe that effective product development occurs when there is clear alignment between significant unmet user needs and a product&#39;s primary value proposition, and that this alignment is reliably achieved via a thorough user-centered UX design process. &lt;/p>; &lt;p>; And second, recognizing generative AI&#39;s (GenAI) potential to significantly impact society, we embrace our role as the primary user advocate as we continue to evolve our UX design process to meet the unique challenges AI poses, maximizing the benefits and minimizing the risks. As we navigate through each stage of an AI-powered product design process, we place a heightened emphasis on the ethical, societal, and long-term impact of our decisions. We contribute to the ongoing development of comprehensive &lt;a href=&quot;https://ai.google/responsibility/ai-governance-operations&quot;>;safety and inclusivity protocols&lt;/a>; that define design and deployment guardrails around key issues like content curation, security, privacy, model capabilities, model access, equitability, and fairness that help mitigate GenAI risks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s1920/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF6fWFT9CPcFgDfbPhNuCcrNiTCCSUlP1c0Dnr_sSYCnFt3J-7j3axB8sgk34-jdo6L7Xsp9XpNSz7_xp6uEZD5_GumzOt491oPcnWsbI74tkmBNh5QQ07ra2R-1CrgcnbhVexR48bt_YVRriqIGhF_qgO_PIDseseNvOYISz6bPHjYg5zBkS5FxeM08m-/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Responsible AI UX is constantly evolving its user-centered product design process to meet the needs of a GenAI-powered product landscape with greater sensitivity to the needs of users and society and an emphasis on ethical, societal, and long-term impact.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Responsibility in product design is also reflected in the user and societal problems we choose to address and the programs we resource. Thus, we encourage the &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;prioritization of user problems with significant scale and severity&lt;/a>; to help maximize the positive impact of GenAI technology. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s1920/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrU1niTXyrjzqlRSA8w6qyV4zKtquz0QP8faCIfMp9oPYYZjNCWe0pjW3z3AE9dLMHIR8OKVmzbUlU3oRW9POZnEpmlVGK8hws3E5sgNhj9cR7bY78gGteQN1ekl9LDz61s-WQjxTcPYnxfqO6RXs-Ax-dCOIe9vn-xdX-K9Hjta1PIFDHjlvrxbXeQSk9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Communication across teams and disciplines is essential to responsible product design. The seamless flow of information and insight from user research teams to product design and engineering teams, and vice versa, is essential to good product development. One of our team&#39;s core objectives is to ensure the practical application of deep user-insight into AI-powered product design decisions at Google by bridging the communication gap between the vast technological expertise of our engineers and the user/societal expertise of our academics, research scientists, and user-centered design research experts. We&#39;ve built a multidisciplinary team with expertise in these areas, deepening our empathy for the communication needs of our audience, and enabling us to better interface between our user &amp;amp; society experts and our technical experts. We create frameworks, guidebooks, prototypes, cheatsheets, and multimedia tools to help bring insights to life for the right people at the right time.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s1920/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1080&quot; data-original-width=&quot;1920&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQrlbUi_jNRom8l7asIw8JHH12fjthtD_iPFDrWhxlItMd3L01hZpxdMx3bGEoRa3QUzHBcal0wvd3eLOKMyDZscFoIgfI4IHYdKkyaLxNhifdcl2ODH5nOV3VyYFtpCZaeze-zhCghpHY72da-OSdhvuTYrkqrYC0887_rVckCCTPCzOL-ZugV5oqHtlX/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Facilitating responsible GenAI prototyping and development &lt;/h2>; &lt;p>; During collaborations between Responsible AI UX, the &lt;a href=&quot;https://ai.googleblog.com/2023/05/responsible-ai-at-google-research-pair.html&quot;>;People + AI Research&lt;/a>; (PAIR) initiative and &lt;a href=&quot;https://labs.google/&quot;>;Labs&lt;/a>;, we identified that &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3491101.3503564&quot;>;prototyping&lt;/a>; can afford a creative opportunity to engage with large language models (LLM), and is often the first step in GenAI product development. To address the need to introduce LLMs into the prototyping process, we explored a range of different prompting designs. Then, we went out into the field, employing various external, first-person UX design research methodologies to draw out insight and gain empathy for the user&#39;s perspective. Through user/designer co-creation sessions, iteration, and prototyping, we were able to bring internal stakeholders, product managers, engineers, writers, sales, and marketing teams along to ensure that the user point of view was well understood and to reinforce alignment across teams. &lt;/p>; &lt;p>; The result of this work was &lt;a href=&quot;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&quot;>;MakerSuite&lt;/a>;, a generative AI platform launched at &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;Google I/O 2023&lt;/a>; that enables people, even those without any ML experience, to prototype creatively using LLMs. The team&#39;s first-hand experience with users and understanding of the challenges they face allowed us to incorporate our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; into the MakerSuite product design 。 Product features like &lt;a href=&quot;https://developers.generativeai.google/guide/safety_setting&quot;>;safety filters&lt;/a>;, for example, enable users to manage outcomes, leading to easier and more responsible product development with MakerSuite. &lt;/p>; &lt;p>; Because of our close collaboration with product teams, we were able to adapt text-only prototyping to support multimodal interaction with &lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI Studio&lt;/a>;, an evolution of MakerSuite. Now, Google AI Studio enables developers and non-developers alike to seamlessly leverage Google&#39;s latest &lt;a href=&quot;https://ai.google.dev/&quot;>;Gemini&lt;/a>; model to merge multiple modality inputs, like text and image, in product explorations. Facilitating product development in this way provides us with the opportunity to better use AI to identify &lt;a href=&quot;https://arxiv.org/pdf/2310.15428.pdf&quot;>;appropriateness of outcomes&lt;/a>; and unlocks opportunities for developers and non-developers to play with AI sandboxes. Together with our partners, we continue to actively push this effort in the products we support.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1406&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht40iWgBGeov03IQ-OXhPgLMF8vC9NPSZag-3PyHJMRKHRoTKOAShqTJCueqijS_jdyd7kOMQa-PjmZBQg-EqyAn3f56tucPSLjvFEmaVTuS3Eo9YYq9gIV22MqFeL0qiU4AblFB46S46Y-czdfct-2dfPCh_NaH9zMHJwUlGWRYb37XLHj6_tvqAQrsV3/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://makersuite.google.com/app/prompts/new_freeform&quot;>;Google AI studio&lt;/a>; enables developers and non-developers to leverage Google Cloud infrastructure and merge multiple modality inputs in their product explorations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Equitable speech recognition&lt;/h2>; &lt;p>; Multiple &lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.1915768117&quot;>;external studies&lt;/a>;, as well as Google&#39;s &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frai.2021.725911/full&quot;>;own research,&lt;/a>; have identified an unfortunate deficiency in the ability of current speech recognition technology to understand Black speakers on average, relative to White speakers. As multimodal AI tools begin to rely more heavily on speech prompts, this problem will grow and continue to alienate users. To address this problem, the Responsible AI UX team is &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with world-renowned linguists and scientists at Howard University&lt;/a>;, a prominent &lt;a href=&quot;https://en.wikipedia.org/wiki/Historically_black_colleges_and_universities&quot; target=&quot;_blank&quot;>;HBCU&lt;/a>;, to build a high quality African-American English dataset to improve the design of our speech technology products to make them more accessible. Called Project Elevate Black Voices, this effort will allow Howard University to share the dataset with those looking to improve speech technology while establishing a framework for responsible data collection, ensuring the data benefits Black communities. Howard University will retain the ownership and licensing of the dataset and serve as stewards for its responsible use. At Google, we&#39;re providing funding support and collaborating closely with our partners at Howard University to ensure the success of this program. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/t_pdlrU8qhs?si=5xY1AoGc_d2HTzQf&quot; width=&quot;640&quot; youtube-src-id=&quot;5xY1AoGc_d2HTzQf&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Equitable computer vision&lt;/h2>; &lt;p>; The &lt;a href=&quot;http://gendershades.org/&quot;>;Gender Shades&lt;/a>; project highlighted that computer vision systems struggle to detect people with darker skin tones, and performed particularly poorly for women with darker skin tones. This is largely due to the fact that the datasets used to train these models were not inclusive to a wide range of skin tones. To address this limitation, the Responsible AI UX team has been partnering with sociologist &lt;a href=&quot;https://www.ellismonk.com/&quot;>;Dr. Ellis Monk&lt;/a>; to release the &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Monk Skin Tone Scale&lt;/a>; (MST), a skin tone scale designed to be more inclusive of the spectrum of skin tones around the world. It provides a tool to assess the inclusivity of datasets and model performance across an inclusive range of skin tones, resulting in features and products that work better for everyone. &lt;/p>; &lt;p>; We have integrated MST into a range of &lt;a href=&quot;https://blog.google/products/search/monk-skin-tone-scale/&quot;>;Google products&lt;/a>;, such as Search, Google Photos, and others. We also open sourced MST, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3632120&quot;>;published our research&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/05/consensus-and-subjectivity-of-skin-tone_15.html&quot;>;described our annotation practices&lt;/a>;, and &lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;shared an example dataset&lt;/a>; to encourage others to easily integrate it into their products. The Responsible AI UX team continues to collaborate with Dr. Monk, utilizing the MST across multiple product applications and continuing to do international research to ensure that it is globally inclusive. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Consulting &amp;amp; guidance&lt;/h2>; &lt;p>; As teams across Google continue to develop products that leverage the capabilities of GenAI models, our team recognizes that the challenges they face are varied and that market competition is significant. To support teams, we develop actionable assets to facilitate a more streamlined and responsible product design process that considers available resources. We act as a product-focused design consultancy, identifying ways to scale services, share expertise, and apply our design principles more broadley. Our goal is to help all product teams at Google connect significant unmet user needs with technology benefits via great responsible product design. &lt;/p>; &lt;p>; One way we have been doing this is with the creation of the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>;, an evolving summative resource of many of the responsible design lessons we&#39;ve learned and recommendations we&#39;ve made for internal and external stakeholders. With its forthcoming, rolling &lt;a href=&quot;https://medium.com/people-ai-research/updating-the-people-ai-guidebook-in-the-age-of-generative-ai-cace6c846db4&quot;>;updates&lt;/a>; focusing specifically on how to best design and consider user needs with GenAI, we hope that our internal teams, external stakeholders, and larger community will have useful and actionable guidance at the most critical milestones in the product development journey. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s1100/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;622&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOMslkRaMAkYwAUCOHS5tWTuCBQJ7sPNjkDbopWKKl21XD_1Q_VrK3tCctspa72hY63uOMZKipV5flHTW69S5bVjCVBvE8oMKmEax3VWNj7Wx20UlYRPZABdJhq0DJlegrtSIbQBxtkf18ygJtSxk2kY5f8L82WKEw9vLmiRDgrZiIXtsJ5RtbgvmISRw4/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The People + AI Guidebook has six chapters, designed to cover different aspects of the product life cycle.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If you are interested in reading more about Responsible AI UX and how we are specifically thinking about designing responsibly with Generative AI, please check out this &lt;a href=&quot;https://medium.com/people-ai-research/meet-ay%C3%A7a-%C3%A7akmakli-googles-new-head-of-responsible-ai-ux-d8f2700df95b&quot;>;Q&amp;amp;A piece&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Shout out to our the Responsible AI UX team members: Aaron Donsbach, Alejandra Molina, Courtney Heldreth, Diana Akrong, Ellis Monk, Femi Olanubi, Hope Neveux, Kafayat Abdul, Key Lee, Mahima Pushkarna, Sally Limb, Sarah Post, Sures Kumar Thoddu Srinivasan, Tesh Goyal, Ursula Lauriston, and Zion Mengesha. Special thanks to Michelle Cohn for her contributions to this work. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7197275876457161088/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7197275876457161088&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2024/01/responsible-ai-at-google-research-user.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: User Experience Team&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhssQETGEGXjqvZARNVbKQATaocb0RDAkEOzrkJXtbqiJhZ0_hAAeb8zgOqbiGutvlvU1BTaE96y-0Mc6xXX-bP1-xyVa6xPsmmSwqxZlk_nn6UgmycZGYztCOgV1G3IKT9YCnmKFggXUmrEKFW1Y9NtfXNOHmSfaLoIxk8UxQked-9QDeDkSOCZMBaIYrL/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8816183473385638131&lt;/id>;&lt;published>;2023-12-22T10:37:00.000-08:00&lt;/published>;&lt;updated>;2024-01-11T16:01:33.313-08:00&lt;/updated>;&lt;title type=&quot;text&quot;>;2023: A year of groundbreaking advances in AI and computing&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jeff Dean, Chief Scientist, Google DeepMind &amp;amp; Google Research, Demis Hassabis, CEO, Google DeepMind, and James Manyika, SVP, Google Research, Technology &amp;amp; Society&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU12G_p2DZO4gQ-P95aP2IFvtKRHRG5Oik9VzJ4WtT_VznggjUrL4tTzqto-C8yx6ULgvugKgH9usiZxnKGk97pOFyHWnu-1S4sSbR2Jjq5T36tQRbZucTAP7gmXkGw77xN6s39IKxPaxbo5tw_Cq52ZfPWOCBkWL-2XTuzsIh6viRIqgGcdUdNq-pHjzl/s1100/year_in_review-hero.jpg&quot; style=&quot;display ： 没有任何;” />; &lt;p>; This has been a year of incredible progress in the field of Artificial Intelligence (AI) research and its practical applications. &lt;/p>; &lt;p>; As ongoing research pushes AI even farther, we look back to our &lt;a href=&quot;https://ai.google/static/documents/google-why-we-focus-on-ai.pdf&quot;>;perspective&lt;/a>; published in January of this year, titled “Why we focus on AI (and to what end),” where we noted: &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; We are committed to leading and setting the standard in developing and shipping useful and beneficial applications, applying ethical principles grounded in human values, and evolving our approaches as we learn from research, experience, users, and the wider community. &lt;/p>; &lt;p>; We also believe that getting AI right — which to us involves innovating and delivering widely accessible benefits to people and society, while mitigating its risks — must be a collective effort involving us and others, including researchers, developers, users (individuals, businesses, and other organizations), governments, regulators, and citizens. &lt;/p>; &lt;p>; We are convinced that the AI-enabled innovations we are focused on developing and delivering boldly and responsibly are useful, compelling, and have the potential to assist and improve lives of people everywhere — this is what compels us. &lt;/p>; &lt;/div>; &lt;p>; In this Year-in-Review post we&#39;ll go over some of Google Research&#39;s and Google DeepMind&#39;s efforts putting these paragraphs into practice safely throughout 2023. &lt;/p>; &lt;br />; &lt;h2>;Advances in products &amp;amp; technologies &lt;/h2>; &lt;p>; This was the year generative AI captured the world&#39;s attention, creating imagery, music, stories, and engaging conversation about everything imaginable, at a level of creativity and a speed almost implausible a few years ago. &lt;/p>; &lt;p>; In February, we &lt;a href=&quot;https://blog.google/technology/ai/bard-google-ai-search-updates/&quot;>;first launched&lt;/a>; &lt;a href=&quot;https://bard.google.com&quot;>;Bard&lt;/a>;, a tool that you can use to explore creative ideas and explain things simply. It can generate text, translate languages, write different kinds of creative content and more. &lt;/p>; &lt;p>; In May, we watched the results of months and years of our foundational and applied work announced on stage &lt;a href=&quot;https://blog.research.google/2023/05/google-research-at-io-2023.html&quot;>;at Google I/O&lt;/a>;. Principally, this included &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>;, a large language model (LLM) that brought together compute-optimal scaling, an improved dataset mixture, and model architecture to excel at advanced reasoning tasks. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5R7E2f0BQIsiLZm25kxfR_Ix_Bvm6HlQQCXI12sB42siKUAf8eZvVEDU5bi8EQc22BoG6SV_H8NbC-PKd2pPv7FhC-uBR43ZWpbrgvaGJ7699j-uUctPbFBO9Bf-u81gkfU1OP4oGZhs6KKkub2znNsvcElREn5kwKh2npPJxFJdSVZZUIPZhyphenhyphen3B_jCq4/s1920/lockup_ic_PaLM-2_H_4297x745px_clr_@1x.jpg&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;555&quot; data-original-width=&quot;1920&quot; height=&quot;116&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5R7E2f0BQIsiLZm25kxfR_Ix_Bvm6HlQQCXI12sB42siKUAf8eZvVEDU5bi8EQc22BoG6SV_H8NbC-PKd2pPv7FhC-uBR43ZWpbrgvaGJ7699j-uUctPbFBO9Bf-u81gkfU1OP4oGZhs6KKkub2znNsvcElREn5kwKh2npPJxFJdSVZZUIPZhyphenhyphen3B_jCq4/w400-h116/lockup_ic_PaLM-2_H_4297x745px_clr_@1x.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/div>; &lt;p>;By fine-tuning and instruction-tuning PaLM 2 for different purposes, we were able to integrate it into numerous Google products and features, including:&lt;/p>; &lt;ul>; &lt;li>;An update to Bard, which enabled multilingual capabilities. Since its initial launch, Bard is now available in more than &lt;a href=&quot;https://support.google.com/bard/answer/13575153?hl=en&quot;>;40 languages and over 230 countries and territories&lt;/a>;, and &lt;a href=&quot;https://blog.google/products/bard/google-bard-new-features-update-sept-2023/&quot;>;with extensions&lt;/a>;, Bard can find and show relevant information from Google tools used every day — like Gmail, Google Maps, YouTube, and more. &lt;/li>; &lt;li>;&lt;a href=&quot;https://blog.google/products/search/generative-ai-search/&quot;>;Search Generative Experience&lt;/a>; (SGE), which uses LLMs to reimagine both how to organize information and how to help people navigate through it, creating a more fluid, conversational interaction model for our core Search product. This work extended the search engine experience from primarily focused on information retrieval into something much more — capable of retrieval, synthesis, creative generation and continuation of previous searches — while continuing to serve as a connection point between users and the web content they seek. &lt;/li>; &lt;li>;&lt;a href=&quot;https://google-research.github.io/seanet/musiclm/examples/&quot;>;MusicLM&lt;/a>;, a text-to-music model powered by &lt;a href=&quot;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&quot;>;AudioLM&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2208.12415&quot;>;MuLAN&lt;/a>;, which can make music from text, humming, images or video and musical accompaniments to singing. &lt;/li>; &lt;li>;Duet AI, our AI-powered collaborator that provides users with assistance when they use Google Workspace and Google Cloud. &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai&quot;>;Duet AI in Google Workspace&lt;/a>;, for example, helps users write, create images, analyze spreadsheets, draft and summarize emails and chat messages, and summarize meetings. &lt;a href=&quot;https://cloud.google.com/blog/products/application-modernization/introducing-duet-ai-for-google-cloud&quot;>;Duet AI in Google Cloud&lt;/a>; helps users code, deploy, scale, and monitor applications, as well as identify and accelerate resolution of cybersecurity threats. &lt;/li>; &lt;li>;And many &lt;a href=&quot;https://blog.google/technology/developers/google-io-2023-100-announcements/&quot;>;other developments&lt;/a>;. &lt;/li>; &lt;/ul>; &lt;p>; In June, following last year&#39;s release of our text-to-image generation model &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, we released &lt;a href=&quot;https://blog.research.google/2023/06/imagen-editor-and-editbench-advancing.html&quot;>;Imagen Editor&lt;/a>;, which provides the ability to use region masks and natural language prompts to interactively edit generative images to provide much more precise control over the model output. &lt;/p>; &lt;div style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU-6oYBC5383noYqno-cYzUboAAOgXvDlWtqwu-zl94M2r02Fsid0jjgLBJl3JCVR4lc8ZVw7-c7q9OlHjzmrRXz5i5H/s1261/image4.png&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;306&quot; data-original-width=&quot;1261&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfNnxCCKKLZgjajw30Ml1CiPruUIScAPwpa77LQ8Auqkf_KJh9rlJWhYRnQf1g4L0qhVDUJNHabkpL_ZW60FJs8XUWV1kT5M32YU-6oYBC5383noYqno-cYzUboAAOgXvDlWtqwu-zl94M2r02Fsid0jjgLBJl3JCVR4lc8ZVw7-c7q9OlHjzmrRXz5i5H/s16000/image4.png&quot; />;&lt;/a>;&lt;/div>; &lt;p>; Later in the year, we released Imagen 2, which improved outputs via a specialized image aesthetics model based on human preferences for qualities such as good lighting, framing, exposure, and sharpness. &lt;/p>; &lt;p>; In October, we launched a feature that &lt;a href=&quot;https://blog.research.google/2023/10/google-search-can-now-help-with-english-speaking-practice.html&quot;>;helps people practice speaking and improve their language skills&lt;/a>;. The key technology that enabled this functionality was a novel deep learning model developed in collaboration with the Google Translate team, called Deep Aligner. This single new model has led to dramatic improvements in alignment quality across all tested language pairs, reducing average alignment error rate from 25% to 5% compared to alignment approaches based on &lt;a href=&quot;https://aclanthology.org/C96-2141/&quot;>;Hidden Markov models&lt;/a>; (HMMs). &lt;/p>; &lt;p>; In November, in partnership with &lt;a href=&quot;https://blog.youtube/inside-youtube/ai-and-music-experiment/&quot;>;YouTube&lt;/a>;, we announced &lt;a href=&quot;https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/&quot;>;Lyria&lt;/a>;, our most advanced AI music generation model to date. We released two experiments designed to open a new playground for creativity, DreamTrack and music AI tools, in concert with &lt;a href=&quot;https://blog.youtube/inside-youtube/partnering-with-the-music-industry-on-ai/&quot;>;YouTube&#39;s Principles for partnering with the music industry on AI technology&lt;/a>;. &lt;/p>; &lt;p>; Then in December, we launched &lt;a href=&quot;https://blog.google/technology/ai/google-gemini-ai/&quot;>;Gemini&lt;/a>;, our most capable and general AI模型。 Gemini was built to be multimodal from the ground up across text, audio, image and videos. Our initial family of Gemini models comes in three different sizes, Nano, Pro, and Ultra. Nano models are our smallest and most efficient models for powering on-device experiences in products like Pixel. The Pro model is highly-capable and best for scaling across a wide range of tasks. The Ultra model is our largest and most capable model for highly complex tasks. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://www.youtube.com/watch?v=jV1vkHv4zq8&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/jV1vkHv4zq8&quot; width=&quot;640&quot; youtube-src-id=&quot;jV1vkHv4zq8&quot;>;&lt;/iframe>;&lt;/a>;&lt;/div>; &lt;br />; &lt;p>;In a &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&quot;>;technical report&lt;/a>; about &lt;a href=&quot;https://deepmind.google/technologies/gemini&quot;>;Gemini models&lt;/a>;, we showed that Gemini Ultra&#39;s performance exceeds current state-of-the-art results on 30 of the 32 widely-used academic benchmarks used in LLM research and development. With a score of 90.04%, Gemini Ultra was the first model to outperform human experts on &lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>;MMLU&lt;/a>;, and achieved a state-of-the-art score of 59.4% on the new &lt;a href=&quot;https://arxiv.org/abs/2009.03300&quot;>;MMMU&lt;/a>; benchmark. &lt;/p>; &lt;p>; Building on &lt;a href=&quot;https://deepmind.google/discover/blog/competitive-programming-with-alphacode/&quot;>;AlphaCode&lt;/a>;, the first AI system to perform at the level of the median competitor in competitive programming, we &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf&quot;>;introduced AlphaCode 2&lt;/a>; powered by a specialized version of Gemini 。 When evaluated on the same platform as the original AlphaCode, we found that AlphaCode 2 solved 1.7x more problems, and performed better than 85% of competition participants &lt;/p>; &lt;p>; At the same time, &lt;a href=&quot;https://blog.google/products/bard/google-bard-try-gemini-ai/&quot;>;Bard got its biggest upgrade&lt;/a>; with its use of the Gemini Pro model, making it far more capable at things like understanding, summarizing, reasoning, coding, and planning. In six out of eight benchmarks, Gemini Pro outperformed GPT-3.5, including in MMLU, one of the key standards for measuring large AI models, and &lt;a href=&quot;https://huggingface.co/datasets/gsm8k&quot;>;GSM8K&lt;/a>;, which measures grade school math reasoning. Gemini Ultra will come to Bard early next year through Bard Advanced, a new cutting-edge AI experience. &lt;/p>; &lt;p>; Gemini Pro is also available on &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/gemini-support-on-vertex-ai&quot;>;Vertex AI&lt;/a>;, Google Cloud&#39;s end-to-end AI platform that empowers developers to build applications that can process information across text, code, images, and video. &lt;a href=&quot;https://blog.google/technology/ai/gemini-api-developers-cloud/&quot;>;Gemini Pro was also made available in AI Studio&lt;/a>; in December. &lt;/p>; &lt;p>; To best illustrate some of Gemini&#39;s capabilities, we produced a &lt;a href=&quot;https://deepmind.google/technologies/gemini/#hands-on&quot;>;series of short videos&lt;/a>; with explanations of how Gemini could: &lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=sPiOP_CB54A&quot;>;Unlock insights in scientific literature&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=LvGmVmHv69s&amp;amp;t=1s&quot;>;Excel at competitive programming&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=D64QD7Swr3s&quot;>;Process and understand raw audio&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=K4pX1VAxaAI&quot;>;Explain reasoning in math and physics&lt;/a>; &lt;/li>;&lt;li>;&lt;a href=&quot;https://www.youtube.com/watch?v=v5tRc_5-8G4&quot;>;Reason about user intent to generate bespoke experiences&lt;/a>; &lt;/li>; &lt;/ul>; &lt;br />; &lt;h2>;ML/AI Research&lt;/h2>; &lt;p>; In addition to our advances in products and technologies, we&#39;ve also made a number of important advancements in the broader fields of machine learning and AI research. &lt;/p>; &lt;p>; At the heart of the most advanced ML models is the Transformer model architecture, &lt;a href=&quot;https://blog.research.google/2017/08/transformer-novel-neural-network.html&quot;>;developed by Google researchers in 2017&lt;/a>;. Originally developed for language, it has proven useful in domains as varied as &lt;a href=&quot;https://blog.research.google/2020/12/transformers-for-image-recognition-at.html&quot;>;computer vision&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/&quot;>;audio&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/&quot;>;genomics&lt;/a>;, &lt;a href=&quot;https://deepmind.google/technologies/alphafold/&quot;>;protein folding&lt;/a>;, and more. This year, our work on &lt;a href=&quot;https://blog.research.google/2023/03/scaling-vision-transformers-to-22.html&quot;>;scaling vision transformers&lt;/a>; demonstrated state-of-the-art results across a wide variety of vision tasks, and has also been useful in building &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;more capable robots&lt;/a>;. &lt;/p>; &lt;p>; &lt;/p>; &lt;p>; Expanding the versatility of models requires the ability to perform higher-level and multi-step reasoning. This year, we approached this target following several research tracks. For example, &lt;a href=&quot;https://blog.research.google/2023/08/teaching-language-models-to-reason.html&quot;>;algorithmic prompting&lt;/a>; is a new method that teaches language models reasoning by demonstrating a sequence of algorithmic steps, which the model can then apply in new contexts. This approach improves accuracy on one middle-school mathematics benchmark from 25.9% to 61.1%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s1200/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;166&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqMgWRH7DtSwqbAImqRRsW26oyKnDiinTNvtkUuvASZJSaChsNXG1-4EeDkTr22E7xjRzwcFdWCZSKFuuBoLfsZiH27pZ1d6XMef8ns6RGx619oZnHdeCVZb7EOPWigNqbGsmu4FrU2Xgampr0HIASv7ks8ha9DE8L3hmAhKU8_Aps8L_1evceD2MKyG23/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;By providing algorithmic prompts, we can teach a model the rules of arithmetic via in-context learning.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the domain of visual question answering, in a collaboration with UC Berkeley researchers, we showed how we could &lt;a href=&quot;https://blog.research.google/2023/07/modular-visual-question-answering-via.html&quot;>;better answer complex visual questions&lt;/a>; (“Is the carriage to the right of the horse?”) by combining a visual model with a language model trained to answer visual questions by synthesizing a program to perform multi-step reasoning. &lt;/p>; &lt;p>; We are now using a &lt;a href=&quot;https://blog.research.google/2023/05/large-sequence-models-for-software.html&quot;>;general model that understands many aspects of the software development life cycle&lt;/a>; to automatically generate code review comments, respond to code review comments, make performance-improving suggestions for pieces of code (by learning from past such changes in other contexts), fix code in response to compilation错误等等。 &lt;/p>; &lt;p>; In a multi-year research collaboration with the Google Maps team, we were able to scale inverse reinforcement learning and apply it to the &lt;a href=&quot;https://blog.research.google/2023/09/world-scale-inverse-reinforcement.html&quot;>;world-scale problem of improving route suggestions&lt;/a>; for over 1 billion users. Our work culminated in a 16–24% relative improvement in global route match rate, helping to ensure that routes are better aligned with user preferences. &lt;/p>; &lt;p>; We also continue to work on techniques to improve the inference performance of machine learning models. In work on &lt;a href=&quot;https://blog.research.google/2023/08/neural-network-pruning-with.html&quot;>;computationally-friendly approaches to pruning connections in neural networks&lt;/a>;, we were able to devise an approximation algorithm to the computationally intractable best-subset selection problem that is able to prune 70% of the edges from an image classification model and still retain almost all of the accuracy of the original. &lt;/p>; &lt;p>; In work on &lt;a href=&quot;https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html&quot;>;accelerating on-device diffusion models&lt;/a>;, we were also able to apply a variety of optimizations to attention mechanisms, convolutional kernels, and fusion of operations to make it practical to run high quality image generation models on-device; for example, enabling “a photorealistic and high-resolution image of a cute puppy with surrounding flowers” to be generated in just 12 seconds on a smartphone. &lt;/p>; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/s522/image7.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;522&quot; data-original-width=&quot;270&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM5NuphyphenhyphenlH7_H5PfABZrn1a-CuFJm8XyEpAlodQqpcrTvYj3XNWarRULjbSgKqY1trCsC0hbVvHTU9l4pUyn3vFupsfyLDVgdMgsTYHtE1b8AWQwZWUgXGAAJ_F12rcOqVDjbe1q5OX0TdYEQBOt-FpKIqvfYivuAcPU3bVTZcYxUdFOdtaW5JE6Fii7ej/w208-h400/image7.gif&quot; width=&quot;208&quot; />;&lt;/a>;&lt;/div>; &lt;br />; &lt;p>;Advances in capable language and multimodal models have also benefited our robotics research efforts. We combined separately trained language, vision, and robotic control models into &lt;a href=&quot;https://blog.research.google/2023/03/palm-e-embodied-multimodal-language.html&quot;>;PaLM-E&lt;/a>;, an embodied multi-modal model for robotics, and &lt;a href=&quot;https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/&quot;>;Robotic Transformer 2&lt;/a>; (RT-2), a novel vision-language-action (VLA) model that &lt;a href=&quot;https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/&quot;>;learns&lt;/a>; from both web and robotics data, and translates this knowledge into generalized instructions for robotic control.&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s616/image8.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;559&quot; data-original-width=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJL6HUJ6swdYZlAdRsPttHH9EmdE7TpGlK92U9hxHu29ANiHMQLC3QB1PX8HFWwatiJ6V-rjeImQ67oRUGtQMR4TDXB7mOVqsz-BouN1y29vbUU7rc-nTkj2H-V0V3VNCTMujhLSyNM3duUEVOYhm0nf8wBVrIghfEdpRsKtHn_gg2dWVxzzSXTk6ROTb/s16000/image8.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;RT-2 architecture and training: We co-fine-tune a pre-trained vision-language model on robotics and web data. The resulting model takes in robot camera images and directly predicts actions for a robot to perform.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore, we showed how &lt;a href=&quot;https://blog.research.google/2023/08/saytap-language-to-quadrupedal.html&quot;>;language can also be used to control the gait of quadrupedal robots&lt;/a>; and explored the &lt;a href=&quot;https://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html&quot;>;use of language to help formulate more explicit reward functions&lt;/a>; to bridge the gap between human language and robotic actions. Then, in &lt;a href=&quot;https://blog.research.google/2023/05/barkour-benchmarking-animal-level.html&quot;>;Barkour&lt;/a>; we benchmarked the agility limits of quadrupedal robots.&lt;/p>; &lt;br />; &lt;h2>;Algorithms &amp;amp; optimization&lt;/h2>; &lt;p>; Designing efficient, robust, and scalable algorithms remains a high priority. This year, our work included: applied and scalable algorithms, market algorithms, system efficiency and optimization, and privacy. &lt;/p>; &lt;p>; We introduced &lt;a href=&quot;https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/&quot;>;AlphaDev&lt;/a>;, an AI system that uses reinforcement learning to discover enhanced computer science algorithms. AlphaDev uncovered a faster algorithm for sorting, a method for ordering data, which led to improvements in the LLVM libc++ sorting library that were up to 70% faster for shorter sequences and about 1.7% faster for sequences exceeding 250,000 elements. &lt;/p>; &lt;p>; We developed a novel model to &lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;predict the properties of large graphs&lt;/a>;, enabling estimation of performance for large programs. We released a new dataset, &lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TPUGraphs&lt;/a>;, to accelerate &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime&quot;>;open research in this area&lt;/a>;, and showed how we can use &lt;a href=&quot;https://blog.research.google/2023/12/advancements-in-machine-learning-for.html&quot;>;modern ML to improve ML efficiency&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/s1223/image11.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1042&quot; data-original-width=&quot;1223&quot; height=&quot;341&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC00uLmFXRBzNXoaXirQMkAV7j91dBVwgvY4BEFuOLP4V9MquLuKt9imKFHBHsc7laEKUIuXUe_-v0DJyCauIQMrOzUaSOKl15hxBeAbgLGPWNYehM7Z8seK3P9JcjyMmeSZNyXWMYNME84KZwIygF1deRSpaZ1oBeK-uFnxEqCJcm6M0z4hA18JVGew-H/w400-h341/image11.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The TPUGraphs dataset has 44 million graphs for ML program optimization.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;We developed a new &lt;a href=&quot;https://en.wikipedia.org/wiki/Load_balancing_(computing)&quot;>;load balancing&lt;/a>; algorithm for distributing queries to a server, called &lt;a href=&quot;https://arxiv.org/abs/2312.10172&quot;>;Prequal&lt;/a>;, which minimizes a combination of requests-in-flight and estimates the latency. Deployments across several systems have saved CPU, latency, and RAM significantly. We also designed a new &lt;a href=&quot;https://arxiv.org/abs/2305.02508&quot;>;analysis framework&lt;/a>; for the classical caching problem with capacity reservations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s1896/image12.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1896&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRpdeenR8s32zMYfc6hmCq1OKu_Fk8NnhymDBWweQky1o9OIqARGqtzA-SUPAX1UZVQsrvPDXXbr20ZBz70RNBS3njSwCtkGYmBbWYOV7J87xFTMXCDRoiOh4EtGqf_aKBtegTJrbyru3ompVpfYzMx6EKWX26xHs_POZJ2dd3lbvDX-JggUoXFDn-HDJa/s16000/image12.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Heatmaps of normalized CPU usage transitioning to&amp;nbsp;&lt;a href=&quot;Prequal&quot;>;Prequal&lt;/a>;&amp;nbsp;at 08:00.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We improved state-of-the-art in clustering and &lt;a href=&quot;https://en. wikipedia.org/wiki/Graph_neural_network&quot;>;graph algorithms&lt;/a>; by developing new techniques for&lt;a href=&quot;https://arxiv.org/abs/2106.05513&quot;>; computing minimum-cut&lt;/a>;, &lt;a href =&quot;https://arxiv.org/abs/2309.17243&quot;>;approximating correlation clustering&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2308.00503&quot;>;massively parallel graph clustering&lt;/a>; 。 Additionally, we introduced&lt;a href=&quot;https://arxiv.org/abs/2308.03578&quot;>; TeraHAC&lt;/a>;, a novel hierarchical clustering algorithm for trillion-edge graphs, designed a &lt;a href=&quot;https://blog.research.google/2023/11/best-of-both-worlds-achieving.html&quot;>;text clustering algorithm&lt;/a>; for better scalability while maintaining quality, and designed the most efficient &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot;>;algorithm for approximating the Chamfer Distance&lt;/a>;, the standard similarity function for multi-embedding models, offering &amp;gt;50× speedups over highly-optimized exact algorithms and scaling to billions of points.&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; We continued optimizing Google&#39;s large embedding models (LEMs), which power many of our core products and recommender systems. Some new techniques include &lt;a href=&quot;https://arxiv.org/abs/2305.12102&quot;>;Unified Embedding&lt;/a>; for battle-tested feature representations in web-scale ML systems and &lt;a href=&quot;https://arxiv.org/abs/2209.14881&quot;>;Sequential Attention&lt;/a>;, which uses attention mechanisms to discover high-quality sparse model architectures during training. &lt;/p>; &lt;!--&lt;p>; This year, we also continued our research in market algorithms to design computationally efficient marketplaces and causal inference. First, we remain committed to advancing the rapidly growing interest in ads automation for which our recent work &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3543507.3583416&quot;>;explains the adoption of autobidding mechanisms&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597725&quot;>;examines the effect of different auction formats on the incentives of advertisers&lt;/a>;. In the multi-channel setting, our findings shed light on how the choice between local and global optimizations affects the design of multi-channel &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3580507.3597707&quot;>;auction systems&lt;/a>; and &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618709&quot;>;bidding systems&lt;/a>;. &lt;/p>;-->; &lt;p>; Beyond auto-bidding systems, we also studied auction design in other complex settings, such as &lt;a href=&quot;https://arxiv.org/abs/2204.01962&quot;>;buy-many mechanisms&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2207.09429&quot;>;auctions for heterogeneous bidders&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2309.10766&quot;>;contract designs&lt;/a>;, and innovated &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3618408.3618478&quot;>;robust online bidding algorithms&lt;/a>;. Motivated by the application of generative AI in collaborative creation (eg, joint ad for advertisers), we proposed &lt;a href=&quot;https://arxiv.org/abs/2310.10826&quot;>;a novel token auction model &lt;/a>;where LLMs bid for influence in the collaborative AI creation. Finally, we show how to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3580507.3597702&quot;>;mitigate personalization effects in experimental design&lt;/a>;, which, for example, may cause recommendations to drift over time. &lt;/p>; &lt;p>; The Chrome Privacy Sandbox, a multi-year collaboration between Google Research and Chrome, has publicly launched several APIs, including for &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#protected-audience&quot;>;Protected Audience&lt;/a>;, &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#topics&quot;>;Topics&lt;/a>;, and &lt;a href=&quot;https://privacysandbox.com/intl/en_us/learning-hub/#attribution-reporting&quot;>;Attribution Reporting&lt;/a>;. This is a major step in protecting user privacy while supporting the open and free web ecosystem. These efforts have been facilitated by fundamental research on &lt;a href=&quot;https://arxiv.org/abs/2304.07210&quot;>;re-identification risk&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2301.05605&quot;>;private streaming computation&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot;>;optimization&lt;/a>; of privacy caps and budgets, &lt;a href=&quot;https://arxiv.org/pdf/2308.13510.pdf&quot;>;hierarchical aggregation&lt;/a>;, and training models with &lt;a href=&quot;https://arxiv.org/pdf/2312.05659.pdf&quot;>;label privacy&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Science and society&lt;/h2>; &lt;p>; In the not too distant future, there is a very real possibility that AI applied to scientific problems can accelerate the rate of discovery in certain domains by 10× or 100×, or more, and lead to major advances in diverse areas including bioengineering, &lt;a href=&quot;https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning&quot;>;materials science&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/&quot;>;weather prediction&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;climate forecasting&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot;>;neuroscience&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/an-ml-based-approach-to-better.html&quot;>;genetic medicine&lt;/a>;, and &lt;a href=&quot;https://health.google/health-research/publications/&quot;>;healthcare&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Sustainability and climate change &lt;/h3>; &lt;p>; In &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-reduce-greenhouse-emissions-project-greenlight/&quot;>;Project Green Light&lt;/a>;, we partnered with 13 cities around the world to help improve traffic flow at intersections and reduce stop-and-go emissions. Early numbers from these partnerships indicate a potential for up to 30% reduction in stops and up to 10% reduction in emissions. &lt;/p>; &lt;p>; In our &lt;a href=&quot;https://sites.research.google/contrails/&quot;>;contrails work&lt;/a>;, we analyzed large-scale weather data, historical satellite images, and past flights 。 We &lt;a href=&quot;https://blog.google/technology/ai/ai-airlines-contrails-climate-change/&quot;>;trained an AI model&lt;/a>; to predict where contrails form and reroute airplanes accordingly. In partnership with American Airlines and Breakthrough Energy, we used this system to demonstrate contrail reduction by 54%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s957/image14.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;400&quot; data-original-width=&quot;957&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnU7wUIVSDG3HicS_tHszwAD_RlhU_SXJO0quz5CUJ0RLT03erljh8ckLW8NLAlYtOPpX6lzsohacC7x2X-_2abBGDGWGpIN0KZpYJ0YH3FOzRDhq-zVTeXne4LjpKGPoDtNtljKkee2R4hE3ju3wYhltF5q8oaPZ1I9R39eB2uYBnFfRxjka1vCg8H5Vv/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Contrails detected over the United States using AI and GOES-16 satellite imagery.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We are also developing novel technology-driven approaches to &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;help communities with the effects of climate change&lt;/a>;. For example, we have &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;expanded our flood forecasting coverage to 80 countries&lt;/a>;, which directly impacts more than 460 million people. We have initiated a &lt;a href=&quot;https://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot;>;number of research efforts&lt;/a>; to help mitigate the increasing danger of wildfires, including &lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;real-time tracking of wildfire boundaries&lt;/a>; using satellite imagery, and work that &lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot;>;improves emergency evacuation plans&lt;/a>; for communities at risk to rapidly-spreading wildfires. Our &lt;a href=&quot;https://www.americanforests.org/article/american-forests-unveils-updates-for-tree-equity-score-tool-to-address-climate-justice/&quot;>;partnership&lt;/a>; with American Forests puts data from our &lt;a href=&quot;https://insights.sustainability.google/places/ChIJVTPokywQkFQRmtVEaUZlJRA/trees?hl=en-US&quot;>;Tree Canopy&lt;/a>; project to work in their &lt;a href=&quot;https://treeequityscore.org/&quot;>;Tree Equity Score&lt;/a>; platform, helping communities identify and address unequal access to trees.&lt;/p>; &lt;p>; Finally, we continued to develop better models for weather prediction at longer time horizons. Improving on &lt;a href=&quot;https://blog.research.google/2020/03/a-neural-weather-model-for-eight-hour.html&quot;>;MetNet&lt;/a>; and &lt;a href=&quot;https ://blog.research.google/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;MetNet-2&lt;/a>;, in this year&#39;s work on &lt;a href=&quot;https: //blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html&quot;>;MetNet-3&lt;/a>;, we now outperform traditional numerical weather simulations up to twenty-four hours 。 In the area of medium-term, global weather forecasting, our work on &lt;a href=&quot;https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/&quot;>;GraphCast&lt;/a>; showed significantly better prediction accuracy for up to 10 days compared to &lt;a href=&quot;https://en.wikipedia.org/wiki/Integrated_Forecast_System&quot;>;HRES&lt;/a>;, the most accurate operational deterministic forecast, produced by the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&lt;/a>; (ECMWF). In collaboration with ECMWF, we released &lt;a href=&quot;https://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot;>;WeatherBench-2&lt;/a>;, a benchmark for evaluating the accuracy of weather forecasts in a common framework. &lt;/p>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/Q6fOlW-Y_Ss&quot; width=&quot;640&quot; youtube-src-id=&quot;Q6fOlW-Y_Ss&quot;>;&lt;/iframe>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A selection of GraphCast&#39;s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Health and the life sciences&lt;/h3>; &lt;p>; The potential of AI to dramatically improve processes in healthcare is significant. Our initial &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06291-2&quot;>;Med-PaLM&lt;/a>; model was the first model capable of achieving a passing score on the US medical licensing exam. Our more recent &lt;a href=&quot;https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/&quot;>;Med-PaLM 2 model&lt;/a>; improved by a further 19%, achieving an expert-level accuracy of 86.5%. These &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM models&lt;/a>; are language-based, enable clinicians to ask questions and have a dialogue about complex medical conditions, and are &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/introducing-medlm-for-the-healthcare-industry&quot;>;available&lt;/a>; to healthcare organizations as part of &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview&quot;>;MedLM&lt;/a>; through Google Cloud. &lt;/p>; &lt;p>; In the same way our general language models are evolving to handle multiple modalities, we have recently shown research on a &lt;a href=&quot;https://blog.research.google/2023/08/multimodal-medical-ai.html&quot;>;multimodal version of Med-PaLM&lt;/a>; capable of interpreting medical images, textual data, and other modalities, &lt;a href=&quot;https://arxiv.org/abs/2307.14334&quot;>;describing a path&lt;/a>; for how we can realize the exciting potential of AI models to help advance real-world clinical care. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s1600/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIHNosDcouStVqnHjoNr_b7YQQGCEXsxlCOy1ltKotOv5GQfl6JA9We90L6Ej3TZ2tiutOrASoon-BPt__Fh9jN2NiXY1W6z5emcW63JkkS7sS1LrsMz7mINkzvSuD_i4NR3GdRbsQFlVrNrC3cv1bNHETISJ_Ml0n7ddXbtHmO_AzfSd8EPq7-ud7iBNH/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same model weights.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>;We have also been working on &lt;a href=&quot;https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/&quot;>;how best to harness AI models in clinical workflows&lt;/a>;. We have shown that &lt;a href=&quot;https://blog.research.google/2023/03/learning-from-deep-learning-case-study.html&quot;>;coupling deep learning with interpretability methods&lt;/a>; can yield new insights for clinicians. We have also shown that self-supervised learning, with careful consideration of privacy, safety, fairness and ethics, &lt;a href=&quot;https://blog.research.google/2023/04/robust-and-efficient-medical-imaging.html&quot;>;can reduce the amount of de-identified data needed&lt;/a>; to train clinically relevant medical imaging models by 3×–100×, reducing the barriers to adoption of models in real clinical settings. We also released an &lt;a href=&quot;https://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot;>;open source mobile data collection platform&lt;/a>; for people with chronic disease to provide tools to the community to build their own studies.&lt;/p>; &lt;p>; AI systems can also discover completely new signals and biomarkers in existing forms of medical data. In work on &lt;a href=&quot;https://blog.research.google/2023/03/detecting-novel-systemic-biomarkers-in.html&quot;>;novel biomarkers discovered in retinal images&lt;/a>;, we demonstrated that a number of systemic biomarkers spanning several organ systems (eg, kidney, blood, liver) can be predicted from external eye photos. In other work, we showed that combining &lt;a href=&quot;https://blog.research.google/2023/04/developing-aging-clock-using-deep.html&quot;>;retinal images and genomic information&lt;/a>; helps identify some underlying factors of aging. &lt;/p>; &lt;p>; In the genomics space, we worked with 119 scientists across 60 institutions to create a &lt;a href=&quot;https://blog.research.google/2023/05/building-better-pangenomes-to-improve.html&quot;>;new map of the human genome&lt;/a>;, or pangenome. This more equitable pangenome better represents the genomic diversity of global populations. Building on our ground-breaking &lt;a href=&quot;https://www.nature.com/articles/s41586-021-03819-2&quot;>;AlphaFold&lt;/a>; work, our work on &lt;a href=&quot;https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/&quot;>;AlphaMissense&lt;/a>; this year provides a catalog of predictions for 89% of all 71 million possible &lt;a href=&quot;https://en.wikipedia.org/wiki/Missense_mutation&quot;>;missense variants&lt;/a>; as either likely pathogenic or likely benign. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s1070/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;491&quot; data-original-width=&quot;1070&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHNPYsNIjVTRyfPZODcVpp-XnQAtz2b0HcKsa8F9GyJXoKfp-9PH8N_IcXO_lJ7WfuTZ2ezeAVYCDPnqeyu-qeXahqu1lwJXb6Zq00Mt7M-WMyFff9eUL67L_QZBwK95DNlVAcFpd9cr1GW5gxqNb0mOJszQphyphenhyphen6Yi_QelNzeYnvZ1XNKZqNU2EP_x8MjW/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Examples of AlphaMissense predictions overlaid on AlphaFold predicted structures (red – predicted as pathogenic; blue – predicted as benign; grey – uncertain). Red dots represent known pathogenic missense variants, blue dots represent known benign variants.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;HBB protein. Variants in this protein can cause sickle cell anaemia.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;CFTR protein. Variants in this protein can cause cystic fibrosis.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We also shared &lt;a href=&quot;https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/&quot;>;an update&lt;/a>; on progress towards the next generation of AlphaFold. Our latest model can now generate predictions for nearly all molecules in the &lt;a href=&quot;https://www.wwpdb.org/&quot;>;Protein Data Bank&lt;/a>; (PDB), frequently reaching atomic accuracy. This unlocks new understanding and significantly improves accuracy in multiple key biomolecule classes, including ligands (small molecules), proteins, nucleic acids (DNA and RNA), and those containing post-translational modifications (PTMs).&lt;/p>;&lt;p>;&lt;/p>; &lt;p>; On the neuroscience front, we &lt;a href=&quot;https://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot;>;announced a new collaboration&lt;/a>; with Harvard, Princeton, the NIH, and others to map an entire mouse brain at synaptic resolution, beginning with a first phase that will focus on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hippocampal_formation&quot;>;hippocampal formation&lt;/a>; — the area of the brain responsible for memory formation, spatial navigation, and other important functions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Quantum computing&lt;/h3>; &lt;p>; Quantum computers have the potential to solve big, real-world problems across science and industry. But to realize that potential, they must be significantly larger than they are today, and they must reliably perform tasks that cannot be performed on classical computers. &lt;/p>; &lt;p>; This year, we took an important step towards the development of a large-scale, useful quantum computer. Our breakthrough is the first demonstration of &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;quantum error correction&lt;/a>;, showing that it&#39;s possible to reduce errors while also increasing the number of qubits. To enable real-world applications, these qubit building blocks must perform more reliably, lowering the error rate from ~1 in 10&lt;sup>;3&lt;/sup>; typically seen today, to ~1 in 10&lt;sup>;8&lt;/sup>; 。 &lt;/p>; &lt;br />; &lt;h2>;Responsible AI research&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Design for Responsibility &lt;/h3>; &lt;p>; Generative AI is having a transformative impact in a wide range of fields including healthcare, education, security, energy, transportation, manufacturing, and entertainment. Given these advances, the importance of designing technologies consistent with our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>; remains a top priority. We also recently published case studies of &lt;a href=&quot;https://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot;>;emerging practices in society-centered AI&lt;/a>; 。 And in our annual &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf&quot; target=&quot;_blank&quot;>;AI Principles Progress Update&lt;/a>;, we offer details on how our Responsible AI research is integrated into products and risk management processes. &lt;/p>; &lt;p>; Proactive design for Responsible AI begins with identifying and documenting potential harms. For example, we recently &lt;a href=&quot;https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/&quot;>;introduced&lt;/a>; a &lt;a href=&quot;https://arxiv.org/abs/2310.11986&quot;>;three-layered&lt;/a>; context-based framework for comprehensively evaluating the social and ethical risks of AI systems. During model design, harms can be mitigated with the use of &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;responsible datasets&lt;/a>; 。 &lt;/p>; &lt;p>; We are &lt;a href=&quot;https://blog.google/technology/research/project-elevate-black-voices-google-research/&quot;>;partnering with Howard University&lt;/a>; to build high quality African-American English (AAE) datasets to improve our products and make them work well for more people. Our research on &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3593013.3594016&quot;>;globally inclusive cultural representation&lt;/a>; and our publication of the &lt;a href=&quot;https://skintone.google/&quot;>;Monk Skin Tone scale&lt;/a>; furthers our commitments to equitable representation of all people. The insights we gain and techniques we develop not only help us improve our own models, they also power &lt;a href=&quot;https://blog.google/intl/en-in/company-news/using-ai-to-study-demographic-representation-in-indian-tv/&quot;>;large-scale studies of representation in popular media&lt;/a>; to inform and inspire more inclusive content creation around the world. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s1196/image9.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;158&quot; data-original-width=&quot;1196&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYAo6ClPM9zD8ctnoTvdyhnwW1VdVT2p677EMKGrN0oULjyK9TdS05z1OzhTTuyxp0kfuUUbHEieZXYRW6hUe3XlJM6HYOS68rXneSGQeLTy_Bo_SCvbxnjHdG2CB_8aLCz5pP-B_dciLKZYlIo9j8bEyUdKtZQhg7DukG9pJudJKU-o0DRuM5XrbvkBI/s16000/image9.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Monk Skin Tone (MST) Scale. See more at&amp;nbsp;&lt;a href=&quot;http://skintone.google/&quot;>;skintone.google&lt;/a>;.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With advances in generative image models, &lt;a href=&quot;https://blog.research.google/2023/08/responsible-ai-at-google-research.html&quot;>;fair and inclusive representation of people&lt;/a>; remains重点。 In the development pipeline, we are working to &lt;a href=&quot;https://blog.research.google/2023/07/using-societal-context-knowledge-to.html&quot;>;amplify underrepresented voices and to better integrate social context knowledge&lt;/a>;. We proactively address potential harms and bias using &lt;a href=&quot;https://arxiv.org/pdf/2306.06135.pdf&quot;>;classifiers and filters&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2311.17259.pdf&quot;>;careful dataset analysis&lt;/a>;, and in-model mitigations such as fine-tuning, &lt;a href=&quot;https://arxiv.org/abs/2310.16523&quot;>;reasoning&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2306.14308&quot;>;few-shot prompting&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2310.16959&quot;>;data augmentation&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2310.17022&quot;>;controlled decoding&lt;/a>;, and our research showed that generative AI enables &lt;a href=&quot;https://arxiv.org/abs/2302.06541&quot;>;higher quality safety classifiers&lt;/a>; to be developed with far less data. We also released &lt;a href=&quot;https://developers.googleblog.com/2023/10/make-with-makersuite-part-2-tuning-llms.html&quot;>;a powerful way to better tune models with less data&lt;/a>; giving developers more control of responsibility challenges in generative AI.&lt;/p>; &lt;p>; We have developed new&lt;a href=&quot;https://arxiv.org/abs/2303.08114&quot;>; state-of-the-art explainability methods&lt;/a>; to identify the role of training data on model behaviors. By &lt;a href=&quot;https://arxiv.org/abs/2302.06598&quot;>;combining training data attribution methods with agile classifiers&lt;/a>;, we found that we can identify mislabelled training examples. This makes it possible to reduce the noise in training data, leading to significant improvements in model accuracy. &lt;/p>; &lt;p>; We initiated several efforts to improve safety and transparency about online content. For example, we introduced &lt;a href=&quot;https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/&quot;>;SynthID&lt;/a>;, a tool for watermarking and identifying AI-生成的图像。 SynthID is imperceptible to the human eye, doesn&#39;t compromise image quality, and allows the watermark to remain detectable, even after modifications like adding filters, changing colors, and saving with various lossy compression schemes. &lt;/p>; &lt;p>; We also launched &lt;a href=&quot;https://blog.google/products/search/google-search-new-fact-checking-features/&quot;>;About This Image&lt;/a>; to help people assess the credibility of images, showing information like an image&#39;s history, how it&#39;s used on other pages, and available metadata about an image. And we &lt;a href=&quot;https://arxiv.org/abs/2210.03535&quot;>;explored safety methods&lt;/a>; that have been developed in other fields, learning from established situations where there is low-risk tolerance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s616/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;346&quot; data-original-width=&quot;616&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBYUNXlxnFiBnnB_-EFKhdc-1W8MwG-VZKrhyKtWFmfAcqlrbSdPl7TAslOAMaH1Zon0TvGKpj23nlO7XZyg2ovFuNHpgXbsyUUPrxzf1RtJFlBPzR5Hh9KAus1l79qrBFP5JJDScQgn_5cq3ZVf7T0VuPiNLLJ-PsENlba_BA0nORQkofZYY7K1DhJGXt/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;SynthID generates an imperceptible digital watermark for AI-generated images.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Privacy remains an essential aspect of our commitment to Responsible AI. We continued improving our state-of-the-art privacy preserving learning algorithm &lt;a href=&quot;https://arxiv.org/abs/2103.00039&quot;>;DP-FTRL&lt;/a>;, developed the DP-Alternating Minimization algorithm (&lt;a href=&quot;https://arxiv.org/pdf/2310.15454.pdf&quot;>;DP-AM&lt;/a>;) to enable personalized recommendations with rigorous privacy protection, and defined a new &lt;a href=&quot;https://blog.research.google/2023/09/differentially-private-median-and-more.html&quot;>;general paradigm&lt;/a>; to reduce the privacy costs for many aggregation and learning tasks. We also proposed a scheme for &lt;a href=&quot;https://openreview.net/pdf?id=q15zG9CHi8&quot;>;auditing differentially private machine learning systems&lt;/a>;.&lt;/p>; &lt;p>; On the applications front we demonstrated that &lt;a href=&quot;https://arxiv.org/pdf/2308.10888.pdf&quot;>;DP-SGD offers a practical solution&lt;/a>; in the large model fine-tuning regime and showed that images generated by DP diffusion models are &lt;a href=&quot;https://arxiv.org/pdf/2302.13861.pdf&quot;>;useful for a range of downstream tasks&lt;/a>;. We &lt;a href=&quot;https://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot;>;proposed&lt;/a>; a new algorithm for DP training of large embedding models that provides efficient training on TPUs without compromising accuracy. &lt;/p>; &lt;p>; We also teamed up with a broad group of academic and industrial researchers to organize the &lt;a href=&quot;https://unlearning-challenge.github.io/&quot;>;first Machine Unlearning Challenge&lt;/a>; to address the scenario in which training images are forgotten to protect the privacy or rights of individuals. We shared a mechanism for &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;extractable memorization&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;participatory systems&lt;/a>; that give users more control over their sensitive data. &lt;/p>; &lt;p>; We continued to expand the world&#39;s largest corpus of atypical speech recordings to &amp;gt;1M utterances in &lt;a href=&quot;https://sites.research.google/euphonia/about/&quot;>;Project Euphonia&lt;/a>;, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize atypical speech by 37%&lt;/a>; on real-world benchmarks. &lt;/p>; &lt;p>; We also built an &lt;a href=&quot;https://blog.research.google/2023/08/study-socially-aware-temporally-causal.html&quot;>;audiobook recommendation system&lt;/a>; for students with reading disabilities such as dyslexia. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Adversarial testing&lt;/h3>; &lt;p>; Our work in adversarial testing &lt;a href=&quot;https://blog.research.google/2023/03/responsible-ai-at-google-research.html&quot;>;engaged community voices&lt;/a>; from historically marginalized communities. We partnered with groups such as the &lt;a href=&quot;https://arxiv.org/abs/2303.08177&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR) to ensure we represent the diverse communities who use our models and &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;engage with external users&lt;/a>; to identify potential harms in generative model outputs. &lt;/p>; &lt;p>; We &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot;>;established a dedicated Google AI Red Team&lt;/a>; focused on testing AI models and products for security, privacy, and abuse risks. We showed that attacks such as “&lt;a href=&quot;https://arxiv.org/pdf/2302.10149.pdf?isApp=1&quot;>;poisoning&lt;/a>;” or &lt;a href=&quot;https://arxiv.org/pdf/2306.15447.pdf&quot;>;adversarial examples&lt;/a>; can be applied to production models and surface additional risks such as memorization in both &lt;a href=&quot;https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf&quot;>;image&lt;/a>; and &lt;a href=&quot;https://arxiv.org/pdf/2311.17035.pdf&quot;>;text generative models&lt;/a>;. We also demonstrated that defending against such attacks can be challenging, as merely applying defenses can cause other &lt;a href=&quot;https://arxiv.org/pdf/2309.05610.pdf&quot;>;security and privacy leakages&lt;/a>;. We also introduced model evaluation for &lt;a href=&quot;https://arxiv.org/abs/2305.15324&quot;>;extreme risks&lt;/a>;, such as offensive cyber capabilities or strong manipulation skills. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Democratizing AI though tools and education&lt;/h3>; &lt;p>; As we advance the state-of-the-art in ML and AI, we also want to ensure people can understand and apply AI to specific problems. We released &lt;a href=&quot;https://makersuite.google.com/&quot;>;MakerSuite&lt;/a>; (now &lt;a href=&quot;https://makersuite.google.com&quot;>;Google AI Studio&lt;/a>;), a web-based tool that enables AI developers to quickly iterate and build lightweight AI-powered apps. To help AI engineers better understand and debug AI, we released &lt;a href=&quot;https://pair-code.github.io/lit/&quot;>;LIT 1.0&lt;/a>;, a state-of-the-art, open-source debugger for machine learning models. &lt;/p>; &lt;p>; &lt;a href=&quot;https://colab.google/&quot;>;Colab&lt;/a>;, our tool that helps developers and students access powerful computing resources right in their web browser, reached over 10 million users 。 We&#39;ve just added &lt;a href=&quot;https://blog.google/technology/ai/democratizing-access-to-ai-enabled-coding-with-colab/&quot;>;AI-powered code assistance&lt;/a>; to all users at no cost — making Colab an even more helpful and integrated experience in data and ML workflows. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s844/image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;470&quot; data-original-width=&quot;844&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy5OhqeP7Rf5gTbCP-gYsmTFyc9ztUMXrcv_M8Lya5zLPzzRVrHmldGiR-rf1PnggxP_rlG2mo6YJ9NhnNnFHEbtn8f-FJk_cxOTtO9hL0ZhxV9hSGd0LW0-RymxkPVBbqXKDk4nRV7mJE6heVnn-6tYcLdpofuhqKPHnqxDTZf1hjifTg3k7K4VNcS2S6/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;One of the most used features is “Explain error” — whenever the user encounters an execution error in Colab, the code assistance model provides an explanation along with a potential fix.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To ensure AI produces accurate knowledge when put to use, we also recently introduced &lt;a href=&quot;https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/&quot;>;FunSearch&lt;/a>;, a new approach that generates verifiably true knowledge in mathematical sciences using evolutionary methods and large language models.&lt;/p>; &lt;p>; For AI engineers and product designers, we&#39;re updating the &lt;a href=&quot;https://pair.withgoogle.com/guidebook/&quot;>;People + AI Guidebook&lt;/a>; with generative AI best practices, and we continue to design &lt;a href=&quot;https://pair.withgoogle.com/explorables/&quot;>;AI Explorables&lt;/a>;, which includes &lt;a href=&quot;https://pair.withgoogle.com/explorables/uncertainty-ood/&quot;>;how and why models sometimes make incorrect predictions confidently&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Community engagement&lt;/h3>; &lt;p>; We continue to advance the fields of AI and computer science by publishing much of our work and participating in and organizing conferences. We have published more than 500 papers so far this year, and have strong presences at conferences like ICML (see the &lt;a href=&quot;https://blog.research.google/2023/07/google-at-icml-2023.html&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/&quot;>;Google DeepMind&lt;/a>; posts), ICLR (&lt;a href=&quot;https://blog.research.google/2023/04/google-at-iclr-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/&quot;>;Google DeepMind&lt;/a>;), NeurIPS (&lt;a href=&quot;https://blog.research.google/2023/12/google-at-neurips-2023.html&quot;>;Google Research&lt;/a>;, &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/&quot;>;Google DeepMind&lt;/a>;), &lt;a href=&quot;https://blog.research.google/2023/10/google-at-iccv-2023.html&quot;>;ICCV&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/06/google-at-cvpr-2023.html&quot;>;CVPR&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/07/google-at-acl-2023.html&quot;>;ACL&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/04/google-at-chi-2023.html&quot;>;CHI&lt;/a>;, and &lt;a href=&quot;https://blog.research.google/2023/08/google-at-interspeech-2023.html&quot;>;Interspeech&lt;/a>;. We are also working to support researchers around the world, participating in events like the &lt;a href=&quot;https://deeplearningindaba.com/2023/google-outreach-mentorship-programme/&quot;>;Deep Learning Indaba&lt;/a>;, &lt;a href=&quot;https://khipu.ai/khipu2023/khipu-2023-speakers2023/&quot;>;Khipu&lt;/a>;, supporting &lt;a href=&quot;https://blog.google/around-the-globe/google-latin-america/phd-fellowship-research-latin-america/&quot;>;PhD Fellowships in Latin America&lt;/a>;, and more. We also worked with partners from 33 academic labs to pool data from 22 different robot types and create the &lt;a href=&quot;https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/&quot;>;Open X-Embodiment dataset and RT-X model&lt;/a>; to better advance responsible AI development. &lt;/p>; &lt;p>; Google has spearheaded an industry-wide effort to develop &lt;a href=&quot;https://mlcommons.org/working-groups/ai-safety/ai-safety/&quot;>;AI safety benchmarks&lt;/a >; under the &lt;a href=&quot;https://mlcommons.org/&quot;>;MLCommons&lt;/a>; standards organization with participation from several major players in the generative AI space including OpenAI, Anthropic, Microsoft, Meta, Hugging Face, and more 。 Along with others in the industry we also &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;co-founded&lt;/a>; the &lt;a href=&quot;https://www.frontiermodelforum.org/&quot;>;Frontier Model Forum&lt;/a>; (FMF), which is focused on ensuring safe and responsible development of frontier AI models. With our FMF partners and other philanthropic organizations, we launched a $10 million &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum-executive-director/&quot;>;AI Safety Fund&lt;/a>; to advance research into the ongoing development of the tools for society to effectively test and evaluate the most capable AI models. &lt;/p>; &lt;p>; In close partnership with &lt;a href=&quot;http://Google.org&quot;>;Google.org&lt;/a>;, we &lt;a href=&quot;https://blog.google/technology/ai/google-ai-data-un-global-goals/&quot;>;worked with the United Nations&lt;/a>; to build the &lt;a href=&quot;https://unstats.un.org/UNSDWebsite/undatacommons/sdgs&quot;>;UN Data Commons for the Sustainable Development Goals&lt;/a>;, a tool that tracks metrics across the 17 &lt;a href=&quot;https://sdgs.un.org/goals&quot;>;Sustainable Development Goals&lt;/a>;, and &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/supported-organizations&quot;>;supported projects&lt;/a>; from NGOs, academic institutions, and social enterprises on &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;using AI to accelerate progress on the SDGs&lt;/a>;. &lt;/p>; &lt;p>; The items highlighted in this post are a small fraction of the research work we have done throughout the last year. Find out more at the &lt;a href=&quot;https://blog.research.google/&quot;>;Google Research&lt;/a>; and &lt;a href=&quot;https://deepmind.google/discover/blog/&quot;>;Google DeepMind&lt;/a>; blogs, and our &lt;a href=&quot;https://research.google/pubs/&quot;>;list of publications&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Future vision&lt;/h2>; &lt;p>; As multimodal models become even more capable, they will empower people to make incredible progress in areas from science to education to entirely new areas of knowledge. &lt;/p>; &lt;p>; Progress continues apace, and as the year advances, and our products and research advance as well, people will find more and interesting creative uses for AI. &lt;/p>; &lt;p>; Ending this Year-in-Review where we began, as we say in &lt;em>;&lt;a href=&quot;https://ai.google/static/documents/google-why-we-focus-on-ai.pdf&quot;>;Why We Focus on AI (and to what end)&lt;/a>;&lt;/em>;: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; If pursued boldly and responsibly, we believe that AI can be a foundational technology that transforms the lives of people everywhere — this is what excites us! &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;This Year-in-Review is cross-posted on both the &lt;a href=&quot;https://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html&quot;>;Google Research Blo&lt;/a>;g and the &lt;a href=&quot;https://deepmind.google/discover/blog/2023-a-year-of-groundbreaking-advances-in-ai-and-computing/&quot;>;Google DeepMind Blog&lt;/a>;.&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8816183473385638131/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8816183473385638131&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8816183473385638131&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/2023-year-of-groundbreaking-advances-in.html&quot; rel=&quot;alternate&quot; title=&quot;2023: A year of groundbreaking advances in AI and computing&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU12G_p2DZO4gQ-P95aP2IFvtKRHRG5Oik9VzJ4WtT_VznggjUrL4tTzqto-C8yx6ULgvugKgH9usiZxnKGk97pOFyHWnu-1S4sSbR2Jjq5T36tQRbZucTAP7gmXkGw77xN6s39IKxPaxbo5tw_Cq52ZfPWOCBkWL-2XTuzsIh6viRIqgGcdUdNq-pHjzl/s72-c/year_in_review-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5516200703494636207&lt;/id>;&lt;published>;2023-12-19T13:08:00.000-08:00&lt;/published>;&lt;updated>;2024-01-12T11:04:04.629-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Large Language Models&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;video&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Vision Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VideoPoet: A large language model for zero-shot video generation&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dan Kondratyuk and David Ross, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s1600/videopoetpreview.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; A recent wave of video generation models has burst onto the scene, in many cases showcasing stunning picturesque quality. One of the current bottlenecks in video generation is in the ability to produce coherent large motions. In many cases, even the current leading models either generate small motion or, when producing larger motions, exhibit noticeable artifacts. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To explore the application of language models in video generation, we introduce VideoPoet (&lt;a href=&quot;http://sites.research.google/videopoet&quot;>;website&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2312.14125&quot;>;research paper&lt;/a>;), a large language model (LLM) that is capable of a wide variety of video generation tasks, including text-to-video, image-to-video, video stylization, video &lt;a href=&quot;https://en.wikipedia.org/wiki/Inpainting&quot;>;inpainting&lt;/a>; and &lt;a href=&quot;https://paperswithcode.com/task/image-outpainting&quot;>;outpainting&lt;/a>;, and video-to-audio. One notable observation is that the leading video generation models are almost exclusively diffusion-based (for one example, see &lt;a href=&quot;https://imagen.research.google/video/&quot;>;Imagen Video&lt;/a>;). On the other hand, LLMs are widely recognized as the &lt;em>;de facto&lt;/em>; standard due to their exceptional learning capabilities across various modalities, including language, code, and audio (eg, &lt;a href=&quot;https://google-research.github.io/seanet/audiopalm/examples/&quot;>;AudioPaLM&lt;/a>;). In contrast to alternative models in this space, our approach seamlessly integrates many video generation capabilities within a single LLM, rather than relying on separately trained components that specialize on each task. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview&lt;/h2>; &lt;p>; The diagram below illustrates VideoPoet&#39;s capabilities. Input images can be animated to produce motion, and (optionally cropped or masked) video can be edited for inpainting or outpainting. For stylization, the model takes in a video representing the depth and optical flow, which represent the motion, and paints contents on top to produce the text-guided style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;682&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfgEjHPIukR1pHUT7VBU8DecJaayp8sIV1WC4HM6EOW-K1-E_Xu9-qE2hrTBrtgrks3awr8IiT9NhxVMDOR0qoFZ8nDQT_Si3LY60CWKySkaybXRW5Uf6EwZIiDRy7qkQGuoLUxoysR-fjEr0NTMqAGP2Cm8cyUQHVOOlS7MysnwHwqhdM-eA63PXy5XsV/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An overview of VideoPoet, capable of multitasking on a variety of video-centric inputs and outputs. The LLM can optionally take text as input to guide generation for text-to-video, image-to-video, video-to-audio, stylization, and outpainting tasks. Resources used: &lt;a href=&quot;https://commons.wikimedia.org/wiki/Main_Page&quot;>;Wikimedia Commons&lt;/a>; and &lt;a href=&quot;https://davischallenge.org/&quot;>;DAVIS&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Language models as video generators&lt;/h2>; &lt;p>; One key advantage of using LLMs for training is that one can reuse many of the scalable efficiency improvements that have been introduced in existing LLM training infrastructure. However, LLMs operate on discrete tokens, which can make video generation challenging. Fortunately, there exist &lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;video&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>;audio&lt;/a>; tokenizers, which serve to encode video and audio clips as sequences of discrete tokens (ie, integer indices), and which can also be converted back into the original representation. &lt;/p>; &lt;p>; VideoPoet trains an &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive language model&lt;/a>; to learn across video, image, audio, and text modalities through the use of multiple tokenizers (&lt;a href=&quot;https://magvit.cs.cmu.edu/v2/&quot;>;MAGVIT V2&lt;/a>; for video and image and &lt;a href=&quot;https://arxiv.org/abs/2107.03312&quot;>;SoundStream&lt;/a>; for audio). Once the model generates tokens conditioned on some context, these can be converted back into a viewable representation with the tokenizer decoders. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s2680/image21.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;824&quot; data-original-width=&quot;2680&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s16000/image21.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A detailed look at the VideoPoet task design, showing the training and inference inputs and outputs of various tasks. Modalities are converted to and from tokens using tokenizer encoder and decoders. Each modality is surrounded by boundary tokens, and a task token indicates the type of task to perform.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Examples generated by VideoPoet&lt;/h2>; &lt;p>; Some examples generated by our model are shown below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s1100/image18.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;963&quot; data-original-width=&quot;1100&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgoUJFpPd377ceVnh3Yi0Y1oM6pVPL_FSEwugxBVKfEwHV8VA-1ZPmddz1VRBtqESjjEP83EJ4HOLLSBmXOVLEODZVFZYUuDiCRyMUIvSaRsxieR-58iAwHPBf7SNeSBU2a3Pm80JOFivTSjZsqlerHxAGg_Ko_8gCDLWWtNAQffyGCNJrw2G5PPG-vSYtz/s16000/image18.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Videos generated by VideoPoet from various text prompts. For specific text prompts refer to &lt;a href=&quot;http://sites.research.google/videopoet&quot;>;the website&lt;/a>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; For text-to-video, video outputs are variable length and can apply a range of motions and styles depending on the text content. To ensure responsible practices, we reference artworks and styles in the public domain eg, Van Gogh&#39;s “Starry Night”.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Text Input&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A Raccoon dancing in Times Square”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A horse galloping through Van-Gogh&#39;s &#39;Starry Night&#39;”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“Two pandas playing cards”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;20%&quot;>;&lt;em>;“A large blob of exploding splashing rainbow paint, with an apple emerging, 8k”&lt;/em>;&lt;/td>; &lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Video Output&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFNvzU_hzVGT7XCvA4AdBprfzwpOV_b8xs6Q54No72B88fgtHFK7eHFr3UJ9Ac0tXIemkOUR6VxNC2I8HQWznWs2x4IsB8biOEe2DXBrMxW6HveNhuiae40mF8asd3jYh2WqZMTTObSKiHb0RJIFsK_C7VzBq685OPUY_uqPC5NJGXqKs3AWJOG-Gqgk0V/s448/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFNvzU_hzVGT7XCvA4AdBprfzwpOV_b8xs6Q54No72B88fgtHFK7eHFr3UJ9Ac0tXIemkOUR6VxNC2I8HQWznWs2x4IsB8biOEe2DXBrMxW6HveNhuiae40mF8asd3jYh2WqZMTTObSKiHb0RJIFsK_C7VzBq685OPUY_uqPC5NJGXqKs3AWJOG-Gqgk0V/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s448/image11.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0FAaXVgkTSmoK90XYZq3sbW1fgRUmvOLZUpZ4dubLjJOFYID6w_q5GGIU1hy1E8B4J7hF01OOAupDxJWyD2OBMJEs6AIAbJEzH0qrx7TovnikAUXZyN_MQBu33QYe17CTtI95oI6x91qJhSSPyXNGlmkFKbWX8xwd6nT7Esd9RNE8tjiSbWwWQTKoAfjg/s16000/image11.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453hyphenhyphenXE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxdwiWsMiQ59NDH3axwd0UOIFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s448/image12.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4yqm_nd4mtu-3d_AdfxkKAX453hyphenhyphenXE9e7ohBzZC9RvboIZWxF_5fLw4XaCU3x1aUtW4WRckKzfqB-yzHdV_uzPiCAAwv6zgv__7MZtxdwiWsMiQ59NDH3axwd0UOIFA8C2aq0XNSgcV1ieCN1fc9MXFVIszTG8z7gpcAkgqTn5HOBBJ-pks8I1tOudAR8/s16000/image12.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s448/image17.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEg2huAujTA8vjmb5rlgj6vXF7uIHr0N7KrnLFiTXyjuN0l6kxSp7gQRPES5hD30ZzN-XTzUZSC-ROT19x0wE5cjQA_FOovY-Zox_68e0Dl4Dxanqvyuos3S1TExRdg3WZANucc-DXtKUsnim9Kh0GwU6LyFhpCJgHal0bI0UbpBYrXtlNGBYWuT8XVNIt6u/s16000/image17.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;For image-to-video, VideoPoet can take the input image and animate it with a prompt.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left:汽车; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq-Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s1536/image13.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5kAQCs7GJoJR0m_8hmYsq-Wd-KsjuF782YuV5D33BlPE8f3-AU1iTKwOVrpxnnBDHa-5AXgkXNBNil61r5eVhXa2v16VUraEt6DAa-4_v-xHJq6lJfwkJ9ATQZTGdxKsbvnfielzv_6iJyLrubPyrGhle_BUecTVJkGo_7S8sM-3yl6vVVtqNNg4nf0mw/s16000/image13.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of image-to-video with text prompts to guide the motion. Each video is paired with an image to its left. &lt;strong>;Left&lt;/strong>;: “A ship navigating the rough seas, thunderstorm and lightning, animated oil on canvas”. &lt;strong>;Middle&lt;/strong>;: “Flying through a nebula with many twinkling stars”. &lt;strong>;Right&lt;/strong>;: “A wanderer on a cliff with a cane looking down at the swirling sea fog below on a windy day”. Reference: &lt;a href=&quot;https://commons.wikimedia.org/wiki/Main_Page&quot;>;Wikimedia Commons&lt;/a>;, public domain**.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For video stylization, we predict the optical flow and depth information before feeding into VideoPoet with some additional input text. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s1536/image16.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ2eDU0pNOfQJF8cKPOV5QkKthIO3-98jbqyZJ7lFLk7cckq8Gg4FXrro6oikQRxDVDKKz8rg9CU6wihsfU68RnnLkHGxJUeFNGBobjsbJ4VHGFtUg-nerlt2rPiJ9bu8i2VkkXX5yEK650t4ay8F7K2zSW5-TjjkbR61TskVhsaQCw1-8lkP1gMDg96i1/s16000/image16.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Examples of video stylization on top of VideoPoet text-to-video generated videos with text prompts, depth, and optical flow used as conditioning. The left video in each pair is the input video, the right is the stylized output. &lt;b>;Left&lt;/b>;: “Wombat wearing sunglasses holding a beach ball on a sunny beach.” &lt;b>;Middle&lt;/b>;: “Teddy bears ice skating on a crystal clear frozen lake.” &lt;b>;Right&lt;/b>;: “A metal lion roaring in the light of a forge.”&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; VideoPoet is also capable of generating audio. Here we first generate 2-second clips from the model and then try to predict the audio without any text guidance. This enables generation of video and audio from a single model.&lt;/p>; &lt;br />; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/105_drums_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/107_cat_piano_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/108_train_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;video controls=&quot;controls&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://storage.googleapis.com/videopoet/videos/104_dog_popcorn_with_audio.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of video-to-audio, generating audio from a video example without any text input.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; By default, the VideoPoet model generates videos in portrait orientation to tailor its output towards short-form content. To showcase its capabilities, we have produced a brief movie composed of many short clips generated by VideoPoet. For the script, we asked &lt;a href=&quot;https://bard.google.com/&quot;>;Bard&lt;/a>; to write a short story about a traveling raccoon with a scene-by-scene breakdown and a list of accompanying prompts. We then generated video clips for each prompt, and stitched together all resulting clips to produce the final video below. &lt;/p>; &lt;br />; &lt;br />; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>; &lt;iframe allowfullscreen=&quot;&quot; class=&quot;BLOG_video_class&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/70wZKfx6Ylk&quot; width=&quot;640&quot; youtube-src-id=&quot;70wZKfx6Ylk&quot;>;&lt;/iframe>; &lt;/div>; &lt;br />; &lt;p>; When we developed VideoPoet, we noticed some nice properties of the model&#39;s capabilities, which we highlight below. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Long video&lt;/h3>; &lt;p>; We are able to generate longer videos simply by conditioning on the last 1 second of video and predicting the next 1 second. By chaining this repeatedly, we show that the model can not only extend the video well but also faithfully preserve the appearance of all objects even over several iterations. &lt;/p>; &lt;p>; Here are two examples of VideoPoet generating long video from text input: &lt;br />; &lt;br />; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot;>; &lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Text Input&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;35%&quot;>;&lt;em>;“An astronaut starts dancing on Mars. Colorful fireworks then explode in the background.”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td align=&quot;center&quot; width=&quot;35%&quot;>;&lt;em>;“FPV footage of a very sharp elven city of stone in the jungle with a brilliant blue river, waterfall, and large steep vertical cliff faces.”&lt;/em>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;b>;Video Output&lt;/b>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAIV6ZbjeXK5JPtQr8Mb_ZcPKiIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/s448/image14.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaT8uErAOI-bKPCwsVpEQ_SSxqdgjVB9ai-Db3M9YXhGM3X9N0Jwt-UcDb6X8n2V_4-Tf76xkwlSS4ftV8TvAIV6ZbjeXK5JPtQr8Mb_ZcPKiIvOdwFXJOBEfDk1Gp1hzRBkoYwmoH3bAu6NVNBo-ficSneXgvhDF7fwMGVqMik0KWGwv_GLf7clQ2l5e5/s16000/image14.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqWtNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/s448/image9.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;256&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDbroT8i5N-AdcRsTLjkLWoqzif1nJj-z1bRxcZwM_-1213gK6Or85VxKIFBMsnvAF_KLAmXWWLeDPxA5ZqWtNI5nqfp_6wzgKnqACckJMjBU2eNy8ySgvWjuFOPNarVcBwx9ZlrAdyMrWCdt29xDE7dPHhlwcxbRSyO7-ZllKs1SRGDgVm5XUc21I3Ddv/s16000/image9.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; It is also possible to interactively edit existing video clips generated by VideoPoet. If we supply an input video, we can change the motion of objects to perform different actions. The object manipulation can be centered at the first frame or the middle frames, which allow for a high degree of editing control. &lt;/p>; &lt;p>; For example, we can randomly generate some clips from the input video and select the desired next clip. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2NFzPadmS-8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s1280/image10.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2NFzPadmS-8v2ShkxFaqage2MkmSopCm17wtoYnVCFufD5GKZHzM9ZUeL4EvCtVLZGMJYiUA1NVJhplymInJr4_K-G9s9263JAVRMxPb9_15zipLZIwHcmYpwyZmGRwgtbFwpONB9CrOqnDmG9bJBvr7pXNbEqLtms_7QNMbSYSspRefkisKLzeWXAIW9/s16000/image10.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An input video on the left is used as conditioning to generate four choices given the initial prompt: “Closeup of an adorable rusty broken-down steampunk robot covered in moss moist and budding vegetation, surrounded by tall grass”. For the first three outputs we show what would happen for unprompted motions. For the last video in the list below, we add to the prompt, “powering up with smoke in the background” to guide the action.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Image to video control&lt;/h3>; &lt;p>; Similarly, we can apply motion to an input image to edit its contents towards the desired state, conditioned on a text prompt. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q-Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s512/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjlvHv0EU7YHj88knDA8pnCs5VDEsHI8OQeWDx8-dhNXcVKteNqMMrMczg9k0j-T8kevUiQua-Eet5BzT9xJ5CR-lpmFjMYyOQFZcAfXu-rlgTmes9_4-GONo7NFHYAw1q-Ivk6MVj34iyjj6KGpQ8dp6OwJH1SKGyxA2BDPvvEUUIJB6UBkvu1c1ILCgdr/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Animating a painting with different prompts. &lt;b>;Left&lt;/b>;: “A woman turning to look at the camera.” &lt;b>;Right&lt;/b>;: “A woman yawning.” **&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Camera motion&lt;/h3>; &lt;p>; We can also accurately control camera movements by appending the type of desired camera motion to the text prompt. As an example, we generated an image by our model with the prompt, &lt;em>;“Adventure game concept art of a sunrise over a snowy mountain by a crystal clear river”&lt;/em>;. The examples below append the given text suffix to apply the desired motion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s1536/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;448&quot; data-original-width=&quot;1536&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4w_RIRlokn88vR5u6GdRE32zOr5wKLUphlx4BwiZDQL0J6i-yhyphenhyphensc4wCsJ07izsk_MkLVT-TAfRIqU9sJ4E_cYVRszJ1bw-Ha4jYsv1oBgSMjAENhCPHIvg2aXBIULeH4UzR-0K5AHPixzxBYD7rP11xf4m2s7e1WFUyRHLv-RkKhAC9whQvwUovphkgy/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Prompts from left to right: “Zoom out”, “Dolly zoom”, “Pan left”, “Arc shot”, “Crane shot”, “FPV drone shot”.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation results&lt;/h2>; &lt;p>; We evaluate VideoPoet on text-to-video generation with a variety of benchmarks to compare the results to other approaches. To ensure a neutral evaluation, we ran all models on a wide variation of prompts without cherry-picking examples and asked people to rate their preferences. The figure below highlights the percentage of the time VideoPoet was chosen as the preferred option in green for the following questions. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Text fidelity&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IfiFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;800&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjotT5lhyphenhyphenreDLFlq_hZRSAKKI2jWx9Pp2y1xvBTQflO-H7EQ3VZYmsRTiaTV1creTpMo0It1-IfiFh313zzhhjDPeSxSW3nnRWsQC1toPBSsRlQD0T6UmzFqSNGaeQ0CGBKmn6xyAJXTG3NaF9o0icxag6f_eRzjTvu71gNRB3lOLN4xK8iQWA7dT5FKo2F/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User preference ratings for text fidelity, ie, what percentage of videos are preferred in terms of accurately following a prompt.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Motion interestingness&lt;/h3>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s1999/image15.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;797&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIWoXGSpe80GopYbQLJcyISxwM7DVtB-OFr2gqCUC33D3J6aLCS9sE4LKuoXhA89YNZE9yg_VmvUwJg2N1_nKt9m5z2NJgWiM2Ylqs2_Y2nAULojUuwpNmLv7LhYv4aGs4WgffyECcQtKM3Z83bmosuuXvHw4DeekkzAIpCkF2LlN6jExQysy68Ovgmgk1/s16000/image15.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;User preference ratings for motion interestingness, ie, what percentage of videos are preferred in terms of producing interesting motion.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Based on the above, on average people selected 24–35% of examples from VideoPoet as following prompts better than a competing model vs. 8–11% for competing models. Raters also preferred 41–54% of examples from VideoPoet for more interesting motion than 11–21% for other models. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Through VideoPoet, we have demonstrated LLMs&#39; highly-competitive video generation quality across a wide variety of tasks, especially in producing interesting and high quality motions within videos. Our results suggest the promising potential of LLMs in the field of video generation. For future directions, our framework should be able to support “any-to-any” generation, eg, extending to text-to-audio, audio-to-video, and video captioning should be possible, among many others. &lt;/p>; &lt;p>; To view more examples in original quality, see the &lt;a href=&quot;http://sites.research.google/videopoet&quot;>;website demo&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research has been supported by a large body of contributors, including Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, and Lu Jiang.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We give special thanks to Alex Siegman,Victor Gomes, and Brendan Jou for managing computing resources.我们还要感谢 Aren Jansen、Marco Tagliasacchi、Neil Zeghidour、John Hershey 进行音频标记化和处理，感谢 Angad Singh 负责“Rookie the Raccoon”中的故事板，感谢 Cordelia Schmid 进行研究讨论，感谢 David Salesin、Tomas Izo 和 Rahul Sukthankar 的贡献support, and Jay Yagnik as architect of the initial concept.&lt;/em>; &lt;/p>; &lt;br />; &lt;p>; &lt;em>;**&lt;/em>; &lt;br />; &lt;em>;(a) &lt;a href= &quot;https://commons.wikimedia.org/wiki/File:Rembrandt_Christ_in_the_Storm_on_the_Lake_of_Galilee.jpg&quot;>;The Storm on the Sea of Galilee&lt;/a>;, by Rembrandt 1633, public domain.&lt;/em>; &lt;br />; &lt;em>; (b) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Pillars_of_creation_2014_HST_WFC3-UVIS_full-res.jpg&quot;>;Pillars of Creation&lt;/a>;, by NASA 2014, public domain.&lt;/em>; &lt;br />; &lt;em>;(c) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Caspar_David_Friedrich_-_Wanderer_above_the_Sea_of_Fog.jpeg&quot;>;Wanderer above the Sea of Fog&lt;/a>;, by Caspar David Friedrich, 1818, public domain&lt;/em>; &lt;br />; &lt;em>;(d) &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg&quot;>;Mona Lisa &lt;/a>;, by Leonardo Da Vinci, 1503, public domain.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5516200703494636207/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/videopoet-large-language-model-for -zero.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/ default/5516200703494636207&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5516200703494636207&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html&quot; rel=&quot;alternate&quot; title =&quot;VideoPoet: A large language model for zero-shot video generation&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile /12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https ://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger .googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFG8pxLk-uvJVPesDUxU7Ox7Tb0VR4lB4jNygxiiIl9gyeX-rgtCb5jhWbPDKGad4d5GkPFr7-uzdZOngFtnPbmV6IurKEd5vHTiLesCvx8RDzBy_5u31e6C93kuirOG8pKcwEBkBrw4TBTzh3WIoX1TZYzYM3M4aj4zYD2r_p5FflI7ntpqoD9fsGQhwO/s72-c/videopoetpreview.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot; >;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1587965303727576226&lt;/id>;&lt;published >;2023-12-19T08:01:00.000-08:00&lt;/published>;&lt;updated>;2023-12-19T08:15:57.101-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger .com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category >;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Simulations illuminate the path to post-event traffic flow &lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yechen Li and Neha Arora, Software Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s600/TrafficFlow.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Fifteen minutes. That&#39;s &lt;a href=&quot;https://colosseum.tours/interesting-facts#:~:text=THE%20COLOSSEUM%20COULD%20BE%20FILLED,in%20a%20matter%20of%20minutes.&quot;>;how long it took to empty the Colosseum&lt;/a>;, an engineering marvel that&#39;s still standing as the largest amphitheater in the world. Two thousand years later, this design continues to work well to move enormous crowds out of sporting and entertainment venues. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; But of course, exiting the arena is only the first step. Next, people must navigate the traffic that builds up in the surrounding streets. This is an age-old problem that remains unsolved to this day. In Rome, they addressed the issue by prohibiting private traffic on the street that passes directly by the Colosseum. This policy worked there, but what if you&#39;re not in Rome? What if you&#39;re at the Superbowl? Or at a Taylor Swift concert? &lt;/p>; &lt;p>; An approach to addressing this problem is to use simulation models, sometimes called &quot;digital twins&quot;, which are virtual replicas of real-world transportation networks that attempt to capture every detail from the layout of streets and intersections to the flow of vehicles. These models allow traffic experts to mitigate congestion, reduce accidents, and improve the experience of drivers, riders, and walkers alike. Previously, our team used these models to &lt;a href=&quot;https://arxiv.org/abs/2111.03426&quot;>;quantify sustainability impact of routing&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot;>;test evacuation plans&lt;/a>; and show simulated traffic in &lt;a href=&quot;https://blog.google/products/maps/google-maps-immersive-view-routes/&quot;>;Maps Immersive View&lt;/a>;. &lt;/p>; &lt;p>; Calibrating high-resolution traffic simulations to match the specific dynamics of a particular setting is a longstanding challenge in the field. The availability of aggregate mobility data, detailed Google Maps road network data, advances in transportation science (such as understanding the relationship &lt;a href=&quot;https://tristan2022.org/Papers/TRISTAN_2022_paper_3704.pdf&quot;>;between segment demands and speeds&lt;/a>; for road segments with traffic signals), and &lt;a href=&quot;https://research.google/pubs/pub52679/&quot;>;calibration techniques&lt;/a>; which make use of speed data in physics-informed traffic models are paving the way for compute-efficient optimization at a global scale. &lt;/p>; &lt;p>; To test this technology in the real world, Google Research partnered with the Seattle Department of Transportation (SDOT) to develop simulation-based traffic guidance plans. Our goal is to help thousands of attendees of major sports and entertainment events leave the stadium area quickly and safely. The proposed plan reduced average trip travel times by 7 minutes for vehicles leaving the stadium region during large events. We deployed it in collaboration with SDOT using Dynamic Message Signs (DMS) and verified impact over multiple events between August and November, 2023. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s1200/PreImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;518&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhy_Re7-eWmfiWXL9-7r7r-w6IztNjfP3VPyfkvQc2TkBVfrCovR1enYO45bnwd5f04jM8WjwwmAEYWQwtSsp3bkY1PmiRfzkAFLSSskHaUy5PVsLvlPNV48GhlUoh9o70zJI0GWilCDCFUFQabAjQHj35bFR4IpDMHzEU2my_ayCGeLuMSV7Ml2wkrqgLV/s16000/PreImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIcVEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s1200/PostImplementation.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;519&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfOuJeVqGFMNIcVEAWUPtxWJWEjMrxQ-5lP9ytEbGd8yzq13b8s6m1YdIxviZWyBZRsM5DR6ro0O4qmeCxXYcxGf5dvjGxAkpwVRz6AMq_RHvxsdovmiVFifWxe66vGRnIpu1M-bsML2NTqlM4s8TKuwjilDCn0tuvDKNty9lzZFMirDoiHqR2ktBA3wSR/s16000/PostImplementation.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One policy recommendation we made was to divert traffic from S Spokane St, a major thoroughfare that connects the area to highways I-5 and SR 99, and is often congested after events. Suggested changes improved the flow of traffic through highways and arterial streets near the stadium, and reduced the length of vehicle queues that formed behind traffic signals. (Note that vehicles are larger than reality in this clip for demonstration.)&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Simulation model&lt;/h2>; &lt;p>; For this project, we created a new simulation model of the area around Seattle&#39;s stadiums. The intent for this model is to replay each traffic situation for a specified day as closely as possible. We use an open-source simulation software, &lt;a href=&quot;https://www.eclipse.org/sumo/&quot;>;Simulation of Urban MObility&lt;/a>; (SUMO). SUMO&#39;s behavioral models help us describe traffic dynamics, for instance, how drivers make decisions, like car-following, lane-changing and speed limit compliance. We also use insights from Google Maps to define the network&#39;s structure and various static segment attributes (eg, number of lanes, speed limit, presence of traffic lights). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s1601/SUMO.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;643&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNDnNe6WYClfVTVeD4lwsBtcDbo595yieDfOLEjPxH9e7gp9KRmYpp52c-oRiEaaILGodzScZE0E_IpianSL6pump2hGcB9-Cdj0QgfSDvJ_k8UIuvt9iraOEPCesgw3aeYoKvc5cgSF4H__xspISt4OJulVnhJZ1NEkKQBC_4dGirle3zd9ALtqnyBW2d/s16000/SUMO.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview of the Simulation framework.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Travel demand is an important simulator input. To compute it, we first decompose the road network of a given metropolitan area into zones, specifically level 13 &lt;a href=&quot;http://s2geometry.io/devguide/s2cell_hierarchy.html&quot;>;S2 cells&lt;/a>; with 1.27 km&lt;sup>;2 &lt;/sup>;area per cell. From there, we define the travel demand as the expected number of trips that travel from an origin zone to a destination zone in a given time period. The demand is represented as aggregated origin–destination (OD) matrices. &lt;/p>; &lt;p>; To get the initial expected number of trips between an origin zone and a destination zone, we use aggregated and anonymized mobility statistics. Then we solve the OD calibration problem by combining initial demand with observed traffic statistics, like segment speeds, travel times and vehicular counts, to reproduce event scenarios. &lt;/p>; &lt;p>; We model the traffic around multiple past events in Seattle&#39;s T-Mobile Park and Lumen Field and evaluate the accuracy by computing aggregated and anonymized traffic statistics. Analyzing these event scenarios helps us understand the effect of different routing policies on congestion in the region. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_CqMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s1601/TrafficHeatMap.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;655&quot; data-original-width=&quot;1601&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjipEowD8pS0yHrySDZtCCOvA_tvr-dty0jq4kBZLaGdN5U_9zcdH67bW8xkOK1Wd6n3zlwjnm4gyeYkVQD3dannZZ877CNTOfnCt8LqCbWGkWmw8IM_CqMLdg6odPan_uKoQlbAzlkvfk__nPGURcq6SqpGdsUIaJKBX1-fv3M7VFD-kQYT6THUKkRyjF4/s16000/TrafficHeatMap.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Heatmaps demonstrate a substantial increase in numbers of trips in the region after a game as compared to the same time on a non-game day.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s1999/TrafficFlow.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;931&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEiF10JbnM2SHjvrnedtjNB45oN_yi8jN6wEeGyV4zRVnI7eV53aGVdAojwUge-mDG2zFRC_o4RjROXggnY49mtkYWemI11FS9dF4hi73RhAHQKXUYkozX6MXA3o04JkoZ0WYXSyeXu-lgBwbHQIOOMRnI6UpTZoKIBiNlDhk3YxB4ksNQtrD6uaXPgQot0M/s16000/TrafficFlow.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot; >;The graph shows observed segment speeds on the x-axis and simulated speeds on the y-axis for a modeled event. The concentration of data points along the red x=y line demonstrates the ability of the simulation to reproduce realistic traffic conditions.&lt; /td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Routing policies&lt;/h2>; &lt;p>; SDOT and the Seattle Police Department&#39;s (SPD) local knowledge helped us determine the most congested routes that needed improvement: &lt; /p>; &lt;ul>; &lt;li>;Traffic from T-Mobile Park stadium parking lot&#39;s Edgar Martinez Dr. S exit to eastbound I-5 highway / westbound SR 99 highway &lt;/li>; &lt;li>;Traffic through Lumen Field stadium parking lot to northbound Cherry St. I-5 on-ramp &lt;/li>; &lt;li>;Traffic going southbound through Seattle&#39;s SODO neighborhood to S Spokane St. &lt;/li>; &lt;/ul>; &lt;p>; We developed routing policies and evaluated them using the simulation模型。 To disperse traffic faster, we tried policies that would route northbound/southbound traffic from the nearest ramps to further highway ramps, to shorten the wait times. We also experimented with opening HOV lanes to event traffic, recommending alternate routes (eg, SR 99), or load sharing between different lanes to get to the nearest stadium ramps. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s1600/NorthboundCherry.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;840&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_UIXMPB7t5xkh_obQI7KsQe8MXF0RcWk8fmU3g-wh8q5sasXBOhh3L6nB2pgixz6JinYIdCetv0215Xz-GjfLJ3SGTcgVYTALQ5raMDjeIIR-MXXbnly6CNDplcn0vDcqkLG91B1TKRyOHzFQWrZ3K5aMb87EPPrA1PhGmommkDKaKDGuJPDi4Lru9K1x/s16000/NorthboundCherry.gif&quot; />;&lt;/a>; &lt;br />; &lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s1600/SouthboundSpokane.gif&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;837&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2TMCOc-7sYPAHOoFvwEcRlAACKZjelEvcYajm7EgoPS4il1TxabdxuOTjNZ81N0bXurNwpq_w9i-U0wupOjHnES3A0uRPwiTO1KpmI1PDffBOZt4rxagHmwra3hweXVtBx3NRPxZ7VSw75NsygwD2ijowkTSUUATiOUhlEAIVJ2W3EmoYKGE1LmPIdHds/s16000/SouthboundSpokane.gif&quot; />;&lt;/a>;&lt;/div>; &lt;h2>;Evaluation results&lt;/h2>; &lt;p>; We model multiple events with different traffic conditions, event times, and attendee counts. For each policy, the simulation reproduces post-game traffic and reports the travel time for vehicles, from departing the stadium to reaching their destination or leaving the Seattle SODO area. The time savings are computed as the difference of travel time before/after the policy, and are shown in the below table, per policy, for small and large events. We apply each policy to a percentage of traffic, and re-estimate the travel times. Results are shown if 10%, 30%, or 50% of vehicles are affected by a policy. &lt;/p>; &lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s1252/TrafficImprovement.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;916&quot; data-original-width=&quot;1252&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbtXIUDzRNBrnUuMOh1uGJsM85brY5Z5q37x87YVaJngDk-5hY5YAGduh7x-K1suIbpEc1E1CKzvo67pdIRgP1pCGL1iGQlCuOiVT2zRcMI-ab0ABBhI2-3tABYfpfjcD6ai2XjsUjKusOqAFHSMO7iT7XLgFEsdheSL2lbtEpvToeC23gv6oLzidPT6X3/s16000/TrafficImprovement.png&quot; />;&lt;/a>;&lt;/div>; &lt;p>;Based on these simulation results, the feasibility of implementation, and other considerations, SDOT has decided to implement the “Northbound Cherry St ramp” and “Southbound S Spokane St ramp” policies using DMS during large events. The signs suggest drivers take alternative routes to reach their destinations. The combination of these two policies leads to an average of 7 minutes of travel time savings per vehicle, based on rerouting 30% of traffic during large events. &lt;/p>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; This work demonstrates the power of simulations to model, identify, and quantify the effect of proposed traffic guidance policies. Simulations allow network planners to identify underused segments and evaluate the effects of different routing policies, leading to a better spatial distribution of traffic. The offline modeling and online testing show that our approach can reduce total travel time. Further improvements can be made by adding more traffic management strategies, such as optimizing traffic lights. Simulation models have been historically time consuming and hence affordable only for the largest cities and high stake projects. By investing in more scalable techniques, we hope to bring these models to more cities and use cases around the world. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;In collaboration with Alex Shashko, Andrew Tomkins, Ashley Carrick, Carolina Osorio, Chao Zhang, Damien Pierce, Iveel Tsogsuren, Sheila de Guia, and Yi-fan Chen. Visual design by John Guilyard. We would like to thank our SDOT partners Carter Danne, Chun Kwan, Ethan Bancroft, Jason Cambridge, Laura Wojcicki, Michael Minor, Mohammed Said, Trevor Partap, and SPD partners Lt. Bryan Clenna and Sgt. Brian Kokesh.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1587965303727576226/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/simulations-illuminate-path-to-post.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1587965303727576226&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/simulations-illuminate-path-to-post.html&quot; rel=&quot;alternate&quot; title=&quot;Simulations illuminate the path to post-event traffic flow&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj35HCKLS0slKFX6LVrCCK0crWWJ-YwNVxNkDqze9UpfuVTWfD7URw9CyRwyFwAdIT-CHG59vl19KgfrGWEtoufCS6SQOzLuV1n7SYpun6EOAML0MfdxwjGhDKyKrfIz2t6Ivv_T07YVCPlA7uQMmPr8LYrXPSdE3V1WAwOwU0DYR_8wMWMJZh7MLeshXeP/s72-c/TrafficFlow.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6088118107306075362&lt;/id>;&lt;published>;2023-12-15T14:34:00.000-08:00&lt;/published>;&lt;updated>;2023-12-15T14:34:10.381-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Kaggle&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Advancements in machine learning for machine learning&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, Google DeepMind, and Bryan Perozzi, Senior Staff Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s1600/Screenshot%202023-12-15%20at%202.33.10%E2%80%AFPM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; With the recent and accelerated advances in machine learning (ML), machines can &lt;a href=&quot;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&quot;>;understand natural language&lt;/a>;, &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;>;engage in conversations&lt;/a>;, &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview&quot;>;draw images&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;>;create videos&lt;/a>; and more. Modern ML models are programmed and trained using ML programming frameworks, such as &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>;, &lt;a href=&quot;https://github.com/google/jax&quot;>;JAX&lt;/a>;, &lt;a href=&quot;https://pytorch.org/&quot;>;PyTorch&lt;/a>;, among many others. These libraries provide high-level instructions to ML practitioners, such as linear algebra operations (eg, matrix multiplication, convolution, etc.) and neural network layers (eg, &lt;a href=&quot;https://keras.io/api/layers/convolution_layers/convolution2d/&quot;>;2D convolution layers&lt;/a>;, &lt;a href=&quot;https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/&quot;>;transformer layers&lt;/a>;). Importantly, practitioners need not worry about how to make their models run efficiently on hardware because an ML framework will automatically optimize the user&#39;s model through an underlying &lt;em>;compiler&lt;/em>;. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on heuristics to solve complex optimization problems, often resulting in suboptimal performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this blog post, we present exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Prior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small sub-programs, such as basic blocks or kernels. We introduce “&lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs&lt;/a>;” (presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), which we recently released to fuel more research in ML for program optimization. We hosted a &lt;a href=&quot;https://www.kaggle.com/competitions/predict-ai-model-runtime/overview&quot;>;Kaggle competition&lt;/a>; on the dataset, which recently completed with 792 participants on 616 teams from 66 countries. Furthermore, in “&lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;Learning Large Graph Property Prediction via Graph Segment Training&lt;/a>;”, we cover a novel method to scale &lt;a href=&quot;https://arxiv.org/abs/2005.03675&quot;>;graph neural network&lt;/a>; (GNN) training to handle large programs represented as graphs. The technique both enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the model. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ML compilers&lt;/h2>; &lt;p>; ML compilers are software routines that convert user-written programs (here, mathematical instructions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML program can be represented as a computation graph, where a node represents a tensor operation (such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_multiplication&quot;>;matrix multiplication&lt;/a>;), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimization problems, including &lt;em>;graph-level &lt;/em>;and &lt;em>;kernel-level&lt;/em>; optimizations. A graph-level optimization requires the context of the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization transforms one kernel (a fused subgraph) at a time, independently of other kernels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;465&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Important optimizations in ML compilers include graph-level and kernel-level optimizations.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To provide a concrete example, imagine a &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_(mathematics)&quot;>;matrix&lt;/a>; (2D tensor): &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;290&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/ R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; It can be stored in computer memory as [ABC abc] or [A a B b C c], known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Row-_and_column-major_order&quot;>;row- and column-major memory layout&lt;/a>;, respectively. One important ML compiler optimization is to assign memory layouts to all intermediate tensors in the program. The figure below shows two different layout configurations for the same program. Let&#39;s assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual operator. However, this layout configuration requires the compiler to insert a &lt;em>;copy&lt;/em>; operation to transform the memory layout between the &lt;em>;add&lt;/em>; and &lt;em>;convolution&lt;/em>;运营。 On the other hand, the right-hand side configuration might be less efficient for each individual operator, but it doesn&#39;t require the additional memory transformation. The layout assignment optimization has to trade off between local computation efficiency and layout transformation overhead. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;343&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A node represents a tensor operator, annotated with its output tensor shape [&lt;em>;n&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;n&lt;sub>;1&lt;/sub>;&lt;/em>;, ...], where &lt;em>;n&lt;sub>;i &lt;/sub>;&lt;/em>;is the size of dimension &lt;em>;i&lt;/em>;. Layout {&lt;em>;d&lt;sub>;0&lt;/sub>;&lt;/em>;, &lt;em>;d&lt;sub>;1&lt;/sub>;&lt;/em>;, ...} represents minor-to-major ordering in memory. Applied configurations are highlighted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inputs and outputs of influential operators (ie, convolution and reshape). A copy operator is inserted when there is a layout mismatch.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If the compiler makes optimal choices, significant speedups can be made. For example, we have seen &lt;a href=&quot;https://ieeexplore.ieee.org/document/9563030&quot;>;up to a 32% speedup&lt;/a>; when choosing an optimal layout configuration over the default compiler&#39;s configuration in the &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;XLA&lt;/a>; benchmark suite. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TpuGraphs dataset&lt;/h2>; &lt;p>; Given the above, we aim to improve ML model efficiency by improving the ML compiler. Specifically, it can be very effective to equip the compiler&lt;strong>; &lt;/strong>;with a &lt;a href=&quot;https://arxiv.org/abs/2008.01040&quot;>;learned cost model&lt;/a>;&lt;strong>; &lt;/strong>;that takes in an input program and compiler configuration and then outputs the predicted runtime of the program. &lt;/p>; &lt;p>; With this motivation, we &lt;a href=&quot;https://arxiv.org/abs/2308.13490&quot;>;release TpuGraphs&lt;/a>;, a dataset for learning cost models for programs running on Google&#39;s custom &lt;a href=&quot;https://cloud.google.com/tpu/docs/intro-to-tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs). The dataset targets two XLA compiler configurations: &lt;em>;layout&lt;/em>; (generalization of row- and column-major ordering, from matrices, to higher dimension tensors) and &lt;em>;tiling&lt;/em>; (configurations of tile sizes) 。 We provide download instructions and starter code on the &lt;a href=&quot;https://github.com/google-research-datasets/tpu_graphs&quot;>;TpuGraphs GitHub&lt;/a>;. Each example in the dataset contains a computational graph of an ML workload, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source ML programs, featuring popular model architectures, eg, &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;>;ResNet&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;>;EfficientNet&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;>;Mask R-CNN&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>;. The dataset provides 25× more graphs than the largest (earlier) graph property prediction dataset (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction datasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction task on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1546&quot; data-original-width=&quot;2868&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scale of TpuGraphs compared to other graph property prediction datasets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We provide baseline learned cost models with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represented as a graph. Node features, shown in blue below, consist of two parts. The first part is an &lt;em>;opcode id&lt;/em>;, the most important information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id to an &lt;em>;opcode embedding&lt;/em>; via an embedding lookup table. The opcode embedding is then concatenated with the second part, the rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed-size embedding of the graph using a simple graph pooling reduction (ie, sum and mean). The resulting graph embedding is then linearly transformed into the final scalar output by a feedforward layer. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1106&quot; data-original-width=&quot;2284&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Our baseline learned cost model employs a GNN since programs can be naturally represented as graphs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Furthermore we present &lt;a href=&quot;https://arxiv.org/abs/2305.12322&quot;>;Graph Segment Training&lt;/a>; (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases where the prediction task is on the entire-graph (ie, graph-level prediction). Unlike scaling training for node- or edge-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs can contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN model is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a random subset of segments is selected to update the model; embeddings for the remaining segments are produced without saving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to generate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the historical embedding table to efficiently obtain graph segments&#39; embeddings and segment dropout to mitigate the staleness from historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;434&quot; data-original-width=&quot;790&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparing Full Graph Training (typical method) vs Graph Segment Training (our proposed method).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Kaggle competition&lt;/h2>; &lt;p>; Finally, we ran the “&lt;a href=&quot;https://kaggle.com/competitions/predict-ai-model-runtime&quot;>;Fast or Slow? Predict AI Model Runtime&lt;/a>;” competition over the TpuGraph dataset. This competition ended with 792 participants on 616 teams. We had 10507 submissions from 66 countries. For 153 users (including 47 in the top 100), this was their first competition. We learned many interesting new techniques employed by the participating teams, such as: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Graph pruning / compression&lt;/em>;: Instead of using the GST method, many teams experimented with different ways to compress large graphs (eg, keeping only subgraphs that include the configurable nodes and their immediate neighbors). &lt;/li>;&lt;li>;&lt;em>;Feature padding value&lt;/em>;: Some teams observed that the default padding value of 0 is problematic because 0 clashes with a valid feature value, so using a padding value of -1 can improve the model accuracy significantly. &lt;/li>;&lt;li>;&lt;em>;Node features&lt;/em>;: Some teams observed that additional node features (such as &lt;a href=&quot;https://www.tensorflow.org/xla/operation_semantics#dot&quot;>;dot general&#39;s contracting dimensions&lt;/a>;) are important. A few teams found that different encodings of node features also matter. &lt;/li>;&lt;li>;&lt;em>;Cross-configuration attention&lt;/em>;: A winning team designed a simple layer that allows the model to explicitly &quot;compare&quot; configs against each other. This technique is shown to be much better than letting the model infer for each config individually. &lt;/li>; &lt;/ul>; &lt;p>; We will debrief the competition and preview the winning solutions at the competition session at the &lt;a href=&quot;https://mlforsystems.org/&quot;>;ML for Systems workshop&lt;/a>; at NeurIPS on December 16, 2023. Finally, congratulations to all the winners and thank you for your contributions to advancing research in ML for systems! &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;NeurIPS expo&lt;/h2>; &lt;p>; If you are interested in more research about structured data and artificial intelligence, we hosted the NeurIPS Expo panel &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;Graph Learning Meets Artificial Intelligence&lt;/a>; on December 9, which covered advancing learned cost models and more! &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Sami Abu-el-Haija (Google Research) contributed significantly to this work and write-up. The research in this post describes joint work with many additional collaborators including Mike Burrows, Kaidi Cao, Bahare Fatemi, Jure Leskovec, Charith Mendis, Dustin Zelle, and Yanqi Zhou.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6088118107306075362/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/advancements-in-machine-learning-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6088118107306075362&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6088118107306075362&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/advancements-in-machine-learning-for.html&quot; rel=&quot;alternate&quot; title=&quot;Advancements in machine learning for machine learning&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8VzlEwBVhUqzyMz9pok3Vx0ft_8X_IZyvjpiFtX7PewhMGRzI6UmfEtUKSQ_kiO7CW8-KQ1OmDujYmHpwLvRndQFqNnmQJnEgnMo5Gj5tJi5aVnscmlo7vbHFFhS8l-yMoIICJLm80V9EPpyIObG-07Cw_VYjzWNfaZ0E_vp4f8v_WGpn1lU8F3LsJHIx/s72-c/Screenshot%202023-12-15%20at%202.33.10%E2%80%AFPM.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6479608460479432217&lt;/id>;&lt;published>;2023-12-15T11:40:00.000-08:00&lt;/published>;&lt;updated>;2023-12-15T12:18:41.742-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NeurIPS&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Style Transfer&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;StyleDrop: Text-to-image generation in any style&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kihyuk Sohn and Dilip Krishnan, Research Scientists, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC-uKMbsFvxRmRWkokKR4VinfUZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s1600/StyleDrop%20hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Text-to-image models trained on large volumes of image-text pairs have enabled the creation of rich and diverse images encompassing many genres and themes. Moreover, popular styles such as “anime” or “steampunk”, when added to the input text prompt, may translate to specific visual outputs. While many efforts have been put into &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;prompt engineering&lt;/a>;, a wide range of styles are simply hard to describe in text form due to the nuances of color schemes, illumination, and other characteristics. As an example, “watercolor painting” may refer to various styles, and using a text prompt that simply says “watercolor painting style” may either result in one specific style or an unpredictable mix of several. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNKqXN-7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s959/image5.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;959&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjNi6hwWTqFdzKvJ9qxXf5ppyfhixq1qERPyXZqBmdrPVP4ObR94N1pVVIJAGZseKbsLtOcf6qm2M0LgRtNKqXN-7qjC7Isz1keA8Pm4_ADYuzywpXLb3h7W4H5p5X3f7Y_A21uLNQWceazq-Ex6YOLCDzacl0Umk-EhqZvMUz0aZHG9nvxxb52gw-zKz/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;When we refer to &quot;watercolor painting style,&quot; which do we mean? Instead of specifying the style in natural language, StyleDrop allows the generation of images that are consistent in style by referring to a style reference image&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In this blog we introduce “&lt;a href=&quot;https://arxiv.org/abs/2306.00983&quot;>;StyleDrop: Text-to-Image Generation in Any Style&lt;/a>;”, a tool that allows a significantly higher level of stylized text-to-image synthesis. Instead of seeking text prompts to describe the style, StyleDrop uses one or more style&lt;em>; reference images&lt;/em>; that describe the style for text-to-image generation. By doing so, StyleDrop enables the generation of images in a style consistent with the reference, while effectively circumventing the burden of text prompt engineering. This is done by efficiently fine-tuning the pre-trained text-to-image generation models via &lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;adapter tuning&lt;/a>; on a few style参考图像。 Moreover, by iteratively fine-tuning the StyleDrop on a set of images it generated, it achieves the style-consistent image generation from text prompts. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Method overview&lt;/h2>; &lt;p>; StyleDrop is a text-to-image generation model that allows generation of images whose visual styles are consistent with the user-provided style reference images. This is achieved by a couple of iterations of parameter-efficient fine-tuning of pre-trained text-to-image generation models. Specifically, we build StyleDrop on &lt;a href=&quot;https://muse-model.github.io/&quot;>;Muse&lt;/a>;, a text-to-image generative vision transformer. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Muse: text-to-image generative vision transformer&lt;/h3>; &lt;p>; &lt;a href=&quot;https://muse-model.github.io/&quot;>;Muse&lt;/a>; is a state-of-the-art text-to-image generation model based on the masked generative image transformer (&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf&quot;>;MaskGIT&lt;/a>;). Unlike diffusion models, such as &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>; or &lt;a href=&quot;https://github.com/CompVis/stable-diffusion&quot;>;Stable Diffusion&lt;/a>;, Muse represents an image as a sequence of discrete tokens and models their distribution using a transformer architecture. Compared to diffusion models, Muse is known to be faster while achieving competitive generation quality. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Parameter-efficient adapter tuning&lt;/h3>; &lt;p>; StyleDrop is built by fine-tuning the pre-trained Muse model on a few style reference images and their corresponding text prompts. There have been many works on parameter-efficient fine-tuning of transformers, including &lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;>;prompt tuning&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;>;Low-Rank Adaptation&lt;/a>; (LoRA) of large language models. Among those, we opt for adapter tuning, which is shown to be effective at fine-tuning a large transformer network for &lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;>;language&lt;/a>; and &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf&quot;>;image generation&lt;/a>; tasks in a parameter-efficient manner. For example, it introduces less than one million trainable parameters to fine-tune a Muse model of 3B parameters, and it requires only 1000 training steps to converge. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s1632/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1152&quot; data-original-width=&quot;1632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCPhYjNXD7G0dv1imverMl1A5gav3nkrpRawxAUFWmpsFaqE70_IlTN4jAMGc9V8HrYLzgY6bgoOfc0QtqszPFzKu6a6so7Abf52d3Kyu-77Di6YvRncF81xEJoEhSfSKHFlvrhH7JAs9Unmpp43ixlUat-9X8O4g0AF-4XeuW_RXAzmuIpfpTjt06KQyn/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Parameter-efficient adapter tuning of Muse.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Iterative training with feedback&lt;/h3>; &lt;p>; While StyleDrop is effective at learning styles from a few style reference images, it is still challenging to learn from a single style reference image. This is because the model may not effectively disentangle the &lt;em>;content &lt;/em>;(ie, what is in the image) and the &lt;em>;style&lt;/em>; (ie, how it is being presented), leading to reduced &lt;em>;text controllability&lt;/em>; in generation. For example, as shown below in Step 1 and 2, a generated image of a chihuahua from StyleDrop trained from a single style reference image shows a leakage of content (ie, the house) from the style reference image. Furthermore, a generated image of a temple looks too similar to the house in the reference image (concept collapse). &lt;/p>; &lt;p>; We address this issue by training a new StyleDrop model on a subset of synthetic images, chosen by the user or by image-text alignment models (eg, &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>;), whose images are generated by the first round of the StyleDrop model trained on a single image. By training on multiple synthetic image-text aligned images, the model can easily disentangle the style from the content, thus achieving improved image-text alignment. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s1295/image3.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;825&quot; data-original-width=&quot;1295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWlXhdkbVOB2b33N_H_vfUEXv4ivi4pIsKmUj8d43-V5BCrWM2thw5UBF6ErzS150xlcWSvi8adysDj6WYWAIw416CwaV4R3GvkMSa3CVubpTR0FC-JsX6zmgnG-EtKQPQvzhdKY20wJ-vko62BXQ8GdbQ8c0qi7AtrEUSNycZ3XWht7MWxJm0I7gwrUeT/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Iterative training with feedback&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>;. The first round of StyleDrop may result in reduced text controllability, such as a content leakage or concept collapse, due to the difficulty of content-style disentanglement. Iterative training using synthetic images, generated by the previous rounds of StyleDrop models and chosen by human or image-text alignment models, improves the text adherence of stylized text-to-image generation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;StyleDrop gallery&lt;/h3>; &lt;p>; We show the effectiveness of StyleDrop by running experiments on 24 distinct style reference images. As shown below, the images generated by StyleDrop are highly consistent in style with each other and with the style reference image, while depicting various contexts, such as a baby penguin, banana, piano, etc. Moreover, the model can render alphabet images with a consistent style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/s1632/image6.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1632&quot; data-original-width=&quot;1536&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPYKGwhNBOcWcPl8NMXQvJdUhAQhmmUiaPLHLLk0iHTxCKCkTL2gib1sNq8Df0HbdMbdpsxhp4wEH4z5uNtnWnR2ldPyRTLwUFp8tDyQgt3EQTl8g557icN8XaWCIrK_Lr516C9z66szRIzFTtCZpgeisARikILbRT8qKdgNpodKgbO9msxNcgbKRcWXhf/w602-h640/image6.gif&quot; width=&quot;602&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stylized text-to-image generation. Style reference images&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>; are on the left inside the yellow box. Text prompts used are:&lt;br>; First row: a baby penguin, a banana, a bench.&lt;br>; Second row: a butterfly, an F1 race car, a Christmas tree.&lt;br>; Third row: a coffee maker, a hat, a moose.&lt;br>; Fourth row: a robot, a towel, a wood cabin.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s1526/image1.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;767&quot; data-original-width=&quot;1526&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2y-TXUKBTk37cq0owx6UTkkIogGQMYEBXp6-1cYy6TwhGv97_4ySH3UmsTo9SH6PVO9NIti2uv94c1FIJrPYpAyPPdjBT2pS6Bgq2CAppCGBU-sw4QmUpDrN-1lrlpmtxCBRIN3AQN07IJ7tPeSRUvJ5clWly_CclPYukT3zTesLMT2iau076NwlYzrsi/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Stylized visual character generation. Style reference images&lt;sup id=&quot;fnref1&quot;>;&lt;a href=&quot;#fn1&quot; rel=&quot;footnote&quot;>;&lt;span style=&quot;font-size: x-small;&quot;>;*&lt;/span>;&lt;/a>;&lt;/sup>; are on the left inside the yellow box. Text prompts used are: (first row) letter &#39;A&#39;, letter &#39;B&#39;, letter &#39;C&#39;, (second row) letter &#39;E&#39;, letter &#39;F&#39;, letter &#39;G&#39;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Generating images of my object in my style&lt;/h3>; &lt;p>; Below we show generated images by sampling from two personalized generation distributions, one for an object and another for the style. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lrXSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s1006/image8.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;412&quot; data-original-width=&quot;1006&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3rr2OGiWAUqJ6GzI9BpKwzmQlyOJKqALq2oY_3lrXSqGDi63z6i876V6POhhioRDctXt-e4lVaKXkae0yKJdurRp8-rmsqoLkj-oXWqi4xe4HCM2FmKf6knGa_jd5MLGOg6zYHIu62Ye7xSU5q516AcHijL5UsPUAQ2wXFznnT90pn3wxp_s5PBEXf_rc/s16000/image8.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images at the top in the blue border are object reference images from the DreamBooth dataset (teapot, vase, dog and cat), and the image on the left at the bottom in the red border is the style reference image*. Images in the purple border (ie the four lower right images) are generated from the style image of the specific object.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h3>;Quantitative results&lt;/h3>; &lt;p>; For the quantitative evaluation, we synthesize images from a subset of &lt;a href=&quot;https://github.com/google-research/parti/blob/main/PartiPrompts.tsv&quot;>;Parti prompts&lt;/a>; and measure the &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-to-image CLIP score&lt;/a>; for style consistency and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;image-to-text CLIP score&lt;/a>; for text consistency. We study non–fine-tuned models of Muse and Imagen. Among fine-tuned models, we make a comparison to &lt;a href=&quot;https://dreambooth.github.io/&quot;>;DreamBooth&lt;/a>; on &lt;a href=&quot;https://imagen.research.google/&quot;>;Imagen&lt;/a>;, state-of-the-art personalized text-to-image method for subjects. We show two versions of StyleDrop, one trained from a single style reference image, and another, “StyleDrop (HF)”, that is trained iteratively using synthetic images with human feedback as described above. As shown below, StyleDrop (HF) shows significantly improved style consistency score over its non–fine-tuned counterpart (0.694 vs. 0.556), as well as DreamBooth on Imagen (0.694 vs. 0.644). We observe an improved text consistency score with StyleDrop (HF) over StyleDrop (0.322 vs. 0.313). In addition, in a human preference study between DreamBooth on Imagen and StyleDrop on Muse, we found that 86% of the human raters preferred StyleDrop on Muse over DreamBooth on Imagen in terms of consistency to the style reference image. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s2980/image16.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1000&quot; data-original-width=&quot;2980&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiID8cTIDI32_6aRRoVhwxHDBJnPrnk_etBx904nBw0kFBDD6z99ttJqzz2574I-irFJ4_5dg0xHPqHPdpgeFi68Tl1gu4pciiChyphenhyphenFS-wxEgkqvML-2aNHBFSD9qc9-aBrhCBfPhJHHQG5hmWEDn5YwXzH3aQ91kNKUqDPude-kCIcKNpmhiJNvYDvI4ux5/s16000/image16.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; StyleDrop achieves style consistency at text-to-image generation using a few style reference images. Google&#39;s AI Principles guided our development of Style Drop, and we urge the responsible use of the technology. StyleDrop was adapted to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-style&quot;>;create a custom style model in Vertex AI&lt;/a>;, and we believe it could be a helpful tool for art directors and graphic designers — who might want to brainstorm or prototype visual assets in their own styles, to improve their productivity and boost their creativity — or businesses that want to generate new media assets that reflect a particular brand. As with other generative AI capabilities, we recommend that practitioners ensure they align with copyrights of any media assets they use. More results are found on our &lt;a href=&quot;https://styledrop.github.io/&quot;>;project website&lt;/a>; and &lt;a href=&quot;https://youtu.be/gNHD0_hkJ9A&quot;>;YouTube video&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan. &lt;/em>;We thank owners of images used in our experiments (&lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/main/images/assets/data.md&quot;>;links&lt;/a>; for attribution) for sharing their valuable assets. &lt;/p>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: x-small;&quot;>;&lt;sup>;&lt;a name=&quot;fn1&quot;>;&lt;b>;*&lt;/b>;&lt;/a>;&lt;/sup>;See &lt;a href=&quot;https://github.com/styledrop/styledrop.github.io/blob/main/images/assets/data.md&quot;>;image sources&lt;/a>;&amp;nbsp;&lt;a href=&quot;#fnref1&quot; rev=&quot;footnote&quot;>;&lt;sup>;↩&lt;/sup>;&lt;/a>;&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6479608460479432217/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/styledrop-text-to-image-generation-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6479608460479432217&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/styledrop-text-to-image-generation-in.html&quot; rel=&quot;alternate&quot; title=&quot;StyleDrop: Text-to-image generation in any style&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEAbsx3XSFVK_2XyQI-KD2x-h0D2qTD0QZzeVyTLk9umNZJbXEiQk7O84xb8eyNQkfNIu6bqg9UcqVUFC-uKMbsFvxRmRWkokKR4VinfUZsYZlSBdY7BU905YIxWfrCtmCMkT7wcnpL1nnRgN0xPE15uhsLl0CvI8D2OI3ZgBTcRq3G1zVPUJu7L9tO_P8/s72-c/StyleDrop%20hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8223613466421639113&lt;/id>;&lt;published>;2023-12-10T06:11:00.000-08:00&lt;/published>;&lt;updated>;2024-01-22T09:18:36.334-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at NeurIPS 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Catherine Armato, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s1100/google_research_sticker-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; This week the 37th annual &lt;a href=&quot;https://neurips.cc/Conferences/2023&quot;>;Conference on Neural Information Processing Systems&lt;/a>; (NeurIPS 2023), the biggest machine learning conference of the year, kicks off in New Orleans, LA. Google is proud to be a &lt;a href=&quot;https://neurips.cc/Conferences/2023/Sponsors&quot;>;Diamond Level sponsor&lt;/a>; of NeurIPS this year and will have a strong presence with &amp;gt;170 accepted papers, two keynote talks, and additional contributions to the broader research community through organizational support and involvement in &amp;gt;20 workshops and tutorials. Google is also proud to be a Platinum Sponsor for both the &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;Women in Machine Learning&lt;/a>; and &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;LatinX in AI&lt;/a>; workshops. We look forward to sharing some of our extensive ML research and expanding our partnership with the broader ML research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Attending for NeurIPS 2023 in person? Come visit the Google Research booth to learn more about the exciting work we&#39;re doing to solve some of the field&#39;s most interesting challenges. Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) account to find out about Google booth activities (eg, demos and Q&amp;amp;A sessions). &lt;/p>; &lt;p>; You can learn more about our latest cutting edge work being presented at the conference in the list below (Google affiliations highlighted in &lt;strong>;bold&lt;/strong>;). And see &lt;a href=&quot;https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023&quot;>;Google DeepMind&#39;s blog&lt;/a>; to learn more about their participation at NeurIPS 2023. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; NeurIPS Board: &lt;strong>;Corinna Cortes&lt;/strong>;&lt;br />; Advisory Board: &lt;strong>;John C. Platt&lt;/ strong>;&lt;br />; Senior Area Chair: &lt;strong>;Inderjit S. Dhillon&lt;/strong>;&lt;br />; Creative AI Chair: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; Program Chair: &lt;strong>;Amir Globerson &lt;/strong>;&lt;br />; Datasets and Benchmarks Chair:&lt;b>; Remi Denton&lt;/b>;&lt;br />;Expo Chair: &lt;b>;Wenming Ye&lt;/b>;&lt;/p>; &lt;/div>; &lt;div style= &quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research Booth Demo/Q&amp;amp;A Schedule&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;h4>;该时间表可能会发生变化。 Please visit the Google booth (#215) for more information.&lt;/h4>; &lt;p>; What You See is What You Read? Improving Text-Image Alignment Evaluation&lt;br />; Presenter: &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;12:15PM - 1:45PM&lt;/em>; &lt;/p>; &lt;p>; Talk like a Graph: Encoding Graphs for Large Language Models&lt;br />; Presenters: &lt;strong>;Bahar Fatemi&lt;/strong>;,&lt;strong>; Jonathan Halcrow&lt;/strong>;,&lt;strong>; Bryan Perozzi&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use&lt;br />; Presenter: &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Monday, Dec 11 | &lt;em>;4:00PM - 4:45PM&lt;/em>; &lt;/p>; &lt;p>; MLCommons Croissant&lt;br />; Presenters: &lt;strong>;Omar Benjelloun&lt;/strong>;,&lt;strong>; Meg Risdal&lt;/strong>;, &lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model&lt;br />; Presenter: &lt;strong>;Xiuye Gu&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Embedding Large Graphs&lt;br />; Presenters: &lt;strong>;Bryan Perozzi&lt;/strong>;, &lt;strong>;Anton Tsitsulin&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Correlated Noise Provably Beats Independent Noise for Differentially Private Learning&lt;br />; Presenter: &lt;strong>;Krishna Pillutla&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;3:20PM - 3:40PM&lt;/em>; &lt;/p>; &lt;p>; Med-PaLM&lt;br />; Presenter: &lt;strong>;Tao Tu&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; StyleDrop: Text-to-Image Generation in Any Style&lt;br />; Presenters: &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;&lt;br />; Tuesday, Dec 12 | &lt;em>;4:45PM - 5:15PM&lt;/em>; &lt;/p>; &lt;p>; DICES Dataset: Diversity in Conversational AI Evaluation for Safety&lt;br />; Presenters: &lt;strong>;Lora Aroyo&lt;/strong>;, &lt;strong>;Alicia Parrish&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;9:15AM - 10:00AM&lt;/em>; &lt;/p>; &lt;p>; Resonator: Scalable Game-Based Evaluation of Large Models&lt;br />; Presenters: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Michal Todorovic&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Adversarial Nibbler&lt;br />; Presenter: &lt;strong>;Lora Aroyo&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;12:45PM - 2:15PM&lt;/em>; &lt;/p>; &lt;p>; Towards Generalist Biomedical AI&lt;br />; Presenter: &lt;strong>;Tao Tu&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; Conditional Adaptors&lt;br />; Presenter: &lt;strong>;Junwen Bai&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;3:15PM - 3:30PM&lt;/em>; &lt;/p>; &lt;p>; Patient Assistance with Multimodal RAG&lt;br />; Presenters: &lt;strong>;Ryan Knuffman&lt;/strong>;, &lt;strong>;Milica Cvetkovic&lt;/strong>;&lt;br />; Wednesday, Dec 13 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;p>; How Hessian Structure Explains Mysteries in Sharpness Regularization&lt;br />; Presenter: &lt;strong>;Hossein Mobahi&lt;/strong>;&lt;br />; Wednesday, 12 月 13 日 | &lt;em>;4:15PM - 5:00PM&lt;/em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Keynote Speakers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73993&quot;>;The Many Faces of Responsible AI&lt;/a>;&lt;br />; Speaker: &lt;strong>;Lora Aroyo&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/invited-talk/73987&quot;>;Sketching: Core Tools, Learning-Augmentation, and Adaptive Robustness&lt;/a>;&lt;br />; Speaker: &lt;strong>;Jelani Nelson&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Affinity Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66602&quot;>;Women in ML&lt;/a>;&lt;br />; &lt;strong>;Google Sponsored - Platinum&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66607&quot;>;LatinX in AI&lt;/a>;&lt;br />; &lt;strong>;Google Sponsored - Platinum&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/affinity-workshop/66605&quot;>;New in ML&lt;/a>;&lt;br />; Organizer: &lt;strong>;Sangnie Bhardwaj&lt;/strong>;&lt;br>; Speaker: &lt;strong>;Milind Tambe&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66541&quot;>;AI for Accelerated Materials Design&lt;/a>; (AI4Mat-2023)&lt;br />; Fireside Chat: &lt;strong>;Gowoon Cheon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66524&quot;>;Associative Memory &amp;amp; Hopfield Networks in 2023&lt;/a>;&lt;br />; Panelist: &lt;strong>;Blaise Agüera y Arcas&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66535&quot;>;Information-Theoretic Principles in Cognitive Systems&lt;/a>; (InfoCog)&lt;br />; Speaker: &lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66518&quot;>;Machine Learning and the Physical Sciences&lt;/a>;&lt;br />; Speaker: &lt;strong>;Alexander Alemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66494&quot;>;UniReps: Unifying Representations in Neural Models&lt;/a>;&lt;br />; Organizer: &lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66517&quot;>;Robustness of Zero/Few-shot Learning in Foundation Models&lt;/a>; (R0-FoMo)&lt;br />; Speaker: &lt;strong>;Partha Talukdar&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Ananth Balashankar&lt;/strong>;, &lt;strong>;Yao Qin&lt;/strong>;, &lt;strong>;Ahmad Beirami&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66539&quot;>;Workshop on Diffusion Models&lt;/a>;&lt;br />; Speaker: &lt;strong>;Tali Dekel&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66502&quot;>;Algorithmic Fairness through the Lens of Time&lt;/a>;&lt;br />; Roundtable Lead: &lt;strong>;Stephen Pfohl&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Golnoosh Farnadi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66550&quot;>;Backdoors in Deep Learning: The Good, the Bad, and the Ugly&lt;/a>;&lt;br />; Organizer: &lt;strong>;Eugene Bagdasaryan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66549&quot;>;OPT 2023: Optimization for Machine Learning&lt;/a>;&lt;br />; Organizer: &lt;strong>;Cristóbal Guzmán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66545&quot;>;Machine Learning for Creativity and Design&lt;/a>;&lt;br />; Speaker: &lt;strong>;Aleksander Holynski&lt;/strong>;, &lt;strong>;Alexander Mordvintsev&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66511&quot;>;Robot Learning Workshop: Pretraining, Fine-Tuning, and Generalization with Large Scale Models&lt;/a>;&lt;br />; Speaker: &lt;strong>;Matt Barnes&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66516&quot;>;Machine Learning for Audio&lt;/a>;&lt;br />; Organizer: &lt;strong>;Shrikanth Narayanan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66531&quot;>;Federated Learning in the Age of Foundation Models&lt;/a>; (FL@FM-NeurIPS&#39;23)&lt;br />; Speaker: &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, &lt;strong>;Zheng Xu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66526&quot;>;Socially Responsible Language Modelling Research&lt;/a>; (SoLaR)&lt;br />; Panelist: &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66506&quot;>;I Can&#39;t Believe It&#39;s Not Better (ICBINB): Failure Modes in the Age of Foundation Models&lt;/a>;&lt;br />; Advisory Board: &lt;strong>;Javier Antorán&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66501&quot;>;Machine Learning for Systems&lt;/a>;&lt;br />; Organizer: &lt;strong>;Yawen Wang&lt;/strong>;&lt;br />; Competition Committee: &lt;strong>;Bryan Perozzi&lt;/strong>;, &lt;strong>;Sami Abu-el-haija&lt;/strong>;&lt;br />; Steering Committee: &lt;strong>;Milad Hashemi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66514&quot;>;Self-Supervised Learning: Theory and Practice&lt;/a>;&lt;br />; Organizer: &lt;strong>;Mathilde Caron&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/workshop/66496&quot;>;Attributing Model Behavior at Scale&lt;/a>; (ATTRIB 2023)&lt;br />; Organizers: Kevin Guu*, Tolga Bolukbasi* &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Competitions&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66581&quot;>;NeurIPS 2023 Machine Unlearning Competition&lt;/a>;&lt;br />; Organizer: &lt;b>;Isabelle Guyon&lt;/b>;, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/competition/66593&quot;>;Lux AI Challenge Season 2 NeurIPS Edition&lt;/a>;&lt;br />; Organizer: &lt;strong>;Bovard Doerschuk-Tiberi&lt;/strong>;, &lt;strong>;Addison Howard&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/tutorial/73949&quot;>;Data-Centric AI for Reliable and Responsible AI: From Theory to Practice&lt;/a>;&lt;br />; &lt;strong>;Isabelle Guyon&lt;/strong>;, Nabeel Seedat, Mihaela va der Schaar &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Creative AI Track&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Creative AI Performances 1 &amp;amp; 2&lt;br />; Speaker: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74093&quot;>;Performance 1&lt;/a>;: Mon, Dec 11 | 6:30PM - 8:30PM, Lobby Stage&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74094&quot;>;Performance 2&lt;/a>;: 12 月 14 日，星期四 | 7:00PM - 9:00PM, Lobby Stage&lt;/em>; &lt;/p>; &lt;p>; Creative AI Sessions 1 – 3&lt;br />; Speaker: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>;&lt;br />;&lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74090&quot;>;Session 1&lt;/a>;: Tue, Dec 12 | 3:05PM - 3:40PM, Hall D2&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74091&quot;>;Session 2&lt;/a>;: Wed, Dec 13 | 10:45AM - 2:15PM, Hall D2&lt;/em>;&lt;br />; &lt;em>;&lt;a href=&quot;https://nips.cc/virtual/2023/session/74092&quot;>;Session 3&lt;/a>;: 12 月 14 日，星期四 | 10:45 AM - 2:15PM, Hall D2&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/session/75889&quot;>;Creative AI Videos&lt;/a>;&lt;br />; Organizer: &lt;strong>;Isabelle Guyon&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Expo Talks&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78252&quot;>;Graph Learning Meets Artificial Intelligence&lt;/a>;&lt;br />; Speaker: &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78243&quot;>;Resonator: Music Space&lt;/a>;&lt;br />; Speakers: &lt;strong>;Erin Drake Kajioka&lt;/strong>;, &lt;strong>;Michal Todorovic&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/Expo/Conferences/2023/talk%20panel/78237&quot;>;Empirical Rigor in ML as a Massively Parallelizable Challenge&lt;/a>;&lt;br />; Speaker: &lt;strong>;Megan Risdal (Kaggle)&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Oral Talks&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sW8yGZ4uVJ&quot;>;Ordering-based Conditions for Global Convergence of Policy Gradient Methods&lt;/a>;&lt;br />; Jincheng Mei, Bo Dai, &lt;strong>;Alekh Agarwal&lt;/strong>;,&lt;strong>; &lt;/strong>;Mohammad Ghavamzadeh*, Csaba Szepesvari, Dale Schuurmans &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=y8UAQQHVTX&quot;>;Private Everlasting Prediction&lt;/a>;&lt;br />; Moni Naor, Kobbi Nissim, &lt;strong>;Uri Stemmer&lt;/strong>;, Chao Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PITeSdYQkv&quot;>;User-Level Differential Privacy With Few Examples Per User&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, Raghu Meka, &lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dVaWCDMBof&quot;>;DataComp: In Search of the Next Generation of Multimodal Datasets&lt;/a>;&lt;br />; Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, &lt;strong>;Pang Wei Koh&lt;/strong>;, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=w116w62fxH&quot;>;Optimal Learners for Realizable Regression: PAC Learning and Online Learning&lt;/a>;&lt;br />; Idan Attias, Steve Hanneke, Alkis Kalavasis, &lt;strong>;Amin Karbasi&lt;/strong>;, Grigoris Velegkas &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jDIlzSU8wJ&quot;>;The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation&lt;/a>;&lt;br />; Saurabh Saxena, &lt;strong>;Charles Herrmann&lt;/strong>;,&lt;strong>; Junhwa Hur&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;,&lt;strong>; &lt;/strong>;Mohammad Norouzi*, &lt;strong>;Deqing Sun&lt;/strong>;,&lt;strong>; &lt;/strong>;David J. Fleet &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Journal Track&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://jmlr.org/papers/v24/20-998.html&quot;>;Graph Clustering with Graph Neural Networks&lt;/a>;&lt;br />; &lt;strong>;Anton Tsitsulin&lt;/strong>;, &lt;strong>;John Palowitch&lt;/strong>;,&lt;strong>; Bryan Perozzi&lt;/strong>;, Emmanuel Müller &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Spotlight Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1p6teT6F73&quot;>;Alternating Updates for Efficient Transformers&lt;/a>; (see blog post)&lt;br />; &lt;strong>;Cenk Baykal&lt;/strong>;, &lt;strong>;Dylan Cutler&lt;/strong>;, &lt;strong>;Nishanth Dikkala&lt;/strong>;, Nikhil Ghosh*, &lt;strong>;Rina Panigrahy&lt;/strong>;, &lt;strong>;Xin Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EldbUlZtbd&quot;>;Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models&lt;/a>;&lt;br />; &lt;strong>;Peter Hase&lt;/strong>;, Mohit Bansal, &lt;strong>;Been Kim&lt;/strong>;,&lt;strong>; Asma Ghandeharioun&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jR2FkqW6GB&quot;>;Is Learning in Games Good for the Learners?&lt;/a>;&lt;br />; William Brown,&lt;strong>; Jon Schneider&lt;/strong>;,&lt;strong>; Kiran Vodrahalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Bj1QSgiBPP&quot;>;Participatory Personalization in Classification&lt;/a>;&lt;br />; Hailey Joren, &lt;strong>;Chirag Nagpal&lt;/strong>;,&lt;strong>; Katherine Heller&lt;/strong>;,&lt;strong>; &lt;/strong>;Berk Ustun &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=c2eedxSlPJ&quot;>;Tight Risk Bounds for Gradient Descent on Separable Data&lt;/a>;&lt;br />; Matan Schliserman, &lt;strong>;Tomer Koren&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=67o9UQgTD0&quot;>;Counterfactual Memorization in Neural Language Models&lt;/a>;&lt;br />; &lt;strong>;Chiyuan Zhang&lt;/strong>;,&lt;strong>; &lt;/strong>;Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, Nicholas Carlini &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5NxJuc0T1P&quot;>;Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models&lt;/a>;&lt;br />; &lt;strong>;Zhong Yi Wan&lt;/strong>;, Ricardo Baptista,&lt;strong>; Anudhyan Boral&lt;/strong>;, &lt;strong>;Yi-Fan Chen&lt;/strong>;,&lt;strong>; John Anderson&lt;/strong>;,&lt;strong>; Fei Sha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=vTug54Uunq&quot;>;Faster Margin Maximization Rates for Generic Optimization Methods&lt;/a>;&lt;br />; Guanghui Wang, Zihao Hu, Vidya Muthukumar, &lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3PjCt4kmRx&quot;>;From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces&lt;/a>;&lt;br />; Peter Shaw, Mandar Joshi, &lt;strong>;James Cohan&lt;/strong>;,&lt;strong>; &lt;/strong>;Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina N Toutanova &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5Gw9YkJkFF&quot;>;PAC Learning Linear Thresholds from Label Proportions&lt;/a>;&lt;br />; &lt;strong>;Anand Brahmbhatt&lt;/strong>;,&lt;strong>; Rishi Saket&lt;/strong>;,&lt;strong>; Aravindan Raghuveer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CXPUg86A1D&quot;>;SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs&lt;/a>;&lt;br />; Lijun Yu*, &lt;strong>;Yong Cheng&lt;/strong>;,&lt;strong>; &lt;/strong>;Zhiruo Wang, &lt;strong>;Vivek Kumar&lt;/strong>;,&lt;strong>; Wolfgang Macherey&lt;/strong>;, &lt;strong>;Yanping Huang&lt;/strong>;,&lt;strong>; David Ross&lt;/strong>;,&lt;strong>; Irfan Essa&lt;/strong>;,&lt;strong>; &lt;/strong>;Yonatan Bisk,&lt;strong>; Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Kevin Murphy&lt;/strong>;,&lt;strong>; &lt;/strong>;Alexander Hauptmann,&lt;strong>; Lu Jiang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QatZNssk7T&quot;>;Adaptive Data Analysis in a Balanced Adversarial Model&lt;/a>;&lt;br />; Kobbi Nissim,&lt;strong>; Uri Stemmer&lt;/strong>;,&lt;strong>; &lt;/strong>;Eliad Tsfadia &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NiQTy0NW1L&quot;>;Lexinvariant Language Models&lt;/a>;&lt;br />; Qian Huang, Eric Zelikman, Sarah Chen,&lt;strong>; Yuhuai Wu&lt;/strong>;, Gregory Valiant, Percy Liang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HF6bnhfSqH&quot;>;On Quantum Backpropagation, Information Reuse, and Cheating Measurement Collapse&lt;/a>;&lt;br />; &lt;strong>;Amira Abbas&lt;/strong>;,&lt;strong>; &lt;/strong>;Robbie King, Hsin-Yuan Huang, &lt;strong>;William J. Huggins&lt;/strong>;, &lt;strong>;Ramis Movassagh&lt;/strong>;,&lt;strong>; Dar Gilboa&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Jarrod McClean&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Pya0kCEpDk&quot;>;Private Estimation Algorithms for Stochastic Block Models and Mixture Models&lt;/a>;&lt;br />; Hongjie Chen,&lt;strong>; Vincent Cohen-Addad&lt;/strong>;,&lt;strong>; &lt;/strong>;Tommaso d&#39;Orsi,&lt;strong>; Alessandro Epasto&lt;/strong>;, Jacob Imola, David Steurer, Stefan Tiegel &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DBz9E5aZey&quot;>;Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation&lt;/a>;&lt;br />; &lt;strong>;Aniket Das&lt;/strong>;, &lt;strong>;Dheeraj Nagaraj&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=e0pRF9tOtm&quot;>;Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>;,&lt;strong>; &lt;/strong>;Daogao Liu*, Sewoong Oh, Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=bKqrWLCMrX&quot;>;Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts&lt;/a>;&lt;br />; Pritam Sarkar, &lt;strong>;Ahmad Beirami&lt;/strong>;,&lt;strong>; Ali Etemad&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ikkdTD3hQJ&quot;>;AIMS: All-Inclusive Multi-Level Segmentation for Anything&lt;/a>;&lt;br />; Lu Qi, Jason Kuen, Weidong Guo, Jiuxiang Gu, Zhe Lin, Bo Du, Yu Xu, &lt;strong>;Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rheCTpRrxI&quot;>;DreamHuman: Animatable 3D Avatars from Text&lt;/a>;&lt;br />; &lt;strong>;Nikos Kolotouros&lt;/strong>;,&lt;strong>; Thiemo Alldieck&lt;/strong>;,&lt;strong>; Andrei Zanfir&lt;/strong>;, &lt;strong>;Eduard Gabriel Bazavan&lt;/strong>;, &lt;strong>;Mihai Fieraru&lt;/strong>;, &lt;strong>;Cristian Sminchisescu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=oaCDiKoJ2w&quot;>;Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts&lt;/a>;&lt;br />; Chaoqi Wang, Ziyu Ye, &lt;strong>;Zhe Feng&lt;/strong>;, &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;, Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=m21rQusNgb&quot;>;Learning List-Level Domain-Invariant Representations for Ranking&lt;/a>;&lt;br />; Ruicheng Xian*, &lt;strong>;Honglei Zhuang&lt;/strong>;, &lt;strong>;Zhen Qin&lt;/strong>;, Hamed Zamani*, &lt;strong>;Jing Lu&lt;/strong>;,&lt;strong>; Ji Ma&lt;/strong>;, &lt;strong>;Kai Hui&lt;/strong>;, Han Zhao, &lt;strong>;Xuanhui Wang&lt;/strong>;, &lt;strong>;Michael Bendersky&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hCdqDkA25J&quot;>;Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization&lt;/a>;&lt;br />; Liang Zhang, Junchi Yang, &lt;strong>;Amin Karbasi&lt;/strong>;, Niao He &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=hJzEoQHfCe&quot;>;Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems&lt;/a>;&lt;br />; Benjamin Coleman, Wang-Cheng Kang, &lt;strong>;Matthew Fahrbach&lt;/strong>;,&lt;strong>; &lt;/strong>;Ruoxi Wang, Lichan Hong, Ed Chi, Derek Cheng &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xOJUmwwlJc&quot;>;Proximity-Informed Calibration for Deep Neural Networks&lt;/a>;&lt;br />; Miao Xiong, Ailin Deng, &lt;strong>;Pang Wei Koh&lt;/strong>;, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WfsWy59bX2&quot;>;Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization&lt;/a>;&lt;br />; &lt;strong>;Adel Javanmard&lt;/strong>;,&lt;strong>; Vahab Mirrokni&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EUiIbwV379&quot;>;Better Private Linear Regression Through Better Private Feature Selection&lt;/a>;&lt;br />; &lt;strong>;Travis Dick&lt;/strong>;, Jennifer Gillenwater*, &lt;strong>;Matthew Joseph&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=XAyPlfmWpu&quot;>;Binarized Neural Machine Translation&lt;/a>;&lt;br />; Yichi Zhang, Ankush Garg, Yuan Cao, &lt;strong>;Łukasz Lew&lt;/strong>;, Behrooz Ghorbani*, Zhiru Zhang, Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://nips.cc/virtual/2023/poster/73665&quot;>;BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information&lt;/a>;&lt;br />; &lt;strong>;Mehran Kazemi&lt;/strong>;,&lt;strong>; Quan Yuan&lt;/strong>;, &lt;strong>;Deepti Bhatia&lt;/strong>;, &lt;strong>;Najoung Kim&lt;/strong>;, &lt;strong>;Xin Xu&lt;/strong>;, &lt;strong>;Vaiva Imbrasaite&lt;/strong>;, &lt;strong>;Deepak Ramachandran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1vvsIJtnnr&quot;>;Boosting with Tempered Exponential Measures&lt;/a>;&lt;br />; &lt;strong>;Richard Nock&lt;/strong>;,&lt;strong>; &lt;/strong>;Ehsan Amid,&lt;strong>; Manfred Warmuth&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SGlrCuwdsB&quot;>;Concept Algebra for (Score-Based) Text-Controlled Generative Models&lt;/a>;&lt;br />; Zihao Wang, Lin Gui, Jeffrey Negrea, &lt;strong>;Victor Veitch&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q8mH2d6uw2&quot;>;Deep Contract Design via Discontinuous Networks&lt;/a>;&lt;br />; Tonghan Wang, &lt;strong>;Paul Dütting&lt;/strong>;, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YoghyvSG0H&quot;>;Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection&lt;/a>;&lt;br />; Cheng-Ju Ho, Chen-Hsuan Tai, Yen-Yu Lin, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;,&lt;strong>; Yi-Hsuan Tsai&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PYASzxr2OP&quot;>;Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback&lt;/a>;&lt;br />; Han Shao, Lee Cohen, Avrim Blum, &lt;strong>;Yishay Mansour&lt;/strong>;, Aadirupa Saha, Matthew Walter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qCglMj6A4z&quot;>;Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy&lt;/a>;&lt;br />; Anastasia Koloskova*, &lt;strong>;Ryan McKenna&lt;/strong>;, &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;J Keith Rush&lt;/strong>;, &lt;strong>;Hugh Brendan McMahan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCwToX315b&quot;>;Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products&lt;/a>;&lt;br />; &lt;strong>;Tamas Sarlos&lt;/strong>;,&lt;strong>; &lt;/strong>;Xingyou Song, David P. Woodruff, Qiuyi (Richard) Zhang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JhQP33aMx2&quot;>;Module-wise Adaptive Distillation for Multimodality Foundation Models&lt;/a>;&lt;br />;&lt;br />; Chen Liang, &lt;strong>;Jiahui Yu&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;, &lt;strong>;Matthew Brown&lt;/strong>;, Yin Cui, Tuo Zhao, &lt;strong>;Boqing Gong&lt;/strong>;, Tianyi Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zGRWp7yRqd&quot;>;Multi-Swap k-Means++&lt;/a>;&lt;br />; Lorenzo Beretta, &lt;strong>;Vincent Cohen-Addad&lt;/strong>;, &lt;strong>;Silvio Lattanzi&lt;/strong>;,&lt;strong>; Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8vuDHCxrmy&quot;>;OpenMask3D: Open-Vocabulary 3D Instance Segmentation&lt;/a>;&lt;br />; Ayça Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys,&lt;strong>; Federico Tombari&lt;/strong>;, &lt;strong>;Francis Engelmann&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7RMGI4slcb&quot;>;Order Matters in the Presence of Dataset Imbalance for Multilingual Learning&lt;/a>;&lt;br />; Dami Choi*, &lt;strong>;Derrick Xin&lt;/strong>;, &lt;strong>;Hamid Dadkhahi&lt;/strong>;, Justin Gilmer, Ankush Garg, Orhan Firat, Chih-Kuan Yeh, Andrew M. Dai, Behrooz Ghorbani &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=yEf8NSqTPu&quot;>;PopSign ASL v1.0: An Isolated American Sign Language Dataset Collected via Smartphones&lt;/a>;&lt;br />; Thad Starner, Sean Forbes, Matthew So, David Martin, Rohit Sridhar, Gururaj Deshpande, &lt;strong>;Sam Sepah&lt;/strong>;, Sahir Shahryar, Khushi Bhardwaj, Tyler Kwok, Daksh Sehgal, Saad Hassan, Bill Neubauer, Sofia Vempala, Alec Tan, Jocelyn Heath, Unnathi Kumar, Priyanka Mosur, Tavenner Hall, Rajandeep Singh, Christopher Cui, &lt;strong>;Glenn Cameron&lt;/strong>;, &lt;strong>;Sohier Dane&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gaktiSjatl&quot;>;Semi-Implicit Denoising Diffusion Models (SIDDMs)&lt;/a>;&lt;br />; Yanwu Xu*, Mingming Gong, Shaoan Xie, &lt;strong>;Wei Wei&lt;/strong>;,&lt;strong>; Matthias Grundmann&lt;/strong>;,&lt;strong>; &lt;/strong>;Kayhan Batmanghelich, &lt;strong>;Tingbo Hou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xGz0wAIJrS&quot;>;State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding&lt;/a>;&lt;br />; Devleena Das, Sonia Chernova, &lt;strong>;Been Kim&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AwhpBEqmyo&quot;>;StoryBench: A Multifaceted Benchmark for Continuous Story Visualization&lt;/a>;&lt;br />; Emanuele Bugliarello*, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan,&lt;strong>; Vittorio Ferrari&lt;/strong>;, Pieter-Jan Kindermans, &lt;strong>;Paul Voigtlaender&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wv3bHyQbX7&quot;>;Subject-driven Text-to-Image Generation via Apprenticeship Learning&lt;/a>;&lt;br />; Wenhu Chen, Hexiang Hu, &lt;strong>;Yandong Li&lt;/strong>;, &lt;strong>;Nataniel Ruiz&lt;/strong>;, &lt;strong>;Xuhui Jia&lt;/strong>;, Ming-Wei Chang, William W. Cohen &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=plAix1NxhU&quot;>;TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs&lt;/a>;&lt;br />; &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;, &lt;strong>;Sami Abu-El-Haija&lt;/strong>;, Kaidi Cao*, &lt;strong>;Bahare Fatemi&lt;/strong>;, &lt;strong>;Mike Burrows&lt;/strong>;, Charith Mendis*, &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=a147pIS2Co&quot;>;Training Chain-of-Thought via Latent-Variable Inference&lt;/a>;&lt;br />; &lt;strong>;Du Phan&lt;/strong>;, &lt;strong>;Matthew D. Hoffman&lt;/strong>;, David Dohan*, Sholto Douglas,&lt;strong>; Tuan Anh Le&lt;/strong>;, Aaron Parisi, &lt;strong>;Pavel Sountsov&lt;/strong>;, Charles Sutton, Sharad Vikram,&lt;strong>; Rif A. Saurous&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1ZzG6td0el&quot;>;Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints&lt;/a>;&lt;br />; Jayadev Acharya, Clement L. Canonne, &lt;strong>;Ziteng Sun&lt;/strong>;, Himanshu Tyagi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=j5AoleAIru&quot;>;What You See is What You Read? Improving Text-Image Alignment Evaluation&lt;/a>;&lt;br />; &lt;strong>;Michal Yarom&lt;/strong>;, &lt;strong>;Yonatan Bitton&lt;/strong>;, &lt;strong>;Soravit Changpinyo&lt;/strong>;, &lt;strong>;Roee Aharoni&lt;/strong>;, &lt;strong>;Jonathan Herzig&lt;/strong>;,&lt;strong>; Oran Lang&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Eran Ofek&lt;/strong>;, &lt;strong>;Idan Szpektor&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=4KZhZJSPYU&quot;>;When Does Confidence-Based Cascade Deferral Suffice?&lt;/a>;&lt;br />; &lt;strong>;Wittawat Jitkrittum&lt;/strong>;, &lt;strong>;Neha Gupta&lt;/strong>;, &lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Harikrishna Narasimhan&lt;/strong>;, &lt;strong>;Ankit Singh Rawat&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=A18PgVSUgf&quot;>;Accelerating Molecular Graph Neural Networks via Knowledge Distillation&lt;/a>;&lt;br />; Filip Ekström Kelvinius, Dimitar Georgiev, Artur Petrov Toshev, &lt;strong>;Johannes Gasteiger&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7EMphtUgCI&quot;>;AVIS: Autonomous Visual Information Seeking with Large Language Model Agent&lt;/a>;&lt;br />; Ziniu Hu*, &lt;strong>;Ahmet Iscen&lt;/strong>;, &lt;strong>;Chen Sun&lt;/strong>;, Kai-Wei Chang, Yizhou Sun, &lt;strong>;David Ross&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>;, &lt;strong>;Alireza Fathi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=9mJXDcr17V&quot;>;Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing &quot;Spurious&quot; Correlations&lt;/a>;&lt;br />; Qingyao Sun, Kevin Patrick Murphy, &lt;strong>;Sayna Ebrahimi&lt;/strong>;, Alexander D&#39;Amour &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0tEjORCGFD&quot;>;Collaborative Score Distillation for Consistent Visual Editing&lt;/a>;&lt;br />; Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, &lt;strong>;Kihyuk Sohn&lt;/strong>;, Jinwoo Shin &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1SF2tiopYJ&quot;>;CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs&lt;/a>;&lt;br />; Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, &lt;strong>;Federico Tombari&lt;/strong>;, Nassir Navab, Benjamin Busam &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=65aDEXIhih&quot;>;Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;, Nathan Srebro, Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BopG5dhH7L&quot;>;A Computationally Efficient Sparsified Online Newton Method&lt;/a>;&lt;br />; Fnu Devvrit*, Sai Surya Duvvuri,&lt;strong>; &lt;/strong>;Rohan Anil, &lt;strong>;Vineet Gupta&lt;/strong>;, &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, &lt;strong>;Inderjit S Dhillon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6EDHfVHicP&quot;>;DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field&lt;/a>;&lt;br />; Chenyangguang Zhang, Yan Di, Ruida Zhang, Guangyao Zhai, &lt;strong>;Fabian Manhardt&lt;/strong>;, &lt;strong>;Federico Tombari&lt;/strong>;, Xiangyang Ji &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2nTpPxJ5Bs&quot;>;Double Auctions with Two-sided Bandit Feedback&lt;/a>;&lt;br />; &lt;strong>;Soumya Basu&lt;/strong>;, Abishek Sankararaman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2305.19234&quot;>;Grammar Prompting for Domain-Specific Language Generation with Large Language Models&lt;/a>;&lt;br />; Bailin Wang, Zi Wang, Xuezhi Wang, &lt;strong>;Yuan Cao&lt;/strong>;, Rif A. Saurous, Yoon Kim &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5SIz31OGFV&quot;>;Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training&lt;/a>;&lt;br />; Rie Johnson, Tong Zhang* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3YDukx2cpr&quot;>;Large Graph Property Prediction via Graph Segment Training&lt;/a>;&lt;br />; Kaidi Cao*, &lt;strong>;Phitchaya Mangpo Phothilimthana&lt;/strong>;, &lt;strong>;Sami Abu-El-Haija&lt;/strong>;, &lt;strong>;Dustin Zelle&lt;/strong>;, &lt;strong>;Yanqi Zhou&lt;/strong>;,&lt;strong>; &lt;/strong>;Charith Mendis*, Jure Leskovec, &lt;strong>;Bryan Perozzi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=1GxKVprbwM&quot;>;On Computing Pairwise Statistics with Local Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Adam Sealfon&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=7UdVPRmpif&quot;>;On Student-teacher Deviations in Distillation: Does it Pay to Disobey?&lt;/a>;&lt;br />; &lt;strong>;Vaishnavh Nagarajan&lt;/strong>;, &lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Srinadh Bhojanapalli&lt;/strong>;, &lt;strong>;Hossein Mobahi&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CzkOzKWpMa&quot;>;Optimal Cross-learning for Contextual Bandits with Unknown Context Distributions&lt;/a>;&lt;br />; &lt;strong>;Jon Schneider&lt;/strong>;, &lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8XRMbNAP6Z&quot;>;Near-Optimal k-Clustering in the Sliding Window Model&lt;/a>;&lt;br />; David Woodruff, &lt;strong>;Peilin Zhong&lt;/strong>;, Samson Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=3H37XciUEv&quot;>;Post Hoc Explanations of Language Models Can Improve Language Models&lt;/a>;&lt;br />; Satyapriya Krishna, Jiaqi Ma, Dylan Z Slack, &lt;strong>;Asma Ghandeharioun&lt;/strong>;, Sameer Singh, Himabindu Lakkaraju &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=BJ0fQUU32w&quot;>;Recommender Systems with Generative Retrieval&lt;/a>;&lt;br />; Shashank Rajput*&lt;strong>;, &lt;/strong>;Nikhil Mehta, Anima Singh, &lt;strong>;Raghunandan Hulikal Keshavan&lt;/strong>;, &lt;strong>;Trung Vu, Lukasz Heldt&lt;/strong>;,&lt;strong>; &lt;/strong>;Lichan Hong, Yi Tay&lt;strong>;, Vinh Q. Tran&lt;/strong>;, &lt;strong>;Jonah Samost&lt;/strong>;, Maciej Kula, Ed H. Chi, Maheswaran Sathiamoorthy &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8OTPepXzeh&quot;>;Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models&lt;/a>;&lt;br />; &lt;strong>;Ying Fan&lt;/strong>;, Olivia Watkins, Yuqing Du, Hao Liu, &lt;strong>;Moonkyung Ryu&lt;/strong>;,&lt;strong>; Craig Boutilier&lt;/strong>;, Pieter Abbeel, Mohammad Ghavamzadeh*, Kangwook Lee, Kimin Lee* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5VQFAvUHcd&quot;>;Replicable Clustering&lt;/a>;&lt;br />; &lt;strong>;Hossein Esfandiari&lt;/strong>;, &lt;strong>;Amin Karbasi&lt;/strong>;, &lt;strong>;Vahab Mirrokni&lt;/strong>;, Grigoris Velegkas, Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=5cPz5hrjy6&quot;>;Replicability in Reinforcement Learning&lt;/a>;&lt;br />; &lt;strong>;Amin Karbasi&lt;/strong>;, Grigoris Velegkas, Lin Yang, Felix Zhou &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=szFqlNRxeS&quot;>;Riemannian Projection-free Online Learning&lt;/a>;&lt;br />; Zihao Hu, Guanghui Wang, &lt;strong>;Jacob Abernethy&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=29WbraPk8U&quot;>;Sharpness-Aware Minimization Leads to Low-Rank Features&lt;/a>;&lt;br />; Maksym Andriushchenko, &lt;strong>;Dara Bahri&lt;/strong>;, &lt;strong>;Hossein Mobahi&lt;/strong>;, Nicolas Flammarion &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=2hQ7MBQApp&quot;>;What is the Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models&lt;/a>;&lt;br />; Khashayar Gatmiry, Zhiyuan Li, Ching-Yao Chuang, &lt;strong>;Sashank Reddi&lt;/strong>;, Tengyu Ma, Stefanie Jegelka &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=JzQlGqBm8d&quot;>;Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization&lt;/a>;&lt;br />; Jui-Nan Yen, Sai Surya Duvvuri, &lt;strong>;Inderjit S Dhillon&lt;/strong>;, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Ntd6X7uWYF&quot;>;Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints&lt;/a>;&lt;br />; &lt;strong>;Soumyabrata Pal&lt;/strong>;, &lt;strong>;Arun Sai Suggala&lt;/strong>;, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, &lt;strong>;Prateek Jain&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=NIrTSCiIZ7&quot;>;Boundary Guided Learning-Free Semantic Control with Diffusion Models&lt;/a>;&lt;br />; Ye Zhu, Yu Wu, &lt;strong>;Zhiwei Deng&lt;/strong>;, Olga Russakovsky, Yan Yan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=IyYyKov0Aj&quot;>;Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference&lt;/a>;&lt;br />; Tao Lei, &lt;strong>;Junwen Bai&lt;/strong>;, Siddhartha Brahma,&lt;strong>; Joshua Ainslie&lt;/strong>;, Kenton Lee, Yanqi Zhou, Nan Du*, &lt;strong>;Vincent Y. Zhao&lt;/strong>;, &lt;strong>;Yuexin Wu&lt;/strong>;, Bo Li,&lt;strong>; Yu Zhang&lt;/strong>;, Ming-Wei Chang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KTRwpWCMsC&quot;>;Conformal Prediction for Time Series with Modern Hopfield Networks&lt;/a>;&lt;br />; Andreas Auer, &lt;strong>;Martin Gauch&lt;/strong>;, Daniel Klotz, Sepp Hochreiter &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PzYAMXmIT3&quot;>;Does Visual Pretraining Help End-to-End Reasoning?&lt;/a>;&lt;br />; &lt;strong>;Chen Sun&lt;/strong>;, Calvin Luo, &lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Anurag Arnab&lt;/strong>;, &lt;strong>;Cordelia Schmid&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PAYXfIUKWY&quot;>;Effective Robustness Against Natural Distribution Shifts for Models with Different Training Data&lt;/a>;&lt;br />; Zhouxing Shi*, &lt;strong>;Nicholas Carlini&lt;/strong>;, &lt;strong>;Ananth Balashankar&lt;/strong>;, Ludwig Schmidt, &lt;strong>;Cho-Jui Hsieh&lt;/strong>;, Alex Beutel*, &lt;strong>;Yao Qin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Nh5dp6Uuvx&quot;>;Improving Neural Network Representations Using Human Similarity Judgments&lt;/a>;&lt;br />; Lukas Muttenthaler*, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew K. Lampinen, Simon Kornblith &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=N6FhEMnxCU&quot;>;Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency&lt;/a>;&lt;br />; Xiyang Liu, &lt;strong>;Prateek Jain&lt;/strong>;, &lt;strong>;Weihao Kong&lt;/strong>;, &lt;strong>;Sewoong Oh&lt;/strong>;, &lt;strong>;Arun Sai Suggala&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Fdfyga5i0A&quot;>;Mnemosyne: Learning to Train Transformers with Transformers&lt;/a>;&lt;br />; Deepali Jain, Krzysztof Choromanski, &lt;strong>;Avinava Dubey&lt;/strong>;, Sumeet Singh, Vikas Sindhwani, Tingnan Zhang, Jie Tan &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=MCkUS1P3Sh&quot;>;Nash Regret Guarantees for Linear Bandits&lt;/a>;&lt;br />; Ayush Sawarni, &lt;strong>;Soumyabrata Pal&lt;/strong>;, Siddharth Barman &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2307.03043&quot;>;A Near-Linear Time Algorithm for the Chamfer Distance&lt;/a>;&lt;br />; Ainesh Bakshi, Piotr Indyk, &lt;strong>;Rajesh Jayaram&lt;/strong>;, Sandeep Silwal, Erik Waingarten. &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FviF8vuz5B&quot;>;On Differentially Private Sampling from Gaussian and Product Distributions&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Xiao Hu*, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LelK6Mfoey&quot;>;On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes&lt;/a>;&lt;br />; Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh*, Marek Petrik &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=HFQFAyNucq&quot;>;ResMem: Learn What You Can and Memorize the Rest&lt;/a>;&lt;br />; Zitong Yang, &lt;strong>;Michal Lukasik&lt;/strong>;,&lt;strong>; Vaishnavh Nagarajan&lt;/strong>;, &lt;strong>;Zonglin Li&lt;/strong>;, &lt;strong>;Ankit Singh Rawat&lt;/strong>;, &lt;strong>;Manzil Zaheer, &lt;/strong>;&lt;strong>;Aditya Krishna Menon&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PcNpL9Q39p&quot;>;Responsible AI (RAI) Games and Ensembles&lt;/a>;&lt;br />; Yash Gupta, Runtian Zhai, &lt;strong>;Arun Suggala&lt;/strong>;, Pradeep Ravikumar &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=DVlawv2rSI&quot;>;RoboCLIP: One Demonstration Is Enough to Learn Robot Policies&lt;/a>;&lt;br />; Sumedh A Sontakke, Jesse Zhang, &lt;strong>;Sébastien MR Arnold&lt;/strong>;, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, Laurent Itti &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=I6aOjhpcNQ&quot;>;Robust Concept Erasure via Kernelized Rate-Distortion Maximization&lt;/a>;&lt;br />; Somnath Basu Roy Chowdhury, Nicholas Monath, &lt;strong>;Kumar Avinava Dubey&lt;/strong>;, &lt;strong>;Amr Ahmed&lt;/strong>;, Snigdha Chaturvedi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=FmZVRe0gn8&quot;>;Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms&lt;/a>;&lt;br />; Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, &lt;strong>;Zhehui Chen&lt;/strong>;, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=PTvxck0QDE&quot;>;Simplicity Bias in 1-Hidden Layer Neural Networks&lt;/a>;&lt;br />; Depen Morwani*, Jatin Batra, &lt;strong>;Prateek Jain&lt;/strong>;,&lt;strong>; Praneeth Netrapalli&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/2302.03806&quot;>;SLaM: Student-Label Mixing for Distillation with Unlabeled Examples&lt;/a>;&lt;br />; Vasilis Kontonis,&lt;strong>; Fotis Iliopoulos&lt;/strong>;, &lt;strong>;Khoa Trinh&lt;/strong>;, &lt;strong>;Cenk Baykal&lt;/strong>;, &lt;strong>;Gaurav Menghani&lt;/strong>;, &lt;strong>;Erik Vee&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LCHmP68Gtj&quot;>;SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding&lt;/a>;&lt;br />; Paul-Edouard Sarlin*, &lt;strong>;Eduard Trulls&lt;/strong>;, Marc Pollefeys, &lt;strong>;Jan Hosang&lt;/strong>;, &lt;strong>;Simon Lynen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QvIvWMaQdX&quot;>;SOAR: Improved Indexing for Approximate Nearest Neighbor Search&lt;/a>;&lt;br />; &lt;strong>;Philip Sun&lt;/strong>;, &lt;strong>;David Simcha&lt;/strong>;, &lt;strong>;Dave Dopson&lt;/strong>;, &lt;strong>;Ruiqi Guo&lt;/strong>;, &lt;strong>;Sanjiv Kumar&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=KoaFh16uOc&quot;>;StyleDrop: Text-to-Image Synthesis of Any Style&lt;/a>;&lt;br />; &lt;strong>;Kihyuk Sohn&lt;/strong>;, &lt;strong>;Lu Jiang&lt;/strong>;, &lt;strong>;Jarred Barber&lt;/strong>;, Kimin Lee*, &lt;strong>;Nataniel Ruiz&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>;, Huiwen Chang*,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;strong>;Irfan Essa&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Yuan Hao&lt;/strong>;, &lt;strong>;Glenn Entis&lt;/strong>;, &lt;strong>;Irina Blok&lt;/strong>;, &lt;strong>;Daniel Castro Chin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=LSYQB4CwD3&quot;>;Three Towers: Flexible Contrastive Learning with Pretrained Image Models&lt;/a>;&lt;br />; Jannik Kossen*,&lt;strong>; Mark Collier&lt;/strong>;, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, &lt;strong>;Jesse Berent&lt;/strong>;,&lt;strong>; &lt;/strong>;Rodolphe Jenatton,&lt;strong>; Efi Kokiopoulou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GIlsH0T4b2&quot;>;Two-Stage Learning to Defer with Multiple Experts&lt;/a>;&lt;br />; Anqi Mao, Christopher Mohri, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ZBzYWP2Gpl&quot;>;AdANNS: A Framework for Adaptive Semantic Search&lt;/a>;&lt;br />; Aniket Rege, &lt;strong>;Aditya Kusupati&lt;/strong>;, Sharan Ranjit S, Alan Fan, Qingqing Cao, Sham Kakade, &lt;strong>;Prateek Jain&lt;/strong>;, Ali Farhadi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Srt1hhQgqa&quot;>;Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer&lt;/a>;&lt;br />; Bowen Tan*, &lt;strong>;Yun Zhu&lt;/strong>;, &lt;strong>;Lijuan Liu&lt;/strong>;, Eric Xing, Zhiting Hu, &lt;strong>;Jindong Chen&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dJZ3MvDw86&quot;>;Causal-structure Driven Augmentations for Text OOD Generalization&lt;/a>;&lt;br />; &lt;strong>;Amir Feder&lt;/strong>;, Yoav Wald, Claudia Shi, Suchi Saria, David Blei &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=S0xrBMFihS&quot;>;Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel&lt;/a>;&lt;br />; Valerii Likhosherstov, Krzysztof Choromanski, &lt;strong>;Avinava Dubey&lt;/strong>;, &lt;strong>;Frederick Liu&lt;/strong>;, &lt;strong>;Tamas Sarlos&lt;/strong>;, Adrian Weller &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vm1zeYqwdc&quot;>;Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence&lt;/a>;&lt;br />; Grace Luo, Lisa Dunlap, Dong Huk Park, &lt;strong>;Aleksander Holynski&lt;/strong>;, Trevor Darrell &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgv56R2YJ7&quot;>;Diffusion Self-Guidance for Controllable Image Generation&lt;/a>;&lt;br />; &lt;strong>;Dave Epstein&lt;/strong>;, Allan Jabri, &lt;strong>;Ben Poole&lt;/strong>;, Alexei A Efros,&lt;strong>; Aleksander Holynski&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=dnGEPkmnzO&quot;>;Fully Dynamic k-Clustering in Õ(k) Update Time&lt;/a>;&lt;br />; Sayan Bhattacharya, Martin Nicolas Costa, &lt;strong>;Silvio Lattanzi&lt;/strong>;, &lt;strong>;Nikos Parotsidis&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SVjDiiVySh&quot;>;Improving CLIP Training with Language Rewrites&lt;/a>;&lt;br />; &lt;strong>;Lijie Fan&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>;, Phillip Isola, Dina Katabi, &lt;strong>;Yonglong Tian&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=UzUhiKACmS&quot;>;k-Means Clustering with Distance-Based Privacy&lt;/a>;&lt;br />; &lt;strong>;Alessandro Epasto&lt;/strong>;, &lt;strong>;Vahab Mirrokni&lt;/strong>;, Shyam Narayanan, &lt;strong>;Peilin Zhong&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Xu8aG5Q8M3&quot;>;LayoutGPT: Compositional Visual Planning and Generation with Large Language Models&lt;/a>;&lt;br />; Weixi Feng, Wanrong Zhu, Tsu-Jui Fu,&lt;strong>; Varun Jampani&lt;/strong>;, &lt;strong>;Arjun Reddy Akula&lt;/strong>;, Xuehai He, &lt;strong>;Sugato Basu&lt;/strong>;, Xin Eric Wang, William Yang Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SxXN3kNTsV&quot;>;Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management&lt;/a>;&lt;br />; Dhawal Gupta*, &lt;strong>;Yinlam Chow&lt;/strong>;,&lt;strong>; Azamat Tulepbergenov&lt;/strong>;, Mohammad Ghavamzadeh*, &lt;strong>;Craig Boutilier&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=aG6xOP9QY7&quot;>;Optimal Unbiased Randomizers for Regression with Label Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Ashwinkumar Badanidiyuru&lt;/strong>;, &lt;strong>;Badih Ghazi&lt;/strong>;, &lt;strong>;Pritish Kamath&lt;/strong>;,&lt;strong>; Ravi Kumar&lt;/strong>;, &lt;strong>;Ethan Jacob Leeman&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Avinash V Varadarajan&lt;/strong>;, &lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=WbFhFvjjKj&quot;>;Paraphrasing Evades Detectors of AI-generated Text, but Retrieval Is an Effective Defense&lt;/a>;&lt;br />; &lt;strong>;Kalpesh Krishna&lt;/strong>;, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SaMrN9tnxE&quot;>;ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation&lt;/a>;&lt;br />; Shuyang Sun*, &lt;strong>;Weijun Wang&lt;/strong>;, Qihang Yu*, &lt;strong>;Andrew Howard&lt;/strong>;, Philip Torr, Liang-Chieh Chen* &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SouroWC5Un&quot;>;Robust and Actively Secure Serverless Collaborative Learning&lt;/a>;&lt;br />; Nicholas Franzese, Adam Dziedzic, &lt;strong>;Christopher A. Choquette-Choo&lt;/strong>;, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, &lt;strong>;Somesh Jha&lt;/strong>;, Nicolas Papernot, Xiao Wang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SdYHLTCC5J&quot;>;SpecTr: Fast Speculative Decoding via Optimal Transport&lt;/a>;&lt;br />; &lt;strong>;Ziteng Sun&lt;/strong>;, &lt;strong>;Ananda Theertha Suresh&lt;/strong>;, &lt;strong>;Jae Hun Ro&lt;/strong>;,&lt;strong>; Ahmad Beirami&lt;/strong>;, &lt;strong>;Himanshu Jain&lt;/strong>;,&lt;strong>; Felix Yu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=YZ7ip645Ra&quot;>;Structured Prediction with Stronger Consistency Guarantees&lt;/a>;&lt;br />; Anqi Mao, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=qgiG7WZohZ&quot;>;Affinity-Aware Graph Networks&lt;/a>;&lt;br />; &lt;strong>;Ameya Velingker&lt;/strong>;,&lt;strong>; Ali Kemal Sinop&lt;/strong>;, Ira Ktena, Petar Veličković, &lt;strong>;Sreenivas Gollapudi&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rJc5Lsn5QU&quot;>;ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections&lt;/a>;&lt;br />; Chun-Han Yao*, &lt;strong>;Amit Raj&lt;/strong>;, Wei-Chih Hung,&lt;strong>; Yuanzhen Li&lt;/strong>;, &lt;strong>;Michael Rubinstein&lt;/strong>;, &lt;strong>;Ming-Hsuan Yang&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Varun Jampani&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=eoDNaH3pfB&quot;>;Black-Box Differential Privacy for Interactive ML&lt;/a>;&lt;br />; &lt;strong>;Haim Kaplan&lt;/strong>;, &lt;strong>;Yishay Mansour&lt;/strong>;, &lt;strong>;Shay Moran&lt;/strong>;, Kobbi Nissim, &lt;strong>;Uri Stemmer&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=orh4e0AO9R&quot;>;Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits&lt;/a>;&lt;br />; Haolin Liu, Chen-Yu Wei, &lt;strong>;Julian Zimmert&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kXOXrVnwbb&quot;>;DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model&lt;/a>;&lt;br />; &lt;strong>;Xiuye Gu&lt;/strong>;, Yin Cui*, &lt;strong>;Jonathan Huang&lt;/strong>;, &lt;strong>;Abdullah Rashwan&lt;/strong>;, &lt;strong>;Xuan Yang&lt;/strong>;, &lt;strong>;Xingyi Zhou&lt;/strong>;, &lt;strong>;Golnaz Ghiasi&lt;/strong>;, &lt;strong>;Weicheng Kuo&lt;/strong>;, &lt;strong>;Huizhong Chen&lt;/strong>;, Liang-Chieh Chen*, &lt;strong>;David Ross&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kqBUgrkm1c&quot;>;Easy Learning from Label Proportions&lt;/a>;&lt;br />; &lt;strong>;Robert Busa-Fekete&lt;/strong>;, Heejin Choi*, &lt;strong>;Travis Dick&lt;/strong>;, &lt;strong>;Claudio Gentile&lt;/strong>;, &lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=q3fCWoC9l0&quot;>;Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks&lt;/a>;&lt;br />; Eeshaan Jain, Tushar Nandy, &lt;strong>;Gaurav Aggarwal&lt;/strong>;, &lt;strong>;Ashish Tendulkar&lt;/strong>;, Rishabh Iyer, Abir De &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h2lkx9SQCD&quot;>;Faster Differentially Private Convex Optimization via Second-Order Methods&lt;/a>;&lt;br />; &lt;strong>;Arun Ganesh&lt;/strong>;, Mahdi Haghifam*, Thomas Steinke, Abhradeep Guha Thakurta &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gdVcFOvxT3&quot;>;Finding Safe Zones of Markov Decision Processes Policies&lt;/a>;&lt;br />; Lee Cohen, &lt;strong>;Yishay Mansour&lt;/strong>;, Michal Moshkovitz &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=s1FjXzJ0jy&quot;>;Focused Transformer: Contrastive Training for Context Scaling&lt;/a>;&lt;br />; Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu*, Henryk Michalewski, Piotr Miłoś &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=h3kuB4z2G9&quot;>;Front-door Adjustment Beyond Markov Equivalence with Limited Graph Knowledge&lt;/a>;&lt;br />; Abhin Shah, &lt;strong>;Karthikeyan Shanmugam&lt;/strong>;, Murat Kocaoglu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nI7EmXq2PL&quot;>;H-Consistency Bounds: Characterization and Extensions&lt;/a>;&lt;br />; Anqi Mao, &lt;strong>;Mehryar Mohri&lt;/strong>;, Yutao Zhong &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=kjMGHTo8Cs&quot;>;Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation&lt;/a>;&lt;br />; David Brandfonbrener, &lt;strong>;Ofir Nachum&lt;/strong>;, Joan Bruna &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pvPujuvjQd&quot;>;Most Neural Networks Are Almost Learnable&lt;/a>;&lt;br />; &lt;strong>;Amit Daniely&lt;/strong>;, Nathan Srebro, Gal Vardi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=nQ84YY9Iut&quot;>;Multiclass Boosting: Simple and Intuitive Weak Learning Criteria&lt;/a>;&lt;br />; Nataly Brukhim, &lt;strong>;Amit Daniely&lt;/strong>;, &lt;strong>;Yishay Mansour&lt;/strong>;, &lt;strong>;Shay Moran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=gJHAT79cZU&quot;>;NeRF Revisited: Fixing Quadrature Instability in Volume Rendering&lt;/a>;&lt;br />; Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas,&lt;strong>; Ke Li&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=izNfcaHJk0&quot;>;Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation&lt;/a>;&lt;br />; Wei-Ning Chen, Dan Song, Ayfer Ozgur, &lt;strong>;Peter Kairouz&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=rzDBoh1tBh&quot;>;Private Federated Frequency Estimation: Adapting to the Hardness of the Instance&lt;/a>;&lt;br />; Jingfeng Wu*, &lt;strong>;Wennan Zhu&lt;/strong>;, &lt;strong>;Peter Kairouz&lt;/strong>;, Vladimir Braverman &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=pVlC0reMKq&quot;>;RETVec: Resilient and Efficient Text Vectorizer&lt;/a>;&lt;br />; &lt;strong>;Elie Bursztein&lt;/strong>;, &lt;strong>;Marina Zhang&lt;/strong>;, &lt;strong>;Owen Skipper Vallis&lt;/strong>;, &lt;strong>;Xinyu Jia&lt;/strong>;, &lt;strong>;Alexey Kurakin&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=ne6zeqLFCZ&quot;>;Symbolic Discovery of Optimization Algorithms&lt;/a>;&lt;br />; Xiangning Chen*, &lt;strong>;Chen Liang&lt;/strong>;, &lt;strong>;Da Huang&lt;/strong>;, &lt;strong>;Esteban Real&lt;/strong>;, &lt;strong>;Kaiyuan Wang&lt;/strong>;, &lt;strong>;Hieu Pham&lt;/strong>;, &lt;strong>;Xuanyi Dong&lt;/strong>;, &lt;strong>;Thang Luong&lt;/strong>;, Cho-Jui Hsieh, &lt;strong>;Yifeng Lu&lt;/strong>;, &lt;strong>;Quoc V. Le&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=lds9D17HRd&quot;>;A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence&lt;/a>;&lt;br />; Junyi Zhang, &lt;strong>;Charles Herrmann&lt;/strong>;, &lt;strong>;Junhwa Hur&lt;/strong>;, &lt;strong>;Luisa F. Polania&lt;/strong>;, &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Deqing Sun&lt;/strong>;,&lt;strong>; Ming-Hsuan Yang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=iSd8g75QvP&quot;>;A Trichotomy for Transductive Online Learning&lt;/a>;&lt;br />; Steve Hanneke, &lt;strong>;Shay Moran&lt;/strong>;, Jonathan Shafer &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=i2H2sEiq2T&quot;>;A Unified Fast Gradient Clipping Framework for DP-SGD&lt;/a>;&lt;br />; &lt;strong>;William Kong&lt;/strong>;, &lt;strong>;Andres Munoz Medina&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=mlbes5TAAg&quot;>;Unleashing the Power of Randomization in Auditing Differentially Private ML&lt;/a>;&lt;br />; &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>;Galen Andrew&lt;/strong>;, &lt;strong>;Peter Kairouz&lt;/strong>;, &lt;strong>;H. Brendan McMahan&lt;/strong>;, &lt;strong>;Alina Oprea&lt;/strong>;, &lt;strong>;Sewoong Oh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zEm6hF97Pz&quot;>;(Amplified) Banded Matrix Factorization: A unified approach to private training&lt;/a>;&lt;br />; Christopher A Choquette-Choo,&lt;strong>; Arun Ganesh&lt;/strong>;, &lt;strong>;Ryan McKenna&lt;/strong>;, &lt;strong>;H Brendan McMahan&lt;/strong>;, &lt;strong>;Keith Rush&lt;/strong>;,&lt;strong>; &lt;/strong>;Abhradeep Guha Thakurta, &lt;strong>;Zheng Xu&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xcGhx9FdxM&quot;>;Adversarial Resilience in Sequential Prediction via Abstention&lt;/a>;&lt;br />; Surbhi Goel, Steve Hanneke, &lt;strong>;Shay Moran&lt;/strong>;, Abhishek Shetty &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uTlKUAm68H&quot;>;Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception&lt;/a>;&lt;br />; &lt;strong>;Hassan Akbari&lt;/strong>;, &lt;strong>;Dan Kondratyuk&lt;/strong>;, &lt;strong>;Yin Cui&lt;/strong>;, &lt;strong>;Rachel Hornung&lt;/strong>;, &lt;strong>;Huisheng Wang&lt;/strong>;, &lt;strong>;Hartwig Adam&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/pdf/0083ab45a583fe1992b3c4f9222081c50b9d30c3.pdf&quot;>;Android in the Wild: A Large-Scale Dataset for Android Device Control&lt;/a>;&lt;br />; &lt;strong>;Christopher Rawles&lt;/strong>;, &lt;strong>;Alice Li&lt;/strong>;, &lt;strong>;Daniel Rodriguez&lt;/strong>;, &lt;strong>;Oriana Riva&lt;/strong>;, Timothy Lillicrap &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=CiRHWaRbp0&quot;>;Benchmarking Robustness to Adversarial Image Obfuscations&lt;/a>;&lt;br />; Florian Stimberg, &lt;strong>;Ayan Chakrabarti&lt;/strong>;, &lt;strong>;Chun-Ta Lu&lt;/strong>;, &lt;strong>;Hussein Hazimeh&lt;/strong>;, &lt;strong>;Otilia Stretcu&lt;/strong>;, &lt;strong>;Wei Qiao&lt;/strong>;,&lt;strong>; Yintao Liu&lt;/strong>;, &lt;strong>;Merve Kaya&lt;/strong>;, &lt;strong>;Cyrus Rashtchian&lt;/strong>;, &lt;strong>;Ariel Fuxman&lt;/strong>;, &lt;strong>;Mehmet Tek&lt;/strong>;, Sven Gowal &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uIj1jDc8k6&quot;>;Building Socio-culturally Inclusive Stereotype Resources with Community Engagement&lt;/a>;&lt;br />; &lt;strong>;Sunipa Dev&lt;/strong>;, Jaya Goyal, &lt;strong>;Dinesh Tewari&lt;/strong>;, &lt;strong>;Shachi Dave&lt;/strong>;, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=L9I9FhHfS3&quot;>;Consensus and Subjectivity of Skin Tone Annotation for ML Fairness&lt;/a>;&lt;br />; &lt;strong>;Candice Schumann&lt;/strong>;, &lt;strong>;Gbolahan O Olanubi&lt;/strong>;, &lt;strong>;Auriel Wright&lt;/strong>;, Ellis Monk Jr*, &lt;strong>;Courtney Heldreth&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Susanna Ricco&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zdli6OxpWd&quot;>;Counting Distinct Elements Under Person-Level Differential Privacy&lt;/a>;&lt;br />; &lt;strong>;Alexander Knop&lt;/strong>;, Thomas Steinke &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=GjNvvswoUL&quot;>;DICES Dataset: Diversity in Conversational AI Evaluation for Safety&lt;/a>;&lt;br />; &lt;strong>;Lora Aroyo&lt;/strong>;, Alex S. Taylor, &lt;strong>;Mark Diaz&lt;/strong>;, &lt;strong>;Christopher M. Homan&lt;/strong>;, &lt;strong>;Alicia Parrish&lt;/strong>;, Greg Serapio-García, &lt;strong>;Vinodkumar Prabhakaran&lt;/strong>;, &lt;strong>;Ding Wang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=SS3CK3yx5Z&quot;>;Does Progress on ImageNet Transfer to Real-world Datasets?&lt;/a>;&lt;br />; Alex Fang, &lt;strong>;Simon Kornblith&lt;/strong>;, Ludwig Schmidt &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=AA2uO0HHmr&quot;>;Estimating Generic 3D Room Structures from 2D Annotations&lt;/a>;&lt;br />; Denys Rozumnyi*, &lt;strong>;Stefan Popov&lt;/strong>;, &lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;, Matthias Nießner, &lt;strong>;Vittorio Ferrari&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=6hZIfAY9GD&quot;>;Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias&lt;/a>;&lt;br />; Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, &lt;strong>;Jiaming Shen&lt;/strong>;,&lt;strong>; &lt;/strong>;Chao Zhang &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Y45ZCxslFx&quot;>;MADLAD-400: A Multilingual And Document-Level Large Audited Dataset&lt;/a>;&lt;br />; Sneha Kudugunta, &lt;strong>;Isaac Caswell&lt;/strong>;, Biao Zhang, Xavier Garcia, Derrick Xin, &lt;strong>;Aditya Kusupati&lt;/strong>;, Romi Stella, Ankur Bapna, Orhan Firat &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=uhKtQMn21D&quot;>;Mechanic: A Learning Rate Tuner&lt;/a>;&lt;br />; Ashok Cutkosky, Aaron Defazio, &lt;strong>;Harsh Mehta&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=8TMhs2pIfG&quot;>;NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations&lt;/a>;&lt;br />; &lt;strong>;Varun Jampani&lt;/strong>;, &lt;strong>;Kevis-kokitsi Maninis&lt;/strong>;, &lt;strong>;Andreas Engelhardt&lt;/strong>;, &lt;strong>;Arjun Karpur&lt;/strong>;, &lt;strong>;Karen Truong&lt;/strong>;, &lt;strong>;Kyle Sargent&lt;/strong>;, &lt;strong>;Stefan Popov&lt;/strong>;, &lt;strong>;Andre Araujo&lt;/strong>;, &lt;strong>;Ricardo Martin Brualla&lt;/strong>;, &lt;strong>;Kaushal Patel&lt;/strong>;, &lt;strong>;Daniel Vlasic&lt;/strong>;,&lt;strong>; Vittorio Ferrari&lt;/strong>;, &lt;strong>;Ameesh Makadia&lt;/strong>;, Ce Liu*, &lt;strong>;Yuanzhen Li&lt;/strong>;, &lt;strong>;Howard Zhou&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=x6cOcxRnxG&quot;>;Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations&lt;/a>;&lt;br />; &lt;strong>;Anudhyan Boral&lt;/strong>;, &lt;strong>;Zhong Yi Wan&lt;/strong>;, &lt;strong>;Leonardo Zepeda-Nunez&lt;/strong>;, &lt;strong>;James Lottes&lt;/strong>;, &lt;strong>;Qing Wang&lt;/strong>;, &lt;strong>;Yi-Fan Chen&lt;/strong>;,&lt;strong>; John Roberts Anderson&lt;/strong>;, &lt;strong>;Fei Sha&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=wFuemocyHZ&quot;>;Restart Sampling for Improving Generative Processes&lt;/a>;&lt;br />; Yilun Xu, Mingyang Deng, Xiang Cheng, &lt;strong>;Yonglong Tian&lt;/strong>;, Ziming Liu, Tommi Jaakkola &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xUyBP16Q5J&quot;>;Rethinking Incentives in Recommender Systems: Are Monotone Rewards Always Beneficial?&lt;/a>;&lt;br />; Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao, &lt;strong>;Yan Zhu&lt;/strong>;, Qifan Wang, Hongning Wang, Haifeng Xu &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=jGyMUum1Lq&quot;>;Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union&lt;/a>;&lt;br />; Zifu Wang, &lt;strong>;Maxim Berman&lt;/strong>;, Amal Rannen-Triki, Philip Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=0H5fRQcpQ7&quot;>;RoboHive: A Unified Framework for Robot Learning&lt;/a>;&lt;br />; &lt;strong>;Vikash Kumar&lt;/strong>;, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Abhishek Gupta, &lt;strong>;Aravind Rajeswaran&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=Vn5qZGxGj3&quot;>;SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data&lt;/a>;&lt;br />; Mélisande Teng, Amna Elmustafa, Benjamin Akera, Yoshua Bengio, Hager Radi, &lt;strong>;Hugo Larochelle&lt;/strong>;, David Rolnick &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sqTcCXkG4P&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;&lt;br />; &lt;strong>;Badih Ghazi&lt;/strong>;, Yangsibo Huang*, &lt;strong>;Pritish Kamath&lt;/strong>;, &lt;strong>;Ravi Kumar&lt;/strong>;, &lt;strong>;Pasin Manurangsi&lt;/strong>;, &lt;strong>;Amer Sinha&lt;/strong>;,&lt;strong>; &lt;/strong>;&lt;strong>;Chiyuan Zhang&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=xpjsOQtKqx&quot;>;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/a>;&lt;br />; &lt;strong>;Yonglong Tian&lt;/strong>;, &lt;strong>;Lijie Fan&lt;/strong>;, Phillip Isola, &lt;strong>;Huiwen Chang&lt;/strong>;, &lt;strong>;Dilip Krishnan&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=EPz1DcdPVE&quot;>;Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning&lt;/a>;&lt;br />; &lt;strong>;Zachary Charles&lt;/strong>;, &lt;strong>;Nicole Mitchell&lt;/strong>;, &lt;strong>;Krishna Pillutla&lt;/strong>;, &lt;strong>;Michael Reneer&lt;/strong>;, &lt;strong>;Zachary Garrett&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=zWxKYyW9ik&quot;>;Universality and Limitations of Prompt Tuning&lt;/a>;&lt;br />; Yihan Wang, Jatin Chauhan, Wei Wang, &lt;strong>;Cho-Jui Hsieh&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=sovxUzPzLN&quot;>;Unsupervised Semantic Correspondence Using Stable Diffusion&lt;/a>;&lt;br />; Eric Hedlin, Gopal Sharma, Shweta Mahajan, &lt;strong>;Hossam Isack&lt;/strong>;, &lt;strong>;Abhishek Kar&lt;/strong>;, &lt;strong>;Andrea Tagliasacchi&lt;/strong>;,&lt;strong>; &lt;/strong>;Kwang Moo Yi &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=QEDjXv9OyY&quot;>;YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus&lt;/a>;&lt;br />; &lt;strong>;Dave Uthus&lt;/strong>;, &lt;strong>;Garrett Tanzer&lt;/strong>;, &lt;strong>;Manfred Georg&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://openreview.net/forum?id=swNtr6vGqg&quot;>;The Noise Level in Linear Regression with Dependent Data&lt;/a>;&lt;br />; Ingvar Ziemann, &lt;strong>;Stephen Tu&lt;/strong>;, George J. Pappas, Nikolai Matni &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8223613466421639113/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8223613466421639113&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-neurips-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at NeurIPS 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC9GkRLKp8GI42AvrsSQqN2F8gF8QDo06YU1ulV067EXp6MU3F1yYSZ44VRxn7BZlgrSZ18249wN7vuwyeDAH4AmXHHo-ryPYOmZ771K3yhsUCkfWguTDsOTx2wBqnH1gF_hxcALrj7nq-kRL2lNttCipemJXYUitvDbRi_LNWk7bRgFpzpsNEqDeetCM2/s72-c/google_research_sticker-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6514019106482066697&lt;/id>;&lt;published>;2023-12-08T10:34:00.000-08:00&lt;/published>;&lt;updated>;2024-01-08T07:50:55.303-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Differential Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Sparsity-preserving differentially private training&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Yangsibo Huang, Research Intern, and Chiyuan Zhang, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s1600/Sparse%20DP-SGD.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;em>;Large embedding models&lt;/em>; have emerged as a fundamental tool for various applications in recommendation systems [&lt;a href=&quot;https://research.google/pubs/pub45530/&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.05158&quot;>;2&lt;/a>;] and natural language processing [&lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;>;3&lt;/a>;, &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;>;4&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;5&lt;/a>;]. Such models enable the integration of non-numerical data into deep learning models by mapping categorical or &lt;a href=&quot;https://en.wikipedia.org/wiki/String_(computer_science)&quot;>;string&lt;/a>;-valued input attributes with large vocabularies to fixed-length representation vectors using embedding layers. These models are widely deployed in personalized recommendation systems and achieve state-of-the-art performance in language tasks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;>;language modeling&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;>;sentiment analysis&lt;/a>;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;question answering&lt;/a>;. In many such scenarios, privacy is an equally important feature when deploying those models. As a result, various techniques have been proposed to enable private data analysis. Among those, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) is a widely adopted definition that limits exposure of individual user information while still allowing for the analysis of population-level patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; For training deep neural networks with DP guarantees, the most widely used algorithm is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>; (DP &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>;). One key component of DP-SGD is adding &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to every coordinate of the gradient vectors during training. However, this creates scalability challenges when applied to &lt;em>;large embedding models&lt;/em>;, because they rely on gradient sparsity for efficient training, but adding noise to all the coordinates destroys sparsity. &lt;/p>; &lt;p>; To mitigate this gradient sparsity problem, in “&lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;Sparsity-Preserving Differentially Private Training of Large Embedding Models&lt;/a>;” (to be presented at &lt;a href=&quot;https://nips.cc/Conferences/2023&quot;>;NeurIPS 2023&lt;/a>;), we propose a new algorithm called &lt;em>;adaptive filtering-enabled sparse training&lt;/em>; (DP-AdaFEST). At a high level, the algorithm maintains the sparsity of the gradient by selecting only a subset of feature rows to which noise is added at each iteration. The key is to make such selections differentially private so that a three-way balance is achieved among the privacy cost, the training efficiency, and the model utility. Our empirical evaluation shows that DP-AdaFEST achieves a substantially sparser gradient, with a reduction in gradient size of over 10&lt;sup>;5&lt;/sup>;X compared to the dense gradient produced by standard DP-SGD, while maintaining comparable levels of accuracy 。 This gradient size reduction could translate into 20X wall-clock time improvement. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Overview&lt;/h2>; &lt;p>; To better understand the challenges and our solutions to the gradient sparsity problem, let us start with an overview of how DP-SGD works during training. As illustrated by the figure below, DP-SGD operates by clipping the gradient contribution from each example in the current random subset of samples (called a mini-batch), and adding coordinate-wise &lt;a href=&quot;https://en. wikipedia.org/wiki/Gaussian_noise&quot;>;Gaussian noise&lt;/a>; to the average gradient during each iteration of &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;>;stochastic gradient descent&lt;/a>; （SGD）。 DP-SGD has demonstrated its effectiveness in protecting user privacy while maintaining model utility in a variety of applications [&lt;a href=&quot;https://arxiv.org/pdf/2204.13650.pdf&quot;>;6&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/2110.06500.pdf&quot;>;7&lt;/a>;]. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;768&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1x7i1Ul5_mXMdBgLSsDKsP2iN4vUSaSfC6qoNpOwoz_TYMVysh4JV1kru7q8hhwGcuwc5Hfj_1rW_r-_Unw2kAmdhKKD7_3I4uQE5Z34fHQdG71quOt5u82iuK1M-vTBv6MopTnVPitOisx0w_fLlbdpDZMOPh7yDdgGM3U74jweKtCnRTom8yXmVF2vG/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An illustration of how DP-SGD works. During each training step, a mini-batch of examples is sampled, and used to compute the per-example gradients. Those gradients are processed through clipping, aggregation and summation of Gaussian noise to produce the final privatized gradients.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The challenges of applying DP-SGD to large embedding models mainly come from 1) the non-numerical feature fields like user/product IDs and categories, and 2) words and tokens that are transformed into dense vectors through an embedding layer. Due to the vocabulary sizes of those features, the process requires large embedding tables with a substantial number of parameters. In contrast to the number of parameters, the gradient updates are usually extremely sparse because each mini-batch of examples only activates a tiny fraction of embedding rows (the figure below visualizes the ratio of zero-valued coordinates, ie, the sparsity, of the gradients under various batch sizes). This sparsity is heavily leveraged for industrial applications that efficiently handle the training of large-scale embeddings. For example, &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Google Cloud TPUs&lt;/a>;, custom-designed AI accelerators that are optimized for training and inference of large AI models, have &lt;a href=&quot;https://cloud.google.com/blog/topics/developers-practitioners/building-large-scale-recommenders-using-cloud-tpus&quot;>;dedicated APIs&lt;/a>; to handle large embeddings with sparse updates. This leads to &lt;a href=&quot;https://eng.snap.com/training-models-with-tpus&quot;>;significantly improved training throughput&lt;/a>; compared to training on GPUs, which at this time did not have specialized optimization for sparse embedding lookups. On the other hand, DP-SGD completely destroys the gradient sparsity because it requires adding independent Gaussian noise to &lt;em>;all&lt;/em>; the coordinates. This creates a road block for private training of large embedding models as the training efficiency would be significantly reduced compared to non-private training. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s1814/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1058&quot; data-original-width=&quot;1814&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdsCOpdtbHyfGjQfTWV7AYcyq8dEt3g2pY2Kx5BgwonmVb1XgKPMW8nEM6h-j9M1dniCOpviwIhxqNgrvC4N4T3Zwqdj-OJEXYOUiaSreBfWljgEEIIaStjpmDHrh9on18CaB_Fc4KDD1m4msv9BM5uqWC3q0qmjJe5BBlbxYJIJGMUAa4vgMkCa5jwS/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Embedding gradient sparsity (the fraction of zero-value gradient coordinates) in the &lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo pCTR&lt;/a>; model (see below). The figure reports the gradient sparsity, averaged over 50 update steps, of the top five categorical features (out of a total of 26) with the highest number of buckets, as well as the sparsity of all categorical features. The sprasity decreases with the batch size as more examples hit more rows in the embedding table, creating non-zero gradients. However, the sparsity is above 0.97 even for very large batch sizes. This pattern is consistently observed for all the five features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Algorithm&lt;/h2>; &lt;p>; Our algorithm is built by extending standard DP-SGD with an extra mechanism at each iteration to privately select the “hot features”, which are the features that are activated by multiple training examples in the current mini-batch. As illustrated below, the mechanism works in a few steps: &lt;/p>; &lt;ol>; &lt;li>;Compute how many examples contributed to each feature bucket (we call each of the possible values of a categorical feature a “bucket”). &lt;/li>;&lt;li>;Restrict the total contribution from each example by clipping their counts. &lt;/li>;&lt;li>;Add Gaussian noise to the contribution count of each feature bucket. &lt;/li>;&lt;li>;Select only the features to be included in the gradient update that have a count above a given threshold (a sparsity-controlling parameter), thus maintaining sparsity. This mechanism is differentially private, and the privacy cost can be easily computed by composing it with the standard DP-SGD iterations. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s1600/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;743&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgw4grjzgMGorydps6Oi3iCvQ6OnlWeqhbc9p68PJiycBZBkianO6esb9mo0hRlScm5zHod3isaGDp8OhkaM1d8VPuFRzUhIX34uNNrsHU5_jUrIzR2N1fJvQenxGteqxWc1t1_xMrkLGyYoxEhlBe7g-kd5AcwJwrpH4tcfmxVth2wjl-wG3iNUd-oTYTB/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of the process of the algorithm on a synthetic categorical feature that has 20 buckets. We compute the number of examples contributing to each bucket, adjust the value based on per-example total contributions (including those to other features), add Gaussian noise, and retain only those buckets with a noisy contribution exceeding the threshold for (noisy) gradient update.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Theoretical motivation&lt;/h2>; &lt;p>; We provide the theoretical motivation that underlies DP-AdaFEST by viewing it as optimization using stochastic &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_complexity_(optimization)&quot;>;gradient oracles&lt;/a>;. Standard analysis of stochastic gradient descent in a theoretical setting decomposes the test error of the model into &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot;>;“bias” and “variance” terms&lt;/a>;. The advantage of DP-AdaFEST can be viewed as reducing variance at the cost of slightly increasing the bias. This is because DP-AdaFEST adds noise to a smaller set of coordinates compared to DP-SGD, which adds noise to all the coordinates. On the other hand, DP-AdaFEST introduces some bias to the gradients since the gradient on the embedding features are dropped with some probability. We refer the interested reader to Section 3.4 of the &lt;a href=&quot;https://arxiv.org/abs/2311.08357&quot;>;paper&lt;/a>; for more details. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiments&lt;/h2>; &lt;p>; We evaluate the effectiveness of our algorithm with large embedding model applications, on public datasets, including one ad prediction dataset (&lt;a href=&quot;http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset&quot;>;Criteo-Kaggle&lt;/a>;) and one language understanding dataset (&lt;a href=&quot;https://huggingface.co/datasets/sst2&quot;>;SST-2&lt;/a>;). We use &lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>; as a baseline comparison. &lt;/p>; &lt;p>; The effectiveness of DP-AdaFEST is evident in the figure below, where it achieves significantly higher gradient size reduction (ie, gradient sparsity) than the baseline while maintaining the same level of utility (ie, only minimal performance降解）。 &lt;/p>; &lt;p>; Specifically, on the Criteo-Kaggle dataset, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 5x10&lt;sup>;5&lt;/sup>; times while maintaining a comparable &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;>;AUC&lt;/a>; (which we define as a loss of less than 0.005). This reduction translates into a more efficient and cost-effective training process. In comparison, as shown by the green line below, the baseline method is not able to achieve reasonable cost reduction within such a small utility loss threshold. &lt;/p>; &lt;p>; In language tasks, there isn&#39;t as much potential for reducing the size of gradients, because the vocabulary used is often smaller and already quite compact (shown on the right below). However, the adoption of sparsity-preserving DP-SGD effectively obviates the dense gradient computation. Furthermore, in line with the bias-variance trade-off presented in the theoretical analysis, we note that DP-AdaFEST occasionally exhibits superior utility compared to DP-SGD when the reduction in gradient size is minimal. Conversely, when incorporating sparsity, the baseline algorithm faces challenges in maintaining utility. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s1860/image6.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1860&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjve3hfmF7VyXPfhxUNK2GT3kUR6tS-5HSlvInImOwkvfPCdxRSJeGHit-2DXKjmlTkl8WY1s8vTJxAPz59C_5YirJhPAUV8-j7Z7b6ECRilTtqxvT4l6rPIWoIUqgdJxm41yv3avYS8vZ1MUHejRJMSYWmBHq70dsLHpHr5ouZ_8r2GFfYbVY5WRfCYXCS/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction (the ratio of the non-zero gradient value counts between regular DP-SGD and sparsity-preserving algorithms) achieved under ε =1.0 by DP -AdaFEST (our algorithm) and the baseline algorithm (&lt;a href=&quot;https://arxiv.org/abs/2103.01294&quot;>;DP-SGD with exponential selection&lt;/a>;) compared to DP-SGD at different thresholds for utility不同之处。 A higher curve indicates a better utility/efficiency trade-off.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In practice, most ad prediction models are being continuously trained and evaluated. To simulate this online learning setup, we also evaluate with time-series data, which are notoriously challenging due to being non-stationary. Our evaluation uses the &lt;a href=&quot;https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/&quot;>;Criteo-1TB&lt;/a>; dataset, which comprises real-world user-click data collected over 24 days. Consistently, DP-AdaFEST reduces the gradient computation cost of regular DP-SGD by more than 10&lt;sup>;4&lt;/sup>; times while maintaining a comparable AUC. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s2500/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;891&quot; data-original-width=&quot;2500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgovxJa4jAZSTvwKVCPc0hp6S8KD9hWik-3raaGC-E54_9Udj60TDZPy31ozVcZOhUbpk7QigBSYLLRYgDqvdlfSPOaDHik-_4WpyU1AmF-3ER11_RwkOGF10uSiDs18lBwnYGKbHrFX9wT4awNj3wlROpMjS4V9XtS6yf7I6_z4FlQBExtsHBCI50FV2fU/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A comparison of the best gradient size reduction achieved under ε =1.0 by DP-AdaFEST (our algorithm) and DP-SGD with exponential selection (a previous algorithm) compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms the previous method.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present a new algorithm, DP-AdaFEST, for preserving gradient sparsity in differentially private training — particularly in applications involving large embedding models, a fundamental tool for various applications in recommendation systems and natural language processing. Our algorithm achieves significant reductions in gradient size while maintaining accuracy on real-world benchmark datasets. Moreover, it offers flexible options for balancing utility and efficiency via sparsity-controlling parameters, while our proposals offer much better privacy-utility loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was a collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi and Amer Sinha.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6514019106482066697/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6514019106482066697&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/sparsity-preserving-differentially.html&quot; rel=&quot;alternate&quot; title=&quot;Sparsity-preserving differentially private training&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBm5u6Sg9vPlH8YH8q9cA1g_3nkf1tXzB6qNqZTSbpB38DQERn-pOicng-98oTsQNtza5De5ohjQV4t43a4BhICFwU7EqAs9AZI6aMHlKflVfj6s9DRSn7bkjUvW-Uq1zCtziKPi2Li3KutFXGJtENqw-HEkAa0p8r1tJgoq48PDqd7FePR3ipWWj2Iz0B/s72-c/Sparse%20DP-SGD.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5912569868305742102&lt;/id>;&lt;published>;2023-12-07T09:51:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T09:53:23.920-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Augmented Reality&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;HCI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Virtual Reality&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;VALID: A perceptually validated virtual avatar library for inclusion and diversity&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Mar Gonzalez-Franco, Research Scientist, Google AR &amp;amp; VR&lt;/span>;&lt;p>; As virtual reality (VR) and augmented reality (AR) technologies continue to grow in popularity, virtual avatars are becoming an increasingly important part of our digital interactions. In particular, virtual avatars are at the center of many social VR and AR interactions, as they are key to representing remote participants and facilitating collaboration. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In the last decade, interdisciplinary scientists have dedicated a significant amount of effort to better understand the use of avatars, and have made many interesting observations, including the capacity of the users to &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frvir.2020.575943/full&quot;>;embody their avatar&lt;/a>; (ie, the illusion that the avatar body is their own) and the &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9089510&quot;>;self-avatar follower effect&lt;/a>;, which creates a binding between the actions of the avatar and the user strong enough that the avatar can actually affect user behavior. &lt;/p>; &lt;p>; The use of avatars in experiments isn&#39;t just about how users will interact and behave in VR spaces, but also about discovering the limits of human perception and neuroscience. In fact, some VR social experiments often rely on recreating scenarios that can&#39;t be reproduced easily in the real world, such as bar crawls to &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0052766&quot; >;explore ingroup vs. outgroup effects&lt;/a>;, or deception experiments, such as the &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0209704&quot;>;Milgram obedience to authority inside virtual reality&lt;/一个>;。 Other studies try to explore deep neuroscientific phenomena, like the &lt;a href=&quot;https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2021.0453&quot;>;human mechanisms for motor control&lt;/a>;. This perhaps follows the trail of the &lt;a href=&quot;https://www.nature.com/articles/35784&quot;>;rubber hand illusion&lt;/a>; on brain plasticity, where a person can start feeling as if they own a rubber hand while their real hand is hidden behind a curtain. There is also an increased number of possible therapies for psychiatric treatment using personalized &lt;a href=&quot;https://doi.org/10.1186/s13063-022-06683-1&quot;>;avatars&lt;/a>;. In these cases, VR becomes an &lt;a href=&quot;https://en.wikipedia.org/wiki/Ecological_validity&quot;>;ecologically valid&lt;/a>; tool that allows scientists to explore or treat human behavior and perception. &lt;/p>; &lt;p>; None of these experiments and therapies could exist without good access to research tools and libraries that can enable easy experimentation. As such, multiple systems and open source tools have been released around avatar creation and animation over recent years. However, existing avatar libraries have not been validated systematically on the diversity spectrum. Societal &lt;a href=&quot;https://doi.org/10.1016/j.concog.2013.04.016&quot;>;bias and dynamics&lt;/a>; also transfer to VR/AR when interacting with avatars, which could lead to incomplete conclusions for studies on human behavior inside VR/AR. &lt;/p>; &lt;p>; To partially overcome this problem, we partnered with the University of Central Florida to create and release the open-source &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID). Described in &lt;a href=&quot;https://doi.org/10.3389/frvir.2023.1248915&quot;>;our recent paper&lt;/a>;, published in &lt;a href=&quot;https://www.frontiersin.org/journals/virtual -reality&quot;>;&lt;i>;Frontiers in Virtual Reality&lt;/i>;&lt;/a>;, this library of avatars is readily available for usage in VR/AR experiments and includes 210 avatars of seven different races and ethnicities recognized by the US Census Bureau 。 The avatars have been perceptually validated and designed to advance diversity and inclusion in virtual avatar research. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s1717/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1413&quot; data-original-width=&quot;1717&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Headshots of all 42 base avatars available on the VALID library were created in extensive interaction with members of the 7 ethnic and racial groups from the &lt;a href=&quot;https://www.federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs-race-and-ethnicity-statistical-standards&quot;>;Federal Register&lt;/a>;, which include (AIAN, Asian, Black, Hispanic, MENA, NHPI and White).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Creation and validation of the library&lt;/h2>; &lt;p>; Our initial selection of races and ethnicities for the diverse avatar library follows the most recent guidelines of the &lt;a href=&quot;https://www.npr.org/2023/01/26/1151608403/mena-race-categories-us-census-middle-eastern-latino-hispanic&quot;>;US Census Bureau&lt;/a>; that as of 2023 recommended the use of 7 ethnic and racial groups representing a large demographic of the US society, which can also be extrapolated to the global population. These groups include &lt;em>;Hispanic or Latino&lt;/em>;, &lt;em>;American Indian or Alaska Native (AIAN), Asian,&lt;/em>; &lt;em>;Black or African American,&lt;/em>; &lt;em>;Native Hawaiian or Other Pacific Islander (NHPI),&lt;/em>; &lt;em>;White, Middle East or North Africa&lt;/em>; (MENA). We envision the library will continue to evolve to bring even more diversity and representation with future additions of avatars. &lt;/p>; &lt;p>; The avatars were hand modeled and created using a process that combined average facial features with extensive collaboration with representative stakeholders from each racial group, where their feedback was used to artistically modify the facial mesh of the avatars. Then we conducted an online study with participants from 33 countries to determine whether the race and gender of each avatar in the library are recognizable. In addition to the avatars, we also provide labels statistically validated through observation of users for the race and gender of all 42 base avatars (see below). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;736&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtBeViSCY2bs4WRXaYkkiJAHNJ5LbfUocNzYR43jFoNuRHbeBWZQpHOf-smqwriUJrR-FTbFA_BJYAPLmGary8-omCVdMSOG8cTPYqBTj0rnFfLPanvDqyQGi2m8nL4bqkn6xg1x0U6o9-na1MiGK_RZTKhEloOjN4272h8JTt-v9WLquuMb1yMbwIYi-V/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of the headshots of a Black/African American avatar presented to participants during the validation of the library.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; We found that all Asian, Black, and White avatars were universally identified as their modeled race by all participants, while our American Indian or Native Alaskan (AIAN), Hispanic, and Middle Eastern or North African (MENA) avatars were typically only identified by participants of the same race. This also indicates that participant race can improve identification of a virtual avatar of the same race. The paper accompanying the library release highlights how this ingroup familiarity should also be taken into account when studying avatar behavior in VR. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s1999/image1.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1509&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsN3AL9HGdePu3n_Q8ttp48pP-JRrg9Hc1dXBSL0ouJ0l8qyC13fkWqEDtgl-jRhUovE1srSLEOy_mUyJLxfLljkA9JrVTEpKDrliNHzRadqBYBy1MvkKiJxCTJFsDJMxTXOaNj6oxLfFLuxq9EBgSpR8znJ3H2KHrm5G1_UKejqS_DX2SE1_wZqHhsrww/s16000/image1.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Confusion matrix heatmap of agreement rates for the 42 base avatars separated by other-race participants and same-race participants. One interesting aspect visible in this matrix, is that participants were significantly better at identifying the avatars of their own race than other races.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Dataset details&lt;/h2>; &lt;p>; Our models are available in &lt;a href=&quot;https://www.autodesk.com/products/fbx/overview&quot;>;FBX format&lt;/a>;, are compatible with previous avatar libraries like the commonly used &lt;a href=&quot;https://github.com/microsoft/Microsoft-Rocketbox&quot;>;Rocketbox&lt;/a>;, and can be easily integrated into most game engines such as &lt;a href=&quot;https://unity.com/&quot;>;Unity&lt;/a>; and &lt;a href=&quot;https://www.unrealengine.com/&quot;>;Unreal&lt;/a>;. Additionally, the avatars come with 69 bones and 65 facial blendshapes to enable researchers and developers to easily create and apply dynamic facial expressions and animations. The avatars were intentionally made to be partially cartoonish to avoid extreme look-a-like scenarios in which a person could be impersonated, but still representative enough to be able to run reliable user studies and social experiments. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s1999/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;986&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIlAuxGll4kdY4JweX4OnUk4IYp1FYyGARe-CKX-v1vW9H3W_xmKU_2Z4yuYDDtStl73HXdz_ncyWV3w11nGteTgNWC12kdwkvcDLaUSuq1lod1RUh67-lU0j8tekGZ3dDQ8KUGNGZj8Cl5_y1AzntFxj6Ah_akwRVJpbZVv4rIXxmunmD0CIa8y1Z0sYG/s16000/image4.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images of the skeleton rigging (bones that allow for animation) and some facial blend shapes included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The avatars can be further combined with variations of casual attires and five professional attires, including medical, military, worker and business. This is an intentional improvement from prior libraries that in some cases reproduced stereotypical gender and racial bias into the avatar attires, and provided very limited diversity to certain professional avatars. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s1537/image5.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1023&quot; data-original-width=&quot;1537&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPP4EicGt6Wa8y5Oxh5Ne8vmmCvgDtdKlep13d6sgaiHTsDgVqoRv28dH2Gg_RkEMOGK09fdC8Krp-BcZBy6t7PYyNRxatgREydoyV9-3_89_CNHWdt1eY2jwrbAxgIqQ9s7eyeJHSpMf72AYDccehHYian4WGWED6CA7XHKDxywc16Rgt_eF9FecGLEja/s16000/image5.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Images of some sample attire included with the VALID avatars.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Get started with VALID&lt;/h2>; &lt;p>; We believe that the &lt;a href=&quot;https://github.com/google/valid-avatar-library&quot;>;Virtual Avatar Library for Inclusion and Diversity&lt;/a>; (VALID) will be a valuable resource for researchers and developers working on VR/AR applications. We hope it will help to create more inclusive and equitable virtual experiences. To this end, we invite you to explore the avatar library, which we have released under the open source &lt;a href=&quot;https://en.wikipedia.org/wiki/MIT_License&quot;>;MIT license&lt;/a>;. You can download the avatars and use them in a variety of settings at no charge. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This library of avatars was born out of a collaboration with Tiffany D. Do, Steve Zelenty and Prof. Ryan P McMahan from the University of Central Florida.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5912569868305742102/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5912569868305742102&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/valid-perceptually-validated-virtual.html&quot; rel=&quot;alternate&quot; title=&quot;VALID: A perceptually validated virtual avatar library for inclusion and diversity&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmXa7LCZLvQy98sLB16VdQJa2RZqxfRzHvi_bfXYrUEdVaeqBqBo3yinO4yHFm8dmknahkVxAMt84i1Fv613KSBV0LLPZ0fXoj3ML2obxjHqpJkE8IXB-aFX95ahDzz6zgszPI2-9PH_6hGGAJfwA3lU2KGZcq8TPj4VKJH0UEl-UiH4FTzoUowwHjkOiA/s72-c/image2.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8414954450937764241&lt;/id>;&lt;published>;2023-12-05T17:32:00.000-08:00&lt;/published>;&lt;updated>;2024-01-03T14:16:52.565-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at EMNLP 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Malaya Jules, Program Manager, Google &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s320/EMNLP%202023.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Google is proud to be a &lt;a href=&quot;https://2023.emnlp.org/sponsors/&quot;>;Diamond Sponsor&lt;/a>; of &lt;a href=&quot;https://2023.emnlp.org/&quot;>;Empirical Methods in Natural Language Processing&lt;/a>; (EMNLP 2023), a premier annual conference, which is being held this week in Sentosa, Singapore. Google has a strong presence at this year&#39;s conference with over 65 accepted papers and active involvement in 11 workshops and tutorials. Google is also happy to be a &lt;a href=&quot;https://www.winlp.org/winlp-2023-workshop/winlp-2023-sponsors/&quot;>;Major Sponsor&lt;/a>; for the &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/winlp-2023-workshop/&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1701403043253017&amp;amp;usg=AOvVaw2oPPnFe0AEcdP1iSblNV91&quot;>;Widening NLP&lt;/a>; workshop (WiNLP), which aims to highlight global representations of people, perspectives, and cultures in AI and ML. We look forward to sharing some of our extensive NLP research and expanding our partnership with the broader research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We hope you&#39;ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in NLP, and check out some of the scheduled booth activities (eg, demos and Q&amp;amp;A sessions listed below). Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; X (Twitter) and &lt;a href=&quot;https://www.linkedin.com/showcase/googleresearch/?viewAsMember=true&quot;>;LinkedIn&lt;/a>; accounts to find out more about the Google booth activities at EMNLP 2023. &lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at EMNLP 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board &amp;amp; Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Sponsorship Chair: &lt;strong>;&lt;em>;Shyam Upadyay&lt;/em>;&lt;/strong>; &lt;br />; Industry Track Chair: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em>;&lt;/strong>; &lt;br />; Senior Program Committee: &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;em>;&lt;strong>;Annie Louis&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Vinodkumar Prabhakaran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Brian Roark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03291.pdf&quot;>;SynJax: Structured Probability Distributions for JAX&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Miloš Stanojević&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Laurent Sartran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.11077.pdf&quot;>;Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning&lt;/a>; &lt;br />; &lt;em>;Clifton Poth&lt;/em>;, &lt;em>;Hannah Sterz&lt;/em>;, &lt;em>;Indraneil Paul&lt;/em>;, &lt;em>;Sukannya Purkayastha&lt;/em>;, &lt;em>;Leon Engländer&lt;/em>;,&lt;em>; Timo Imhof&lt;/em>;, &lt;em>;Ivan Vulić&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Iryna Gurevych&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.08937.pdf&quot;>;DocumentNet: Bridging the Data Gap in Document Pre-training&lt;/a>; &lt;br />; &lt;em>;Lijun Yu&lt;/em>;, &lt;strong>;&lt;em>;Jin Miao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xiaoyu Sun&lt;/em>;&lt;/strong>;, &lt;em>;Jiayi Chen&lt;/em>;, &lt;em>;Alexander Hauptmann&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wei Wei&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.08592.pdf&quot;>;AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-Powered Applications&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Bhaktipriya Radharapu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kevin Robinson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.15239.pdf&quot;>;CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks&lt;/a>; &lt;br />; &lt;em>;Mete Ismayilzada&lt;/em>;, &lt;em>;Debjit Paul&lt;/em>;, &lt;em>;Syrielle Montariol&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;, &lt;em>;Antoine Bosselut&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11610.pdf&quot;>;Large Language Models Can Self-Improve&lt;/a>; &lt;br />; &lt;em>;Jiaxin Huang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Shixiang Shane Gu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuexin Wu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hongkun Yu&lt;/em>;&lt;/strong>;, &lt;em>;Jiawei Han&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14767.pdf&quot;>;Dissecting Recall of Factual Associations in Auto-Regressive Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jasmijn Bastings&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.10160.pdf&quot;>;Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;em>;Yoav Goldberg&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.16391.pdf&quot;>;Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yichao Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Bradley Wendt&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Navneet Potti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jing Xie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sandeep Tata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2112.12870.pdf&quot;>;Measuring Attribution in Natural Language Generation Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Hannah Rashkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthew Lamm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lora Aroyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Collins&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan&lt;/em>; &lt;em>;Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gaurav Singh Tomar&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Iulia Turc&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.02011.pdf&quot;>;Inverse Scaling Can Become U-Shaped&lt;/a>; &lt;br />; &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Najoung Kim&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Quoc Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14282.pdf&quot;>;INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback&lt;/a>; &lt;br />; &lt;em>;Wenda Xu&lt;/em>;, &lt;em>;Danqing Wang&lt;/em>;, &lt;em>;Liangming Pan&lt;/em>;, &lt;em>;Zhenqiao Song&lt;/em>;, &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;William Yang Wang&lt;/em>;, &lt;em>;Lei Li&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2206.14796.pdf&quot;>;On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-Based Method&lt;/a>; &lt;br />; &lt;em>;Zorik Gekhman&lt;/em>;, &lt;em>;Nadav Oved&lt;/em>;,&lt;strong>; &lt;em>;Orgad Keller&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roi Reichart&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2208.04347.pdf&quot;>;Investigating Efficiently Extending Transformers for Long-Input Summarization&lt;/a>; &lt;br />; &lt;em>;Jason Phang&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Peter J Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09744.pdf&quot;>;DSI++: Updating Transformer Memory with New Documents&lt;/a>; &lt;br />; &lt;em>;Sanket Vaibhav Mehta&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Marc Najork&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Emma Strubell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12029.pdf&quot;>;MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup&lt;/a>; &lt;br />; &lt;em>;Hua Shen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vicky Zayats&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Johann C Rocholl&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Daniel David Walker&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dirk Padfield&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14318.pdf&quot;>;q2d: Turning Questions into Dialogs to Teach Models How to Search&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shlomi Cohen-Ganor&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ido Hakimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yoad Lewenberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Enav Weinreb&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02171.pdf&quot;>;Emergence of Abstract State Representations in Embodied Sequence Modeling&lt;/a>; &lt;br />; &lt;em>;Tian Yun&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Zilai Zeng&lt;/em>;, &lt;em>;Kunal Handa&lt;/em>;, &lt;strong>;&lt;em>;Ashish V Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bo Pang&lt;/em>;&lt;/strong>;, &lt;em>;Ellie Pavlick&lt;/em>;, &lt;em>;Chen Sun&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14332.pdf&quot;>;Evaluating and Modeling Attribution for Cross-Lingual Question Answering&lt;/a>; &lt;br />; &lt;em>;Benjamin Muller&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2305.14281.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701762357021319&amp;amp;usg=AOvVaw2Q_4he8TIMmr8URRV1w4uX&quot;>;Weakly-Supervised Learning of Visual Relations in Multimodal Pre-training&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Emanuele Bugliarello&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Lisa Anne Hendricks&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13286.pdf&quot;>;How Do Languages Influence Each Other? Studying Cross-Lingual Data Sharing During LM Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Rochelle Choenni&lt;/em>;, &lt;strong>;&lt;em>;Dan Garrette&lt;/em>;&lt;/strong>;, &lt;em>;Ekaterina Shutova&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14214.pdf&quot;>;CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models&lt;/a>; &lt;br>; &lt;em>;Benjamin Minixhofer&lt;/em>;, &lt;strong>;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;, &lt;em>;Ivan Vulić&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.01328.pdf&quot;>;IC3: Image Captioning by Committee Consensus&lt;/a>; &lt;br />; &lt;em>;David Chan&lt;/em>;, &lt;strong>;&lt;em>;Austin Myers&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sudheendra Vijayanarasimhan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David A Ross&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; John Canny&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11877.pdf&quot;>;The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;em>;Omer Goldman&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Ido Dagan&lt;/em>;, &lt;em>;Shauli Ravfogel&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14542.pdf&quot;>;Evaluating Large Language Models on Controlled Generation Tasks&lt;/a>; &lt;br />; &lt;em>;Jiao Sun&lt;/em>;, &lt;em>;Yufei Tian&lt;/em>;, &lt;em>;Wangchunshu Zhou&lt;/em>;, &lt;em>;Nan Xu&lt;/em>;, &lt;em>;Qian Hu&lt;/em>;, &lt;em>;Rahul Gupta&lt;/em>;, &lt;strong>;&lt;em>;John Wieting&lt;/em>;&lt;/strong>;, &lt;em>;Nanyun Peng&lt;/em>;, &lt;em>;Xuezhe Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14324.pdf&quot;>;Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Daniel Deutsch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;George Foster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Markus Freitag&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.11399.pdf&quot;>;Transcending Scaling Laws with 0.1% Extra Compute&lt;/a>; &lt;br />; &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Jason Wei&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; Hyung Won Chung&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;, &lt;em>;David R. So&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Siamak Shakeri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xavier Garcia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Huaixiu Steven Zheng&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jinfeng Rao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha Chowdhery&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Donald&lt;/em>; &lt;em>;Metzler&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Slav Petrov,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Neil Houlsby&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Quoc V. Le&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.09006.pdf&quot;>;Data Similarity is Not Enough to Explain Language Model Performance&lt;/a>; &lt;br />; &lt;em>;Gregory Yauney&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Emily Reif&lt;/em>;&lt;/strong>;, &lt;em>;David Mimno&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00913.pdf&quot;>;Self-Influence Guided Data Reweighting for Language Model Pre-training&lt;/a>; &lt;br />; &lt;em>;Megh Thakkar&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Tolga Bolukbasi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sriram Ganapathy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shikhar Vashishth&lt;/em>;&lt;/strong>;, &lt;em>; Sarath Chandar&lt;/em>;, &lt;strong>;&lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11826.pdf&quot;>;ReTAG: Reasoning Aware Table to Analytic Text Generation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Deepanway Ghosal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Preksha Nema&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aravindan Raghuveer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15265v1.pdf&quot;>;GATITOS: Using a New Multilingual Lexicon for Low-Resource Machine Translation&lt;/a>; &lt;br />; &lt;em>;Alex Jones&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ishank Saxena&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.20201v1.pdf&quot;>;Video-Helpful Multimodal Machine Translation&lt;/a>; &lt;br />; &lt;em>;Yihang Li, Shuichiro Shimizu&lt;/em>;, &lt;em>;Chenhui Chu&lt;/em>;, &lt;em>;Sadao Kurohashi&lt;/em>;, &lt;strong>;&lt;em>;Wei Li&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.08298.pdf&quot;>;Symbol Tuning Improves In-Context Learning in Language Models&lt;/a>; &lt;br />; &lt;em>;Jerry Wei&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Le Hou&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Andrew Kyle Lampinen&lt;/strong>;&lt;/em>;, &lt;em>;Xiangning Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Da Huang&lt;/em>;&lt;/strong>;, &lt;em>;Yi Tay&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Xinyun&lt;/em>; &lt;em>;Chen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yifeng Lu&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Denny Zhou&lt;/em>;&lt;/strong>;, &lt;em>;Tengyu Ma&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Quoc V Le&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14755.pdf&quot;>;&quot;Don&#39;t Take This Out of Context!&quot; On the Need for Contextual Models and Evaluations for Stylistic Rewriting&lt;/a>; &lt;br />; &lt;em>;Akhila Yerukola&lt;/em>;, &lt;em>;Xuhui Zhou&lt;/em>;, &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;em>;Maarten Sap&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08264.pdf&quot;>;QAmeleon: Multilingual QA with Only 5 Examples&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Priyanka Agrawal&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chris Alberti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Fantine Huot&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Joshua Maynez&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ji Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kuzman Ganchev&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mirella Lapata&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.03540.pdf&quot;>;Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Eugene Kharitonov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Damien Vincent&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zalán Borsos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raphaël Marinier&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sertan Girgin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Olivier Pietquin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Matt Sharifi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Marco Tagliasacchi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Zeghidour&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09939.pdf&quot;>;AnyTOD: A Programmable Task-Oriented Dialog System&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeffrey Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuan Cao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghav Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Harrison Lee&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhinav Rastogi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hagen Soltau&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Izhak Shafran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yonghui Wu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14613.pdf&quot;>;Selectively Answering Ambiguous Questions&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael JQ Zhang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bhuwan Dhingra&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jacob Eisenstein&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08954.pdf&quot;>;PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/03/presto-multilingual-dataset-for-parsing.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;strong>;&lt;em>;Rahul Goel&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Waleed Ammar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aditya Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddharth Vashishtha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Motoki Sano&lt;/em>;&lt;/strong>;,&lt;em>; Faiz Surani&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Max Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;HyunJeong Choe&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;David Greene&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chuan He&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rattima Nitisaroj&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna&lt;/em>; &lt;em>;Trukhina&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shachi Paul&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pararth Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rushin Shah&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhou Yu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13281.pdf&quot;>;LM vs LM: Detecting Factual Errors via Cross Examination&lt;/a>; &lt;br />; &lt;em>;Roi Cohen&lt;/em>;, &lt;em>;May Hamri&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.03668.pdf&quot;>;A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding&lt;/a>; &lt;br />; &lt;em>;Andrea Burns&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Krishna Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Geoff Brown&lt;/em>;&lt;/strong>;, &lt;em>;Bryan A. Plummer&lt;/em>;, &lt;em>;Kate&lt;/em>; &lt;em>;Saenko&lt;/em>;, &lt;strong>;&lt;em>;Jianmo Ni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mandy Guo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2302.08956.pdf&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701763216883702&amp;amp;usg=AOvVaw1QisabqC-Gy-U4ou1jbHAY&quot;>;AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages&lt;/a>; &lt;br />; &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Idris Abdulmumin&lt;/em>;, &lt;em>;Abinew Ali Ayele&lt;/em>;, &lt;em>;Nedjma Ousidhoum&lt;/em>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Seid Muhie Yimam&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Meriem Beloucif&lt;/em>;, &lt;em>;Saif M.&lt;/em>; &lt;em>;Mohammad&lt;/em>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;Oumaima Hourrane&lt;/em>;, &lt;em>;Alipio Jorge&lt;/em>;, &lt;em>;Pavel Brazdil&lt;/em>;, &lt;em>;Felermino D. M&lt;/em>;. &lt;em>;A. Ali&lt;/em>;, &lt;em>;Davis David&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Bello Shehu-Bello&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Tajuddeen&lt;/em>; &lt;em>;Gwadabe&lt;/em>;, &lt;em>;Samuel Rutunda&lt;/em>;, &lt;em>;Tadesse Destaw Belay&lt;/em>;, &lt;em>;Wendimu Baye Messelle&lt;/em>;,&lt;em>; Hailu Beshada&lt;/em>; &lt;em>;Balcha&lt;/em>;, &lt;em>;Sisay Adugna Chala&lt;/em>;, &lt;em>;Hagos Tesfahun Gebremichael&lt;/em>;,&lt;em>; Bernard Opoku&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13682.pdf&quot;>;Optimizing Retrieval-Augmented Reader Models via Token Elimination&lt;/a>; &lt;br />; &lt;em>;Moshe Berchansky&lt;/em>;, &lt;em>;Peter Izsak&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;em>; Ido Dagan&lt;/em>;, &lt;em>;Moshe Wasserblat&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13194.pdf&quot;>;SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vitaly Nikolaev&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Thibault Sellam&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Aditya Siddhant&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Dipanjan Das&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13245.pdf&quot;>;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;, &lt;em>;Michiel de Jong&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Federico Lebron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.09752.pdf&quot;>;CoLT5: Faster Long-Range Transformers with Conditional Computation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Joshua Ainslie&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tao Lei&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michiel de Jong&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Santiago Ontanon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Siddhartha Brahma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yury Zemlyanskiy&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Uthus&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandy Guo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;James Lee-Thorp&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Tay&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yun-Hsuan Sung&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sumit Sanghai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16523.pdf&quot;>;Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nicholas Blumm&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xiao Ma&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Raghavendra Kotikalapudi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sahitya Potluri&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Qijun Tan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hansa Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ahmad Beirami&lt;/em>;&lt;/strong>;, &lt;em>;Alex Beutel&lt;/em>;, &lt;strong>;&lt;em>;Jilin Chen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14926.pdf&quot;>;Universal Self-Adaptive Prompting&lt;/a>; (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; &lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Ruoxi Sun, Hootan Nakhost&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Julian Martin Eisenschlos&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O. Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11171.pdf&quot;>;TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zorik Gekhman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Chen Elkind&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Idan Szpektor&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.07871.pdf&quot;>;Hierarchical Pre-training on Multimodal Electronic Health Records&lt;/a>; &lt;br />; &lt;em>;Xiaochen Wang&lt;/em>;, &lt;em>;Junyu Luo&lt;/em>;, &lt;em>;Jiaqi Wang&lt;/em>;, &lt;em>;Ziyi Yin&lt;/em>;, &lt;em>;Suhan Cui&lt;/em>;, &lt;em>;Yuan Zhong&lt;/em>;, &lt;strong>;&lt;em>;Yaqing Wang&lt;/em>;&lt;/strong>;, &lt;em>;Fenglong Ma&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14499.pdf&quot;>;NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Daniel Gillick&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jeremy R. Cole&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.11841.pdf&quot;>;How Does Generative Retrieval Scale to Millions of Passages?&lt;/a>; &lt;br />; &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Kai Hui&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;, &lt;em>;Jimmy Lin&lt;/em>;, &lt;strong>;&lt;em>;Donald Metzler&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.13959.pdf&quot;>;Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets&lt;/a>; &lt;br />; &lt;em>;Irina Bejan&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Artem Sokolov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Katja Filippova&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Findings of EMNLP&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.11689.pdf&quot;>;Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs&lt;/a>; &lt;br />; &lt;em>;Jiefeng Chen&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Jinsung Yoon&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sayna Ebrahimi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sercan O Arik&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Somesh Jha&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.10062.pdf&quot;>;A Comprehensive Evaluation of Tool-Assisted Generation Strategies&lt;/a>; &lt;br />; &lt;em>;Alon Jacovi&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Herzig&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Roee Aharoni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.16568.pdf&quot;>;1-PAGER: One Pass Answer Generation and Evidence Retrieval&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Tom Kwiatkowski&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05401.pdf&quot;>;MaXM: Towards Multilingual Visual Question Answering&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Linting Xue, Michal Yarom&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ashish V. Thapliyal&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Idan Szpektor&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Julien&lt;/em>; &lt;em>;Amelot&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xi Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.18431v1.pdf&quot;>;SDOH-NLI: A Dataset for Inferring Social Determinants of Health from Clinical Notes&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Adam D. Lelkes&lt;/em>;&lt;/strong>;, &lt;em>;Eric Loreaux&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Tal Schuster&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ming-Jun Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alvin Rajkomar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14815.pdf&quot;>;Machine Reading Comprehension Using Case-based Reasoning&lt;/a>; &lt;br />; &lt;em>;Dung Ngoc Thai&lt;/em>;, &lt;em>;Dhruv Agarwal&lt;/em>;, &lt;em>;Mudit Chaudhary&lt;/em>;, &lt;em>;Wenlong Zhao&lt;/em>;, &lt;em>;Rajarshi Das&lt;/em>;,&lt;em>; Jay-Yoon Lee&lt;/em>;, &lt;em>;Hannaneh Hajishirzi&lt;/em>;, &lt;strong>;&lt;em>;Manzil Zaheer&lt;/em>;&lt;/strong>;, &lt;em>;Andrew McCallum&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.06897.pdf&quot;>;Cross-lingual Open-Retrieval Question Answering for African Languages&lt;/a>; &lt;br />; &lt;em>;Odunayo Ogundepo&lt;/em>;, &lt;em>;Tajuddeen Gwadabe&lt;/em>;, &lt;strong>;&lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>;, &lt;em>;David Ifeoluwa Adelani&lt;/em>;, &lt;em>;Bonaventure FP Dossou&lt;/em>;, &lt;em>;Abdou Aziz DIOP&lt;/em>;, &lt;em>;Claytone Sikasote&lt;/em>;, &lt;em>;Gilles HACHEME&lt;/em>;, &lt;em>;Happy Buzaaba&lt;/em>;,&lt;em>; Ignatius Ezeani&lt;/em>;, &lt;em>;Rooweither Mabuya&lt;/em>;, &lt;em>;Salomey Osei&lt;/em>;, &lt;em>;Chris&lt;/em>; &lt;em>;Chinenye Emezue&lt;/em>;, &lt;em>;Albert Kahira&lt;/em>;, &lt;em>;Shamsuddeen Hassan Muhammad&lt;/em>;, &lt;em>;Akintunde Oladipo&lt;/em>;, &lt;em>;Abraham Toluwase Owodunni&lt;/em>;, &lt;em>;Atnafu Lambebo Tonja&lt;/em>;, &lt;em>;Iyanuoluwa Shode&lt;/em>;, &lt;em>;Akari Asai&lt;/em>;, &lt;em>;Anuoluwapo Aremu&lt;/em>;, &lt;em>;Ayodele Awokoya&lt;/em>;, &lt;em>;Bernard Opoku&lt;/em>;, &lt;em>;Chiamaka Ijeoma Chukwuneke&lt;/em>;, &lt;em>;Christine Mwase&lt;/em>;, &lt;em>;Clemencia Siro&lt;/em>;, &lt;em>;Stephen Arthur&lt;/em>;, &lt;em>;Tunde Oluwaseyi Ajayi&lt;/em>;, &lt;em>;Verrah Akinyi Otiende&lt;/em>;, &lt;em>;Andre Niyongabo Rubungo&lt;/em>;, &lt;em>;Boyd Sinkala&lt;/em>;, &lt;em>;Daniel Ajisafe&lt;/em>;, &lt;em>;Emeka Felix Onwuegbuzia&lt;/em>;, &lt;em>;Falalu Ibrahim Lawan&lt;/em>;, &lt;em>;Ibrahim Said Ahmad&lt;/em>;, &lt;em>;Jesujoba Oluwadara Alabi&lt;/em>;, &lt;em>;CHINEDU EMMANUEL&lt;/em>; &lt;em>;MBONU&lt;/em>;, &lt;em>;Mofetoluwa Adeyemi&lt;/em>;, &lt;em>;Mofya Phiri&lt;/em>;, &lt;em>;Orevaoghene Ahia&lt;/em>;, &lt;em>;Ruqayya Nasir Iro&lt;/em>;, &lt;em>;Sonia Adhiambo&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.08653.pdf&quot;>;On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Polina Zablotskaia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Du Phan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Joshua Maynez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Shashi Narayan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.09860.pdf&quot;>;Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Markus Freitag&lt;/em>;&lt;/strong>;, &lt;em>;Behrooz Ghorbani&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;em>;Patrick Fernandes&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14552.pdf&quot;>;Sources of Hallucination by Large Language Models on Inference Tasks&lt;/a>; &lt;br />; &lt;em>;Nick McKenna&lt;/em>;, &lt;em>;Tianyi Li&lt;/em>;, &lt;em>;Liang Cheng&lt;/em>;, &lt;strong>;&lt;em>;Mohammad Javad Hosseini&lt;/em>;&lt;/strong>;, &lt;em>;Mark Johnson&lt;/em>;, &lt;em>;Mark Steedman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.09017v2.pdf&quot;>;Don&#39;t Add, Don&#39;t Miss: Effective Content Preserving Generation from Pre-selected Text Spans&lt;/a>; &lt;br />; &lt;em>;Aviv Slobodkin&lt;/em>;, &lt;strong>;&lt;em>;Avi Caciularu&lt;/em>;&lt;/strong>;, &lt;em>;Eran Hirsch&lt;/em>;, &lt;em>;Ido Dagan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07686.pdf&quot;>;What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study&lt;/a>; &lt;br />; &lt;em>;Aman Madaan&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;em>; &lt;strong>;Katherine Hermann&lt;/strong>;&lt;/em>;, &lt;strong>;&lt; em>;Amir Yazdanbakhsh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2210.03945.pdf&quot;>;Understanding HTML with Large Language Models&lt;/a>; &lt; br />; &lt;strong>;&lt;em>;Izzeddin Gur&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Ofir Nachum&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yingjie Miao&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Mustafa Safdari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Austin Huang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aakanksha&lt;/em>; &lt; em>;Chowdhery&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Noah Fiedel&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Aleksandra Faust&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.09928.pdf&quot;>;Improving the Robustness of Summarization Models by Detecting and Removing Input Noise&lt;/a>; &lt;br />; &lt;em>;Kundan Krishna&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Yao Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Jie Ren&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Balaji Lakshminarayanan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jiaming Luo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mohammad&lt;/em>; &lt;em>;Saleh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter J. Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https ://arxiv.org/pdf/2310.15916.pdf&quot;>;In-Context Learning Creates Task Vectors&lt;/a>; &lt;br />; &lt;em>;Roee Hendel&lt;/em>;, &lt;strong>;&lt;em>;Mor Geva&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Amir Globerson&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.10544.pdf&quot;>;Pre -training Without Attention&lt;/a>; &lt;br />; &lt;em>;Junxiong Wang&lt;/em>;, &lt;em>;Jing Nathan Yan&lt;/em>;, &lt;strong>;&lt;em>;Albert Gu&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Alexander M Rush&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12441.pdf&quot;>;MUX-PLMs: Data Multiplexing for High-Throughput Language Models&lt;/a>; &lt;br />; &lt;em>;Vishvak Murahari&lt;/em>;, &lt;em>;Ameet Deshpande&lt;/em>;, &lt;em>;Carlos E Jimenez&lt;/em>;,&lt;em>; &lt;strong >;Izhak Shafran&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Mingqiu Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yuan&lt;/em>; &lt;em>;Cao&lt;/em>;&lt;/ strong>;, &lt;em>;Karthik R Narasimhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.14408.pdf&quot;>;PaRaDe: Passage Ranking Using Demonstrations with LLMs&lt;/ a>; &lt;br />; &lt;em>;Andrew Drozdov&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Honglei Zhuang&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; Zhuyun Dai&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhen Qin&lt;/em>;&lt;/strong>;, &lt;em>;Razieh Rahimi&lt;/em>;, &lt;strong>;&lt;em>;Xuanhui Wang&lt;/em>;&lt;/strong >;,&lt;strong>; &lt;em>;Dana Alon&lt;/em>;&lt;/strong>;, &lt;em>;Mohit Iyyer&lt;/em>;, &lt;em>;Andrew McCallum&lt;/em>;, &lt;em>;Donald Metzler&lt;sup>;*&lt;/ sup>;&lt;/em>;,&lt;strong>; &lt;em>;Kai Hui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.13678.pdf&quot;>; Long-Form Speech Translation Through Segmentation with Finite-State Decoding Constraints on Large Language Models&lt;/a>; &lt;br />; &lt;em>;Arya D. McCarthy&lt;/em>;, &lt;strong>;&lt;em>;Hao Zhang&lt;/em>;&lt; /strong>;,&lt;strong>; &lt;em>;Shankar Kumar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Felix Stahlberg&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ke Wu&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.07496.pdf&quot;>;Unsupervised Opinion Summarization Using Approximate Geodesics&lt;/a>; &lt;br />; &lt;em>;Somnath Basu Roy Chowdhury&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Nicholas Monath&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kumar Avinava Dubey&lt;/em>;&lt;/strong>;, &lt;strong>; &lt;em>;Amr Ahmed&lt;/em>;&lt;/strong>;, &lt;em>;Snigdha Chaturvedi&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.02883. pdf&quot;>;SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sercan O . Arik&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Rajarishi Sinha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hootan Nakhost&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em >;Hanjun Dai&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pengcheng Yin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tomas Pfister&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://zi-lin.com/pdf/EMNLP_2023_retrieval.pdf&quot;>;Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty&lt;/a>; &lt;br />; &lt;em>;Zi Lin&lt; /em>;, &lt;strong>;&lt;em>;Quan Yuan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Panupong Pasupat&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jeremiah Zhe Liu&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Jingbo Shang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2310.08740.pdf&quot;>;A Zero-Shot Language Agent for Computer Control with Structured Reflection&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Tao Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gang Li&lt;/em>;&lt;/ strong>;,&lt;strong>; &lt;em>;Zhiwei Deng&lt;/em>;&lt;/strong>;, &lt;em>;Bryan Wang&lt;sup>;*&lt;/sup>;&lt;/em>;,&lt;strong>; &lt;em>;Yang Li&lt;/em>; &lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.08371.pdf&quot;>;Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches&lt;/a>; &lt;br / >; &lt;em>;Daniel Fried&lt;/em>;, &lt;em>;Nicholas Tomlin&lt;/em>;, &lt;em>;Jennifer Hu&lt;/em>;,&lt;strong>; &lt;em>;Roma Patel&lt;/em>;&lt;/strong>;, &lt;strong >;&lt;em>;Aida Nematzadeh&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.13535.pdf&quot;>;Improving Classifier Robustness Through Active Generation of Pairwise Counterfactuals &lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Ananth Balashankar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuezhi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yao Qin &lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Packer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nithum Thain&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jilin Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ed H.&lt;/em>; &lt;em>;Chi&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alex Beutel&lt;/em>;&lt;/ strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.14224.pdf&quot;>;mmT5: Modular Multilingual Pre-training Solves Source Language Hallucinations&lt;/a>; &lt;br />; &lt;strong >;&lt;em>;Jonas Pfeiffer&lt;/em>;&lt;/strong>;,&lt;em>; &lt;strong>;Francesco Piccinno&lt;/strong>;&lt;/em>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt; strong>;&lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Machel Reid&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian&lt;/em>; &lt;em>;Ruder&lt;/ em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2207.10551.pdf&quot;>;Scaling Laws vs Model Architectures: How Does Inductive Bias Influence Scaling?&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yi Tay&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mostafa Dehghani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Samira Abnar&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Hyung Won Chung&lt;/em>;&lt;/strong>;,&lt;strong>;&lt;em>; William Fedus&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jinfeng Rao&lt;/ em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sharan Narang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Q. Tran&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dani Yogatama&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Donald Metzler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2211.00142. pdf&quot;>;TaTA: A Multilingual Table-to-Text Dataset for African Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Gehrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Ruder&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vitaly Nikolaev,&lt;/em>; &lt;em>;Jan A. Botha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael Chavinda&lt;/em>;&lt; /strong>;, &lt;strong>;&lt;em>;Ankur P Parikh&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Clara E. Rivera&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href =&quot;https://arxiv.org/pdf/2305.11938.pdf&quot;>;XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Sebastian Ruder ,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Jonathan H. Clark&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Alexander Gutkin&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em >;Mihir Kale&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Min Ma&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Massimo Nicosia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt; em>;Shruti Rijhwani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Parker Riley&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jean Michel Amath Sarr&lt;/em>;&lt;/strong>;,&lt; strong>; &lt;em>;Xinyi Wang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;John Frederick&lt;/em>; &lt;em>;Wieting&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nitish Gupta&lt; /em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anna Katanova&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Christo Kirov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dana L Dickinson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Brian Roark&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bidisha Samanta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>; Connie Tao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Ifeoluwa Adelani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vera Axelrod&lt;/em>;&lt;/strong>;,&lt;strong>; &lt; em>;Isaac Rayburn&lt;/em>; &lt;em>;Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Colin Cherry&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Garrette&lt;/em>; &lt;/strong>;,&lt;strong>; &lt;em>;Reeve Ingle&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Melvin Johnson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dmitry Panteleev&lt;/em >;&lt;/strong>;,&lt;strong>; &lt;em>;Partha Talukdar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2311.00693.pdf&quot;>;On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval&lt;/a>; &lt;br />; &lt;em>;Jiayi Chen&lt;sup>;*&lt;/sup>;&lt;/em>;, &lt;strong>;&lt;em>;Hanjun Dai&lt; /em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bo Dai&lt;/em>;&lt;/strong>;, &lt;em>;Aidong Zhang&lt;/em>;, &lt;em>;Wei Wei&lt;sup>;*&lt;/sup>;&lt;/ em>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px ;&quot;>; &lt;p>; &lt;a href=&quot;https://www.google.com/url?q=https://www.winlp.org/&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1701752203060961&amp;amp;usg =AOvVaw3sRozMNVYuwvyXeLluOgKI&quot;>;The Seventh Widening NLP Workshop&lt;/a>; (WiNLP) &lt;br />; Major Sponsor &lt;br />; Organizers: &lt;strong>;&lt;em>;Sunipa Dev&lt;/em>;&lt;/strong>; &lt;br />; Panelist: &lt;strong>;&lt;em>;Preethi Lahoti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/crac2023/?pli=1&quot;>; The Sixth Workshop on Computational Models of Reference, Anaphora and Coreference&lt;/a>; (CRAC) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Bernd Bohnet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt; a href=&quot;https://nlposs.github.io/2023/index.html&quot;>;The 3rd Workshop for Natural Language Processing Open Source Software&lt;/a>; (NLP-OSS) &lt;br />; Organizer: &lt;strong>;&lt; em>;Geeticka Chauhan&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://splu-robonlp-2023.github.io/&quot;>;Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics&lt;/a>; (SpLU-RoboNLP) &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Andy Zeng&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gem -benchmark.com/workshop&quot;>;Natural Language Generation, Evaluation, and Metric&lt;/a>; (GEM) &lt;br />; Organizer: &lt;strong>;&lt;em>;Elizabeth Clark&lt;/em>;&lt;/strong>; &lt;/p>; &lt; p>; &lt;a href=&quot;https://arabicnlp2023.sigarab.org/&quot;>;The First Arabic Natural Language Processing Conference&lt;/a>; (ArabicNLP) &lt;br />; Organizer: &lt;strong>;&lt;em>;Imed Zitouni&lt;/em >;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.bigpictureworkshop.com/&quot;>;The Big Picture: Crafting a Research Narrative&lt;/a>; (BigPicture) &lt;br />; Organizer: &lt;strong>;&lt;em>;Nora Kassner&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://blackboxnlp .github.io/&quot;>;BlackboxNLP 2023: The 6th Workshop on Analysing and Interpreting Neural Networks for NLP&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Najoung Kim&lt;/em>;&lt;/strong>; &lt;br / >; Panelist: &lt;strong>;&lt;em>;Neel Nanda&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.conll.org/2023&quot;>;The SIGNLL Conference on Computational Natural Language Learning&lt;/a>; (CoNLL) &lt;br />; Co-Chair: &lt;strong>;&lt;em>;David Reitter&lt;/em>;&lt;/strong>; &lt;br />; Areas and ACs: &lt;strong>;&lt;em>;Kyle Gorman&lt; /em>;&lt;/strong>; (Speech and Phonology), &lt;strong>;&lt;em>;Fei Liu &lt;/em>;&lt;/strong>;(Natural Language Generation) &lt;/p>; &lt;p>; &lt;a href=&quot;https:// sigtyp.github.io/ws2023-mrl.html&quot;>;The Third Workshop on Multi-lingual Representation Learning&lt;/a>; (MRL) &lt;br />; Organizer: &lt;strong>;&lt;em>;Omer Goldman&lt;/em>;&lt;/strong >;, &lt;strong>;&lt;em>;Sebastian Ruder&lt;/em>;&lt;/strong>; &lt;br />; Invited Speaker: &lt;strong>;&lt;em>;Orhan Firat&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt; div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorials&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot; https://emnlp2023-creative-nlg.github.io/&quot;>;Creative Natural Language Generation&lt;/a>; &lt;br />; Organizer: &lt;em>;Tuhin Chakrabarty&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;/p >; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;p>; &lt;i>;This schedule is subject to change 。 Please visit the Google booth for more information.&lt;/i>;&lt;/p>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Developing and Utilizing Evaluation Metrics for Machine Translation &amp;amp; Improving Multilingual NLP &lt;br />; Presenter: &lt;strong>;&lt;em>;Isaac Caswell&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dan Deutch&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jan-Thorsten Peter&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;David Vilar Torres&lt;/em>;&lt;/strong>; &lt;br />; Fri, Dec 8 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Differentiable Search Indexes &amp;amp; Generative Retrieval &lt;br />; Presenter: &lt;strong>;&lt;em>;Sanket Vaibhav Mehta&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vinh Tran&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>; Kai Hui&lt;/em>;&lt;/strong>;, &lt;em>;Ronak Pradeep&lt;sup>;*&lt;/sup>;&lt;/em>; &lt;br />; Fri, Dec 8 | 3:30PM -4:00PM SST &lt;/p>; &lt;p>; Retrieval and Generation in a single pass &lt;br />; Presenter: &lt;strong>;&lt;em>;Palak Jain&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Livio Baldini Soares&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 10:30AM -11:00AM SST &lt;/p>; &lt;p>; Amplifying Adversarial Attacks &lt;br />; Presenter:&lt;strong>; &lt;em>;Anu Sinha&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 12:30PM -1:45PM SST &lt;/p>; &lt;p>; Automate prompt design: Universal Self-Adaptive Prompting (see &lt;a href=&quot;https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html&quot;>;blog post&lt;/a>;) &lt;br />; Presenter: &lt;strong>;&lt;em>;Xingchen Wan&lt;sup>;*&lt;/sup>;&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ruoxi Sun&lt;/em>;&lt;/strong>; &lt;br />; Sat, Dec 9 | 3:30PM -4:00PM SST &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8414954450937764241/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8414954450937764241&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/google-at-emnlp-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at EMNLP 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi63HbbcxUJqps2nNQBmiEoOpsCkh24PH9YXd_Z7VSQ5f00T_shhNlDZ03_dPNw4ge8XALlXyvIfFMmNvWzWMzHWUs80ZVGz_O9dm-bz9p0dnl4bfXPuk34a-lXfU2IKReWUkshFPQFVpL4L6IOreL2Z7RnEUTm-iEKM2XAjj9PdyVXjwGNLi7CK4JhMxnV/s72-c/EMNLP%202023.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3665865722098768988&lt;/id>;&lt;published>;2023-12-04T14:41:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T14:46:06.688-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A new quantum algorithm for classical mechanics with an exponential speedup&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Robin Kothari and Rolando Somma, Research Scientists, Google Research, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s1100/glued-trees.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Quantum computers promise to solve some problems exponentially faster than classical computers, but there are only a handful of examples with such a dramatic speedup, such as &lt;a href=&quot;https://en.wikipedia.org/wiki /Shor%27s_algorithm&quot;>;Shor&#39;s factoring algorithm&lt;/a>; and &lt;a href=&quot;https://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot;>;quantum simulation&lt;/一个>;。 Of those few examples, the majority of them involve simulating physical systems that are inherently quantum mechanical — a natural application for quantum computers. But what about simulating systems that are not inherently quantum? Can quantum computers offer an exponential advantage for this? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://link.aps.org/doi/10.1103/PhysRevX.13.041041&quot;>;Exponential quantum speedup in simulating coupled classical oscillators&lt;/a>;”, published in &lt;a href=&quot;https://journals.aps.org/prx/&quot;>;Physical Review X&lt;/a>; (PRX) and presented at the &lt;a href=&quot;https://focs.computer.org/2023/&quot;>;Symposium on Foundations of Computer Science&lt;/a>; (FOCS 2023), we report on the discovery of a new quantum algorithm that offers an exponential advantage for simulating coupled &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_oscillator&quot;>;classical harmonic oscillators&lt;/a>;. These are some of the most fundamental, ubiquitous systems in nature and can describe the physics of countless natural systems, from electrical circuits to molecular vibrations to the mechanics of bridges. In collaboration with Dominic Berry of Macquarie University and Nathan Wiebe of the University of Toronto, we found a mapping that can transform any system involving coupled oscillators into a problem describing the time evolution of a quantum system. Given certain constraints, this problem can be solved with a quantum computer exponentially faster than it can with a classical computer. Further, we use this mapping to prove that any problem efficiently solvable by a quantum algorithm can be recast as a problem involving a network of coupled oscillators, albeit exponentially many of them. In addition to unlocking previously unknown applications of quantum computers, this result provides a new method of designing new quantum algorithms by reasoning purely about classical systems. &lt;/p>; &lt;br />; &lt;h2>;Simulating coupled oscillators&lt;/h2>; &lt;p>; The systems we consider consist of classical harmonic oscillators. An example of a single harmonic oscillator is a mass (such as a ball) attached to a spring. If you displace the mass from its rest position, then the spring will induce a restoring force, pushing or pulling the mass in the opposite direction. This restoring force causes the mass to oscillate back and forth. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;116&quot; data-original-width=&quot;359&quot; height=&quot;129&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A simple example of a harmonic oscillator is a mass connected to a wall by a spring. [Image Source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif&quot;>;Wikimedia&lt;/a>;]&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Now consider &lt;em>;coupled &lt;/em>;harmonic oscillators, where &lt;em>;multiple&lt;/em>; masses are attached to one another through springs. Displace one mass, and it will induce a wave of oscillations to pulse through the system. As one might expect, simulating the oscillations of a large number of masses on a classical computer gets increasingly difficult. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/s1129/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;832&quot; data-original-width=&quot;1129&quot; height=&quot;295&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example system of masses connected by springs that can be simulated with the quantum algorithm.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To enable the simulation of a large number of coupled harmonic oscillators, we came up with a mapping that encodes the positions and velocities of all masses and springs into the quantum wavefunction of a system of qubits. Since the number of parameters describing the wavefunction of a system of qubits grows exponentially with the number of qubits, we can encode the information of &lt;em>;N&lt;/em>; balls into a quantum mechanical system of only about log(&lt;em>;N&lt;/em>;) qubits. As long as there is a compact description of the system (ie, the properties of the masses and the springs), we can evolve the wavefunction to learn coordinates of the balls and springs at a later time with far fewer resources than if we had used a naïve classical approach to simulate the balls and springs. &lt;/p>; &lt;p>; We showed that a certain class of coupled-classical oscillator systems can be efficiently simulated on a quantum computer. But this alone does not rule out the possibility that there exists some as-yet-unknown clever classical algorithm that is similarly efficient in its use of resources. To show that our quantum algorithm achieves an exponential speedup over &lt;em>;any&lt;/em>; possible classical algorithm, we provide two additional pieces of evidence. &lt;/p>; &lt;br />; &lt;h2>;The glued-trees problem and the quantum oracle&lt;/h2>; &lt;p>; For the first piece of evidence, we use our mapping to show that the quantum algorithm can efficiently solve a famous problem about graphs known to be difficult to solve classically, called the &lt;a href=&quot;https://arxiv.org/abs/quant-ph/0209131&quot;>;glued-trees problem&lt;/a>;. The problem takes two branching trees — a graph whose nodes each branch to two more nodes, resembling the branching paths of a tree — and glues their branches together through a random set of edges, as shown in the figure below. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;556&quot; data-original-width=&quot;930&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual representation of the glued trees problem. Here we start at the node labeled ENTRANCE and are allowed to locally explore the graph, which is obtained by randomly gluing together two binary trees. The goal is to find the node labeled EXIT.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The goal of the glued-trees problem is to find the exit node — the “root” of the second tree — as efficiently as possible. But the exact configuration of the nodes and edges of the glued trees are initially hidden from us. To learn about the system, we must query an &lt;a href=&quot;https://en.wikipedia.org/wiki/Oracle_machine&quot;>;oracle&lt;/a>;, which can answer specific questions about the setup. This oracle allows us to explore the trees, but only locally. Decades ago, &lt;a href=&quot;https://doi.org/10.1145/780542.780552&quot;>;it was shown&lt;/a>; that the number of queries required to find the exit node on a classical computer is proportional to a polynomial factor of &lt;em>;N&lt;/em>;, the total number of nodes. &lt;/p>; &lt;p>; But recasting this as a problem with balls and springs, we can imagine each node as a ball and each connection between two nodes as a spring. Pluck the entrance node (the root of the first tree), and the oscillations will pulse through the trees. It only takes a time that scales with the &lt;em>;depth&lt;/em>; of the tree — which is exponentially smaller than &lt;em>;N&lt;/em>; — to reach the exit node. So, by mapping the glued-trees ball-and-spring system to a quantum system and evolving it for that time, we can detect the vibrations of the exit node and determine it exponentially faster than we could using a classical computer. &lt;/p>; &lt;br />; &lt;h2>;BQP-completeness&lt;/h2>; &lt;p>; The second and strongest piece of evidence that our algorithm is exponentially more efficient than any possible classical algorithm is revealed by examination of the set of problems a quantum computer can solve efficiently (ie, solvable in &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time&quot;>;polynomial time&lt;/a>;), referred to as &lt;a href=&quot;https://en.wikipedia.org/wiki/BQP&quot;>;bounded-error quantum polynomial time&lt;/a>; or BQP. The hardest problems in BQP are called “BQP-complete”. &lt;/p>; &lt;p>; While it is generally accepted that there exist some problems that a quantum algorithm can solve efficiently and a classical algorithm cannot, this has not yet been proven. So, the best evidence we can provide is that our problem is BQP-complete, that is, it is among the hardest problems in BQP. If someone were to find an efficient classical algorithm for solving our problem, then every problem solved by a quantum computer efficiently would be classically solvable! Not even the &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_factorization&quot;>;factoring problem&lt;/a>; (finding the prime factors of a given large number), which forms the basis of &lt;a href=&quot;https://en.wikipedia.org/wiki/RSA_(cryptosystem)&quot;>;modern encryption&lt;/a>; and was famously solved by Shor&#39;s algorithm, is expected to be BQP-complete. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;563&quot; data-original-width=&quot;574&quot; height=&quot;314&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png&quot; width=&quot;320&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A diagram showing the believed relationships of the classes BPP and BQP, which are the set of problems that can be efficiently solved on a classical computer and quantum computer, respectively. BQP-complete problems are the hardest problems in BQP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; To show that our problem of simulating balls and springs is indeed BQP-complete, we start with a standard BQP-complete problem of simulating universal quantum circuits, and show that every quantum circuit can be expressed as a system of many balls coupled with springs. Therefore, our problem is also BQP-complete. &lt;/p>; &lt;br />; &lt;h2>;Implications and future work&lt;/h2>; &lt;p>; This effort also sheds light on work from 2002, when theoretical computer scientist Lov K. Grover and his colleague, Anirvan M. Sengupta, used an &lt;a href=&quot;https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319&quot;>;analogy to coupled&lt;/a>; pendulums to illustrate how Grover&#39;s famous quantum &lt;a href=&quot;https://en.wikipedia.org/wiki/Grover%27s_algorithm&quot;>;search algorithm&lt;/a>; could find the correct element in an unsorted database quadratically faster than could be done classically. With the proper setup and initial conditions, it would be possible to tell whether one of &lt;em>;N&lt;/em>; pendulums was different from the others — the analogue of finding the correct element in a database — after the system had evolved for time that was only ~√(&lt;em>;N)&lt;/em>;. While this hints at a connection between certain classical oscillating systems and quantum algorithms, it falls short of explaining why Grover&#39;s quantum algorithm achieves a quantum advantage. &lt;/p>; &lt;p>; Our results make that connection precise. We showed that the dynamics of any classical system of harmonic oscillators can indeed be equivalently understood as the dynamics of a corresponding quantum system of exponentially smaller size. In this way we can simulate Grover and Sengupta&#39;s system of pendulums on a quantum computer of log(&lt;em>;N&lt;/em>;) qubits, and find a different quantum algorithm that can find the correct element in time ~√(&lt;em>;N&lt;/em>;). The analogy we discovered between classical and quantum systems can be used to construct other quantum algorithms offering exponential speedups, where the reason for the speedups is now more evident from the way that classical waves propagate. &lt;/p>; &lt;p>; Our work also reveals that every quantum algorithm can be equivalently understood as the propagation of a classical wave in a system of coupled oscillators. This would imply that, for example, we can in principle build a classical system that solves the factoring problem after it has evolved for time that is exponentially smaller than the runtime of any known classical algorithm that solves factoring. This may look like an efficient classical algorithm for factoring, but the catch is that the number of oscillators is exponentially large, making it an impractical way to solve factoring. &lt;/p>; &lt;p>; Coupled harmonic oscillators are ubiquitous in nature, describing a broad range of systems from electrical circuits to chains of molecules to structures such as bridges. While our work here focuses on the fundamental complexity of this broad class of problems, we expect that it will guide us in searching for real-world examples of harmonic oscillator problems in which a quantum computer could offer an exponential advantage. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank our Quantum Computing Science Communicator, Katie McCormick, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3665865722098768988/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3665865722098768988&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html&quot; rel=&quot;alternate&quot; title=&quot;A new quantum algorithm for classical mechanics with an exponential speedup&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFB1yk-wkwGzjxoAmN6xdTY1qut_K7Aad1WNMlW-z7O02NTE4nQL1kJheHNEbJxACsUlmu4HEmk8uXnPkeO9Jf_T3WE5qPgJZgJcbmXbf06nTiL3MRO-ull3C6SWsxVVzIsyK-ZojzHoP1e_elh4im6LHDblGgiviZrUPlOIy92QMVIF87_j5y83WMLn49/s72-c/glued-trees.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-68038970217457115&lt;/id>;&lt;published>;2023-12-04T10:00:00.000-08:00&lt;/published>;&lt;updated>;2023-12-04T11:29:40.224-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ads&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Summary report optimization in the Privacy Sandbox Attribution Reporting API&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Hidayet Aksu, Software Engineer, and Adam Sealfon, Research Scientist, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, the &lt;a href=&quot;https://privacysandbox.com/&quot;>;Privacy Sandbox&lt;/a>; initiative was launched to explore responsible ways for advertisers to measure the effectiveness of their campaigns, by aiming to &lt;a href=&quot;https://blog.chromium.org/2020/01/building-more-private-web-path-towards.html&quot;>;deprecate third-party cookies&lt;/a>; (subject to &lt;a href=&quot;https://www.gov.uk/cma-cases/investigation-into-googles-privacy-sandbox-browser-changes&quot;>;resolving any competition concerns with the UK&#39;s Competition and Markets Authority&lt;/a>;). &lt;a href=&quot;https://en.wikipedia.org/wiki/HTTP_cookie#Third-party_cookie&quot;>;Cookies&lt;/a>; are small pieces of data containing user preferences that websites store on a user&#39;s device; they can be used to provide a better browsing experience (eg, allowing users to automatically sign in) and to serve relevant content or ads. The Privacy Sandbox attempts to address concerns around the use of cookies for tracking browsing data across the web by providing a privacy-preserving alternative. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Many browsers use &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;differential privacy&lt;/a>; (DP) to provide privacy-preserving APIs, such as the &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting&quot;>;Attribution Reporting API&lt;/a>; (ARA), that don&#39;t rely on cookies for ad conversion measurement. ARA encrypts individual user actions and collects them in an aggregated &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/&quot;>;summary report&lt;/a>;, which estimates measurement goals like the number and value of conversions (useful actions on a website, such as making a purchase or signing up for a mailing list) attributed to ad campaigns. &lt;/p>; &lt;p>; The task of configuring API parameters, eg, allocating a contribution budget across different conversions, is important for maximizing the utility of the summary reports. In “&lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;Summary Report Optimization in the Privacy Sandbox Attribution Reporting API&lt;/a>;”, we introduce a formal mathematical framework for modeling summary reports. Then, we formulate the problem of maximizing the utility of summary reports as an optimization problem to obtain the optimal ARA parameters. Finally, we evaluate the method using real and synthetic datasets, and demonstrate significantly improved utility compared to baseline non-optimized summary reports. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ARA summary reports&lt;/h2>; &lt;p>; We use the following example to illustrate our notation. Imagine a fictional gift shop called &lt;em>;Du &amp;amp; Penc&lt;/em>; that uses digital advertising to reach its customers. The table below captures their holiday sales, where each record contains impression features with (i) an impression ID, (ii) the campaign, and (iii) the city in which the ad was shown, as well as conversion features with (i) the number of items purchased and (ii) the total dollar value of those items. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/s1035/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;397&quot; data-original-width=&quot;1035&quot; height=&quot;245&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqrhtaPxDlj8o5sbubBQLdCq_RRnT2ZPmhxuvEIC_9SvLfhKkXh1t_elS4eRCQyHv2pYX4YkAX1UkEX3c3BdbB5PejqLF0uq_MAXuXKVA1YPKVnZ5tqertr02eNDFjJ0nnoeD_rGdDFWxLrTsCNGXOE9LrGSsuPhrtZ6SgqRomWRjpun1JdrwWDfnfJF0n/w640-h245/image4.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Impression and conversion feature logs for Du &amp;amp; Penc.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Mathematical model&lt;/h3>; &lt;p>; ARA summary reports can be modeled by four algorithms: (1) Contribution Vector, (2) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATE.md#contribution-bounding-and-budgeting&quot;>;Contribution Bounding&lt;/a>;, (3) &lt;a href=&quot;https://github.com/WICG/attribution-reporting-api/blob/main/AGGREGATION_SERVICE_TEE.md?&quot;>;Summary Reports&lt;/a>;, and (4) Reconstruct Values. Contribution Bounding and Summary Reports are performed by the ARA, while Contribution Vector and Reconstruct Values are performed by an AdTech provider — tools and systems that enable businesses to buy and sell digital advertising. The objective of this work is to assist AdTechs in optimizing summary report algorithms. &lt;/p>; &lt;p>; The Contribution Vector algorithm converts measurements into an ARA format that is discretized and scaled. Scaling needs to account for the overall contribution limit per impression. Here we propose a method that clips and performs randomized rounding. The outcome of the algorithm is a histogram of aggregatable keys and values. &lt;/p>; &lt;p>; Next, the Contribution Bounding algorithm runs on client devices and enforces the contribution bound on attributed reports where any further contributions exceeding the limit are dropped. The output is a histogram of attributed conversions. &lt;/p>; &lt;p>; The Summary Reports algorithm runs on the server side inside a &lt;a href=&quot;https://en.wikipedia.org/wiki/Trusted_execution_environment&quot;>;trusted execution environment&lt;/a>; and returns noisy aggregate results that satisfy DP. Noise is sampled from the discrete &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_distribution&quot;>;Laplace distribution&lt;/a>;, and to enforce privacy budgeting, a report may be queried only once. &lt;/p>; &lt;p>; Finally, the Reconstruct Values algorithm converts measurements back to the original scale. Reconstruct Values and Contribution Vector Algorithms are designed by the AdTech, and both impact the utility received from the summary report. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1014&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0gUXyl66S0s9xVnnRLOOITvsG3eL9r9K53ouX_0VS7PBeXLepqo2IVasreHXMPjsMOQhf3qOfmhBITnoOvQ1_usH9rRPeKysIXb5Zeebn5FS9vfpU2Qhst8VFHNZvdzV7spvp4Sc3fVVyH4cG3pmbipD4gz6etV2-MsbrrByChGE7WP1ua8iIDvC97LA7/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustrative usage of ARA summary reports, which include Contribution Vector (Algorithm A), Contribution Bounding (Algorithm C), Summary Reports (Algorithm S), and Reconstruct Values (Algorithm R). Algorithms C and S are fixed in the API. The AdTech designs A and R.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Error metrics&lt;/h2>; &lt;p>; There are several factors to consider when selecting an error metric for evaluating the quality of an approximation. To choose a particular metric, we considered the desirable properties of an error metric that further can be used as an objective function. Considering desired properties, we have chosen &lt;a href=&quot;https://developer.chrome.com/docs/privacy-sandbox/summary-reports/design-decisions/#rmsre&quot;>;𝜏-truncated root mean square relative error&lt;/a>; (RMSRE&lt;sub>;𝜏&lt;/sub>;) as our error metric for its properties. See the &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;paper&lt;/a>; for a detailed discussion and comparison to other possible metrics. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Optimization&lt;/h2>; &lt;p>; To optimize utility as measured by RMSRE&lt;sub>;𝜏&lt;/sub>;, we choose a capping parameter, &lt;em>;C&lt;/em>;, and privacy budget, 𝛼, for each slice. The combination of both determines how an actual measurement (such as two conversions with a total value of $3) is encoded on the AdTech side and then passed to the ARA for Contribution Bounding algorithm processing. RMSRE&lt;sub>;𝜏&lt;/sub>; can be computed exactly, since it can be expressed in terms of the bias from clipping and the variance of the noise distribution. Following those steps we find out that RMSRE&lt;sub>;𝜏&lt;/sub>; for a fixed privacy budget, 𝛼,&lt;sub>; &lt;/sub>;or a capping parameter, C, is &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_optimization&quot;>;convex&lt;/a>; (so the error-minimizing value for the other parameter can be obtained efficiently), while for joint variables (C, 𝛼) it becomes non-convex (so we may not always be able to select the best possible parameters). In any case, any off-the-shelf optimizer can be used to select privacy budgets and capping parameters. In our experiments, we use the &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html&quot;>;SLSQP&lt;/a>; minimizer from the &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html&quot;>;scipy.optimize&lt;/a>; library. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Synthetic data&lt;/h2>; &lt;p>; Different ARA configurations can be evaluated empirically by testing them on a conversion dataset. However, access to such data can be restricted or slow due to privacy concerns, or simply unavailable. One way to address these limitations is to use synthetic data that replicates the characteristics of real data. &lt;/p>; &lt;p>; We present a method for generating synthetic data responsibly through statistical modeling of real-world conversion datasets. We first perform an empirical analysis of real conversion datasets to uncover relevant characteristics for ARA. We then design a pipeline that uses this distribution knowledge to create a realistic synthetic dataset that can be customized via input parameters. &lt;/p>; &lt;p>; The pipeline first generates impressions drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_law&quot;>;power-law distribution&lt;/a>; (step 1), then for each impression it generates conversions drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_distribution&quot;>;Poisson distribution&lt;/a>; (step 2) and finally, for each conversion, it generates conversion values drawn from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;>;log-normal distribution&lt;/a>; (step 3). With dataset-dependent parameters, we find that these distributions closely match ad-dataset characteristics. Thus, one can learn parameters from historical or public datasets and generate synthetic datasets for experimentation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s841/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;841&quot; data-original-width=&quot;840&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhj7tcj0k-h4e2ZsEdIfG6FtkppEv0_Z_Vl4smOLgUoXKqFUSByNdDuvutA58TATLM9BmjZVugPG9TWL5VUd4fIQ_acxdIhJx-EK3qY-D8WJi_aqTvhgNZXfqwLyXxBOSS-vIHIWHYKWa-7_kNyrqHlkWxmRlBl4LbeYdx9sonX159djwHBP41W1jNBLMEJ/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overall dataset generation steps with features for illustration.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>; Experimental evaluation &lt;/h2>; &lt;p>; We evaluate our algorithms on three real-world datasets (&lt;a href=&quot;https://ailab.criteo.com/criteo-sponsored-search-conversion-log-dataset/&quot;>;Criteo&lt;/a>;, AdTech Real Estate, and AdTech Travel) and three synthetic datasets. Criteo consists of 15M clicks, Real Estate consists of 100K conversions, and Travel consists of 30K conversions. Each dataset is partitioned into a training set and a test set. The training set is used to choose contribution budgets, clipping threshold parameters, and the conversion count limit (the real-world datasets have only one conversion per click), and the error is evaluated on the test set. Each dataset is partitioned into slices using impression features. For real-world datasets, we consider three queries for each slice; for synthetic datasets, we consider two queries for each slice. &lt;/p>; &lt;p>; For each query we choose the RMSRE&lt;sub>;𝝉&lt;/sub>; 𝜏 value to be five times the median value of the query on the training dataset. This ensures invariance of the error metric to data rescaling, and allows us to combine the errors from features of different scales by using 𝝉 per each feature. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s1971/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1028&quot; data-original-width=&quot;1971&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_y5lBRvm3MhfZn6vSJgY2kAQw-95oT5wVCl2Mblkv3Cn1069EBJIaNFrXddOqSk1b5YUbNZUGAmmbFYdKLIqg5ngm5wUJXwnTcnhya3l_ovwMhgsPwJN5I6tKKJGYI4iNgEynK6ismsXKeay6UfhlVIZt8aEmLrIybPwHbkmt9L07K_s1C9rKIxwrKCpr/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Scatter plots of real-world datasets illustrating the probability of observing a conversion value. The fitted curves represent best log-normal distribution models that effectively capture the underlying patterns in the data.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Results&lt;/h3>; &lt;p>; We compare our optimization-based algorithm to a simple baseline approach. For each query, the baseline uses an equal contribution budget and a fixed quantile of the training data to choose the clipping threshold. Our algorithms produce substantially lower error than baselines on both real-world and synthetic datasets. Our optimization-based approach adapts to the privacy budget and data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1999&quot; data-original-width=&quot;1730&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJcmMU1Snvx2N4hdh5iMmadldy6nLn1jXjOGCoefqSRT7auQ3u_zZsgefqfzmsyHUBEZHR6z6ZW2CkFgxrEBe0hBeYTMihk1qtHOF-qBw_WbEdq6C8x0iQy_DvXHqMrCBqH7tnmXNCPlZI29oAiTitD-RU3hH29MKlCHiLUN_3SFkXB71-fv3fU4jx-1q/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RMSRE&lt;sub>;τ&lt;/sub>; for privacy budgets {1, 2, 4, 8, 16, 32, 64} for our algorithms and baselines on three real-world and three synthetic datasets. Our optimization-based approach consistently achieves lower error than baselines that use a fixed quantile for the clipping threshold and split the contribution budget equally among the queries.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We study the optimization of summary reports in the ARA, which is currently deployed on hundreds of millions of Chrome browsers. We present a rigorous formulation of the contribution budgeting optimization problem for ARA with the goal of equipping researchers with a robust abstraction that facilitates practical improvements. &lt;/p>; &lt;p>; Our recipe, which leverages historical data to bound and scale the contributions of future data under differential privacy, is quite general and applicable to settings beyond advertising. One approach based on this work is to use past data to learn the parameters of the data distribution, and then to apply synthetic data derived from this distribution for privacy budgeting for queries on future data. Please see the &lt;a href=&quot;https://arxiv.org/abs/2311.13586&quot;>;paper&lt;/a>; and &lt;a href=&quot;https://github.com/google-research/google-research/tree/master/ara_optimization&quot;>;accompanying code&lt;/a>; for detailed algorithms and proofs. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was done in collaboration with Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, and Avinash Varadarajan. We thank Akash Nadan for his help.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/68038970217457115/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/68038970217457115&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/summary-report-optimization-in-privacy.html&quot; rel=&quot;alternate&quot; title=&quot;Summary report optimization in the Privacy Sandbox Attribution Reporting API&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxpfW7ZN6OUw0lsK3lruiscqMgB3Jt8aBViPGNvHA0MzUPSFp6TRDOPdqSfO4A_0QgpWQo5mT0Vdplv5uBFQmdSePT1LPlwN-hLJ1TP-SHwmIMXYfoNL9CxBw11ABp89ril2o6lDJcT8MCJQ6HwU13vmN6CqnCnNwSpH9d4lgO_CISNxVqKCYSyT_SiwN/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8288223977991952319&lt;/id>;&lt;published>;2023-12-01T09:19:00.000-08:00&lt;/published>;&lt;updated>;2023-12-05T09:38:21.187-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Translation&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Translate&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Unsupervised speech-to-speech translation from monolingual data&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eliya Nachmani, Research Scientist, and Michelle Tadmor Ramanovich, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s1600/T3.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Speech-to-speech translation (S2ST) is a type of &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;>;machine translation&lt;/a>; that converts spoken language from one language to another. This technology has the potential to break down language barriers and facilitate communication between people from different cultures and backgrounds. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Previously, we introduced &lt;a href=&quot;https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html&quot;>;Translatotron 1&lt;/a>; and &lt;a href=&quot;https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html&quot;>;Translatotron 2&lt;/a>;, the first ever models that were able to directly translate speech between two languages. However they were trained in supervised settings with parallel speech data. The scarcity of parallel speech data is a major challenge in this field, so much that most public datasets are semi- or fully-synthesized from text. This adds additional hurdles to learning translation and reconstruction of speech attributes that are not represented in the text and are thus not reflected in the synthesized training data. &lt;/p>; &lt;p>; Here we present &lt;a href=&quot;https://arxiv.org/abs/2305.17547&quot;>;Translatotron 3&lt;/a>;, a novel unsupervised speech-to-speech translation architecture. In Translatotron 3, we show that it is possible to learn a speech-to-speech translation task from monolingual data alone. This method opens the door not only to translation between more language pairs but also towards translation of the non-textual speech attributes such as pauses, speaking rates, and speaker identity. Our method does not include any direct supervision to target languages and therefore we believe it is the right direction for paralinguistic characteristics (eg, such as tone, emotion) of the source speech to be preserved across translation. To enable speech-to-speech translation, we use &lt;a href=&quot;https://arxiv.org/abs/1511.06709&quot;>;back-translation&lt;/a>;, which is a technique from unsupervised machine translation (UMT) where a synthetic translation of the source language is used to &lt;a href=&quot;https://arxiv.org/abs/1710.11041&quot;>;translate texts without bilingual text datasets&lt;/a>;. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Translatotron 3 &lt;/h2>; &lt;p>; Translatotron 3 addresses the problem of unsupervised S2ST, which can eliminate the requirement for bilingual speech datasets. To do this, Translatotron 3&#39;s design incorporates three key aspects: &lt;/p>; &lt;ol>; &lt;li>;Pre-training the entire model as a &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/html /He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html&quot;>;masked autoencoder&lt;/a>; with &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;>;SpecAugment&lt;/a>;, a simple data augmentation method for speech recognition that operates on the logarithmic &lt;a href=&quot;https://en.wikipedia.org/wiki/Mel-frequency_cepstrum&quot;>;mel spectogram&lt;/a>; of the input audio (instead of the raw audio itself) and is shown to effectively improve the generalization capabilities of编码器。 &lt;/li>;&lt;li>;Unsupervised embedding mapping based on &lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;>;multilingual unsupervised embeddings&lt;/a>; (MUSE), which is trained on unpaired languages but allows the model to learn an embedding space that is shared between the source and target languages. &lt;/li>;&lt;li>;A reconstruction loss based on back-translation, to train an encoder-decoder direct S2ST model in a fully unsupervised manner. &lt;/li>; &lt;/ol>; &lt;p>; The model is trained using a combination of the unsupervised MUSE embedding loss, reconstruction loss, and S2S back-translation loss. During inference, the shared encoder is utilized to encode the input into a multilingual embedding space, which is subsequently decoded by the target language decoder. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Architecture&lt;/h3>; &lt;p>; Translatotron 3 employs a shared encoder to encode both the source and target语言。 The decoder is composed of a linguistic decoder, an acoustic synthesizer (responsible for acoustic generation of the translation speech), and a singular attention module, like Translatotron 2. However, for Translatotron 3 there are two decoders, one for the source language and another for the target language. During training, we use monolingual speech-text datasets (ie, these data are made up of speech-text pairs; they are &lt;em>;not&lt;/em>; translations). &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Encoder&lt;/h3>; &lt;p>; The encoder has the same architecture as the speech encoder in the Translatotron 2. The output of the encoder is split into two parts: the first part incorporates semantic information whereas the second part incorporates acoustic information. By using the MUSE loss, the first half of the output is trained to be the MUSE embeddings of the text of the input speech spectrogram. The latter half is updated without the MUSE loss. It is important to note that the same encoder is shared between source and target languages. Furthermore, the MUSE embedding is multilingual in nature. As a result, the encoder is able to learn a multilingual embedding space across source and target languages. This allows a more efficient and effective encoding of the input, as the encoder is able to encode speech from both languages into a common embedding space, rather than maintaining a separate embedding space for each language. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Decoder&lt;/h3>; &lt;p>; Like Translatotron 2, the decoder is composed of three distinct components, namely the linguistic decoder, the acoustic synthesizer, and the attention module. To effectively handle the different properties of the source and target languages, however, Translatotron 3 has two separate decoders, for the source and target languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Two part training&lt;/h3>; &lt;p>; The training methodology consists of two parts: (1) auto-encoding with reconstruction and (2) a back-translation term. In the first part, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and the reconstruction loss. This phase aims to ensure that the network generates meaningful multilingual representations. In the second part, the network is further trained to translate the input spectrogram by utilizing the back-translation loss. To mitigate the issue of &lt;a href=&quot;https://en.wikipedia.org/wiki/Catastrophic_interference#:~:text=Catastrophic%20interference%2C%20also%20known%20as,information%20upon%20learning%20new%20information.&quot;>;catastrophic forgetting&lt;/a>; and enforcing the latent space to be multilingual, the MUSE loss and the reconstruction loss are also applied in this second part of training. To ensure that the encoder learns meaningful properties of the input, rather than simply reconstructing the input, we apply SpecAugment to encoder input at both phases. It has been shown to effectively improve the generalization capabilities of the encoder by augmenting the input data. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Training objective&lt;/h3>; &lt;p>; During the back-translation training phase (illustrated in the section below), the network is trained to translate the input spectrogram to the target language and then back to the source language. The goal of back-translation is to enforce the latent space to be multilingual. To achieve this, the following losses are applied: &lt;/p>; &lt;ul>; &lt;li>;MUSE loss: The MUSE loss measures the similarity between the multilingual embedding of the input spectrogram and the multilingual embedding of the back-translated spectrogram. &lt;/li>;&lt;li>;Reconstruction loss: The reconstruction loss measures the similarity between the input spectrogram and the back-translated spectrogram. &lt;/li>; &lt;/ul>; &lt;p>; In addition to these losses, SpecAugment is applied to the encoder input at both phases. Before the back-translation training phase, the network is trained to auto-encode the input to a multilingual embedding space using the MUSE loss and reconstruction loss. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MUSE loss&lt;/h3>; &lt;p>; To ensure that the encoder generates multilingual representations that are meaningful for both decoders, we employ a MUSE loss during training. The MUSE loss forces the encoder to generate such a representation by using pre-trained MUSE embeddings. During the training process, given an input text transcript, we extract the corresponding MUSE embeddings from the embeddings of the input language. The error between MUSE embeddings and the output vectors of the encoder is then minimized. Note that the encoder is indifferent to the language of the input during inference due to the multilingual nature of the embeddings. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s1999 /image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;999&quot; data-original-width=&quot;1999&quot; src=&quot; https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK9JBHeNh-ZSB1HNPC4Czr65zaUaybMGUImC6UV9ZXkwJLKm-50R4D53tyKq4J-HpMfVLjm65eyAE3_A87e1z5N9sXsUHPGlRtMB08uE2zmkd6bbcHT4ftxzNN_vbYw3lLQqXgaTjL2yLaOG-cBd3Xumg8o38TUvFqKIlNuj0h9Xl4zQt1OzhwwWIr7d-V/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr >;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The training and inference in Translatotron 3. Training includes the reconstruction loss via the auto-encoding path and employs the reconstruction loss via back-translation. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Audio samples&lt;/h2>; &lt;p>; Following are examples of direct speech-to-speech translation from Translatotron 3: &lt;/p>; &lt;h3>; Spanish-to-English (on Conversational dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left ： 汽车; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/conv_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 Synthesized dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS-synthesized reference (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-s_es_en/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;br />; &lt;h3>;Spanish-to-English (on CommonVoice11 dataset)&lt;/h3>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>; &lt;tbody>; &lt;tr>; &lt;td>;Input (Spanish)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/src.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;TTS reference (English)&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/ref.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Translatotron 3 (English)&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td>;&lt;audio controls=&quot;controls&quot; src=&quot;https://google-research.github.io/lingvo-lab/translatotron3/examples/cm-e/1/pred.wav&quot;>;&lt;/audio>;&lt;/td>; &lt;/tr>; &lt;/tbody>; &lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Performance &lt;/h2>; &lt;p>; To empirically evaluate the performance of the proposed approach, we conducted experiments on English and Spanish using various datasets, including the &lt;a href=&quot;https://aclanthology.org/2020.lrec-1.520/&quot;>;Common Voice 11&lt;/a>; dataset, as well as two synthesized datasets derived from the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;Conversational&lt;/a>; and Common Voice 11 datasets. &lt;/p>; &lt;p>; The translation quality was measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLEU&lt;/a>; (higher is better) on ASR (automatic speech recognition) transcriptions from the translated speech, compared to the corresponding reference translation text. Whereas, the speech quality is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>; score (higher is better). Furthermore, the speaker similarity is measured by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;>;average cosine similarity&lt;/a>; (higher is better). &lt;/p>; &lt;p>; Because Translatotron 3 is an &lt;em>;unsupervised&lt;/em>; method, as a baseline we used a cascaded S2ST system that is combined from ASR, unsupervised machine translation (UMT), and TTS (text-to -演讲）。 Specifically, we employ UMT that uses the nearest neighbor in the embedding space in order to create the translation. &lt;/p>; &lt;p>; Translatotron 3 outperforms the baseline by large margins in every aspect we measured: translation quality, speaker similarity, and speech quality. It particularly excelled on the &lt;a href=&quot;https://arxiv.org/abs/1811.02050&quot;>;conversational corpus&lt;/a>;. Moreover, Translatotron 3 achieves speech naturalness similar to that of the ground truth audio samples (measured by &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;MOS&lt;/a>;, higher is better). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUTs76BWvt7hs_NTQSVtz_rQjl1WDu-xadGg5xL7wFo6JLZZ7jYIUmKC6M8HXL1RvnDrjjHTESOnvxKeSX1G-yh9vCPEKshy4TDiYAjlYWlTCXEi2HtiKZjwnzFoYNzQwZrJIL-YGA08MVT1fYwlUDDD6wBsvfTOPHKYMGm0rZXJEyva3XbkQd3hSxyFAb/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Translation quality (measured by BLEU, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJMoR3EyxNHlSyVzTpiIkMMsVXYJa0c3uJjdRkfCTKhLpLoNBCCBlzMQkWgNxCJj1GJFpCqAUtLLFZjuv6F6WYzj_q9tqt4Qq9xYzs8osVN2IjfxhY2weXdPJ2AdecRKP4MD8pgB0-dCwCP2QWcJx0cJs1Dbg-WgJRrgPvHY6OvZPOtuAwm_HqKuSyCh5V/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Speech similarity (measured by average cosine similarity between input speaker and output speaker, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s1200/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ4hT_YlcTEbE-WAqE0IjP_uBP_q6TttLPO8CwF_Gm9WBJyW6UwEHaMr2nQo6kBs8ffUvx20mJCX_Gcj93iUCh1CDueObHoU4PhaCgqmzKcmZCijHVCr9uvRX99d2LHGM3DgnKyyfX6xg4PmDwyPg0I7tFhHE-CX-iPsV0zygPL8z1P74h4XXZYXc42_XJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Mean-opinion-score (measured by average MOS metric, where higher is better) evaluated on three Spanish-English corpora.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future work&lt;/h2>; &lt;p>; As future work, we would like to extend the work to more languages and investigate whether zero-shot S2ST can be applied with the back-translation technique. We would also like to examine the use of back-translation with different types of speech data, such as noisy speech and low-resource languages. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The direct contributors to this work include Eliya Nachmani, Alon Levkovitch, Yifan Ding, Chulayutsh Asawaroengchai, Heiga Zhen, and Michelle Tadmor Ramanovich. We also thank Yu Zhang, Yuma Koizumi, Soroosh Mariooryad, RJ Skerry-Ryan, Neil Zeghidour, Christian Frank, Marco Tagliasacchi, Nadav Bar, Benny Schlesinger and Yonghui Wu.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8288223977991952319/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8288223977991952319&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/12/unsupervised-speech-to-speech.html&quot; rel=&quot;alternate&quot; title=&quot;Unsupervised speech-to-speech translation from monolingual data&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuXgYf5aJuSobrasrRUpb5IUKH05RFHJ5_vcwW-XyOLQdKOEonciXcyXtcJN2zPkHu5_k4wvIsl6oFZd4UfYsBfjSK27YaIcw7s1-3dekdJVxTBM-4hrBvndT-go6YOsscswtV6FvE_3QHml8zZjWIz7J4quRh8UBL9gzW9guAVYd4czuHYhY6lu9TkK4K/s72-c/T3.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1055595481112584523&lt;/id>;&lt;published>;2023-11-22T08:03:00.000-08:00&lt;/published>;&lt;updated>;2023-11-30T13:46:35.804-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;High-Performance Computing&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Physics&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Weather&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving simulations of clouds and their effects on climate&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Tapio Schneider, Visiting Researcher, and Yi-fan Chen, Engineering Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Today&#39;s climate models successfully capture broad global warming trends. However, because of uncertainties about processes that are &lt;a href=&quot;https://physicstoday.scitation.org/doi/abs/10.1063/PT.3.4772&quot;>;small in scale yet globally important&lt;/a>;, such as &lt;a href=&quot;http://rdcu.be/ohot&quot;>;clouds&lt;/a>; and &lt;a href=&quot;https://doi.org/10.3389/fmars.2019.00065&quot;>;ocean turbulence&lt;/a>;, these models&#39; predictions of upcoming climate changes are not very accurate in detail. For example, predictions of the time by which the global mean surface temperature of Earth will have warmed 2℃, relative to preindustrial times, &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_TS.pdf&quot;>;vary by 40–50 years&lt;/a>; (a full human generation) among today&#39;s models. As a result, we do not have the &lt;a href=&quot;https://www.nature.com/articles/s41558-020-00984-6&quot;>;accurate and geographically granular predictions&lt;/a>; we need to plan resilient infrastructure, adapt supply chains to climate disruption, and assess the risks of climate-related hazards to vulnerable communities. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In large part this is because clouds dominate errors and uncertainties in climate predictions for the coming decades [&lt;a href=&quot;https://doi.org/10.1029/2005GL023851&quot;>;1&lt;/a>;, &lt;a href=&quot;https://link.springer.com/article/10.1007/s00382-013-1725-9&quot;>;2&lt;/a>;, &lt;a href=&quot;https://doi.org/10.1038/nclimate3402&quot;>;3&lt;/a>;]. Clouds reflect sunlight and exert a &lt;a href=&quot;https://en.wikipedia.org/wiki/Greenhouse_effect&quot;>;greenhouse effect&lt;/a>;, making them crucial for regulating Earth&#39;s energy balance and mediating the response of the climate system to changes in greenhouse gas concentrations. However, they are too small in scale to be directly resolvable in today&#39;s climate models. Current climate models resolve motions at scales of tens to a hundred kilometers, with a &lt;a href=&quot;https://link.springer.com/article/10.1186/s40645-019-0304-z&quot;>;few pushing toward&lt;/a>; the &lt;a href=&quot;https://journals.ametsoc.org/view/journals/bams/101/5/bams-d-18-0167.1.xml&quot;>;kilometer-scale&lt;/a>;. However, the turbulent air motions that sustain, for example, the low clouds that cover large swaths of tropical oceans have scales of meters to tens of meters. Because of this wide difference in scale, climate models use empirical parameterizations of clouds, rather than simulating them directly, which result in large errors and uncertainties. &lt;/p>; &lt;p>; While clouds cannot be directly resolved in global climate models, their turbulent dynamics can be simulated in limited areas by using high-resolution &lt;a href=&quot;https://en.wikipedia.org/wiki/Large_eddy_simulation&quot;>;large eddy simulations&lt;/a>; (LES). However, the high computational cost of simulating clouds with LES has inhibited broad and systematic numerical experimentation, and it has held back the generation of large datasets for training parameterization schemes to represent clouds in coarser-resolution global climate models. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;Accelerating Large-Eddy Simulations of Clouds with Tensor Processing Units&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/journal/19422466&quot;>;Journal of Advances in Modeling Earth Systems&lt;/a>;&lt;/em>; (JAMES), and in collaboration with a &lt;a href=&quot;https://clima.caltech.edu/&quot;>;Climate Modeling Alliance&lt;/a>; (CliMA) lead who is a visiting researcher at Google, we demonstrate that &lt;a href=&quot;https://cloud.google.com/tpu&quot;>;Tensor Processing Units&lt;/a>; (TPUs) — application-specific integrated circuits that were originally developed for machine learning (ML) applications — can be effectively used to perform LES of clouds. We show that TPUs, in conjunction with tailored software implementations, can be used to simulate particularly computationally challenging &lt;a href=&quot;https://en.wikipedia.org/wiki/Marine_stratocumulus&quot;>;marine stratocumulus clouds&lt;/a>; in the conditions observed during the &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;Dynamics and Chemistry of Marine Stratocumulus&lt;/a>; (DYCOMS) field study. This successful TPU-based LES code reveals the utility of TPUs, with their large computational resources and tight interconnects, for cloud simulations. &lt;/p>; &lt;p>; Climate model accuracy for critical metrics, like precipitation or the energy balance at the top of the atmosphere, has improved roughly 10% per decade in the last 20 years. Our goal is for this research to enable a 50% reduction in climate model errors by improving their representation of clouds. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Large-eddy simulations on TPUs&lt;/h2>; &lt;p>; In this work, we focus on stratocumulus clouds, which cover ~20% of the tropical oceans and are the most prevalent cloud type on earth. Current climate models are not yet able to reproduce stratocumulus cloud behavior correctly, which has been one of the largest sources of errors in these models. Our work will provide a much more accurate ground truth for large-scale climate models. &lt;/p>; &lt;p>; Our simulations of clouds on TPUs exhibit unprecedented computational throughput and scaling, making it possible, for example, to simulate stratocumulus clouds with 10× speedup over real-time evolution across areas up to about 35 × 54 km&lt;sup>;2&lt;/sup>;. Such domain sizes are close to the cross-sectional area of typical global climate model grid boxes. Our results open up new avenues for computational experiments, and for substantially enlarging the sample of LES available to train parameterizations of clouds for global climate models.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;8&quot; cellspacing=&quot;4&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s540/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbx0OTXHC32wHSDDNIqQkj-NLjggGrcW9Hfy4o_TCIiP_n6Lt0zpOSu0OIVd0BkWmPaGUNS6oF0qZ2KfUcBZQQi9EGgq6dEhvMiY4PGq_bj6nrnP1p3uQz-dO3AlK7TJCIlMSZcQIRxuOwm7q2lhEVdJTumMfLubHFNZpg7FnRh5GLG5lqhZygSdT-0AY5/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>; &lt;td>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s540/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;320&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5_r1YHNKYhQtxQZKBIAZ67d7AJOwEm79UlI-d-MftNzbnogH2CwkB8XpC8XN1FMa_Y6fVo9dTz2AG4DrbXkLjauQTLsu11KXfgdNxt_AAHpfCeSovo8cCrjWs7KqBOCCYU3-Os3smHz0bZIax9Iyf8L39edQD-rSq7kqV8SGkhlO5VelXE8MJfPk0rjwP/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>; &lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of the cloud evolution from a simulation of a 285 x 285 x 2 km&lt;sup>;3&lt;/sup>; stratocumulus cloud sheet. This is the largest cloud sheet of its kind ever simulated. &lt;strong>;Left&lt;/strong>;: An oblique view of the cloud field with the camera cruising. &lt;strong>;Right&lt;/strong>;: Top view of the cloud field with the camera gradually pulled away.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The LES code is written in TensorFlow, an open-source software platform developed by Google for ML applications. The code takes advantage of TensorFlow&#39;s graph computation and &lt;a href=&quot;https://www.tensorflow.org/xla&quot;>;Accelerated Linear Algebra&lt;/a>; (XLA) optimizations, which enable the full exploitation of TPU hardware, including the high-speed, low-latency &lt;a href=&quot;https://patents.google.com/patent/US9372800&quot;>;inter-chip interconnects&lt;/a>; (ICI) that helped us achieve this unprecedented performance. At the same time, the TensorFlow code makes it easy to incorporate ML components directly within the physics-based fluid solver. &lt;/p>; &lt;p>; We validated the code by simulating canonical test cases for atmospheric flow solvers, such as a buoyant bubble that rises in neutral stratification, and a negatively buoyant bubble that sinks and impinges on the surface. These test cases show that the TPU-based code faithfully simulates the flows, with increasingly fine turbulent details emerging as the resolution increases. The validation tests culminate in simulations of the conditions during the DYCOMS field campaign. The TPU-based code reliably reproduces the cloud fields and turbulence characteristics observed by aircraft during a field campaign — a feat that is &lt;a href=&quot;https://doi.org/10.1175/MWR2930.1&quot;>;notoriously difficult to achieve for LES&lt;/a>; because of the &lt;a href=&quot;https://doi.org/10.1029/2018MS001312&quot;>;rapid changes in temperature and other thermodynamic properties&lt;/a>; at the top of the stratocumulus decks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/s1023/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;815&quot; data-original-width=&quot;1023&quot; height=&quot;510&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiwHPP1IE_dz44C374eIoiL9Gq7OcYIdAuZkUQ4t1PPAcdgnU-Cf8C_7UAYBKkEQZBBY1tbBzBkqGi01-kUAMgs7tbjvH5PtSQDxGHVRPawtZTDEc8ounOJTnzAi5fG1EgKTbxZ1TQJiZc7blSmDKTFJzdHp6B_xwfqM_-n4DXp9nR_F3Zx5QramRMLCjV/w640-h510/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;One of the test cases used to validate our TPU Cloud simulator. The fine structures from the density current generated by the negatively buoyant bubble impinging on the surface are much better resolved with a high resolution grid (10m, bottom row) compared to a low resolution grid (200 m, top row).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Outlook&lt;/h2>; &lt;p>; With this foundation established, our next goal is to substantially enlarge existing &lt;a href=&quot;https://doi.org/10.1029/2021MS002631&quot;>;databases&lt;/a>; of high-resolution cloud simulations that researchers building climate models can use to develop better cloud parameterizations — whether these are for physics-based models, ML models, or hybrids of the two. This requires additional physical processes beyond that described in the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023MS003619&quot;>;paper&lt;/a>;; for example, the need to integrate radiative transfer processes into the code. Our goal is to generate data across a variety of cloud types, eg, thunderstorm clouds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s540/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;303&quot; data-original-width=&quot;540&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAj2-AUpLOJOGakUMp5e4anHQLS8k9yJNN4MKg7Eh9skhe2zJz7PRvjv_0Xob4pVEvS2ypPJWa2LxO6b_zygTMw9Yq6c3PfCm2NttTdmv0thdw4yBye9hT-6CokiI-ddKCGqGVl-yLCJmZa0adj8jQpVR93amGh9EbvDzuMTAjIxfO3G8W2Vu9x5LU9rdF/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Rendering of a thunderstorm simulation using the same simulator as the stratocumulus simulation work. Rainfall can also be observed near the ground.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; This work illustrates how advances in hardware for ML can be surprisingly effective when repurposed in other research areas — in this case, climate modeling. These simulations provide detailed training data for processes such as in-cloud turbulence, which are not directly observable, yet are crucially important for climate modeling and prediction. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank the co-authors of the paper: Sheide Chammas, Qing Wang, Matthias Ihme, and John Anderson. We&#39;d also like to thank Carla Bromberg, Rob Carver, Fei Sha, and Tyler Russell for their insights and contributions to the work.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1055595481112584523/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1055595481112584523&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/improving-simulations-of-clouds-and.html&quot; rel=&quot;alternate&quot; title=&quot;Improving simulations of clouds and their effects on climate&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipIpAUM7d1E-XNQdqZ3hiRC7VgksSfuI249KFq9yL5ZTZF8-DJ5Sfo-fRf2m7hTKuTeJwSGQzwz6rtfdZs8PxlualXz7U53miz6ahU3oyO9WQWkePA1fgr5mxRa75NYKKPe0KLOYSPLxzfDfoIABePqL1etrqaDuRPjJotAVEIbuBRw6EWrJSxXB-BzCTS/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3970135078050750650&lt;/id>;&lt;published>;2023-11-21T10:09:00.000-08:00&lt;/published>;&lt;updated>;2023-11-21T10:09:33.541-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Open sourcing Project Guideline: A platform for computer vision accessibility technology&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Dave Hawkey, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Two years ago we &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;announced Project Guideline&lt;/a>;, a collaboration between Google Research and &lt;a href=&quot;https://www.guidingeyes.org/&quot;>;Guiding Eyes for the Blind&lt;/a>; that enabled people with visual impairments (eg, blindness and low-vision) to walk, jog, and run independently. Using only a Google Pixel phone and headphones, Project Guideline leverages on-device machine learning (ML) to navigate users along outdoor paths marked with a painted line. The technology has been &lt;a href=&quot;https://projectguidelinejp.withgoogle.com/intl/en/&quot;>;tested all over the world&lt;/a>; and even demonstrated during the &lt;a href=&quot;https://www.youtube.com/live/2cW1-plwqeQ?si=MTIX2uJkyWuLluht&amp;amp;t=7334&quot;>;opening ceremony at the Tokyo 2020 Paralympic Games&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Since the original announcement, we set out to improve Project Guideline by embedding new features, such as obstacle detection and advanced path planning, to safely and reliably navigate users through more complex scenarios (such as sharp turns and nearby pedestrians). The early version featured a simple frame-by-frame image segmentation that detected the position of the path line relative to the image frame. This was sufficient for orienting the user to the line, but provided limited information about the surrounding environment. Improving the navigation signals, such as alerts for obstacles and upcoming turns, required a much better understanding and mapping of the users&#39; environment. To solve these challenges, we built a platform that can be utilized for a variety of spatially-aware applications in the accessibility space and beyond. &lt;/p>; &lt;p>; Today, we announce the &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;open source release of Project Guideline&lt;/a>;, making it available for anyone to use to improve upon and build new accessibility experiences. The release includes &lt;a href=&quot;https://github.com/google-research/project-guideline&quot;>;source code&lt;/a>; for the core platform, an &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/android&quot;>;Android application&lt;/a>;, pre-trained &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/vision/models&quot;>;ML models&lt;/a>;, and a &lt;a href=&quot;https://github.com/google-research/project-guideline/tree/main/project_guideline/unreal&quot;>;3D simulation framework&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;System design&lt;/h2>; &lt;p>; The primary use-case is an Android application, however we wanted to be able to run, test, and debug the core logic in a variety of environments in a reproducible way. This led us to design and build the system using C++ for close integration with &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>; and other core libraries, while still being able to integrate with Android using the &lt;a href=&quot;https://developer.android.com/ndk&quot;>;Android NDK&lt;/a>;. &lt;/p>; &lt;p>; Under the hood, Project Guideline uses &lt;a href=&quot;https://developers.google.com/ar&quot;>;ARCore&lt;/a>; to estimate the position and orientation of the user as they navigate the课程。 A segmentation model, built on the &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;>;DeepLabV3+&lt;/a>; framework, processes each camera frame to generate a binary mask of the guideline (see the &lt;a href=&quot;https://ai.googleblog.com/2021/05/project-guideline-enabling-those-with.html&quot;>;previous blog post&lt;/a>; for more details). Points on the segmented guideline are then projected from image-space coordinates onto a world-space ground plane using the camera pose and lens parameters (intrinsics) provided by ARCore. Since each frame contributes a different view of the line, the world-space points are aggregated over multiple frames to build a virtual mapping of the real-world guideline. The system performs piecewise curve approximation of the guideline world-space coordinates to build a spatio-temporally consistent trajectory. This allows refinement of the estimated line as the user progresses along the path. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s800/image1.gif&quot; style =&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;355&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEhr5dbXb4Dhjd-HqBRwKYd0YAjHxUr4avoU_rlp6aBR3LJlBtRGD2OjgCibJCIE_WRxwo6SYYKL7oQHOuW39kEvTH7B2rMnoI5wKosp3L8hliQJivnl4LdWDqZ_cK3BlQxFDedBiFy_JR3HYaAVd53KwRYeOmMVQiL3prW6qYcpjBbvV4-cRXhmYyPyr4nY/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;Project Guideline builds a 2D map of the guideline, aggregating detected points in each frame (&lt;strong>;red&lt;/strong>;) to build a stateful representation (&lt;strong>;blue&lt;/strong>;) as the runner progresses along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A control system dynamically selects a target point on the line some distance ahead based on the user&#39;s current position, velocity,和方向。 An audio feedback signal is then given to the user to adjust their heading to coincide with the upcoming line segment. By using the runner&#39;s velocity vector instead of camera orientation to compute the navigation signal, we eliminate noise caused by irregular camera movements common during running. We can even navigate the user back to the line while it&#39;s out of camera view, for example if the user overshot a turn. This is possible because ARCore continues to track the pose of the camera, which can be compared to the stateful line map inferred from previous camera images. &lt;/p>; &lt;p>; Project Guideline also includes obstacle detection and avoidance features. An ML model is used to estimate depth from single images. To train this monocular depth model, we used &lt;a href=&quot;https://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot;>;SANPO&lt;/a>;, a large dataset of outdoor imagery from urban, park, and suburban environments that was curated in-house. The model is capable of detecting the depth of various obstacles, including people, vehicles, posts, and more. The depth maps are converted into 3D point clouds, similar to the line segmentation process, and used to detect the presence of obstacles along the user&#39;s path and then alert the user through an audio signal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/s800/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;800&quot; height=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpEuQPa-p3TKMgYhMl6BD7e53W7PNZPxJTqaE2gqhEx6b8wHIcVpWv9b3C21z_-c1dsR5Qlb2LqRjmTOsPV2YH9nflHTaPsjiKoILBpl5sDkWBrmn5PJ7Yeaq0Uismxtbv6CmHBlK_sNqM__4TxHkFZFuuy5fry6Z5EKsevc_2jMfngNp7JBpc78Sb3MCG/w640-h480/image2.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Using a monocular depth ML model, Project Guideline constructs a 3D point cloud of the environment to detect and alert the user of potential obstacles along the path.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; A low-latency audio system based on the &lt;a href=&quot;https://developer.android.com/ndk/guides/audio/aaudio/aaudio&quot;>;AAudio API&lt;/a>; was implemented to provide the navigational sounds and cues to the user. Several &lt;em>;sound packs&lt;/em>; are available in Project Guideline, including a spatial sound implementation using the &lt;a href=&quot;https://resonance-audio.github.io/resonance-audio/&quot;>;Resonance Audio API&lt; /a>;. The sound packs were developed by a team of sound researchers and engineers at Google who designed and tested many different sound models. The sounds use a combination of panning, pitch, and spatialization to guide the user along the line. For example, a user veering to the right may hear a beeping sound in the left ear to indicate the line is to the left, with increasing frequency for a larger course correction. If the user veers further, a high-pitched warning sound may be heard to indicate the edge of the path is approaching. In addition, a clear “stop” audio cue is always available in the event the user veers too far from the line, an anomaly is detected, or the system fails to provide a navigational signal. &lt;/p>; &lt;p>; Project Guideline has been built specifically for Google Pixel phones with the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Google Tensor&lt;/a>; chip. The Google Tensor chip enables the optimized ML models to run on-device with higher performance and lower power consumption. This is critical for providing real-time navigation instructions to the user with minimal delay. On a Pixel 8 there is a 28x latency improvement when running the depth model on the &lt;a href=&quot;https://store.google.com/intl/en/ideas/articles/google-tensor-pixel-smartphone/&quot;>;Tensor Processing Unit&lt;/a>; (TPU) instead of CPU, and 9x improvement compared to GPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s1124/image3.jpg&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;748&quot; data-original-width=&quot;1124&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEho9fs3Uxbd3L59poeHKbR_sbeCPN8jRjVhwtoXoeKHS8uJuKyAtHdaQQA8ZRHw_J5n-g4g3JN_mWQFt2ze1TKYfgIMyquc5-0oANrRXL2g6wDwDB8qibAQDbRoYZ2wVQjP-j1B9lnjUpHKVMtBu2wc81nGAza65E9VY-8X7JFlkfYh0hQb3UWK4GoCzgvJ/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Testing and simulation&lt;/h2>; &lt;p>; Project Guideline includes a simulator that enables rapid testing and prototyping of the system in a virtual environment. Everything from the ML models to the audio feedback system runs natively within the simulator, giving the full Project Guideline experience without needing all the hardware and physical environment set up. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/s1200/image4.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1200&quot; height=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwv0GHi2zGV5Qk3Tsun98oSCf0au8mc4kV4XPkuzp0V_zgxoM5ymQhuRRja93BkwtvdCN9HJPxWmRWTVI1eXPIj9Jh-9aS57qCz1vIGvm2uylQzT70F53hOQIoHYNTJefwIav_GEz2zK0z2lB1MtD0hnAojnxPXKXGIfV1QO9kksIMaM_d0qoeHMJxsjWc/w640-h480/image4.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Screenshot of Project Guideline simulator.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; To launch the technology forward, &lt;a href=&quot;https://www.wear.works&quot;>;WearWorks&lt;/a>; has become an early adopter and teamed up with Project Guideline to integrate their patented haptic navigation experience, utilizing haptic feedback in addition to sound to guide runners. WearWorks has been developing haptics for over 8 years, and previously empowered the first blind marathon runner to complete the NYC Marathon without sighted assistance. We hope that integrations like these will lead to new innovations and make the world a more accessible place.&lt;br />; &lt;/p>; &lt;p>; The Project Guideline team is also working towards removing the painted line completely, using the latest advancements in mobile ML technology, such as the &lt;a href=&quot;https://developers.google.com/ar/develop/scene-semantics&quot;>;ARCore Scene Semantics API&lt;/a>;, which can identify sidewalks, buildings, and other objects in outdoor scenes. We invite the accessibility community to build upon and improve this technology while exploring new use cases in other fields. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of Project Guideline and the technologies behind it. We&#39;d like to thank Project Guideline team members: Dror Avalon, Phil Bayer, Ryan Burke, Lori Dooley, Song Chun Fan, Matt Hall, Amélie Jean-aimée, Dave Hawkey, Amit Pitaru, Alvin Shi, Mikhail Sirotenko, Sagar Waghmare, John Watkinson, Kimberly Wilber, Matthew Willson, Xuan Yang, Mark Zarich, Steven Clark, Jim Coursey, Josh Ellis, Tom Hoddes, Dick Lyon, Chris Mitchell, Satoru Arao, Yoojin Chung, Joe Fry, Kazuto Furuichi, Ikumi Kobayashi, Kathy Maruyama, Minh Nguyen, Alto Okamura, Yosuke Suzuki, and Bryan Tanaka. Thanks to ARCore contributors: Ryan DuToit, Abhishek Kar, and Eric Turner. Thanks to Alec Go, Jing Li, Liviu Panait, Stefano Pellegrini, Abdullah Rashwan, Lu Wang, Qifei Wang, and Fan Yang for providing ML platform support. We&#39;d also like to thank Hartwig Adam, Tomas Izo, Rahul Sukthankar, Blaise Aguera y Arcas, and Huisheng Wang for their leadership support. Special thanks to our partners Guiding Eyes for the Blind and Achilles International.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3970135078050750650/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3970135078050750650&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/open-sourcing-project-guideline.html&quot; rel=&quot;alternate&quot; title=&quot;Open sourcing Project Guideline: A platform for computer vision accessibility technology&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrfdKR8ZnuQ1jMtsIsG9AViwd_vkXhbwavfXUC_RYoIeTF8EppGOxlWSp9DNCgzFlUY0Ea4q3deujDXSdXJ_C4lOC89eGfIXz_POk_upHVlx5DdBab8rh0FQuigi3fhluHAOX30GhT9LkZnpU5KMmDMp8jWNpjxKDI8nLwYTqAcExYk3tW7klykfOZ-Sm_/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6563276834599281497&lt;/id>;&lt;published>;2023-11-17T11:36:00.000-08:00&lt;/published>;&lt;updated>;2023-11-17T11:36:32.036-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research Awards&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;University Relations&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Emerging practices for Society-Centered AI&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Anoop Sinha, Research Director, Technology &amp;amp; Society, and Yossi Matias, Vice President, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The first of &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;Google&#39;s AI Principles&lt;/a>; is to “Be socially beneficial.” As AI practitioners, we&#39;re inspired by the transformative potential of AI technologies to benefit society and our shared environment at a scale and swiftness that wasn&#39;t possible before. From &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/&quot;>;helping address the climate crisis&lt;/a>; to &lt;a href=&quot;https://blog.google/technology/health/how-were-using-ai-to-help-transform-healthcare/&quot;>;helping transform healthcare&lt;/a>;, to &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/global-accessibility-awareness-day-google-product-update/&quot;>;making the digital world more accessible&lt;/a>;, our goal is to apply AI responsibly to be helpful to more people around the globe. Achieving global scale requires researchers and communities to think ahead — and act — collectively across the AI ecosystem. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We call this approach Society-Centered AI. It is both an extension and an expansion of &lt;a href=&quot;https://hcil.umd.edu/human-centered-ai/&quot;>;Human-Centered AI, &lt;/a>;focusing on the aggregate needs of society that are still informed by the needs of individual users, specifically within the context of the larger, shared human experience. Recent AI advances offer unprecedented, societal-level capabilities, and we can now methodically address those needs — if we apply collective, multi-disciplinary AI research to society-level, shared challenges, from forecasting hunger to predicting diseases to improving productivity. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/AI_Opportunity_Agenda.pdf&quot;>;opportunity for AI&lt;/a>; to benefit society increases each天。 We took a look at our work in these areas and at the research projects we have supported. Recently, Google &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;announced that 70 professors were selected&lt;/a>; for the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;2023 Award for Inclusion Research Program&lt;/a>;, which supports academic research that addresses the needs of historically marginalized groups globally. Through evaluation of this work, we identified a few emerging practices for Society-Centered AI: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Understand society&#39;s needs&lt;/strong>; &lt;br />;Listening to communities and partners is crucial to understanding major issues deeply and identifying priority challenges to address. As an emerging general purpose technology, AI has the potential to address major global societal issues that can significantly impact people&#39;s lives (eg, educating workers, improving healthcare, and improving productivity). We have found the key to impact is to be centered on society&#39;s needs. For this, we focus our efforts on goals society has agreed should be prioritized, such as the United Nations&#39; &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/globalgoals&quot;>;17 Sustainable Development Goals&lt;/a>;, a set of interconnected goals jointly developed by more than 190 countries to address global challenges. &lt;/li>;&lt;li>;&lt;strong>;Collective efforts to address those needs &lt;br />;&lt;/strong>;Collective efforts bring stakeholders (eg, local and academic communities, NGOs, private-public collaborations) into a joint process of design, development, implementation, and evaluation of AI technologies as they are being developed and deployed to address societal needs. &lt;/li>;&lt;li>;&lt;strong>;Measuring success by how well the effort addresses society&#39;s needs &lt;br />;&lt;/strong>;It is important and challenging to measure how well AI solutions address society&#39;s needs. In each of our cases, we identified primary and secondary indicators of impact that we optimized through our collaborations with stakeholders. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Why is Society-Centered AI important?&lt;/h2>; &lt;p>; The case examples described below show how the Society-Centered AI approach has led to impact across topics, such as accessibility, health, and climate. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Understanding the needs of individuals with non-standard speech&lt;/h3>; &lt;p>; There are &lt;a href=&quot;https://www.nidcd.nih.gov/health/statistics/quick-statistics-voice-speech-language&quot;>;millions of people&lt;/a>; with non-standard speech (eg, impaired articulation, &lt;a href=&quot;https://en.wikipedia.org/wiki/Dysarthria&quot;>;dysarthria&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Spasmodic_dysphonia&quot;>;dysphonia&lt;/a>;)仅在美国。 In &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/&quot;>;2019&lt;/a>;, Google Research launched &lt;a href=&quot;https://blog.research.google/2019/08/project-euphonias-personalized-speech.html&quot;>;Project Euphonia&lt;/a>;, a methodology that allows individual users with non-standard speech to train personalized speech recognition models. Our success began with the impact we had on each individual who is now able to use voice dictation on their mobile device. &lt;/p>; &lt;p>; Euphonia started with a Society-Centered AI approach, including collective efforts with the non-profit organizations &lt;a href=&quot;http://als.net/&quot;>;ALS Therapy Development Institute&lt;/a>; and &lt;a href=&quot;http://www.alsri.org/&quot;>;ALS Residence Initiative&lt;/a>; to understand the needs of individuals with &lt;a href=&quot;https://en.wikipedia.org/wiki/ALS&quot;>;amyotrophic lateral sclerosis&lt;/a>; (ALS) and their ability to use automatic speech recognition systems. Later, we developed &lt;a href=&quot;https://blog.research.google/2021/09/personalized-asr-models-from-large-and.html&quot;>;the world&#39;s largest corpus&lt;/a>; of non-standard speech recordings, which enabled us to train a &lt;a href=&quot;https://blog.research.google/2023/03/universal-speech-model-usm-state-of-art.html&quot;>;Universal Speech Model&lt;/a>; to &lt;a href=&quot;https://blog.research.google/2023/06/responsible-ai-at-google-research-ai.html&quot;>;better recognize disordered speech by 37%&lt;/a>; on real conversation &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_error_rate&quot;>;word error rate&lt;/a>; (WER) measurement. This also led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/speech-accessibility-project/&quot;>;2022&lt;/a>; collaboration between the University of Illinois Urbana-Champaign, Alphabet, Apple, Meta, Microsoft, and Amazon to begin the &lt;a href=&quot;https://speechaccessibilityproject.beckman.illinois.edu/&quot;>;Speech Accessibility Project&lt;/a>;, an ongoing initiative to create a publicly available dataset of disordered speech samples to improve products and make speech recognition more inclusive of diverse speech patterns. Other technologies that use AI to help remove barriers of modality and languages, include &lt;a href=&quot;https://about.google/stories/making-conversation-more-accessible-with-live-transcribe/&quot;>;live transcribe&lt;/a>;, &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/first-time-i-was-able-call-my-23-year-old-son/&quot;>;live caption&lt;/a>; and &lt;a href=&quot;https://blog.google/intl/en-in/products/explore-communicate/easier-access-to-web-pages-let/&quot;>;read aloud&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Focusing on society&#39;s health needs&lt;/h3>; &lt;p>; Access to timely maternal health information can save lives globally: &lt;a href=&quot;https://www.who.int/news/item/23-02-2023-a-woman-dies-every-two-minutes-due-to-pregnancy-or-childbirth--un-agencies&quot;>;every two minutes a woman dies during pregnancy or childbirth&lt;/a>; and &lt;a href=&quot;https://data.unicef.org/topic/child-survival/under-five-mortality/&quot;>;1 in 26 children die before reaching age five&lt;/a>;. In rural India, the education of expectant and new mothers around key health issues pertaining to pregnancy and infancy required scalable, low-cost technology solutions. Together with &lt;a href=&quot;https://armman.org/&quot;>;ARMMAN&lt;/a>;, Google Research supported &lt;a href=&quot;https://blog.research.google/2022/08/using-ml-to-boost-engagement-with.html&quot;>;a program&lt;/a>; that uses mobile messaging and machine learning (ML) algorithms to predict when women might benefit from receiving interventions (ie, targeted preventative care information) and encourages them to engage with the &lt;a href=&quot;https://armman.org/mmitra/&quot;>;mMitra&lt;/a>; free voice call program. Within a year, the mMitra program has shown a 17% increase in infants with tripled birth weight and a 36% increase in women understanding the importance of taking iron tablets during pregnancy. Over 175K mothers and growing have been reached through this automated solution, which public health workers use to improve the quality of information delivery. &lt;/p>; &lt;p>; These efforts have been successful in improving health due to the close collective partnership among the community and those building the AI technology. We have adopted this same approach via collaborations with caregivers to address a variety of medical needs. Some examples include: the use of the &lt;a href=&quot;https://health.google/caregivers/arda/&quot;>;Automated Retinal Disease Assessment&lt;/a>; (ARDA) to &lt;a href=&quot;https://blog.google/technology/health/5-myths-about-medical-ai-debunked/&quot;>;help screen for diabetic retinopathy&lt;/a>; in 250,000 patients in clinics around the world; our partnership with &lt;a href=&quot;https://www.icadmed.com/&quot;>;iCAD&lt;/a>; to bring our &lt;a href=&quot;https://blog.google/technology/ai/icad-partnership-breast-cancer-screening/&quot;>;mammography&lt;/a>; AI models to clinical settings to aid in breast cancer detection; and the development of &lt;a href=&quot;https://sites.research.google/med-palm/&quot;>;Med-PaLM 2&lt;/a>;, a medical large language model that is now being &lt;a href=&quot;https: //cloud.google.com/blog/topics/healthcare-life-sciences/sharing-google-med-palm-2-medical-large-language-model&quot;>;tested with Cloud partners&lt;/a>; to help doctors provide better病人护理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Compounding impact from sustained efforts for crisis response&lt;/h3>; &lt;p>; Google Research&#39;s flood prediction efforts began in 2018 with &lt;a href=&quot;https://www.blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/&quot;>;flood forecasting&lt;/a>; in India and &lt;a href=&quot;https://www.undp.org/bangladesh/blog/climate-change-google-and-bangladesh-floods&quot;>;expanded to Bangladesh&lt;/a>; to help combat the catastrophic damage from yearly floods. The initial efforts began with partnerships with &lt;a href=&quot;https://cwc.gov.in/&quot;>;India&#39;s Central Water Commission&lt;/a>;, local governments and communities. The implementation of these efforts used &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>; on Search and Maps, and, more recently, broadly expanded access via &lt;a href=&quot;https://sites.research.google/floods/l/0/0/3&quot;>;Flood Hub&lt;/a>;. Continued &lt;a href=&quot;https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html&quot;>;collaborations&lt;/a>; and advancing an AI-based global flood forecasting model allowed us to &lt;a href=&quot;https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/&quot;>;expand this capability&lt;/a>; to &lt;a href=&quot;https://blog .google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/&quot;>;over 80 countries&lt;/a>; across Africa, the Asia-Pacific region, Europe, and South, Central, and North美国。 We also partnered with networks of community volunteers to further amplify flood alerts. By working with governments and communities to measure the impact of these efforts on society, we refined our approach and algorithms each year. &lt;/p>; &lt;p>; We were able to leverage those methodologies and some of the underlying technology, such as &lt;a href=&quot;https://www.blog.google/products/search/helping-people-crisis/&quot;>;SOS Alerts&lt;/a>;, from &lt;a href=&quot;https://sites.research.google/floodforecasting/&quot;>;flood forecasting&lt;/a>; to similar societal needs, such as &lt;a href=&quot;https://blog.google/products/search/mapping-wildfires-with-satellite-data/&quot;>;wildfire forecasting&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/extreme-heat-support/&quot;>;heat alerts&lt;/a>;. Our continued engagements with organizations led to the &lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/&quot;>;support of additional efforts&lt;/a>;, such as the World Meteorological Organization&#39;s (WMO) &lt;a href=&quot;https://public.wmo.int/en/earlywarningsforall&quot;>;Early Warnings For All Initiative&lt;/a>;. The continued engagement with communities has allowed us to learn about our users&#39; needs on a societal level over time, expand our efforts, and compound the societal reach and impact of our efforts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Further supporting Society-Centered AI research&lt;/h2>; &lt;p>; &lt;a href=&quot;https://research.google/outreach/air-program/recipients/&quot;>;We recently funded&lt;/a>; 18 university research proposals exemplifying a Society-Centered AI approach, a new track within the &lt;a href=&quot;https://research.google/outreach/air-program/&quot;>;Google Award for Inclusion Research Program&lt;/a>;. These researchers are taking the Society-Centered AI methodology and helping create beneficial applications across the world. Examples of some of the projects funded include: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;AI-Driven Monitoring of Attitude Polarization in Conflict-Affected Countries for Inclusive Peace Process and Women&#39;s Empowerment:&lt;/strong>; This project&#39;s goal is to create LLM-powered tools that can be used to monitor peace in online conversations in developing nations. The initial target communities are where peace is in flux and the effort will put a particular emphasis on mitigating polarization that impacts women and promoting harmony.&lt;/li>; &lt;li>;&lt;strong>;AI-Assisted Distributed Collaborative Indoor Pollution Meters: A Case Study , Requirement Analysis, and Low-Cost Healthy Home Solution for Indian Communities: &lt;/strong>;This project is looking at the usage of low-cost pollution monitors combined with AI-assisted methodology for identifying recommendations for communities to improve air quality and at home健康。 The initial target communities are highly impacted by pollution, and the joint work with them includes the goal of developing how to measure improvement in outcomes in the local community. &lt;/li>; &lt;li>;&lt;strong>;Collaborative Development of AI Solutions for Scaling Up Adolescent Access to Sexual and Reproductive Health Education and Services in Uganda: &lt;/strong>;This project&#39;s goal is to create LLM-powered tools to provide personalized coaching and learning for users&#39; needs on topics of sexual and reproductive health education in low-income settings in Sub-Saharan Africa. The local societal need is significant, with an estimated &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9652643/&quot;>;25% rate of teenage pregnancy&lt;/a>;, and the project aims to address the needs with a collective development process for the AI solution.&lt;strong>; &lt;/strong>; &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Future direction&lt;/h2>; &lt;p>; Focusing on society&#39;s needs, working via multidisciplinary collective research, and measuring the impact on society helps lead to AI solutions that are relevant, long-lasting, empowering, and beneficial. See the &lt;a href=&quot;https://globalgoals.withgoogle.com/globalgoals/&quot;>;AI for the Global Goals&lt;/a>; to learn more about potential Society-Centered AI research problems. &lt;a href=&quot;https://blog.google/outreach-initiatives/google-org/httpsbloggoogleoutreach-initiativesgoogle-orgunited-nations-global-goals-google-ai-/&quot;>;Our efforts&lt;/a>; with non-profits in these areas is complementary to the research that we are doing and encouraging. We believe that further initiatives using Society-Centered AI will help the collective research community solve problems and positively impact society at large. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many thanks to the many individuals who have worked on these projects at Google including Shruti Sheth, Reena Jana, Amy Chung-Yu Chou, Elizabeth Adkison, Sophie Allweis, Dan Altman, Eve Andersson, Ayelet Benjamini&lt;/em>;, &lt;em>;Julie Cattiau, Yuval Carny, Richard Cave, Katherine Chou, Greg Corrado, Carlos De Segovia, Remi Denton, Dotan Emanuel, Ashley Gardner, Oren Gilon, Taylor Goddu, Brigitte Hoyer Gosselink, Jordan Green, Alon Harris&lt;/em>;, &lt;em>;Avinatan Hassidim, Rus Heywood, Sunny Jansen, Pan-Pan Jiang, Anton Kast, Marilyn Ladewig, Ronit Levavi Morad, Bob MacDonald, Alicia Martin, Shakir Mohamed, Philip Nelson, Moriah Royz, Katie Seaver, Joel Shor, Milind Tambe, Aparna Taneja, Divy Thakkar, Jimmy Tobin, Katrin Tomanek, Blake Walsh, Gal Weiss, Kasumi Widner, Lihong Xi, and teams.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6563276834599281497/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6563276834599281497&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/emerging-practices-for-society-centered.html&quot; rel=&quot;alternate&quot; title=&quot;Emerging practices for Society-Centered AI&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6928081948246813203&lt;/id>;&lt;published>;2023-11-16T13:11:00.000-08:00&lt;/published>;&lt;updated>;2023-12-07T08:16:11.190-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Generative AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Adversarial testing for generative AI safety&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Kathy Meier-Hellstern, Building Responsible AI &amp;amp; Data Systems, Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s1200/lockup_GoogleResearch_FullColor_Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; The&lt;em>; &lt;/em>;&lt;a href=&quot;https://research.google/teams/responsible-ai/&quot;>;Responsible AI and Human-Centered Technology&lt;/a>; (RAI-HCT) team within Google Research is committed to advancing the theory and practice of responsible human-centered AI through a lens of culturally-aware research, to meet the needs of billions of users today, and blaze the path forward for a better AI future. The BRAIDS (Building Responsible AI Data and Solutions) team within RAI-HCT aims to simplify the adoption of RAI practices through the utilization of scalable tools, high-quality data, streamlined processes, and novel research with a current emphasis on addressing the unique challenges posed by &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_artificial_intelligence&quot;>;generative AI&lt;/a>; (GenAI). &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; GenAI models have enabled unprecedented capabilities leading to a rapid surge of innovative applications. Google actively leverages &lt;a href=&quot;https://ai.google/discover/foundation-models/&quot;>;GenAI&lt;/a>; to &lt;a href=&quot;https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview&quot;>;enhance&lt;/a>; its &lt;a href=&quot;https://workspace.google.com/blog/product-announcements/duet-ai-in-workspace-now-available&quot;>;products&#39; utility&lt;/a>; and to improve lives. While enormously beneficial, GenAI also presents risks for disinformation, bias, and security. In 2018, Google pioneered the &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;AI Principles&lt;/a>;, emphasizing beneficial use and prevention of harm. Since then, Google has focused on effectively implementing our principles in &lt;a href=&quot;https://ai.google/responsibility/responsible-ai-practices/&quot;>;Responsible AI practices&lt;/a>; through 1) a comprehensive risk assessment framework, 2) internal governance structures, 3) education, empowering Googlers to integrate AI Principles into their work, and 4) the development of processes and tools that identify, measure, and analyze ethical risks throughout the lifecycle of AI-powered products. The BRAIDS team focuses on the last area, creating tools and techniques for identification of ethical and safety risks in GenAI products that enable teams within Google to apply appropriate mitigations. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;What makes GenAI challenging to build responsibly?&lt;/h2>; &lt;p>; The unprecedented capabilities of GenAI models have been accompanied by a new spectrum of potential failures, underscoring the urgency for a comprehensive and systematic RAI approach to understanding and mitigating potential safety concerns before the model is made broadly available. One key technique used to understand potential risks is &lt;em>;adversarial testing&lt;/em>;, which is testing performed to systematically evaluate the models to learn how they behave when provided with malicious or inadvertently harmful inputs across a range of scenarios. To that end, our research has focused on three directions: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Scaled adversarial data generation&lt;/em>;&lt;br />; Given the diverse user communities, use cases, and behaviors, it is difficult to comprehensively identify critical safety issues prior to launching a product or service. Scaled adversarial data generation with humans-in-the-loop addresses this need by creating test sets that contain a wide range of diverse and potentially unsafe model inputs that stress the model capabilities under adverse circumstances. Our unique focus in BRAIDS lies in identifying societal harms to the diverse user communities impacted by our models. &lt;/li>; &lt;li>;&lt;em>;Automated test set evaluation and community engagement&lt;/em>;&lt;br />; Scaling the testing process so that many thousands of model responses can be quickly evaluated to learn how the model responds across a wide range of potentially harmful scenarios is aided with automated test set evaluation. Beyond testing with adversarial test sets, community engagement is a key component of our approach to identify “unknown unknowns” and to seed the data generation process.&lt;/li>; &lt;li>;&lt;em>;Rater diversity&lt;/em>;&lt;br />; Safety evaluations rely on human judgment, which is shaped by community and culture and is not easily automated. To address this, we prioritize research on rater diversity.&lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Scaled adversarial data generation&lt;/h2>; &lt;p>; High-quality, comprehensive data underpins many key programs across Google. Initially reliant on manual data generation, we&#39;ve made significant strides to automate the adversarial data generation process. A centralized data repository with use-case and policy-aligned prompts is available to jump-start the generation of new adversarial tests. We have also developed multiple synthetic data generation tools based on large language models (LLMs) that prioritize the generation of data sets that reflect diverse societal contexts and that integrate data quality metrics for improved dataset quality and diversity. &lt;/p>; &lt;p>; Our data quality metrics include: &lt;/p>; &lt;ul>; &lt;li>;Analysis of language styles, including query length, query similarity, and diversity of language styles.&lt;/li>; &lt;li>;Measurement across a wide range of societal and multicultural dimensions, leveraging datasets such as &lt;a href=&quot;https://github.com/google-research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>;, &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;, the &lt;a href=&quot;https://medium.com/jigsaw/scaling-machine-learning-fairness-with-societal-context-be73d4ad38e2&quot;>;Societal Context Repository&lt;/a>;.&lt;/li>; &lt;li>;Measurement of alignment with Google&#39;s &lt;a href=&quot;https://policies.google.com/terms/generative-ai/use-policy&quot;>;generative AI policies&lt;/a>; and intended use cases.&lt;/li>; &lt;li>;Analysis of adversariality to ensure that we examine both explicit (the input is clearly designed to produce an unsafe output) and implicit (where the input is innocuous but the output is harmful) queries. &lt;/li>; &lt;/ul>; &lt;p>; One of our approaches to scaled data generation is exemplified in our paper on &lt;a href=&quot;https://arxiv.org/abs/2311.08592&quot;>;AI-Assisted Red Teaming&lt;/a>; (AART). AART generates evaluation datasets with high diversity (eg, sensitive and harmful concepts specific to a wide range of cultural and geographic regions), steered by AI-assisted recipes to define, scope and prioritize diversity within an application context. Compared to some state-of-the-art tools, AART shows promising results in terms of concept coverage and data quality. Separately, we are also working with MLCommons to contribute to &lt;a href=&quot;https://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot;>;public benchmarks for AI Safety&lt;/一个>;。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Adversarial testing and community insights &lt;/h2>; &lt;p>; Evaluating model output with adversarial test sets allows us to identify critical safety issues prior to deployment. Our initial evaluations relied exclusively on human ratings, which resulted in slow turnaround times and inconsistencies due to a lack of standardized safety definitions and policies. We have improved the quality of evaluations by introducing policy-aligned rater guidelines to improve human rater accuracy, and are researching additional improvements to better reflect the perspectives of diverse communities. Additionally, automated test set evaluation using LLM-based auto-raters enables efficiency and scaling, while allowing us to direct complex or ambiguous cases to humans for expert rating. &lt;/p>; &lt;p>; Beyond testing with adversarial test sets, gathering community insights is vital for continuously discovering “unknown unknowns”. To provide high quality human input that is required to seed the scaled processes, we partner with groups such as the &lt;a href=&quot;https://sites.google.com/corp/google.com/earr-external-research-group/home&quot;>;Equitable AI Research Round Table&lt;/a>; (EARR), and with our internal ethics and analysis teams to ensure that we are representing the diverse communities who use our models. The &lt;a href=&quot;https://dynabench.org/tasks/adversarial-nibbler&quot;>;Adversarial Nibbler Challenge&lt;/a>; engages external users to understand potential harms of &lt;a href=&quot;https://arxiv.org/abs/2305.14384&quot;>;unsafe, biased or violent outputs&lt;/a>; to end users at scale. Our continuous commitment to community engagement includes gathering feedback from diverse communities and collaborating with the research community, for example during &lt;a href=&quot;https://sites.google.com/view/art-of-safety&quot;>;The ART of Safety workshop&lt;/a>; at the &lt;a href=&quot;http://www.ijcnlp-aacl2023.org/&quot;>;Asia-Pacific Chapter of the Association for Computational Linguistics Conference&lt;/a>; (IJCNLP-AACL 2023) to address adversarial testing challenges for GenAI. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Rater diversity in safety evaluation&lt;/h2>; &lt;p>; Understanding and mitigating GenAI safety risks is both a technical and social challenge. Safety perceptions are intrinsically subjective and influenced by a wide range of intersecting factors. Our in-depth study on demographic influences on safety perceptions explored the &lt;a href=&quot;https://arxiv.org/abs/2306.11530&quot;>;intersectional effects of rater demographics&lt;/a>; (eg, race/ethnicity, gender, age) and content characteristics (eg, degree of harm) on safety assessments of GenAI outputs. Traditional approaches largely ignore inherent subjectivity and the systematic disagreements among raters, which can mask important cultural differences. Our &lt;a href=&quot;https://arxiv.org/abs/2311.05074&quot;>;disagreement analysis framework&lt;/a>; surfaced a variety of disagreement patterns between raters from diverse backgrounds including also with “ground truth” expert ratings. This paves the way to new approaches for assessing quality of human annotation and model evaluations beyond the simplistic use of gold labels. Our &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;NeurIPS 2023 publication&lt;/a>; introduces the &lt;a href=&quot;https://github.com/google-research-datasets/dices-dataset&quot;>;DICES&lt;/a>; (Diversity In Conversational AI Evaluation for Safety) dataset that facilitates nuanced safety evaluation of LLMs and accounts for variance, ambiguity, and diversity in various cultural contexts. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Summary&lt;/h2>; &lt;p>; GenAI has resulted in a technology transformation, opening possibilities for rapid development and customization even without coding. However, it also comes with a risk of generating harmful outputs. Our proactive adversarial testing program identifies and mitigates GenAI risks to ensure inclusive model behavior. Adversarial testing and red teaming are essential components of a Safety strategy, and conducting them in a comprehensive manner is essential. The rapid pace of innovation demands that we constantly challenge ourselves to find “unknown unknowns” in cooperation with our internal partners, diverse user communities, and other industry experts. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6928081948246813203/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6928081948246813203&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research_16.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Adversarial testing for generative AI safety&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghkgEei5l04Gs6hz4cLk9IqdDbXcx-41xZsRsJ-b2WHLDng1eZzJqo9eAwBZjZHc5P2akAKxT6vQLbn0-5nxy_OuPA5QIyWAV9Yy_4iIGch-zM2W-88Y-6e8HxkbG39hemnzheAx5GUvkMspUCaQiqE5RtxYvFKj-bI0pKzOhFVrWmSOgHJisYHBDfGdCl/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1338549955376716163&lt;/id>;&lt;published>;2023-11-14T12:28:00.000-08:00&lt;/published>;&lt;updated>;2023-11-14T14:09:51.250-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Multimodal Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Video Analysis&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Scaling multimodal understanding to long videos&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Isaac Noble, Software Engineer, Google Research, and Anelia Angelova, Research Scientist, Google DeepMind &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s1600/mirasol.png&quot; style=&quot;display: none;&quot; />; &lt;p>; When building machine learning models for real-life applications, we need to consider inputs from multiple modalities in order to capture various aspects of the world around us. For example, audio, video, and text all provide varied and complementary information about a visual input. However, building multimodal models is challenging due to the heterogeneity of the modalities. Some of the modalities might be well synchronized in time (eg, audio, video) but not aligned with text. Furthermore, the large volume of data in video and audio signals is much larger than that in text, so when combining them in multimodal models, video and audio often cannot be fully consumed and need to be disproportionately compressed. This problem is exacerbated for longer video inputs. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2311.05698&quot;>;Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities&lt;/a>;”, we introduce a multimodal &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; model (Mirasol3B) for learning across audio, video, and text方式。 The main idea is to decouple the multimodal modeling into separate focused autoregressive models, processing the inputs according to the characteristics of the modalities. Our model consists of an autoregressive component for the time-synchronized modalities (audio and video) and a separate autoregressive component for modalities that are not necessarily time-aligned but are still sequential, eg, text inputs, such as a title or description. Additionally, the time-aligned modalities are partitioned in time where local features can be jointly learned. In this way, audio-video inputs are modeled in time and are allocated comparatively more parameters than prior works. With this approach, we can effortlessly handle much longer videos (eg, 128-512 frames) compared to other multimodal models. At 3B parameters, Mirasol3B is compact compared to prior &lt;a href=&quot;https://arxiv.org/abs/2204.14198&quot;>;Flamingo&lt;/a>; (80B) and &lt;a href=&quot;https://arxiv.org/abs/2305.18565&quot;>;PaLI-X&lt;/a>; (55B) models. Finally, Mirasol3B outperforms the state-of-the-art approaches on &lt;a href=&quot;https://blog.research.google/2022/08/efficient-video-text-learning-with.html&quot;>;video question answering&lt;/a>; (video QA), long video QA, and audio-video-text benchmarks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s1240/image5.gif&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1240&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgshWPBWMbvzrYbGl-41QRmA5NWiQ4GKMb_nlTl-uKlY6D9TEKosZWbJk_wnllLqyq4GUQT6feI_rLH4rrYVoZHHQET750qT1pxkkiju6mWqG7ddCxLjgpywTb3rnQdDtaUTOeRvnZD0_a2mMauvs7vu6pzboeWcTCt_7bloNMDdZfNyM3Y_7UKcN-7VSqS/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The Mirasol3B architecture consists of an autoregressive model for the time-aligned modalities (audio and video), which are partitioned in chunks, and a separate autoregressive model for the unaligned context modalities (eg, text). Joint feature learning is conducted by the Combiner, which learns compact but sufficiently informative features, allowing the processing of long video/audio inputs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Coordinating time-aligned and contextual modalities&lt;/h2>; &lt;p>; Video, audio and text are diverse modalities with distinct characteristics. For example, video is a spatio-temporal visual signal with 30–100 frames per second, but due to the large volume of data, typically only 32–64 frames &lt;em>;per video&lt;/em>; are consumed by current models. Audio is a one-dimensional temporal signal obtained at much higher frequency than video (eg, at 16 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hertz&quot;>;Hz&lt;/a>;), whereas text inputs that apply to the whole video, are typically 200–300 word-sequence and serve as a context to the audio-video inputs. To that end, we propose a model consisting of an autoregressive component that fuses and jointly learns the time-aligned signals, which occur at high frequencies and are roughly synchronized, and another autoregressive component for processing non-aligned signals. Learning between the components for the time-aligned and contextual modalities is coordinated via cross-attention mechanisms that allow the two to exchange information while learning in a sequence without having to synchronize them in time. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Time-aligned autoregressive modeling of video and audio&lt;/h2>; &lt;p>; Long videos can convey rich information and activities happening in a sequence. However, present models approach video modeling by extracting all the information at once, without sufficient temporal information. To address this, we apply an autoregressive modeling strategy where we condition jointly learned video and audio representations for one time interval on feature representations from previous time intervals. This preserves temporal information. &lt;/p>; &lt;p>; The video is first partitioned into smaller video chunks. Each chunk itself can be 4–64 frames. The features corresponding to each chunk are then processed by a learning module, called the Combiner (described below), which generates a joint audio and video feature representation at the current step — this step extracts and compacts the most important information per chunk. Next, we process this joint feature representation with an autoregressive Transformer, which applies attention to the previous feature representation and generates the joint feature representation for the next step. Consequently, the model learns how to represent not only each individual chunk, but also how the chunks relate temporally. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s1259/image1.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;744&quot; data-original-width=&quot;1259&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh90Ao2yyoC5TSypNxQzA7F80lQla0f4STDrB6ZdVhINwiiQjOq1WppROFurlUn9-Lq_UUQ9uuQIeRDld-j2NMYl1e5q2Cg2L0zOHXSG0dAkpCSqVe-wEyE8QZtUup7AUazE3sBEyoO2T3RhWSc5Z7o1w0pyqMH0WzTts3vaxUnMNOa61uWXvQ2Wj92evO5/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use an autoregressive modeling of the audio and video inputs, partitioning them in time and learning joint feature representations, which are then autoregressively learned in sequence.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Modeling long videos with a modality combiner&lt;/h2>; &lt;p>; To combine the signals from the video and audio information in each video chunk, we propose a learning module called the Combiner. Video and audio signals are aligned by taking the audio inputs that correspond to a specific video timeframe. We then process video and audio inputs spatio-temporally, extracting information particularly relevant to &lt;em>;changes in the inputs&lt;/em>; (for videos we use &lt;a href=&quot;https://arxiv.org/abs/2212.03229&quot;>;sparse video tubes&lt;/a>;, and for audio we apply the &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrogram&quot;>;spectrogram&lt;/a>; representation, both of which are processed by a &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;>;Vision Transformer)&lt;/a>;. We concatenate and input these features to the Combiner, which is designed to learn a new feature representation capturing both these inputs. To address the challenge of the large volume of data in video and audio signals, another goal of the Combiner is to reduce the dimensionality of the joint video/audio inputs, which is done by selecting a smaller number of output features to be produced. The Combiner can be implemented simply as a causal Transformer, which processes the inputs in the direction of time, ie, using only inputs of the prior steps or the current one. Alternatively, the Combiner can have a learnable memory, described below. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Combiner styles&lt;/h2>; &lt;p>; A simple version of the Combiner adapts a Transformer architecture. More specifically, all audio and video features from the current chunk (and optionally prior chunks) are input to a Transformer and projected to a lower dimensionality, ie, a smaller number of features are selected as the output “combined” features. While Transformers are not typically used in this context, we find it effective for reducing the dimensionality of the input features, by selecting the last &lt;em>;m&lt;/em>; outputs of the Transformer, if &lt;em>;m&lt;/em>; is the desired output dimension (shown below). Alternatively, the Combiner can have a memory component. For example, we use the &lt;a href=&quot;https://arxiv.org/abs/2211.09119&quot;>;Token Turing Machine&lt;/a>; (TTM), which supports a differentiable memory unit, accumulating and compressing features from all previous timesteps 。 Using a fixed memory allows the model to work with a more compact set of features at every step, rather than process all the features from previous steps, which reduces computation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s1798/image8.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;1798&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0C7t17njiE70O5UrLHjfEHAw5dbAlNvYj7bMxQt_GkxNUdGtnqKyAwpWi7uvE_IGiS-50Q2Qyo4CAzq8uZbkXZ-HzNh-DCh6UNSSIwxUlWSOtihmChwFXD4F3LS73C_1fusf5fQl7TyTQv2L5ycmfSCj36GK3IrN4yaOAjODj09NBmcAMJzV0TZS_0qNI/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use a simple Transformer-based Combiner (&lt;b>;left&lt;/b>;) and a Memory Combiner (&lt;b>;right&lt;/b>;), based on the Token Turing Machine (TTM), which uses memory to compress previous history of features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We evaluate our approach on several benchmarks, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1906.02467&quot;>;ActivityNet-QA&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>;, for the video QA task, where a text-based question about a video is issued and the model needs to answer. This evaluates the ability of the model to understand both the text-based question and video content, and to form an answer, focusing on only relevant information. Of these benchmarks, the latter two target long video inputs and feature more complex questions. &lt;/p>; &lt;p>; We also evaluate our approach in the more challenging open-ended text generation setting, wherein the model generates the answers in an unconstrained fashion as free form text, requiring an exact match to the ground truth answer. While this stricter evaluation counts synonyms as incorrect, it may better reflect a model&#39;s ability to generalize. &lt;/p>; &lt;p>; Our results indicate improved performance over state-of-the-art approaches for most benchmarks, including all with open-ended generation evaluation — notable considering our model is only 3B parameters, considerably smaller than prior approaches, eg, Flamingo 80B. We used only video and text inputs to be comparable to other work. Importantly, our model can process 512 frames without needing to increase the model parameters, which is crucial for handling longer videos. Finally with the TTM Combiner, we see both better or comparable performance while reducing compute by 18%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s1200/image3.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfCp3kk_xskD3rGbE9BU7nwO3t1JtjU55NX1gqHw0LLMII8I48WB979sAhPCefH-GmGsUUxfGseTqjmMnhyphenhyphenFRP1LHlZXuAJvsHhZGdSoEblQ4WUs6BnRqjFpy-iFyqEhW3FTKpVN-_mo8h0jDWJt0EUctIHjOWPW4iaD859TaN3N3omt5ejmydnN8C0uv/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf&quot;>;MSRVTT-QA&lt;/a>; (video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s1200/image7.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;860&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFgcLlauVA3EKGZt031I4pWR3gpl4kk0jdyP_Oaia1J5pyp3t4VS8mUPG7TvPXevsu9Zs4cuVuJgA6IKiqsySkDDyvlBFiy3VkmcDRWJ-WRoZ-c7Tl-fozWXSEkznQSmJlAND23SfaA9jgPQDf645TVGpn3hsNV3tw9rHkpagwhuioiD4W1diJ8XxfavYK/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on &lt;a href=&quot;https://arxiv.org/abs/2105.08276&quot;>;NeXT-QA&lt;/a>; benchmark, which features long videos for the video QA task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Results on audio-video benchmarks&lt;/h2>; &lt;p>; Results on the popular audio-video datasets &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>; and &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; are shown below. Since these benchmarks are classification-only, we treat them as an open-ended text generative setting where our model produces the text of the desired class; eg, for the class ID corresponding to the “playing drums” activity, we expect the model to generate the text “playing drums”. In some cases our approach outperforms the prior state of the art by large margins, even though our model outputs the results in the generative open-ended setting. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s1200/image6.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilvMcRgE6UcfYVRrHeniJ4K7wlOvHHXB76h_2pJ63eNEZ-1LASKSIPsCx_hntsQPG7k619EQTpV5mgvt2EIezwqnLAbdnYOvatfMD0zq97fnW3pDuQz1TUjUiNt-b2Ka9z5z3bwPZWhnDgQFXNbYdqTzTP50qKvg99Qb6UsUee7dNZfuxOWsq-6HJjdOs6/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&quot;>;VGG-Sound&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s1200/image2.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMsUe9-4QKm4lQlvDS4BymTU47VxMvtdOIishS-QnLBV_sYWTVtp8dZATo3vyqeemFlV0uvBt7AY6yCFsd9DCMwRQO-o8HiQEPe_A4Ilb540-h5cAmymsQkgC3oW2BfPtEWi_w_N6bTGi6FFKogwe9fuJ4v2_Zid9_KcR5pnulxx7HujwkxV5cbCRdqny_/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Results on the &lt;a href=&quot;https://epic-kitchens.github.io/epic-sounds/&quot;>;EPIC-SOUNDS&lt;/a>; (audio-video QA) dataset.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Benefits of autoregressive modeling&lt;/h2>; &lt;p>; We conduct an ablation study comparing our approach to a set of baselines that use the same input information but with standard methods (ie, without autoregression and the Combiner). We also compare the effects of pre-training. Because standard methods are ill-suited for processing longer video, this experiment is conducted for 32 frames and four chunks only, across all settings for fair comparison. We see that Mirasol3B&#39;s improvements are still valid for relatively short videos. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s1200/image4.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjD4KZfUgaGluUNoSERSnuaHWMsbyx6AzHrgKrvDB1ZJxpOqNonvOzTI4hGVz4I8KZQT4aDLkQ6rvMjQIHJCY9otQIZHuSeoNK1ciQ2ALE3n1zYzQEvPdwUR_81uEDeD5U9DXDABK8ozbONAHkK0hZFMHS0j6IoumYfmjKI-M9ZHb76F2kHl-6XTPEn2eG/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Ablation experiments comparing the main components of our model. Using the Combiner, the autoregressive modeling, and pre-training all improve performance.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Conclusion &lt;/h2>; &lt;p>; We present a multimodal autoregressive model that addresses the challenges associated with the heterogeneity of multimodal data by coordinating the learning between time-aligned and time-unaligned modalities. Time-aligned modalities are further processed autoregressively in time with a Combiner, controlling the sequence length and producing powerful representations. We demonstrate that a relatively small model can successfully represent long video and effectively combine with other modalities. We outperform the state-of-the-art approaches (including some much bigger models) on video- and audio-video question answering. &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research is co-authored by AJ Piergiovanni, Isaac Noble, Dahun Kim, Michael Ryoo, Victor Gomes, and Anelia Angelova. We thank Claire Cui, Tania Bedrax-Weiss, Abhijit Ogale, Yunhsuan Sung, Ching-Chung Chang, Marvin Ritter, Kristina Toutanova, Ming-Wei Chang, Ashish Thapliyal, Xiyang Luo, Weicheng Kuo, Aren Jansen, Bryan Seybold, Ibrahim Alabdulmohsin, Jialin Wu, Luke Friedman, Trevor Walker, Keerthana Gopalakrishnan, Jason Baldridge, Radu Soricut, Mojtaba Seyedhosseini, Alexander D&#39;Amour, Oliver Wang, Paul Natsev, Tom Duerig, Younghui Wu, Slav Petrov, Zoubin Ghahramani for their help and support. We also thank Tom Small for preparing the animation. &lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1338549955376716163/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1338549955376716163&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/scaling-multimodal-understanding-to.html&quot; rel=&quot;alternate&quot; title=&quot;Scaling multimodal understanding to long videos&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhS9q_iPaPNyYexT4zpSTnERwHJJGcmOemvRjjqD9sNPMwibC-iV5AU2P4J9VrPE_NGFyFz5QLxHpCti9Y-bOPEi9XPCev5YwIpNKPMECDxk1prmksf99tPHgf1uQb7EW3zSmnIMWIFwAjvM7GldhsEtW7eogo1sG1L1nxD8UPIi0fpHR_Iwp8fJTdoUnQB/s72-c/mirasol.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5546775652504697591&lt;/id>;&lt;published>;2023-11-10T10:05:00.000-08:00&lt;/published>;&lt;updated>;2023-11-10T10:05:13.301-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;open source&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Enabling large-scale health studies for the research community&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Chintan Ghate, Software Engineer, and Diana Mincu, Research Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s1100/MSSignals-Hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; As consumer technologies like &lt;a href=&quot;https://cloud.google.com/blog/topics/healthcare-life-sciences/google-cloud-fitbit-haga-collaborate-on-pilot-heart-study&quot;>;fitness trackers&lt;/a>; and &lt;a href=&quot;https://blog.google/products/pixel/health-ai-better-sleep/&quot;>;mobile phones&lt;/a>; become more widely used for health-related data collection, so does the opportunity to leverage these data pathways to study and advance our understanding of medical conditions. We have &lt;a href=&quot;https://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot;>;previously&lt;/a>; touched upon how our work explores the use of this technology within the context of chronic diseases, in particular multiple sclerosis (MS). This effort leverages the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, an open-source platform used to create clinical study apps, that makes it easier for anyone to run their own studies and collect good quality healthcare data, in a trusted and safe way. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we describe the setup that we developed by expanding the FDA MyStudies platform and demonstrate how it can be used to set up a digital health study. We also present our exploratory research study created through this platform, called MS Signals, which consists of a symptom tracking app for MS patients. The goal for this app is twofold: 1) to ensure that the enhancements to the FDA MyStudies platform made for a more streamlined study creation experience; and 2) to understand how new data collection mechanisms can be used to revolutionize patients&#39; chronic disease management and tracking. We have &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; our extension to the FDA MyStudies platform under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;>;Apache 2.0 license&lt;/a>; to provide a resource for the community to build their own studies. &lt;/p>; &lt;br />; &lt;h2>;Extending the FDA MyStudies platform&lt;/h2>; &lt;p>; The original FDA MyStudies platform allowed people to configure their own study apps, manage participants, and create separate iOS and Android apps. To simplify the study creation process and ensure increased study engagement, we made a number of accessibility changes. Some of the main improvements include: cross-platform (iOS and Android) app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>;, an open source framework by Google for building multi-platform applications from a single codebase; a simplified setup, so that users can prototype their study quickly (under a day in most cases); and, most importantly, an emphasis on accessibility so that diverse patient&#39;s voices are heard. The accessibility enhancements include changes to the underlying features of the platform and to the particular study design of the MS Signals study app. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Multi-platform support with rapid prototyping&lt;/h3>; &lt;p>; We decided on the use of Flutter as it would be a single point that would generate both iOS and Android apps in one go, reducing the work required to support multiple platforms. Flutter also provides &lt;a href=&quot;https://docs.flutter.dev/tools/hot-reload&quot;>;hot-reloading&lt;/a>;, which allows developers to build &amp;amp; preview features quickly. The design-system in the app takes advantage of this feature to provide a central point from which the branding &amp;amp; theme of the app can be changed to match the tone of a new study and previewed instantly. The demo environment in the app also utilizes this feature to allow developers to mock and preview questionnaires locally on their machines. In our experience this has been a huge time-saver in A/B testing the UX and the format and wording of questions live with clinicians. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;System accessibility enhancements&lt;/h3>; &lt;p>; To improve the accessibility of the platform for more users, we made several usability enhancements: &lt;/p>; &lt;ol>; &lt;li>;Light &amp;amp; dark theme support&lt;/li>; &lt;li>;Bold text &amp;amp; variable font-sizes&lt;/li>; &lt;li>;High-contrast mode&lt;/li>; &lt;li>;Improving user awareness of accessibility settings&lt;/li>; &lt;/ol>; &lt;p>; Extended exposure to bright light themes can strain the eyes, so supporting dark theme features was necessary to make it easier to use the study app frequently. Some small or light text-elements are illegible to users with vision impairments, so we added 1) bold-text and support for larger font-sizes and 2) high-contrast color-schemes. To ensure that accessibility settings are easy to find, we placed an introductory one-time screen that was presented during the app&#39;s first launch, which would directly take users to their system accessibility settings. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Study accessibility enhancements&lt;/h3>; &lt;p>; To make the study itself easier to interact with and reduce cognitive overload, we made the following changes: &lt;/p>; &lt;ol>; &lt;li>;Clarified the onboarding process&lt;/li>; &lt;li>;Improved design for questionnaires&lt;/li>; &lt;/ol>; &lt;p>; First, we clarified the on-boarding process by presenting users with a list of required steps when they first open the app in order to reduce confusion and participant drop-off. &lt;/p>; &lt;p>; The original questionnaire design in the app presented each question in a card format, which utilizes part of the screen for shadows and depth effects of the card. In many situations, this is a pleasant aesthetic, but in apps where accessibility is priority, these visual elements restrict the space available on the screen. Thus, when more accessible, larger font-sizes are used there are more frequent word breaks, which reduces readability. We fixed this simply by removing the card design elements and instead using the entire screen, allowing for better visuals with larger font-sizes. &lt;/p>; &lt;br />; &lt;h2>;The MS Signals prototype study&lt;/h2>; &lt;p>; To test the usability of these changes, we used our redesigned platform to create a prototype study app called MS Signals, which uses surveys to gather information about a participant&#39;s MS-related symptoms. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/s1760/MSSignals.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1728&quot; data-original-width=&quot;1760&quot; height=&quot;628&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikbf3GdEuJptK8hRicFp-sjYhfRZfYc3XyqEf7A3FDJZ_XTiDOBeMQXUYAtO5ZNoP7eI9GJRJS6zJfK9abPIFJCtOEYIOjBphF7qDqSCObwC7YpQ2BeLNTYtKFKyYGzM-h6wPrcyLA4QqLttVAsMd_B2lXxE47OxfBZJ4RgqLW7IfUqT3xlzcOKh2w_hzB/w640-h628/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1971&quot; data-original-width=&quot;1999&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCBgKDha7HDQNbbG-dPx4AEoZfWd1SMYdzdZglFFkE_LbHA-_-rzwU1o1VbOvHE5aLMngCXs5AkCWp4jez0glZ1s7HFzK0deFGodiUWlj5xXhWQYGLEXOk94h2NlaSwjizkaY6jJgZfnDlngu69r4O01ifsiLs5KfOd48G7wSSjvL5AYWbN9oiB4d79k7/w640-h632/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;MS Studies app design&lt;/h3>; &lt;p>; As a first step, before entering any study information, participants are asked to complete an eligibility and study comprehension questionnaire to ensure that they have read through the potentially lengthy terms of study participation. This might include, for example, questions like &quot;In what country is the study available?&quot; or “Can you withdraw from the study?&quot; A section like this is common in most health studies, and it tends to be the first drop-off point for participants. &lt;/p>; &lt;p>; To minimize study drop-off at this early stage, we kept the eligibility test brief and reflected correct answers for the comprehension test back to the participants. This helps minimize the number of times a user may need to go through the initial eligibility questionnaire and ensures that the important aspects of the study protocol are made clear to them. &lt;/p>; &lt;p>; After successful enrollment, participants are taken to the main app view, which consists of three pages: &lt;/p>; &lt;ul>; &lt;li>;&lt;strong>;Activities:&lt;/strong>; &lt;br />; This page lists the questionnaires available to the participant and is where the majority of their time is spent. The questionnaires vary in frequency — some are one-time surveys created to gather medical history, while others are repeated daily, weekly or monthly, depending on the symptom or area they are exploring. For the one-time survey we provide a counter above each question to signal to users how far they have come and how many questions are left, similar to the questionnaire during the eligibility and comprehension step. &lt;/li>; &lt;li>;&lt;strong>;Dashboard:&lt;/strong>; &lt;br />; To ensure that participants get something back in return for the information they enter during a study, the Dashboard area presents a summary of their responses in graph or pie chart form. Participants could potentially show this data to their care provider as a summary of their condition over the last 6 months, an improvement over the traditional pen and paper methods that many employ today. &lt;/li>; &lt;li>;&lt;strong>;Resources:&lt;/strong>; &lt;br />; A set of useful links, help articles and common questions related to MS. &lt;/li>; &lt;/ul>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Questionnaire design&lt;/h3>; &lt;p>; Since needing to frequently input data can lead to cognitive overload, participant drop off, and bad data quality, we reduced the burden in two ways: &lt;/p>; &lt;ol>; &lt;li>;We break down large questionnaires into smaller ones, resulting in 6 daily surveys, containing 3–5 questions each, where each question is multiple choice and related to a single symptom. This way we cover a total of 20 major symptoms, and present them in a similar way to how a clinician would ask these questions in an in-clinic setting.&lt;/li>; &lt;li>;We ensure previously entered information is readily available in the app, along with the time of the entry.&lt;/li>; &lt;/ol>; &lt;p>; In designing the survey content, we collaborated closely with experienced clinicians and researchers to finalize the wording and layout. While studies in this field typically use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Likert_scale&quot;>;Likert scale&lt;/a>; to gather symptom information, we defined a more intuitive verbose scale to provide better experience for participants tracking their disease and the clinicians or researchers viewing the disease history. For example, in the case of vision issues, rather than asking participants to rate their symptoms on a scale from 1 to 10, we instead present a multiple choice question where we detail common vision problems that they may be experiencing. &lt;/p>; &lt;p>; This verbose scale helps patients track their symptoms more accurately by including context that helps them more clearly define their symptoms. This approach also allows researchers to answer questions that go beyond symptom correlation. For example, for vision issues, data collected using the verbose scale would reveal to researchers whether &lt;a href=&quot;https://www.webmd.com/eye-health/nystagmus&quot;>;nystagmus&lt;/a>; is more prominent in patients with MS compared to double vision. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/s1780/MSSignals-2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;1780&quot; height=&quot;630&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQHtaxn1OL9zKbNnPfUkfEmXV8zb2dR7HJaTezZrNyhN8D_V35gfHaLtQNjoBLBxIE5lP9eiB-mxXQ7FqxbSB1d3tgTOB7fA7o6boudBmPzfiQ8CuKc117fBIFLuzem5Lo8938LqpQkxofoAL5bnpMD3hI4vOJNzHbbuB8b5RqoIP_zcD0dpr7IL9w3ysa/w640-h630/MSSignals-2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side-by-side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!--&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1901&quot; data-original-width=&quot;1999&quot; height=&quot;608&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjei2lEyI2z_brlZOM7JFHMQ6B0HniXXoIP2dMU61PBDA2f79eRYpMSPXuButbrt6epHhNNLdxcRa6AtIdPl9PF6mE4QFdFbMEJTNxEOhyphenhyphenERmIniUNLDaTH5BMNBOOv7kOTQpsNjYqEsGxbXnftHlUtbznCm377vz6nDJyC_mLSJTH_LVGU91JgDqGeCr6r/w640-h608/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Side by side comparison with a Likert scale on the left, and a Verbose scale on the right.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;-->; &lt;h2>;Focusing on accessibility&lt;/h2>; &lt;p>; Mobile-based studies can often present additional challenges for participants with chronic conditions: the text can be hard to read, the color contrast could make it difficult to see certain bits of information, or it may be challenging to scroll through pages. This may result in participant drop off, which, in turn, could yield a biased dataset if the people who are experiencing more advanced forms of a disease are unable to provide data. &lt;/p>; &lt;p>; In order to prevent such issues, we include the following accessibility features: &lt;/p>; &lt;ul>; &lt;li>;Throughout, we employ color blind accessible color schemes. This includes improving the contrast between crucial text and important additional information, which might otherwise be presented in a smaller font and a faded text color.&lt;/li>; &lt;li>;We reduced the amount of movement required to access crucial controls by placing all buttons close to the bottom of the page and ensuring that pop-ups are controllable from the bottom part of the screen.&lt;/li>; &lt;/ul>; &lt;p>; To test the accessibility of MS Signals, we collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study. For this, a call for participation was sent out by the Society to their members, and 9 respondents were asked to test out the various app flows. The majority indicated that they would like a better way than their current method to track their symptom data, that they considered MS Signals to be a unique and valuable tool that would enhance the accuracy of their symptom tracking, and that they would want to share the dashboard view with their healthcare providers. &lt;/p>; &lt;br />; &lt;h2>;Next steps&lt;/h2>; &lt;p>; We want to encourage everyone to make use of the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open source&lt;/a>; platform to start setting up and running their own studies. We are working on creating a set of standard study templates, which would incorporate what we learned from above, and we hope to release those soon. For any issues, comments or questions please check out our &lt;a href=&quot;https://goo.gle/ms-signals&quot;>;resource page&lt;/a>;. &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5546775652504697591/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5546775652504697591&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/enabling-large-scale-health-studies-for.html&quot; rel=&quot;alternate&quot; title=&quot;Enabling large-scale health studies for the research community&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDeCRv8m_GTaIXN2BwUWSYHbbj6g4jpo6TSJB6UYQsO-LLCbn6WeWceSXR01Ng1mFcHOifnZgb_Mn4IPoxy_5y1s8m3RUp_08hodTOz87SoQUh2wFjh7KhfKZbxyylB0c1iZfQVCQOn-laEBhjd96wTYlibS03ejrTEovnaYrWpBhb-A7RYtaLfcfJ9sKX/s72-c/MSSignals-Hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7987071997778787277&lt;/id>;&lt;published>;2023-11-09T14:23:00.000-08:00&lt;/published>;&lt;updated>;2023-11-09T14:23:38.431-08:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Africa&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;RAI-HCT Highlights&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Responsible AI at Google Research: Context in AI Research (CAIR)&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Katherine Heller, Research Scientist, Google Research, on behalf of the CAIR Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s1100/CAIR-hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Artificial intelligence (AI) and related machine learning (ML) technologies are increasingly influential in the world around us, making it imperative that we consider the potential impacts on society and individuals in all aspects of the technology that we create. To these ends, the Context in AI Research (CAIR) team develops novel AI methods in the context of the entire AI pipeline:&lt;strong>; &lt;/strong>;from data to end-user feedback. The pipeline for building an AI system typically starts with &lt;em>;data&lt;/em>; collection, followed by designing a &lt;em>;model&lt;/em>; to run on that data, &lt;em>;deployment&lt;/em>; of the model in the real world, and lastly, compiling and incorporation of &lt;em>;human feedback&lt;/em>;. Originating in the health space, and now expanded to additional areas, the work of the CAIR team impacts every aspect of this pipeline. While specializing in model building, we have a particular focus on building systems with responsibility in mind, including fairness, robustness, transparency, and inclusion. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/s1050/CAIR.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;383&quot; data-original-width=&quot;1050&quot; height=&quot;234&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidxBmC-Vmh8A8AdqFMHkEIqz5pcp85_vX6uPteEiaF00boHGuUo3M45rtunHL58dOillPI_7Vn3BwxavTGbmPpWcUQorJsjY6x5gh8agM9jbPio8c29iFvYUgVhO1BkEsU5eg133JKfLkcQHN3FteCyeorkzS79e0u7TDig_xCv_B_36UYLfK7gx2pgtQK/w640-h234/CAIR.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Data &lt;/h2>; &lt;p>; The CAIR team focuses on understanding the data on which ML systems are built. Improving the standards for the transparency of ML datasets is instrumental in our work. First, we employ documentation frameworks to elucidate dataset and model characteristics as guidance in the development of data and model documentation techniques — &lt;a href=&quot;https://arxiv.org/abs/1803.09010&quot;>;Datasheets for Datasets&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1810.03993&quot;>;Model Cards for Model Reporting&lt;/a>;. &lt;/p>; &lt;p>; For example, health datasets are highly sensitive and yet can have high impact. For this reason, we developed &lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533239&quot;>;Healthsheets&lt;/a>;, a health-contextualized adaptation of a Datasheet. Our motivation for developing a health-specific sheet lies in the limitations of existing regulatory frameworks for AI and health. &lt;a href=&quot;https://www.nejm.org/doi/10.1056/NEJMp1816373&quot;>;Recent research&lt;/a>; suggests that data privacy regulation and standards (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;>;HIPAA&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;>;GDPR&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act&quot;>;California Consumer Privacy Act&lt;/a>;) do not ensure ethical collection, documentation, and use of data. Healthsheets aim to fill this gap in ethical dataset analysis. The development of Healthsheets was done in collaboration with many stakeholders in relevant job roles, including clinical, legal and regulatory, bioethics, privacy, and product. &lt;/p>; &lt;p>; Further, we studied how Datasheets and Healthsheets could serve as diagnostic tools that surface the limitations and strengths of datasets. Our aim was to start a conversation in the community and tailor Healthsheets to dynamic healthcare scenarios over time. &lt;/p>; &lt;p>; To facilitate this effort, we joined the &lt;a href=&quot;http://www.datadiversity.org/&quot;>;STANDING Together&lt;/a>; initiative, a consortium that aims to develop &lt;a href=&quot;https://www.nature.com/articles/s41591-022-01987-w&quot;>;international, consensus-based standards for documentation of diversity and representation&lt;/a>; within health datasets and to provide guidance on how to mitigate risk of bias translating to harm and health inequalities. Being part of this international, interdisciplinary partnership that spans academic, clinical, regulatory, policy, industry, patient, and charitable organizations worldwide enables us to engage in the conversation about responsibility in AI for healthcare internationally. Over 250 stakeholders from across 32 countries have contributed to refining the standards&lt;strong>;.&lt;/strong>; &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s1050/Healthsheets.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;731&quot; data-original-width=&quot;1050&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUogIg5rJu-5i259KxpG9DrCD7an9L1KTpugqjw__6LWGUIF-78hkPUpjbUWc8SAZ7BZk82RlI7ddaW3Fe9spfn-hbyNLk94LAFWb3XvynnbLJddwxv8sSd0swacF5_C6UV2NjM0FXzLWKiYDnW3F_SjyOvUVp0BO2YADXBqbabbUP5D7X1i5czhqfrOcw/s16000/Healthsheets.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Healthsheets and STANDING Together: towards health data documentation and standards.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Model&lt;/h2>; &lt;p>; When ML systems are deployed in the real world, they may fail to behave in expected ways, making poor predictions in new contexts. Such failures can occur for a myriad of reasons and can carry negative consequences, especially within the context of healthcare. Our work aims to identify situations where unexpected model behavior may be discovered, before it becomes a substantial problem, and to mitigate the unexpected and undesired consequences. &lt;/p>; &lt;p>; Much of the CAIR team&#39;s modeling work focuses on identifying and mitigating when models are &lt;a href=&quot;https://blog.research.google/2021/10/how-underspecification-presents.html&quot;>;underspecified&lt;/a>;. We show that models that perform well on held-out data drawn from a training domain are not equally robust or fair under distribution shift because the models vary in the extent to which they rely on spurious correlations. This poses a risk to users and practitioners because it can be difficult to anticipate model instability using standard model evaluation practices. &lt;a href=&quot;https://www.jmlr.org/papers/v23/20-1335.html&quot;>;We have demonstrated&lt;/a>; that this concern arises in several domains, including computer vision, natural language processing, medical imaging, and prediction from electronic health records. &lt;/p>; &lt;p>; We have also shown how to use knowledge of causal mechanisms to diagnose and mitigate fairness and robustness issues in new contexts. Knowledge of causal structure allows practitioners to anticipate &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/hash/7a969c30dc7e74d4e891c8ffb217cf79-Abstract-Conference.html&quot;>;the generalizability of fairness properties under distribution shift in real-world medical settings&lt;/a>;. Further, investigating the capability for specific causal pathways, or “shortcuts”, to introduce bias in ML systems, we demonstrate how to identify cases where &lt;a href=&quot;https://www.nature.com/articles/s41467-023-39902-7&quot;>;shortcut learning&lt;/a>; leads to predictions in ML systems that are unintentionally dependent on sensitive attributes (eg, age, sex, race). We have shown how to use causal &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;>;directed acyclic graphs&lt;/a>; to &lt;a href=&quot;https://proceedings.mlr.press/v206/alabdulmohsin23a&quot;>;adapt ML systems to changing environments&lt;/a>; under complex forms of distribution shift. Our team is currently investigating how a causal interpretation of different forms of bias, including &lt;a href=&quot;https://en.wikipedia.org/wiki/Selection_bias&quot;>;selection bias&lt;/a>;, label bias, and measurement error, &lt;a href=&quot;https://nips.cc/virtual/2022/58452&quot;>;motivates the design of techniques to mitigate bias during model development and evaluation&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s727/image6.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;442&quot; data-original-width=&quot;727&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjq4THavzli3C1qzxuAwBNk72Olybnrj4UdZeMNDna14jN8eiiq9vScEJ18bYFVUEjO4l70CYIVYQ3RVt7AvpCPPlIJNGteYIsWnEeamRV4XVibAzb_fktVXjfcXUBMjzkNsyivBNDJsqRhcM_AdsGbUOrOpoYnj_iCFF-CG_0aa16GHxc58XweM07XnWYW/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s1078/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;382&quot; data-original-width=&quot;1078&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_HHXvW2hIuenXNIkORatVFSgqfaqBJbbsqFqlWCYTdfOR9RwTVqhTxGqqSPBwUhy24wYAlKn9j-vpDCths0heIL7WQk5xVxkj1OHzJpZrzZVebGB6wAP-WOn7NImE6drZjMiSchFM5JdYJulLZULSHuVEjX10-wnhOANX9BsRefZiL0v0vH4E2lU1eW2n/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Shortcut Learning: For some models, age may act as a shortcut in classification when using medical images.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team focuses on developing methodology to build more inclusive models broadly. For example, we also have &lt;a href=&quot;https://arxiv.org/abs/2302.03874&quot;>;work on the design of participatory systems&lt;/a>;, which allows individuals to choose whether to disclose sensitive attributes, such as race, when an ML system makes predictions. We hope that our methodological research positively impacts the societal understanding of inclusivity in AI method development. &lt;/p>; &lt;br />; &lt;h2>;Deployment &lt;/h2>; &lt;p>; The CAIR team aims to build technology that improves the lives of all people through the use of mobile device technology. We aim to reduce suffering from health conditions, address systemic inequality, and enable transparent device-based data collection. As consumer technology, such as fitness trackers and mobile phones, become central in data collection for health, we explored the use of these technologies within the context of chronic disease, in particular, for &lt;a href=&quot;https://en.wikipedia.org/wiki/Multiple_sclerosis&quot;>;multiple sclerosis&lt;/a>; (MS). We developed new data collection mechanisms and predictions that we hope will eventually revolutionize patient&#39;s chronic disease management, clinical trials, medical reversals and drug development. &lt;/p>; &lt;p>; First, we extended the open-source &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies&quot;>;FDA MyStudies platform&lt;/a>;, which is used to create clinical study apps, to make it easier for anyone to run their own studies and collect good quality data, in a trusted and safe way. Our improvements include zero-config setups, so that researchers can prototype their study in a day, cross-platform app generation through the use of &lt;a href=&quot;https://flutter.dev/&quot;>;Flutter&lt;/a>; and, most importantly, an emphasis on accessibility so that all patient&#39;s voices are heard. We are excited to announce this work has now been &lt;a href=&quot;https://github.com/GoogleCloudPlatform/fda-mystudies-flutter&quot;>;open sourced&lt;/a>; as an extension to the original FDA-Mystudies platform. You can start setting up your own studies today! &lt;/p>; &lt;p>; To test this platform, we built a prototype app, which we call MS Signals, that uses surveys to interface with patients in a novel consumer setting. We collaborated with the &lt;a href=&quot;https://www.nationalmssociety.org/&quot;>;National MS Society&lt;/a>; to recruit participants for a user experience study for the app, with the goal of reducing dropout rates and improving the platform further. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/s915/MSSignals.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;904&quot; data-original-width=&quot;915&quot; height=&quot;632&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmHAb0ppbGGoplKXFyrazbLPjXWf9FKDKwOh0yfiuiTkq_kVQsme9ckeS6mnSXWkfitNJKmtMvnUm0b39xr7cvGHs_oZx9mEYAf7wlwdghFSbVqvmV8MDw4TVLontCm-zT6KhUf0kEsQ3RoroWJWv0D2z29P4_vcaby4Udx6ENdaCXx9kep73iQ5HoufCq/w640-h632/MSSignals.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;MS Signals app screenshots.&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;Study welcome screen.&amp;nbsp;&lt;strong>;Right:&lt;/strong>;&amp;nbsp;Questionnaire.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Once data is collected, researchers could potentially use it to drive the frontier of ML research in MS. In a separate study, we established a research collaboration with the &lt;a href=&quot;https://neurology.duke.edu/&quot;>;Duke Department of Neurology&lt;/a>; and demonstrated that ML models can &lt;a href=&quot;https://www.researchsquare.com/article/rs-2547289/v1&quot;>;accurately predict the incidence of high-severity symptoms&lt;/a>; within three months using continuously collected data from mobile apps. Results suggest that the trained models can be used by clinicians to evaluate the symptom trajectory of MS participants, which may inform decision making for administering interventions. &lt;/p>; &lt;p>; The CAIR team has been involved in the deployment of many other systems, for both internal and external use. For example, we have also partnered with &lt;a href=&quot;https://learningally.org/&quot;>;Learning Ally&lt;/a>; to &lt;a href=&quot;https://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html&quot;>;build a book recommendation system&lt;/a>; for children with learning disabilities, such as dyslexia. We hope that our work positively impacts future product development. &lt;/p>; &lt;br />; &lt;h2>;Human feedback &lt;/h2>; &lt;p>; As ML models become ubiquitous throughout the developed world, it can be far too easy to leave voices in less developed countries behind. A priority of the CAIR team is to bridge this gap, develop deep relationships with communities, and work together to address ML-related concerns through community-driven approaches. &lt;/p>; &lt;p>; One of the ways we are doing this is through working with grassroots organizations for ML, such as &lt;a href=&quot;https://www.sisonkebiotik.africa/home&quot;>;Sisonkebiotik&lt;/a>;, an open and inclusive community of researchers, practitioners and enthusiasts at the intersection of ML and healthcare working together to build capacity and drive forward research initiatives in Africa. We worked in collaboration with the Sisonkebiotik community to detail limitations of historical top-down approaches for global health, and suggested complementary health-based methods, specifically those of grassroots participatory communities (GPCs). We jointly created a &lt;a href=&quot;https://openreview.net/forum?id=jHY_G91R880&quot;>;framework for ML and global health&lt;/a>;, laying out a practical roadmap towards setting up, growing and maintaining GPCs, based on common values across various GPCs such as &lt;a href=&quot;https://www.masakhane.io/&quot;>;Masakhane&lt;/a>;, Sisonkebiotik and &lt;a href=&quot;https://ro-ya-cv4africa.github.io/homepage/&quot;>;Ro&#39;ya&lt;/a>;. &lt;/p>; &lt;p>; We are engaging with open initiatives to better understand the role, perceptions and use cases of AI for health in non-western countries through human feedback, with an initial focus in Africa. Together with &lt;a href=&quot;https://ghananlp.org/&quot;>;Ghana NLP&lt;/a>;, we have worked to detail the need to &lt;a href=&quot;https://arxiv.org/abs/2304.02190&quot;>;better understand algorithmic fairness and bias in health in non-western contexts&lt;/a>;. We recently launched a study to expand on this work using human feedback. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s927/MLPipelineBiases.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;927&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuoLebMzarwMci1s0ro3vlHuvp5qSTs3DXz6tA4KgCdMHyaNW77Zviz2QWtIW-zVo2GStM53cBiuRqpFTEANJPJE7TLyEHfA_LAWxlsqZDaokH213r1U4zjr3M4u-YP-Dx0ndt_lCmJul6QPAboMIfFLkGl-7z95ulWktZeQnZBmU-vF-2CWV2DD2Q9ymy/s16000/MLPipelineBiases.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Biases along the ML pipeline and their associations with African-contextualized axes of disparities.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; The CAIR team is committed to creating opportunities to hear more perspectives in AI development. We partnered with Sisonkebiotik to co-organize the &lt;a href=&quot;https://www.sisonkebiotik.africa/events/workshops/dl-indaba-2023&quot;>;Data Science for Health Workshop&lt;/a>; at &lt;a href=&quot;https://deeplearningindaba.com/2023/&quot;>;Deep Learning Indaba 2023&lt;/a>; in Ghana. Everyone&#39;s voice is crucial to developing a better future using AI technology. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Negar Rostamzadeh, Stephen Pfohl, Subhrajit Roy, Diana Mincu, Chintan Ghate, Mercy Asiedu, Emily Salkey, Alexander D&#39;Amour, Jessica Schrouff, Chirag Nagpal, Eltayeb Ahmed, Lev Proleev, Natalie Harris, Mohammad Havaei, Ben Hutchinson, Andrew Smart, Awa Dieng, Mahima Pushkarna, Sanmi Koyejo, Kerrie Kauer, Do Hee Park, Lee Hartsell, Jennifer Graves, Berk Ustun, Hailey Joren, Timnit Gebru and Margaret Mitchell for their contributions and influence, as well as our many friends and collaborators at Learning Ally, National MS Society, Duke University Hospital, STANDING Together, Sisonkebiotik, and Masakhane.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7987071997778787277/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7987071997778787277&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/11/responsible-ai-at-google-research.html&quot; rel=&quot;alternate&quot; title=&quot;Responsible AI at Google Research: Context in AI Research (CAIR)&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjN3mIK7184k0E4B4Pddafelrcf4yEqd1wTOxyK5LZlxKL8dFWwbEyATjS0Zbj8HBdAnIXQ40fVedg4O5oUi5_fVvOvKRQWhgIX05olZkb9YakVdRLXus3b9Pzze5oHu32X0VUpoykEvGwbZk9W2lEIJ8jUMlgzyML0yFFsyBbpbAUZgBk6TSli7bRaNQRs/s72-c/CAIR-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;