<feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:opensearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>标签：blogger.com，1999：blog-8474926331452026626</id><updated> 2023-10-28T06:19:17.082-07:00 </updated><category term="Machine Learning"></category><category term="Deep Learning"></category><category term="Computer Vision"></category><category term="Natural Language Processing"></category><category term="Google Brain"></category><category term="open source"></category><category term="Research"></category><category term="Publications"></category><category term="Machine Perception"></category><category term="TensorFlow"></category><category term="conference"></category><category term="Natural Language Understanding"></category><category term="conferences"></category><category term="Education"></category><category term="datasets"></category><category term="Neural Networks"></category><category term="Reinforcement Learning"></category><category term="University Relations"></category><category term="Robotics"></category><category term="Health"></category><category term="AI"></category><category term="CVPR"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Multimodal Learning"></category><category term="Quantum Computing"></category><category term="Speech"></category><category term="Computational Photography"></category><category term="Research Awards"></category><category term="Machine Intelligence"></category><category term="On-device Learning"></category><category term="Computer Science"></category><category term="Security and Privacy"></category><category term="AI for Social Good"></category><category term="HCI"></category><category term="MOOC"></category><category term="ICLR"></category><category term="Machine Translation"></category><category term="Image Classification"></category><category term="Pixel"></category><category term="Quantum AI"></category><category term="Self-Supervised Learning"></category><category term="Visualization"></category><category term="YouTube"></category><category term="accessibility"></category><category term="optimization"></category><category term="AutoML"></category><category term="Hardware"></category><category term="ACL"></category><category term="Audio"></category><category term="NeurIPS"></category><category term="Android"></category><category term="ICML"></category><category term="TPU"></category><category term="Awards"></category><category term="EMNLP"></category><category term="ML"></category><category term="Search"></category><category term="Structured Data"></category><category term="Image Processing"></category><category term="Information Retrieval"></category><category term="ML Fairness"></category><category term="Physics"></category><category term="Responsible AI"></category><category term="TTS"></category><category term="User Experience"></category><category term="distributed systems"></category><category term="video"></category><category term="Automatic Speech Recognition"></category><category term="Collaboration"></category><category term="Google Accelerated Science"></category><category term="Google Maps"></category><category term="Graph Mining"></category><category term="Speech Recognition"></category><category term="Supervised Learning"></category><category term="Video Analysis"></category><category term="DeepMind"></category><category term="Environment"></category><category term="Google Translate"></category><category term="2022 Year-in-Review"></category><category term="ACM"></category><category term="Chemistry"></category><category term="Earth Engine"></category><category term="K-12"></category><category term="Vision Research"></category><category term="statistics"></category><category term="Acoustic Modeling"></category><category term="Diversity"></category><category term="Google Genomics"></category><category term="Interspeech"></category><category term="Systems"></category><category term="UI"></category><category term="Voice Search"></category><category term="data science"></category><category term="grants"></category><category term="ph.d. fellowship"></category><category term="Cloud Computing"></category><category term="Compression"></category><category term="Google Cloud Platform"></category><category term="ICCV"></category><category term="Machine Hearing"></category><category term="NIPS"></category><category term="Semi-supervised Learning"></category><category term="Software"></category><category term="Unsupervised Learning"></category><category term="market algorithms"></category><category term="Augmented Reality"></category><category term="Faculty Summit"></category><category term="RAI-HCT Highlights"></category><category term="Recommender Systems"></category><category term="Semantic Models"></category><category term="Translate"></category><category term="crowd-sourcing"></category><category term="Art"></category><category term="Biology"></category><category term="Course Builder"></category><category term="Data Discovery"></category><category term="Google Photos"></category><category term="Google+"></category><category term="PhD Fellowship"></category><category term="Social Networks"></category><category term="WWW"></category><category term="renewable energy"></category><category term="schema.org"></category><category term="Computational Imaging"></category><category term="Europe"></category><category term="Expander"></category><category term="Fusion Tables"></category><category term="Google Books"></category><category term="Moore's Law"></category><category term="Ngram"></category><category term="Optical Character Recognition"></category><category term="Year in Review"></category><category term="ads"></category><category term="API"></category><category term="App Engine"></category><category term="Gmail"></category><category term="Google Play Apps"></category><category term="Graph"></category><category term="High Dynamic Range Imaging"></category><category term="Image Annotation"></category><category term="India"></category><category term="Internet of Things"></category><category term="Kaggle"></category><category term="NAACL"></category><category term="Networks"></category><category term="Virtual Reality"></category><category term="economics"></category><category term="internationalization"></category><category term="publication"></category><category term="resource optimization"></category><category term="search ads"></category><category term="wikipedia"></category><category term="Adaptive Data Analysis"></category><category term="Africa"></category><category term="Android Wear"></category><category term="App Inventor"></category><category term="China"></category><category term="DeepDream"></category><category term="Differential Privacy"></category><category term="EMEA"></category><category term="Exacycle"></category><category term="Gboard"></category><category term="Google Docs"></category><category term="Google Drive"></category><category term="Google Science Fair"></category><category term="Google Sheets"></category><category term="Inbox"></category><category term="KDD"></category><category term="Keyboard Input"></category><category term="Labs"></category><category term="Low-Light Photography"></category><category term="MapReduce"></category><category term="Policy"></category><category term="Proposals"></category><category term="Style Transfer"></category><category term="TensorBoard"></category><category term="VLDB"></category><category term="electronics"></category><category term="osdi"></category><category term="patents"></category><category term="trends"></category><category term="April Fools"></category><category term="Australia"></category><category term="BigQuery"></category><category term="Cantonese"></category><category term="Chrome"></category><category term="Climate"></category><category term="Conservation"></category><category term="Data Center"></category><category term="ECCV"></category><category term="Electronic Commerce and Algorithms"></category><category term="Encryption"></category><category term="Entity Salience"></category><category term="Faculty Institute"></category><category term="Flu Trends"></category><category term="Google I/O"></category><category term="Google Trips"></category><category term="Google Voice Search"></category><category term="Government"></category><category term="ICSE"></category><category term="IPython"></category><category term="Journalism"></category><category term="Klingon"></category><category term="Korean"></category><category term="Linear Optimization"></category><category term="Magenta"></category><category term="Market Research"></category><category term="Mixed Reality"></category><category term="Network Management"></category><category term="Nexus"></category><category term="Peer Review"></category><category term="PhotoScan"></category><category term="PiLab"></category><category term="Professional Development"></category><category term="Public Data Explorer"></category><category term="SIGCOMM"></category><category term="SIGMOD"></category><category term="Site Reliability Engineering"></category><category term="Sound Search"></category><category term="TV"></category><category term="UNIX"></category><category term="Visiting Faculty"></category><category term="Wiki"></category><category term="adsense"></category><category term="adwords"></category><category term="correlate"></category><category term="entities"></category><category term="gamification"></category><category term="jsm"></category><category term="jsm2011"></category><category term="localization"></category><category term="materials science"></category><category term="operating systems"></category><category term="osdi10"></category><title type="text">Google AI 博客&lt;/stitle>;&lt;subtitle type=&quot;html&quot;>;来自 Google AI 的最新新闻。&lt;/substitle>;&lt;link href=&quot;http://blog.research.google/feeds/posts/default&quot; rel=&quot; http://schemas.google.com/g/2005#feed&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default? alt=atom&amp;redirect=false&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot; />;&lt;link href=&quot;http://pubsubhubbub.appspot.com/&quot; rel=&quot;hub&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default?alt= atom&amp;start-index=26&amp;max-results=25&amp;redirect=false&quot; rel=&quot;next&quot; type=&quot;application/atom+xml&quot;/>;&lt;author>;&lt;name>;ewood&lt;/name>;&lt;uri>;http://www.blogger. com/profile/12341551220176883769&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src =&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;generator uri=&quot;http://www.blogger.com &quot; version=&quot;7.00&quot;>;Blogger&lt;/generator>;&lt;opensearch:totalresults>;1299&lt;/opensearch:totalresults>;&lt;opensearch:startindex>;1&lt;/opensearch:startindex>;&lt;opensearch:itemsperpage>;25&lt;/opensearch:itemsperpage>;&lt;entry >;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-4973261706617691472&lt;/id>;&lt;发布>;2023-10-27T13:22:00.000-07:00&lt;/发布>;&lt;更新>;2023-10- 27T13:22:42.527-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Android Wear&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;Health&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用可听设备进行心脏监测的听体积描记术&lt;/stitle>;&lt;content type=&quot;html&quot; >;&lt;span class=&quot;byline-author&quot;>;发布人：实验科学家范晓然 (Xiaoran &quot;Van&quot; Fan) 和 Google 总监 Trausti Thormundsson &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/ b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78bRxDflolUzzfol5UbfkPmCaNn8bIUxwabLDatqZTPxP7SYOT45g9qJODdM1 kvT6NQUsixtFTiBYr_h_Hx-pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s320/hero.jpeg&quot; style=&quot;显示：无；&quot; />; &lt;p>; &lt;a href=&quot;https://www.telink-semi.com/introduction-true-wireless-stereo/&quot;>;真正的无线立体声&lt;/a>; (TWS) 市场&lt;a href=&quot; https://en.wikipedia.org/wiki/Active_noise_control&quot;>;主动降噪&lt;/a>;（ANC）耳戴式设备（耳机和耳塞）近年来猛增，全球出货量近&lt;a href=&quot; https://www.idc.com/promo/wearablevendor&quot;>;2023 年将是智能腕带和手表的两倍&lt;/a>;。由于 ANC、透明模式、和人工智能。用户经常佩戴耳穿戴设备不仅是为了听音乐，还为了锻炼、集中注意力或只是调节情绪。然而，听觉健康对于消费市场来说仍然是一个未知领域。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 在“&lt;a href=&quot;https://research.google/pubs/pub52460/&quot;>;APG：用于耳戴式设备心脏监测的音频容积描记术&lt;/ a>;”，在 &lt;a href=&quot;https://sigmobile.org/mobicom/2023/&quot;>;MobiCom 2023&lt;/a>; 上展示，我们引入了一种新颖的主动入耳式健康传感模式。音频体积描记法 (APG) 使 ANC 耳戴式设备能够监测用户的生理信号，例如心率和心率变异性，而无需添加额外的传感器或影响电池寿命。 APG 对运动伪影表现出高度弹性，遵循 &lt;a href=&quot;https://www.health.belgium.be/sites/default/files/uploads/fields/fpshealth_theme_file/19099349/Canadian%20guidelines%20for%20ultrasound.pdf &quot;>;安全规定&lt;/a>;，裕量低于限制 80 dB，不受密封条件的影响，并且包含所有肤色。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD- u3qDR-r8Z0rnB7lsDuF9iab9CdR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s1600/image4.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3if_joqkSgp5nI0YvfWgUtgi3FoOtmeNcwcDdNNaMlPizqHpbuaeVFrp1MguLPQpT0hqrQaldH0ymqwjDzsD-u3qDR-r8Z0rnB7lsDuF9iab9C dR6GRPI7OPtqLFR29mRwfHz06kwqhppe7mvI9iVIBi8DtpIYh6E-UXs1RwUffTRxKVViTuXpSLXLAXI/s16000/image4.gif&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- Caption&quot; style=&quot;text-align: center;&quot;>;APG 使用 ANC 耳机扬声器发送低强度超声波发射波（TX 波），并通过板载反馈麦克风收集接收波（RX 波）。 APG信号是一种脉冲状波形，与心跳同步，揭示丰富的心脏信息，例如&lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text=The% 20重搏%20切迹%20是%20a，%20舒张%20在%20这些%20动脉中。&quot;>;重搏切迹&lt;/a>;。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;耳朵健康感应耳道&lt;/h2>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Ear_canal&quot;>;听道&lt;/a>;从&lt;a href=&quot;https:// en.wikipedia.org/wiki/Deep_aurillary_artery&quot;>;耳廓深动脉&lt;/a>;，也称为深耳动脉。这条动脉形成了一个由广泛渗透耳道的小血管组成的复杂网络。心跳（和&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8834671&quot;>;血压&lt;/a>;）引起的血管形状的轻微变化可能会导致体积和体积的细微变化。耳道的压力，使耳道成为健康传感的理想位置。 &lt;/p>; &lt;p>; 最近的研究探索了通过将大量传感器封装在一起，使用耳戴式设备进行健康传感 - 例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Photoplethysmogram&quot;>;光电体积描记图&lt;/a >; (PPG) 和&lt;a href=&quot;https://en.wikipedia.org/wiki/Electrocardiography&quot;>;心电图&lt;/a>; (ECG) — 使用微控制器支持健康应用，例如睡眠监测、心率和血压追踪。然而，这种传感器安装模式不可避免地增加了成本、重量、功耗、声学设计复杂性以及耳戴式设备的外形挑战，构成了其广泛采用的强大障碍。 &lt;/p>; &lt;p>; 现有的 ANC 耳戴式设备部署反馈和前馈麦克风来导航 ANC 功能。这些麦克风为各种传感应用创造了&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3550314&quot;>;新机会&lt;/a>;，因为它们可以检测或记录内部和外部的许多生物信号耳道。例如，反馈麦克风可用于聆听心跳，前馈麦克风可用于聆听呼吸。 &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3458864.3467680&quot;>;关于这种被动传感范式的学术研究&lt;/a>;催生了许多移动应用，包括心率监测、耳病诊断、呼吸监测和身体活动识别。不过，消费级 ANC 耳机中的麦克风配有&lt;a href=&quot;https://research.google/pubs/pub52579/&quot;>;内置高通滤波器&lt;/a>;，以防止身体运动或强风造成的饱和噪音。耳道内被动聆听的信号质量也很大程度上依赖于耳塞的密封条件。因此，在商用 ANC 耳机上嵌入依赖低频信号（≤ 50 Hz）被动聆听的健康功能具有挑战性。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;测量微小的生理信号&lt;/h2>; &lt;p>; APG 通过发送信号绕过上述 ANC 耳机硬件限制通过 ANC 耳机扬声器发出低强度超声波探测信号。该信号会触发回声，通过板载反馈麦克风接收回声。我们观察到微小的耳道皮肤位移和心跳振动调节这些超声波回声。 &lt;/p>; &lt;p>; 我们构建了一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Acoustic_resonance&quot;>;圆柱共振&lt;/a>;模型来了解 APG 的基础物理原理。这种现象发生在极小的范围内，这使得原始脉冲信号在原始接收的超声波中不可见。我们采用相干检测来检索本底噪声下的微生理调制（我们将检索到的信号称为混合信号）信号，请参阅&lt;a href=&quot;https://research.google/pubs/pub52460/&quot;>;论文&lt;/a>;了解更多详情）。最终的 APG 波形看起来与 PPG 波形惊人地相似，但提供了心脏活动的改进视图，更明显 &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/32403992/#:~:text= %20重搏%20切迹%20是%20a,of%20舒张%20in%20这些%20动脉。&quot;>;重搏切迹&lt;/a>;（即提供有关中央动脉系统丰富信息的压力波形，例如血压）。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrO YihU61CaKl8CN5K8ET_g8vOFxHhuKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s1600/image5.gif &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1200&quot; data-original-width=&quot;1600&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjEoN64wQBOUKAhV6SHbEMqIcAuVCk01zBki8pnV1xoEseaJCT1uNRprkjKyFLYRin4a-iSrPjg3Pr-eEgc_YrOYihU61CaKl8CN5K8ET_g8vOF xHhUKQaq7Fb9xMhGVVLMS_AouTZvfDfXYRVz0ExhCIV4je2Hxqd-CVfKmmpfGjLIGeKawPEff6Ei5gfT/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;具有心脏活动 ℎ(𝑡) 的圆柱形模型，可调节 &lt;a href=&quot;https://www.digikey.com/en/articles /the-basics-of-mixers&quot;>;混合信号&lt;/a>;。根据我们的分析模型的模拟，混合 APG 信号的幅度 𝑅(𝑡) 和相位 Φ(𝑡) 都反映了心脏活动 ℎ(𝑡)。&lt;/td>;&lt;/tr>;&lt;/tbody>; &lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;APG 传感实践&lt;/h2>; &lt;p>; 在我们最初的实验中，我们据观察，APG 在耳塞密封不良和音乐播放的情况下仍能正常工作。然而，我们注意到 APG 信号有时可能非常嘈杂，并且可能会受到身体运动的严重干扰。那时，我们决定，为了使 APG 有用，我们必须使其更加强大，以便与 &lt;a href=&quot;https://curiouscyborg.com/history-of-photoplethysmography/#:~:text=The %20光电体积描记图%20是%20第一个%20观察到的，%20光%20与%20组织的相互作用%20。&quot;>;超过80年&lt;/a>;的PPG发展。 &lt;/p>; &lt;p>; 虽然 PPG 被广泛使用并且非常先进，但它们确实有一些局限性。例如，PPG 传感器通常使用两到四个&lt;a href=&quot;https://en.wikipedia.org/wiki/Diode&quot;>;二极管&lt;/a>;来发送和接收光频率以进行传感。然而，由于光的超高频特性（数百&lt;a href=&quot;https://en.wikipedia.org/wiki/Terahertz_radiation&quot;>;太赫兹&lt;/a>;），单个二极管很难以不同的频率发送多种颜色。另一方面，我们可以轻松设计一个低成本、低功耗的系统，生成和接收十种以上的音频音调（频率）。我们利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Diversity_scheme&quot;>;信道多样性&lt;/a>;，这是一种物理现象，描述不同频率的无线信号（例如光和音频）如何具有不同的特征当信号在介质中传播时（例如，不同的衰减和反射系数），以实现更高质量的APG信号和运动弹性。 &lt;/p>; &lt;p>; 接下来，我们通过实验证明在 APG 信令中使用多个频率的有效性。我们同时传输三个探测信号，它们的频率均匀地分布在 30 KHz 到 32 KHz 之间。实验期间，参与者被要求摇头四次以引入干扰。下图显示，可以通过&lt;a href=&quot;https://www.rp-photonics.com/optical_heterodyne_detection.html&quot;>;相干检测&lt;/a>;同时传输不同频率来收集各种信息，这是APG的独特优势。 &lt;/p>; &lt;p>; 30 kHz 相位显示四个头部运动，31 kHz 幅度（振幅）显示脉搏波信号。这一观察结果表明，某些超声波频率可能对心脏活动敏感，而另一些超声波频率可能对运动敏感。因此，我们可以使用多音APG作为校准信号来寻找测量心率的最佳频率，并仅使用最佳频率来获得高质量的脉搏波形。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GB qEfyQjkjChwVqL_Cxc0dO6i3vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s1761/image2.png&quot;样式=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1380&quot; data-original-width=&quot;1761&quot; src=&quot;https://blogger. googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP_DLCbWWNJRvMPnIbdAbij_YKuIuYGbTiN-drwsJnEkrybDQT_yNHIqKYrqhxfq8CrWSEVZ6kvCvZ1tIUoXtH5GBqEfyQjkjChwVqL_Cxc0dO6i3 vJi_sPo7hjElUuxH9f8d3E6nkDBNGNNgCRdFhfllW8OnhGXVS120-e4Yh67vECY7_lQpDOH2I42TU/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style= &quot;text-align: center;&quot;>;自定义多音 APG 信号的混合幅度（上行）和相位（下行），范围从 30 kHz 到 32 kHz。利用通道分集，可以在某些频率（例如，31 kHz 的幅度）中捕获心脏活动，并在其他频率（例如，30 kHz、30 kHz 的幅度和31 kHz 的相位）中捕获头部运动。 &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 选择最佳频率测量心率后，APG 脉搏波形变得更加明显，重搏切迹明显，并且能够准确&lt;a href=&quot;https ://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;心率变异性&lt;/a>;测量。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkY TMRiNVtzUidLMc1lp48n6O8hsLMqFQc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s1999/image1.png &quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1999&quot; src=&quot;https:// blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0mjyX5qTrzW9rN1qexcI3Nm0hgn03m-K_kGn_A8yfqlviJ3Rc3tod-srqQ-6vuoPEo6em7z3Qx7NpgkYTMRiNVtzUidLMc1lp48n6O8hsLMqF Qc8z5CKI8mqSowmEbg5ojxdG4vaYreLbv0q2YwQAk3hKwdSd1cdwQMGVtFmgwn46nvCYRuPEi6hdpcVh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr- title&quot; style=&quot;text-align: center;&quot;>;测量阶段（&lt;strong>;左&lt;/strong>;）和胸部使用的最终 APG 信号&lt;a href=&quot;https://en.wikipedia.org/wiki /心电图&quot;>;ECG&lt;/a>; 信号（&lt;strong>;右&lt;/strong>;）。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 多音转换为多个同时观察，这促进&lt;a href=&quot;https://en.wikipedia.org/wiki/Array_processing&quot;>;阵列信号处理&lt;/a>;技术的开发。我们演示了应用&lt;a href=&quot;https://en.wikipedia.org/wiki/Signal_separation&quot;>;盲源分离&lt;/a>;之前和之后正在运行的会话 APG 实验的频谱图（请参阅&lt;a href=&quot;https ://research.google/pubs/pub52460/&quot;>;论文&lt;/a>;了解更多详情）。我们还在同一跑步实验中使用 &lt;a href=&quot;https://www.polar.com/us-en/sensors/h10-heart-rate-sensor&quot;>;Polar ECG 胸带展示了真实心率测量结果&lt;/a>;。在原始 APG 中，我们看到跑步节奏（大约 3.3 Hz）以及两条暗线（大约 2 Hz 和 4 Hz），指示用户的心率频率及其谐波。盲源分离后心率频率的信噪比（SNR）显着增强，这与地面真实心率频率一致。我们还显示根据 APG 和 ECG 计算出的心率和跑步步频。我们可以看到APG准确地跟踪了跑步过程中心率的增长。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUTfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05 Alnf_68701LYb0DQrpbbqAYGP7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s1888/image3 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1423&quot; data-original-width=&quot;1888&quot; src=&quot;https: //blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbwRfWHxYHVk1ARp9gzcaRTMIUO-FjAgaTqOUTfYMzJ1lpAsWSxm1MczllG4aq-3Rbi-M6u7KXqnPOdsiLJS4le05Alnf_68701LYb0DQrpbbqAYGP 7jzYivDhoeniPeTmsBU7Uhg4HRyOlYdQCYWxL-xabuULYFlGdH_rIA1b1Q8ZXH8MHIYlPQcaUpYW/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td 类=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;APG 在跑步过程中准确追踪心率并测量跑步步频。&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;实地考察和结束语&lt;/h2>; &lt;p>;我们进行了两轮用户体验（UX ）有 153 名参与者参与的研究。我们的结果表明，APG 始终能够实现准确的心率（所有活动场景中参与者的中位误差为 3.21%）和&lt;a href=&quot;https://en.wikipedia.org/wiki/Heart_rate_variability&quot;>;心率变异性&lt;/a>; （节拍间间隔中值误差为 2.70%）测量。与 PPG 不同肤色表现出不同的性能，我们的研究表明 APG 对以下方面的变化具有弹性：肤色、次优密封条件和耳道尺寸。更详细的评估可以在&lt;a href=&quot;https://research.google/pubs/pub52460/&quot;>;论文&lt;/a>;中找到。 &lt;/p>; &lt;p>; APG 通过简单的软件升级即可将任何 TWS ANC 耳机转变为智能传感耳机，并在各种用户活动中稳定运行。感应载波信号完全听不见，并且不受音乐播放的影响。更重要的是，APG代表了生物医学和移动研究领域的新知识，并为低成本健康传感开启了新的可能性。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>;&lt;i>; APG 是整个 Google Health 合作的成果，产品、用户体验和法律团队。我们要感谢 David Pearl、Jesper Ramsgaard、Cody Wortham、Octavio Ponce、Patrick Amihood、Sam Shen、Michael Pate、Leonardo Kusumo、Simon Tong、Tim Gladwin、Russ Mirov、Kason Walker、Govind Kannan、Jayvon Timmons、Dennis Rauschmayer、 Chiong Lai、Shwetak Patel、杰克·加里森、王安然、Shiva Rajagopal、Shelten Yuen、Seobin Jung、刘云、约翰·埃尔南德斯、Issac Galatzer-Levy、Isaiah Fischer-Brown、Jamie Rogers、Pramod Rudrapatna、Andrew Barakat、Jason Guss、Ethan格拉鲍、波尔·佩弗、比尔·帕克、海伦·奥康纳、米亚·程、汤场敬一郎、费利克斯·博尔斯、朴雅卡·詹特、徐陆舟、王健、杰米·连恩、格里·帕利普拉姆、尼古拉斯·吉莲、米哈尔·马图扎克、雅库布·沃伊切霍夫斯基、布莱恩·艾伦、Jane感谢 Hilario 和 Phil Carmack 的宝贵见解和支持。感谢外部合作者上官龙飞和 Rich Howard、罗格斯大学和匹兹堡大学。&lt;/i>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/4973261706617691472/comments/默认&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html #comment-form&quot; rel=&quot;replies&quot; title=&quot;0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/4973261706617691472&quot; rel=&quot;self&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/audioplethysmography-for-cardiac.html&quot; rel=&quot;alternate&quot; title=&quot;使用耳戴式设备进行心脏监测的音频体积描记法&quot; type=&quot;text/html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt; /email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded. gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnCItsRGs939CGCBBAi5fA-xwk9JQi7b2CQ3voKK583p4uTkQkLIXUd1lU71SLynom0Yt78b RxDflolUzzfol5UbfkPmCaNn8bIUxwAbLDatqZTPxP7SYOT45g9qJODdM1kvT6NQUsixtFTiBYr_h_Hx- pFRsmbcBKdnW3WkbcD4Bcr_cZE590VL-cSu-a/s72-c/hero.jpeg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0 &lt;/thr：total>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-6605352025607219737&lt;/id>;&lt;发布>;2023-10-26T11：01：00.000-07： 00&lt;/已发布>;&lt;更新>;2023-10-26T11：01：04.276-07：00&lt;/更新>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“AI” >;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“协作”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/” atom/ns#&quot; term=&quot;Responsible AI&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;通过 MLCommons 支持 AI 安全基准&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-作者&quot;>;作者：技术与社会 Anoop Sinha 和 Google 研究负责人工智能和以人为本的技术团队 Marian Croak&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl /AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUNEsldIlb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcE ZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s1600/GoogleResearch.png&quot; style=&quot;显示：无；&quot; />; &lt;p>; 标准基准是衡量重要产品质量的一致方法，存在于许多领域。一些标准基准衡量安全性：例如，当汽车制造商宣传“五星级总体安全评级”时，他们引用的是基准。机器学习 (ML) 和人工智能技术中已经存在标准基准：例如，&lt;a href=&quot;https://mlcommons.org/en/&quot;>;MLCommons&lt;/a>; 协会运营 &lt;a href=&quot;https: //mlcommons.org/en/news/mlperf-inference-storage-q323/&quot;>;MLPerf&lt;/a>; 基准测试，用于测量 Google TPU 等尖端 AI 硬件的速度。然而，尽管在&lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building-guardrails-for-generative-ai/&quot;>;人工智能安全方面已经开展了大量工作&lt; /a>;，目前还没有类似的人工智能安全标准基准。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 我们很高兴能够支持非营利性 MLCommons 协会制定标准 AI 安全基准的新努力。开发有效且值得信赖的基准需要先进的人工智能安全测试技术并纳入广泛的观点。 MLCommons 的目标是汇集学术界和工业界的专家研究人员，制定标准基准，用于衡量人工智能系统的安全性，得出每个人都能理解的分数。我们鼓励整个社区，从人工智能研究人员到政策专家，&lt;a href=&quot;https://mlcommons.org/ai-safety&quot;>;加入我们&lt;/a>;为这项工作做出贡献。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;为什么要制定人工智能安全基准？&lt;/h2>; &lt;p>; 与大多数先进技术一样，人工智能有潜力巨大的好处，但如果没有适当的护理也可能导致负面结果。例如，人工智能技术可以在各种活动中提高人类生产力（例如，&lt;a href=&quot;https://blog.google/technology/health/how-ai-can-improve-health-for-everyone-everywhere /&quot;>;改善健康诊断&lt;/a>;和疾病研究，分析&lt;a href=&quot;https://blog.google/outreach-initiatives/sustainability/google-transportation-energy-emissions-reduction/&quot;>;能源使用情况&lt; /a>; 等）。然而，如果没有足够的预防措施，人工智能也可能被用来支持有害或恶意活动，并以有偏见或攻击性的方式做出反应。 &lt;/p>; &lt;p>; 通过提供跨类别的标准安全措施，例如有害使用、超出范围的响应、人工智能控制风险等，标准人工智能安全基准可以帮助社会获得人工智能的好处，同时确保正在采取足够的预防措施来减轻这些风险。最初，新兴的安全基准可以帮助推动人工智能安全研究并为负责任的人工智能开发提供信息。随着时间的推移和成熟，它们可以帮助告知人工智能系统的用户和购买者。最终，它们可能成为政策制定者的宝贵工具。 &lt;/p>; &lt;p>; 在计算机硬件中，基准测试（例如，&lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_Performance_Evaluation_Corporation&quot;>;SPEC&lt;/a>;、&lt;a href=&quot;https:// en.wikipedia.org/wiki/Transaction_Processing_Performance_Council&quot;>;TPC&lt;/a>;）已经表现出了惊人的能力，可以协调整个行业的研究、工程甚至营销以追求进步，我们相信标准的人工智能安全基准可以帮助做到这一点在这个重要区域也是如此。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;标准人工智能安全基准是什么？&lt;/h2>; &lt;p>; 学术界和企业研究工作已经尝试过一系列人工智能安全测试（例如，&lt;a href=&quot;https://arxiv.org/abs/2009.11462&quot;>;RealToxicityPrompts&lt;/a>;、&lt;a href=&quot;https://crfm.stanford.edu/2022/ 11/17/helm.html&quot;>;斯坦福 HELM&lt;/a>; 公平性、偏见、毒性测量以及&lt;a href=&quot;https://blog.google/technology/ai/our-responsible-approach-to-building- Guardrails-for-generative-ai/&quot;>;Google 的生成式 AI 护栏&lt;/a>;）。然而，大多数这些测试的重点是向人工智能系统提供提示并通过算法对输出进行评分，这是一个有用的开始，但仅限于测试提示的范围。此外，他们通常使用开放数据集进行提示和响应，这些数据集可能已经（通常无意中）合并到训练数据中。 MLCommons 提出了一个多利益相关者流程，用于选择测试并将其分组为子集，以衡量特定人工智能用例的安全性，并将这些测试的高科技结果转化为每个人都能理解的分数。 MLCommons 提议创建一个平台，将这些现有测试集中到一个地方，并鼓励创建更严格的测试，以推动最先进的技术向前发展。用户将能够通过在线测试（可以生成和查看分数）和使用私人测试引擎的离线测试来访问这些测试。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;人工智能安全基准应该是集体努力&lt;/h2>; &lt;p>; 负责任的人工智能开发人员使用不同的范围安全措施，包括自动测试、手动测试、红队（人类测试人员试图产生对抗性结果）、软件施加的限制、数据和模型最佳实践以及审计。然而，确定是否已采取足够的预防措施可能具有挑战性，特别是随着提供人工智能系统的公司社区的发展和多元化。标准人工智能基准可以提供强大的工具，通过帮助供应商和用户衡量人工智能安全性，并鼓励专注于提高人工智能安全性的资源和专业提供商生态系统，帮助社区负责任地发展。 &lt;/p>; &lt;p>; 与此同时，如果没有社区的参与，就不可能开发出既有效又可信的成熟人工智能安全基准。这项工作需要研究人员和工程师齐心协力，对安全测试技术进行创新且实用的改进，使测试更加严格和高效。同样，公司需要联合起来提供测试数据、工程支持和财务支持。人工智能安全的某些方面可能是主观的，建立由广泛共识支持的可信基准需要纳入多种观点，包括公众倡导者、政策制定者、学者、工程师、数据工作者、商界领袖和企业家的观点。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;Google 对 MLCommons 的支持&lt;/h2>; &lt;p>; 植根于我们的 &lt;a href=&quot;https:/ /ai.google/responsibility/principles/&quot;>;人工智能原则&lt;/a>;于 2018 年&lt;a href=&quot;https://blog.google/technology/ai/ai-principles/&quot;>;宣布&lt;/a>;， Google 致力于安全、可靠且值得信赖的 AI 开发和使用的具体实践（请参阅我们的 &lt;a href=&quot;https://ai.google/static/documents/ai-principles-2019-progress-update.pdf &quot;>;2019&lt;/a>;、&lt;a href=&quot;https://ai.google/static/documents/ai-principles-2020-progress-update.pdf&quot;>;2020&lt;/a>;、&lt;a href=&quot;https ://ai.google/static/documents/ai-principles-2021-progress-update.pdf&quot;>;2021&lt;/a>;，&lt;a href=&quot;https://ai.google/static/documents/ai-principles -2022-progress-update.pdf&quot;>;2022&lt;/a>; 更新）。我们还在关键承诺方面&lt;a href=&quot;https://static.googleusercontent.com/media/publicpolicy.google/en//resources/whcommitments.pdf&quot;>;取得了重大进展&lt;/a>;，这将有助于确保大胆、负责任地开发人工智能，造福所有人。 &lt;/p>; &lt;p>; Google 通过多种方式支持 MLCommons 协会制定 AI 安全基准的努力。 &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;测试平台&lt;/em>;：我们正在与其他公司一起提供资金以支持测试平台的开发。 &lt;li>;&lt;em>;技术专业知识和资源&lt;/em>;：我们提供技术专业知识和资源，例如&lt;a href=&quot;https://skintone.google/mste-dataset&quot;>;僧侣肤色示例数据集&lt; /a>;，帮助确保基准设计良好且有效。 &lt;li>;&lt;em>;数据集&lt;/em>;：我们正在为多语言代表性偏见提供内部数据集，以及已经针对刻板印象危害进行的外部测试，例如 &lt;a href=&quot;https://github.com/google- Research-datasets/seegull/tree/main&quot;>;SeeGULL&lt;/a>; 和 &lt;a href=&quot;https://github.com/google-research-datasets/SPICE/tree/main&quot;>;SPICE&lt;/a>;。此外，我们正在共享专注于负责任和包容性地收集人类注释的数据集，例如 &lt;a href=&quot;https://arxiv.org/abs/2306.11247&quot;>;DICES&lt;/a>; 和 &lt;a href=&quot;https:/ /www.kaggle.com/datasets/google/jigsaw-specialized-rater-pools-dataset&quot;>;SRP&lt;/a>;。 &lt;/li>; &lt;/ol>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;未来方向&lt;/h2>; &lt;p>; 我们相信这些基准测试将非常有用促进人工智能安全研究并确保以负责任的方式开发和部署人工智能系统。人工智能安全是&lt;a href=&quot;https://blog.google/technology/ai/a-shared-agenda-for-responsible-ai-progress/&quot;>;一个集体行动问题&lt;/a>;。 &lt;a href=&quot;https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/&quot;>;前沿模型论坛&lt;/a>;和&lt;a href=&quot;https://partnershiponai.org/&quot;>;人工智能合作伙伴关系&lt;/a>;也在引领重要的标准化举措。我们很高兴自这些团体和 MLCommons 成立以来就成为其中的一员。我们期待更多的集体努力，促进负责任地开发新的生成式人工智能工具。 &lt;/p>; &lt;div style=&quot;line-height:40%;&quot;>; &lt;br>; &lt;/div>; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;非常感谢为这项工作做出贡献的 Google 团队主演: 彼得·马特森 / 洛拉·阿罗约 / 克里斯·韦尔蒂 / 凯西·梅尔-赫尔斯特恩 / 帕克·巴恩斯 / 图尔西·多西 / 曼温德·辛格 / 布莱恩·戈德曼 / 尼特什·戈亚尔 / 爱丽丝·弗兰德 / 妮可·德兰格 / 克里·巴克 / 玛德琳·埃利什 / 什鲁蒂·谢斯 / 道恩·布洛克斯威奇 / 威廉·艾萨克，克里斯蒂娜·巴特菲尔德。&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6605352025607219737/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html#comment-form&quot; rel=&quot;replies &quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;edit&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6605352025607219737&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/supporting-benchmarks-for-ai-safety.html&quot; rel=&quot;alternate&quot; title=&quot;通过 MLCommons 支持人工智能安全基准&quot; type=&quot;text/ html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd ：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度=“ 16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg5URA9ivhxcgmfXwa0668O0HslgYQ_p_x9JJbg6nDgryyNc2QImQernPyzkLPcj1esCMOQTUnEsld Ilb21E0DWPDIiE2m76qSruF0jA6jfl1sNl6mBW8JUPSWzOfE7IahHRtviFpxUTPcEZoADXHNMy3gZ2hg369y5QxhY01QRVj5kwJx4uwRKzdcBFp9v/s72-c/GoogleResearch.png “ width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>; &lt;id>;标签：blogger.com，1999：blog-8474926331452026626.post-7441213378686175839&lt;/id>;&lt;发布>;2023-10-26T08:57:00.001-07:00&lt;/发布>;&lt;更新>;2023-10-26T09 :00:37.984-07:00&lt;/updated>;&lt;category schema=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;自然语言处理&quot;>;&lt;/category>;&lt;category schema=&quot;http ://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;使用声谱图驱动的法学硕士进行语音问答和语音延续&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;发布者：Google 研究科学家 Eliya Nachmani 和学生研究员 Alon Levkovitch&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent. com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_178hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCV qtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/s1100/Spectron-hero.jpg&quot; style=&quot;显示：无；&quot; />; &lt;p>; 自然语言处理（NLP）的目标是开发能够理解和生成自然语言的计算模型。通过捕获基于文本的自然语言的统计模式和结构，语言模型可以预测并生成连贯且有意义的单词序列。通过越来越多地使用非常成功的 &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;Transformer&lt;/a>; 模型架构以及对大量文本进行训练（具有成比例的计算和模型大小）来实现，大型语言模型（LLM）在 NLP 任务中表现出了显着的成功。 &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; 然而，对人类口语进行建模仍然是一个具有挑战性的前沿领域。语音对话系统通常被构建为级联的&lt;a href=&quot;https://en.wikipedia.org/wiki/Speech_recognition&quot;>;自动语音识别&lt;/a>; (ASR)、&lt;a href=&quot;https:// /en.wikipedia.org/wiki/Natural-language_understanding&quot;>;自然语言理解&lt;/a>; (NLU)、响应生成和&lt;a href=&quot;https://simple.wikipedia.org/wiki/Text_to_speech&quot;>;文本语音转换（TTS）系统。然而，迄今为止，还很少有能够用于口语建模的端到端系统：即可以接受语音输入并生成其延续作为语音输出的单一模型。 &lt;/p>; &lt;p>;今天，我们提出了一种新的口语建模方法，称为 Spectron，发表于“&lt;a href=&quot;https://arxiv.org/abs/2305.15255&quot;>;使用声谱图进行口语问答和语音延续” -由法学硕士提供支持&lt;/a>;。” Spectron 是第一个经过端到端训练的口语模型，可以直接处理声谱图作为输入和输出，而不是学习离散的语音表示。仅使用预先训练的文本语言模型，就可以对其进行微调以生成高质量、语义准确的口语。此外，所提出的模型在直接初始化的基础上进行了改进，保留了原始 LLM 的知识，如通过口语&lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;>;问答&lt;/a>;数据集所证明的那样。&lt; /p>; &lt;p>; 我们证明，预训练的语音编码器和语言模型解码器可以实现端到端训练和最先进的性能，而不会牺牲表征保真度。其关键是一种新颖的端到端训练目标，它以联合方式隐式监督语音识别、文本延续和条件语音合成。新的频谱图回归损失还监督模型在时域和频域中匹配频谱图的高阶导数。这些导数表达了从多个帧同时聚合的信息。因此，它们表达了关于信号形状的丰富的、更远距离的信息。我们的整体方案总结如下图： &lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin -right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6 -4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8Tt_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s1815 /Spectron.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1146&quot; data-original-width= “1815”src =“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4yanW_7FsK-zlJ5XKgusakSaIGAEvDPuRZpAZ5NGJl8mW6-4hd06ZWOsk4IIf0hl5XwWFcjkdu9gH4VwqcWCqLUfZQXDM80MuDml8T t_P0RT3fDq5cxfoPCPNagOuU0SZbSz8Vn7x_bLkxNFXKpakBPwCkgXh8T6bB4lfHQevGBjs-U4c5WA6RgC8gukW/s16000/Spectron.jpg&quot;/>;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Spectron 模型将语音识别模型的编码器与预编码器连接起来-训练有素的基于 Transformer 的解码器语言模型。在训练中，语音分为提示及其延续。然后，重建完整的文字记录（提示和延续）以及延续的语音特征。推理时仅给出提示；提示的转录、文本延续和语音延续均由模型生成。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Spectron 架构&lt;/h2>; &lt;p>; 该架构使用预训练的语音编码器和预训练的解码器语言模型进行初始化。编码器以语音作为输入，将其编码为连续的语言特征。这些特征作为前缀输入到解码器中，并且整个编码器-解码器被优化以共同最小化&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;交叉熵&lt;/a >; 损失（用于语音识别和转录延续）和新颖的重建损失（用于语音延续）。在推理过程中，人们提供口头语音提示，该语音提示被编码然后解码以提供文本和语音延续。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;语音编码器&lt;/h3>; &lt;p>; 语音编码器是一个600M参数&lt;a href= “https://arxiv.org/abs/2303.01037&quot;>;conformer&lt;/a>; 编码器在&lt;a href=&quot;https://arxiv.org/abs/2303.01037&quot;>;大规模数据&lt;/a>;上进行预训练（1200 万小时）。它将源语音的频谱图作为输入，生成包含语言和声学信息的隐藏表示。输入频谱图首先使用卷积层进行二次采样，然后由一系列构象块进行处理。每个构象块由前馈层、自注意力层、卷积层和第二前馈层组成。输出通过投影层将隐藏表示与语言模型的嵌入维度相匹配。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;语言模型&lt;/h3>; &lt;p>; 我们使用350M或1B参数的解码器语言模型（用于分别是延续任务和问答任务）以 &lt;a href=&quot;https://ai.google/static/documents/palm2techreport.pdf&quot;>;PaLM 2&lt;/a>; 的方式进行训练。该模型接收提示的编码特征作为前缀。请注意，这是语音编码器和 LM 解码器之间的唯一连接；即，编码器和解码器之间不存在交叉注意力。与大多数口语模型不同，在训练期间，解码器被教师强制预测文本转录、文本延续和语音嵌入。为了将语音嵌入与声谱图相互转换，我们在网络前和网络后引入了轻量级模块。 &lt;/p>; &lt;p>; 通过使用相同的架构解码中间文本和频谱图，我们获得了两个好处。首先，LM 在文本域中的预训练允许在合成语音之前在文本域中继续提示。其次，预测文本充当中间推理，提高合成语音的质量，类似于使用中间 &lt;a href=&quot;https://arxiv.org/abs/2112.00114&quot;>;scratchpad&lt; 时基于文本的语言模型的改进/a>; 或&lt;a href=&quot;https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html&quot;>;思想链&lt;/a>; (CoT) 推理。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;声学投影层&lt;/h3>; &lt;p>; 为了使语言模型解码器能够对谱图帧进行建模，我们采用多层“预网络”将地面真实声谱图语音延续投影到语言模型维度。该预网络将频谱图输入压缩到较低维度，从而创建有助于解码过程的瓶颈。这种瓶颈机制阻止模型在解码过程中重复生成相同的预测。为了将 LM 输出从语言模型维度投影到声谱图维度，该模型采用了“后网络”，它也是一个多层感知器。前置网络和后置网络都是两层多层感知器。 &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;训练目标&lt;/h3>; &lt;p>; Spectron 的训练方法使用两种不同的损失函数：( i) &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-entropy&quot;>;交叉熵损失&lt;/a>;，用于语音识别和转录延续，以及 (ii) &lt;a href= “https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;回归损失&lt;/a>;，用于语音延续。在训练期间，所有参数都会更新（语音编码器、投影层、LM、预网络和后网络）。 &lt;/p>; &lt;br />; &lt;h2>;音频样本&lt;/h2>; &lt;p>; 以下是 Spectron 的语音延续和问答示例：&lt;/p>; &lt;tablealign=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing= &quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;&lt;h3 >;语音延续&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;提示：&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: right;&quot;>;&lt;音频控件=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/1/src.wav&quot;>;&lt;/audio>; &lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;继续：&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-对齐：右;&quot;>;&lt;音频控制=“控制”src=“https://michelleramanovich.github.io/spectron/spectron/librispeech/1/pred.wav”>;&lt;/音频>;&lt;/td>;&lt;/tr >; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;提示：&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;音频控件=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/ 2/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;继续：&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr >; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;音频控件=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/2/pred.wav&quot;>; &lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left ;&quot;>;提示：&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https://michelleramanovich .github.io/spectron/spectron/librispeech/3/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;继续： &amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;音频控件=&quot;controls&quot; src=&quot;https://michelleramanovich.github.io/spectron/ Spectron/librispeech/3/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr >;&lt;td style=&quot;text-align: left;&quot;>;提示：&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;音频控件= “控制” src=&quot;https://michelleramanovich.github.io/spectron/spectron/librispeech/4/src.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text -align: left;&quot;>;继续：&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https ://michelleramanovich.github.io/spectron/spectron/librispeech/4/pred.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp ;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td colspan=&quot;2&quot; style=&quot;text-align: left;&quot;>;&lt;h3>;问题解答&lt;/h3>;&lt;/td>;&lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;问题：&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audiocontrols=&quot;控件&quot; src=&quot;https://michelleramanovich.github.io/spectron/spectron/llama_questions/1/Q.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text- align: left;&quot;>;答案：&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;audiocontrols=&quot;controls&quot; src=&quot;https: //michelleramanovich.github.io/spectron/spectron/llama_questions/1/S.wav&quot;>;&lt;/audio>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td>;&amp;nbsp;&lt;/td>;&lt;td>;&amp;nbsp; &lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;问题：&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-对齐：右;&quot;>;&lt;音频控制=“控制”src=“https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/Q.wav”>;&lt;/音频>;&lt;/td>;&lt;/tr >; &lt;tr>;&lt;td style=&quot;text-align: left;&quot;>;答案：&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: right;&quot;>;&lt;音频控制=“控制”src =“https://michelleramanovich.github.io/spectron/spectron/llama_questions/2/S.wav”>;&lt;/音频>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/ table>; &lt;br />; &lt;h2>;性能&lt;/h2>; &lt;p>;为了凭经验评估所提出方法的性能，我们在&lt;a href=&quot;https://arxiv.org/abs/1912.07875&quot;>;上进行了实验Libri-Light 数据集&lt;/a>;。 Libri-Light 是一个 60k 小时的英语数据集，由 LibriVox 有声读物中未标记的语音朗读组成。我们利用名为 &lt;a href=&quot;https://research.google/pubs/pub51736/&quot;>;WaveFit&lt;/a>; 的冻结神经声码器将预测的频谱图转换为原始音频。我们尝试了两项任务：语音延续和口语问答 (QA)。语音延续质量在 LibriSpeech 测试集上进行测试。 Spoken QA 在 Spoken WebQuestions 数据集和我们创建的名为 LLama questions 的新测试集上进行了测试。对于所有实验，我们使用 3 秒的音频提示作为输入。我们将我们的方法与现有的口语模型进行比较：&lt;a href=&quot;https://arxiv.org/abs/2209.03143&quot;>;AudioLM&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2102.01192 &quot;>;GSLM&lt;/a>;、&lt;a href=&quot;https://arxiv.org/abs/2305.13009&quot;>;TWIST&lt;/a>; 和 &lt;a href=&quot;https://arxiv.org/abs/2305.11000&quot;>;语音GPT&lt;/a>;。对于语音延续任务，我们使用 350M 参数版本的 LM，对于口语 QA 任务使用 1B 版本。 &lt;/p>; &lt;p>; 对于语音延续任务，我们使用三个指标来评估我们的方法。第一个是 &lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;>;log-perplexity&lt;/a>;，它使用 LM 来评估生成语音的凝聚力和语义质量。第二个是&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_opinion_score&quot;>;平均意见得分&lt;/a>; (MOS)，它衡量语音对人类评估者来说听起来有多自然。第三种是说话人相似度，使用说话人编码器来测量输出中的说话人与输入中的说话人的相似程度。所有 3 个指标的表现如下图所示。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQX CMkVzuM81oBck4Wb3N1VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/s1200/image5 .png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;396&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_o_DUlZ-28VmNk20zvWBWC5FNl3GMVeigztDAtB4TR5voCdX3zzuCF-6W6DDrMVGZ2szVhyphenhyphendPz1DHvjBrALQXCMkVzuM81oBck4Wb3N1 VH1ndDReAPsfbQv5x7msOD3wMiHeZM-_AcN2ekgSI_G9sF5_u0lrLKc-xxSWc2d9qi6Ubt5cutgQfpIt8uKGr/w640-h396/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/ td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;完成 LibriSpeech 话语的日志困惑度给出 3 秒提示。越低越好。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoVgPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RD qSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/s1200/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot;数据原始宽度=“1200”高度=“396”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjUncRH61ZdMzRhsx9NEXoY0P0jIMkaXhycc9AjB2I0pr8bEWxGNaY0KEF4UG0dm6ZVjRT3_aXlNKny3mmjoV gPveK4rskOgU5sApzqzIXixabv7kMeZNMxUpxs_RDqSsnij-HILgUzVZT39hoBDimw9Ecnd2lMq2ylyRCVtcU_owo8jx9ndVrLUHbGhNT/w640-h396/image4.png&quot;宽度=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;提示语音和使用说话人编码器生成的语音之间的说话人相似度。越高越好。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;tablealign=&quot;center&quot;cellpadding=&quot;0&quot;cellspacing=&quot;0&quot;class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com /img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eVdOcB7ML4FHduIHCuDweZwJiv2USbNqi1EXinzyiwQYM3Sxio3g-61xTBvtb 9rVt_Y4Cvns-vl3V-4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0 “数据原始高度=“742”数据原始宽度=“1200”高度=“396”src=“https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBdPk77at7a-tSVl1cgPTlMsj4tnV391evuyYPzMXxeviG48kljCNy828jCQ5eV dOCB7ML4FHduIHCuDweZwJiv2USBNqi1EXinzyiwQYM3Sxio3g-61xTBvtb9rVt_Y4Cvns-vl3V -4Gvn1RTpBquX3a6W_NNu6u-1DrK_GT5uGXOvP0WivyO7FF2-WTf/w640-h396/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text -align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;人类用户给出的关于语音自然度的 MOS。评估者对给定语音的自然程度进行 5 级主观平均意见得分 (MOS) 评分，范围在 0 - 5 之间。越高越好。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 从第一张图中可以看出，我们的方法在对数困惑度指标上明显优于 GSLM 和 TWIST，并且比最先进的方法 AudioLM 和 SpeechGPT 稍好一些。就 MOS 而言，Spectron 超过了除 AudioLM 之外的所有其他方法的性能。就说话者相似度而言，我们的方法优于所有其他方法。 &lt;/p>; &lt;p>; 为了评估模型执行问答的能力，我们使用两个语音问答数据集。第一个是 LLama Questions 数据集，它使用使用 LLama2 70B LLM 生成的不同领域的一般知识问题。第二个数据集是 &lt;a href=&quot;https://huggingface.co/datasets/web_questions&quot;>;WebQuestions&lt;/a>; 数据集，它是一个通用问答数据集。为了进行评估，我们仅使用适合 3 秒提示长度的问题。为了计算准确性，答案会被转录并与文本形式的真实答案进行比较。 &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi60RnurzgglmP14m9BIScZH4BmupewxydVJ5GZMKclwDgyGc3-zZdj8CK7WXmGR_-pwno1Aql0N_u0ocmftoAlTCJY0 H-1cYU8YTpxZu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/s1200 /image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; height=&quot;第396章Zu5sdIdmG-wZAc-hIwlAbCEVDPgGz4J4a1VMcXWctbjcfCTxsDSZPD69eGfBeGyAxvyYtUX0fE2dhdT5zb71gsJ0/w640-h396/image1.png&quot; width=&quot;640&quot; />;&lt;/ a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;问题回答的准确性在 LLama Questions 和 Spoken WebQuestions 数据集上。准确性是使用语音答案的 ASR 记录来计算的。&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; 首先，我们观察到所有方法在回答 Spoken WebQuestions 中的问题时都更加困难数据集而不是 LLama 问题数据集。其次，我们观察到以口语建模为中心的方法（例如 GSLM、AudioLM 和 TWIST）具有以完成为中心的行为，而不是直接回答问题，这阻碍了它们执行 QA 的能力。在 LLama 问题数据集上，我们的方法优于所有其他方法，而 SpeechGPT 的性能非常接近。在 Spoken WebQuestions 数据集上，我们的方法优于除 SpeechGPT 之外的所有其他方法，SpeechGPT 的表现稍好一些。 &lt;/p>; &lt;br />; &lt;h2>;致谢&lt;/h2>; &lt;p>; &lt;em>;这项工作的直接贡献者包括 Eliya Nachmani、Alon Levkovitch、Julian Salazar、Chulayutsh Asawaroengchai、Soroosh Mariooryad、RJ Skerry-Ryan 和 Michelle塔德莫尔·拉马诺维奇。我们还要感谢甄海嘉、丁一凡、张宇、小泉由马、尼尔·泽吉杜尔、克里斯蒂安·弗兰克、马可·塔利亚萨基、纳达夫·巴尔、本尼·施莱辛格和布莱斯·阿格拉-阿卡斯。 &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7441213378686175839/comments/default&quot; rel=&quot;replies&quot; title=&quot;发表评论&quot; type=&quot;application /atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/spoken-question-answering-and-speech.html#comment-form&quot; rel=&quot;replies&quot; title=&quot; 0 条评论&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml &quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7441213378686175839&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http ://blog.research.google/2023/10/spoken-question-answering-and-speech.html&quot; rel=&quot;alternate&quot; title=&quot;使用声谱图驱动的 LLM 进行口语问答和语音延续&quot; type=&quot;text /html&quot;/>;&lt;作者>;&lt;名称>;Google AI&lt;/名称>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;电子邮件>;noreply@blogger.com&lt;/email>;&lt; gd：图像高度=“16”rel=“http://schemas.google.com/g/2005#thumbnail”src=“https://img1.blogblog.com/img/b16-rounded.gif”宽度= &quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQEWecXit-a9UwHz781_B9s9ZqxsJIGt7ZXx2Lk0bcSQxXBkmLfjhNQxSiq3gqVjiUZ81_17 8hArCQ8nNL0OaVyAi8mKixeWN2PvXTLL4I08ht-eCVqtrTRo36dxGBSDNMdlathtEd4g_qdU3T4ZmPMdGNSXHwlDP689sxzbI4Wwosyu9wp-mjadKqL3MN/ s72-c/Spectron-hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:总计>;&lt;/entry>;&lt;entry>;&lt;id>;标签：blogger.com,1999:blog-8474926331452026626.post-7317378303167958176&lt;/id>;&lt;已发布>;2023-10-25T15:10:00.002-07:00&lt;/已发布>;&lt;更新>;2023-10-26T14:03:20.633-07:00&lt;/更新>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>; &lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom/ns#”术语=“气候”>;&lt;/类别>;&lt;类别方案=“http://www.blogger.com/atom” /ns#&quot; term=&quot;机器学习&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;回顾 2023 年野火研究&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author &quot;>;发布者：Google 研究部软件工程师陈一帆和项目主管 Carla Bromberg&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagw gswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W- 68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s600/WildfireModeling.gif&quot; style=&quot;显示：无；&quot; />; &lt;p>;野火规模越来越大，影响到世界各地越来越多的社区，常常造成大规模破坏。就在今年，&lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Greece_wildfires&quot;>;希腊&lt;/a>;、&lt;a href=&quot;https://en.wikipedia.org /wiki/2023_Hawaii_wildfires#Maui&quot;>;毛伊岛&lt;/a>;和&lt;a href=&quot;https://en.wikipedia.org/wiki/2023_Canadian_wildfires&quot;>;加拿大&lt;/a>;等等。虽然导致这种增长的根本原因很复杂——包括气候模式的变化、森林管理实践、土地利用开发政策等等——但很明显，技术的进步可以帮助应对新的挑战。&lt;/p>; &lt; a name=&#39;more&#39;>;&lt;/a>; &lt;p>;在 Google Research，我们一直在投资多项&lt;a href=&quot;https://research.google/teams/climate-and-sustainability/&quot;>;气候适应工作，包括应用机器学习（ML）来帮助预防野火并在这些事件期间向人们提供信息。例如，为了帮助绘制火灾边界，我们的野火边界&lt;a href=&quot;https://blog.research.google/2023/02/real-time-tracking-of-wildfire.html&quot;>;跟踪器&lt;/a>;使用机器学习模型和卫星图像可近乎实时地绘制大型火灾地图，每 15 分钟更新一次。为了推进我们的各种研究工作，我们正在与世界各地的野火专家和政府机构合作。&lt;/p>; &lt;p>;今天，我们很高兴分享更多关于我们与 &lt;a href=&quot;https://www&quot; 正在进行的合作的信息.fs.usda.gov/&quot;>;美国林务局&lt;/a>; (USFS) 推进火灾建模工具和火势蔓延预测算法。从新开发的 USFS 野火行为模型开始，我们使用机器学习来显着减少计算时间，从而使模型能够近乎实时地使用。这个新模型还能够在预测中纳入局部燃料特征，例如燃料类型和分布。最后，我们描述了新的高保真 3D 火势蔓延模型的早期版本。&lt;/p>; &lt;br />; &lt;h2>;野火建模的当前技术水平&lt;/h2>; &lt;p>;当今使用最广泛的状态 -用于消防操作和培训的最先进的火灾行为模型基于 &lt;a href=&quot;https://www.fs.usda.gov/research/treesearch/55928&quot;>;Rothermel 火灾模型&lt;/a>;，开发于美国林务局消防实验室，由 Rothermel 等人于 20 世纪 70 年代设计。该模型考虑了许多影响火势蔓延的关键因素，如风的影响、地形的坡度、湿度、可燃量（如森林中可燃物质的密度）等，并提供在当时的计算可行性和准确性之间取得了良好的平衡。 Rothermel 模型已在世界各地的消防管理界得到广泛使用。&lt;/p>; &lt;p>;采用 Rothermel 模型的各种操作工具，例如 &lt;a href=&quot;https://www.frames.gov/behaveplus /home&quot;>;行为&lt;/a>;，&lt;a href=&quot;https://www.firelab.org/project/farsite&quot;>;FARSITE&lt;/a>;，&lt;a href=&quot;https://wfdss.usgs.gov /wfdss/pdfs/FSPro.pdf&quot;>;FSPro&lt;/a>; 和 &lt;a href=&quot;https://firelab.org/project/flammap&quot;>;FlamMap&lt;/a>; 经过多年的开发和改进。这些工具和底层模型主要用于三个重要方面：（1）培训消防员和消防管理人员，以培养他们对火灾行为的洞察力和直觉，（2）让火灾行为分析师预测火灾期间火灾的发展操作并为态势感知和资源分配规划提供指导，以及 (3) 分析旨在减轻大面积火灾危险的森林管理方案。这些模型是当今消防作业安全和效率的基础。&lt;/p>; &lt;p>; 然而，这些最先进的模型存在局限性，主要与底层物理过程的简化有关（当创建了这些模型）。通过简化物理过程以产生稳态预测，燃料源和天气所需的输入变得实用，但与可测量的数量相比也更加抽象。因此，这些模型通常由经验丰富的火灾行为分析师进行“调整”和“调整”，以便它们在某些情况下更准确地工作，并补偿不确定性和不可知的环境特征。然而，这些专家的调整意味着许多计算是不可重复的。 &lt;/p>; &lt;p>;为了克服这些限制，USFS 研究人员一直在研究一种新模型，以大幅提高火灾行为预测的物理保真度。这项工作代表了过去 50 年来火灾建模领域的首次重大转变。虽然新模型&lt;a href=&quot;https://ebooks.publish.csiro.au/content/wildland-fire-behaviour&quot;>;在捕获火灾行为方面不断改进&lt;/a>;，但计算成本和推理时间使其部署在现场或具有近实时要求的应用程序是不切实际的。在现实场景中，要使该模型在训练和操作中有用且实用，需要至少 1000 倍的加速。&lt;/p>; &lt;br />; &lt;h2>;机器学习加速&lt;/h2>; &lt;p>; partnership with the USFS, we have undertaken a program to apply ML to decrease computation times for complex fire models. Researchers knew that many complex inputs and features could be characterized using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;>;deep neural network&lt;/a>;, and if successful, the trained model would lower the computational cost and latency of evaluating new scenarios. Deep learning is a branch of machine learning that uses neural networks with multiple hidden layers of nodes that do not directly correspond to actual observations. The model&#39;s hidden layers allow a rich representation of extremely complex systems — an ideal technique for modeling wildfire spread.&lt;/p>; &lt;p>; We used the USFS physics-based, numerical prediction models to generate many simulations of wildfire behavior and then used these simulated examples to train the deep learning model on the inputs and features to best capture the system behavior accurately. We found that the deep learning model can perform at a much lower computational cost compared to the original and is able to address behaviors resulting from fine-scale processes. In some cases, computation time for capturing the fine-scale features described above and providing a fire spread estimate was 100,000 times faster than running the physics-based numerical models. &lt;/p>; &lt;p>;This project has continued to make great progress since the first report at &lt;a href=&quot;https://www.adai.pt/newevent/event/home/index.php?target=home&amp;amp;event=4&amp;amp;defLang=2&quot;>;ICFFR&lt;/a>; in December 2022. The joint Google–USFS &lt;a href=&quot;http://books.uc.pt/chapter?chapter=978989262298921&quot;>;presentation at ICFFR 2022&lt;/a>; and the &lt;a href=&quot;https://www.firelab.org/project/deep-learning-high-resolution-wildfire-modeling&quot;>;USFS Fire Lab&#39;s project page&lt;/a>; provides a glimpse into the ongoing work in this direction. Our team has expanded the dataset used for training by an order of magnitude, from 40M up to 550M training examples. Additionally, we have delivered a prototype ML model that our USFS Fire Lab partner is integrating into a training app that is currently being developed for release in 2024.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/s1999/image2.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdwdwG4NlOjrJlRs25p5yGRUdNDErx7I9aZYLVgoPhrlyiOOx6x8toan0DKoBm2-SG2JfWkZsOluT7g_BYJpDYCTqMRu3cUHqQtyeE-Hgli5j9J3ap2Sg9SrjEfOMXj_CYhbi66iIFwSJjzkii0kJv3VV5V01jbIYFQP-DWjBc9RJKKEoJZPpIZKKYO2TJ/w640-h360/image2.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;Google researchers visiting the USFS Fire Lab in Missoula, MT, stopping by&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://inciweb.nwcg.gov/incident-information/mtfha-big-knife&quot; style=&quot;text-align: left;&quot;>;Big Knife Fire&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;&amp;nbsp;Operation Command Center.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Fine-grained fuel representation&lt;/h2>; &lt;p>;Besides training, another key use-case of the new model is for operational fire prediction. To fully leverage the advantages of the new model&#39;s capability to capture the detailed fire behavior changes from small-scale differences in fuel structures, high resolution fuel mapping and representation are needed. To this end, we are currently working on the integration of high resolution satellite imagery and geo information into ML models to allow fuel specific mapping at-scale. Some of the preliminary results will be presented at the upcoming &lt;a href=&quot;https://afefirecongress.org/schedule/&quot;>;10th International Fire Ecology and Management Congress&lt;/a>; in November 2023.&lt;/p>; &lt;br />; &lt;h2>;Future work&lt;/h2>; &lt;p>; Beyond the collaboration on the new fire spread model, there are many important and challenging problems that can help fire management and safety. Many such problems require even more accurate fire models that fully consider 3D flow interactions and fluid dynamics, thermodynamics and combustion physics. Such detailed calculations usually require high-performance computers (HPCs) or supercomputers. &lt;/p>; &lt;p>; These models can be used for research and longer-term planning purposes to develop insights on extreme fire development scenarios, build ML classification models, or establish a meaningful “danger index” using the simulated results. These high-fidelity simulations can also be used to supplement physical experiments that are used in expanding the operational models mentioned above. &lt;/p>; &lt;p>;In this direction, Google research has also &lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;developed a high-fidelity large-scale 3D fire simulator&lt;/a>; that can be run on Google TPUs. In the near future, there is a plan to further leverage this new capability to augment the experiments, and to generate data to build insights on the development of extreme fires and use the data to design a fire-danger classifier and fire-danger index protocol.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s480/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4SgYHTgu7Uv6WNEJsEJCuFWKolnL13g9BxK2cDuMq7-SRM4dIUK9LbNxy3v0VyY4dU87T_NnehBz4iXCfHCWwoOU6V8-15P55Oi6FvT5BvcLgR_vzYB1xMPWUtj3AiwSsoPZY-9K5i9IwIADDAIjG7hzZZZZ00n7N2jVnjlp65ePQDsJdNIuX6avw8kD_/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;span style=&quot;text-align: left;&quot;>;An example of 3D high-fidelity simulation. This is a controlled burn field experiment (&lt;/span>;&lt;a href=&quot;https://www.fireweather.org/fireflux2&quot; style=&quot;text-align: left;&quot;>;FireFlux II&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;) simulated using&amp;nbsp;&lt;/span>;&lt;a href=&quot;https://www.publish.csiro.au/WF/WF22225&quot;>;Google&#39;s high fidelity fire simulator&lt;/a>;&lt;span style=&quot;text-align: left;&quot;>;.&lt;/span>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>;&lt;i>;We thank Mark Finney, Jason Forthofer, William Chatham and Issac Grenfell from US Forest Service Missoula Fire Science Laboratory and our colleagues John Burge, Lily Hu, Qing Wang, Cenk Gazen, Matthias Ihme,&amp;nbsp;Vivian Yang, Fei Sha and John Anderson for core contributions and useful discussions. We also thank Tyler Russell for his assistance with program management and coordination.&lt;/i>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7317378303167958176/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7317378303167958176&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/looking-back-at-wildfire-research-in.html&quot; rel=&quot;alternate&quot; title=&quot;Looking back at wildfire research in 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCepSXB1QxIgipeUbH1YGd8N3TkVoT1tAgxG0fC0PPREhbsygTQ4i58OVw6-Dt_lagwgswT0jtxUorsVmtyO9UgPEDezJHz_QiKvbJlgYF8Db8W-68KFdyZsq_uvM7YuZo9BRo__NC4BNxD6nHZpzsimeNOaV3X8dh9aliqbAbk8ycXb25s5NfLTfNe42_/s72-c/WildfireModeling.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-420799196308935505&lt;/id>;&lt;published>;2023-10-25T10:45:00.000-07:00&lt;/published>;&lt;updated>;2023-10-25T10:45:37.205-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;EMNLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;NLP&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Search&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Grammar checking at Google Search scale&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Eric Malmi, Senior Research Scientist, and Jakub Adamek, Senior Software Engineer, Google, Bard Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s320/HeroGC.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Many people with questions about grammar turn to Google Search for guidance. While existing features, such as “Did you mean”, already handle simple typo corrections, more complex grammatical error correction (GEC) is beyond their scope. What makes the development of new Google Search features challenging is that they must have high precision and recall while outputting results &lt;a href=&quot;https://ai.googleblog.com/2009/06/speed-matters.html&quot;>;quickly&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; The conventional approach to GEC is to treat it as a &lt;a href=&quot;https://workspace.google.com/blog/ai-and-machine-learning/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;>;translation problem&lt;/a>; and use autoregressive &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;>;Transformer&lt;/a>; models to decode the response token-by-token, conditioning on the previously generated tokens. However, although Transformer models have proven to be effective at GEC, they aren&#39;t particularly efficient because the generation cannot be parallelized due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoregressive_model&quot;>;autoregressive&lt;/a>; decoding. Often, only a few modifications are needed to make the input text grammatically correct, so another possible solution is to treat GEC as a &lt;a href=&quot;https://arxiv.org/abs/2206.07043&quot;>;text editing&lt;/a>; problem. If we could run the autoregressive decoder only to generate the modifications, that would substantially decrease the latency of the GEC model. &lt;/p>; &lt;p>; To this end, in “&lt;a href=&quot;https://aclanthology.org/2022.findings-emnlp.156/&quot;>;EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start&lt;/a>;”, published at Findings of &lt;a href=&quot;https://2022.emnlp.org/&quot; target=&quot;_blank&quot;>;EMNLP 2022&lt;/a>;, we describe a novel text-editing model that is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture. EdiT5 powers the new Google Search &lt;a href=&quot;https://support.google.com/websearch/answer/13420782&quot;>;grammar check feature&lt;/a>; that allows you to check if a phrase or sentence is grammatically correct and provides corrections when needed. Grammar check shows up when the phrase &quot;grammar check&quot; is included in a search query, and if the underlying model is confident about the correction. Additionally, it shows up for some queries that don&#39;t contain the “grammar check” phrase when Search understands that is the likely intent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s1412/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;1412&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-BAXZ3wH-FcJmFoKvppChwvfZay7xH5bhiYmKQocV47sCb3IMfwEec1OpBWY3b1fd2rnA4Vhy4EqOCZiAR4Xpv6xPok9yC-ZqLogbER2lcmaje1NfBCoPAtdW7rEPG22PtNn4dVuYtYH61D9fnx7kudLZilxzUbi01ePhirqtTwMu6g9trhFFuLCDUCgP/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Model architecture&lt;/h2>; &lt;p>; For low-latency applications at Google, Transformer models are typically run on &lt;a href=&quot;https://cloud.google.com/tpu/docs/intro-to-tpu&quot;>;TPUs&lt;/a>;. Due to their fast matrix multiplication units (MMUs), these devices are optimized for performing large matrix multiplications quickly, for example running a Transformer encoder on hundreds of tokens in only a few milliseconds. In contrast, Transformer decoding makes poor use of a TPU&#39;s capabilities, because it forces it to process only one token at a time. This makes autoregressive decoding the most time-consuming part of a translation-based GEC model. &lt;/p>; &lt;p>; In the EdiT5 approach, we reduce the number of decoding steps by treating GEC as a text editing problem. The EdiT5 text-editing model is based on the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>; Transformer encoder-decoder architecture with a few crucial modifications. Given an input with grammatical errors, the EdiT5 model uses an encoder to determine which input tokens to keep or delete. The kept input tokens form a draft output, which is optionally reordered using a non-autoregressive &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf&quot;>;pointer network&lt;/a>;. Finally, a decoder outputs the tokens that are missing from the draft, and uses a pointing mechanism to indicate where each new token should be placed to generate a grammatically correct output. The decoder is only run to produce tokens that were missing in the draft, and as a result, runs for much fewer steps than would be needed in the translation approach to GEC. &lt;/p>; &lt;p>;To further decrease the decoder latency, we reduce the decoder down to a single layer, and we compensate by increasing the size of the encoder. Overall, this decreases latency significantly because the extra work in the encoder is efficiently parallelized.&lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s1878/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1332&quot; data-original-width=&quot;1878&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2_MGEcdadlbXM1zMBQzROuRbYyqmcdhcytlnwdHeIJ_M0wrtioaRixoDOxMlE3acuMZRf4KB_T5TgzUCfBBmz3eeEMnnAKHZkvFmMzFnNu_2UAI1l3jhOp2dyQyOxElXwt7iEjun_gd77uk1TMiaBLlMfxEOxcFo8W48fdD9WqbYNvUZECUEd-LHTk1-R/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an input with grammatical errors (“Guess when was I borned”), the EdiT5 model uses an encoder to determine which input tokens to keep (K) or delete (D), a pointer network (pointer) to reorder kept tokens, and a decoder to insert any new tokens that are needed to generate a grammatically correct output.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We applied the EdiT5 model to the &lt;a href=&quot;https://aclanthology.org/W19-4406/&quot;>;public BEA grammatical error correction benchmark&lt;/a>;, comparing different model sizes. The experimental results show that an EdiT5 large model with 391M parameters yields a higher &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F0.5 score&lt;/a>;, which measures the accuracy of the corrections, while delivering a 9x speedup compared to a T5 base model with 248M parameters. The mean latency of the EdiT5 model was merely 4.1 milliseconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s1987/image1.jpeg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1311&quot; data-original-width=&quot;1987&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQys5EavCOAEs0dy3M8r5Ar-9v7n5qtFkek1VSfeGBXH1pQHNv6Ebem3vDedO60BlNcp3HLGqYtsT4v4Evz2u2ksTgFfQcQIqd87NPTxVzYVyzvA85Vg_jO8a18KrMT2hOQQA5hg2frOJx2L7S5SM88VBCJzd13ClrSzHnl5xg29TMMpvqA4Yl-d98vTc1/s16000/image1.jpeg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of the T5 and EdiT5 models of various sizes on the public BEA GEC benchmark plotted against mean latency. Compared to T5, EdiT5 offers a better latency-F0.5 trade-off. Note that the x axis is logarithmic.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Improved training data with large language models&lt;/h2>; &lt;p>; Our &lt;a href=&quot;https://arxiv.org/abs/2106.03830&quot;>;earlier research&lt;/a>;, as well as the results above, show that model size plays a crucial role in generating accurate grammatical corrections. To combine the advantages of large language models (LLMs) and the low latency of EdiT5, we leverage a technique called &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;hard distillation&lt;/a>;. First, we train a teacher LLM using similar datasets used for the &lt;a href=&quot;https://ai.googleblog.com/2021/10/grammar-correction-as-you-type-on-pixel.html&quot;>;Gboard grammar model&lt;/a>;. The teacher model is then used to generate training data for the student EdiT5 model. &lt;/p>; &lt;p>; Training sets for grammar models consist of &lt;em>;ungrammatical source / grammatical target&lt;/em>; sentence pairs. Some of the training sets have noisy targets that contain grammatical errors, unnecessary paraphrasing, or unwanted artifacts. Therefore, we generate new pseudo-targets with the teacher model to get cleaner and more consistent training data. Then, we re-train the teacher model with the pseudo-targets using a technique called &lt;em>;&lt;a href=&quot;https://arxiv.org/abs/1909.13788&quot;>;self-training&lt;/a>;&lt;/em>;. Finally, we found that when the source sentence contains many errors, the teacher sometimes corrects only part of the errors. Thus, we can further improve the quality of the pseudo-targets by feeding them to the teacher LLM for a second time, a technique called &lt;em>;&lt;a href=&quot;https://aclanthology.org/D19-1435/&quot;>;iterative refinement&lt;/a>;&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s1089/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1089&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJzgocbj86SS8moSqWrxIKz8E3CxZn712Di2CY7pwgXZ355Z4qUnDBxuEc91AU9jAdKwq4tvyaCzIqKfcJzThrjkka5q1LwXGWNa1xPi5AU7RpJDq-XmoNHs386B2bJqKDDnN8_l1mvLCSShlGjb66RWy25Z_1QXwMDpHDgsPON7r0pUqYdXZLPsx06hOC/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Steps for training a large teacher model for grammatical error correction (GEC). Self-training and iterative refinement remove unnecessary paraphrasing, artifacts, and grammatical errors appearing in the original targets.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Putting it all together&lt;/h2>; &lt;p>; Using the improved GEC data, we train two EdiT5-based models: a grammatical error correction model, and a grammaticality classifier. When the grammar check feature is used, we run the query first through the correction model, and then we check if the output is indeed correct with the classifier model. Only then do we surface the correction to the user. &lt;/p>; &lt;p>; The reason to have a separate classifier model is to more easily trade off between precision and recall. Additionally, for ambiguous or nonsensical queries to the model where the best correction is unclear, the classifier reduces the risk of serving erroneous or confusing corrections. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We have developed an efficient grammar correction model based on the state-of-the-art EdiT5 model architecture. This model allows users to check for the grammaticality of their queries in Google Search by including the “grammar check” phrase in the query. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We gratefully acknowledge the key contributions of the other team members, including Akash R, Aliaksei Severyn, Harsh Shah, Jonathan Mallinson, Mithun Kumar SR, Samer Hassan, Sebastian Krause, and Shikhar Thakur. We&#39;d also like to thank Felix Stahlberg, Shankar Kumar, and Simon Tong for helpful discussions and pointers.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/420799196308935505/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/420799196308935505&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html&quot; rel=&quot;alternate&quot; title=&quot;Grammar checking at Google Search scale&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW7pWsW1-E7U1LH1bxmjBL_2W5fS6uN2iRb2cNPBks57ubvFNJj-SjE2Zk1lFqndKWBVAeO3g3MLeJ96QUIUNJDyLTcWjeO835NT6HfJMYQBEdGqL-X_sxQ1yxMy16a0OcOLb6gSz2lqM6VofToVtN_s_F_wGB41AZwlj146y7ZXQ4PFsdywX10QO54MJ4/s72-c/HeroGC.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2376347423475108178&lt;/id>;&lt;published>;2023-10-20T10:07:00.000-07:00&lt;/published>;&lt;updated>;2023-10-20T10:07:14.082-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;distributed systems&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Answering billions of reporting queries each day with low latency&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jagan Sankaranarayanan, Senior Staff Software Engineer, and Indrajit Roy, Head of Napa Product, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIZSvPVFwZPeTv8ivB-lHMGfoLkP-fDBuAy9GEklUVNBPPoqOXRfme5Psui7DbKssImVjFHtxoygKhFBvgpAG_C5852ocu9i7AOfWPeC1mSlaim8jfqsV55wZIULDUPk7WhxW1OISfL_CjZswN3CcZN7GJgVLBdepic8lfYAUCT0rAlXGGbnf-WuA6mRc6/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://ads.google.com/home/&quot;>;Google Ads&lt;/a>; infrastructure runs on an internal data warehouse called &lt;a href=&quot;https://research.google/pubs/pub50617/&quot;>;Napa&lt;/a>;. Billions of reporting queries, which power critical dashboards used by advertising clients to measure campaign performance, run on tables stored in Napa. These tables contain records of ads performance that are keyed using particular customers and the campaign identifiers with which they are associated. Keys are tokens that are used both to associate an ads record with a particular client and campaign (eg, customer_id, campaign_id) and for efficient retrieval. A record contains dozens of keys, so clients use reporting queries to specify keys needed to filter the data to understand ads performance (eg, by region, device and metrics such as clicks, etc.). What makes this problem challenging is that the data is &lt;em>;skewed&lt;/em>; since queries require varying levels of effort to be answered and have stringent latency expectations. Specifically, some queries require the use of millions of records while others are answered with just a few. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To this end, in “&lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;Progressive Partitioning for Parallelized Query Execution in Napa&lt;/a>;”, presented at &lt;a href=&quot;https://vldb.org/2023/&quot;>;VLDB 2023&lt;/a>;, we describe how the Napa data warehouse determines the amount of machine resources needed to answer reporting queries while meeting strict latency targets. We introduce a new progressive query partitioning algorithm that can &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_computing&quot;>;parallelize&lt;/a>; query execution in the presence of complex data skews to perform consistently well in a matter of a few milliseconds. Finally, we demonstrate how Napa allows Google Ads infrastructure to serve billions of queries every day. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Query processing challenges&lt;/h2>; &lt;p>; When a client inputs a reporting query, the main challenge is to determine how to parallelize the query effectively. Napa&#39;s parallelization technique breaks up the query into even sections that are equally distributed across available machines, which then process these in parallel to significantly reduce query latency. This is done by estimating the number of records associated with a specified key, and assigning more or less equal amounts of work to machines. However, this estimation is not perfect since reviewing all records would require the same effort as answering the query. A machine that processes significantly more than others would result in run-time skews and poor performance. Each machine also needs to have sufficient work since needless parallelism leads to underutilized infrastructure. Finally, parallelization has to be a per query decision that must be executed near-perfectly billions of times, or the query may miss the stringent latency requirements. &lt;/p>; &lt;p>; The reporting query example below extracts the records denoted by keys (ie, &lt;code>;customer_id&lt;/code>; and &lt;code>;campaign_id&lt;/code>;) and then computes an aggregate (ie, &lt;code>;SUM(cost)&lt;/code>;) from an advertiser table. In this example the number of records is too large to process on a single machine, so Napa needs to use a subsequent key (eg, &lt;code>;adgroup_id&lt;/code>;) to further break up the collection of records so that equal distribution of work is achieved. It is important to note that at petabyte scale, the size of the data statistics needed for parallelization may be several terabytes. This means that the problem is not just about collecting enormous amounts of metadata, but also how it is managed. &lt;/p>; &lt;span style=&quot;font-size: small;&quot;>; &lt;pre class=&quot;prettyprint&quot; style=&quot;margin-left: 40px; margin-right: 40px; white-space: pre-wrap;&quot;>; SELECT customer_id, campaign_id, SUM(cost) FROM advertiser_table WHERE customer_id in (1, 7, ..., x ) AND campaign_id in (10, 20, ..., y) GROUP BY customer_id, campaign_id; &lt;/pre>;&lt;/span>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;This reporting query example extracts records denoted by keys (ie, &lt;code>;customer_id&lt;/code>; and &lt;code>;campaign_id&lt;/code>;) and then computes an aggregate (ie, &lt;code>;SUM(cost)&lt;/code>;) from an advertiser table. The query effort is determined by the keys&#39; included in the query. Keys belonging to clients with larger campaigns may touch millions of records since the data volume directly correlates with the size of the ads campaign. This disparity of matching records based on keys reflects the &lt;em>;skewness&lt;/em>; in data, which makes query processing a challenging problem.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; An effective solution minimizes the amount of metadata needed, focuses effort primarily on the skewed part of the key space to partition data efficiently, and works well within the allotted time. For example, if the query latency is a few hundred milliseconds, partitioning should take no longer than tens of milliseconds. Finally, a parallelization process should determine when it&#39;s reached the best possible partitioning that considers query latency expectations. To this end, we have developed a progressive partitioning algorithm that we describe later in this article. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Managing the data deluge&lt;/h2>; &lt;p>; Tables in Napa are constantly updated, so we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;>;log-structured merge forests&lt;/a>; (LSM tree) to organize the deluge of table updates. LSM is a forest of sorted data that is temporally organized with a &lt;a href=&quot;https://en.wikipedia.org/wiki/B-tree&quot;>;B-tree index&lt;/a>; to support efficient key lookup queries. B-trees store summary information of the sub-trees in a hierarchical manner. Each B-tree node records the number of entries present in each subtree, which aids in the parallelization of queries. LSM allows us to &lt;a href=&quot;https://research.google/pubs/pub50617/&quot;>;decouple&lt;/a>; the process of updating the tables from the mechanics of query serving in the sense that live queries go against a different version of the data, which is atomically updated once the next batch of ingest (called delta) has been fully prepared for querying. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrdgCgdHtjKomZM08fuF3qd6UWrgHHYvTgclc2iMV1UffAAMqVgsJVhGDSP4aPzPh4-kNb0hU7ZWkOAaf37k7PaTaH4k3m45fDzBaX9x0wxRSpvPADOXIDrtdTL62lFGJKzqEaDT6oz-fPxbkSxU3omqQAm8sWKmsA8SR4wHHKh6iAXbDGcU4a_WvEHsaL/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;667&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrdgCgdHtjKomZM08fuF3qd6UWrgHHYvTgclc2iMV1UffAAMqVgsJVhGDSP4aPzPh4-kNb0hU7ZWkOAaf37k7PaTaH4k3m45fDzBaX9x0wxRSpvPADOXIDrtdTL62lFGJKzqEaDT6oz-fPxbkSxU3omqQAm8sWKmsA8SR4wHHKh6iAXbDGcU4a_WvEHsaL/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>;&lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The partitioning problem&lt;/h2>; &lt;p>; The data partitioning problem in our context is that we have a massively large table that is represented as an LSM tree. In the figure below, Delta 1 and 2 each have their own B-tree, and together represent 70 records. Napa breaks the records into two pieces, and assigns each piece to a different machine. The problem becomes a partitioning problem of a forest of trees and requires a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tree_traversal&quot;>;tree-traversal algorithm&lt;/a>; that can quickly split the trees into two equal parts. &lt;/p>; &lt;p>; To avoid visiting all the nodes of the tree, we introduce the concept of “good enough” partitioning. As we begin cutting and partitioning the tree into two parts, we maintain an estimate of how bad our current answer would be if we terminated the partitioning process at that instant. This is the yardstick of how close we are to the answer and is represented below by a total error margin of 40 (at this point of execution, the two pieces are expected to be between 15 and 35 records in size, the uncertainty adds up to 40). Each subsequent traversal step reduces the error estimate, and if the two pieces are approximately equal, it stops the partitioning process. This process continues until the desired error margin is reached, at which time we are guaranteed that the two pieces are more or less equal. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP1MUE1iWt7K2hKPBmTmr4LfIgARhI3a-nKsMqrcmiOejHLJbxDyKIoV-9ZF8k9_X4w9EMLikeVOufzTTmBGMqBKdH_AyrWz_UihzrZFsaNPWFXB2dD8o2RIBbXCM0shFN8a6fUMW0g830qrJx0Q3mpOLfzyQwwuYB9nilFeKt3VfKQYde9Eq5VmN8D-2i/s473/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;327&quot; data-original-width=&quot;473&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjP1MUE1iWt7K2hKPBmTmr4LfIgARhI3a-nKsMqrcmiOejHLJbxDyKIoV-9ZF8k9_X4w9EMLikeVOufzTTmBGMqBKdH_AyrWz_UihzrZFsaNPWFXB2dD8o2RIBbXCM0shFN8a6fUMW0g830qrJx0Q3mpOLfzyQwwuYB9nilFeKt3VfKQYde9Eq5VmN8D-2i/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Progressive partitioning algorithm&lt;/h2>; &lt;p>; Progressive partitioning encapsulates the notion of “good enough” in that it makes a series of moves to reduce the error estimate. The input is a set of B-trees and the goal is to cut the trees into pieces of more or less equal size. The algorithm traverses one of the trees (“drill down&#39;&#39; in the figure) which results in a reduction of the error estimate. The algorithm is guided by statistics that are stored with each node of the tree so that it makes an informed set of moves at each step. The challenge here is to decide how to direct effort in the best possible way so that the error bound reduces quickly in the fewest possible steps. Progressive partitioning is conducive for our use-case since the longer the algorithm runs, the more equal the pieces become. It also means that if the algorithm is stopped at any point, one still gets good partitioning, where the quality corresponds to the time spent. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsnglKJOlArk8nIcelyOi8Yej5O2OUWCwWzitU9PsT5h-0zWDyvpVhxrdMsbsB0ECvC_q6Y91peJ6Q2r5HjXJWsH47LSEDODJw1lm-aOt8lpMhcTyMU2LOf7m2KcykAnyRLFrpx_I95spiJi5qNecVwlJ7x_aRhSDTCON6G_o09WeN8x4BwQeXbJOBOzFW/s390/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;281&quot; data-original-width=&quot;390&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsnglKJOlArk8nIcelyOi8Yej5O2OUWCwWzitU9PsT5h-0zWDyvpVhxrdMsbsB0ECvC_q6Y91peJ6Q2r5HjXJWsH47LSEDODJw1lm-aOt8lpMhcTyMU2LOf7m2KcykAnyRLFrpx_I95spiJi5qNecVwlJ7x_aRhSDTCON6G_o09WeN8x4BwQeXbJOBOzFW/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Prior work in this space uses a &lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;sampled table to drive the partitioning process&lt;/a>;, while the Napa approach uses a B-tree. As mentioned earlier, even just a sample from a petabyte table can be massive. A tree-based partitioning method can achieve partitioning much more efficiently than a sample-based approach, which does not use a tree organization of the sampled records. We compare progressive partitioning with an alternative approach, where sampling of the table at various resolutions (eg, 1 record sample every 250 MB and so on) aids the partitioning of the query. Experimental results show the relative speedup from progressive partitioning for queries requiring varying numbers of machines. These results demonstrate that progressive partitioning is much faster than existing approaches and the speedup increases as the size of the query increases. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3V3INsImX_TRgcEa2VFXMK2LtVwNyCjzqYco_QMSTQWMAIfb1D5rpF7iM4xFesoltnJI9RUQNVkuuZv7nMjDIfjzhIklmdcj1xl9B8uBMfDKLIhQ9UXgH1Vrgrf2xoNvHMrv0icZDI_PKQLo9ZA6bCzrlyvX3eayjxCH_IZjFxZ1Hy9atHm6oEUqFhZz7/s1452/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1192&quot; data-original-width=&quot;1452&quot; height=&quot;525&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3V3INsImX_TRgcEa2VFXMK2LtVwNyCjzqYco_QMSTQWMAIfb1D5rpF7iM4xFesoltnJI9RUQNVkuuZv7nMjDIfjzhIklmdcj1xl9B8uBMfDKLIhQ9UXgH1Vrgrf2xoNvHMrv0icZDI_PKQLo9ZA6bCzrlyvX3eayjxCH_IZjFxZ1Hy9atHm6oEUqFhZz7/w640-h525/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Napa&#39;s progressive partitioning algorithm efficiently optimizes database queries, enabling Google Ads to serve client reporting queries billions of times each day. We note that tree traversal is a common technique that students in introductory computer science courses use, yet it also serves a critical use-case at Google. We hope that this article will inspire our readers, as it demonstrates how simple techniques and carefully designed data structures can be remarkably potent if used well. Check out the &lt;a href=&quot;https://research.google/pubs/pub52572/&quot;>;paper&lt;/a>; and a &lt;a href=&quot;https://www.youtube.com/watch?v=dtWwUWB5JyQ&quot;>;recent talk&lt;/a>; describing Napa to learn more. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This blog post describes a collaborative effort between Junichi Tatemura, Tao Zou, Jagan Sankaranarayanan, Yanlai Huang, Jim Chen, Yupu Zhang, Kevin Lai, Hao Zhang, Gokul Nath Babu Manoharan, Goetz Graefe, Divyakant Agrawal, Brad Adelberg, Shilpa Kolhar and Indrajit Roy.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2376347423475108178/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/answering-billions-of-reporting-queries.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2376347423475108178&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2376347423475108178&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/answering-billions-of-reporting-queries.html&quot; rel=&quot;alternate&quot; title=&quot;Answering billions of reporting queries each day with low latency&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIZSvPVFwZPeTv8ivB-lHMGfoLkP-fDBuAy9GEklUVNBPPoqOXRfme5Psui7DbKssImVjFHtxoygKhFBvgpAG_C5852ocu9i7AOfWPeC1mSlaim8jfqsV55wZIULDUPk7WhxW1OISfL_CjZswN3CcZN7GJgVLBdepic8lfYAUCT0rAlXGGbnf-WuA6mRc6/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3886518375378801095&lt;/id>;&lt;published>;2023-10-19T09:24:00.002-07:00&lt;/published>;&lt;updated>;2023-10-19T09:43:21.465-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Education&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Search&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Speech&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;English learners can now practice speaking on Search&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Christian Plagemann, Director, and Katya Cox, Product Manager, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfHhrdHOFN2-ZoUw8hxN1Gdmd5WBtL5kVUe27TeajsI3nrES8Ah13W7MIKpZ2avrsxFTdkzk-sD1noswtk20cMyPS8GjDnVMPquyxc6EhR9r53bMzncRYlj5ZBM3jMF15ndusFp2fe9ipy4bksiTLfWJ1umdcUrQUxg78SOtXfjqMbSxW_OQsSTAVZ9HV6/s600/Tivoli.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Learning a language can open up new opportunities in a person&#39;s life. It can help people connect with those from different cultures, travel the world, and advance their career. English alone is estimated to have 1.5 billion learners worldwide. Yet proficiency in a new language is difficult to achieve, and many learners cite a lack of opportunity to practice speaking actively and receiving actionable feedback as a barrier to learning. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; We are excited to announce a new feature of Google Search that helps people practice speaking and improve their language skills. Within the next few days, Android users in Argentina, Colombia, India (Hindi), Indonesia, Mexico, and Venezuela can get even more language support from Google through interactive speaking practice in English — expanding to more countries and languages in the future. Google Search is already a valuable tool for language learners, providing translations, definitions, and other resources to improve vocabulary. Now, learners translating &lt;em>;to&lt;/em>; or &lt;em>;from&lt;/em>; English on their Android phones will find a new English speaking practice experience with personalized feedback. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRV7Q3FKxiVB0qI6W7mk7mf_kZ_I6H_FKhzhrHcxHh5rYUNFa4E59XqRG7u0KpGX6lgvz6ihkD8fddSlHbWI18YkN9BF-KoHnyXLZdXqMdGApRNffllQR8neHSI8hFFDzeBpSkvyLjNb5_u7MJPIIIEPBpskiR_IfeN7YWXpz46seBFjEpIjghLs4-lBj2/s939/image9.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;939&quot; data-original-width=&quot;428&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRV7Q3FKxiVB0qI6W7mk7mf_kZ_I6H_FKhzhrHcxHh5rYUNFa4E59XqRG7u0KpGX6lgvz6ihkD8fddSlHbWI18YkN9BF-KoHnyXLZdXqMdGApRNffllQR8neHSI8hFFDzeBpSkvyLjNb5_u7MJPIIIEPBpskiR_IfeN7YWXpz46seBFjEpIjghLs4-lBj2/w292-h640/image9.gif&quot; width=&quot;292&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A new feature of Google Search allows learners&lt;br />;to practice speaking words in context.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Learners are presented with real-life prompts and then form their own spoken answers using a provided vocabulary word. They engage in practice sessions of 3-5 minutes, getting personalized feedback and the option to sign up for daily reminders to keep practicing. With only a smartphone and some quality time, learners can practice at their own pace, anytime, anywhere. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Activities with personalized feedback, to supplement existing learning tools&lt;/h2>; &lt;p>; Designed to be used alongside other learning services and resources, like personal tutoring, mobile apps, and classes, the new speaking practice feature on Google Search is another tool to assist learners on their journey. &lt;/p>; &lt;p>; We have partnered with linguists, teachers, and &lt;a href=&quot;https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language&quot;>;ESL/EFL&lt;/a>; pedagogical experts to create a speaking practice experience that is effective and motivating. Learners practice vocabulary in authentic contexts, and material is repeated over dynamic intervals to increase retention — approaches that are known to be effective in helping learners become confident speakers. As one partner of ours shared: &lt;/p>; &lt;div style=&quot;margin-left: 40px;&quot;>; &lt;p>; &lt;em>;&quot;Speaking in a given context is a skill that language learners often lack the opportunity to practice. Therefore this tool is very useful to complement classes and other resources.&quot; - Judit Kormos, Professor, Lancaster University&lt;/em>; &lt;/p>; &lt;/div>; &lt;p>; We are also excited to be working with several language learning partners to surface content they are helping create and to connect them with learners around the world. We look forward to expanding this program further and working with any interested partner. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Personalized real-time feedback &lt;/h2>; &lt;p>; Every learner is different, so delivering personalized feedback in real time is a key part of effective practice. Responses are analyzed to provide helpful, real-time suggestions and corrections.&lt;br />; &lt;/p>; &lt;p>; The system gives &lt;em>;semantic feedback&lt;/em>;, indicating whether their response was relevant to the question and may be understood by a conversation partner. &lt;em>;Grammar feedback&lt;/em>; provides insights into possible grammatical improvements, and a set of &lt;em>;example answers&lt;/em>; at varying levels of language complexity give concrete suggestions for alternative ways to respond in this context. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjEZsUjl4VVmcpQhX43ZJgAtH2S46YWrmpYQuoNvtTMovHKXVPCGoDqDVlyRv_XLzgDWfEWTlbGhTsQO-EvaSJC2K5rnxi5xEoM2amjSMVUKwP13r8DDurLkYyIFTcS7yzL7n11rZiCsfeRx0lLnIueV66BmAn9GSelvSCg27IOZNfqvc2JanmrGI1hKE5/s929/image8.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;929&quot; data-original-width=&quot;428&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjEZsUjl4VVmcpQhX43ZJgAtH2S46YWrmpYQuoNvtTMovHKXVPCGoDqDVlyRv_XLzgDWfEWTlbGhTsQO-EvaSJC2K5rnxi5xEoM2amjSMVUKwP13r8DDurLkYyIFTcS7yzL7n11rZiCsfeRx0lLnIueV66BmAn9GSelvSCg27IOZNfqvc2JanmrGI1hKE5/w294-h640/image8.gif&quot; width=&quot;294&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The feedback is composed of three elements: Semantic analysis, grammar correction, and example answers.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Contextual translation&lt;/h3>; &lt;p>; Among the several new technologies we developed, contextual translation provides the ability to translate individual words and phrases &lt;em>;in context&lt;/em>;. During practice sessions, learners can tap on any word they don&#39;t understand to see the translation of that word considering its context. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfGcZuy2godKFrUvo8R6jmayMNo4pT7C3lBLuApdsJxIxqmaLfXuoQjk3E4Hs5UK5xFrb03QeABCSDgphyphenhyphencnCy8-smihT_WUZ5nPLqPjpGIM4yMhL9PnqqDG5ZsyHqLjag_uxBTIJZqw4w0lkfoF8iJAerlP2tRcE10D9f6RhSIuej6cVP9pl8xjmhiqz/s1999/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;949&quot; data-original-width=&quot;1999&quot; height=&quot;190&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwfGcZuy2godKFrUvo8R6jmayMNo4pT7C3lBLuApdsJxIxqmaLfXuoQjk3E4Hs5UK5xFrb03QeABCSDgphyphenhyphencnCy8-smihT_WUZ5nPLqPjpGIM4yMhL9PnqqDG5ZsyHqLjag_uxBTIJZqw4w0lkfoF8iJAerlP2tRcE10D9f6RhSIuej6cVP9pl8xjmhiqz/w400-h190/image3.jpg&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of contextual translation feature.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This is a difficult technical task, since individual words in isolation often have multiple alternative meanings, and multiple words can form clusters of meaning that need to be translated in unison. Our novel approach translates the entire sentence, then estimates how the words in the original and the translated text relate to each other. This is commonly known as the &lt;a href=&quot;https://aclanthology.org/J03-1002/&quot;>;word alignment problem&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipp97I_J7eGvrIEjtu71XmBw2GDbNTOtF4lEBnI0kP1B7fK541JKlVfle0Qv8lh3ME7rOoAwOCH26TlvsogltG_krPLLoEBRsYLKfHSFq9w28cNisWL7VVU126PxSj3r2VdGJs1Tg3SJ8aLxVFTAFzqL_KPhlVZ4baUTeex7-pUzmRyFQIMZrtfR2DJErM/s1059/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1059&quot; data-original-width=&quot;796&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipp97I_J7eGvrIEjtu71XmBw2GDbNTOtF4lEBnI0kP1B7fK541JKlVfle0Qv8lh3ME7rOoAwOCH26TlvsogltG_krPLLoEBRsYLKfHSFq9w28cNisWL7VVU126PxSj3r2VdGJs1Tg3SJ8aLxVFTAFzqL_KPhlVZ4baUTeex7-pUzmRyFQIMZrtfR2DJErM/w301-h400/image1.gif&quot; width=&quot;301&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Example of a translated sentence pair and its word alignment. A deep learning alignment model connects the different words that create the meaning to suggest a translation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The key technology piece that enables this functionality is a novel &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;>;deep learning&lt;/a>; model developed in collaboration with the Google Translate team, called Deep Aligner. The basic idea is to take a multilingual language model trained on hundreds of languages, then fine-tune a novel alignment model on a set of word alignment examples (see the figure above for an example) provided by human experts, for several language pairs. From this, the single model can then accurately align any language pair, reaching state-of-the-art alignment error rate (AER, a metric to measure the quality of word alignments, where lower is better). This single new model has led to dramatic improvements in alignment quality across all tested language pairs, reducing average AER from 25% to 5% compared to alignment approaches based on &lt;a href=&quot;https://aclanthology.org/C96-2141/&quot;>;Hidden Markov models&lt;/a>; (HMMs). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjlqhOWugi22TAIKRsT6IcyjdZaWRP17p1C80QywrJfNSRqcpwiUmOJw_Km3EEu9f-E3o8ZiH6DKlzk8Yv4xrk-CFmlsVBMoQYf3Zp5UZDF6SD_cUsY2nA9uBBFsbPBz6s5j6YOL1Q0Fx4Kt-eKpyqnP2RdGhYfKLIxjDyOPPuISBwmruGdnxUP5NdPYkF/s1084/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;374&quot; data-original-width=&quot;1084&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjlqhOWugi22TAIKRsT6IcyjdZaWRP17p1C80QywrJfNSRqcpwiUmOJw_Km3EEu9f-E3o8ZiH6DKlzk8Yv4xrk-CFmlsVBMoQYf3Zp5UZDF6SD_cUsY2nA9uBBFsbPBz6s5j6YOL1Q0Fx4Kt-eKpyqnP2RdGhYfKLIxjDyOPPuISBwmruGdnxUP5NdPYkF/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Alignment error rates (lower is better) between English (EN) and other languages.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; This model is also incorporated into Google&#39;s translation APIs, greatly improving, for example, the formatting of translated PDFs and websites in Chrome, the translation of YouTube captions, and enhancing Google Cloud&#39;s translation API. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Grammar feedback &lt;/h3>; &lt;p>; To enable grammar feedback for accented spoken language, our research teams adapted grammar correction models for written text (see the &lt;a href=&quot;https://workspace.google.com/blog/ai-and-machine-learning/using-neural-machine-translation-to-correct-grammatical-in-google-docs&quot;>;blog&lt;/a>; and &lt;a href=&quot;https://aclanthology.org/N19-1333/&quot;>;paper&lt;/a>;) to work on automatic speech recognition (ASR) transcriptions, specifically for the case of accented speech. The key step was fine-tuning the written text model on a corpus of human and ASR transcripts of accented speech, with expert-provided grammar corrections. Furthermore, inspired by &lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.418/&quot;>;previous work&lt;/a>;, the teams developed a novel edit-based output representation that leverages the high overlap between the inputs and outputs that is particularly well-suited for short input sentences common in language learning settings. &lt;/p>; &lt;p>; The edit representation can be explained using an example: &lt;/p>; &lt;ul>; &lt;li>;&lt;em>;Input&lt;/em>;: I&lt;sup>;1&lt;/sup>; am&lt;sup>;2&lt;/sup>; so&lt;sup>;3&lt;/sup>; bad&lt;sup>;4&lt;/sup>; cooking&lt;sup>;5&lt;/sup>; &lt;/li>;&lt;li>;&lt;em>;Correction&lt;/em>;: I&lt;sup>;1&lt;/sup>; am&lt;sup>;2&lt;/sup>; so&lt;sup>;3&lt;/sup>; bad&lt;sup>;4&lt;/sup>; at&lt;sup>;5&lt;/sup>; cooking&lt;sup>;6&lt;/sup>; &lt;/li>;&lt;li>;&lt;em>;Edits&lt;/em>;: (&#39;at&#39;, 4, PREPOSITION, 4) &lt;/li>; &lt;/ul>; &lt;p>; In the above, “at” is the word that is inserted at position 4 and “PREPOSITION” denotes this is an error involving prepositions. We used the error tag to select tag-dependent acceptance thresholds that improved the model further. The model increased the recall of grammar problems from 4.6% to 35%. &lt;/p>; &lt;p>; Some example output from our model and a model trained on written corpora: &lt;/p>; &lt;br>; &lt;table style=&quot;text-align: center&quot;>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;Example 1 &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td>;Example 2 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left&quot;>; User input (transcribed speech) &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I live of my profession.&lt;/em>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I need a efficient card and reliable.&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left&quot;>; Text-based grammar model &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I live by my profession.&lt;/em>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I need an efficient card and a reliable.&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left&quot;>; New speech-optimized model &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I live off my profession.&lt;/em>; &lt;/td>; &lt;td>; &lt;/td>; &lt;td>;&lt;em>;I need an efficient and reliable card.&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Semantic analysis&lt;/h3>; &lt;p>; A primary goal of conversation is to communicate one&#39;s intent clearly. Thus, we designed a feature that visually communicates to the learner whether their response was relevant to the context and would be understood by a partner. This is a difficult technical problem, since early language learners&#39; spoken responses can be syntactically unconventional. We had to carefully balance this technology to focus on the clarity of intent rather than correctness of syntax. &lt;/p>; &lt;p>; Our system utilizes a combination of two approaches: &lt;/p>; &lt;ol>; &lt;li>;&lt;em>;Sensibility&lt;/em>; classification: Large language models like &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;>;LaMDA&lt;/a>; or &lt;a href=&quot;https://blog.research.google/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;PaLM&lt;/a>; are designed to give natural responses in a conversation, so it&#39;s no surprise that they do well on the reverse: judging whether a given response is contextually sensible. &lt;/li>;&lt;li>;&lt;em>;Similarity&lt;/em>; to good responses: We used an &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf&quot;>;encoder architecture&lt;/a>; to compare the learner&#39;s input to a set of known good responses in a semantic embedding space. This comparison provides another useful signal on semantic relevance, further improving the quality of feedback and suggestions we provide.&lt;br />; &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjnk1fQyMbEd1RoCpXD7st3-gFzcFlqF-02BbDilw5b2vG5IuZrs_IOkV0nbFCQXNVslsjxsgnKKSHLxHiDUN4riv4xuDGRnrxQe2O3CUhgcuWNzTxJhVMykcZOO0UZTCHCc0S5vG-VimzA_jZ8hrPBSN8KtUqfOpTNYyE6AXYPNOLZgq8ytF08y6X2GB7/s533/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;533&quot; data-original-width=&quot;428&quot; height=&quot;400&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjnk1fQyMbEd1RoCpXD7st3-gFzcFlqF-02BbDilw5b2vG5IuZrs_IOkV0nbFCQXNVslsjxsgnKKSHLxHiDUN4riv4xuDGRnrxQe2O3CUhgcuWNzTxJhVMykcZOO0UZTCHCc0S5vG-VimzA_jZ8hrPBSN8KtUqfOpTNYyE6AXYPNOLZgq8ytF08y6X2GB7/w321-h400/image4.gif&quot; width=&quot;321&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The system provides feedback about whether the response was relevant to the prompt, and would be understood by a communication partner.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;ML-assisted content development&lt;/h2>; &lt;p>; Our available practice activities present a mix of human-expert created content, and content that was created with AI assistance and human review. This includes speaking prompts, focus words, as well as sets of example answers that showcase meaningful and contextual responses. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacP_kIzsIz7Aqz_3hnuQsYMxeZ95DXI7ezmYnTvYOM3LXBNYR7mUJApgtw7BDLS1sSEAbjoc760T-D7gaZ8SAeo9qK40AUHkdpfiUJIItvdhagT2B-InWhoFomzhddu6ueT7gyDQcIIzrGGcZXmNjxfFo42VxQpmqkkgGS8dOh7NE3fPYZUwnGH4WhdMe/s1284/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1263&quot; data-original-width=&quot;1284&quot; height=&quot;394&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgacP_kIzsIz7Aqz_3hnuQsYMxeZ95DXI7ezmYnTvYOM3LXBNYR7mUJApgtw7BDLS1sSEAbjoc760T-D7gaZ8SAeo9qK40AUHkdpfiUJIItvdhagT2B-InWhoFomzhddu6ueT7gyDQcIIzrGGcZXmNjxfFo42VxQpmqkkgGS8dOh7NE3fPYZUwnGH4WhdMe/w400-h394/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A list of example answers is provided when the learner receives feedback and when they tap the help button.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Since learners have different levels of ability, the language complexity of the content has to be adjusted appropriately. Prior work on language complexity estimation focuses on text of &lt;a href=&quot;https://cris.fbk.eu/handle/11582/329866&quot;>;paragraph length&lt;/a>; or &lt;a href=&quot;https://amontgomerie.github.io/2021/03/14/cefr-level-prediction.html&quot;>;longer&lt;/a>;, which differs significantly from the type of responses that our system processes. Thus, we developed novel models that can estimate the complexity of a single sentence, phrase, or even individual words. This is challenging because even a phrase composed of simple words can be hard for a language learner (eg, &quot;Let&#39;s cut to the chase”). Our best model is based on &lt;a href=&quot;https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;>;BERT&lt;/a>; and achieves complexity predictions closest to human expert consensus. The model was pre-trained using a large set of LLM-labeled examples, and then fine-tuned using a human expert–labeled dataset. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2ZhobYkx2Zfqj4BtN-5UkvVEeLlXLTkLxDAjF_fbdBNmT48F4O0MGm6u70837x4cqNlO2uNBdd4yLVFKzFgi9ox0_F27J3eHyuZUyeFKUc2mNcuiI51-iHJIKObdFGlDkZ2QnGQV4IwYIDCgvRYpXOT_4bbRaRItkFtypYSTeAv90v2R4fjUl059Bgqf/s1134/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;546&quot; data-original-width=&quot;1134&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2ZhobYkx2Zfqj4BtN-5UkvVEeLlXLTkLxDAjF_fbdBNmT48F4O0MGm6u70837x4cqNlO2uNBdd4yLVFKzFgi9ox0_F27J3eHyuZUyeFKUc2mNcuiI51-iHJIKObdFGlDkZ2QnGQV4IwYIDCgvRYpXOT_4bbRaRItkFtypYSTeAv90v2R4fjUl059Bgqf/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;Mean squared error&lt;/a>; of various approaches&#39; performance estimating content difficulty on a diverse corpus of ~450 conversational passages (text / transcriptions). &lt;b>;Top row:&lt;/b>; Human raters labeled the items on a scale from 0.0 to 5.0, roughly aligned to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Common_European_Framework_of_Reference_for_Languages&quot;>;CEFR scale&lt;/a>; (from A1 to C2). &lt;b>;Bottom four rows&lt;/b>;: Different models performed the same task, and we show the difference to the human expert consensus.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Using this model, we can evaluate the difficulty of text items, offer a diverse range of suggestions, and most importantly challenge learners appropriately for their ability levels. For example, using our model to label examples, we can fine-tune our system to generate speaking prompts at various language complexity levels. &lt;/p>; &lt;br>; &lt;table>; &lt;tbody>;&lt;tr>; &lt;td>; &lt;/td>; &lt;td style=&quot;text-align: center&quot; colspan=&quot;6&quot;>;Vocabulary focus words, to be elicited by the questions &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;guitar &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;apple &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;lion &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Simple &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What do you like to play?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you like fruit?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you like big cats?&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Intermediate &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you play any musical instruments?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What is your favorite fruit?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What is your favorite animal?&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;td style=&quot;text-align: center;height:15px&quot;>;&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;Complex &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;What stringed instrument do you enjoy playing?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Which type of fruit do you enjoy eating for its crunchy texture and sweet flavor?&lt;/em>; &lt;/td>; &lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td>; &lt;td style=&quot;text-align: center&quot;>;&lt;em>;Do you enjoy watching large, powerful predators?&lt;/em>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br>; &lt;p>; Furthermore, content difficulty estimation is used to gradually increase the task difficulty over time, adapting to the learner&#39;s progress. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; With these latest updates, which will roll out over the next few days, Google Search has become even more helpful. If you are an Android user in India (Hindi), Indonesia, Argentina, Colombia, Mexico, or Venezuela, give it a try by translating &lt;em>;to&lt;/em>; or &lt;em>;from&lt;/em>; English with Google. &lt;/p>; &lt;p>; We look forward to expanding to more countries and languages in the future, and to start offering partner practice content soon. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;Many people were involved in the development of this project. Among many others, we thank our external advisers in the language learning field: Jeffrey Davitz, Judit Kormos, Deborah Healey, Anita Bowles, Susan Gaer, Andrea Revesz, Bradley Opatz, and Anne Mcquade.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3886518375378801095/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-search-can-now-help-with-english-speaking-practice.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3886518375378801095&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3886518375378801095&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-search-can-now-help-with-english-speaking-practice.html&quot; rel=&quot;alternate&quot; title=&quot;English learners can now practice speaking on Search&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfHhrdHOFN2-ZoUw8hxN1Gdmd5WBtL5kVUe27TeajsI3nrES8Ah13W7MIKpZ2avrsxFTdkzk-sD1noswtk20cMyPS8GjDnVMPquyxc6EhR9r53bMzncRYlj5ZBM3jMF15ndusFp2fe9ipy4bksiTLfWJ1umdcUrQUxg78SOtXfjqMbSxW_OQsSTAVZ9HV6/s72-c/Tivoli.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2371558111773883902&lt;/id>;&lt;published>;2023-10-18T14:05:00.000-07:00&lt;/published>;&lt;updated>;2023-10-18T14:05:40.676-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Measurement-induced entanglement phase transitions in a quantum circuit&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Jesse Hoke, Student Researcher, and Pedram Roushan, Senior Research Scientist, Quantum AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjD0d2Z3JPDRdCIvFvlT3VvnLnv7sx7HndEKEnek1t_g84zs__aAh3c2TAG5hPEPYjtjJM8hikDRiSHREYGrW_TrSbHeWVC6eCq6lN2nv4ZVqmtAEg2lTyK1G26q1T-Vo9TUL9YCxVD1tG_Q8hiWBU_cbvIUo_NWrs12hPFTvbJvML4x-OU_RBzidMcwAc_/s1100/order_param_blog_1200.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Quantum mechanics allows many phenomena that are classically impossible: a quantum particle can exist in a superposition of two states simultaneously or be entangled with another particle, such that anything you do to one seems to instantaneously also affect the other, regardless of the space between them. But perhaps no aspect of quantum theory is as striking as the act of measurement. In classical mechanics, a measurement need not affect the system being studied. But a measurement on a quantum system can profoundly influence its behavior. For example, when a quantum bit of information, called a qubit, that is in a superposition of both “0” and “1” is measured, its state will suddenly collapse to one of the two classically allowed states: it will be either “0” or “1,” but not both. This transition from the quantum to classical worlds seems to be facilitated by the act of measurement. How exactly it occurs is one of the fundamental unanswered questions in physics. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In a large system comprising many qubits, the effect of measurements can cause new phases of quantum information to emerge. Similar to how changing parameters such as temperature and pressure can cause a phase transition in water from liquid to solid, tuning the strength of measurements can induce a phase transition in the entanglement of qubits. &lt;/p>; &lt;p>; Today in “&lt;a href=&quot;https://www.nature.com/articles/s41586-023-06505-7&quot;>;Measurement-induced entanglement and teleportation on a noisy quantum processor&lt;/a>;”, published in &lt;em>;&lt;a href=&quot;https://www.nature.com/&quot;>;Nature&lt;/a>;&lt;/em>;, we describe experimental observations of measurement-induced effects in a system of 70 qubits on our &lt;a href=&quot;https://quantumai.google/hardware&quot;>;Sycamore quantum processor&lt;/a>;. This is, by far, the largest system in which such a phase transition has been observed. Additionally, we detected &quot;quantum teleportation&quot; — when a quantum state is transferred from one set of qubits to another, detectable even if the details of that state are unknown — which emerged from measurements of a random circuit. We achieved this breakthrough by implementing a few clever “tricks” to more readily see the signatures of measurement-induced effects in the system. &lt;/p>; &lt;br />; &lt;h2>;Background: Measurement-induced entanglement&lt;/h2>; &lt;p>; Consider a system of qubits that start out independent and unentangled with one another. If they interact with one another , they will become entangled. You can imagine this as a web, where the strands represent the entanglement between qubits. As time progresses, this web grows larger and more intricate, connecting increasingly disparate points together. &lt;/p>; &lt;p>; A full measurement of the system completely destroys this web, since every entangled superposition of qubits collapses when it&#39;s measured. But what happens when we make a measurement on only a few of the qubits? Or if we wait a long time between measurements? During the intervening time, entanglement continues to grow. The web&#39;s strands may not extend as vastly as before, but there are still patterns in the web. &lt;/p>; &lt;p>; There is a balancing point between the strength of interactions and measurements, which compete to affect the intricacy of the web. When interactions are strong and measurements are weak, entanglement remains robust and the web&#39;s strands extend farther, but when measurements begin to dominate, the entanglement web is destroyed. We call the crossover between these two extremes the &lt;em>;measurement-induced phase transition&lt;/em>;. &lt;/p>; &lt;p>; In our quantum processor, we observe this measurement-induced phase transition by varying the relative strengths between interactions and measurement. We induce interactions by performing entangling operations on pairs of qubits. But to actually see this web of entanglement in an experiment is notoriously challenging. First, we can never actually look at the strands connecting the qubits — we can only infer their existence by seeing statistical correlations between the measurement outcomes of the qubits. So, we need to repeat the same experiment many times to infer the pattern of the web. But there&#39;s another complication: the web pattern is different for each possible measurement outcome. Simply averaging all of the experiments together without regard for their measurement outcomes would wash out the webs&#39; patterns. To address this, some previous experiments used “post-selection,” where only data with a particular measurement outcome is used and the rest is thrown away. This, however, causes an exponentially decaying bottleneck in the amount of “usable” data you can acquire. In addition, there are also practical challenges related to the difficulty of mid-circuit measurements with superconducting qubits and the presence of noise in the system. &lt;/p>; &lt;br />; &lt;h2>;How we did it&lt;/h2>; &lt;p>; To address these challenges, we introduced three novel tricks to the experiment that enabled us to observe measurement-induced dynamics in a system of up to 70 qubits. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Trick 1: Space and time are interchangeable&lt;/h3>; &lt;p>; As counterintuitive as it may seem, interchanging the roles of space and time dramatically reduces the technical challenges of the experiment. Before this “space-time duality” transformation, we would have had to interleave measurements with other entangling operations, frequently checking the state of selected qubits. Instead, after the transformation, we can postpone all measurements until after all other operations, which greatly simplifies the experiment. As implemented here, this transformation turns the original 1-spatial-dimensional circuit we were interested in studying into a 2-dimensional one. Additionally, since all measurements are now at the end of the circuit, the relative strength of measurements and entangling interactions is tuned by varying the number of entangling operations performed in the circuit. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjo5-hDtmBK9Odeg2d1KpiDe_fyn3e1yK0hAcYacblUt-B9-bWo9GrGUWTAdeC9jMsURYwDADlYlRm2RB9YpT6pHtaMqahpwZGMqVVuREn_J9Fi88FKFg-oiZzsW8W14HntQjXrSktkRZ3gXDyTcRUFJ_cYsk6VTrwETric5XpDK7QNhl_UGxZzc7KbmO9P/s4346/fig1_blog_1200.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;2555&quot; data-original-width=&quot;4346&quot; height=&quot;376&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjo5-hDtmBK9Odeg2d1KpiDe_fyn3e1yK0hAcYacblUt-B9-bWo9GrGUWTAdeC9jMsURYwDADlYlRm2RB9YpT6pHtaMqahpwZGMqVVuREn_J9Fi88FKFg-oiZzsW8W14HntQjXrSktkRZ3gXDyTcRUFJ_cYsk6VTrwETric5XpDK7QNhl_UGxZzc7KbmO9P/w640-h376/fig1_blog_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Exchanging space and time. To avoid the complication of interleaving measurements into our experiment (shown as gauges in the&amp;nbsp;&lt;strong>;left&lt;/strong>;&amp;nbsp;panel), we utilize a space-time duality mapping to exchange the roles of space and time. This mapping transforms the 1D circuit (&lt;strong>;left&lt;/strong>;) into a 2D circuit (&lt;strong>;right&lt;/strong>;), where the circuit depth (T) now tunes the effective measurement rate.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Trick 2: Overcoming the post-selection bottleneck&lt;/h3>; &lt;p>; Since each combination of measurement outcomes on all of the qubits results in a unique web pattern of entanglement, researchers often use post-selection to examine the details of a particular web. However, because this method is very inefficient, we developed a new “decoding” protocol that compares each instance of the real “web” of entanglement to the same instance in a classical simulation. This avoids post-selection and is sensitive to features that are common to all of the webs. This common feature manifests itself into a combined classical–quantum “order parameter”, akin to the &lt;a href=&quot;https://quantumai.google/cirq/noise/qcvv/xeb_theory#:~:text=Cross%20entropy%20benchmarking%20uses%20the,performance%20of%20a%20large%20device.&quot;>;cross-entropy benchmark&lt;/a>; used in the random circuit sampling used in our &lt;a href=&quot;https://blog.research.google/2019/10/quantum-supremacy-using-programmable.html&quot;>;beyond-classical demonstration&lt;/a>;. &lt;/p>; &lt;p>; This order parameter is calculated by selecting one of the qubits in the system as the “probe” qubit, measuring it, and then using the measurement record of the nearby qubits to classically “decode” what the state of the probe qubit should be. By cross-correlating the measured state of the probe with this “decoded” prediction, we can obtain the entanglement between the probe qubit and the rest of the (unmeasured) qubits. This serves as an order parameter, which is a proxy for determining the entanglement characteristics of the entire web. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIbaxw2gTCg4rZzWx3HpDcwwVtlqe6ES7EzW5VKPhyQECZK2aVgUa0TcFI8s7Ovn41ZDobntlrRVJ3J84cAu_fQRUx4vCqZD7VDQegfA_P9LBQR6Yl64g-0Cp8pur0E4858K-Vl2UKwFZ5rar4NVHDBTL7R2Nkxy-xsAOxmI_1gBrE8FQRRvjYlkdLO9U3/s2320/order_param_blog_1200.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1760&quot; data-original-width=&quot;2320&quot; height=&quot;486&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIbaxw2gTCg4rZzWx3HpDcwwVtlqe6ES7EzW5VKPhyQECZK2aVgUa0TcFI8s7Ovn41ZDobntlrRVJ3J84cAu_fQRUx4vCqZD7VDQegfA_P9LBQR6Yl64g-0Cp8pur0E4858K-Vl2UKwFZ5rar4NVHDBTL7R2Nkxy-xsAOxmI_1gBrE8FQRRvjYlkdLO9U3/w640-h486/order_param_blog_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;In the decoding procedure we choose a “probe” qubit (pink) and classically compute its expected value, conditional on the measurement record of the surrounding qubits (yellow). The order parameter is then calculated by the cross correlation between the measured probe bit and the classically computed value.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Trick 3: Using noise to our advantage&lt;/h3>; &lt;p>; A key feature of the so-called “disentangling phase” — where measurements dominate and entanglement is less widespread — is its insensitivity to noise. We can therefore look at how the probe qubit is affected by noise in the system and use that to differentiate between the two phases. In the disentangling phase, the probe will be sensitive only to local noise that occurs within a particular area near the probe. On the other hand, in the entangling phase, any noise in the system can affect the probe qubit. In this way, we are turning something that is normally seen as a nuisance in experiments into a unique probe of the system. &lt;/p>; &lt;br />; &lt;h2>;What we saw&lt;/h2>; &lt;p>; We first studied how the order parameter was affected by noise in each of the two phases. Since each of the qubits is noisy, adding more qubits to the system adds more noise. Remarkably, we indeed found that in the disentangling phase the order parameter is unaffected by adding more qubits to the system. This is because, in this phase, the strands of the web are very short, so the probe qubit is only sensitive to the noise of its nearest qubits. In contrast, we found that in the entangling phase, where the strands of the entanglement web stretch longer, the order parameter is very sensitive to the size of the system, or equivalently, the amount of noise in the system. The transition between these two sharply contrasting behaviors indicates a transition in the entanglement character of the system as the “strength” of measurement is increased. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7Z96NKSvSsw83CeFO_C3NDha5dk069lVQQIhY0w9KLzNk-uGpuy2x4B3dWTdd5WK_gu31FP4uSOPHvZT0HbKJWC3K_-YfMKUpEzFZPKdEPJLgfAkDkSgB3d5kSn5Xnevus-0lvvH6s19Zt4ix5H-qjdMixurIH7uew-9rmsXV6ke39BQS8BpgHPhp9eP-/s2934/noise_1200.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1840&quot; data-original-width=&quot;2934&quot; height=&quot;401&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7Z96NKSvSsw83CeFO_C3NDha5dk069lVQQIhY0w9KLzNk-uGpuy2x4B3dWTdd5WK_gu31FP4uSOPHvZT0HbKJWC3K_-YfMKUpEzFZPKdEPJLgfAkDkSgB3d5kSn5Xnevus-0lvvH6s19Zt4ix5H-qjdMixurIH7uew-9rmsXV6ke39BQS8BpgHPhp9eP-/w640-h401/noise_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Order parameter vs. gate density (number of entangling operations) for different numbers of qubits. When the number of entangling operations is low, measurements play a larger role in limiting the entanglement across the system. When the number of entangling operations is high, entanglement is widespread, which results in the dependence of the order parameter on system size (inset).&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In our experiment, we also demonstrated a novel form of quantum teleportation that arises in the entangling phase. Typically, a specific set of operations are necessary to implement quantum teleportation, but here, the teleportation emerges from the randomness of the non-unitary dynamics. When all qubits, except the probe and another system of far away qubits, are measured, the remaining two systems are strongly entangled with each other. Without measurement, these two systems of qubits would be too far away from each other to know about the existence of each other. With measurements, however, entanglement can be generated faster than the limits typically imposed by locality and causality. This “measurement-induced entanglement” between the qubits (that must also be aided with a classical communications channel) is what allows for quantum teleportation to occur. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgj7SwfWiDaUv0pwEPs9frmRcXUnOVzLTQTfR4CkxZeBFCJITsGB5Bm66mF978UAa_b73UGBKFara6m_LjhVOCfOtxpG17sPBp94_nVrNT2OuNKLBLU0Ntl11WEatAI2aJ2U34gkAKoMfQwxpxPcDxGIXnF8Sy9DeX0rZ9AsikPyI7KXeBWF9515HajWJnX/s2969/teleport_1200.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1754&quot; data-original-width=&quot;2969&quot; height=&quot;378&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgj7SwfWiDaUv0pwEPs9frmRcXUnOVzLTQTfR4CkxZeBFCJITsGB5Bm66mF978UAa_b73UGBKFara6m_LjhVOCfOtxpG17sPBp94_nVrNT2OuNKLBLU0Ntl11WEatAI2aJ2U34gkAKoMfQwxpxPcDxGIXnF8Sy9DeX0rZ9AsikPyI7KXeBWF9515HajWJnX/w640-h378/teleport_1200.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Proxy entropy vs. gate density for two far separated subsystems (pink and black qubits) when all other qubits are measured. There is a finite-size crossing at ~0.9. Above this gate density, the probe qubit is entangled with qubits on the opposite side of the system and is a signature of the teleporting phase.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Our experiments demonstrate the effect of measurements on a quantum circuit. We show that by tuning the strength of measurements, we can induce transitions to new phases of quantum entanglement within the system and even generate an emergent form of quantum teleportation. This work could potentially have relevance to quantum computing schemes, where entanglement and measurements both play a role. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was done while Jesse Hoke was interning at Google from Stanford University. We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2371558111773883902/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/measurement-induced-entanglement-phase.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2371558111773883902&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2371558111773883902&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/measurement-induced-entanglement-phase.html&quot; rel=&quot;alternate&quot; title=&quot;Measurement-induced entanglement phase transitions in a quantum circuit&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjD0d2Z3JPDRdCIvFvlT3VvnLnv7sx7HndEKEnek1t_g84zs__aAh3c2TAG5hPEPYjtjJM8hikDRiSHREYGrW_TrSbHeWVC6eCq6lN2nv4ZVqmtAEg2lTyK1G26q1T-Vo9TUL9YCxVD1tG_Q8hiWBU_cbvIUo_NWrs12hPFTvbJvML4x-OU_RBzidMcwAc_/s72-c/order_param_blog_1200.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-524891095627230116&lt;/id>;&lt;published>;2023-10-16T10:12:00.000-07:00&lt;/published>;&lt;updated>;2023-10-16T10:12:58.579-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Collaboration&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Research&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Improving traffic evacuations: A case study&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Damien Pierce, Software Engineer, and John Anderson, Senior Research Director, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiq0gQYBrF4sBptEnwyqdXMyAuYpCBTZJf7JBevRgt8ZuEtflAdzaNbenGSfItI98BCpXDMpUkO3nEJyDs4XTVaCmpGrqBsLGL1E11V6-QbB629_OL_IrMhX1AfJRZIbL_O9AufL5o-kbESrDz6y_zJbP7Kj6MTn_RLQ8B7hOSlbKguvdd6jjthWNRTPZYl/s1700/trafficevac.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Some cities or communities develop an evacuation plan to be used in case of an emergency. There are a number of reasons why city officials might enact their plan, a primary one being a natural disaster, such as a tornado, flood, or wildfire. An evacuation plan can help the community more effectively respond to an emergency, and so could help save lives. However, it can be difficult for a city to evaluate such a plan because it is not practical to have an entire town or city rehearse a full blown evacuation. For example, Mill Valley, a city in northern California, created a wildfire evacuation plan but lacked an estimate for how long the evacuation would take. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today we describe a case study in which we teamed up with the city of Mill Valley to test and improve their evacuation plan. We outline our approach in our paper, “&lt;a href=&quot;https://arxiv.org/abs/2307.07108&quot;>;Mill Valley Evacuation Study&lt;/a>;”. We started by using a traffic simulator to model a citywide evacuation. The research goal was to provide the city with detailed estimates for how long it would take to evacuate the city, and, by studying the egress pattern, to find modifications to make the plan more effective. While our &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2590198220301214&quot;>;prior work&lt;/a>; on this subject provided an estimate for the evacuation time and showed how the time could be reduced if certain road changes were implemented, it turns out the recommendations in that paper — such as changing the number of outgoing lanes on an arterial — were not feasible. The current round of research improves upon the initial study by more accurately modeling the number and starting locations of vehicles, by using a more realistic map, and by working closely with city officials to ensure that recommended changes to the plan are deemed viable. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>;&lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Geography and methodology&lt;/h2>; &lt;p>; Mill Valley is in Marin County, California, north of San Francisco. Many of the residences are located on the steep hillsides of several valleys surrounded by dense redwood forests. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-rEeqPv7eEzcaCjLSS_ch9Nzuwe6cVoFjxFVDO2b-SSyRGrBZv7qybrvcSVyIsiDCTsi6ZthJRz2fESC1eHmSmrEo7aGReWESSakOtYqLfiz0usyE40XQA0Jn_RXXKHrIyWDNvIfQoBeWuT-QDacbz0kx0Wr8jKo9TryGf2drFfPdbSWW8VxRuF0eGD9h/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-rEeqPv7eEzcaCjLSS_ch9Nzuwe6cVoFjxFVDO2b-SSyRGrBZv7qybrvcSVyIsiDCTsi6ZthJRz2fESC1eHmSmrEo7aGReWESSakOtYqLfiz0usyE40XQA0Jn_RXXKHrIyWDNvIfQoBeWuT-QDacbz0kx0Wr8jKo9TryGf2drFfPdbSWW8VxRuF0eGD9h/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm-LcwnvHIYfP0WbN0YrA_LsgYRbOrXsP6_wkjU_y0b5NQROw2uPHr9VrJw3uCXzdm5DIDRgHIcdi2VjICkX3_mIYHN7WF1eyf3CjWxD1NJHh6DBaafwaYarEE-NlcqkaCemu2yghi7rUHtjWa6jrLCDKO97rWKtjsBvnVWeQENvU0sx7ne0xQI7M_el-H/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1125&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm-LcwnvHIYfP0WbN0YrA_LsgYRbOrXsP6_wkjU_y0b5NQROw2uPHr9VrJw3uCXzdm5DIDRgHIcdi2VjICkX3_mIYHN7WF1eyf3CjWxD1NJHh6DBaafwaYarEE-NlcqkaCemu2yghi7rUHtjWa6jrLCDKO97rWKtjsBvnVWeQENvU0sx7ne0xQI7M_el-H/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Aerial views of Mill Valley, courtesy of the City of Mill Valley. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Many of those residences are in areas that have only one exit direction, toward the town center. From there the best evacuation route is toward Highway 101, which is in the flat part of the city and is the most likely area to be far from potential wildfires. Some neighborhoods have other routes that lead away from both the city and Highway 101, but those routes pass through hilly forested areas, which could be dangerous or impassable during a wildfire. So, the evacuation plan directs all vehicles west of Highway 101 to head east, to the highway (see map below). The neighborhoods east of Highway 101 are not included in the simulation because they are away from areas with a high fire hazard rating, and are close to the highway. &lt;/p>; &lt;p>; Mill Valley has about 11,400 households west of Highway 101. Most Mill Valley households have two vehicles. Evacuation times scale with the number of vehicles, so it is in the common interest to minimize the number of vehicles used during an evacuation. To that end, Mill Valley has a public awareness campaign aimed at having each household evacuate in one vehicle. While no one knows how many vehicles would be used during an evacuation, it is safe to assume it is on average between one and two per household. The basic evacuation problem, then, is how to efficiently get between 11 and 23 thousand vehicles from the various residences onto one of the three sets of Highway 101 on-ramps. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwsFEZpO2_VYt-i0FHVg3C4Jn_PMQMKMl37lNWYQTXEskYSyy4KTpVmTne00MASABWnes1eLqE0XN16dG0WN9WrvKSVL_4dbB9e5NZchWh9VOVLhzxUjfCEJ1kuAlCM5FVXLy8ydwI-SqhC40_VkOBj1rqTRBgjPdFEM6JRZqSPcwE5apKPQL-x7qdWf9u/s549/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;549&quot; data-original-width=&quot;536&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwsFEZpO2_VYt-i0FHVg3C4Jn_PMQMKMl37lNWYQTXEskYSyy4KTpVmTne00MASABWnes1eLqE0XN16dG0WN9WrvKSVL_4dbB9e5NZchWh9VOVLhzxUjfCEJ1kuAlCM5FVXLy8ydwI-SqhC40_VkOBj1rqTRBgjPdFEM6JRZqSPcwE5apKPQL-x7qdWf9u/w624-h640/image3.png&quot; width=&quot;624&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The simulated part of Mill Valley west of Highway 101 is inside the blue border. Highway 101 is shown in green. The red squares indicate the three sets of Highway 101 on-ramps. The pink area has the highest fire hazard rating.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The current work uses the same general methodology as the previous research, namely, running the &lt;a href=&quot;https://en.wikipedia.org/wiki/Simulation_of_Urban_MObility&quot;>;open source SUMO agent-based traffic simulator&lt;/a>; on a map of Mill Valley. The traffic simulator models traffic by simulating each vehicle individually. The detailed behaviors of vehicles are dictated by a &lt;a href=&quot;https://sumo.dlr.de/docs/Car-Following-Models.html&quot;>;car-following model&lt;/a>;. Each vehicle is given a point and time at which to start and an initial route. The routes of most vehicles are updated throughout the simulation, depending on conditions. To consider potential changes in driver behavior under the high stress conditions of an evacuation, the effects of the “aggressiveness” of each car is also investigated, but in our case the impacts are minimal. Some simplifying assumptions are that vehicles originate at residential addresses and the roads and highways are initially empty. These assumptions correspond approximately to conditions that could be encountered if an evacuation happens in the middle of the night. The main inputs in the simulation are the road network, the household locations, the average number of vehicles per household, and a departure temporal distribution. We have to make assumptions about the departure distribution. After discussing with the city officials, we chose a distribution such that most vehicles depart within an hour. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Four bottlenecks&lt;/h2>; &lt;p>; Mill Valley has three sets of Highway 101 on-ramps: northern, middle, and southern. All the vehicles must use one of these sets of on-ramps to reach their destination (either the northernmost or southernmost segment of Highway 101 included in our map). Given that we are only concerned with the majority of Mill Valley that lies west of the highway, there are two lanes that approach the northern on-ramps, and one lane that approaches each of the middle and southern on-ramps. Since every vehicle has to pass over one of these four lanes to reach the highway, they are the bottlenecks. Given the geography and existing infrastructure, adding more lanes is infeasible. The aim of this research, then, is to try to modify traffic patterns to maximize the rate of traffic on each of the four lanes. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evacuation plan&lt;/h2>; &lt;p>; When we started this research, Mill Valley had a preliminary evacuation plan. It included modifying traffic patterns — disabling traffic lights and changing traffic rules — on a few road segments, as well as specifying the resources (traffic officers, signage) necessary to implement the changes. As an example, a two-way road may be changed to a one-way road to double the number of outgoing lanes. Temporarily changing the direction of traffic is called &lt;em>;contraflow&lt;/em>;. &lt;/p>; &lt;p>; The plot below shows the simulated fraction of vehicles that have departed or reached their destinations versus time, for 1, 1.5, and 2 vehicles per household (left to right). The dashed line on the far left shows the fraction that have departed. The solid black lines show the preliminary evacuation plan results and the dotted lines indicate the normal road network (baseline) results. The preliminary evacuation plan significantly speeds up the evacuation. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGFqAZbQu6Nt75U1UzXpyDmD0Hei9HCOEY-AeGExQQjIp62ubwFI0LqatEBGYtzwECiHgxtFRgZfDBeaNmFhV-WDM4vo1V0W_UJTh59oJdfjC6IusT3QZZJPcr2BAirIFp5MNe_O1_uu0EzNUFIAMJH0u_K4dmesGxEKmjejv4NMAp6DLwNlfS4mUUJ6_6/s812/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;425&quot; data-original-width=&quot;812&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGFqAZbQu6Nt75U1UzXpyDmD0Hei9HCOEY-AeGExQQjIp62ubwFI0LqatEBGYtzwECiHgxtFRgZfDBeaNmFhV-WDM4vo1V0W_UJTh59oJdfjC6IusT3QZZJPcr2BAirIFp5MNe_O1_uu0EzNUFIAMJH0u_K4dmesGxEKmjejv4NMAp6DLwNlfS4mUUJ6_6/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The cumulative fraction of vehicles vs. time in hours. The demand curve is shown in the dashed line on the far left. The solid lines show the preliminary evacuation plan curves for 1, 1.5 and 2 vehicles per household (left to right). The dotted lines show the same for the baseline case.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We can understand how effective the preliminary evacuation plan is by measuring the rates at the bottlenecks. The below plots show the rate of traffic on each of the four lanes leading to the highway on-ramps for the case of 1.5 vehicles per household for both the baseline case (the normal road rules; shown shaded in gray) and the preliminary evacuation plan (shown outlined in black). The average rate per lane varies greatly in the different cases. It is clear that, while the evacuation plan leads to increased evacuation rates, there is room for improvement. In particular, the middle on-ramps are quite underutilized.&lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilGFPn9SRnlDgD_btIeFw0BM2YqNp9kSivDA0OuGqrezvaDrjyrCKalxWyzlK2HGwkzKzDEJNHKBDKp9YEpvi8AyN20raA_H88JQY2YQ96Bzx4oGEk_dXcadtyhyphenhyphenx5SVJqiLpSEwIq9Vgy9aqhKX4YiIRWwCje2LsfaiEjs1jZvahUsv-xuJV3oQyCWmFD/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1500&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilGFPn9SRnlDgD_btIeFw0BM2YqNp9kSivDA0OuGqrezvaDrjyrCKalxWyzlK2HGwkzKzDEJNHKBDKp9YEpvi8AyN20raA_H88JQY2YQ96Bzx4oGEk_dXcadtyhyphenhyphenx5SVJqiLpSEwIq9Vgy9aqhKX4YiIRWwCje2LsfaiEjs1jZvahUsv-xuJV3oQyCWmFD/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The rates of traffic on the four lanes leading to Highway 101 on-ramps for both the baseline case (normal road rules; shown shaded in gray) and the preliminary evacuation plan (shown outlined in black).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Final evacuation plan&lt;/h2>; &lt;p>; After studying the map and investigating different alternatives, we, working together with city officials, found a minimal set of new road changes that substantially lower the evacuation time compared to the preliminary evacuation plan (shown below). We call this the final evacuation plan. It extends the contraflow section of the preliminary plan 1000 feet further west, to a main intersection. Crucially, this allows for one of the (normally) two outgoing lanes to be dedicated to routing traffic to the middle on-ramps. It also creates two outgoing lanes from that main intersection clear through to the northern on-ramps, over ¾ of a mile to the east. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVZzJcpphq9-XvpGma97D-9IPEsZ-7A_k_j9809MEEwtuI5te14hozh7VAe1i9xH8iOv8TMwErCBoK_Rxfux3GURoOqDMhNFW5VoEMl5bAII5gkoWrjFfdWlSvMhxl2hdbYb00JDAZTA-yiFsXo7Sm057QEia81G-otuKy1lZF04u_PFWY_hroXxTmtDka/s1510/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;705&quot; data-original-width=&quot;1510&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVZzJcpphq9-XvpGma97D-9IPEsZ-7A_k_j9809MEEwtuI5te14hozh7VAe1i9xH8iOv8TMwErCBoK_Rxfux3GURoOqDMhNFW5VoEMl5bAII5gkoWrjFfdWlSvMhxl2hdbYb00JDAZTA-yiFsXo7Sm057QEia81G-otuKy1lZF04u_PFWY_hroXxTmtDka/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A map of the main changes in the final evacuation plan. The red line shows that traffic heading north on Camino Alto gets diverted to the middle Highway 101 on-ramps. The blue line shows traffic in the northern lane of E Blithedale Ave gets routed on the new contraflow section.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The rate per lane plots comparing the preliminary and final evacuation plans are shown below for 1.5 vehicles per household. The simulation indicates that the final plan increases the average rate of traffic on the lane leading to the middle on-ramps from about 4 vehicles per minute to about 18. It also increases the through rate of the northern on-ramps by over 60%. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEip473wwGuslqnXu9FY1DsHgVfUbRC-9Unt1yONvUPBfQ0ekAdYVOZYFLWPjS4qtilW61kQJNBa_MKWYju4CJKabqrxu21KIQGGT0rxhOFNQrFIi46PY08lBiZNMRNnpwWozNGKbjynNdUDO5YdlxWocijTh4cvBkgh_jmsYE9PgFAWFDTbem_JLuzVrDqY/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1504&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEip473wwGuslqnXu9FY1DsHgVfUbRC-9Unt1yONvUPBfQ0ekAdYVOZYFLWPjS4qtilW61kQJNBa_MKWYju4CJKabqrxu21KIQGGT0rxhOFNQrFIi46PY08lBiZNMRNnpwWozNGKbjynNdUDO5YdlxWocijTh4cvBkgh_jmsYE9PgFAWFDTbem_JLuzVrDqY/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The rates of traffic on the four lanes leading to Highway 101 on-ramps for both the preliminary case (shown shaded in gray) and the final evacuation plan (shown outlined in black).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The below plot shows the cumulative fraction of vehicles vs. time, comparing the cases of 1, 1.5 and 2 vehicles per household for the preliminary and final evacuation plans. The speedup is quite significant, on the scale of hours. For example, with 1.5 vehicles per household, it took 5.3 hours to evacuate the city using the preliminary evacuation plan, and only 3.5 hours using the final plan. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhchH8NchEEmevzOxMoC4Rne0WOMAcijZqydDY5Qq_-5eehyphenhyphenSBp4A-GaRPVc_pQgl4etQydVCrX6gY6gNFkqt0Zk7QDkrDEMu630BoTxvppbHKvAL52aCEJ4Azzu2UlGSXhKHpzZ0-8tfa3HUpP36WYDLCRdJD4geCypjLDwatL8qB4Y6a_cYrxsJg-CnOz/s812/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;425&quot; data-original-width=&quot;812&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhchH8NchEEmevzOxMoC4Rne0WOMAcijZqydDY5Qq_-5eehyphenhyphenSBp4A-GaRPVc_pQgl4etQydVCrX6gY6gNFkqt0Zk7QDkrDEMu630BoTxvppbHKvAL52aCEJ4Azzu2UlGSXhKHpzZ0-8tfa3HUpP36WYDLCRdJD4geCypjLDwatL8qB4Y6a_cYrxsJg-CnOz/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The cumulative fraction of vehicles vs. time in hours. The demand curve is shown in the dashed line on the far left. The solid lines show the final evacuation plan curves for 1, 1.5 and 2 vehicles per household (left to right). The dotted lines show the same for the preliminary evacuation plan.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Evacuation plans can be crucial in quickly getting many people to safety in emergency situations. While some cities have traffic evacuation plans in place, it can be difficult for officials to learn how well the plan works or whether it can be improved. Google Research helped Mill Valley test and evaluate their evacuation plan by running traffic simulations. We found that, while the preliminary plan did speed up the evacuation time, some minor changes to the plan significantly expedited evacuation. We worked closely with the city during this research, and Mill Valley has adopted the final plan. We were able to provide the city with more simulation details, including results for evacuating the city one area at a time. Full details can be found in &lt;a href=&quot;https://arxiv.org/abs/2307.07108&quot;>;the paper&lt;/a>;. &lt;/p>; &lt;p>; Detailed recommendations for a particular evacuation plan are necessarily specific to the area under study. So, the specific road network changes we found for Mill Valley are not directly applicable for other cities. However, we used only public data (road network from &lt;a href=&quot;http://openstreetmap.org&quot;>;OpenStreetMap&lt;/a>;; household information from census data) and an open source simulator (&lt;a href=&quot;https://sumo.dlr.de/docs/index.html&quot;>;SUMO&lt;/a>;), so any city or agency could use the methodology used in our paper to obtain results for their area. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We thank former Mayor John McCauley and City of Mill Valley personnel Tom Welch, Lindsay Haynes, Danielle Staude, Rick Navarro and Alan Piombo for numerous discussions and feedback, and Carla Bromberg for program management.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/524891095627230116/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/524891095627230116&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/524891095627230116&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/improving-traffic-evacuations-case-study.html&quot; rel=&quot;alternate&quot; title=&quot;Improving traffic evacuations: A case study&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiq0gQYBrF4sBptEnwyqdXMyAuYpCBTZJf7JBevRgt8ZuEtflAdzaNbenGSfItI98BCpXDMpUkO3nEJyDs4XTVaCmpGrqBsLGL1E11V6-QbB629_OL_IrMhX1AfJRZIbL_O9AufL5o-kbESrDz6y_zJbP7Kj6MTn_RLQ8B7hOSlbKguvdd6jjthWNRTPZYl/s72-c/trafficevac.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1355231158088692780&lt;/id>;&lt;published>;2023-10-13T11:01:00.000-07:00&lt;/published>;&lt;updated>;2023-10-13T11:01:35.679-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Intelligence&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Batch calibration: Rethinking calibration for in-context learning and prompt engineering&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Han Zhou, Student Researcher, and Subhrajit Roy, Senior Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcUnpQACR3ZhIfLnnazC6jATK0TZkwm-zGcHW0iMGYkd5bOrHtp0UAzDZ51hkI-ZesK4PAW_zn29G7r9hnq6bsNrp54dCEoiinN7l2bkNkIZVOj18ym1WQBk4QZfB_LlJnImIms-LSh6E88bcDP6NZ3yDssRhyJGUFgt2IhZurObVP8jn3Bb4aiAnB_Ysg/s1550/Screenshot%202023-10-13%20at%2010.57.44%20AM.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;Prompting&lt;/a>; large language models (LLMs) has become an efficient learning paradigm for adapting LLMs to a new task by conditioning on human-designed instructions. The remarkable &lt;a href=&quot;https://en.wikipedia.org/wiki/Prompt_engineering&quot;>;in-context learning&lt;/a>; (ICL) ability of LLMs also leads to efficient few-shot learners that can generalize from few-shot input-label pairs. However, the predictions of LLMs are highly sensitive and even biased to the &lt;a href=&quot;https://aclanthology.org/2022.emnlp-main.759/&quot;>;choice of templates&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2104.08315&quot;>;label spaces&lt;/a>; (such as yes/no, true/false, correct/incorrect), and &lt;a href=&quot;https://aclanthology.org/2022.deelio-1.10/&quot;>;demonstration examples&lt;/a>;, resulting in unexpected performance degradation and barriers for pursuing robust LLM applications. To address this problem, &lt;a href=&quot;http://proceedings.mlr.press/v139/zhao21c.html&quot;>;calibration&lt;/a>; methods have been developed to mitigate the effects of these biases while recovering LLM performance. Though multiple calibration solutions have been provided (eg, &lt;a href=&quot;http://proceedings.mlr.press/v139/zhao21c.html&quot;>;contextual calibration&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2305.19148&quot;>;domain-context calibration&lt;/a>;), the field currently lacks a unified analysis that systematically distinguishes and explains the unique characteristics, merits, and downsides of each approach. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; With this in mind, in “&lt;a href=&quot;https://arxiv.org/abs/2309.17249&quot;>;Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering&lt;/a>;”, we conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that mitigates the bias from a batch of inputs, unifies various prior approaches, and effectively addresses the limitations in previous methods. BC is zero-shot, self-adaptive (ie, inference-only), and incurs negligible additional costs. We validate the effectiveness of BC with &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP&lt;/a>; models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Motivation&lt;/h2>; &lt;p>; In pursuit of practical guidelines for ICL calibration, we started with understanding the limitations of current methods. We find that the calibration problem can be framed as an unsupervised &lt;a href=&quot;https://en.wikipedia.org/wiki/Decision_boundary&quot;>;decision boundary&lt;/a>; learning problem. We observe that uncalibrated ICL can be biased towards predicting a class, which we explicitly refer to as &lt;em>;contextual bias&lt;/em>;, the &lt;em>;a priori&lt;/em>; propensity of LLMs to predict certain classes over others unfairly given the context. For example, the prediction of LLMs can be biased towards predicting the most frequent label, or the label towards the end of the demonstration. We find that, while theoretically more flexible, non-linear boundaries (&lt;a href=&quot;https://openreview.net/forum?id=nUsP9lFADUF&quot;>;prototypical calibration&lt;/a>;) tend to be susceptible to overfitting and may suffer from instability for challenging multi-class tasks. Conversely, we find that linear decision boundaries can be more robust and generalizable across tasks. In addition, we find that relying on additional content-free inputs (eg, “N/A” or random in-domain tokens) as the grounds for estimating the contextual bias is &lt;em>;not&lt;/em>; always optimal and may even introduce additional bias, depending on the task type. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Batch calibration&lt;/h2>; &lt;p>; Inspired by the previous discussions, we designed BC to be a zero-shot, inference-only and generalizable calibration technique with negligible computation cost. We argue that the most critical component for calibration is to accurately estimate the contextual bias. We, therefore, opt for a linear decision boundary for its robustness, and instead of relying on content-free inputs, we propose to estimate the contextual bias for each class from a batch in a content-based manner by marginalizing the output score over all samples within the batch, which is equivalent to measuring the mean score for each class (visualized below). &lt;/p>; &lt;p>; We then obtain the calibrated probability by dividing the output probability over the contextual prior, which is equivalent to aligning the log-probability (LLM scores) distribution to the estimated mean of each class. It is noteworthy that because it requires no additional inputs to estimate the bias, this BC procedure is zero-shot, only involves unlabeled test samples, and incurs negligible computation costs. We may either compute the contextual bias once all test samples are seen, or alternatively, in an on-the-fly manner that dynamically processes the outputs. To do so, we may use a running estimate of the contextual bias for BC, thereby allowing BC&#39;s calibration term to be estimated from a small number of mini-batches that is subsequently stabilized when more mini-batches arrive. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDELq3xigMVCgnLqTDh51T8oHF0WW8FopRyRFJcv8TkN86buqLelyJ77y9kCQJd9Buae_jWy4aXpeMgWDuOZQoXjhgQz_-OYDOtmurpdD-0nuROOSbVxiQBGDTxuWxl-FTQbhiOKYO6DxtloMAQU6mG-VFYdcdlIZ3qN3ibZQ2RA321mhWlBB__vO3Jipc/s800/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;450&quot; data-original-width=&quot;800&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDELq3xigMVCgnLqTDh51T8oHF0WW8FopRyRFJcv8TkN86buqLelyJ77y9kCQJd9Buae_jWy4aXpeMgWDuOZQoXjhgQz_-OYDOtmurpdD-0nuROOSbVxiQBGDTxuWxl-FTQbhiOKYO6DxtloMAQU6mG-VFYdcdlIZ3qN3ibZQ2RA321mhWlBB__vO3Jipc/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration of Batch Calibration (BC). Batches of demonstrations with in-context examples and test samples are passed into the LLM. Due to sources of implicit bias in the context, the score distribution from the LLM becomes biased. BC is a modular and adaptable layer option appended to the output of the LLM that generates calibrated scores (visualized for illustration only).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experiment design&lt;/h2>; &lt;p>; For natural language tasks, we conduct experiments on 13 more diverse and challenging classification tasks, including the standard &lt;a href=&quot;https://arxiv.org/abs/1804.07461&quot;>;GLUE&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1905.00537&quot;>;SuperGLUE&lt;/a>; datasets. This is in contrast to previous works that only report on relatively simple single-sentence classification tasks.. For image classification tasks, we include &lt;a href=&quot;https://research.google/pubs/pub37648/&quot;>;SVHN&lt;/a>;, &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8736785&quot;>;EuroSAT&lt;/a>;, and &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.html&quot;>;CLEVR&lt;/a>;. We conduct experiments mainly on the state-of-the-art &lt;a href=&quot;https://ai.google/discover/palm2/&quot;>;PaLM 2&lt;/a>; with size variants PaLM 2-S, PaLM 2-M, and PaLM 2-L. For VLMs, we report the results on &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;>;CLIP ViT-B/16&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; Notably, BC consistently outperforms ICL, yielding a significant performance enhancement of 8% and 6% on small and large variants of PaLM 2, respectively. This shows that the BC implementation successfully mitigates the contextual bias from the in-context examples and unleashes the full potential of LLM in efficient learning and quick adaptation to new tasks. In addition, BC improves over the state-of-the-art prototypical calibration (PC) baseline by 6% on PaLM 2-S, and surpasses the competitive contextual calibration (CC) baseline by another 3% on average on PaLM 2-L. Specifically, BC is a generalizable and cheaper technique across all evaluated tasks, delivering stable performance improvement, whereas previous baselines exhibit varying degrees of performance across tasks. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkw7ccRrKee_tz5U8eECx6PaI2u8XxxZl_eeFz9XrpEFO7LUYofj3rgzfX-zpso66oSEPJ0V_1uhXtmeb8q4S-9zfIAcrzN-wWrmM6PbpuuJcOFwHpHC9tANyx1zlCFlOR7juacZ6zhkWU0gfAmG6NjGmLU5vj0u7NltSoWW0UQ0p9jejiu2asQGIh3fEZ/s1996/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;752&quot; data-original-width=&quot;1996&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkw7ccRrKee_tz5U8eECx6PaI2u8XxxZl_eeFz9XrpEFO7LUYofj3rgzfX-zpso66oSEPJ0V_1uhXtmeb8q4S-9zfIAcrzN-wWrmM6PbpuuJcOFwHpHC9tANyx1zlCFlOR7juacZ6zhkWU0gfAmG6NjGmLU5vj0u7NltSoWW0UQ0p9jejiu2asQGIh3fEZ/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Batch Calibration (BC) achieves the best performance on 1-shot ICL over calibration baselines: &lt;a href=&quot;http://proceedings.mlr.press/v139/zhao21c.html&quot;>;contextual calibration&lt;/a>; (CC), &lt;a href=&quot;https://arxiv.org/abs/2305.19148&quot;>;domain-context calibration&lt;/a>; (DC), and &lt;a href=&quot;https://openreview.net/forum?id=nUsP9lFADUF&quot;>;prototypical calibration&lt;/a>; (PC) on an average of 13 NLP tasks on PaLM 2 and outperforms the zero-shot CLIP on image tasks.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We analyze the performance of BC by varying the number of ICL shots from 0 to 4, and BC again outperforms all baseline methods. We also observe an overall trend for improved performance when more shots are available, where BC demonstrates the best stability. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW3n6u0ou6R2VkY9rCWZ7uk271EVCGQPQnQriIUlFlxbwl-9YLvYBEiVl0of7uNmhAFEn4J8pHanLx0Zd1cU-XPLDRKWEBFGWeKaXbD1g6MlobIEybW0ScVo9BDYGsfdhGgVbrhRqa3aVZMcASoULP4MLscCy0Fbv6foCXAPOZossV6ZDZJBLwG74UXnj4/s1577/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;488&quot; data-original-width=&quot;1577&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhW3n6u0ou6R2VkY9rCWZ7uk271EVCGQPQnQriIUlFlxbwl-9YLvYBEiVl0of7uNmhAFEn4J8pHanLx0Zd1cU-XPLDRKWEBFGWeKaXbD1g6MlobIEybW0ScVo9BDYGsfdhGgVbrhRqa3aVZMcASoULP4MLscCy0Fbv6foCXAPOZossV6ZDZJBLwG74UXnj4/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The ICL performance on various calibration techniques over the number of ICL shots on PaLM 2-S. We compare BC with the uncalibrated ICL, &lt;a href=&quot;http://proceedings.mlr.press/v139/zhao21c.html&quot;>;contextual calibration&lt;/a>; (CC), &lt;a href=&quot;https://arxiv.org/abs/2305.19148&quot;>;domain-context calibration&lt;/a>; (DC), and &lt;a href=&quot;https://openreview.net/forum?id=nUsP9lFADUF&quot;>;prototypical calibration&lt;/a>; (PC) baselines.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We further visualize the decision boundaries of uncalibrated ICL after applying existing calibration methods and the proposed BC. We show success and failure cases for each baseline method, whereas BC is consistently effective. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdaLhETWK3H1Z6PE3b78M3s9mvpi0KJF55WimbcaRoBPxfCL06FKaeqv0p5zjAvwgxPHpIHOP5FlKh2nYSYNjaraaD7h6eUDD5_8iJb-LUks62D8sX6zpXg8B66QK0sHUr3owOkvqdYkyohJZDIkbmHFIbprIPksb_sR-eQaZKE2Q1nyB0ZFkkfg7rEm/s1578/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;836&quot; data-original-width=&quot;1578&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwrdaLhETWK3H1Z6PE3b78M3s9mvpi0KJF55WimbcaRoBPxfCL06FKaeqv0p5zjAvwgxPHpIHOP5FlKh2nYSYNjaraaD7h6eUDD5_8iJb-LUks62D8sX6zpXg8B66QK0sHUr3owOkvqdYkyohJZDIkbmHFIbprIPksb_sR-eQaZKE2Q1nyB0ZFkkfg7rEm/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visualization of the decision boundaries of uncalibrated ICL, and after applying existing calibration methods and the proposed BC in representative binary classification tasks of &lt;a href=&quot;https://aclanthology.org/D13-1170/&quot;>;SST-2&lt;/a>; (&lt;strong>;top row&lt;/strong>;) and &lt;a href=&quot;https://arxiv.org/abs/1804.07461&quot;>;QNLI&lt;/a>; (&lt;strong>;bottom row&lt;/strong>;) on 1-shot PaLM 2-S. Each axis indicates the LLM score on the defined label.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Robustness and ablation studies&lt;/h2>; &lt;p>; We analyze the robustness of BC with respect to common prompt engineering design choices that were previously shown to significantly affect LLM performance: choices and orders of in-context examples, the prompt template for ICL, and the label space. First, we find that BC is more robust to ICL choices and can mostly achieve the same performance with different ICL examples. Additionally, given a single set of ICL shots, altering the order between each ICL example has minimal impact on the BC performance. Furthermore, we analyze the robustness of BC under 10 designs of prompt templates, where BC shows consistent improvement over the ICL baseline. Therefore, though BC improves performance, a well-designed template can further enhance the performance of BC. Lastly, we examine the robustness of BC to variations in label space designs (see appendix &lt;a href=&quot;https://arxiv.org/abs/2309.17249&quot;>;in our paper&lt;/a>;). Remarkably, even when employing unconventional choices such as emoji pairs as labels, leading to dramatic oscillations of ICL performance, BC largely recovers performance. This observation demonstrates that BC increases the robustness of LLM predictions under common prompt design choices and makes prompt engineering easier. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje_sLla9vvNG-zGdTd7j4kVQponyMZQ-_SXJVua50uisPpP6Vp0y1WMmMDUiFNufrKRCR2UdtYbdd0l5XDXyJI8kFBOx5FLuYtF8XmFpeGPsHfVVpUC-1aw0QIYs6nos8veCYdjJMQ-vX9d6a2OylTf3z8-F6FaBoNscIvC7hxf1isKO57csItgvzOTgET/s785/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;703&quot; data-original-width=&quot;785&quot; height=&quot;358&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje_sLla9vvNG-zGdTd7j4kVQponyMZQ-_SXJVua50uisPpP6Vp0y1WMmMDUiFNufrKRCR2UdtYbdd0l5XDXyJI8kFBOx5FLuYtF8XmFpeGPsHfVVpUC-1aw0QIYs6nos8veCYdjJMQ-vX9d6a2OylTf3z8-F6FaBoNscIvC7hxf1isKO57csItgvzOTgET/w400-h358/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Batch Calibration makes prompt engineering easier while being data-efficient. Data are visualized as a standard &lt;a href=&quot;https://en.wikipedia.org/wiki/Box_plot&quot;>;box plot&lt;/a>;, which illustrates values for the median, first and third quartiles, and minimum and maximum.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Moreover, we study the impact of batch size on the performance of BC. In contrast to PC, which also leverages an unlabeled estimate set, BC is remarkably more sample efficient, achieving a strong performance with only around 10 unlabeled samples, whereas PC requires more than 500 unlabeled samples before its performance stabilizes. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirFWtQYRsULQPGbTFS_ACr2xvyHcv6EuSo9267r4cyX7Im7TFcFBlaDQNBXI_TBWThkqKXR4mvRncG2rEgJnvlaa6WQivBgi5TX6xQ_Xq1Y81nfhdlKGm-b8B6jBgWBB8zxhk_Vnu48TJpuv6_fc_cUfxykC9T63XlFXX0V7hE7gJ5molSshIMQLqIVnIZ/s790/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;479&quot; data-original-width=&quot;790&quot; height=&quot;243&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirFWtQYRsULQPGbTFS_ACr2xvyHcv6EuSo9267r4cyX7Im7TFcFBlaDQNBXI_TBWThkqKXR4mvRncG2rEgJnvlaa6WQivBgi5TX6xQ_Xq1Y81nfhdlKGm-b8B6jBgWBB8zxhk_Vnu48TJpuv6_fc_cUfxykC9T63XlFXX0V7hE7gJ5molSshIMQLqIVnIZ/w400-h243/image2.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Batch Calibration makes prompt engineering easier while being insensitive to the batch size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We first revisit previous calibration methods while addressing two critical research questions from an interpretation of decision boundaries, revealing their failure cases and deficiencies. We then propose Batch Calibration, a zero-shot and inference-only calibration technique. While methodologically simple and easy to implement with negligible computation cost, we show that BC scales from a language-only setup to the vision-language context, achieving state-of-the-art performance in both modalities. BC significantly improves the robustness of LLMs with respect to prompt designs, and we expect easy prompt engineering with BC. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was conducted by Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy. We would like to thank Mohammad Havaei and other colleagues at Google Research for their discussion and feedback.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1355231158088692780/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/batch-calibration-rethinking.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1355231158088692780&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1355231158088692780&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/batch-calibration-rethinking.html&quot; rel=&quot;alternate&quot; title=&quot;Batch calibration: Rethinking calibration for in-context learning and prompt engineering&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcUnpQACR3ZhIfLnnazC6jATK0TZkwm-zGcHW0iMGYkd5bOrHtp0UAzDZ51hkI-ZesK4PAW_zn29G7r9hnq6bsNrp54dCEoiinN7l2bkNkIZVOj18ym1WQBk4QZfB_LlJnImIms-LSh6E88bcDP6NZ3yDssRhyJGUFgt2IhZurObVP8jn3Bb4aiAnB_Ysg/s72-c/Screenshot%202023-10-13%20at%2010.57.44%20AM.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-56830389427789775&lt;/id>;&lt;published>;2023-10-12T13:56:00.002-07:00&lt;/published>;&lt;updated>;2023-10-12T14:54:51.773-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Chemistry&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum AI&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Quantum Computing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Developing industrial use cases for physical simulation on future error-corrected quantum computers&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Nicholas Rubin, Senior Research Scientist, and Ryan Babbush, Head of Quantum Algorithms, Quantum AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigr1Q582VTmDaeIOT7X-hnXjEJ8QW237xvxICJqe-ZKg2zBAQn9gPfoAbLsJXmG8IFQ5B0ysoh7O60-U4mMKA4mJxB92Tm5MnY50n8B7dWpHwap3lk9_at6c4oEZ0lqjpeS-sqRZyKdyu1UjzJkbb2zRJp9nsZvkikxlK0eTZBjB5hJqvOtbaFPyWe7OfF/s1000/StoppingPower.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; If you&#39;ve paid attention to the quantum computing space, you&#39;ve heard the claim that in the future, quantum computers will solve certain problems exponentially more efficiently than classical computers can. They have the potential to transform many industries, from pharmaceuticals to energy. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; For the most part, these claims have rested on arguments about the asymptotic scaling of algorithms as the problem size approaches infinity, but this tells us very little about the practical performance of quantum computers for finite-sized problems. We want to be more concrete: Exactly which problems are quantum computers more suited to tackle than their classical counterparts, and exactly what quantum algorithms could we run to solve these problems? Once we&#39;ve designed an algorithm, we can go beyond analysis based on asymptotic scaling — we can determine the actual resources required to compile and run the algorithm on a quantum computer, and how that compares to a classical computation. &lt;/p>; &lt;p>; Over the last few years, &lt;a href=&quot;https://quantumai.google/&quot;>;Google Quantum AI&lt;/a>; has collaborated with industry and academic partners to assess the prospects for quantum simulation to revolutionize specific technologies and perform concrete analyses of the resource requirements. In 2022, we developed quantum algorithms to analyze the chemistry of an important enzyme family called &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093435/&quot;>;cytochrome P450&lt;/a>;. Then, in &lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040303&quot;>;our paper&lt;/a>; released this fall, we demonstrated how to use a quantum computer to study sustainable alternatives to cobalt for use in lithium ion batteries. And most recently, as we report in a preprint titled “&lt;a href=&quot;https://arxiv.org/abs/2308.12352v1&quot;>;Quantum computation of stopping power for inertial fusion target design&lt;/a>;,” we&#39;ve found a new application in modeling the properties of materials in inertial confinement fusion experiments, such as those at the &lt;a href=&quot;https://lasers.llnl.gov/&quot;>;National Ignition Facility&lt;/a>; (NIF) at&lt;a href=&quot;https://www.llnl.gov/&quot;>; Lawrence Livermore National Laboratory&lt;/a>;, which recently &lt;a href=&quot;https://www.nytimes.com/2022/12/13/science/nuclear-fusion-energy-breakthrough.html&quot;>;made headlines&lt;/a>; for a breakthrough in nuclear fusion. &lt;/p>; &lt;p>; Below, we describe these three industrially relevant applications for simulations with quantum computers. While running the algorithms will require an error-corrected quantum computer, which is &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html?m=1&quot;>;still years away&lt;/a>;, working on this now will ensure that we are ready with efficient quantum algorithms when such a quantum computer is built. Already, our work has reduced the cost of compiling and running the algorithms significantly, as we &lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.2.030305&quot;>;have&lt;/a>; &lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.2.040332&quot;>;reported&lt;/a>; in the past. Our work is essential for demonstrating the potential of quantum computing, but it also provides our hardware team with target specifications for the number of qubits and time needed to run useful quantum algorithms in the future. &lt;/p>; &lt;br />; &lt;h2>;Application 1: The CYP450 mechanism&lt;/h2>; &lt;p>; The pharmaceutical industry is often touted as a field ripe for discovery using quantum computers. But concrete examples of such potential applications are few and far between. Working with collaborators at the pharmaceutical company &lt;a href=&quot;https://www.boehringer-ingelheim.com/&quot;>;Boehringer Ingelheim&lt;/a>;, our partners at the startup &lt;a href=&quot;https://qsimulate.com/&quot;>;QSimulate&lt;/a>;, and academic colleagues at &lt;a href=&quot;https://www.columbia.edu/&quot;>;Columbia University&lt;/a>;, we explored one example in the 2022 &lt;em>;&lt;a href=&quot;https://www.pnas.org/&quot;>;PNAS&lt;/a>;&lt;/em>; article, “&lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.2203533119&quot;>;Reliably assessing the electronic structure of cytochrome P450 on today&#39;s classical computers and tomorrow&#39;s quantum computers&lt;/a>;”. &lt;/p>; &lt;p>; Cytochrome P450 is an enzyme family naturally found in humans that helps us metabolize drugs. It excels at its job: more than 70% of all drug metabolism is performed by enzymes of the P450 family. The enzymes work by oxidizing the drug — a process that depends on complex correlations between electrons. The details of the interactions are too complicated for scientists to know &lt;em>;a priori&lt;/em>; how effective the enzyme will be on a particular drug. &lt;/p>; &lt;p>; In the &lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.2203533119&quot;>;paper&lt;/a>;, we showed how a quantum computer could approach this problem. The CYP450 metabolic process is a complex chain of reactions with many intermediate changes in the electronic structure of the enzymes throughout. We first use state-of-the-art classical methods to determine the resources required to simulate this problem on a classical computer. Then we imagine implementing a phase-estimation algorithm — which is needed to compute the ground-state energies of the relevant electronic configurations throughout the reaction chain — on a &lt;a href=&quot;https://blog.research.google/2023/02/suppressing-quantum-errors-by-scaling.html&quot;>;surface-code error-corrected quantum computer&lt;/a>;. &lt;/p>; &lt;p>; With a quantum computer, we could follow the chain of changing electronic structure with greater accuracy and fewer resources. In fact, we find that the higher accuracy offered by a quantum computer is needed to correctly resolve the chemistry in this system, so not only will a quantum computer be better, it will be necessary. And as the system size gets bigger, ie, the more quantum energy levels we include in the simulation, the more the quantum computer wins over the classical computer. Ultimately, we show that a few million physical qubits would be required to reach &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_supremacy&quot;>;quantum advantage&lt;/a>; for this problem. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimUWf0soJPXwDCKwwl3b46rIeysW0V9fbV_5m-oKpfZpjscw8Xo4LyI0smpSLDFQ8LOIcEFOXotDKsXvOsRkXP9BjuUhvY_ic36EG2izNuRPujdr_50wcv-GoAYR2JJh4AcmhJj6f9OfTFY_q0UyuK9hJP79QKkskP9iewyBx0FpK0-4FDjakrHLZZZZrt/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;869&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimUWf0soJPXwDCKwwl3b46rIeysW0V9fbV_5m-oKpfZpjscw8Xo4LyI0smpSLDFQ8LOIcEFOXotDKsXvOsRkXP9BjuUhvY_ic36EG2izNuRPujdr_50wcv-GoAYR2JJh4AcmhJj6f9OfTFY_q0UyuK9hJP79QKkskP9iewyBx0FpK0-4FDjakrHLZZZZrt/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left:&lt;/b>; Example of an electron orbital (red and blue) of a CYP enzyme. More than 60 such orbitals are required to model the CYP system. &lt;b>;Right:&lt;/b>; Comparison of actual runtime (CPU) of various classical techniques (blue) to hypothetical runtime (QPU) of a quantum algorithm (green). The lower slope of the quantum algorithm demonstrates the favorable asymptotic scaling over classical methods. Already at about 20-30 orbitals, we see a crossover to the regime where a quantum algorithm would be more efficient than classical methods.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Application 2: Lithium-ion batteries&lt;/h2>; &lt;p>; Lithium-ion batteries rely on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Electrochemical_potential&quot;>;electrochemical potential&lt;/a>; difference between two lithium containing materials. One material used today for the cathodes of Li-ion batteries is LiCoO&lt;sub>;2&lt;/sub>;. Unfortunately, it has drawbacks from a manufacturing perspective. Cobalt mining is expensive, &lt;a href=&quot;https://earth.org/cobalt-mining-in-congo&quot;>;destructive to the environment&lt;/a>;, and often utilizes &lt;a href=&quot;https://www.npr.org/sections/goatsandsoda/2023/02/01/1152893248/red-cobalt-congo-drc-mining-siddharth-kara&quot;>;unsafe or abusive labor practices&lt;/a>;. Consequently, many in the field are interested in alternatives to cobalt for lithium-ion cathodes. &lt;/p>; &lt;p>; In the 1990&#39;s, researchers discovered that nickel could replace cobalt to form LiNiO&lt;sub>;2&lt;/sub>; (called “lithium nickel oxide” or “LNO”) for cathodes. While pure LNO was found to be unstable in production, many cathode materials used in the automotive industry today use a high fraction of nickel and hence, resemble LNO. Despite its applications to industry, however, not all of the chemical properties of LNO are understood — even the properties of its ground state remains a subject of debate. &lt;/p>; &lt;p>; In our recent paper, “&lt;a href=&quot;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.040303&quot;>;Fault tolerant quantum simulation of materials using Bloch orbitals&lt;/a>;,” we worked with the chemical company, &lt;a href=&quot;https://www.basf.com/us/en/who-we-are/change-for-climate.html&quot;>;BASF&lt;/a>;, the molecular modeling startup, QSimulate, and &lt;a href=&quot;https://researchers.mq.edu.au/en/persons/dominic-berry&quot;>;collaborators at Macquarie University&lt;/a>; in Australia to develop techniques to perform quantum simulations on systems with periodic, regularly spaced atomic structure, such as LNO. We then applied these techniques to design algorithms to study the relative energies of a few different candidate structures of LNO. With classical computers, high accuracy simulations of the quantum wavefunction are considered too expensive to perform. In our work, we found that a quantum computer would need tens of millions of physical qubits to calculate the energies of each of the four candidate ground-state LNO structures. This is out of reach of the first error-corrected quantum computers, but we expect this number to come down with future algorithmic improvements. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5aLU-bNmekPT3q2j2ePYkBdvfoZqs2bBp12TC1At6QK0YFD10laEJc2gJ7XA9HQwTKnfSP35o4zLDiSbnRAapqNWNRMOSWOAIQjoI2YVROMLM8rYpa6UGH1vNICKdvFGh6anjk5F10d1gc4_7_guWBwYbzs78hdfudj77ytUgqP_7Ep9KPzRwwRzCIGeb/s1364/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;763&quot; data-original-width=&quot;1364&quot; height=&quot;358&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5aLU-bNmekPT3q2j2ePYkBdvfoZqs2bBp12TC1At6QK0YFD10laEJc2gJ7XA9HQwTKnfSP35o4zLDiSbnRAapqNWNRMOSWOAIQjoI2YVROMLM8rYpa6UGH1vNICKdvFGh6anjk5F10d1gc4_7_guWBwYbzs78hdfudj77ytUgqP_7Ep9KPzRwwRzCIGeb/w640-h358/image3.jpg&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Four candidate structures of LNO. In the paper, we consider the resources required to compare the energies of these structures in order to find the ground state of LNO.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Application 3: Fusion reactor dynamics&lt;/h2>; &lt;p>; In our third and most recent example, we collaborated with theorists at &lt;a href=&quot;https://www.sandia.gov/&quot;>;Sandia National Laboratories&lt;/a>; and our Macquarie University collaborators to put our hypothetical quantum computer to the task of simulating dynamics of charged particles in the extreme conditions typical of &lt;a href=&quot;https://en.wikipedia.org/wiki/Inertial_confinement_fusion&quot;>;inertial confinement fusion&lt;/a>; (ICF) experiments, like those at the National Ignition Facility. In those experiments, high-intensity lasers are focused into a metallic cavity (&lt;a href=&quot;https://en.wikipedia.org/wiki/Hohlraum&quot;>;hohlraum&lt;/a>;) that holds a target capsule consisting of an ablator surrounding &lt;a href=&quot;https://en.wikipedia.org/wiki/Deuterium%E2%80%93tritium_fusion&quot;>;deuterium–tritium fuel&lt;/a>;. When the lasers heat the inside of the hohlraum, its walls radiate x-rays that compress the capsule, heating the deuterium and tritium inside to 10s of millions of Kelvin. This allows the nucleons in the fuel to overcome their &lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb_barrier&quot;>;mutual electrostatic repulsion&lt;/a>; and start fusing into helium nuclei, also called &lt;a href=&quot;https://en.wikipedia.org/wiki/Alpha_particle&quot;>;alpha particles&lt;/a>;. &lt;/p>; &lt;p>; Simulations of these experiments are computationally demanding and rely on models of material properties that are themselves uncertain. Even testing these models, using methods similar to those in quantum chemistry, is extremely computationally expensive. In some cases, such test calculations have consumed &amp;gt;100 million CPU hours. One of the most expensive and least accurate aspects of the simulation is the dynamics of the plasma prior to the sustained fusion stage (&amp;gt;10s of millions of Kelvin), when parts of the capsule and fuel are a more balmy 100k Kelvin. In this “warm dense matter” regime, quantum correlations play a larger role in the behavior of the system than in the “hot dense matter” regime when sustained fusion takes place. &lt;/p>; &lt;p>; In our new preprint, “&lt;a href=&quot;https://arxiv.org/abs/2308.12352v1&quot;>;Quantum computation of stopping power for inertial fusion target design&lt;/a>;”, we present a quantum algorithm to compute the so-called “stopping power” of the warm dense matter in a nuclear fusion experiment. The stopping power is the rate at which a high energy alpha particle slows down due to &lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb%27s_law&quot;>;Coulomb interactions&lt;/a>; with the surrounding plasma. Understanding the stopping power of the system is vital for optimizing the efficiency of the reactor. As the alpha particle is slowed by the plasma around it, it transfers its energy to the plasma, heating it up. This self-heating process is the mechanism by which fusion reactions sustain the burning plasma. Detailed modeling of this process will help inform future reactor designs. &lt;/p>; &lt;p>; We estimate that the quantum algorithm needed to calculate the stopping power would require resources somewhere between the P450 application and the battery application. But since this is the first case study on first-principles dynamics (or any application at finite temperature), such estimates are just a starting point and we again expect to find algorithmic improvements to bring this cost down in the future. Despite this uncertainty, it is still certainly better than the classical alternative, for which the only tractable approaches for these simulations are &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean-field_theory&quot;>;mean-field methods&lt;/a>;. While these methods incur unknown systematic errors when describing the physics of these systems, they are currently the only meaningful means of performing such simulations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBtVt_flUA0xvpBirTdVvlwHewAPQs6z_7fZM4dj6AwdGlrUQg0w4izYIR-vXl_qMr_EUn91KGwXFdgJgbgPT66cFU9nepPbP4SSIhHQz8RJptKV0I0r98sjCCqiKZDUPgoVyEuO3CM2cRMcnA32dX2dXbbRAsHt5FKrVOgZpWOo5sHgNmiR0YT3DOfByG/s1586/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;712&quot; data-original-width=&quot;1586&quot; height=&quot;287&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBtVt_flUA0xvpBirTdVvlwHewAPQs6z_7fZM4dj6AwdGlrUQg0w4izYIR-vXl_qMr_EUn91KGwXFdgJgbgPT66cFU9nepPbP4SSIhHQz8RJptKV0I0r98sjCCqiKZDUPgoVyEuO3CM2cRMcnA32dX2dXbbRAsHt5FKrVOgZpWOo5sHgNmiR0YT3DOfByG/w640-h287/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;b>;Left:&lt;/b>; A projectile (red) passing through a medium (blue) with initial velocity vproj. &lt;b>;Right:&lt;/b>; To calculate the stopping power, we monitor the energy transfer between the projectile and the medium (blue solid line) and determine its average slope (red dashed line).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;h2>;Discussion and conclusion&lt;/h2>; &lt;p>; The examples described above are just three of a large and growing body of concrete applications for a future error-corrected quantum computer in simulating physical systems. This line of research helps us understand the classes of problems that will most benefit from the power of quantum computing. In particular, the last example is distinct from the other two in that it is simulating a dynamical system. In contrast to the other problems, which focus on finding the lowest energy, static ground state of a quantum system, quantum dynamics is concerned with how a quantum system changes over time. Since quantum computers are inherently dynamic — the qubit states evolve and change as each operation is performed — they are particularly well suited to solving these kinds of problems. Together with collaborators at Columbia, Harvard, Sandia National Laboratories and Macquarie University in Australia we recently published a paper in Nature Communications &lt;a href=&quot;https://www.nature.com/articles/s41467-023-39024-0&quot;>;demonstrating&lt;/a>; that quantum algorithms for simulating electron dynamics can be more efficient even than approximate, “mean-field” classical calculations, while simultaneously offering much higher accuracy. &lt;/p>; &lt;p>; Developing and improving algorithms today prepares us to take full advantage of them when an error-corrected quantum computer is eventually realized. Just as in the classical computing case, we expect improvements at every level of the quantum computing stack to further lower the resource requirements. But this first step helps separate hyperbole from genuine applications amenable to quantum computational speedups. &lt;/p>; &lt;br />; &lt;h2>; Acknowledgements&lt;/h2>; &lt;p>;&lt;em>; We would like to thank Katie McCormick, our Quantum Science Communicator, for helping to write this blog post.&lt;/em>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/56830389427789775/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/developing-industrial-use-cases-for.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/56830389427789775&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/56830389427789775&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/developing-industrial-use-cases-for.html&quot; rel=&quot;alternate&quot; title=&quot;Developing industrial use cases for physical simulation on future error-corrected quantum computers&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigr1Q582VTmDaeIOT7X-hnXjEJ8QW237xvxICJqe-ZKg2zBAQn9gPfoAbLsJXmG8IFQ5B0ysoh7O60-U4mMKA4mJxB92Tm5MnY50n8B7dWpHwap3lk9_at6c4oEZ0lqjpeS-sqRZyKdyu1UjzJkbb2zRJp9nsZvkikxlK0eTZBjB5hJqvOtbaFPyWe7OfF/s72-c/StoppingPower.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-1075240247864880418&lt;/id>;&lt;published>;2023-10-09T12:17:00.000-07:00&lt;/published>;&lt;updated>;2023-10-09T12:17:01.762-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;SANPO: A Scene understanding, Accessibility, Navigation, Pathfinding, &amp; Obstacle avoidance dataset&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Sagar M. Waghmare, Senior Software Engineer, and Kimberly Wilber, Software Engineer, Google Research, Perception Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh10zxbymBGZgjXFHDrw-CdxlVL7nRi6yjaI3w3X_x5pjxn8UWA7NnymAqMXomfjSBXWVDQ4czo8nqINhxzIPLVx2Uv1l8RDQAbikuWWjIt9IwxSIuSlEtJ5AJwOJkdaKPwzUdu9BwkJJP1gDj2UJQpkJ15ELGjSKHMl9ce0_470SwRz5snz32lV-vSlMd2/s600/SANPOHero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; As most people navigate their everyday world, they process visual input from the environment using an eye-level perspective. Unlike robots and self-driving cars, people don&#39;t have any &quot;out-of-body&quot; sensors to help guide them. Instead, a person&#39;s sensory input is completely &quot;egocentric&quot;, or &quot;from the self.&quot; This also applies to new technologies that understand the world around us from a human-like perspective, eg, robots navigating through unknown buildings, AR glasses that highlight objects, or &lt;a href=&quot;https://blog.google/outreach-initiatives/accessibility/project-guideline/&quot;>;assistive technology to help people run independently&lt;/a>;. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In computer vision, scene understanding is the subfield that studies how visible objects relate to the scene&#39;s 3D structure and layout by focusing on the spatial, functional, and semantic relationships between objects and their environment. For example, autonomous drivers must understand the 3D structure of the road, sidewalks, and surrounding buildings while identifying and recognizing street signs and stop lights, a task made easier with 3D data from a special laser scanner mounted on the top of the car rather than 2D images from the driver&#39;s perspective. Robots navigating a park must understand where the path is and what obstacles might interfere, which is simplified with a map of their surroundings and GPS positioning data. Finally, AR glasses that help users find their way need to understand where the user is and what they are looking at. &lt;/p>; &lt;p>; The computer vision community typically studies scene understanding tasks in contexts like self-driving, where many other sensors (GPS, wheel positioning, maps, etc.) beyond egocentric imagery are available. Yet most datasets in this space do not focus exclusively on egocentric data, so they are less applicable to human-centered navigation tasks. While there are plenty of self-driving focused scene understanding datasets, they have limited generalization to egocentric human scene understanding. A comprehensive human egocentric dataset would help build systems for related applications and serve as a challenging benchmark for the scene understanding community.&lt;br />; &lt;/p>; &lt;p>; To that end, we present the &lt;a href=&quot;https://arxiv.org/abs/2309.12172&quot;>;Scene understanding, Accessibility, Navigation, Pathfinding, Obstacle avoidance dataset&lt;/a>;, or &lt;a href=&quot;https://github.com/google-research-datasets/sanpo_dataset&quot;>;SANPO&lt;/a>; (also the Japanese word for ”brisk stroll”), a multi-attribute video dataset for outdoor human egocentric scene understanding. The dataset consists of real world data and synthetic data, which we call SANPO-Real and SANPO-Synthetic, respectively. It supports a wide variety of dense prediction tasks, is challenging for current models, and includes real and synthetic data with depth maps and video panoptic masks in which each pixel is assigned a semantic class label (and for some semantic classes, each pixel is also assigned a semantic instance ID that uniquely identifies that object in the scene). The real dataset covers diverse environments and has videos from two stereo cameras to support multi-view methods, including 11.4 hours captured at 15 frames per second (FPS) with dense annotations. Researchers can download and use SANPO &lt;a href=&quot;https://github.com/google-research-datasets/sanpo_dataset&quot;>;here&lt;/a>;. &lt;/p>; &lt;h2>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5921eqIbEdm5k9lS-EqcW5VwLeSRdMhL1pU2jnQTk5NzdoO2FKPcQHbTd6bp3Ol6AmC-CnHmdAz2kc_M0C5Xr5_eDC3HoDzBhd9NCvQ9Tg1Lcd2kDXyC0iATO4c_I-FSBIgGb6dUhmqpokZ7qPl9qmvR0PXi5UWJXtngXUnfR3-ljiYcE9PqE8KRQM/s640/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;360&quot; data-original-width=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjWu5921eqIbEdm5k9lS-EqcW5VwLeSRdMhL1pU2jnQTk5NzdoO2FKPcQHbTd6bp3Ol6AmC-CnHmdAz2kc_M0C5Xr5_eDC3HoDzBhd9NCvQ9Tg1Lcd2kDXyC0iATO4c_I-FSBIgGb6dUhmqpokZ7qPl9qmvR0PXi5UWJXtngXUnfR3-ljiYcE9PqE8KRQM/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;3D scene of a real session built using the provided annotations (segmentation, depth and camera positions). The top center video shows the depth map, and the top right shows the RGB or semantic annotations.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;/h2>;&lt;h2>;SANPO-Real&lt;/h2>; &lt;p>; SANPO-Real is a multiview video dataset containing 701 sessions recorded with two stereo cameras: a head-mounted &lt;a href=&quot;https://www.stereolabs.com/zed-mini/&quot;>;ZED Mini&lt;/a>; and a chest-mounted &lt;a href=&quot;https://www.stereolabs.com/zed-2i/&quot;>;ZED-2i&lt;/a>;. That&#39;s four &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; streams per session at 15 FPS. 597 sessions are recorded at a resolution of 2208x1242 pixels, and the remainder are recorded at a resolution of 1920x1080 pixels. Each session is approximately 30 seconds long, and the recorded videos are rectified using &lt;a href=&quot;https://www.stereolabs.com/docs/&quot;>;Zed software&lt;/a>; and saved in a lossless format. Each session has high-level attribute annotations, camera pose trajectories, dense depth maps from &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf&quot;>;CREStereo&lt;/a>;, and sparse depth maps provided by the &lt;a href=&quot;https://www.stereolabs.com/docs/&quot;>;Zed SDK&lt;/a>;. A subset of sessions have temporally consistent &lt;a href=&quot;https://arxiv.org/abs/1801.00868&quot;>;panoptic segmentation&lt;/a>; annotations of each instance. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLnM9NZMV5rPoS39qD3xw4bykNWCQLAC-M65laB2aFOgE_9sfCXGuC-iggztvkbwAIZXXWF075JOdWSSvFpg-chHQrAl-oeJqBgBevHRtWd_3AgjAmgD2_hOjP3tTBn8N_7AIwN-MBFs6fwDp_WmIuXqNcwFmPBVIiZPR02AHCSXxjTfAAp8Mcy5sh5s/s624/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;624&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLnM9NZMV5rPoS39qD3xw4bykNWCQLAC-M65laB2aFOgE_9sfCXGuC-iggztvkbwAIZXXWF075JOdWSSvFpg-chHQrAl-oeJqBgBevHRtWd_3AgjAmgD2_hOjP3tTBn8N_7AIwN-MBFs6fwDp_WmIuXqNcwFmPBVIiZPR02AHCSXxjTfAAp8Mcy5sh5s/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;The SANPO data collection system for collecting real-world data.&amp;nbsp;&lt;strong>;Right&lt;/strong>;: (i) a backpack with ZED 2i and ZED Mini cameras for data collection (&lt;strong>;bottom&lt;/strong>;), (ii) the inside of the backpack showing the ZED box and battery pack mounted on a 3D printed container (&lt;strong>;middle&lt;/strong>;), and (iii) an Android app showing the live feed from the ZED cameras (&lt;strong>;top&lt;/strong>;).&amp;nbsp;&lt;strong>;Left:&lt;/strong>;&amp;nbsp;The chest-mounted ZED-2i has a stereo baseline of 12cm with a 2.1mm focal length, and the head-mounted ZED Mini has a baseline of 6.3cm with a 2.1mm focal length.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h3>;Temporally consistent panoptic segmentation annotation protocol&lt;/h3>; &lt;p>; SANPO includes thirty different class labels, including various surfaces (road, sidewalk, curb, etc.), fences (guard rails, walls,, gates), obstacles (poles, bike racks, trees), and creatures (pedestrians, riders, animals). Gathering high-quality annotations for these classes is an enormous challenge. To provide temporally consistent panoptic segmentation annotation we divide each video into 30-second sub-videos and annotate every fifth frame (90 frames per sub-video), using a cascaded annotation protocol. At each stage, we ask annotators to draw borders around five mutually exclusive labels at a time. We send the same image to different annotators with as many stages as it takes to collect masks until all labels are assigned, with annotations from previous subsets frozen and shown to the annotator. We use &lt;a href=&quot;https://arxiv.org/abs/2106.02638&quot;>;AOT&lt;/a>;, a machine learning model that reduces annotation effort by giving annotators automatic masks from which to start, taken from previous frames during the annotation process. AOT also infers segmentation annotations for intermediate frames using the manually annotated preceding and following frames. Overall, this approach reduces annotation time, improves boundary precision, and ensures temporally consistent annotations for up to 30 seconds. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiz3jvevdmtzqxOFoJUrQhHDjoo-RZtZ78maxgZoBJCcPnvc3UsFNsOKaPljdY0K9Go7QBqUFZgwN8Jli2dcAffhyphenhyphenQx4R42b2yVT-vfO5l2V0OiH99-iBFG4qHsE3hXkdFxu5AYxwE917JqZ1jfA190VtXqllOUVHPIswpJ9S7b2Qi2WUsCe_HsNEqwBg/s1081/image7.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;325&quot; data-original-width=&quot;1081&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiz3jvevdmtzqxOFoJUrQhHDjoo-RZtZ78maxgZoBJCcPnvc3UsFNsOKaPljdY0K9Go7QBqUFZgwN8Jli2dcAffhyphenhyphenQx4R42b2yVT-vfO5l2V0OiH99-iBFG4qHsE3hXkdFxu5AYxwE917JqZ1jfA190VtXqllOUVHPIswpJ9S7b2Qi2WUsCe_HsNEqwBg/s16000/image7.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Temporally consistent panoptic segmentation annotations. The segmentation mask&#39;s title indicates whether it was manually annotated or AOT propagated.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;SANPO-Synthetic&lt;/h2>; &lt;p>; Real-world data has imperfect ground truth labels due to hardware, algorithms, and human mistakes, whereas synthetic data has near-perfect ground truth and can be customized. We partnered with &lt;a href=&quot;https://paralleldomain.com/&quot;>;Parallel Domain&lt;/a>;, a company specializing in lifelike synthetic data generation, to create SANPO-Synthetic, a high-quality synthetic dataset to supplement SANPO-Real. Parallel Domain is skilled at creating handcrafted synthetic environments and data for machine learning applications. Thanks to their work, SANPO-Synthetic matches real-world capture conditions with camera parameters, placement, and scenery. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKZLoH89yql9CrcYDxR2j6Xy8DWqq0kbGAqElYTQM0yHjW3iWPIbP07vLKnNVo7tqgeVrKnccIbbB03AXVmNto56hNtLLF92T-0Zxln-q9n9Mc2Kia4tfHa-DjFQ9CkTurPK1loDxw8U_2HOAsm5tkicYdY5-ndR3wYToc5Aa8NwyxZYWwWNFWbpLwf1A/s480/image8.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;270&quot; data-original-width=&quot;480&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKZLoH89yql9CrcYDxR2j6Xy8DWqq0kbGAqElYTQM0yHjW3iWPIbP07vLKnNVo7tqgeVrKnccIbbB03AXVmNto56hNtLLF92T-0Zxln-q9n9Mc2Kia4tfHa-DjFQ9CkTurPK1loDxw8U_2HOAsm5tkicYdY5-ndR3wYToc5Aa8NwyxZYWwWNFWbpLwf1A/s16000/image8.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;3D scene of a synthetic session built using the provided annotations (segmentation, depth and odometry). The top center video shows the depth map, and the top right shows the RGB or semantic annotations.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; SANPO-Synthetic is a high quality video dataset, handcrafted to match real world scenarios. It contains 1961 sessions recorded using virtualized Zed cameras, evenly split between chest-mounted and head-mounted positions and calibrations. These videos are monocular, recorded from the left lens only. These sessions vary in length and FPS (5, 14.28, and 33.33) for a mix of temporal resolution / length tradeoffs, and are saved in a lossless format. All the sessions have precise camera pose trajectories, dense pixel accurate depth maps and temporally consistent panoptic segmentation masks. &lt;/p>; &lt;p>; SANPO-Synthetic data has pixel-perfect annotations, even for small and distant instances. This helps develop challenging datasets that mimic the complexity of real-world scenes. SANPO-Synthetic and SANPO-Real are also drop-in replacements for each other, so researchers can study domain transfer tasks or use synthetic data during training with few domain-specific assumptions. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibHOjQgGs8QvEs9YEuvqcrC1eu7ld4dtHpAgr2HUOnCHgDucRq1HpwbqDc4HF3YhCmq8MTwpKyoR-iTCRVvhElXXVSnDICdDfNrkDiwCAWjU2gUWqm_ye2XomqSxpymie9QPR53YxA9WcTM5MTp96zjeE1dMao_pIVO2dMZknhUrZ8yPVxdw1PK5weXeQ/s960/image4.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibHOjQgGs8QvEs9YEuvqcrC1eu7ld4dtHpAgr2HUOnCHgDucRq1HpwbqDc4HF3YhCmq8MTwpKyoR-iTCRVvhElXXVSnDICdDfNrkDiwCAWjU2gUWqm_ye2XomqSxpymie9QPR53YxA9WcTM5MTp96zjeE1dMao_pIVO2dMZknhUrZ8yPVxdw1PK5weXeQ/s16000/image4.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;An even sampling of real and synthetic scenes.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Statistics&lt;/h2>; &lt;h3>;Semantic classes&lt;/h3>; &lt;p>; We designed our SANPO taxonomy: i) with human egocentric navigation in mind, ii) with the goal of being reasonably easy to annotate, and iii) to be as close as possible to the existing segmentation taxonomies. Though built with human egocentric navigation in mind, it can be easily mapped or extended to other human egocentric scene understanding applications. Both SANPO-Real and SANPO-Synthetic feature a wide variety of objects one would expect in egocentric obstacle detection data, such as roads, buildings, fences, and trees. SANPO-Synthetic includes a broad distribution of hand-modeled objects, while SANPO-Real features more “long-tailed” classes that appear infrequently in images, such as gates, bus stops, or animals. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvu3MvAucerGc2NU9GpGvXTCQ0p9zmM9WnyrrjLPIlzG1saJopaIs7UsMB_BWSDAk2U0AH6j0gtRN4ZteYYlA8ooiFRSekhO1Lf_8guS9GHWwY1VC9JIzZgUFQ2vVpsuDq5Yxjh8LMZVrzfNAdsG1cyMsIjibBQ80uXPDpOV7Z1vRADdp60H6fi6ACgPw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1196&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgvu3MvAucerGc2NU9GpGvXTCQ0p9zmM9WnyrrjLPIlzG1saJopaIs7UsMB_BWSDAk2U0AH6j0gtRN4ZteYYlA8ooiFRSekhO1Lf_8guS9GHWwY1VC9JIzZgUFQ2vVpsuDq5Yxjh8LMZVrzfNAdsG1cyMsIjibBQ80uXPDpOV7Z1vRADdp60H6fi6ACgPw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Distribution of images across the classes in the SANPO taxonomy.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Instance masks&lt;/h2>; &lt;p>; SANPO-Synthetic and a portion of SANPO-Real are also annotated with panoptic instance masks, which assign each pixel to a class and instance ID. Because it is generally human-labeled, SANPO-Real has a large number of frames with generally less than 20 instances per frame. Similarly, SANPO-Synthetic&#39;s virtual environment offers pixel-accurate segmentation of most unique objects in the scene. This means that synthetic images frequently feature many more instances within each frame. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF1gYL6YUhpZyIG4vWz8CkuPs3UH4nMJnRyiTqc1Z6cSrAoA5qnQehWBj15TOeYnWgzpQ5RXPTRMTsGZV-2x0PKTkZKBuqV_y-K0TebqbXT0XtzRqpdPOS7XhCRY81tYkDRp6oSPnjzsomDpgtZFQIbIsJu8AhPxkY5vttiFNNH7wwcR3me-bVc5yPJ1Q/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;568&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF1gYL6YUhpZyIG4vWz8CkuPs3UH4nMJnRyiTqc1Z6cSrAoA5qnQehWBj15TOeYnWgzpQ5RXPTRMTsGZV-2x0PKTkZKBuqV_y-K0TebqbXT0XtzRqpdPOS7XhCRY81tYkDRp6oSPnjzsomDpgtZFQIbIsJu8AhPxkY5vttiFNNH7wwcR3me-bVc5yPJ1Q/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;When considering per-frame instance counts, synthetic data frequently features many more instances per frame than the labeled portions of SANPO-Real.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Comparison to other datasets&lt;/h2>; &lt;p>; We compare SANPO to other important video datasets in this field, including &lt;a href=&quot;https://www.cs.utexas.edu/~xiao/SCAND/SCAND.html&quot;>;SCAND&lt;/a>;, &lt;a href=&quot;https://cs.gmu.edu/~xiao/Research/MuSoHu/&quot;>;MuSoHu&lt;/a>;, &lt;a href=&quot;https://ego4d-data.org/&quot;>;Ego4D&lt;/a>;, &lt;a href=&quot;https://github.com/VIPSeg-Dataset/VIPSeg-Dataset&quot;>;VIPSeg&lt;/a>;, and &lt;a href=&quot;https://waymo.com/open/&quot;>;Waymo Open&lt;/a>;. Some of these are intended for robot navigation (SCAND) or autonomous driving (Waymo) tasks. Across these datasets, only Waymo Open and SANPO have both panoptic segmentations and depth maps, and only SANPO has both real and synthetic data. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjv6lPLgaaNdFMyHnTXz2fAKueP_2nPi-W1VHGcoM-YA7bSJhJn6ImFD0uCRPn7rCaKBaQHorWCxnlkB5dBRCGk-IOM_e5BRt3Ovj7MoLp264JDxU5H3vIEYr3NsU0YGXV9te41FeN0epoqh3LvsijQWyP4OP_NjoRWxpyOhqSyrXTcL_IU-k5NzJjiggVm/s848/SANPOComparison.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;619&quot; data-original-width=&quot;848&quot; height=&quot;468&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjv6lPLgaaNdFMyHnTXz2fAKueP_2nPi-W1VHGcoM-YA7bSJhJn6ImFD0uCRPn7rCaKBaQHorWCxnlkB5dBRCGk-IOM_e5BRt3Ovj7MoLp264JDxU5H3vIEYr3NsU0YGXV9te41FeN0epoqh3LvsijQWyP4OP_NjoRWxpyOhqSyrXTcL_IU-k5NzJjiggVm/w640-h468/SANPOComparison.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Comparison to other video datasets. For stereo vs mono video, datasets marked with ★ have stereo video for all scenes and those marked ☆ provide stereo video for a subset. For depth maps, ★ indicates dense depth while ☆ represents sparse depth, eg, from a lower-resolution LIDAR scanner.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Conclusion and future work&lt;/h2>; &lt;p>; We present SANPO, a large-scale and challenging video dataset for human egocentric scene understanding, which includes real and synthetic samples with dense prediction annotations. We hope SANPO will help researchers build visual navigation systems for the visually impaired and advance visual scene understanding. Additional details are available in &lt;a href=&quot;https://arxiv.org/abs/2309.12172&quot;>;the preprint&lt;/a>; and on the &lt;a href=&quot;https://github.com/google-research-datasets/sanpo_dataset&quot;>;SANPO dataset GitHub repository&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This dataset was the outcome of hard work of many individuals from various teams within Google and our external partner, Parallel Domain. &lt;br />;&lt;br />; &lt;/em>;Core Team:&lt;em>; Mikhail Sirotenko, Dave Hawkey, Sagar Waghmare, Kimberly Wilber, Xuan Yang, Matthew Wilson &lt;br />;&lt;br />; &lt;/em>;Parallel Domain:&lt;em>; Stuart Park, Alan Doucet, Alex Valence-Lanoue, &amp;amp; Lars Pandikow. &lt;br />;&lt;br />; We would also like to thank following team members: Hartwig Adam, Huisheng Wang, Lucian Ionita, Nitesh Bharadwaj, Suqi Liu, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Alina Kuznetsova, Stefano Pellegrini, Yiwen Luo, Lily Pagan, Maxine Deines, Alex Siegman, Maura O&#39;Brien, Rachel Stigler, Bobby Tran, Supinder Tohra, Umesh Vashisht, Sudhindra Kopalle, Reet Bhatia.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/1075240247864880418/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1075240247864880418&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/1075240247864880418&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/sanpo-scene-understanding-accessibility.html&quot; rel=&quot;alternate&quot; title=&quot;SANPO: A Scene understanding, Accessibility, Navigation, Pathfinding, &amp; Obstacle avoidance dataset&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh10zxbymBGZgjXFHDrw-CdxlVL7nRi6yjaI3w3X_x5pjxn8UWA7NnymAqMXomfjSBXWVDQ4czo8nqINhxzIPLVx2Uv1l8RDQAbikuWWjIt9IwxSIuSlEtJ5AJwOJkdaKPwzUdu9BwkJJP1gDj2UJQpkJ15ELGjSKHMl9ce0_470SwRz5snz32lV-vSlMd2/s72-c/SANPOHero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8807628089114947298&lt;/id>;&lt;published>;2023-10-04T10:26:00.003-07:00&lt;/published>;&lt;updated>;2023-10-04T10:26:49.868-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;AI for Social Good&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Scalable spherical CNNs for scientific applications&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Carlos Esteves and Ameesh Makadia, Research Scientists, Google Research, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s398/image2.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; Typical deep learning models for computer vision, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;>;convolutional neural networks&lt;/a>; (CNNs) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Vision_transformer&quot;>;vision transformers&lt;/a>; (ViT), process signals assuming planar (flat) spaces. For example, digital images are represented as a grid of pixels on a plane. However, this type of data makes up only a fraction of the data we encounter in scientific applications. Variables sampled from the Earth&#39;s atmosphere, like temperature and humidity, are naturally represented on the sphere. Some kinds of &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosmic_microwave_background&quot;>;cosmological data&lt;/a>; and panoramic photos are also spherical signals, and are better treated as such. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Using methods designed for planar images to process spherical signals is problematic for a couple of reasons. First, there is a sampling problem, ie, there is no way of defining uniform grids on the sphere, which are needed for planar CNNs and ViTs, without heavy distortion. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU4VfEu7zoN6hcXN5a0xaKnCpLgMub71nN059lUTuQgHiW8shAUKxGjcO3ZQddZ-vBPsykL9WQbv4mK9iAEe5iSha93qzlANsJKQJdPzxZfxMkX1PU0YP9j9_nqEJiehqpMAW5p7UmSt6XtQSYzJvOnReXQJTIf6lZcYnHhYZpBfnSzbGs9xVajOP7wVs3/s1692/SphericalDistortions.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;1692&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU4VfEu7zoN6hcXN5a0xaKnCpLgMub71nN059lUTuQgHiW8shAUKxGjcO3ZQddZ-vBPsykL9WQbv4mK9iAEe5iSha93qzlANsJKQJdPzxZfxMkX1PU0YP9j9_nqEJiehqpMAW5p7UmSt6XtQSYzJvOnReXQJTIf6lZcYnHhYZpBfnSzbGs9xVajOP7wVs3/s16000/SphericalDistortions.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;When projecting the sphere into a plane, the patch represented by the red circle is heavily distorted near the poles. This sampling problem hurts the accuracy of conventional CNNs and ViTs on spherical inputs.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Second, signals and local patterns on the sphere are often complicated by rotations, so models need a way to address that. We would like &lt;a href=&quot;https://en.wikipedia.org/wiki/Equivariant_map&quot;>;equivariance&lt;/a>; to 3D rotations, which ensures that learned features follow the rotations of the input. This leads to better utilization of the model parameters and allows training with less data. Equivariance to 3D rotations is also useful in most settings where inputs don&#39;t have a preferred orientation, such as 3D shapes and molecules. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhzyhCxjMHyAkR3LfadfFYdzY2TDks15-Z_pJ9n7UZQgYmf8HDYHX09cl_Qd4j3yOmo7Fr3T-rzZ_OTX5jky3Vhu1R_N9IPqsCeV1LWAB90tPGsYnQykUDKxwte9dd4ONfYzxmbe1l0JtLoIsvcPKq4XjBAPBe1MGZSc2yFAFRMTBlL3Xp33EhAVPGhwaF/s512/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;256&quot; data-original-width=&quot;512&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhhzyhCxjMHyAkR3LfadfFYdzY2TDks15-Z_pJ9n7UZQgYmf8HDYHX09cl_Qd4j3yOmo7Fr3T-rzZ_OTX5jky3Vhu1R_N9IPqsCeV1LWAB90tPGsYnQykUDKxwte9dd4ONfYzxmbe1l0JtLoIsvcPKq4XjBAPBe1MGZSc2yFAFRMTBlL3Xp33EhAVPGhwaF/s16000/image3.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Drone racing with panoramic cameras. Here the sharp turns result in large 3D rotations of the spherical image. We would like our models to be robust to such rotations. Source:&amp;nbsp;&lt;a href=&quot;https://www.youtube.com/watch?v=_J7qXbbXY80&quot;>;https://www.youtube.com/watch?v=_J7qXbbXY80&lt;/a>;&amp;nbsp;(licensed under&amp;nbsp;&lt;a href=&quot;https://creativecommons.org/licenses/by/3.0/legalcode&quot;>;CC BY&lt;/a>;)&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcdTjU_u4KqbG40bY_qPZRkt8mS_wPR2cotwKueGkMqRnz2GCkyFNfZb6UPktZ7hIkELEhEEdSlabTFlryxDNicP7Uvv7e8N96u8WKEe79lWNpeY6eeAPPdCLzrbWEcRcNGPLlZwjgQ16petAInpKme8UPjg-j3wIDO3W9NwxtHkuF5AIi4VqZa6-z_YVw/s711/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;358&quot; data-original-width=&quot;711&quot; height=&quot;322&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcdTjU_u4KqbG40bY_qPZRkt8mS_wPR2cotwKueGkMqRnz2GCkyFNfZb6UPktZ7hIkELEhEEdSlabTFlryxDNicP7Uvv7e8N96u8WKEe79lWNpeY6eeAPPdCLzrbWEcRcNGPLlZwjgQ16petAInpKme8UPjg-j3wIDO3W9NwxtHkuF5AIi4VqZa6-z_YVw/w640-h322/image1.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;In the atmosphere, it is common to see similar patterns appearing at different positions and orientations. We would like our models to share parameters to recognize these patterns.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; With the above challenges in mind, in “&lt;a href=&quot;https://arxiv.org/abs/2306.05420&quot;>;Scaling Spherical CNNs&lt;/a>;”, presented at &lt;a href=&quot;https://icml.cc/Conferences/2023&quot;>;ICML 2023&lt;/a>;, we introduce an open-source &lt;a href=&quot;https://github.com/google-research/spherical-cnn&quot;>;library&lt;/a>; in &lt;a href=&quot;https://jax.readthedocs.io/en/latest/&quot;>;JAX&lt;/a>; for deep learning on spherical surfaces. We demonstrate how applications of this library match or surpass state-of-the-art performance on weather forecasting and molecular property prediction benchmarks, tasks that are typically addressed with transformers and graph neural networks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background on spherical CNNs&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/abs/1801.10130&quot;>;Spherical CNNs&lt;/a>; solve both the problems of &lt;a href=&quot;https://arxiv.org/abs/1711.06721&quot;>;sampling and of robustness to rotation&lt;/a>; by leveraging spherical convolution and cross-correlation operations, which are typically computed via &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform#Compact_non-abelian_groups&quot;>;generalized Fourier transforms&lt;/a>;. For planar surfaces, however, convolution with small filters is faster, because it can be performed on regular grids without using Fourier transforms. The higher computational cost for spherical inputs has so far restricted the application of spherical CNNs to small models and datasets and low resolution datasets. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Our contributions&lt;/h2>; &lt;p>; We have implemented the spherical convolutions from &lt;a href=&quot;https://arxiv.org/abs/2006.10731&quot;>;spin-weighted spherical CNNs&lt;/a>; in &lt;a href=&quot;https://jax.readthedocs.io/en/latest/&quot;>;JAX&lt;/a>; with a focus on speed, and have enabled distributed training over a large number of TPUs using &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_parallelism&quot;>;data parallelism&lt;/a>;. We also introduced a new phase collapse activation and spectral batch normalization layer, and a new residual block that improves accuracy and efficiency, which allows training more accurate models up to 100x larger than before. We apply these new models on molecular property regression and weather forecasting. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggm83f_OaxfBc4k1uMFMGudMpAWMDMZQkAn9pKBLcdzolHk_3f0emRMupSiu10dcCoxWWEf3KexI6FokwtdkwujNcygfMwc2p-OvFCSUAWTmM6RE6sAcRCjwRSZxcCoq9gK_sdqyH_lZVLOxe4rL-gFBTtVdZjWjDsxcLPO23bYhOXaMXTUvZoKSs9AVao/s640/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;431&quot; data-original-width=&quot;640&quot; height=&quot;430&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggm83f_OaxfBc4k1uMFMGudMpAWMDMZQkAn9pKBLcdzolHk_3f0emRMupSiu10dcCoxWWEf3KexI6FokwtdkwujNcygfMwc2p-OvFCSUAWTmM6RE6sAcRCjwRSZxcCoq9gK_sdqyH_lZVLOxe4rL-gFBTtVdZjWjDsxcLPO23bYhOXaMXTUvZoKSs9AVao/w640-h430/image7.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;We scale spherical CNNs by up to two orders of magnitude in terms of feature sizes and model capacity, compared to the literature:&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1801.10130&quot;>;Cohen&#39;18&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1711.06721&quot;>;Esteves&#39;18&lt;/a>;,&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2006.10731&quot;>;Esteves&#39;20&lt;/a>;, and&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2010.11661&quot;>;Cobb&#39;21&lt;/a>;.&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;>;VGG-19&lt;/a>;&amp;nbsp;is included as a conventional CNN reference. Our largest model for weather forecasting has 256 x 256 x 78 inputs and outputs, and runs 96 convolutional layers during training with a lowest internal resolution of 128 x 128 x 256.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Molecular property regression&lt;/h2>; &lt;p>; Predicting properties of molecules has applications in &lt;a href=&quot;https://www.nature.com/articles/d41586-018-05267-x&quot;>;drug discovery&lt;/a>;, where the goal is to quickly screen numerous molecules in search of those with desirable properties. Similar models may also be relevant in the design of drugs targeting the interaction between proteins. &lt;a href=&quot;https://en.wikipedia.org/wiki/Ab_initio_quantum_chemistry_methods&quot;>;Current&lt;/a>; methods in computational or experimental quantum chemistry are expensive, which motivates the use of machine learning. &lt;/p>; &lt;p>; Molecules can be represented by a set of atoms and their positions in 3D space; rotations of the molecule change the positions but not the molecular properties. This motivates the application of spherical CNNs because of their rotation equivariance. However, molecules are not defined as signals on the sphere so the first step is to map them to a set of spherical functions. We do so by leveraging physics-based interactions between the atoms of the molecule. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBdDPSVCSfr2FQ4_Re4Pdhd0UQ4-1pFUROon3__vXr3-N2kOhWr7uN8K3QfukpYcDIIVj6hPCIAgYhltWpGHHtq5ywvJ_WGLMu5lSo0mbJYUZw8I0H0mw_CDb50D_dGQO__tPt-Jb81ooB6WLw-5Zx6C87jX4fKHgQaPxGy27kykUtrxpvlMuRhsV9Jr7i/s1600/image6.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;600&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBdDPSVCSfr2FQ4_Re4Pdhd0UQ4-1pFUROon3__vXr3-N2kOhWr7uN8K3QfukpYcDIIVj6hPCIAgYhltWpGHHtq5ywvJ_WGLMu5lSo0mbJYUZw8I0H0mw_CDb50D_dGQO__tPt-Jb81ooB6WLw-5Zx6C87jX4fKHgQaPxGy27kykUtrxpvlMuRhsV9Jr7i/s16000/image6.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Each atom is represented by a set of spherical signals accumulating physical interactions with other atoms of each type (shown in the three panels on the right). For example, the oxygen atom (O; top panel) has a channel for oxygen (indicated by the sphere labeled “O” on the left) and hydrogen (“H”, right). The accumulated&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Coulomb%27s_law&quot;>;Coulomb&lt;/a>;&amp;nbsp;forces on the oxygen atom with respect to the two hydrogen atoms is indicated by the red shaded regions on the bottom of the sphere labeled “H”. Because the oxygen atom contributes no forces to itself, the “O” sphere is uniform. We include extra channels for the&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Van_der_Waals_force&quot;>;Van der Waals&lt;/a>;&amp;nbsp;forces.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Spherical CNNs are applied to each atom&#39;s features, and results are later combined to produce the property predictions. This results in state-of-the art performance in most properties as typically evaluated in the &lt;a href=&quot;https://www.nature.com/articles/sdata201422&quot;>;QM9&lt;/a>; benchmark: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVQebJURRBto17o1rYA7wz-Ze-vwHt0gtDHHbloA51I9__iaHu9KtRm8eI639aChAIqEE-rlmy4v8Kyg4TEruNP4sJfzOkXYwaJ9CC1DU6-vQf0nH85hBXnuGWYhP6mRMGr_Sns_WonWqAkTtM8bbvnx3QbOdrGFEYF60_f9Q7F6uSzLbzVptd_EnnixNZ/s1409/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;786&quot; data-original-width=&quot;1409&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVQebJURRBto17o1rYA7wz-Ze-vwHt0gtDHHbloA51I9__iaHu9KtRm8eI639aChAIqEE-rlmy4v8Kyg4TEruNP4sJfzOkXYwaJ9CC1DU6-vQf0nH85hBXnuGWYhP6mRMGr_Sns_WonWqAkTtM8bbvnx3QbOdrGFEYF60_f9Q7F6uSzLbzVptd_EnnixNZ/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Error comparison against the state-of-the-art on 12 properties of QM9 (see the dataset&amp;nbsp;&lt;a href=&quot;http://www.nature.com/articles/sdata201422&quot;>;paper&lt;/a>;&amp;nbsp;for details). We show&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2202.02541&quot;>;TorchMD-Net&lt;/a>;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2102.03150&quot;>;PaiNN&lt;/a>;&amp;nbsp;results, normalizing TorchMD-Net errors to 1.0 (lower is better). Our model, shown in green, outperforms the baselines in most targets.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Weather forecasting&lt;/h2>; &lt;p>; Accurate climate forecasts serve as invaluable tools for providing timely warnings of extreme weather events, enabling effective water resource management, and guiding informed infrastructure planning. In a world increasingly &lt;a href=&quot;https://www.ncei.noaa.gov/access/billions/&quot;>;threatened by climate disasters&lt;/a>;, there is an urgency to deliver forecasts much faster and more accurately over a longer time horizon than general circulation models. Forecasting models will also be important for predicting the safety and effectiveness of efforts intended to combat climate change, such as &lt;a href=&quot;https://www.ametsoc.org/index.cfm/ams/about-ams/ams-statements/statements-of-the-ams-in-force/climate-intervention/&quot;>;climate interventions&lt;/a>;. The current state-of-the-art uses costly &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_weather_prediction&quot;>;numerical models&lt;/a>; based on fluid dynamics and thermodynamics, which tend to drift after a few days. &lt;/p>; &lt;p>; Given these challenges, there is an urgency for machine learning researchers to address climate forecasting problems, as data-driven techniques have the potential of both reducing the computational cost and improving long range accuracy. Spherical CNNs are suitable for this task since atmospheric data is natively presented on the sphere. They can also efficiently handle repeating patterns at different positions and orientations that are common in such data. &lt;/p>; &lt;p>; We apply our models to several weather forecasting benchmarks and outperform or match neural weather models based on conventional CNNs (specifically, &lt;a href=&quot;https://arxiv.org/abs/2002.00469&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2008.08626&quot;>;2&lt;/a>;, and &lt;a href=&quot;https://arxiv.org/abs/2205.10972&quot;>;3&lt;/a>;). Below we show results in a &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;test setting&lt;/a>; where the model takes a number of atmospheric variables as input and predicts their values six hours ahead. The model is then iteratively applied on its own predictions to produce longer forecasts. During training, the model predicts up to three days ahead, and is evaluated up to five days. &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler&lt;/a>; proposed a graph neural network for this task, but we show that spherical CNNs can match the GNN accuracy in the same setting. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS-sTrl2nwk9Lys2hVTJznTbLnl3ii-VodPHRD5QkKWRl71Ri5IW1N3j9XqVvdUbSx-vp2UudZiPGtk3Qy5hfHCoktwbyHVwbONnK7hLA7SSn7wnpRTTJ5FC3Z51tMVRAvAocexLw9AJUqyTOEZPYyINqzGmw2Obgy5LlhQVbLUtJgHLep_POyn6ANTqQz/s1300/image5.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;353&quot; data-original-width=&quot;1300&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS-sTrl2nwk9Lys2hVTJznTbLnl3ii-VodPHRD5QkKWRl71Ri5IW1N3j9XqVvdUbSx-vp2UudZiPGtk3Qy5hfHCoktwbyHVwbONnK7hLA7SSn7wnpRTTJ5FC3Z51tMVRAvAocexLw9AJUqyTOEZPYyINqzGmw2Obgy5LlhQVbLUtJgHLep_POyn6ANTqQz/s16000/image5.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Iterative weather forecasting up to five days (120h) ahead with spherical CNNs. The animations show the specific humidity forecast at a given pressure and its error.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s398/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;398&quot; data-original-width=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;em style=&quot;text-align: left;&quot;>;Wind speed and temperature forecasts with spherical CNNs.&lt;/em>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;h2>;Additional resources&lt;/h2>; &lt;p>; Our JAX library for efficient spherical CNNs is now &lt;a href=&quot;https://github.com/google-research/spherical-cnn&quot;>;available&lt;/a>;. We have shown applications to molecular property regression and weather forecasting, and we believe the library will be helpful in other scientific applications, as well as in computer vision and 3D vision. &lt;/p>; &lt;p>; Weather forecasting is an active area of research at Google with the goal of building more accurate and robust models — like &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;Graphcast&lt;/a>;, a recent ML-based mid-range forecasting model — and to build tools that enable further advancement across the research community, such as the recently released &lt;a href=&quot;https://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot;>;WeatherBench 2&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work was done in collaboration with Jean-Jacques Slotine, and is based on previous collaborations with Kostas Daniilidis and Christine Allen-Blanchette. We thank Stephan Hoyer, Stephan Rasp, and Ignacio Lopez-Gomez for helping with data processing and evaluation, and Fei Sha, Vivian Yang, Anudhyan Boral, Leonardo Zepeda-Núñez, and Avram Hershko for suggestions and discussions. We are thankful to Michael Riley and Corinna Cortes for supporting and encouraging this project.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8807628089114947298/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/scalable-spherical-cnns-for-scientific.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8807628089114947298&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8807628089114947298&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/scalable-spherical-cnns-for-scientific.html&quot; rel=&quot;alternate&quot; title=&quot;Scalable spherical CNNs for scientific applications&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiISXk_mf90TkNUcODcg-c9wYJ70zgm713RvUoRkPkkjnia1fO4P97T1icROacexZlBUsovSg8KCNNHzq2tPacdxHLuwUeW1Q2BIFi6bVw6c1au-9_umDgedCgx2RPogqqhPqDZaP-Xwj1ShADnPFRKBQVS0r1M3oOSQw8WA8nUE-Wa8sAnhTYNuWlwXN57/s72-c/image2.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7111364624265266582&lt;/id>;&lt;published>;2023-10-02T00:51:00.005-07:00&lt;/published>;&lt;updated>;2023-10-07T07:25:19.127-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conference&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;conferences&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ICCV&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google at ICCV 2023&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shaina Mehta, Program Manager, Google&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s320/ICCV%20hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Google is proud to be a &lt;a href=&quot;https://iccv2023.thecvf.com/iccv.2023.sponsors-93.php&quot;>;Platinum Sponsor&lt;/a>; of the &lt;a href=&quot;https://iccv2023.thecvf.com/&quot;>;International Conference on Computer Vision&lt;/a>; (ICCV 2023), a premier annual conference, which is being held this week in Paris, France. As a leader in computer vision research, Google has a strong presence at this year&#39;s conference with 60 accepted papers and active involvement in 27 &lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.workshops-90.php&quot;>;workshops&lt;/a>; and &lt;a href=&quot;https://iccv2023.thecvf.com/list.of.accepted.tutorials-91.php&quot;>;tutorials&lt;/a>;. Google is also proud to be a Platinum Sponsor for the &lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;LatinX in CV&lt;/a>; workshop. We look forward to sharing some of our extensive computer vision research and expanding our partnership with the broader research community. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Attending ICCV 2023? We hope you&#39;ll visit the Google booth to chat with researchers who are actively pursuing the latest innovations in computer vision, and check out some of the scheduled booth activities (eg, demos and Q&amp;amp;A sessions listed below). Visit the &lt;a href=&quot;https://twitter.com/GoogleAI&quot;>;@GoogleAI&lt;/a>; Twitter account to find out more about the Google booth activities at ICCV 2023.&lt;/p>; &lt;p>; Take a look below to learn more about the Google research being presented at ICCV 2023 (Google affiliations in &lt;strong>;bold&lt;/strong>;). &lt;/p>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Board and Organizing Committee&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; General Chair: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />; Finance Chair: &lt;strong>;&lt;em>;Ramin Zabih&lt;/em>;&lt;/strong>; &lt;br />; Industrial Relations Chair: &lt;strong>;&lt;em>;Rahul Sukthankar&lt;/em>;&lt;/strong>; &lt;br />; Publicity and Social Media Co-Chair: &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Accepted papers&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.14383.pdf&quot;>;Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor&lt;/a>; &lt;br />; &lt;em>;Xinyang Liu&lt;/em>;, &lt;em>;Yijin Li&lt;/em>;, &lt;em>;Yanbin Teng&lt;/em>;, &lt;em>;Hujun Bao&lt;/em>;, &lt;em>;Guofeng Zhang&lt;/em>;, &lt;strong>;&lt;em>;Yinda Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhaopeng Cui&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.05569.pdf&quot;>;ITI-GEN: Inclusive Text-to-Image Generation&lt;/a>; &lt;br />; &lt;em>;Cheng Zhang&lt;/em>;, &lt;em>;Xuanbai Chen&lt;/em>;, &lt;em>;Siqi Chai&lt;/em>;, &lt;em>;Chen Henry Wu&lt;/em>;, &lt;strong>;&lt;em>;Dmitry Lagun&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>;, &lt;em>;Fernando De la Torre&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.16201.pdf&quot;>;ASIC: Aligning Sparse in-the-wild Image Collections&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kamal Gupta&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Carlos Esteves&lt;/em>;&lt;/strong>;, &lt;em>;Abhinav Shrivastava&lt;/em>;, &lt;strong>;&lt;em>;Ameesh Makadia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Abhishek Kar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.06833.pdf&quot;>;VQ3D: Learning a 3D-Aware Generative Model on ImageNet&lt;/a>; &lt;br />; &lt;em>;Kyle Sargent&lt;/em>;, &lt;em>;Jing Yu Koh&lt;/em>;, &lt;strong>;&lt;em>;Han Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Huiwen Chang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Charles Herrmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pratul Srinivasan&lt;/em>;&lt;/strong>;, &lt;em>;Jiajun Wu&lt;/em>;, &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.11154.pdf&quot;>;Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Hexiang Hu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi Luan&lt;/em>;&lt;/strong>;, &lt;em>;Yang Chen&lt;/em>;*,&lt;strong>; &lt;em>;Urvashi Khandelwal&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mandar Joshi&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Kenton Lee&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Kristina Toutanova&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ming-Wei Chang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15343.pdf&quot;>;Sigmoid Loss for Language Image Pre-training&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Xiaohua Zhai&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Basil Mustafa&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Alexander Kolesnikov&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lucas Beyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05422.pdf&quot;>;Tracking Everything Everywhere All at Once&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Qianqian Wang&lt;/em>;&lt;/strong>;, &lt;em>;Yen-Yu Chang&lt;/em>;, &lt;em>;Ruojin Cai&lt;/em>;, &lt;strong>;&lt;em>;Zhengqi Li&lt;/em>;&lt;/strong>;, &lt;em>;Bharath Hariharan&lt;/em>;, &lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06706.pdf&quot;>;Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Jonathan T. Barron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Mildenhall&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pratul P. Srinivasan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peter Hedman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07090.pdf&quot;>;Delta Denoising Score&lt;/a>; &lt;br />; &lt;em>;Amir Hertz&lt;/em>;*, &lt;strong>;&lt;em>;Kfir Aberman&lt;/em>;&lt;/strong>;, &lt;em>;Daniel Cohen-Or&lt;/em>;* &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.13508.pdf&quot;>;DreamBooth3D: Subject-Driven Text-to-3D Generation&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Amit Raj&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Srinivas Kaza&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Poole&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Niemeyer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Nataniel Ruiz&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ben Mildenhall&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Kfir Aberman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jonathan Barron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yuanzhen Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Varun Jampani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09224.pdf&quot;>;Encyclopedic VQA: Visual Questions about Detailed Properties of Fine-grained Categories&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Thomas Mensink&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jasper Uijlings&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Lluis Castrejon&lt;/em>;&lt;/strong>;, &lt;em>;Arushi Goel&lt;/em>;*, &lt;em>;Felipe Cadar&lt;/em>;*, &lt;strong>;&lt;em>;Howard Zhou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Fei Sha&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vittorio Ferrar&lt;/em>;i&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.05916.pdf&quot;>;GECCO: Geometrically-Conditioned Point Diffusion Models&lt;/a>; &lt;br />; &lt;em>;Michał J. Tyszkiewicz&lt;/em>;, &lt;em>;Pascal Fua&lt;/em>;, &lt;strong>;&lt;em>;Eduard Trulls&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11489.pdf&quot;>;Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition&lt;/a>; &lt;br />; &lt;em>;Qitong Wang&lt;/em>;, &lt;strong>;&lt;em>;Long Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Liangzhe Yuan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ting Liu&lt;/em>;&lt;/strong>;, &lt;em>;Xi Peng&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17806.pdf&quot;>;Neural Microfacet Fields for Inverse Rendering&lt;/a>; &lt;br />; &lt;em>;Alexander Mai&lt;/em>;, &lt;strong>;&lt;em>;Dor Verbin&lt;/em>;&lt;/strong>;, &lt;em>;Falko Kuester&lt;/em>;, &lt;em>;Sara Fridovich-Keil&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09346.pdf&quot;>;Rosetta Neurons: Mining the Common Units in a Model Zoo&lt;/a>; &lt;br />; &lt;em>;Amil Dravid&lt;/em>;, &lt;em>;Yossi Gandelsman&lt;/em>;, &lt;em>;Alexei A. Efros&lt;/em>;, &lt;strong>;&lt;em>;Assaf Shocher&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12066.pdf&quot;>;Teaching CLIP to Count to Ten&lt;/a>; &lt;br />; &lt;em>;Roni Paiss&lt;/em>;*, &lt;strong>;&lt;em>;Ariel Ephrat&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Omer Tov&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Shiran Zada&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Inbar Mosseri&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michal Irani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tali Dekel&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12048.pdf&quot;>;Vox-E: Text-guided Voxel Editing of 3D Objects&lt;/a>; &lt;br />; &lt;em>;Etai Sella&lt;/em>;, &lt;em>;Gal Fiebelman&lt;/em>;, &lt;strong>;&lt;em>;Peter Hedman&lt;/em>;&lt;/strong>;, &lt;em>;Hadar Averbuch-Elor&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.12074.pdf&quot;>;CC3D: Layout-Conditioned Generation of Compositional 3D Scenes&lt;/a>; &lt;br />; &lt;em>;Sherwin Bahmani&lt;/em>;, &lt;em>;Jeong Joon Park&lt;/em>;, &lt;em>;Despoina Paschalidou&lt;/em>;, &lt;em>;Xingguang Yan&lt;/em>;, &lt;em>;Gordon Wetzstein&lt;/em>;, &lt;em>;Leonidas Guibas&lt;/em>;, &lt;strong>;&lt;em>;Andrea Tagliasacchi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11607.pdf&quot;>;Delving into Motion-Aware Matching for Monocular 3D Object Tracking&lt;/a>; &lt;br />; &lt;em>;Kuan-Chih Huang&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yi-Hsuan Tsai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.01172.pdf&quot;>;Generative Multiplane Neural Radiance for 3D-Aware Image Generation&lt;/a>; &lt;br />; &lt;em>;Amandeep Kumar&lt;/em>;, &lt;em>;Ankan Kumar Bhunia&lt;/em>;, &lt;em>;Sanath Narayan&lt;/em>;, &lt;em>;Hisham Cholakkal&lt;/em>;, &lt;em>;Rao Muhammad Anwer&lt;/em>;, &lt;em>;Salman Khan&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;, &lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.07313.pdf&quot;>;M2T: Masking Transformers Twice for Faster Decoding&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Fabian Mentzer&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Eirikur Agustsson&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Michael Tschannen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.02859.pdf&quot;>;MULLER: Multilayer Laplacian Resizer for Vision&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Zhengzhong Tu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peyman Milanfar&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hossein Talebi&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.11305.pdf&quot;>;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&lt;/a>; &lt;br />; &lt;em>;Ligong Han&lt;/em>;*, &lt;strong>;&lt;em>;Yinxiao Li&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Han Zhang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Peyman Milanfar&lt;/em>;&lt;/strong>;, &lt;em>;Dimitris Metaxas&lt;/em>;, &lt;strong>;&lt;em>;Feng Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.08996.pdf&quot;>;Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Yang Zhao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Tingbo Hou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yu-Chuan Su&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Xuhui Jia&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Yandong Li&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Matthias Grundmann&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.08998.pdf&quot;>;Unified Visual Relationship Detection with Vision and Language Models&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Long Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Liangzhe Yuan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Boqing Gong&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yin Cui&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Florian Schroff&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hartwig Adam&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ting Liu&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.03757.pdf&quot;>;3D Motion Magnification: Visualizing Subtle Motions from Time-Varying Radiance Fields&lt;/a>; &lt;br />; &lt;em>;Brandon Y. Feng&lt;/em>;, &lt;em>;Hadi Alzayer&lt;/em>;, &lt;strong>;&lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;William T. Freeman&lt;/em>;&lt;/strong>;, &lt;em>;Jia-Bin Huang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06954.pdf&quot;>;Global Features are All You Need for Image Retrieval and Reranking&lt;/a>; &lt;br />; &lt;em>;Shihao Shao&lt;/em>;, &lt;strong>;&lt;em>;Kaifeng Chen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Arjun Karpur&lt;/em>;&lt;/strong>;, &lt;em>;Qinghua Cui&lt;/em>;, &lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bingyi Cao&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.15827.pdf&quot;>;Introducing Language Guidance in Prompt-Based Continual Learning&lt;/a>; &lt;br />; &lt;em>;Muhammad Gul Zain Ali Khan&lt;/em>;, &lt;em>;Muhammad Ferjad Naeem&lt;/em>;, &lt;em>;Luc Van Gool&lt;/em>;, &lt;em>;Didier Stricker&lt;/em>;, &lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;, &lt;em>;Muhammad Zeshan Afzal&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.01789.pdf&quot;>;Multiscale Structure Guided Diffusion for Image Deblurring&lt;/a>; &lt;br />; &lt;em>;Mengwei Ren&lt;/em>;*, &lt;strong>;&lt;em>;Mauricio Delbracio&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Hossein Talebi&lt;/em>;&lt;/strong>;, &lt;em>;Guido Gerig&lt;/em>;, &lt;strong>;&lt;em>;Peyman Milanfar&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.09711.pdf&quot;>;Robust Monocular Depth Estimation under Challenging Conditions&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;, &lt;em>;Nils Morbitzer&lt;/em>;, &lt;em>;HyunJun Jung&lt;/em>;, &lt;em>;Nassir Navab&lt;/em>;,&lt;strong>; &lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.11751.pdf&quot;>;Score-Based Diffusion Models as Principled Priors for Inverse Imaging&lt;/a>; &lt;br />; &lt;em>;Berthy T. Feng&lt;/em>;*,&lt;strong>; &lt;em>;Jamie Smith&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Michael Rubinstein&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Huiwen Chang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Katherine L. Bouman&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;William T. Freeman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.01858.pdf&quot;>;Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations&lt;/a>; &lt;br />; &lt;em>;Nikolaos-Antonios Ypsilantis&lt;/em>;, &lt;strong>;&lt;em>;Kaifeng Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Bingyi Cao&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mario Lipovsky&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Pelin Dogan-Schonberger&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Grzegorz Makosa&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Boris Bluntschli&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Mojtaba Seyedhosseini&lt;/em>;&lt;/strong>;, &lt;em>;Ondrej Chum&lt;/em>;, &lt;strong>;&lt;em>;André Araujo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.06383.pdf&quot;>;U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds&lt;/a>; &lt;br />; &lt;em>;Yan Di&lt;/em>;, &lt;em>;Chenyangguang Zhang&lt;/em>;, &lt;em>;Ruida Zhang&lt;/em>;, &lt;strong>;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;, &lt;em>;Yongzhi Su&lt;/em>;, &lt;em>;Jason Rambach&lt;/em>;, &lt;em>;Didier Stricker&lt;/em>;, &lt;em>;Xiangyang Ji&lt;/em>;, &lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.17606.pdf&quot;>;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/a>; &lt;br />; &lt;em>;Ruixiang Jiang&lt;/em>;, &lt;em>;Can Wang&lt;/em>;, &lt;em>;Jingbo Zhang&lt;/em>;, &lt;strong>;&lt;em>;Menglei Chai&lt;/em>;&lt;/strong>;, &lt;em>;Mingming He&lt;/em>;, &lt;em>;Dongdong Chen&lt;/em>;, &lt;em>;Jing Liao&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.14700.pdf&quot;>;Learning Versatile 3D Shape Generation with Improved AR Models&lt;/a>; &lt;br />; &lt;em>;Simian Luo&lt;/em>;, &lt;em>;Xuelin Qian&lt;/em>;, &lt;em>;Yanwei Fu&lt;/em>;, &lt;strong>;&lt;em>;Yinda Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Ying Tai&lt;/em>;, &lt;em>;Zhenyu Zhang&lt;/em>;, &lt;em>;Chengjie Wang&lt;/em>;, &lt;em>;Xiangyang Xue&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11198.pdf&quot;>;Novel-view Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views&lt;/a>; &lt;br />; &lt;em>;Wentian Qu&lt;/em>;, &lt;em>;Zhaopeng Cui&lt;/em>;, &lt;strong>;&lt;em>;Yinda Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Chenyu Meng&lt;/em>;, &lt;em>;Cuixia Ma&lt;/em>;, &lt;em>;Xiaoming Deng&lt;/em>;, &lt;em>;Hongan Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05534.pdf&quot;>;PreSTU: Pre-Training for Scene-Text Understanding&lt;/a>; &lt;br />; &lt;em>;Jihyung Kil&lt;/em>;*, &lt;strong>;&lt;em>;Soravit Changpinyo&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xi Chen&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Hexiang Hu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Sebastian Goodman&lt;/em>;&lt;/strong>;, &lt;em>;Wei-Lun Chao&lt;/em>;, &lt;strong>;&lt;em>;Radu Soricut&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects&lt;/a>; &lt;br />; &lt;em>;Baowen Zhang&lt;/em>;, &lt;em>;Jiahe Li&lt;/em>;, &lt;em>;Xiaoming Deng&lt;/em>;, &lt;strong>;&lt;em>;Yinda Zhang&lt;/em>;&lt;/strong>;, &lt;em>;Cuixia Ma&lt;/em>;, &lt;em>;Hongan Wang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.06948.pdf&quot;>;Self-regulating Prompts: Foundational Model Adaptation without Forgetting&lt;/a>; &lt;br />; &lt;em>;Muhammad Uzair Khattak&lt;/em>;, &lt;em>;Syed Talal Wasi&lt;/em>;, &lt;em>;Muzammal Nasee&lt;/em>;, &lt;em>;Salman Kha&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yan&lt;/em>;&lt;/strong>;, &lt;em>;Fahad Shahbaz Khan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11015.pdf&quot;>;Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images&lt;/a>; &lt;br />; &lt;em>;Tze Ho Elden Tse&lt;/em>;*, &lt;strong>;&lt;em>;Franziska Mueller&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Zhengyang Shen&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Danhang Tang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Mingsong Dou&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Yinda Zhang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Sasa Petrovic&lt;/em>;&lt;/strong>;, &lt;em>;Hyung Jin Chang&lt;/em>;, &lt;strong>;&lt;em>;Jonathan Taylor&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Bardia Doosti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2305.12411.pdf&quot;>;Synthesizing Diverse Human Motions in 3D Indoor Scenes&lt;/a>; &lt;br />; &lt;em>;Kaifeng Zhao&lt;/em>;, &lt;em>;Yan Zhang&lt;/em>;, &lt;em>;Shaofei Wang&lt;/em>;, &lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>;, &lt;em>;Siyu Tang&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06419.pdf&quot;>;Tracking by 3D Model Estimation of Unknown Objects in Videos&lt;/a>; &lt;br />; &lt;em>;Denys Rozumnyi&lt;/em>;, &lt;em>;Jiri Matas&lt;/em>;, &lt;em>;Marc Pollefeys&lt;/em>;, &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>;, &lt;em>;Martin R. Oswald&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2308.11062.pdf&quot;>;UnLoc: A Unified Framework for Video Localization Tasks&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Shen Yan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Xuehan Xiong&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Zhonghao Wang&lt;/em>;&lt;/strong>;*, &lt;strong>;&lt;em>;Weina Ge&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;David Ross&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06708.pdf&quot;>;Verbs in Action: Improving Verb Understanding in Video-language Models&lt;/a>; &lt;br />; &lt;em>;Liliane Momeni&lt;/em>;, &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>;, &lt;em>;Andrew Zisserman&lt;/em>;, &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2309.06703v1.pdf&quot;>;VLSlice: Interactive Vision-and-Language Slice Discovery&lt;/a>; &lt;br />; &lt;em>;Eric Slyman&lt;/em>;, &lt;strong>;&lt;em>;Minsuk Kahng&lt;/em>;&lt;/strong>;, &lt;em>;Stefan Lee&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09012.pdf&quot;>;Yes, we CANN: Constrained Approximate Nearest Neighbors for Local Feature-Based Visual Localization&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Dror Aiger&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;André Araujo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Simon Lynen&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2212.05922.pdf&quot;>;Audiovisual Masked Autoencoders&lt;/a>; &lt;br />; &lt;em>;Mariana-Iuliana Georgescu&lt;/em>;*, &lt;strong>;&lt;em>;Eduardo Fonseca&lt;/em>;&lt;/strong>;, &lt;em>;Radu Tudor Ionescu&lt;/em>;, &lt;strong>;&lt;em>;Mario Lucic&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Anurag Arnab&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2307.11386.pdf&quot;>;CLR: Channel-wise Lightweight Reprogramming for Continual Learning&lt;/a>; &lt;br />; &lt;em>;Yunhao Ge&lt;/em>;, &lt;em>;Yuecheng Li&lt;/em>;, &lt;em>;Shuo Ni&lt;/em>;, &lt;strong>;&lt;em>;Jiaping Zhao&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Laurent Itti&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.05410.pdf&quot;>;LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs&lt;/a>; &lt;br />; &lt;em>;Zezhou&lt;/em>; &lt;em>;Cheng&lt;/em>;*,&lt;strong>; &lt;em>;Carlos&lt;/em>; &lt;em>;Esteves&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Varun&lt;/em>; &lt;em>;Jampani&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Abhishek&lt;/em>; &lt;em>;Kar&lt;/em>;&lt;/strong>;, &lt;em>;Subhransu&lt;/em>; &lt;em>;Maji&lt;/em>;, &lt;strong>;&lt;em>;Ameesh&lt;/em>; &lt;em>;Makadia&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.10075.pdf&quot;>;Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering&lt;/a>; &lt;br />; &lt;em>;Dongting&lt;/em>; &lt;em>;Hu&lt;/em>;, &lt;em>;Zhenkai&lt;/em>; &lt;em>;Zhang&lt;/em>;, &lt;strong>;&lt;em>;Tingbo&lt;/em>; &lt;em>;Hou&lt;/em>;&lt;/strong>;, &lt;em>;Tongliang&lt;/em>; &lt;em>;Liu&lt;/em>;, &lt;em>;Huan&lt;/em>; &lt;em>;Fu&lt;/em>;, &lt;em>;Mingming&lt;/em>; &lt;em>;Gong&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.10532.pdf&quot;>;Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs&lt;/a>; &lt;br />; &lt;em>;Frederik Warburg&lt;/em>;, &lt;em>;Ethan Weber&lt;/em>;, &lt;em>;Matthew Tancik&lt;/em>;, &lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Angjoo Kanazawa&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2209.05407.pdf&quot;>;Segmenting Known Objects and Unseen Unknowns without Prior Knowledge&lt;/a>; &lt;br />; &lt;em>;Stefano Gasperini&lt;/em>;, &lt;em>;Alvaro Marcos-Ramiro&lt;/em>;, &lt;em>;Michael Schmidt&lt;/em>;, &lt;em>;Nassir Navab&lt;/em>;, &lt;em>;Benjamin Busam&lt;/em>;, &lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.14340.pdf&quot;>;SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection&lt;/a>; &lt;br />; &lt;em>;Yichen Xie&lt;/em>;, &lt;em>;Chenfeng Xu&lt;/em>;, &lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>;, &lt;em>;Patrick Rim&lt;/em>;, &lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;, &lt;em>;Kurt Keutzer&lt;/em>;, &lt;em>;Masayoshi Tomizuka&lt;/em>;, &lt;em>;Wei Zhan&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2303.15446.pdf&quot;>;SwiftFormer: Efficient Additive Attention for Transformer-Based Real-time Mobile Vision Applications&lt;/a>; &lt;br />; &lt;em>;Abdelrahman Shaker&lt;/em>;, &lt;em>;Muhammad Maa&lt;/em>;, &lt;em>;Hanoona Rashee&lt;/em>;, &lt;em>;Salman Kha&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yan&lt;/em>;&lt;/strong>;, &lt;em>;Fahad Shahbaz Kha&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2302.12948.pdf&quot;>;Agile Modeling: From Concept to Classifier in Minutes&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Otilia Stretcu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Edward Vendrow&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Kenji Hata&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Krishnamurthy Viswanathan&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Vittorio Ferrari,&lt;/em>;&lt;/strong>; &lt;strong>;&lt;em>;Sasan Tavakkol&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Wenlei Zhou&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Aditya Avinash&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Enming Luo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Neil Gordon Alldrin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;MohammadHossein Bateni&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Gabriel Berger&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Andrew Bunner&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Chun-Ta Lu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Javier A Rey&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Giulia DeSalvo&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Ranjay Krishna&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ariel Fuxman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.09011.pdf&quot;>;CAD-Estate: Large-Scale CAD Model Annotation in RGB Videos&lt;/a>; &lt;br />; &lt;strong>;&lt;em>;Kevis-Kokitsi Maninis&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Stefan Popov&lt;/em>;&lt;/strong>;, &lt;em>;Matthias Niessner&lt;/em>;, &lt;strong>;&lt;em>;Vittorio Ferrari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2306.01209.pdf&quot;>;Counting Crowds in Bad Weather&lt;/a>; &lt;br />; &lt;em>;Zhi-Kai Huang&lt;/em>;, &lt;em>;Wei-Ting Chen&lt;/em>;, &lt;em>;Yuan-Chun Chiang&lt;/em>;, &lt;em>;Sy-Yen Kuo&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2304.06025.pdf&quot;>;DreamPose: Fashion Video Synthesis with Stable Diffusion&lt;/a>; &lt;br />; &lt;em>;Johanna Karras&lt;/em>;, &lt;strong>;&lt;em>;Aleksander Holynski&lt;/em>;&lt;/strong>;, &lt;em>;Ting-Chun Wang&lt;/em>;, &lt;em>;Ira Kemelmacher-Shlizerman&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/2301.09637.pdf&quot;>;InfiniCity: Infinite-Scale City Synthesis&lt;/a>; &lt;br />; &lt;em>;Chieh Hubert Lin&lt;/em>;, &lt;em>;Hsin-Ying Lee&lt;/em>;, &lt;em>;Willi Menapace&lt;/em>;, &lt;em>;Menglei Chai&lt;/em>;, &lt;em>;Aliaksandr Siarohin&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;, &lt;em>;Sergey Tulyakov&lt;/em>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://faculty.ucmerced.edu/mhyang/papers/iccv2023_sampling.pdf&quot;>;SAMPLING: Scene-Adaptive Hierarchical Multiplane Images Representation for Novel View Synthesis from a Single Image&lt;/a>; &lt;br />; &lt;em>;Xiaoyu Zhou&lt;/em>;, &lt;em>;Zhiwei Lin&lt;/em>;, &lt;em>;Xiaojun Shan&lt;/em>;, &lt;em>;Yongtao Wang&lt;/em>;, &lt;strong>;&lt;em>;Deqing Sun&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Tutorial&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;http://sifeiliu.net/NDLM_ICCV23tutorial/&quot;>;Learning with Noisy and Unlabeled Data for Large Models beyond Categorization&lt;/a>; &lt;br />; &lt;em>;Sifei Liu&lt;/em>;, &lt;em>;Hongxu Yin&lt;/em>;, &lt;em>;Shalini De Mello&lt;/em>;, &lt;em>;Pavlo Molchanov&lt;/em>;, &lt;em>;Jose M. Alvarez&lt;/em>;, &lt;em>;Jan Kautz&lt;/em>;, &lt;em>;Xiaolong&lt;/em>; &lt;em>;Wang&lt;/em>;, &lt;em>;Anima Anandkumar&lt;/em>;, &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;, &lt;em>;Trevor Darrell&lt;/em>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Varun Jampani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Workshops&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; &lt;a href=&quot;https://www.latinxinai.org/iccv-2023&quot;>;LatinX in AI&lt;/a>; &lt;br />; Platinum Sponsor &lt;br />; Panelists: &lt;strong>;&lt;em>;Daniel Castro Chin&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Andre Araujo&lt;/em>;&lt;/strong>; &lt;br />; Invited Speaker:&lt;strong>; &lt;em>;Irfan Essa&lt;/em>;&lt;/strong>; &lt;br />; Volunteers: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Liangzhe Yuan&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pedro Velez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Vincent Etter&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sg2rl.github.io/index.html&quot;>;Scene Graphs and Graph Representation Learning&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://web.northeastern.edu/smilelab/amfg2023/&quot;>;International Workshop on Analysis and Modeling of Faces and Gestures&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Todd Zickler&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://3dv-in-ecommerce.github.io/&quot;>;3D Vision and Modeling Challenges in eCommerce&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://bigmac-vision.github.io/&quot;>;BigMAC: Big Model Adaptation for Computer Vision&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Mathilde Caron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://iccv23-arow.github.io/&quot;>;Adversarial Robustness In the Real World (AROW)&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Yutong Bai&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://geonet-challenge.github.io/ICCV2023/&quot;>;GeoNet: 1st Workshop on Robust Computer Vision across Geographies&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Sara Beery&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Tarun Kalluri&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://gkioxari.github.io/Tutorials/iccv2023/&quot;>;Quo Vadis, Computer Vision? &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Bill Freeman&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/vschh/home&quot;>;To NeRF or not to NeRF: A View Synthesis Challenge for Human Heads &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Thabo Beeler&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Stefanos Zafeiriou&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/nivt-iccv2023/&quot;>;New Ideas in Vision Transformers &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Cordelia Schmid&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Ming-Hsuan Yang&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://lsfsl.net/limit23/&quot;>;Representation Learning with Very Limited Images: The Potential of Self, Synthetic and Formula Supervision &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Manel Baradad Jurjo&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/rcv2023/&quot;>;Resource Efficient Deep Learning for Computer Vision &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Prateek Jain&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Jiahui Yu&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Rishabh Tiwari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Jai Gupta&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cvaad-workshop.github.io/&quot;>;Computer Vision Aided Architectural Design &lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Noah Snavely&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://av4d.org/&quot;>;AV4D: Visual Learning of Sounds in Spaces&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;David Harwath&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://wvlar.github.io/iccv23/&quot;>;Vision-and-Language Algorithmic Reasoning&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;François Chollet&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;http://v&quot;>;Neural Fields for Autonomous Driving and Robotics&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Jon Barron&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://campworkshop.org/&quot;>;International Challenge on Compositional and Multimodal Perception &lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Ranjay Krishna&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://opensun3d.github.io/&quot;>;Open-Vocabulary 3D Scene Understanding (OpenSUN3D)&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Francis Engelmann&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Johanna Wald&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/mono3d-iccv-workshop&quot;>;Frontiers of Monocular 3D Perception: Geometric Foundation Models&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/perdream/home&quot;>;PerDream: PERception, Decision Making and REAsoning Through Multimodal Foundational Modeling&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Daniel McDuff&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://cmp.felk.cvut.cz/sixd/workshop_2023/&quot;>;Recovering 6D Object Pose&lt;/a>; &lt;br />; Speaker: &lt;strong>;Fabian Manhardt&lt;/strong>;,&lt;strong>; Martin Sundermeyer&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Martin Sundermeyer&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/wicviccv2023/&quot;>;Women in Computer Vision (WiCV)&lt;/a>; &lt;br />; Panelist: &lt;strong>;&lt;em>;Arsha Nagrani&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://languagefor3dscenes.github.io/ICCV2023/&quot;>;Language for 3D Scenes&lt;/a>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://ai3dcc.github.io/&quot;>;AI for 3D Content Creation&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Kai-Hung Chang&lt;/em>;&lt;/strong>; &lt;br />; Organizer: &lt;strong>;&lt;em>;Leonidas Guibas&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://sites.google.com/corp/view/cv4metaverse/&quot;>;Computer Vision for Metaverse&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Jon Barron&lt;/em>;&lt;/strong>;,&lt;strong>; &lt;em>;Thomas Funkhouser&lt;/em>;&lt;/strong>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://www.datacomp.ai/workshop.html#first&quot;>;Towards the Next Generation of Computer Vision Datasets&lt;/a>; &lt;br />; Speaker: &lt;strong>;&lt;em>;Tom Duerig&lt;/em>;&lt;/strong>; &lt;/p>; &lt;/div>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Google Research booth activities&lt;/h2>; &lt;div style=&quot;margin-left: 20px;&quot;>; &lt;p>; Title: Instant Tuning: Instant Personalized Image-to-Image Generation &lt;br />; Presenters: &lt;strong>;&lt;em>;Xuhui Jia&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Suraj Kothawade&lt;/em>;&lt;/strong>; &lt;br />; Wednesday, October 4th at 12:30 PM CEST &lt;/p>; &lt;p>; Title: Open Images V7 (&lt;a href=&quot;https://storage.googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf&quot;>;paper&lt;/a>;, &lt;a href=&quot;https://storage.googleapis.com/openimages/web/index.html&quot;>;dataset&lt;/a>;, &lt;a href=&quot;https://blog.research.google/2022/10/open-images-v7-now-featuring-point.html&quot;>;blog post&lt;/a>;) &lt;br />; Presenters: &lt;strong>;&lt;em>;Rodrigo Benenson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jasper Uijlings&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Jordi Pont-Tuset&lt;/em>;&lt;/strong>; &lt;br />; Wednesday, October 4th at 3:30 PM CEST &lt;/p>; &lt;p>; Title: New Magic Eraser on Pixel 8 Pro &lt;br />; Presenters: &lt;strong>;&lt;em>;Steven Hickson&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Pedro Velez&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Albert Shaw&lt;/em>;&lt;/strong>; &lt;br />; Thursday, October 5th at 10:30 AM CEST &lt;/p>; &lt;p>; Title: Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis &lt;br />; Presenters: &lt;strong>;&lt;em>;Marcel Bühler&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Kripasindhu Sarkar&lt;/em>;&lt;/strong>; &lt;br />; Thursday, October 5th at 12:30 PM CEST &lt;/p>; &lt;p>; Title: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images &lt;br />; Presenters: &lt;strong>;&lt;em>;Yonatan Bitton&lt;/em>;&lt;/strong>; &lt;br />; Thursday, October 5th at 1:00 PM CEST &lt;/p>; &lt;p>; Title: Image Search in Fact Check Explorer (&lt;a href=&quot;https://blog.google/products/news/new-features-coming-to-fact-check-explorer/&quot;>;blog post&lt;/a>;) &lt;br />; Presenters: &lt;strong>;&lt;em>;Yair Alon&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Avneesh Sud&lt;/em>;&lt;/strong>; &lt;br />; Thursday, October 5th at 3:30 PM CEST &lt;/p>; &lt;p>; Title: Prompt-Tuning Latent Diffusion Models for Inverse Problems &lt;br />; Presenters: &lt;strong>;&lt;em>;Hyungjin Chung&lt;/em>;&lt;/strong>; &lt;br />; Friday, October 6th at 12:30 PM CEST &lt;/p>; &lt;p>; Title: Neural Implicit Representations for Real World Applications &lt;br />; Presenters: &lt;strong>;&lt;em>;Federico Tombari&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Fabian Manhardt&lt;/em>;&lt;/strong>;, &lt;strong>;&lt;em>;Marie-Julie Rakotosaona&lt;/em>;&lt;/strong>; &lt;br />; Friday, October 6th at 3:30 PM CEST &lt;/p>; &lt;/div>; &lt;!--Footnotes-->; &lt;hr width=&quot;80%&quot; />; &lt;p>; &lt;span class=&quot;Apple-style-span&quot; style=&quot;font-size: small;&quot;>;&lt;b>;*&lt;/b>;&amp;nbsp;Work done while at Google&lt;/span>;&lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7111364624265266582/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-at-iccv-2023.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7111364624265266582&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7111364624265266582&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/10/google-at-iccv-2023.html&quot; rel=&quot;alternate&quot; title=&quot;Google at ICCV 2023&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVnC8dDc8c44GFT7cAZh8PfC8_Mpt4h1rhl-uNMNGoGlNCAZVsT51z89pMqEcVgwa7UPUuvXlr07PpOJlxomCAyRRTOEssXQxDwm4SX8J4JC_63fKWKywHLuqPHBLRZLl4yYIC311eAXC4r47i1zeZoPg2OXhjxuBmzVXCFn5MrJtH7QhZtMLKzzFvXyvx/s72-c/ICCV%20hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-3765645246777916540&lt;/id>;&lt;published>;2023-09-28T13:01:00.000-07:00&lt;/published>;&lt;updated>;2023-09-28T13:01:53.849-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computational Photography&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;CVPR&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;DynIBaR: Space-time view synthesis from videos of dynamic scenes&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Zhengqi Li and Noah Snavely, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGWa8cFgl_c_2WUJqw1O3J8nVhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s320/hero.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; A mobile phone&#39;s camera is a powerful tool for capturing everyday moments. However, capturing a dynamic scene using a single camera is fundamentally limited. For instance, if we wanted to adjust the camera motion or timing of a recorded video (eg, to freeze time while sweeping the camera around to highlight a dramatic moment), we would typically need an expensive Hollywood setup with a synchronized camera rig. Would it be possible to achieve similar effects solely from a video captured using a mobile phone&#39;s camera, without a Hollywood budget? &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.11082&quot;>;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/a>;”, a &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023/Awards&quot;>;best paper honorable mention&lt;/a>; at &lt;a href=&quot;https://cvpr2023.thecvf.com/Conferences/2023&quot;>;CVPR 2023&lt;/a>;, we describe a new method that generates photorealistic free-viewpoint renderings from a single video of a complex, dynamic scene. Neural Dynamic Image-Based Rendering (DynIBaR) can be used to generate a range of video effects, such as “&lt;a href=&quot;https://en.wikipedia.org/wiki/Bullet_time&quot;>;bullet time&lt;/a>;” effects (where time is paused and the camera is moved at a normal speed around a scene), video stabilization, depth of field, and slow motion, from a single video taken with a phone&#39;s camera. We demonstrate that DynIBaR significantly advances video rendering of complex moving scenes, opening the door to new kinds of video editing applications. We have also released the &lt;a href=&quot;https://github.com/google/dynibar&quot;>;code&lt;/a>; on the DynIBaR &lt;a href=&quot;https://dynibar.github.io/&quot;>;project page&lt;/a>;, so you can try it out yourself. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-3.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given an in-the-wild video of a complex, dynamic scene, DynIBaR can freeze time while allowing the camera to continue to move freely through the scene.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Background&lt;/h2>; &lt;p>; The last few years have seen tremendous progress in computer vision techniques that use &lt;a href=&quot;https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot;>;neural radiance fields&lt;/a>; (NeRFs) to reconstruct and render static (non-moving) 3D scenes. However, most of the videos people capture with their mobile devices depict moving&lt;em>; &lt;/em>;objects, such as people, pets, and cars. These moving scenes lead to a much more challenging 4D (3D + time) scene reconstruction&lt;em>; &lt;/em>;problem that cannot be solved using standard view synthesis methods. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-5.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Standard view synthesis methods output blurry, inaccurate renderings when applied to videos of dynamic scenes.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Other recent methods tackle view synthesis for dynamic scenes using space-time neural radiance fields (ie, &lt;em>;&lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;Dynamic NeRFs&lt;/a>;&lt;/em>;), but such approaches still exhibit inherent limitations that prevent their application to casually captured, in-the-wild videos. In particular, they struggle to render high-quality novel views from videos featuring long time duration, uncontrolled camera paths and complex object motion. &lt;/p>; &lt;p>; The key pitfall is that they store a complicated, moving scene in a single data structure. In particular, they encode scenes in the weights of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;>;multilayer perceptron&lt;/a>; (MLP) neural network. MLPs can approximate any function — in this case, a function that maps a 4D space-time point (&lt;em>;x&lt;/em>;, &lt;em>;y&lt;/em>;, &lt;em>;z&lt;/em>;, &lt;em>;t&lt;/em>;) to an RGB color and density that we can use in rendering images of a scene. However, the capacity of this MLP (defined by the number of parameters in its neural network) must increase according to the video length and scene complexity, and thus, training such models on in-the-wild videos can be computationally intractable. As a result, we get blurry, inaccurate renderings like those produced by &lt;a href=&quot;https://free-view-video.github.io/&quot;>;DVS&lt;/a>; and &lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>; (shown below). DynIBaR avoids creating such large scene models by adopting a different rendering paradigm. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-0.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto; text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;DynIBaR (&lt;strong>;bottom row&lt;/strong>;) significantly improves rendering quality compared to prior dynamic view synthesis methods (&lt;strong>;top row&lt;/strong>;) for videos of complex dynamic scenes. Prior methods produce blurry renderings because they need to store the entire moving scene in an MLP data structure.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Image-based rendering (IBR)&lt;/h2>; &lt;p>; A key insight behind DynIBaR is that we don&#39;t actually need to store all of the scene contents in a video in a giant MLP. Instead, we directly use pixel data from nearby input video frames to render new views. DynIBaR builds on an &lt;em>;&lt;a href=&quot;https://en.wikipedia.org/wiki/Image-based_modeling_and_rendering&quot;>;image-based rendering&lt;/a>;&lt;/em>; (IBR) method called &lt;a href=&quot;https://ibrnet.github.io/&quot;>;IBRNet&lt;/a>; that was designed for view synthesis for static scenes. IBR methods recognize that a new target view of a scene should be very similar to nearby source images, and therefore synthesize the target by dynamically selecting and warping pixels from the nearby source frames, rather than reconstructing the whole scene in advance. IBRNet, in particular, learns to blend nearby images together to recreate new views of a scene within a volumetric rendering framework. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;DynIBaR: Extending IBR to complex, dynamic videos&lt;/h2>; &lt;p>; To extend IBR to dynamic scenes, we need to take scene motion into account during rendering. Therefore, as part of reconstructing an input video, we solve for the motion&lt;em>; &lt;/em>;of every 3D point, where we represent scene motion using a motion trajectory field encoded by an MLP. Unlike prior dynamic NeRF methods that store the entire scene appearance and geometry in an MLP, we only store motion, a signal that is more smooth and sparse, and use the input video frames to determine everything else needed to render new views. &lt;/p>; &lt;p>; We optimize DynIBaR for a given video by taking each input video frame, rendering rays to form a 2D image using volume rendering (as in &lt;a href=&quot;https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html&quot;>;NeRF&lt;/a>;), and comparing that rendered image to the input frame. That is, our optimized representation should be able to perfectly reconstruct the input video. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW6JJk1ghWO2oma_XWYD2FcKeylPDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/s660/image3.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;480&quot; data-original-width=&quot;660&quot; height=&quot;465&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVxjezPKVF8ofpfJP7WtIN3-wOMHJPXISABUaoBbnS9wMtko3lTDTfMfntgTTkcNfG8DxFP63DobaehzDY2QA3OcNf-tJW6JJk1ghWO2oma_XWYD2FcKeylPDYPUhqKax8FBCy9qxZ3RzizzQluECi6mV2lkuOkd3r16wRaG5mRcpFt_JCSDyUt6Q8BTUb/w640-h465/image3.gif&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We illustrate how DynIBaR renders images of dynamic scenes. For simplicity, we show a 2D world, as seen from above. (&lt;strong>;a&lt;/strong>;) A set of input source views (&lt;strong>;triangular&lt;/strong>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Viewing_frustum&quot;>;camera frusta&lt;/a>;) observe a cube moving through the scene (&lt;strong>;animated square&lt;/strong>;). Each camera is labeled with its timestamp (&lt;em>;t&lt;/em>;-2, &lt;em>;t&lt;/em>;-1, etc). (&lt;strong>;b&lt;/strong>;) To render a view from camera at time &lt;em>;t&lt;/em>;, DynIBaR shoots a virtual ray through each pixel (&lt;strong>;blue line&lt;/strong>;), and computes colors and opacities for sample points along that ray. To compute those properties, DyniBaR projects those samples into other views via multi-view geometry, but first, we must compensate for the estimated motion of each point (&lt;strong>;dashed red line&lt;/strong>;). (&lt;strong>;c&lt;/strong>;) Using this estimated motion, DynIBaR moves each point in 3D to the relevant time before projecting it into the corresponding source camera, to sample colors for use in rendering. DynIBaR optimizes the motion of each scene point as part of learning how to synthesize new views of the scene.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; However, reconstructing and deriving new views for a complex, moving scene is a highly ill-posed problem, since there are many solutions that can explain the input video — for instance, it might create disconnected 3D representations for each time step. Therefore, optimizing DynIBaR to reconstruct the input video alone is insufficient. To obtain high-quality results, we also introduce several other techniques, including a method called &lt;em>;cross-time rendering&lt;/em>;. Cross-time rendering refers to the use of the state of our 4D representation at one time instant to render images from a different time instant, which encourages the 4D representation to be coherent over time. To further improve rendering fidelity, we automatically factorize the scene into two components, a static one and a dynamic one, modeled by time-invariant and time-varying scene representations respectively. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Creating video effects&lt;/h2>; &lt;p>; DynIBaR enables various video effects. We show several examples below. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Video stabilization&lt;/h3>; &lt;p>; We use a shaky, handheld input video to compare DynIBaR&#39;s video stabilization performance to existing 2D video stabilization and dynamic NeRF methods, including &lt;a href=&quot;https://alex04072000.github.io/FuSta/&quot;>;FuSta&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3363550&quot;>;DIFRINT&lt;/a>;, &lt;a href=&quot;https://hypernerf.github.io/&quot;>;HyperNeRF&lt;/a>;, and &lt;a href=&quot;https://www.cs.cornell.edu/~zl548/NSFF/&quot;>;NSFF&lt;/a>;. We demonstrate that DynIBaR produces smoother outputs with higher rendering fidelity and fewer artifacts (eg, flickering or blurry results). In particular, FuSta yields residual camera shake, DIFRINT produces flicker around object boundaries, and HyperNeRF and NSFF produce blurry results. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-4.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Simultaneous view synthesis and slow motion&lt;/h3>; &lt;p>; DynIBaR can perform view synthesis in both space and time simultaneously, producing smooth 3D cinematic effects. Below, we demonstrate that DynIBaR can take video inputs and produce smooth 5X slow-motion videos rendered using novel camera paths. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-2.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Video bokeh&lt;/h3>; &lt;p>; DynIBaR can also generate high-quality &lt;a href=&quot;https://en.wikipedia.org/wiki/Bokeh&quot;>;video bokeh&lt;/a>; by synthesizing videos with dynamically changing &lt;a href=&quot;https://en.wikipedia.org/wiki/Depth_of_field&quot;>;depth of field&lt;/a>;. Given an all-in-focus input video, DynIBar can generate high-quality output videos with varying out-of-focus regions that call attention to moving (eg, the running person and dog) and static content (eg, trees and buildings) in the scene. &lt;/p>; &lt;br />; &lt;video autoplay=&quot;&quot; loop=&quot;&quot; muted=&quot;&quot; playsinline=&quot;&quot; width=&quot;100%&quot;>; &lt;source src=&quot;https://dynibar.github.io/static/videos/blog/blog-1.mp4&quot; type=&quot;video/mp4&quot;>;&lt;/source>; &lt;/video>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; DynIBaR is a leap forward in our ability to render complex moving scenes from new camera paths. While it currently involves per-video optimization, we envision faster versions that can be deployed on in-the-wild videos to enable new kinds of effects for consumer video editing using mobile devices. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;DynIBaR is the result of a collaboration between researchers at Google Research and Cornell University. The key contributors to the work presented in this post include Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/3765645246777916540/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/dynibar-space-time-view-synthesis-from.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3765645246777916540&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/3765645246777916540&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/dynibar-space-time-view-synthesis-from.html&quot; rel=&quot;alternate&quot; title=&quot;DynIBaR: Space-time view synthesis from videos of dynamic scenes&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhGX53UfU9eWiSTRWdkrUWNln3KGyagkwfUi_38zEihOJ2qLkmQ-3yNsuJJ7SkJ-BTLlVrxJlyoEYl7-tAer6v4MnIAw49TWLGWa8cFgl_c_2WUJqw1O3J8nVhU-VtVwO5Z-bq7rH6pZj7APe5yDZCUSZyeDO39shlGFkVsSQDd1ZWYUT0eDmeJ9JLoWlA3/s72-c/hero.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2311847718846675808&lt;/id>;&lt;published>;2023-09-28T11:16:00.002-07:00&lt;/published>;&lt;updated>;2023-09-28T11:22:44.163-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;optimization&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Re-weighted gradient descent via distributionally robust optimization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Ramnath Kumar, Pre-Doctoral Researcher, and Arun Sai Suggala, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR59ivF_uDCUUmbdijaAuiK3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s320/hero%20RGD.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>;Deep neural networks (DNNs) have become essential for solving a wide range of tasks, from standard supervised learning (&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;image classification using ViT&lt;/a>;) to &lt;a href=&quot;https://arxiv.org/pdf/2201.11775.pdf&quot;>;meta-learning&lt;/a>;. The most commonly-used paradigm for learning DNNs is&lt;em>; &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;empirical risk minimization&lt;/a>; (ERM)&lt;/em>;, which aims to identify a network that minimizes the average &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;loss&lt;/a>; on training data points. Several algorithms, including &lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;stochastic gradient descent&lt;/a>; (SGD), &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>;, and &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;>;Adagrad&lt;/a>;, have been proposed for solving ERM. However, a drawback of ERM is that it weights all the samples equally, often ignoring the rare and more difficult samples, and focusing on the easier and abundant samples. This leads to suboptimal performance on unseen data, especially when the training data is scarce.&lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To overcome this challenge, recent works have developed data re-weighting techniques for improving ERM performance. However, these approaches focus on specific learning tasks (such as &lt;a href=&quot;https://arxiv.org/pdf/1708.02002.pdf&quot;>;classification&lt;/a>;) and/or require learning an &lt;a href=&quot;https://arxiv.org/pdf/1902.07379.pdf&quot;>;additional meta model&lt;/a>; that predicts the weights of each data point. The presence of an additional model significantly increases the complexity of training and makes them unwieldy in practice.&lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2306.09222&quot;>;Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization&lt;/a>;” we introduce a variant of the classical &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#&quot;>;SGD algorithm&lt;/a>; that re-weights data points during each optimization step based on their difficulty. Stochastic Re-weighted Gradient Descent (RGD) is a lightweight algorithm that comes with a simple closed-form expression, and can be applied to solve any learning task using just two lines of code. At any stage of the learning process, RGD simply reweights a data point as the exponential of its loss. We empirically demonstrate that the RGD reweighting algorithm improves the performance of numerous learning algorithms across various tasks, ranging from supervised learning to meta learning. Notably, we show improvements over state-of-the-art methods on &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;Tabular classification&lt;/a>;. Moreover, the RGD algorithm also boosts performance for &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; using the &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;GLUE&lt;/a>; benchmarks and &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.pdf&quot;>;ViT on ImageNet-1K&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Distributionally robust optimization&lt;/h2>; &lt;p>; &lt;a href=&quot;https://arxiv.org/pdf/1610.03425.pdf&quot;>;Distributionally robust optimization&lt;/a>; (DRO) is an approach that assumes a “worst-case” data distribution shift may occur, which can harm a model&#39;s performance. If a model has focussed on identifying few spurious features for prediction, these “worst-case” data distribution shifts could lead to the misclassification of samples and, thus, a performance drop. DRO optimizes the loss for samples in that “worst-case” distribution, making the model robust to perturbations (eg, removing a small fraction of points from a dataset, minor up/down weighting of data points, etc.) in the data distribution. In the context of classification, this forces the model to place less emphasis on noisy features and more emphasis on useful and predictive features. Consequently, models optimized using DRO tend to have &lt;a href=&quot;https://arxiv.org/pdf/1610.03425.pdf&quot;>;better generalization guarantees and stronger performance&lt;/a>; on unseen samples. &lt;/p>; &lt;p>; Inspired by these results, we develop the RGD algorithm as a technique for solving the DRO objective. Specifically, we focus on &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;>;Kullback–Leibler divergence&lt;/a>;-based DRO, where one adds perturbations to create distributions that are close to the original data distribution in the KL divergence metric, enabling a model to perform well over all possible perturbations. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJBB5nJdefCX0idWac3WKaJgb9UfvljskRgcYPrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s1999/image7.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1236&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhAuznJBUTcLrt6urFhWEkWCLviEMHf3KsvQAGd1JZSusTYW1kbTYg7mz-mmHUQtmYCHuclpML5JqBhCHB0OmYChuI62CJBB5nJdefCX0idWac3WKaJgb9UfvljskRgcYPrLnakAdnaNFRsWI8nQ3cPTy4pSdABX8k4tg3UrXaFbcT3I9pLuKxbl9UpQ80/s16000/image7.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Figure illustrating DRO. In contrast to &lt;a href=&quot;http://web.mit.edu/6.962/www/www_spring_2001/emin/slt.pdf&quot;>;ERM&lt;/a>;, which learns a model that minimizes expected loss over original data distribution, DRO learns a model that performs well on several perturbed versions of the original data distribution.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Stochastic re-weighted gradient descent&lt;/h2>; &lt;p>; Consider a random subset of samples (called a mini-batch), where each data point has an associated &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss&quot;>;loss&lt;/a>; &lt;i>;L&lt;sub>;i&lt;/sub>;&lt;/i>;. Traditional algorithms like SGD give equal importance to all the samples in the mini-batch, and update the parameters of the model by descending along the averaged gradients of the loss of those samples. With RGD, we reweight each sample in the mini-batch and give more importance to points that the model identifies as more difficult. To be precise, we use the loss as a proxy to calculate the difficulty of a point, and reweight it by the exponential of its loss. Finally, we update the model parameters by descending along the weighted average of the gradients of the samples. &lt;/p>; &lt;p>; Due to stability considerations, in our experiments we clip and scale the loss before computing its exponential. Specifically, we clip the loss at some threshold &lt;em>;T&lt;/em>;, and multiply it with a scalar that is inversely proportional to the threshold. An important aspect of RGD is its simplicity as it doesn&#39;t rely on a meta model to compute the weights of data points. Furthermore, it can be implemented with two lines of code, and combined with any popular optimizers (such as &lt;a href=&quot;https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf&quot;>;SGD&lt;/a>;, &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;>;Adam&lt;/a>;, and &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;>;Adagrad&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH-o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXxfdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobeDrmSSh1sb/s1973/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1541&quot; data-original-width=&quot;1973&quot; height=&quot;500&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg71xwsL38tNMVHtrY7NqXEzpFmHH-o6E9lVSsVVTks8JnrU1uHBg9CA8ZaIBpX-bi0F1w-LBZAcnco0JXxfdOup-6Qn4fk_DC0MAVT1ytwoi8TCeYfNqBPAGwBe6raUde9O7FibdDNjz4_7pMA3poNl-o_6bZM0r3VbWfzSacWhY019yvTobeDrmSSh1sb/w640-h500/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Figure illustrating the intuitive idea behind RGD in a binary classification setting. Feature 1 and Feature 2 are the features available to the model for predicting the label of a data point. RGD upweights the data points with high losses that have been misclassified by the model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Results&lt;/h2>; &lt;p>; We present empirical results comparing RGD with state-of-the-art techniques on standard supervised learning and domain adaptation (refer to the &lt;a href=&quot;https://arxiv.org/abs/2306.09222&quot;>;paper&lt;/a>; for results on meta learning). In all our experiments, we tune the clipping level and the learning rate of the optimizer using a held-out validation set. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Supervised learning&lt;/h3>; &lt;p>; We evaluate RGD on several supervised learning tasks, including language, vision, and tabular classification. For the task of language classification, we apply RGD to the &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; model trained on the &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;General Language Understanding Evaluation&lt;/a>; (&lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;>;GLUE&lt;/a>;) benchmark and show that RGD outperforms the BERT baseline by +1.94% with a standard deviation of 0.42%. To evaluate RGD&#39;s performance on vision classification, we apply RGD to the ViT-S model trained on the ImageNet-1K dataset, and show that RGD outperforms the ViT-S baseline by +1.01% with a standard deviation of 0.23%. Moreover, we perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;hypothesis tests&lt;/a>; to confirm that these results are statistically significant with a p-value that is less than 0.05.&lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgpCOgzhBRzNLyV0OkHlZskXpqOOmCV-xZyZ6NQpuC85jen-YxDfhF-usexsm-lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s1999/image8.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1334&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEioW7ubyangDExeb1OJ8vWgXuU9VnDkeSwOqY1pmRHq22Pyym63a8_WHjd6W3yiXLUDPgPEXQkS31aOnkwgpCOgzhBRzNLyV0OkHlZskXpqOOmCV-xZyZ6NQpuC85jen-YxDfhF-usexsm-lZDlGnoenyoEwS9slSplJOHmoPeySmwvi4Qr7c_KBko4PjaK/s16000/image8.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;RGD&#39;s performance on language and vision classification using GLUE and Imagenet-1K benchmarks. Note that MNLI, QQP, QNLI, SST-2, MRPC, RTE and COLA are diverse datasets which comprise the GLUE benchmark.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; For tabular classification, we use &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>; as our baseline, and consider various binary and multi-class datasets from &lt;a href=&quot;https://archive.ics.uci.edu/&quot;>;UC Irvine&#39;s machine learning repository&lt;/a>;. We show that applying RGD to the &lt;a href=&quot;https://arxiv.org/abs/2206.08564&quot;>;MET&lt;/a>; framework improves its performance by 1.51% and 1.27% on binary and multi-class tabular classification, respectively, achieving state-of-the-art performance in this domain. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOOp_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK44LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYobZ13Q2uashBWpbo58LiuXs2OO0UGKorkJ5VMm0rUUlJS92R6_kBrjRbAPAFRVj88xBEptVFuDSHtlbzjNQKNpfKI0lOOp_KJI_rXdxka8qddjeP5MuQATCRDCbnBAkfEYxqUK44LQ8cTZEUHDThR4aVqZ5_rdfx6931HvMVuY4OPh4G_Z96l6sjWuw9/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>; &lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTYPRLLFGO0R6XzCITFBUVJFsMXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0Uh_DCijOhzBjSzNyJoi92YCp2jD3n2k-l9aLuUWA8geS7QfkgzxZFj8WOseCbOfJu6hZxNFpAFDNFmBLSL7o0_bFHV6O0iZ6GPzderDKYpnW8QTYPRLLFGO0R6XzCITFBUVJFsMXfLUWK4qEffp70g4tEPloFdirGRpgClA0blEOZMPvf7TmxaKS9Vyw/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto;右边距：自动； text-align: center;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of RGD for classification of various tabular datasets.&lt;/td>;&lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Domain generalization&lt;/h3>; &lt;p>; To evaluate RGD&#39;s generalization capabilities, we use the standard &lt;a href=&quot;https://arxiv.org/abs/2007.01434&quot;>;DomainBed&lt;/a>; benchmark, which is commonly used to study a model&#39;s out-of-domain performance. We apply RGD to &lt;a href=&quot;https://arxiv.org/abs/2210.01360&quot;>;FRR&lt;/a>;, a recent approach that improved out-of-domain benchmarks, and show that RGD with FRR performs an average of 0.7% better than the FRR baseline. Furthermore, we confirm with &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;hypothesis tests&lt;/a>; that most benchmark results (except for Office Home) are statistically significant with a p-value less than 0.05. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLrL9Rrx3sqx8CImE00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4Ty3RRHX6IW4BcpQRQhbilp1-nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1232&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhv_KdUdYK72dakGtm0fkZJGsJqe3jRCL1EXAdpC98gHDcHpXirdr2qiMMQfLxFyUrEi5NLrL9Rrx3sqx8CImE00srUKH51MYvsvXzSWImhDBvjokiPE7Oz76CZHr4Ty3RRHX6IW4BcpQRQhbilp1-nw6D0BqPxNuvpx1vaB5Auv2Gtb-y_INiFNTCJmaV9/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of RGD on DomainBed benchmark for distributional shifts. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Class imbalance and fairness&lt;/h3>; &lt;p>; To demonstrate that models learned using RGD perform well despite class imbalance, where certain classes in the dataset are underrepresented, we compare RGD&#39;s performance with ERM on &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html&quot;>;long-tailed CIFAR-10&lt;/a>;. We report that RGD improves the accuracy of baseline ERM by an average of 2.55% with a standard deviation of 0.23%. Furthermore, we perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_hypothesis_testing&quot;>;hypothesis tests&lt;/a>; and confirm that these results are statistically significant with a p-value of less than 0.05. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pFKynE_4kcU1u3ERg_s35uKaRvc2PqvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1356&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBesU8uGmy9DLXZMZyC94wiLHs_swM94ZcUxhyphenhyphen6IbKHwzpDjwH-L8q2E-8iRQ9TmDC2QCWSNqZKm8pFKynE_4kcU1u3ERg_s35uKaRvc2PqvNMOPTBBcMu4ciXtkwudlY7wFXR0DHvu0lUccgcRNi5ZQTHZdaQyKXl7AXwl9sUBSjoLpdUHxztgUzVWgHw/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Performance of RGD on the long-tailed Cifar-10 benchmark for class imbalance domain. &lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Limitations &lt;/h2>; &lt;p>; The RGD algorithm was developed using popular research datasets, which were already curated to remove corruptions (eg, noise and incorrect labels). Therefore, RGD may not provide performance improvements in scenarios where training data has a high volume of corruptions. A potential approach to handle such scenarios is to apply an &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/375663.375668&quot;>;outlier removal technique&lt;/a>; to the RGD algorithm. This outlier removal technique should be capable of filtering out outliers from the mini-batch and sending the remaining points to our algorithm. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; RGD has been shown to be effective on a variety of tasks, including out-of-domain generalization, tabular representation learning, and class imbalance. It is simple to implement and can be seamlessly integrated into existing algorithms with just two lines of code change. Overall, RGD is a promising technique for boosting the performance of DNNs, and could help push the boundaries in various domains. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The paper described in this blog post was written by Ramnath Kumar, Arun Sai Suggala, Dheeraj Nagaraj and Kushal Majmundar. We extend our sincere gratitude to the anonymous reviewers, Prateek Jain, Pradeep Shenoy, Anshul Nasery, Lovish Madaan, and the numerous dedicated members of the machine learning and optimization team at Google Research India for their invaluable feedback and contributions to this work.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2311847718846675808/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2311847718846675808&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/re-weighted-gradient-descent-via.html&quot; rel=&quot;alternate&quot; title=&quot;Re-weighted gradient descent via distributionally robust optimization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONjsYNtXi5AX4ZZ6qPfdY2Gwt2W5xZy1PF1m9ZaxucZtRZ5ZzmsNzHNNWkCKcpX1_n15DkZCR59ivF_uDCUUmbdijaAuiK3tlC9HahtKc1s1r6pQWwbwMhnuNK3Guu5G5QigWU6p4yaJXCBLvIosDHXwtyhxMVfplzK4wTXiwCwOGo1B2aFidcgtaZfN_/s72-c/hero%20RGD.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-109886617977245321&lt;/id>;&lt;published>;2023-09-26T07:10:00.000-07:00&lt;/published>;&lt;updated>;2023-09-26T07:10:39.655-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Neural Networks&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Google Research embarks on effort to map a mouse brain&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Michał Januszewski, Research Scientist, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV_ccWS2g1Qcjssq2HmmGCqbaL_ij-UroaO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s320/connectome%20600x580.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; The human brain is perhaps the most computationally complex machine in existence, consisting of &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_brain#:~:text=brainstem.%5B37%5D-,Microanatomy,-%5Bedit%5D&quot;>;networks of billions of cells&lt;/a>;. Researchers currently don&#39;t understand the full picture of how glitches in its network machinery contribute to mental illnesses and other diseases, such as dementia. However, the emerging &lt;a href=&quot;https://en.wikipedia.org/wiki/Connectomics&quot;>;connectomics&lt;/a>; field, which aims to precisely map the connections between every cell in the brain, could help solve that problem. While maps have only been created for simpler organisms, technological advances for mapping even larger brains can enable us to understand how the human brain works, and how to treat brain diseases. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Today, we&#39;re &lt;a href=&quot;https://www.ninds.nih.gov/news-events/highlights-announcements/nih-brain-initiative-launches-projects-develop-innovative-technologies-map-brain-incredible-detail&quot;>;excited to announce&lt;/a>; that the &lt;a href=&quot;https://research.google/teams/connectomics/&quot;>;Connectomics team&lt;/a>; at Google Research and our collaborators are launching a &lt;a href=&quot;https://reporter.nih.gov/project-details/10665380&quot;>;$33 million project&lt;/a>; to expand the frontiers of connectomics over the next five years. Supported by the &lt;a href=&quot;https://braininitiative.nih.gov/&quot;>;Brain Research Through Advancing Innovative Neurotechnologies&lt;/a>; (BRAIN) Initiative at the &lt;a href=&quot;https://www.nih.gov/&quot;>;National Institutes of Health&lt;/a>; (NIH) and led by researchers at &lt;a href=&quot;https://lichtmanlab.fas.harvard.edu/&quot;>;Harvard University&lt;/a>;, we&#39;ll be working alongside a multidisciplinary team of experts from the &lt;a href=&quot;https://alleninstitute.org/division/brain-science/&quot;>;Allen Institute&lt;/a>;, &lt;a href=&quot;https://fietelab.mit.edu/&quot;>;MIT&lt;/a>;, &lt;a href=&quot;https://www2.mrc-lmb.cam.ac.uk/group-leaders/h-to-m/gregory-jefferis/&quot;>;Cambridge University&lt;/a>;, &lt;a href=&quot;https://pni.princeton.edu/&quot;>;Princeton University&lt;/a>; and &lt;a href=&quot;https://www.jhuapl.edu/&quot;>;Johns Hopkins University&lt;/a>;, with advisers from &lt;a href=&quot;https://www.janelia.org/lab/hess-lab&quot;>;HHMI&#39;s Janelia Research Campus&lt;/a>;. Our project goal is to tackle an immense challenge in neuroscience: mapping a tiny fraction (2-3%) of the mouse brain. We will specifically target the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5690575/&quot;>;hippocampal&lt;/a>; region, which is responsible for encoding memories, attention and spatial navigation. This project is one of 11 funded by the NIH&#39;s $150 million &lt;a href=&quot;https://braininitiative.nih.gov/funding-opportunies/brain-initiative-connectivity-across-scales-brain-connects-comprehensive-0&quot;>;BRAIN Initiative Connectivity Across Scales&lt;/a>; (BRAIN CONNECTS) program. Google Research is contributing computational and analytical resources to this effort, and will not receive any funding from the NIH. Our project asks a critical question: Can we scale and speed up our technologies enough to map the whole connectome of a mouse brain? &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The modern era of connectomics&lt;/h2>; &lt;p>; This effort to map the connectome of a small part of the mouse brain builds on a decade of innovation in the field, including many advances initiated by the Connectomics team at Google Research. We hope to accomplish something similar to the early days of the &lt;a href=&quot;https://www.genome.gov/human-genome-project&quot;>;Human Genome Project&lt;/a>;, when scientists worked for years to sequence a small portion of the human genome as they refined technologies that would enable them to complete the rest of the genome. &lt;/p>; &lt;p>; In 2021, we and collaborators at Harvard successfully &lt;a href=&quot;https://blog.research.google/2021/06/a-browsable-petascale-reconstruction-of.html&quot;>;mapped one cubic millimeter of the human brain&lt;/a>;, which we released as the &lt;a href=&quot;https://h01-release.storage.googleapis.com/landing.html&quot;>;H01&lt;/a>; dataset, a resource for studying the human brain and scaling connectomics technologies. But mapping the entire human brain connectome would require gathering and analyzing as much as a zettabyte of data (one billion terabytes), which is beyond the current capabilities of existing technologies. &lt;/p>; &lt;p>; Analyzing a mouse connectome is the next best thing. It is small enough to be technically feasible and could potentially deliver insights relevant to our own minds; neuroscientists already use mice to study human brain function and dysfunction. By working together to map 10–15 cubic mm of the mouse brain, we hope to develop new approaches that will allow us to map the entire remainder of the mouse brain, and the human brain thereafter. &lt;/p>; &lt;p>; &lt;/p>;&lt;table>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3wC7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/s960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;540&quot; data-original-width=&quot;960&quot; height=&quot;360&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZ3vCYyxz8BDdHUzAEYteV7DWtaSXT7q3ajif8uJXF-Aqfw4-fNrAaVxXU4xnevMe74eCKHtw_OGG3wC7wZUCn4hcN3snTPgqFfD9HrYqft0SoyZ1bN2cGFFEjGoGEU_GV3go90LahMLyCGrPFGVRaniC-ngxMgNiQoGM_03aln2kTqIDGUqXPMZcgH1-W/w640-h360/image3.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Neuroscientists have been working for decades to map increasingly larger and more complicated connectomes.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;One of biology&#39;s largest datasets&lt;/h2>; &lt;p>; In this connectomics project, we will map the connectome of the &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-76447-9_1&quot;>;hippocampal formation&lt;/a>; of the mouse brain, which converts short-term memories into long-term memories and helps the mouse navigate in space. The mouse hippocampal formation is the largest area of any brain we&#39;ve attempted to understand in this way. Through mapping this region of the mouse brain, we will create one of the largest datasets in biology, combining about 25,000 terabytes, or 25 petabytes of brain data. For reference, there are about 250 billion stars in our Milky Way Galaxy. If each of those stars was a single byte, it would take 100,000 Milky Way Galaxies to match the 25 petabytes of data that the project will collect when mapping a small region of the mouse brain. &lt;/p>; &lt;p>; To illustrate the hippocampal project&#39;s scale, we calculated the number of Pixel phones (shown as stacks of Pixels below) needed to store the image data from the completed connectome projects that mapped the roundworm and fruit fly brains, as well as for the mouse hippocampal region and entire mouse brain projects, which are just getting started. &lt;/p>; &lt;p>; Then, we compared the heights of each Pixel stack to familiar objects and landmarks. It would take a stack of 100 Pixels, as tall as a four-year-old girl, to store the image data for the fruit fly brain, the largest completed project thus far. In contrast, the mouse hippocampal connectome effort will require storage equivalent to more than 48,800 Pixels, reaching as high as the Empire State Building. The animation below shows how the mouse hippocampal project will surpass the scale of previous connectome projects. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU61NDijkDpPgnz2eqswz_J_alrpb2FColxo17BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitPA5h7cSox988b0Xr2cEjrzf0word6JAtcNjtU1S7QF4vsULGgdqSR1KjCh0jibAR-keO1kIkIX0ooyGgZaC9wU61NDijkDpPgnz2eqswz_J_alrpb2FColxo17BnxOVytsuq9gIwrWmzmz52tzsE4qUu8sVnPoobeIMBToilH58YpWtNwxCLi5Bi_ASV/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We are partnering with several collaborators to build a connectome (a map of the connections between brain cells) for the hippocampal region of a mouse brain. This project will create the largest connectomic dataset ever, surpassing the scale of previous projects that mapped the smaller roundworm and fruit fly brains. We hope this effort will lead to the development of new approaches that will allow us to later map an entire mouse brain. This animation shows how the field of connectomics is scaling up by calculating the number of Pixel phones needed to store the data from various projects. It would take just two Pixels, the height of an olive, to store the roundworm connectome data, while it would take a stack of Pixels the size of Mount Everest to store the data from an entire mouse connectome.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Understanding the connectome of the mouse hippocampal formation could help illuminate the way our own brains work. For instance, we may find common features between this circuitry in the mouse brain and human brains that explain how we know where we are, how our brains associate memories with specific locations, and what goes wrong in people who can&#39;t properly form new spatial memories. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Opening the petabyte pipeline&lt;/h2>; &lt;p>; Over the last decade, our team has worked to develop tools for managing massive connectomic datasets, and extracting scientific value from them. But a mouse brain has 1,000 times more neurons than the brain of the &lt;em>;Drosophila&lt;/em>; fruit fly, an organism for which &lt;a href=&quot;https://blog.research.google/2020/01/releasing-drosophila-hemibrain.html&quot;>;we helped build a connectome for a large part of the brain&lt;/a>;. Starting the mouse brain connectome will challenge us to improve existing technologies to enable us to map more data faster than ever before. &lt;/p>; &lt;p>; We&#39;ll continue to refine our &lt;a href=&quot;https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html&quot;>;flood-filling networks&lt;/a>;, which use deep learning to trace, or “segment”, each neuron&#39;s path through three-dimensional brain volumes made from electron microscope data. We&#39;ll also extend the capabilities of our self-supervised learning technology, &lt;a href=&quot;https://blog.research.google/2022/11/multi-layered-mapping-of-brain-tissue.html&quot;>;SegCLR&lt;/a>;, which allows us to automatically extract key insights from segmented volumes, such as identifying cell type (eg, &lt;a href=&quot;https://en.wikipedia.org/wiki/Pyramidal_cell#:~:text=Pyramidal%20cells%2C%20or%20pyramidal%20neurons,cortex%20and%20the%20corticospinal%20tract.&quot;>;pyramidal neuron&lt;/a>;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Basket_cell&quot;>;basket neuron&lt;/a>;, etc.) and parts of each neuron (eg, axon, dendrite, etc.). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAnUv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOdZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s720/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;386&quot; data-original-width=&quot;720&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiBHvnYysuO3m0CehlpRcx_XgxxTDUhKsr4BPEW0ypOVs_q2uh8RvJQAu323SXu9IbWVmxPGTsCEqtpaOvbdzAnUv2TLlqFnPEbfOKE1XOIMSyS9OEtlcM3KCIsWOdZgDQ1AqkG2kX6YbrFAMrn8g8FTiLlxA6ASLPz_f2AvlkqcfYzLwBEVkNA7R3OWRD/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A flood filling network traces a neuron through three-dimensional brain space.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We will also continue to enhance the scalability and performance of our core connectomics infrastructure, such as &lt;a href=&quot;https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html&quot;>;TensorStore for storage&lt;/a>; and &lt;a href=&quot;https://github.com/google/neuroglancer&quot;>;Neuroglancer for visualization&lt;/a>;, in order to enable all of our computational pipelines and human analysis workflows to operate at these new scales of data. We&#39;re eager to get to work to discover what peering into a mouse&#39;s mind might tell us about our own. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;The mouse connectomics project described in this blog post will be supported in part by the NIH BRAIN Initiative under award number 1UM1NS132250. Google Research is contributing computational and analytical resources to the mouse connectome project, and will not receive funding from the NIH. Many people were involved in the development of the technologies that make this project possible. We thank our long-term academic collaborators in the Lichtman Lab (Harvard University), HHMI Janelia, and the Denk Lab (Max Planck Institute for Biological Intelligence), and acknowledge core contributions from the Connectomics Team at Google. We also thank John Guilyard for creating the illustrative animation in this post, and Elise Kleeman, and Erika Check Hayden for their support. Thanks to Lizzie Dorfman, Michael Brenner, Jay Yagnik and Jeff Dean for their support, coordination and leadership.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/109886617977245321/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/google-research-embarks-on-effort-to.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/109886617977245321&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/google-research-embarks-on-effort-to.html&quot; rel=&quot;alternate&quot; title=&quot;Google Research embarks on effort to map a mouse brain&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFf6_wflZxOT_rmYJu0VCRvigHGMvE2QXwYJeh6F7GHmxnEtg7_bEraidqQ8uii_-N5EtBr2LfOGV_ccWS2g1Qcjssq2HmmGCqbaL_ij-UroaO6JtJVhyNbxK5PN57OfVlmkfXGSg3DMFawUlj1btghs5Nb9tmvQVSZH3pDoLK0SoKlhMh028164YArWcc/s72-c/connectome%20600x580.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-2762083676649407668&lt;/id>;&lt;published>;2023-09-21T14:25:00.000-07:00&lt;/published>;&lt;updated>;2023-09-21T14:25:35.521-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ACL&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Natural Language Processing&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Cheng-Yu Hsieh, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgbs7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Large language models (LLMs) have enabled a new data-efficient learning paradigm wherein they can be used to solve unseen new tasks via &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;zero-shot or few-shot prompting&lt;/a>;. However, LLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using &lt;a href=&quot;https://arxiv.org/abs/2201.12023&quot;>;specialized infrastructure&lt;/a>;, not to mention that today&#39;s state-of-the-art LLMs are composed of over &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;500 billion parameters&lt;/a>;. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To circumvent these deployment challenges, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;fine-tuning&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;distillation&lt;/a>;. Fine-tuning updates a pre-trained smaller model (eg, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;>;BERT&lt;/a>; or &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;T5&lt;/a>;) using downstream manually-annotated data. Distillation trains the same smaller models with labels generated by a larger LLM. Unfortunately, to achieve comparable performance to LLMs, fine-tuning methods require human-generated labels, which are expensive and tedious to obtain, while distillation requires large amounts of unlabeled data, which can also be hard to collect. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes&lt;/a>;”, presented at &lt;a href=&quot;https://2023.aclweb.org/&quot;>;ACL2023&lt;/a>;, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs&#39; performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;788&quot; data-original-width=&quot;1570&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;While LLMs offer strong zero and few-shot performance, they are challenging to serve in practice. On the other hand, traditional ways of training small task-specific models require a large amount of training data. Distilling step-by-step provides a new paradigm that reduces both the deployed model size as well as the number of data required for training.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Distilling step-by-step&lt;/h2>; &lt;p>; The key idea of distilling step-by-step is to extract informative &lt;em>;natural language&lt;/em>; &lt;em>;rationales (ie, &lt;/em>;intermediate reasoning steps)&lt;em>; &lt;/em>;from LLMs, which can in turn be used to train small models in a more data-efficient way. Specifically, natural language rationales explain the connections between the input questions and their corresponding outputs. For example, when asked, “&lt;em>;Jesse&#39;s room is 11 feet long and 15 feet wide. If she already has 16 square feet of carpet, how much more carpet does she need to cover the whole floor?&lt;/em>;”, an LLM can be prompted by the few-shot &lt;a href=&quot;https://blog.research.google/2022/05/language-models-perform-reasoning-via.html&quot;>;chain-of-thought&lt;/a>; (CoT) prompting technique to provide intermediate rationales, such as, “&lt;em>;Area = length * width. Jesse&#39;s room has 11 * 15 square feet.&lt;/em>;” That better explains the connection from the input to the final answer, “&lt;em>;(11 * 15 ) - 16&lt;/em>;”. These rationales can contain relevant task knowledge, such as “&lt;em>;Area = length * width”&lt;/em>;, that may originally require many data for small models to learn. We utilize these extracted rationales as additional, richer supervision to train small models, in addition to the standard task labels. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;932&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Overview on distilling step-by-step: First, we utilize CoT prompting to extract rationales from an LLM. We then use the generated rationales to train small task-specific models within a multi-task learning framework, where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; Distilling step-by-step consists of two main stages. In the first stage, we leverage few-shot CoT prompting to extract rationales from LLMs. Specifically, given a task, we prepare few-shot exemplars in the LLM input prompt where each example is composed of a triplet containing: (1) input, (2) rationale, and (3) output. Given the prompt, an LLM is able to mimic the triplet demonstration to generate the rationale for any new input. For instance, in a &lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;commonsense question answering task&lt;/a>;, given the input question “Sammy wanted to go to where the people are. Where might he go? Answer Choices: (a) populated areas, (b) race track, (c) desert, (d) apartment, (e) roadblock”, distilling step-by-step provides the correct answer to the question, “(a) populated areas”, paired with the rationale that provides better connection from the question to the answer, “The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people.” By providing CoT examples paired with rationales in the prompt, the &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;in-context learning ability&lt;/a>; allows LLMs to output corresponding rationales for future unseen inputs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1175&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We use the few-shot CoT prompting, which contains both an example rationale (&lt;strong>;highlighted in green&lt;/strong>;) and a label (&lt;strong>;highlighted in blue&lt;/strong>;), to elicit rationales from an LLM on new input examples. The example is from a commonsense question answering task.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;p>; After the rationales are extracted, in the second stage, we incorporate the rationales in training small models by framing the training process as a multi-task problem. Specifically, we train the small model with a novel &lt;em>;rationale generation task&lt;/em>; in addition to the standard &lt;em>;&lt;a href=&quot;https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html?m=1&quot;>;label prediction task&lt;/a>;&lt;/em>;. The rationale generation task enables the model to learn to generate the intermediate reasoning steps for the prediction, and guides the model to better predict the resultant label. We prepend &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;>;task prefixes&lt;/a>; (ie, [label] and [rationale] for label prediction and rationale generation, respectively) to the input examples for the model to differentiate the two tasks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Experimental setup&lt;/h2>; &lt;p>; In the experiments, we consider a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model as the LLM. For task-specific downstream models, we use &lt;a href=&quot;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&quot;>;T5 models&lt;/a>;. For CoT prompting, we use the &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;original CoT prompts&lt;/a>; when available and curate our own examples for new datasets. We conduct the experiments on four benchmark datasets across three different NLP tasks: &lt;a href=&quot;https://arxiv.org/abs/1812.01193&quot;>;e-SNLI&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1910.14599&quot;>;ANLI&lt;/a>; for&lt;a href=&quot;https://arxiv.org/abs/1508.05326&quot;>; natural language inference&lt;/a>;; &lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;>;CQA&lt;/a>; for commonsense question answering; and &lt;a href=&quot;https://arxiv.org/abs/2103.07191&quot;>;SVAMP&lt;/a>; for &lt;a href=&quot;https://aclanthology.org/N16-1136/&quot;>;arithmetic math word problems&lt;/a>;. We include two sets of baseline methods. For comparison to &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;>;few-shot prompted LLMs&lt;/a>;, we compare to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompting&lt;/a>; with a &lt;a href=&quot;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&quot;>;540B PaLM&lt;/a>; model. In the &lt;a href=&quot;https://arxiv.org/abs/2305.02301&quot;>;paper&lt;/a>;, we also compare standard task-specific model training to both &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;>;standard distillation&lt;/a>;. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Less training data&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;>;standard fine-tuning&lt;/a>;, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1477&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Smaller deployed model size&lt;/h3>; &lt;p>; Compared to &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;>;few-shot CoT prompted LLMs&lt;/a>;, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM&#39;s performance using standard fine-tuning. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1384&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, ie, Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, eg, over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM&#39;s performance using the same model size.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Distilling step-by-step outperforms few-shot LLMs with smaller models using less data&lt;/h3>; &lt;p>; Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM&#39;s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM&#39;s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1400&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM&#39;s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM&#39;s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Availability on Google Cloud Platform&lt;/h2>; &lt;p>; Distilling step-by-step is available for private preview on &lt;a href=&quot;https://cloud.google.com/vertex-ai&quot;>;Vertex AI&lt;/a>;. If you are interested in trying it out, please contact &lt;a href=&quot;mailto:vertex-llm-tuning-preview@google.com&quot;>;vertex-llm-tuning-preview@google.com&lt;/a>; with your Google Cloud Project number and a summary of your use case. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/2762083676649407668/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/2762083676649407668&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/distilling-step-by-step-outperforming.html&quot; rel=&quot;alternate&quot; title=&quot;Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7i39GKWT7_noICLVOMuf2x7v7wM21MXu9deGmnVlsieFv9m_rJOb5ytwiI_A9T3N4N1vZCVprlgbs7dePmQ1qjkcz-mT0nkeyLq-LmkF4oJB-ofCmrv81jqXMHPHe8Tl1dQSbDAPS9gADUUByZHBygkr-jlwbfIx3FM14n5cAbE7ZFVm84K6UDQLfeArm/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-7378908661087185403&lt;/id>;&lt;published>;2023-09-15T10:39:00.002-07:00&lt;/published>;&lt;updated>;2023-09-15T10:43:29.705-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Computer Vision&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Perception&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;On-device Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;MediaPipe FaceStylizer: On-device real-time few-shot face stylization&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Haolin Jia, Software Engineer, and Qifei Wang, Senior Software Engineer, Core ML&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s828/FaceStylizer-Hero.png&quot; style=&quot;display: none;&quot; />; &lt;p>; In recent years, we have witnessed rising interest across consumers and researchers in integrated augmented reality (AR) experiences using real-time face feature generation and editing functions in mobile applications, including short videos, virtual reality, and gaming. As a result, there is a growing demand for lightweight, yet high-quality face generation and editing models, which are often based on &lt;a href=&quot;https://arxiv.org/pdf/1406.2661.pdf&quot;>;generative adversarial network&lt;/a>; (GAN) techniques. However, the majority of GAN models suffer from high computational complexity and the need for a large training dataset. In addition, it is also important to employ GAN models responsibly. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In this post, we introduce &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;, an efficient design for few-shot face stylization that addresses the aforementioned model complexity and data efficiency challenges while being guided by Google&#39;s responsible &lt;a href=&quot;http://ai.google/principles&quot;>;AI Principles&lt;/a>;. The model consists of a face generator and a face encoder used as &lt;a href=&quot;https://arxiv.org/pdf/2101.05278.pdf&quot;>;GAN inversion&lt;/a>; to map the image into latent code for the generator. We introduce a mobile-friendly synthesis network for the face generator with an auxiliary head that converts features to &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot;>;RGB&lt;/a>; at each level of the generator to generate high quality images from coarse to fine granularities. We also carefully designed the &lt;a href=&quot;https://developers.google.com/machine-learning/gan/loss&quot;>;loss functions&lt;/a>; for the aforementioned auxiliary heads and combined them with the common GAN loss functions to distill the student generator from the teacher &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;>;StyleGAN&lt;/a>; model, resulting in a lightweight model that maintains high generation quality. The proposed solution is available in open source through &lt;a href=&quot;https://developers.google.com/mediapipe&quot;>;MediaPipe&lt;/a>;. Users can fine-tune the generator to learn a style from one or a few images using MediaPipe Model Maker, and deploy to on-device face stylization applications with the customized model using &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe FaceStylizer&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Few-shot on-device face stylization&lt;/h2>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An end-to-end pipeline&lt;/h3>; &lt;p>; Our goal is to build a pipeline to support users to adapt the MediaPipe FaceStylizer to different styles by fine-tuning the model with a few examples. To enable such a face stylization pipeline, we built the pipeline with a GAN inversion encoder and efficient face generator model (see below). The encoder and generator pipeline can then be adapted to different styles via a few-shot learning process. The user first sends a single or a few similar samples of the style images to MediaPipe ModelMaker to fine-tune the model. The fine-tuning process freezes the encoder module and only fine-tunes the generator. The training process samples multiple latent codes close to the encoding output of the input style images as the input to the generator. The generator is then trained to reconstruct an image of a person&#39;s face in the style of the input style image by optimizing a joint adversarial loss function that also accounts for style and content. With such a fine-tuning process, the MediaPipe FaceStylizer can adapt to the customized style, which approximates the user&#39;s input. It can then be applied to stylize test images of real human faces. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;Generator: BlazeStyleGAN&lt;/h3>; &lt;p>; The &lt;a href=&quot;https://arxiv.org/abs/1812.04948&quot;>;StyleGAN&lt;/a>; model family has been widely adopted for face generation and various face editing tasks. To support efficient on-device face generation, we based the design of our generator on StyleGAN. This generator, which we call &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.html&quot;>;BlazeStyleGAN&lt;/a>;, is similar to StyleGAN in that it also contains a mapping network and synthesis network. However, since the synthesis network of StyleGAN is the major contributor to the model&#39;s high computation complexity, we designed and employed a more efficient synthesis network. The improved efficiency and generation quality is achieved by: &lt;/p>; &lt;ol>; &lt;li>;Reducing the latent feature dimension in the synthesis network to a quarter of the resolution of the counterpart layers in the teacher StyleGAN, &lt;/li>; &lt;li>; Designing multiple auxiliary heads to transform the downscaled feature to the image domain to form a coarse-to-fine image pyramid to evaluate the perceptual quality of the reconstruction, and &lt;/li>; &lt;li>; Skipping all but the final auxiliary head at inference time. &lt;/li>; &lt;/ol>; &lt;p>; With the newly designed architecture, we train the BlazeStyleGAN model by distilling it from a teacher StyleGAN model. We use a multi-scale perceptual loss and adversarial loss in the distillation to transfer the high fidelity generation capability from the teacher model to the student BlazeStyleGAN model and also to mitigate the artifacts from the teacher model. &lt;/p>; &lt;p>; More details of the model architecture and training scheme can be found in &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Jia_BlazeStyleGAN_A_Real-Time_On-Device_StyleGAN_CVPRW_2023_paper.pdf&quot;>;our paper&lt;/a>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s1999/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;488&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3TpxAGbwALgLPFX9ocKbOTjYkRKvx28IY1jWHzM5mdgc3xTBFpn2MfbW1WpJPgjewSY2-zxRyo9FJEjxcx0gzaJxGf2bJGSuVkUUUTM0Eob60tjUuwaF-RUsGERGzl0o5LYkn7G0oFXB1UORri9RhnZ0FIvfbh3F5PFdt7IbVeeNw-yO6Ua8MB0Y2V5A/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Visual comparison between face samples generated by StyleGAN and BlazeStyleGAN. The images on the first row are generated by the teacher StyleGAN. The images on the second row are generated by the student BlazeStyleGAN. The face generated by BlazeStyleGAN has similar visual quality to the image generated by the teacher model. Some results demonstrate the student BlazeStyleGAN suppresses the artifacts from the teacher model in the distillation.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; In the above figure, we demonstrate some sample results of our BlazeStyleGAN. By comparing with the face image generated by the teacher StyleGAN model (top row), the images generated by the student BlazeStyleGAN (bottom row) maintain high visual quality and further reduce artifacts produced by the teacher due to the loss function design in our distillation. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h3>;An encoder for efficient GAN inversion&lt;/h3>; &lt;p>; To support image-to-image stylization, we also introduced an efficient GAN inversion as the encoder to map input images to the latent space of the generator. The encoder is defined by a &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;>;MobileNet V2&lt;/a>; backbone and trained with natural face images. The loss is defined as a combination of image perceptual quality loss, which measures the content difference, style similarity and embedding distance, as well as the &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization&quot;>;L1 loss&lt;/a>; between the input images and reconstructed images. &lt;/p>; &lt;br />; &lt;h2>;On-device performance&lt;/h2>; &lt;p>; We documented model complexities in terms of parameter numbers and computing &lt;a href=&quot;https://en.wikipedia.org/wiki/FLOPS&quot;>;FLOPs&lt;/a>; in the following table. Compared to the teacher StyleGAN (33.2M parameters), BlazeStyleGAN (generator) significantly reduces the model complexity, with only 2.01M parameters and 1.28G FLOPs for output resolution 256x256. Compared to StyleGAN-1024 (generating image size of 1024x1024), the BlazeStyleGAN-1024 can reduce both model size and computation complexity by 95% with no notable quality difference and can even suppress the artifacts from the teacher StyleGAN model. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Image Size&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;#Params (M)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;FLOPs (G)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;StyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;33.17 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;74.3 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1024 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.07 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;4.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;512 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.05 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.57 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;BlazeStyleGAN&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;2.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.28 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Encoder&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;256 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;1.44 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.60 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Model complexity measured by parameter numbers and FLOPs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We benchmarked the inference time of the MediaPipe FaceStylizer on various high-end mobile devices and demonstrated the results in the table below. From the results, both BlazeStyleGAN-256 and BlazeStyleGAN-512 achieved real-time performance on all GPU devices. It can run in less than 10 ms runtime on a high-end phone&#39;s GPU. BlazeStyleGAN-256 can also achieve real-time performance on the iOS devices&#39; CPU. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td>;&lt;strong>;Model&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;BlazeStyleGAN-256 (ms)&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;Encoder-256 (ms)&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 11&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.14 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.48 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 12&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.25 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;iPhone 13 Pro&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;7.22 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;5.41 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Pixel 6&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.24 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;11.23 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Samsung Galaxy S10&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;17.01 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;12.70 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;Samsung Galaxy S20&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.95 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;8.20 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Latency benchmark of the BlazeStyleGAN, face encoder, and the end-to-end pipeline on various mobile devices.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Fairness evaluation&lt;/h2>; &lt;p>; The model has been trained with a high diversity dataset of human faces. The model is expected to be fair to different human faces. The fairness evaluation demonstrates the model performs good and balanced in terms of human gender, skin-tone, and ages. &lt;/p>; &lt;br />; &lt;h2>;Face stylization visualization&lt;/h2>; &lt;p>; Some face stylization results are demonstrated in the following figure. The images in the top row (in orange boxes) represent the style images used to fine-tune the model. The images in the left column (in the green boxes) are the natural face images used for testing. The 2x4 matrix of images represents the output of the MediaPipe FaceStylizer which is blending outputs between the natural faces on the left-most column and the corresponding face styles on the top row. The results demonstrate that our solution can achieve high-quality face stylization for several popular styles. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIGBFYBgQiL-bpMHWRKDl8WF2rO-aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s940/image3.jpg&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;940&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpKZWwOnXcr79JiDtlbXO4sckZWVIpWtneSu3J4OgtIs6o6JtNpF8nbPkPpZqphMOP73LxX3MYS08OEEMQn3dAvmIGBFYBgQiL-bpMHWRKDl8WF2rO-aBca1uWr4y6p5sl8tvH1FgXMfXUyV-1c4UhuhPQ6QfTodu_tUdfkF-U257joD7tkkAAzv6AAoY/s16000/image3.jpg&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Sample results of our MediaPipe FaceStylizer.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;MediaPipe Solutions&lt;/h2>; &lt;p>; The MediaPipe FaceStylizer is going to be released to public users in &lt;a href=&quot;https://developers.google.com/mediapipe/solutions&quot;>;MediaPipe Solutions&lt;/a>;. Users can leverage &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Model Maker&lt;/a>; to train a customized face stylization model using their own style images. After training, the exported bundle of &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TFLite&lt;/a>; model files can be deployed to applications across platforms (Android, iOS, Web, Python, etc.) using the &lt;a href=&quot;https://developers.google.com/mediapipe/solutions/vision/face_stylizer&quot;>;MediaPipe Tasks FaceStylizer API&lt;/a>; in just a few lines of code. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is made possible through a collaboration spanning several teams across Google. We&#39;d like to acknowledge contributions from Omer Tov, Yang Zhao, Andrey Vakunov, Fei Deng, Ariel Ephrat, Inbar Mosseri, Lu Wang, Chuo-Ling Chang, Tingbo Hou, and Matthias Grundmann.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/7378908661087185403/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/7378908661087185403&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/mediapipe-facestylizer-on-device-real.html&quot; rel=&quot;alternate&quot; title=&quot;MediaPipe FaceStylizer: On-device real-time few-shot face stylization&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihspgvRDlnUmToZlZzCcRWCFIt9TRnLH9lepBq6ojBLxZ0nFEPBS7bQgOHY0klCfkewtZy5KSGnKYialzlh4xNSFP5Th4mvWJWcJ8XvNkCAJK9T7f_xHSK-H0uPW0paxcTG3nhQtP7eY7iKSVjX-Oaca182KHKiuGzj2C4yWT_kenY-Ys7LtS7i93RCtcP/s72-c/FaceStylizer-Hero.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-118669777610020383&lt;/id>;&lt;published>;2023-09-14T12:39:00.035-07:00&lt;/published>;&lt;updated>;2023-09-14T18:23:00.311-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;accessibility&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;On-device content distillation with graph neural networks&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Gabriel Barcik and Duc-Hieu Tran, Research Engineers, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s971/ScreenGNN.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In today&#39;s digital age, smartphones and desktop web browsers serve as the primary tools for accessing news and information. However, the proliferation of website clutter — encompassing complex layouts, navigation elements, and extraneous links — significantly impairs both the reading experience and article navigation. This issue is particularly acute for individuals with accessibility requirements. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; To improve the user experience and make reading more accessible, &lt;a href=&quot;https://www.android.com/google-features-on-android/december-2022/#accessibility&quot;>;Android&lt;/a>; and &lt;a href=&quot;https://blog.google/outreach-initiatives/education/chromebook-updates-2023/&quot;>;Chrome&lt;/a>; users may leverage the Reading Mode feature, which enhances accessibility by processing webpages to allow customizable contrast, adjustable text size, more legible fonts, and to enable text-to-speech utilities. Additionally, Android&#39;s Reading Mode is equipped to distill content from apps. Expanding Reading Mode to encompass a wide array of content and improving its performance, while still operating locally on the user&#39;s device without transmitting data externally, poses a unique challenge. &lt;/p>; &lt;p>; To broaden Reading Mode capabilities without compromising privacy, we have developed a novel on-device content distillation model. Unlike early attempts using &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>; — a heuristic approach limited to news articles — our model excels in both quality and versatility across various types of content. We ensure that article content doesn&#39;t leave the confines of the local environment. Our on-device content distillation model smoothly transforms long-form content into a simple and customizable layout for a more pleasant reading journey while also outperforming the leading alternative approaches. Here we explore details of this research highlighting our approach, methodology, and results. &lt;/p>; &lt;br />; &lt;h2>;Graph neural networks&lt;/h2>; &lt;p>; Instead of relying on complicated heuristics that are difficult to maintain and scale to a variety of article layouts, we approach this task as a fully &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;>;supervised learning&lt;/a>; problem. This data-driven approach allows the model to generalize better across different layouts, without the constraints and fragility of heuristics. Previous work for optimizing the reading experience relied on HTML or parsing, filtering, and modeling of a &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction&quot;>;document object model&lt;/a>; (DOM), a programming interface automatically generated by the user&#39;s web browser from site HTML that represents the structure of a document and allows it to be manipulated. &lt;/p>; &lt;p>; The new Reading Mode model relies on &lt;a href=&quot;https://developer.chrome.com/blog/full-accessibility-tree/#what-is-the-accessibility-tree&quot;>;accessibility trees&lt;/a>;, which provide a streamlined and more accessible representation of the DOM. Accessibility trees are automatically generated from the DOM tree and are utilized by assistive technologies to allow people with disabilities to interact with web content. These are available on Chrome Web browser and on Android through &lt;a href=&quot;https://developer.android.com/reference/android/view/accessibility/AccessibilityNodeInfo&quot;>;AccessibilityNodeInfo&lt;/a>; objects, which are provided for both WebView and native application content. &lt;/p>; &lt;p>; We started by manually collecting and annotating accessibility trees. The Android dataset used for this project comprises on the order of 10k labeled examples, while the Chrome dataset contains approximately 100k labeled examples. We developed a novel tool that uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_neural_network&quot;>;graph neural networks&lt;/a>; (GNNs) to distill essential content from the accessibility trees using a multi-class supervised learning approach. The datasets consist of long-form articles sampled from the web and labeled with classes such as headline, paragraph, images, publication date, etc. &lt;/p>; &lt;p>; GNNs are a natural choice for dealing with tree-like data structures, because unlike traditional models that often demand detailed, hand-crafted features to understand the layout and links within such trees, GNNs learn these connections naturally. To illustrate this, consider the analogy of a family tree. In such a tree, each node represents a family member and the connections denote familial relationships. If one were to predict certain traits using conventional models, features like the &quot;number of immediate family members with a trait&quot; might be needed. However, with GNNs, such manual feature crafting becomes redundant. By directly feeding the tree structure into the model, GNNs utilize a message-passing mechanism where each node communicates with its neighbors. Over time, information gets shared and accumulated across the network, enabling the model to naturally discern intricate relationships. &lt;/p>; &lt;p>; Returning to the context of accessibility trees, this means that GNNs can efficiently distill content by understanding and leveraging the inherent structure and relationships within the tree. This capability allows them to identify and possibly omit non-essential sections based on the information flow within the tree, ensuring more accurate content distillation. &lt;/p>; &lt;p>; Our architecture heavily follows the &lt;a href=&quot;https://arxiv.org/abs/1806.01261&quot;>;encode-process-decode&lt;/a>; paradigm using a &lt;a href=&quot;https://arxiv.org/abs/1704.01212&quot;>;message-passing neural network&lt;/a>; to classify text nodes. The overall design is illustrated in the figure below. The tree representation of the article is the input to the model. We compute lightweight features based on bounding box information, text information, and accessibility roles. The GNN then propagates each node&#39;s latent representation through the edges of the tree using a message-passing neural network. This propagation process allows nearby nodes, containers, and text elements to share contextual information with each other, enhancing the model&#39;s understanding of the page&#39;s structure and content. Each node then updates its current state based on the message received, providing a more informed basis for classifying the nodes. After a fixed number of message-passing steps, the now contextualized latent representations of the nodes are decoded into essential or non-essential classes. This approach enables the model to leverage both the inherent relationships in the tree and the hand-crafted features representing each node, thereby enriching the final classification. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHwbSlNBQiYUY7Q9ZAg-cJbR8SpU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s1600/image1.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC2RnSYjrCvHDVNxzOHlR3duUISGpF-xRz_Xv3fGUn-Fg0FLfPrkBBnYgVq8kMpbCQ525gHfFWd5L2vMwHwbSlNBQiYUY7Q9ZAg-cJbR8SpU9-SQDuJ9k49Rd9Vg9b9TTenlIIgcALirLbBa6mMHyl5enrhystjwi0xOxy8gWedhGgB5SyQHi-e8YgSYjR/s16000/image1.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visual demonstration of the algorithm in action, processing an article on a mobile device. A graph neural network (GNN) is used to distill essential content from an article. 1. A tree representation of the article is extracted from the application. 2. Lightweight features are computed for each node, represented as vectors. 3. A message-passing neural network propagates information through the edges of the tree and updates each node representation. 4. Leaf nodes containing text content are classified as essential or non-essential content. 5. A decluttered version of the application is composed based on the GNN output.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; We deliberately restrict the feature set used by the model to increase its broad generalization across languages and speed up inference latency on user devices. This was a unique challenge, as we needed to create an on-device lightweight model that could preserve privacy. &lt;/p>; &lt;p>; Our final lightweight Android model has 64k parameters and is 334kB in size with a median latency of 800ms, while the Chrome model has 241k parameters, is 928kB in size, and has a 378ms median latency. By employing such on-device processing, we ensure that user data never leaves the device, reinforcing our &lt;a href=&quot;https://ai.google/responsibility/principles/&quot;>;responsible approach&lt;/a>; and commitment to user privacy. The features used in the model can be grouped into intermediate node features, leaf-node text features, and element position features. We performed &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;>;feature engineering&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_selection&quot;>;feature selection&lt;/a>; to optimize the set of features for model performance and model size. The final model was transformed into &lt;a href=&quot;https://www.tensorflow.org/lite&quot;>;TensorFlow Lite&lt;/a>; format to deploy as an on-device model on Android or Chrome. &lt;/p>; &lt;br />; &lt;h2>;Results&lt;/h2>; &lt;p>; We trained the GNN for about 50 epochs in a single GPU. The performance of the Android model on webpages and native application test sets is presented below: &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWoDA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s1339/AndroidQuality.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;487&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuC7wLsxGXfTVYqPvr7WMVSHtkC64DQwTlnbMrjZG-_EtJ7UPQO0O3vAiPPStbFESxgc4yronwPMS4MOK1o7b_qiun_zQnXzx6pdHp8mvGXzIlOf2508bsfyaRAaWoDA0th3vFLL_IQuyETrsbM3jNAunGaKkuwjCNn7CX0TfrZZdO21zlrj6XxsPEAhJz/s16000/AndroidQuality.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the content distillation metrics in Android for webpages and native apps. We report &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision, recall&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>; for three classes: non-essential content, headline, and main body text, including macro average and weighted average by number of instances in each class. Node metrics assess the classification performance at the granularity of the accessibility tree node, which is analogous to a paragraph level. In contrast, word metrics evaluate classification at an individual word level, meaning each word within a node gets the same classification.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;13&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Android Quality &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;Webpages&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td colspan=&quot;5&quot; style=&quot;font-size: large; text-align: center;&quot;>;&lt;strong>;Native Apps&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td style=&quot;font-size: normal;&quot;>;&lt;strong>;&lt;em>;Node Metrics&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall &lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;non-essential&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9842 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9846 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9844 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9744 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9350 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9543 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;headline&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9187 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9784 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9476 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9183 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8568 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8865 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;main-text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9223 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9172 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9197 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8443 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9424 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.8907 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;macro-average&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9417 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9600 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9506 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9124 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9114 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9105 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;weighted average&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9736 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9392 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9353 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.9363 &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#e8eaed&quot;>; &lt;td>;&lt;strong>;&lt;em>;Word Metrics&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall &lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Precision&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Recall&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;F1-score&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;&lt;em>;headline + main-text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9510&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9683&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9595&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9473&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9507&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;strong>;0.9490&lt;/strong>; &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the content distillation metrics in Android for webpages and native apps. We report &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;>;precision, recall&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/F-score&quot;>;F1-score&lt;/a>; for three classes: non-essential content, headline, and main body text, including macro average and weighted average by number of instances in each class. Node metrics assess the classification performance at the granularity of the accessibility tree node, which is analogous to a paragraph level. In contrast, word metrics evaluate classification at an individual word level, meaning each word within a node gets the same classification.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;p>; In assessing the results&#39; quality on commonly visited webpage articles, an F1-score exceeding 0.9 for main-text (essentially paragraphs) corresponds to 88% of these articles being processed without missing any paragraphs. Furthermore, in over 95% of cases, the distillation proves to be valuable for readers. Put simply, the vast majority of readers will perceive the distilled content as both pertinent and precise, with errors or omissions being an infrequent occurrence. &lt;/p>; &lt;p>; The comparison of Chrome content distillation with other models such as &lt;a href=&quot;https://github.com/chromium/dom-distiller&quot;>;DOM Distiller&lt;/a>; or &lt;a href=&quot;https://github.com/mozilla/readability&quot;>;Mozilla Readability&lt;/a>; on a set of English language pages is presented in the table below. We reuse the metrics from machine translation to compare the quality of these models. The reference text is from the groundtruth main content and the text from the models as hypothesis text. The results show the excellent performance of our models in comparison to other DOM-based approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9StyysA89xcqSLyj7QWYi-Hc4eye-3c3WNlA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s1339/ChromeQuality.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;437&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgM3va2Pt2otiRp6UesiiUqxZygl24Pxv3EpTYAKcS_fIJEDq5I6DMq3Lcc_DzOEg0VsE41SvIMg9ISgrdAP27yVT9StyysA89xcqSLyj7QWYi-Hc4eye-3c3WNlA9SWIWqtt-cbcMJsPX13UJrPDoMuqTUTJdyp-sW1GqgxmAADmrsze4kCyXHV8dPnEZj/s16000/ChromeQuality.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the comparison between DOM-Distiller, Mozilla Readability and the new Chrome model. We report text-based metrics, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;, &lt;a href=&quot;https://aclanthology.org/W15-3049.pdf&quot;>;CHRF&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;, by comparing the main body text distilled from each model to a ground-truth text manually labeled by raters using our annotation policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;7&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Chrome Model Comparison on Webpages &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: center;&quot;>;&lt;strong>;&lt;em>;Metric / Model&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;DOM Distiller&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Mozilla Readability&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;Our Chrome model&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;BLEU&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;78.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;79.16 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.59 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;CHRF&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.92 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE1&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.10 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.62 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.13 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE2&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;82.66 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.81 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGE3&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;80.21 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;81.45 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;94.60 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;83.58 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.02 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ROUGEL-SUM&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;83.46 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;84.03 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;95.04 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents the comparison between DOM-Distiller, Mozilla Readability and the new Chrome model. We report text-based metrics, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/BLEU&quot;>;BLUE&lt;/a>;, &lt;a href=&quot;https://aclanthology.org/W15-3049.pdf&quot;>;CHRF&lt;/a>; and &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;>;ROUGE&lt;/a>;, by comparing the main body text distilled from each model to a ground-truth text manually labeled by raters using our annotation policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;p>; The F1-score of the Chrome content distillation model for headline and main text content on the test sets of different widely spoken languages demonstrates that the Chrome model, in particular, is able to support a wide range of languages. &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4-OC4_JFrieWniKArBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxeem_J7UTM/s1339/ChromeLanguages.png&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;195&quot; data-original-width=&quot;1339&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6Z1lggR6AGKL0sxRWLmKEN50R4-OC4_JFrieWniKArBKxSSSo1jlry-Q7yRkysHBX1JpLOy0lEGhWNf3UlmEErjJoT_BixDzAUDqHwuKb-XngC0d_6-ynB5cXlRtNUjvneK1M8fzxH1va8jggc5cfROR8DwaTEx-hW0taoeGPtNML3qQ8Trxeem_J7UTM/s16000/ChromeLanguages.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents per language of F1-scores of the Chrome model for the headline and main text classes. The language codes correspond to the following languages: German, English, Spanish, French, Italian, Persian, Japanese, Korean, Portuguese, Vietnamese, simplified Chinese and traditional Chinese.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;!-- &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>; &lt;td colspan=&quot;27&quot; style=&quot;font-size: x-large; text-align: center;&quot;>;Chrome Model on Different Languages &lt;/td>; &lt;/tr>; &lt;tr bgcolor=&quot;#d2e3fc&quot;>; &lt;td style=&quot;font-size: normal; text-align: left;&quot;>;&lt;strong>;&lt;em>;F1-score&lt;/em>;&lt;/strong>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;de&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;en&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;es&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fr&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;it&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;fa&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ja&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;ko&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;pt&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;vi&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hans&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;zh-Hant&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;&lt;em>;average&lt;/em>; &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;headline&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.89 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.99 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.98 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.97 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.96 &lt;/td>; &lt;/tr>; &lt;tr>; &lt;td style=&quot;text-align: left;&quot;>;&lt;em>;main text&lt;/em>; &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.84 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.93 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.87 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.88 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.91 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>;&lt;td>;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td>; &lt;td style=&quot;text-align: center;&quot;>;0.90 &lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>; &lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The table presents per language of F1-scores of the Chrome model for the headline and main text classes. The language codes correspond to the following languages: German, English, Spanish, French, Italian, Persian, Japanese, Korean, Portuguese, Vietnamese, simplified Chinese and traditional Chinese.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; -->; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The digital age demands both streamlined content presentation and an unwavering commitment to user privacy. Our research highlights the effectiveness of Reading Mode in platforms like Android and Chrome, offering an innovative, data-driven approach to content parsing through Graph Neural Networks. Crucially, our lightweight on-device model ensures that content distillation occurs without compromising user data, with all processes executed locally. This not only enhances the reading experience but also reinforces our dedication to user privacy. As we navigate the evolving landscape of digital content consumption, our findings underscore the paramount importance of prioritizing the user in both experience and security. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This project is the result of joint work with Manuel Tragut, Mihai Popa, Abodunrinwa Toki, Abhanshu Sharma, Matt Sharifi, David Petrou and Blaise Aguera y Arcas. We sincerely thank our collaborators Gang Li and Yang Li. We are very grateful to Tom Small for assisting us in preparing the post.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/118669777610020383/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/118669777610020383&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/on-device-content-distillation-with.html&quot; rel=&quot;alternate&quot; title=&quot;On-device content distillation with graph neural networks&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAbKikU6UWOjYuGg9JWDI3s8QKhXHtUEl2STZt_TNmwR4Y_B65GkYs--uMmYUYVVBdbTrtAVLRmlEGc1fTgeMS2E1kaNLmUJk6xWoj0qm0axNj2OcMzzPTZ68ygM0f7ZOo_8qoUXjTGGTO74-LZ9gvt9eK8lZjRDE0HWJNMJWYM_A2ppRGl5v8QVrxBEg7/s72-c/ScreenGNN.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6191878611070084392&lt;/id>;&lt;published>;2023-09-12T14:22:00.000-07:00&lt;/published>;&lt;updated>;2023-09-12T14:22:55.668-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Google Maps&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Publications&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Reinforcement Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;World scale inverse reinforcement learning in Google Maps&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Matt Barnes, Software Engineer, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; Routing in Google Maps remains one of our most helpful and frequently used features. Determining the best route from A to B requires making complex trade-offs between factors including the &lt;a href=&quot;https://www.deepmind.com/blog/traffic-prediction-with-advanced-graph-neural-networks&quot;>;estimated time of arrival&lt;/a>; (ETA), tolls, directness, surface conditions (eg, paved, unpaved roads), and user preferences, which vary across transportation mode and local geography. Often, the most natural visibility we have into travelers&#39; preferences is by analyzing real-world travel patterns. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Learning preferences from observed sequential decision making behavior is a classic application of &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning#Inverse_reinforcement_learning&quot;>;inverse reinforcement learning&lt;/a>; (IRL). Given a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;>;Markov decision process&lt;/a>; (MDP) — a formalization of the road network — and a set of demonstration trajectories (the traveled routes), the goal of IRL is to recover the users&#39; latent reward function. Although &lt;a href=&quot;https://link.springer.com/article/10.1007/s10462-021-10108-x&quot;>;past research&lt;/a>; has created increasingly general IRL solutions, these have not been successfully scaled to world-sized MDPs. Scaling IRL algorithms is challenging because they typically require solving an RL subroutine &lt;em>;at every update step&lt;/em>;. At first glance, even attempting to fit a world-scale MDP into memory to compute a single gradient step appears infeasible due to the large number of road segments and limited high bandwidth memory. When applying IRL to routing, one needs to consider all reasonable routes between each demonstration&#39;s origin and destination. This implies that any attempt to break the world-scale MDP into smaller components cannot consider components smaller than a metropolitan area. &lt;/p>; &lt;p>; To this end, in &quot;&lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;Massively Scalable Inverse Reinforcement Learning in Google Maps&lt;/a>;&quot;, we share the result of a multi-year collaboration among Google Research, Maps, and Google DeepMind to surpass this IRL scalability limitation. We revisit classic algorithms in this space, and introduce advances in graph compression and parallelization, along with a new IRL algorithm called Receding Horizon Inverse Planning (RHIP) that provides fine-grained control over performance trade-offs. The final RHIP policy achieves a 16–24% relative improvement in global route match rate, ie, the percentage of de-identified traveled routes that exactly match the suggested route in Google Maps. To the best of our knowledge, this represents the largest instance of IRL in a real world setting to date. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s1999/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1040&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIjGlEhwPBRCKI-_LLSIF_TRSB7FGohWSwbvTh0K-3wovXgGmWKBYFXBIKhdVOkqHPLq5F4ZLO0tZVXyyprnXDJ2UqzWmDLQV4RwXDqiMjAmbCTkcey7JauVHhhcakVEveWA4U4qnzokEli9E_PblSodxtsRO7hCRnmJBRvU-zRcxNCP_9pG4-Kuff2twW/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Google Maps improvements in route match rate relative to the existing baseline, when using the RHIP inverse reinforcement learning policy.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The benefits of IRL&lt;/h2>; &lt;p>; A subtle but crucial detail about the routing problem is that it is &lt;em>;goal conditioned&lt;/em>;, meaning that every destination state induces a slightly different MDP (specifically, the destination is a terminal, zero-reward state). IRL approaches are well suited for these types of problems because the learned reward function transfers across MDPs, and only the destination state is modified. This is in contrast to approaches that directly learn a policy, which typically require an extra factor of &lt;em>;S&lt;/em>; parameters, where &lt;em>;S&lt;/em>; is the number of MDP states. &lt;/p>; &lt;p>; Once the reward function is learned via IRL, we take advantage of a powerful inference-time trick. First, we evaluate the entire graph&#39;s rewards once in an &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/static-vs-dynamic-inference/video-lecture&quot;>;offline batch setting&lt;/a>;. This computation is performed entirely on servers without access to individual trips, and operates only over batches of road segments in the graph. Then, we save the results to an in-memory database and use a fast online &lt;a href=&quot;https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm&quot;>;graph search algorithm&lt;/a>; to find the highest reward path for routing requests between any origin and destination. This circumvents the need to perform online inference of a deeply parameterized model or policy, and vastly improves serving costs and latency. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/s791/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;406&quot; data-original-width=&quot;791&quot; height=&quot;328&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHgaS1N-fsKIzQTMZEelXDZ8TVZboZvSZ3Z9ZLPlYR48NtGRabWXer_1r38dLm4cCIF8lBuapSVBPWzkVdBkTG7fdFuOwbry0Shdg7_sR19FdVOichywmraMPVUvBLl3XYS2B0rY1JdroZxIqlWB0rjl-mIGV3gASY6IuxhEQgjVdAqVvZJL1IYJ3HU3zc/w640-h328/image2.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Reward model deployment using batch inference and fast online planners.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Receding Horizon Inverse Planning&lt;/h2>; &lt;p>; To scale IRL to the world MDP, we compress the graph and shard the global MDP using a sparse &lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_of_experts&quot;>;Mixture of Experts&lt;/a>; (MoE) based on geographic regions. We then apply classic IRL algorithms to solve the local MDPs, estimate the loss, and send gradients back to the MoE. The worldwide reward graph is computed by decompressing the final MoE reward model. To provide more control over performance characteristics, we introduce a new generalized IRL algorithm called Receding Horizon Inverse Planning (RHIP). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s1617/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;526&quot; data-original-width=&quot;1617&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHUdaeXJURVEV2f--mytRfbx5k3yftkMKBpuu43GKhyMaAa-fJC3O4QfTxZjz3hkGjUsCBiO6LLcYRsGeyrpOgIUzDDPW1lOwlr47YXKSLUwJuXxMELxRV9SKcr4BDoy5_2HzcFZX-Wb7m3PcQhWvx1efvG5gkXG_HfrhqrqfZ_xrNyBmSado-Wf_si3wH/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;IRL reward model training using MoE parallelization, graph compression, and RHIP.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; RHIP is inspired by people&#39;s tendency to perform extensive local planning (&quot;What am I doing for the next hour?&quot;) and approximate long-term planning (&quot;What will my life look like in 5 years?&quot;). To take advantage of this insight, RHIP uses robust yet expensive stochastic policies in the local region surrounding the demonstration path, and switches to cheaper deterministic planners beyond some horizon. Adjusting the horizon &lt;em>;H&lt;/em>; allows controlling computational costs, and often allows the discovery of the performance sweet spot. Interestingly, RHIP generalizes many classic IRL algorithms and provides the novel insight that they can be viewed along a stochastic vs. deterministic spectrum (specifically, for &lt;em>;H&lt;/em>;=∞ it reduces to &lt;a href=&quot;https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf&quot;>;MaxEnt&lt;/a>;, for &lt;em>;H&lt;/em>;=1 it reduces to &lt;a href=&quot;https://www.ijcai.org/Proceedings/07/Papers/416.pdf&quot;>;BIRL&lt;/a>;, and for &lt;em>;H&lt;/em>;=0 it reduces to &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1143844.1143936&quot;>;MMP&lt;/a>;). &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANax2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/s1039/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;422&quot; data-original-width=&quot;1039&quot; height=&quot;260&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVip8f74bfUV6t4yy3-smAgmUday76QLz1AyzFRIzsudZ-MA7vfQACQbm2_6OhbqQ_wA8ZY-aGtwlEb2bZiJs-Ltp98wTlxA92ndfccuowYt5lUa6Ve7ZIoANax2HWMF45mVXQDuQcNHfqZRLebAgOYBdUSlHi-7P6TMTszN5Pwe0jYvVUEHIjYPrigDTm/w640-h260/image5.png&quot; width=&quot;640&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Given a demonstration from s&lt;sub>;o&lt;/sub>; to s&lt;sub>;d&lt;/sub>;, (1) RHIP follows a robust yet expensive stochastic policy in the local region surrounding the demonstration (&lt;strong>;blue region&lt;/strong>;). (2) Beyond some horizon H, RHIP switches to following a cheaper deterministic planner (&lt;strong>;red lines&lt;/strong>;). Adjusting the horizon enables fine-grained control over performance and computational costs.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Routing wins&lt;/h2>; &lt;p>; The RHIP policy provides a 15.9% and 24.1% lift in global route match rate for driving and two-wheelers (eg, scooters, motorcycles, mopeds) relative to the well-tuned Maps baseline, respectively. We&#39;re especially excited about the benefits to more sustainable transportation modes, where factors beyond journey time play a substantial role. By tuning RHIP&#39;s horizon &lt;em>;H&lt;/em>;, we&#39;re able to achieve a policy that is both more accurate than all other IRL policies and 70% faster than MaxEnt. &lt;/p>; &lt;p>; Our 360M parameter reward model provides intuitive wins for Google Maps users in live &lt;a href=&quot;https://en.wikipedia.org/wiki/A/B_testing&quot;>;A/B experiments&lt;/a>;. Examining road segments with a large absolute difference between the learned rewards and the baseline rewards can help improve certain Google Maps routes. For example: &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s1245/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;426&quot; data-original-width=&quot;1245&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhghO43O0wC_DVBWbvclvbXruodtRbalSFRwKCmiNI1ws5NmlvDI6MwPhtLUkPCR5gfjiQ72lFRyI9WUFFX2VwPBNMC2KJxJjX49yrAhfNvU9_YFGQ2Z27K8VROnu9B4K7YNCmptiuoezWZZViqMMYonhmiYlgRnttMbpP2T44XLZE1kvm1bhSxfwgR-zNJ/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Nottingham, UK. The preferred route (&lt;strong>;blue&lt;/strong>;) was previously marked as private property due to the presence of a large gate, which indicated to our systems that the road may be closed at times and would not be ideal for drivers. As a result, Google Maps routed drivers through a longer, alternate detour instead (&lt;strong>;red&lt;/strong>;). However, because real-world driving patterns showed that users regularly take the preferred route without an issue (as the gate is almost never closed), IRL now learns to route drivers along the preferred route by placing a large positive reward on this road segment.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; Increasing performance via increased scale – both in terms of dataset size and model complexity – has proven to be a persistent trend in machine learning. Similar gains for inverse reinforcement learning problems have historically remained elusive, largely due to the challenges with handling practically sized MDPs. By introducing scalability advancements to classic IRL algorithms, we&#39;re now able to train reward models on problems with hundreds of millions of states, demonstration trajectories, and model parameters, respectively. To the best of our knowledge, this is the largest instance of IRL in a real-world setting to date. See the &lt;a href=&quot;https://arxiv.org/abs/2305.11290&quot;>;paper&lt;/a>; to learn more about this work. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This work is a collaboration across multiple teams at Google. Contributors to the project include Matthew Abueg, Oliver Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, Shawn O&#39;Banion, Ryan Epp, Renaud Hartert, Rui Song, Thomas Sharp, Rémi Robert, Zoltan Szego, Beth Luan, Brit Larabee and Agnieszka Madurska.&lt;/em>; &lt;/p>; &lt;p>; &lt;em>;We&#39;d also like to extend our thanks to Arno Eigenwillig, Jacob Moorman, Jonathan Spencer, Remi Munos, Michael Bloesch and Arun Ahuja for valuable discussions and suggestions.&lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6191878611070084392/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6191878611070084392&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/world-scale-inverse-reinforcement.html&quot; rel=&quot;alternate&quot; title=&quot;World scale inverse reinforcement learning in Google Maps&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgA5ZpGidOzCqTYTLV8bj62lAG7yfVa0cson-09oo7hqGA9ayl7h6koU96mxkBOqP_NyqiKamaoFrSAHtBlY7UH7XMnoq5Hn1H0hoiC5Uk0mMOkunNi6-j08iSEmUXTYEmp1YuFEgWLRJtieseqhQseMpy6e8KY5wElwNKDcy99GjCO-j04G0TVaJb0Pcr/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-237108111388574068&lt;/id>;&lt;published>;2023-09-08T15:59:00.002-07:00&lt;/published>;&lt;updated>;2023-09-11T10:00:14.121-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Security and Privacy&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;Differentially private median and more&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Edith Cohen and Uri Stemmer, Research Scientists, Google Research &lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s1554/dpmedian.png&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;>;Differential privacy&lt;/a>; (DP) is a rigorous mathematical definition of privacy. DP algorithms are randomized to protect user data by ensuring that the probability of any particular output is nearly unchanged when a data point is added or removed. Therefore, the output of a DP algorithm does not disclose the presence of any one data point. There has been significant progress in both foundational research and adoption of differential privacy with contributions such as the &lt;a href=&quot;https://privacysandbox.com/&quot;>;Privacy Sandbox&lt;/a>; and &lt;a href=&quot;https://opensource.googleblog.com/2020/06/expanding-our-differential-privacy.html&quot;>;Google Open Source Library&lt;/a>;. &lt;/p>;&lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; ML and data analytics algorithms can often be described as performing multiple basic computation steps on the same dataset. When each such step is differentially private, so is the output, but with multiple steps the overall privacy guarantee &lt;em>;deteriorates,&lt;/em>; a phenomenon known as the &lt;em>;cost of composition&lt;/em>;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy#Composability&quot;>;Composition theorems&lt;/a>; bound the increase in privacy loss with the number &lt;em>;k&lt;/em>; of computations: In the general case, the privacy loss increases with the square root of &lt;em>;k&lt;/em>;. This means that we need much stricter privacy guarantees for each step in order to meet our overall privacy guarantee goal. But in that case, we lose utility. One way to improve the privacy vs. utility trade-off is to identify when the use cases admit a tighter privacy analysis than what follows from composition theorems. &lt;/p>; &lt;p>; Good candidates for such improvement are when each step is applied to a disjoint part (slice) of the dataset. When the slices are selected in a data-independent way, each point affects only one of the &lt;em>;k&lt;/em>; outputs and the privacy guarantees do not deteriorate with &lt;em>;k&lt;/em>;. However, there are applications in which we need to select the slices adaptively (that is, in a way that depends on the output of prior steps). In these cases, a change of a single data point may cascade — changing multiple slices and thus increasing composition cost. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;Õptimal Differentially Private Learning of Thresholds and Quasi-Concave Optimization&lt;/a>;”, presented at &lt;a href=&quot;http://acm-stoc.org/stoc2023/&quot;>;STOC 2023&lt;/a>;, we describe a new paradigm that allows for slices to be selected adaptively and yet avoids composition cost. We show that DP algorithms for multiple fundamental aggregation and learning tasks can be expressed in this Reorder-Slice-Compute (RSC) paradigm, gaining significant improvements in utility. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;The Reorder-Slice-Compute (RSC) paradigm&lt;/h2>; &lt;p>; An algorithm &lt;em>;A&lt;/em>; falls in the RSC paradigm if it can be expressed in the following general form (see visualization below). The input is a sensitive set &lt;em>;D&lt;/em>; of data points. The algorithm then performs a sequence of &lt;em>;k&lt;/em>; steps as follows: &lt;/p>; &lt;ol>; &lt;li>;Select an ordering over data points, a slice size &lt;em>;m&lt;/em>;, and a DP algorithm &lt;em>;M&lt;/em>;. The selection may depend on the output of &lt;em>;A&lt;/em>; in prior steps (and hence is adaptive). &lt;/li>;&lt;li>;Slice out the (approximately) top &lt;em>;m&lt;/em>; data points according to the order from the dataset &lt;em>;D&lt;/em>;, apply &lt;em>;M&lt;/em>; to the slice, and output the result. &lt;/li>; &lt;/ol>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s666/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;478&quot; data-original-width=&quot;666&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj63itU6U--NeU1FGMjUslYsM63Ut85OAGE0bilRt8hy0-bxugE6CvLlNmtf1OtRjnJRsxXcOAPBeoOS2eEIb74whQCRn4ayBbacPR1j4I46TQsZ1Au0b3NTgEwh-GtLs1pGyxItUHx_vMHg-tq2XyyrbZ6SUKi1gDvX9udLoRi98eW5peyOJGpnu6Euzb8/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A visualization of three Reorder-Slice-Compute (RSC) steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; If we analyze the overall privacy loss of an RSC algorithm using DP composition theorems, the privacy guarantee suffers from the expected composition cost, ie, it deteriorates with the square root of the number of steps &lt;em>;k&lt;/em>;. To eliminate this composition cost, we provide a novel analysis that removes the dependence on &lt;em>;k&lt;/em>; altogether: the overall privacy guarantee is close to that of a single step! The idea behind our tighter analysis is a novel technique that limits the potential cascade of affected steps when a single data point is modified (details &lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;in the paper&lt;/a>;). &lt;/p>; &lt;p>; Tighter privacy analysis means better utility. The effectiveness of DP algorithms is often stated in terms of the smallest input size (number of data points) that suffices in order to release a correct result that meets the privacy requirements. We describe several problems with algorithms that can be expressed in the RSC paradigm and for which our tighter analysis improved utility. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private interval point&lt;/h2>; &lt;p>; We start with the following basic aggregation task. The input is a dataset &lt;em>;D&lt;/em>; of &lt;em>;n&lt;/em>; points from an ordered domain &lt;em>;X&lt;/em>; (think of the domain as the natural numbers between &lt;em>;1&lt;/em>; and&lt;em>; |X|&lt;/em>;). The goal is to return a point &lt;em>;y&lt;/em>; in &lt;em>;X&lt;/em>; that is in the &lt;em>;interval&lt;/em>; of &lt;em>;D&lt;/em>;, that is between the minimum and the maximum points in &lt;em>;D&lt;/em>;. &lt;/p>; &lt;p>; The solution to the interval point problem is trivial without the privacy requirement: simply return any point in the dataset &lt;em>;D&lt;/em>;. But this solution is not privacy-preserving as it discloses the presence of a particular datapoint in the input. We can also see that if there is only one point in the dataset, a privacy-preserving solution is not possible, as it must return that point. We can therefore ask the following fundamental question: What is the smallest input size &lt;em>;N&lt;/em>; for which we can solve the private interval point problem? &lt;/p>; &lt;p>; It is known that &lt;em>;N&lt;/em>; must increase with the domain size &lt;em>;|X|&lt;/em>; and that this dependence is at least the &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;iterated log function&lt;/a>;&lt;em>; &lt;/em>;log&lt;em>;* |X|&lt;/em>; [&lt;a href=&quot;https://ieeexplore.ieee.org/document/7354419&quot;>;1&lt;/a>;, &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3313276.3316312&quot;>;2&lt;/a>;]. On the other hand, the &lt;a href=&quot;https://proceedings.mlr.press/v125/kaplan20a.html&quot;>;best prior&lt;/a>; DP algorithm required the input size to be at least (log* &lt;em>;|X|&lt;/em>;)&lt;sup>;1.5&lt;/sup>;. To close this gap, we designed an RSC algorithm that requires only an order of log*&lt;em>; |X|&lt;/em>; points. &lt;/p>; &lt;p>; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Iterated_logarithm&quot;>;iterated log function&lt;/a>; is extremely slow growing: It is the number of times we need to take a logarithm of a value before we reach a value that is equal to or smaller than &lt;em>;1&lt;/em>;. How did this function naturally come out in the analysis? Each step of the RSC algorithm remapped the domain to a logarithm of its prior size. Therefore there were log* &lt;em>;|X|&lt;/em>; steps in total. The tighter RSC analysis eliminated a square root of the number of steps from the required input size. &lt;/p>; &lt;p>; Even though the interval point task seems very basic, it captures the essence of the difficulty of private solutions for common aggregation tasks. We next describe two of these tasks and express the required input size to these tasks in terms of &lt;em>;N&lt;/em>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private approximate median&lt;/h2>; &lt;p>; One of these common aggregation tasks is &lt;em>;approximate median&lt;/em>;: The input is a dataset &lt;em>;D&lt;/em>; of &lt;em>;n&lt;/em>; points from an ordered domain &lt;em>;X&lt;/em>;. The goal is to return a point &lt;em>;y&lt;/em>; that is between the ⅓ and ⅔ quantiles of &lt;em>;D&lt;/em>;. That is, at least a third of the points in &lt;em>;D&lt;/em>; are smaller or equal to &lt;em>;y&lt;/em>; and at least a third of the points are larger or equal to &lt;em>;y&lt;/em>;. Note that returning an exact median is not possible with differential privacy, since it discloses the presence of a datapoint. Hence we consider the relaxed requirement of an approximate median (shown below). &lt;/p>; &lt;p>; We can compute an approximate median by finding an interval point: We slice out the &lt;em>;N&lt;/em>; smallest points and the &lt;em>;N&lt;/em>; largest points and then compute an interval point of the remaining points. The latter must be an approximate median. This works when the dataset size is at least &lt;em>;3N&lt;/em>;. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;545&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf26PttXtoA75NoqKmNacVcsoHkCQFld1jOhqiOuml8CHIMbBWbcWlHXn0r6hC-697XA8ey-GvVDshwQsvaQRYb8DwD6wRMnR0Nuo2rxMqszE8OW06pZ184dCz4s9NdKBfHF0mPhqjoA2l7lepeZ88Xru1Q3Zrxvm85W5onykpmwLeU5Gm3v2fYNYfQEWN/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;An example of a data D over domain X, the set of interval points, and the set of approximate medians.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Private learning of axis-aligned rectangles&lt;/h2>; &lt;p>; For the next task, the input is a set of &lt;em>;n&lt;/em>; labeled data points, where each point &lt;em>;x = (x&lt;sub>;1&lt;/sub>;,....,x&lt;sub>;d&lt;/sub>;)&lt;/em>; is a &lt;em>;d&lt;/em>;-dimensional vector over a domain &lt;em>;X&lt;/em>;. Displayed below, the goal is to learn values &lt;em>;a&lt;sub>;i &lt;/sub>;, b&lt;sub>;i&lt;/sub>;&lt;/em>; for the axes &lt;em>;i=1,...,d&lt;/em>; that define a &lt;em>;d&lt;/em>;-dimensional rectangle, so that for each example &lt;em>;x&lt;/em>; &lt;/p>; &lt;ul>; &lt;li>;If &lt;em>;x&lt;/em>; is positively labeled (shown as red plus signs below) then it lies within the rectangle, that is, for all axes &lt;em>;i&lt;/em>;, &lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>; is in the interval &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>;, and &lt;/li>;&lt;li>;If &lt;em>;x&lt;/em>; is negatively labeled (shown as blue minus signs below) then it lies outside the rectangle, that is, for at least one axis &lt;em>;i&lt;/em>;, &lt;em>;x&lt;sub>;i&lt;/sub>;&lt;/em>; is outside the interval &lt;em>;[a&lt;sub>;i &lt;/sub>;,b&lt;sub>;i&lt;/sub>;]&lt;/em>;. &lt;/li>; &lt;/ul>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/s1656/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1160&quot; data-original-width=&quot;1656&quot; height=&quot;280&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNdbuXJ043M9Z94-l2x2Df--GPDu3CCvfru3EBHSig4GxylqwV-uiEf84bK0oLmGIaC5h0zSyimqMQR3BJPKqbm3fmWTrOVoKA3MJusfKfEHVM0UlYpki5rWfx0z2niP0pckX3FX_0cyV-nTeP5V9J5_PPRLMe13gQM1FZAXlLQ_WBM4nySEXpAO1rQBHS/w400-h280/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;A set of 2-dimensional labeled points and a respective rectangle.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; Any DP solution for this problem must be approximate in that the learned rectangle must be allowed to mislabel some data points, with some positively labeled points outside the rectangle or negatively labeled points inside it. This is because an exact solution could be very sensitive to the presence of a particular data point and would not be private. The goal is a DP solution that keeps this necessary number of mislabeled points small. &lt;/p>; &lt;p>; We first consider the one-dimensional case (&lt;em>;d = 1)&lt;/em>;. We are looking for an interval &lt;em>;[a,b]&lt;/em>; that covers all positive points and none of the negative points. We show that we can do this with at most &lt;em>;2N&lt;/em>; mislabeled points. We focus on the positively labeled points. In the first RSC step we slice out the &lt;em>;N&lt;/em>; smallest points and compute a private interval point as &lt;em>;a&lt;/em>;. We then slice out the &lt;em>;N&lt;/em>; largest points and compute a private interval point as &lt;em>;b&lt;/em>;. The solution&lt;em>; [a,b] &lt;/em>;correctly labels all negatively labeled points and mislabels at most &lt;em>;2N&lt;/em>; of the positively labeled points. Thus, at most ~&lt;em>;2N&lt;/em>; points are mislabeled in total. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s1999/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;441&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0lIq2EkGGyGn15GSI5mEEAya4zPNNoAjoCBOkGj4Qljsk7M02T8s55E3FI9r_0Vfgx14N7PRpXFQN07bdqBgslKAxkkIbjhuV5LNdoMySNWRvvIwzWB9XyTn8ASLvvVQI0f1BWuFtOEbiG7ydKIbvkP3o3R1TgP9lDuXCFFT4wr8zEst5PNvbnlLlCg4z/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Illustration for &lt;em>;d = 1&lt;/em>;, we slice out &lt;em>;N&lt;/em>; left positive points and compute an interval point &lt;em>;a&lt;/em>;, slice out &lt;em>;N&lt;/em>; right positive points and compute an interval point &lt;em>;b&lt;/em>;.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; With &lt;em>;d &amp;gt; 1&lt;/em>;, we iterate over the axes&lt;em>; i = 1,....,d&lt;/em>; and apply the above for the &lt;em>;i&lt;sup>;th &lt;/sup>;&lt;/em>;coordinates of input points to obtain the values &lt;em>;a&lt;sub>;i &lt;/sub>;, b&lt;sub>;i&lt;/sub>; &lt;/em>;. In each iteration, we perform two RSC steps and slice out &lt;em>;2N&lt;/em>; positively labeled points. In total, we slice out &lt;em>;2dN&lt;/em>; points and all remaining points were correctly labeled. That is, all negatively-labeled points are outside the final &lt;em>;d&lt;/em>;-dimensional rectangle and all positively-labeled points, except perhaps ~&lt;em>;2dN&lt;/em>;, lie inside the rectangle. Note that this algorithm uses the full flexibility of RSC in that the points are ordered differently by each axis. Since we perform &lt;em>;d&lt;/em>; steps, the RSC analysis shaves off a factor of square root of &lt;em>;d&lt;/em>; from the number of mislabeled points. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Training ML models with adaptive selection of training examples&lt;/h2>; &lt;p>; The training efficiency or performance of ML models can sometimes be improved by selecting training examples in a way that depends on the current state of the model, eg, self-paced &lt;a href=&quot;https://www.frontiersin.org/research-topics/36658/curriculum-learning-and-related-applications&quot;>;curriculum learning&lt;/a>; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Active_learning_(machine_learning)&quot;>;active learning&lt;/a>;. &lt;/p>; &lt;p>; The most common method for private training of ML models is &lt;a href=&quot;https://arxiv.org/abs/1607.00133&quot;>;DP-SGD&lt;/a>;, where noise is added to the gradient update from each minibatch of training examples. Privacy analysis with DP-SGD typically assumes that training examples are &lt;em>;randomly&lt;/em>; partitioned into minibatches. But if we impose a data-dependent selection order on training examples, and further modify the selection criteria &lt;em>;k&lt;/em>; times during training, then analysis through DP composition results in deterioration of the privacy guarantees of a magnitude equal to the square root of &lt;em>;k&lt;/em>;. &lt;/p>; &lt;p>; Fortunately, example selection with DP-SGD can be naturally expressed in the RSC paradigm: each selection criteria reorders the training examples and each minibatch is a slice (for which we compute a noisy gradient). With RSC analysis, there is no privacy deterioration with &lt;em>;k&lt;/em>;, which brings DP-SGD training with example selection into the practical domain. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; The RSC paradigm was introduced in order to tackle an open problem that is primarily of theoretical significance, but turns out to be a versatile tool with the potential to enhance data efficiency in production environments. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgments&lt;/h2>; &lt;p>; &lt;em>;The &lt;a href=&quot;https://arxiv.org/abs/2211.06387&quot;>;work&lt;/a>; described here was done jointly with &lt;a href=&quot;https://people.eecs.berkeley.edu/~xinlyu/&quot;>;Xin Lyu&lt;/a>;, Jelani Nelson, and Tamas Sarlos.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/237108111388574068/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/differentially-private-median-and-more.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/237108111388574068&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/differentially-private-median-and-more.html&quot; rel=&quot;alternate&quot; title=&quot;Differentially private median and more&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiY4hQkG4Ofm6plcTF56kw-L1RI613kh8ztTMZvJgmn2VpFF84K1TIfBymU6p_xVE0q-SRafCvZtrzdCgNz4qqsyqVTevfEffmxqhQ_Jg6rJQgo5vm3zoOkQ6JnxeMNQ4Y7LrOufGJB8lTnJoKSYZYke2qZi0rvO9PUB5POaYdou4O7lE2-IO7hD0__aJcy/s72-c/dpmedian.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-5967099570177693866&lt;/id>;&lt;published>;2023-09-07T15:03:00.000-07:00&lt;/published>;&lt;updated>;2023-09-07T15:03:10.510-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Algorithms&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;TPU&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;A novel computational fluid dynamics framework for turbulent flow research&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Shantanu Shahane, Software Engineer, and Matthias Ihme, Research Scientist, Athena Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1.png&quot; style=&quot;display: none;&quot; />; &lt;p>; Turbulence is ubiquitous in environmental and engineering fluid flows, and is encountered routinely in everyday life. A better understanding of these turbulent processes could provide valuable insights across a variety of research areas — improving the prediction of cloud formation by atmospheric transport and the spreading of wildfires by turbulent energy exchange, understanding sedimentation of deposits in rivers, and improving the efficiency of combustion in aircraft engines to reduce emissions, to name a few. However, despite its importance, our current understanding and our ability to reliably predict such flows remains limited. This is mainly attributed to the highly chaotic nature and the enormous spatial and temporal scales these fluid flows occupy, ranging from energetic, large-scale movements on the order of several meters on the high-end, where energy is injected into the fluid flow, all the way down to micrometers (μm) on the low-end, where the turbulence is dissipated into heat by viscous friction. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; A powerful tool to understand these turbulent flows is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Direct_numerical_simulation&quot;>;direct numerical simulation&lt;/a>; (DNS), which provides a detailed representation of the unsteady three-dimensional flow-field without making any approximations or simplifications. More specifically, this approach utilizes a discrete grid with small enough grid spacing to capture the underlying continuous equations that govern the dynamics of the system (in this case, variable-density &lt;a href=&quot;https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations&quot;>;Navier-Stokes equations&lt;/a>;, which govern all fluid flow dynamics). When the grid spacing is small enough, the discrete grid points are enough to represent the true (continuous) equations without the loss of accuracy. While this is attractive, such simulations require tremendous computational resources in order to capture the correct fluid-flow behaviors across such a wide range of spatial scales. &lt;/p>; &lt;p>; The actual span in spatial resolution to which direct numerical calculations must be applied depends on the task and is determined by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Reynolds_number&quot;>;Reynolds number&lt;/a>;, which compares inertial to viscous forces. Typically, the Reynolds number can range between 10&lt;sup>;2&lt;/sup>; up to 10&lt;sup>;7 &lt;/sup>;(even larger for atmospheric or interstellar problems). In 3D, the grid size for the resolution required scales roughly with the Reynolds number to the power of 4.5! Because of this strong scaling dependency, simulating such flows is generally limited to flow regimes with moderate Reynolds numbers, and typically requires access to &lt;a href=&quot;https://www.top500.org/lists/top500/2023/06/&quot;>;high-performance computing systems&lt;/a>; with millions of CPU/GPU cores. &lt;/p>; &lt;p>; In “&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;A TensorFlow simulation framework for scientific computing of fluid flows on tensor processing units&lt;/a>;”, we introduce a new simulation framework that enables the computation of fluid flows with TPUs. By leveraging latest advances on &lt;a href=&quot;https://www.tensorflow.org/&quot;>;TensorFlow&lt;/a>; software and TPU-hardware architecture, this software tool allows detailed large-scale simulations of turbulent flows at unprecedented scale, pushing the boundaries of scientific discovery and turbulence analysis. We demonstrate that this framework scales efficiently to accommodate the scale of the problem or, alternatively, improved run times, which is remarkable since most large-scale distributed computation frameworks exhibit reduced efficiency with scaling. The software is available as an open-source project on &lt;a href=&quot;https://github.com/google-research/swirl-lm&quot;>;GitHub&lt;/a>;. &lt;/p>; &lt;br />; &lt;h2>;Large-scale scientific computation with accelerators&lt;/h2>; &lt;p>; The software solves variable-density Navier-Stokes equations on TPU architectures using the TensorFlow framework. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;>;single-instruction, multiple-data&lt;/a>; (SIMD) approach is adopted for parallelization of the TPU solver implementation. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_difference_method&quot;>;finite difference&lt;/a>; operators on a &lt;a href=&quot;https://www.cfd-online.com/Wiki/Collocated_grid&quot;>;colocated&lt;/a>; structured mesh are cast as filters of the convolution function of TensorFlow, leveraging TPU&#39;s matrix multiply unit (MXU). The framework takes advantage of the low-latency high-bandwidth inter-chips interconnect (ICI) between the TPU accelerators. In addition, by leveraging the single-precision floating-point computations and highly optimized executable through the accelerated linear algebra (XLA) compiler, it&#39;s possible to perform large-scale simulations with excellent scaling on TPU hardware architectures. &lt;/p>; &lt;p>; This research effort demonstrates that the &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory&quot;>;graph&lt;/a>;-based TensorFlow in combination with new types of ML special purpose hardware, can be used as a programming paradigm to solve partial differential equations representing multiphysics flows. The latter is achieved by augmenting the Navier-Stokes equations with physical models to account for chemical reactions, heat-transfer, and density changes to enable, for example, simulations of &lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;cloud formation&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;wildfires&lt;/a>;. &lt;/p>; &lt;p>; It&#39;s worth noting that this framework is the first open-source &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_fluid_dynamics&quot;>;computational fluid dynamics&lt;/a>; (CFD) framework for high-performance, large-scale simulations to fully leverage the cloud accelerators that have become common (and become a commodity) with the advancement of machine learning (ML) in recent years. While our work focuses on using TPU accelerators, the code can be easily adjusted for other accelerators, such as GPU clusters. &lt;/p>; &lt;p>; This framework demonstrates a way to greatly reduce the cost and turn-around time associated with running large-scale scientific CFD simulations and enables even greater iteration speed in fields, such as climate and weather research. Since the framework is implemented using TensorFlow, an ML language, it also enables the ready integration with ML methods and allows the exploration of ML approaches on CFD problems. With the general accessibility of TPU and GPU hardware, this approach lowers the barrier for researchers to contribute to our understanding of large-scale turbulent systems. &lt;/p>; &lt;br />; &lt;h2>;Framework validation and homogeneous isotropic turbulence&lt;/h2>; &lt;p>; Beyond demonstrating the performance and the scaling capabilities, it is also critical to validate the correctness of this framework to ensure that when it is used for CFD problems, we get reasonable results. For this purpose, researchers typically use idealized benchmark problems during CFD solver development, many of which we adopted in our work (more details in the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0010465522000108?via%3Dihub&quot;>;paper&lt;/a>;). &lt;/p>; &lt;p>; One such benchmark for turbulence analysis is &lt;a href=&quot;https://en.wikipedia.org/wiki/Homogeneous_isotropic_turbulence&quot;>;homogeneous isotropic turbulence&lt;/a>;&lt;em>; &lt;/em>;(HIT), which is a canonical and well studied flow in which the statistical properties, such as kinetic energy, are invariant under translations and rotations of the coordinate axes. By pushing the resolution to the limits of the current state of the art, we were able to perform direct numerical simulations with more than eight billion degrees of freedom — equivalent to a three-dimensional mesh with 2,048 grid points along each of the three directions. We used 512 TPU-v4 cores, distributing the computation of the grid points along the x, y, and z axes to a distribution of [2,2,128] cores, respectively, optimized for the performance on TPU. The wall clock time per timestep was around 425 milliseconds and the flow was simulated for a total of 400,000 timesteps. 50 TB data, which includes the velocity and density fields, is stored for 400 timesteps (every 1,000th step). To our knowledge, this is one of the largest turbulent flow simulations of its kind conducted to date. &lt;/p>; &lt;p>; Due to the complex, chaotic nature of the turbulent flow field, which extends across several magnitudes of resolution, simulating the system in high resolution is necessary. Because we employ a fine-resolution grid with eight billion points, we are able to accurately resolve the field. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s990/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;894&quot; data-original-width=&quot;990&quot; height=&quot;361&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/w400-h361/image1.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Contours of &lt;em>;x&lt;/em>;-component of velocity along the &lt;em>;z&lt;/em>; midplane. The high resolution of the simulation is critical to accurately represent the turbulent field.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The turbulent kinetic energy and dissipation rates are two statistical quantities commonly used to analyze a turbulent flow. The temporal decay of these properties in a turbulent field without additional energy injection is due to viscous dissipation and the decay asymptotes follow the expected analytical &lt;a href=&quot;https://en.wikipedia.org/wiki/Energy_cascade&quot;>;power law&lt;/a>;. This is in agreement with the theoretical asymptotes and observations reported in the literature and thus, validates our framework. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s562/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;562&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3-1zJMcp9zIxyMwGiM6IXQB1yivDekU3Mjb_qnhQ7zmgjPyYJM0e3Ikula0GC_0tAeg5P58no7xPN97UmS7fkt8YvPwlGZcapjmEL3eQgZo1o1CJB-TtThNjVSWjDeXAyKHUymOUlaJm00z4cCnJDi305wwYtB1DjUpEU1VD_FCQZKGKsClTxvILUg_dC/s16000/image4.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Solid line: Temporal evolution of turbulent kinetic energy (&lt;em>;k&lt;/em>;). Dashed line: Analytical power laws for decaying homogeneous isotropic turbulence (&lt;em>;n&lt;/em>;=1.3) (&lt;em>;Ⲧ&lt;sub>;l&lt;/sub>;&lt;/em>;: &lt;a href=&quot;https://physics.stackexchange.com/questions/79184/what-are-the-length-and-time-scales-in-turbulence&quot;>;eddy turnover time&lt;/a>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s565/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;408&quot; data-original-width=&quot;565&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNrIxFSxg5L4FiX7pUXDuCTJ3GCfwrlPv2z3O7uHFHnQf5gBuWhlGlHWWUXt3i8M3F88hXnHG-wsJXn6mAZd4DsMuf2OR-sIgJeAGmleM2lY2kAijlsJqGuJxbzllHizQjJrWqiRDJra4xsI7rqBJlNp1hSp2aOkAp8yeYlubv9tx-kb4j51sDnhWBib81/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Solid line: Temporal evolution of dissipation rate (&lt;em>;ε&lt;/em>;). Dashed line: Analytical power laws for decaying homogeneous isotropic turbulence (n=1.3).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;p>; The energy spectrum of a turbulent flow represents the energy content across &lt;a href=&quot;https://en.wikipedia.org/wiki/Wavenumber&quot;>;wavenumber&lt;/a>;, where the wavenumber &lt;i>;k&lt;/i>; is proportional to the inverse wavelength &lt;i>;λ&lt;/i>; (ie, &lt;i>;k&lt;/i>; ∝ 1/&lt;i>;λ&lt;/i>;). Generally, the spectrum can be qualitatively divided into three ranges: source range, inertial range and viscous dissipative range (from left to right on the wavenumber axis, below). The lowest wavenumbers in the source range correspond to the largest turbulent eddies, which have the most energy content. These large eddies transfer energy to turbulence in the intermediate wavenumbers (inertial range), which is statistically isotropic (ie, essentially uniform in all directions). The smallest eddies, corresponding to the largest wavenumbers, are dissipated into thermal energy by the viscosity of the fluid. By virtue of the fine grid having 2,048 points in each of the three spatial directions, we are able to resolve the flow field up to the length scale at which viscous dissipation takes place. This direct numerical simulation approach is the most accurate as it does not require any closure model to approximate the energy cascade below the grid size. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s569/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;403&quot; data-original-width=&quot;569&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1DUNwvem2FOcU-F_HjZ1eJW1uHTlkPS_-Qmh31bQveJ4QElPBtTzSpyDeNJ-KFX8GHzsOLIzFzWE-0i94Q5BCAxiJuW8Epx5sVTF0DnW-5NYdIDzt7enLGvQLQk3M8nYHRKbofjquz5rrKTgqAuf72kDc_IZB1X-aI7MVAIvVPxQQ7N-k6DcBbh912IOG/s16000/image2.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Spectrum of turbulent kinetic energy at different time instances. The spectrum is normalized by the instantaneous integral length (&lt;em>;l&lt;/em>;) and the turbulent kinetic energy (&lt;em>;k&lt;/em>;).&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;A new era for turbulent flows research&lt;/h2>; &lt;p>; More recently, we extended this framework to predict &lt;a href=&quot;https://arxiv.org/abs/2212.05141&quot;>;wildfires&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2301.04698&quot;>;atmospheric flows&lt;/a>;, which is relevant for climate-risk assessment. Apart from enabling high-fidelity simulations of complex turbulent flows, this simulation framework also provides capabilities for &lt;a href=&quot;https://www.osti.gov/servlets/purl/1478744&quot;>;scientific machine learning&lt;/a>; (SciML) — for example, downsampling from a fine to a coarse grid (&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_order_reduction&quot;>;model reduction&lt;/a>;) or building models that run at lower resolution while still capturing the correct dynamic behaviors. It could also provide avenues for further scientific discovery, such as building ML-based models to better parameterize microphysics of turbulent flows, including physical relationships between temperature, pressure, vapor fraction, etc., and could improve upon various control tasks, eg, to reduce the energy consumption of buildings or find more efficient propeller shapes. While attractive, a main bottleneck in SciML has been the availability of data for training. To explore this, we have been working with groups at Stanford and Kaggle to make the data from our high-resolution HIT simulation available through a community-hosted web-platform, &lt;a href=&quot;https://blastnet.github.io&quot;>;BLASTNet&lt;/a>;, to provide broad access to high-fidelity data to the research community via a network-of-datasets approach. We hope that the availability of these emerging high-fidelity simulation tools in conjunction with community-driven datasets will lead to significant advances in various areas of fluid mechanics. &lt;/p>; &lt;br />; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;We would like to thank Qing Wang, Yi-Fan Chen, and John Anderson for consulting and advice, Tyler Russell and Carla Bromberg for program management. &lt;/em>; &lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/5967099570177693866/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/5967099570177693866&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/a-novel-computational-fluid-dynamics.html&quot; rel=&quot;alternate&quot; title=&quot;A novel computational fluid dynamics framework for turbulent flow research&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi64RuCU05-EPPWn16XFfdaYYsN4F3WexwvCG-tto-hzDxR2kEsRpngok6epC9bApbGhaYWLsT4r1oj5zaGDi3JygwZWAsOpeJy6WgBiBJ5tBjp-nEP9LDoiF56tqNq0wtkKjiIAzaCwinN8TXddkODNGBlf8PDZmOhZwC92mCfPwFwXFm5P7rRQCfuzZ8E/s72-c/image1.png&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-8763480087701216145&lt;/id>;&lt;published>;2023-09-06T12:47:00.001-07:00&lt;/published>;&lt;updated>;2023-09-12T16:01:52.707-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Deep Learning&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;Machine Learning&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;TSMixer: An all-MLP architecture for time series forecasting&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Si-An Chen, Student Researcher, Cloud AI Team, and Chun-Liang Li, Research Scientist, Cloud AI Team&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s320/hero.jpg&quot; style=&quot;display: none;&quot; />; &lt;p>; &lt;span style=&quot;font-size: small;&quot;>;&lt;i>;&lt;b>;Update — 2023/09/12:&lt;/b>; This post has been updated as the work described is now published in &lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;&lt;em>;Transactions on Machine Learning Research&lt;/em>;&lt;/a>;.&lt;/i>;&lt;/span>; &lt;/p>; &lt;p>; &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_series&quot;>;Time series&lt;/a>; forecasting is critical to various real-world applications, from &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;demand forecasting&lt;/a>; to &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/29606177/&quot;>;pandemic spread prediction&lt;/a>;. In &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;multivariate time series forecasting&lt;/a>; (forecasting multiple variants at the same time), one can split existing methods into two categories: univariate models and multivariate models. Univariate models focus on inter-series interactions or temporal patterns that encompass trends and seasonal patterns on a time series with a single variable. Examples of such trends and seasonal patterns might be the way mortgage rates increase due to inflation, and how traffic peaks during rush hour. In addition to inter-series patterns, multivariate models process intra-series features, known as cross-variate information, which is especially useful when one series is an advanced indicator of another series. For example, a rise in body weight may cause an increase in blood pressure, and increasing the price of a product may lead to a decrease in sales. Multivariate models have &lt;a href=&quot;https://research.google/pubs/pub49697/&quot;>;recently become popular solutions&lt;/a>; for multivariate forecasting as practitioners believe their capability of handling cross-variate information may lead to better performance. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; In recent years, deep learning Transformer-based architectures have become a popular choice for multivariate forecasting models due to their superior performance on sequence tasks. However, advanced multivariate models &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;perform surprisingly worse&lt;/a>; than simple univariate linear models on commonly-used &lt;a href=&quot;https://github.com/thuml/Autoformer&quot;>;long-term forecasting benchmarks&lt;/a>;, such as &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;Electricity Transformer Temperature&lt;/a>; (ETT), &lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption&quot;>;Electricity&lt;/a>;, &lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;Traffic&lt;/a>;, and &lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;Weather&lt;/a>;. These results raise two questions: &lt;/p>; &lt;ul>; &lt;li>;Does cross-variate information benefit time series forecasting? &lt;/li>;&lt;li>;When cross-variate information is not beneficial, can multivariate models still perform as well as univariate models? &lt;/li>; &lt;/ul>; &lt;p>; In “&lt;a href=&quot;https://arxiv.org/abs/2303.06053&quot;>;TSMixer: An All-MLP Architecture for Time Series Forecasting&lt;/a>;”, published in the &lt;em>;&lt;a href=&quot;https://www.jmlr.org/tmlr/&quot;>;Transactions on Machine Learning Research&lt;/a>;&lt;/em>; (TMLR), we analyze the advantages of univariate linear models and reveal their effectiveness. Insights from this analysis lead us to develop Time-Series Mixer (TSMixer), an advanced multivariate model that leverages linear model characteristics and performs well on long-term forecasting benchmarks. To the best of our knowledge, TSMixer is the first multivariate model that performs as well as state-of-the-art univariate models on long-term forecasting benchmarks, where we show that cross-variate information is less beneficial. To demonstrate the importance of cross-variate information, we evaluate a more challenging real-world application, &lt;a href=&quot;https://www.kaggle.com/competitions/m5-forecasting-accuracy&quot;>;M5&lt;/a>;. Finally, empirical results show that TSMixer outperforms state-of-the-art models, such as &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;Fedformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;TSMixer architecture&lt;/h2>; &lt;p>; A key difference between linear models and Transformers is how they capture temporal patterns. On one hand, linear models apply fixed and time-step-dependent weights to capture static temporal patterns, and are unable to process cross-variate information. On the other hand, Transformers use &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;>;attention mechanisms&lt;/a>; that apply dynamic and data-dependent weights at each time step, capturing dynamic temporal patterns and enabling them to process cross-variate information. &lt;/p>; &lt;p>; In our analysis, we show that under common assumptions of temporal patterns, linear models have naïve solutions to perfectly recover the time series or place bounds on the error, which means they are great solutions for learning static temporal patterns of univariate time series more effectively. In contrast, it is non-trivial to find similar solutions for attention mechanisms, as the weights applied to each time step are dynamic. Consequently, we develop a new architecture by replacing Transformer attention layers with linear layers. The resulting TSMixer model, which is similar to the computer vision &lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;>;MLP-Mixer&lt;/a>; method, alternates between applications of the multi-layer perceptron in different directions, which we call &lt;em>;time-mixing&lt;/em>; and &lt;em>;feature-mixing,&lt;/em>; respectively. The TSMixer architecture efficiently captures both temporal patterns and cross-variate information, as shown in the figure below. The residual designs ensure that TSMixer retains the capacity of temporal linear models while still being able to exploit cross-variate information. &lt;/p>; &lt;p>; &lt;/p>;&lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;984&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw_SFP7ILbDipSl_mCrLEj272nQRB9waivQ_4vQMcQxe1WC0WNzIce18W8RI7nMdXJt6GxAqyilxUY3lV1paiioZ2nw4n3mY4JJx8Lo74kiLsL7Brbz_3y-SEhp28PNrnOqjFlkfRp7KoKeYZMfkdgq1RNRAGumLM1NOMueTR2V_R4QjyTuUKgKP2_T4ud/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Transformer block and TSMixer block architectures. TSMixer replaces the multi-head attention layer with time-mixing, a linear model applied on the time dimension.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/s1646/image4.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;802&quot; data-original-width=&quot;1646&quot; height=&quot;195&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr49RR7fy08ybWp3D8WjNZULpQ6fySqxyNaEr_kquu-jvRilCzW16YIAtqOeP32b7k6iuhmfcYnOiojWR3aeYut1sSnF8NuQiznbfrqB0cRopGe4GCKupIQZnbNt8XutFC5Hw_DhQ_T4yFowg-pmm4JuKV5cNYKMgybmG34ym1Uz71iBDFPJC26EKRdAQ1/w400-h195/image4.png&quot; width=&quot;400&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Comparison between data-dependent (attention mechanisms) and time-step-dependent (linear models). This is an example of forecasting the next time step by learning the weights of the previous three time steps.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on long-term forecasting benchmarks&lt;/h2>; &lt;p>; We evaluate TSMixer using seven popular &lt;a href=&quot;https://github.com/thuml/Autoformer&quot;>;long-term forecasting datasets&lt;/a>; (&lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm1&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTm2&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh1&lt;/a>;, &lt;a href=&quot;https://github.com/zhouhaoyi/ETDataset&quot;>;ETTh2&lt;/a>;, &lt;a href=&quot;https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption&quot;>;Electricity&lt;/a>;, &lt;a href=&quot;https://pems.dot.ca.gov/&quot;>;Traffic&lt;/a>;, and &lt;a href=&quot;https://www.bgc-jena.mpg.de/wetter/&quot;>;Weather&lt;/a>;), where &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;recent research&lt;/a>; has shown that univariate linear models outperform advanced multivariate models with large margins. We compare TSMixer with state-of-the-art multivariate models (&lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2201.12740&quot;>;FEDformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;>;Autoformer&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2012.07436&quot;>;Informer&lt;/a>;), and univariate models, including &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;linear models&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;>;PatchTST&lt;/a>;. The figure below shows the average improvement of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;>;mean squared error&lt;/a>; (MSE) by TSMixer compared with others. The average is calculated across datasets and multiple forecasting horizons. We demonstrate that TSMixer significantly outperforms other multivariate models and performs on par with state-of-the-art univariate models. These results show that multivariate models are capable of performing as well as univariate models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s1200/image5.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1BrnOjgp1Ukcg7Cq_K08bcNgdyt8GScMggeM9jDZQdPulids4TRsJiJ9bVVWimhd669L8tPc09aMCexgZoSFTec3iB-TEie_hjFsSG8RhMLex0bPc6lu2GQKHbP3DRbgnu8Vcc4TH1aVZGzrY_2WIS_0Op9rnuzkhxlIFfOOt2pRXBFJORLGUFKikM7YV/s16000/image5.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The average MSE improvement of TSMixer compared with other baselines. The red bars show multivariate methods and the blue bars show univariate methods. TSMixer achieves significant improvement over other multivariate models and achieves comparable results to univariate models.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Ablation study&lt;/h2>; &lt;p>; We performed an ablation study to compare TSMixer with TMix-Only, a TSMixer variant that consists of time mixing layers only. The results show that TMix-Only performs almost the same as TSMixer, which means the additional feature mixing layers do not improve the performance and confirms that cross-variate information is less beneficial on popular benchmarks. The results validate the superior univariate model performance shown in &lt;a href=&quot;https://arxiv.org/abs/2205.13504&quot;>;previous research&lt;/a>;. However, existing long-term forecasting benchmarks are not well representative of the need for cross-variate information in some real-world applications where time series may be intermittent or sparse, hence temporal patterns may not be sufficient for forecasting. Therefore, it may be inappropriate to evaluate multivariate forecasting models solely on these benchmarks. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Evaluation on M5: Effectiveness of cross-variate information&lt;/h2>; &lt;p>; To further demonstrate the benefit of multivariate models, we evaluate TSMixer on the challenging M5 benchmark, a large-scale retail dataset containing crucial cross-variate interactions. M5 contains the information of 30,490 products collected over 5 years. Each product description includes time series data, like daily sales, sell price, promotional event information, and static (non-time-series) features, such as store location and product category. The goal is to forecast the daily sales of each product for the next 28 days, evaluated using the &lt;a href=&quot;https://mofc.unic.ac.cy/m5-competition/&quot;>;weighted root mean square scaled error&lt;/a>; (WRMSSE) from the M5 competition. The complicated nature of retail makes it more challenging to forecast solely using univariate models that focus on temporal patterns, so multivariate models with cross-variate information and even auxiliary features are more essential. &lt;/p>; &lt;p>; First, we compare TSMixer to other methods only considering the historical data, such as daily sales and historical sell prices. The results show that multivariate models outperforms univariate models significantly, indicating the usefulness of cross-variate information. And among all compared methods, TSMixer effectively leverages the cross-variate information and achieves the best performance. &lt;/p>; &lt;p>; Additionally, to leverage more information, such as static features (eg, store location, product category) and future time series (eg, a promotional event scheduled in coming days) provided in M5, we propose a principle design to extend TSMixer. The extended TSMixer aligns different types of features into the same length, and then applies multiple mixing layers to the concatenated features to make predictions. The extended TSMixer architecture outperforms models popular in industrial applications, including &lt;a href=&quot;https://arxiv.org/abs/1704.04110&quot;>;DeepAR&lt;/a>; and &lt;a href=&quot;https://arxiv.org/abs/1912.09363&quot;>;TFT&lt;/a>;, showcasing its strong potential for real-world impact. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/s1950/image2.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1950&quot; data-original-width=&quot;1760&quot; height=&quot;640&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8RkwlTGh8ETAOhDUtMcEn_kvQDAEhBYwL28HLpRt_I9jbVfZzjfaKL-mS0GtR44CxLxpG2bbiFShtqqPUTm5IQQgaVscfd-K5hQniB7N3iM6tc22xUVqu3CTb6Ar5OF0JustcEoFoPoUmXYyfn8AtSqKiAz3aXLaZ4Zjpp1tUlwSe_Uwrn80HVzguxx0/w578-h640/image2.png&quot; width=&quot;578&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The architecture of the extended TSMixer. In the first stage (align stage), it aligns the different types of features into the same length before concatenating them. In the second stage (mixing stage) it applies multiple mixing layers conditioned with static features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s1200/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;742&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn7uznYR6_rpDSjAkdEU0fyrbl0Fg4i9-bnQ6am2v0Q0jo5oTz0YUYDtZdCaMAyHxKPJQqmoSLKDfAL0C6LUC7DiwK6gLtk214dIlklUNbKZQpkugGHhpuQKrpX1Unwpm-WklREEclsjCnzMuZfFtoMrSzlPm29BuNULNXZ_hA46iTaDBpogHn7iVJ615O/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;The WRMSSE on M5. The first three methods (&lt;strong>;blue&lt;/strong>;) are univariate models. The middle three methods (&lt;strong>;orange&lt;/strong>;) are multivariate models that consider only historical features. The last three methods (&lt;strong>;red&lt;/strong>;) are multivariate models that consider historical, future, and static features.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;br />; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; We present TSMixer, an advanced multivariate model that leverages linear model characteristics and performs as well as state-of-the-art univariate models on long-term forecasting benchmarks. TSMixer creates new possibilities for the development of time series forecasting architectures by providing insights into the importance of cross-variate and auxiliary information in real-world scenarios. The empirical results highlight the need to consider more realistic benchmarks for multivariate forecasting models in future research. We hope that this work will inspire further exploration in the field of time series forecasting, and lead to the development of more powerful and effective models that can be applied to real-world applications. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;This research was conducted by Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister.&lt;/em>; &lt;/p>;&lt;p>;&lt;/p>;&lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/8763480087701216145/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/8763480087701216145&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html&quot; rel=&quot;alternate&quot; title=&quot;TSMixer: An all-MLP architecture for time series forecasting&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEii5BTCsKal44jn1oYu-ILYHeAog8SPGZZ-i8g6Q2C0di7Wl-RcKI7jblQEd7sAtFINsCL8gPMBJQ449s1cTbxIzQizRmdjesDg2g4BgCqd24e3Iozp8nC1C9KNmJ7M9iUS8EnuM0uPs5Z6mNzVFOrlq1HcmurtARWu-T7KzL97qpjGqJhAHUzy46yjR2lH/s72-c/hero.jpg&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;entry>;&lt;id>;tag:blogger.com,1999:blog-8474926331452026626.post-6223088894812086013&lt;/id>;&lt;published>;2023-08-31T10:14:00.000-07:00&lt;/published>;&lt;updated>;2023-08-31T10:14:27.378-07:00&lt;/updated>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;datasets&quot;>;&lt;/category>;&lt;category scheme=&quot;http://www.blogger.com/atom/ns#&quot; term=&quot;ML&quot;>;&lt;/category>;&lt;title type=&quot;text&quot;>;WeatherBench 2: A benchmark for the next generation of data-driven weather models&lt;/stitle>;&lt;content type=&quot;html&quot;>;&lt;span class=&quot;byline-author&quot;>;Posted by Stephan Rasp, Research Scientist, and Carla Bromberg, Program Lead, Google Research&lt;/span>; &lt;img src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s800/weatherbenchgif.gif&quot; style=&quot;display: none;&quot; />; &lt;p>; In 1950, weather forecasting started its digital revolution when researchers used the first programmable, general-purpose computer &lt;a href=&quot;https://en.wikipedia.org/wiki/ENIAC#:~:text=ENIAC%20(%2F%CB%88%C9%9Bni,of%20them%20in%20one%20computer.&quot;>;ENIAC&lt;/a>; to solve mathematical equations describing how weather evolves. In the more than 70 years since, continuous advancements in computing power and improvements to the model formulations have led to steady gains in weather forecast skill: a 7-day forecast today is about as accurate as a 5-day forecast in 2000 and a 3-day forecast in 1980. While improving forecast accuracy at the pace of approximately one day per decade may not seem like a big deal, every day improved is important in far reaching use cases, such as for logistics planning, disaster management, agriculture and energy production. This &lt;a href=&quot;https://www.nature.com/articles/nature14956&quot;>;“quiet” revolution&lt;/a>; has been tremendously valuable to society, saving lives and providing economic value across many sectors. &lt;/p>; &lt;a name=&#39;more&#39;>;&lt;/a>; &lt;p>; Now we are seeing the start of yet another revolution in weather forecasting, this time fueled by advances in machine learning (ML). Rather than hard-coding approximations of the physical equations, the idea is to have algorithms learn how weather evolves from looking at large volumes of past weather data. Early attempts at doing so go back to &lt;a href=&quot;https://gmd.copernicus.org/articles/11/3999/2018/&quot;>;2018&lt;/a>; but the pace picked up considerably in the last two years when several large ML models demonstrated weather forecasting skill comparable to the best physics-based models. Google&#39;s MetNet [&lt;a href=&quot;https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html&quot;>;1&lt;/a>;, &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;2&lt;/a>;], for instance, demonstrated state-of-the-art capabilities for forecasting regional weather one day ahead. For global prediction, Google DeepMind created &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>;, a graph neural network to make 10 day predictions at a horizontal resolution of 25 km, competitive with the best physics-based models in many skill metrics. &lt;/p>; &lt;p>; Apart from potentially providing more accurate forecasts, one key advantage of such ML methods is that, once trained, they can create forecasts in a matter of minutes on inexpensive hardware. In contrast, traditional weather forecasts require large super-computers that run for hours every day. Clearly, ML represents a tremendous opportunity for the weather forecasting community. This has also been recognized by leading weather forecasting centers, such as the &lt;a href=&quot;https://www.ecmwf.int/&quot;>;European Centre for Medium-Range Weather Forecasts&#39;&lt;/a>; (ECMWF) &lt;a href=&quot;https://www.ecmwf.int/en/elibrary/81207-machine-learning-ecmwf-roadmap-next-10-years&quot;>;machine learning roadmap&lt;/a>; or the &lt;a href=&quot;https://www.noaa.gov/&quot;>;National Oceanic and Atmospheric Administration&#39;s&lt;/a>; (NOAA) &lt;a href=&quot;https://sciencecouncil.noaa.gov/wp-content/uploads/2023/04/2020-AI-Strategy.pdf&quot;>;artificial intelligence strategy&lt;/a>;. &lt;/p>; &lt;p>; To ensure that ML models are trusted and optimized for the right goal, forecast evaluation is crucial. Evaluating weather forecasts isn&#39;t straightforward, however, because weather is an incredibly multi-faceted problem. Different end-users are interested in different properties of forecasts, for example, renewable energy producers care about wind speeds and solar radiation, while crisis response teams are concerned about the track of a potential cyclone or an impending heat wave. In other words, there is no single metric to determine what a “good” weather forecast is, and the evaluation has to reflect the multi-faceted nature of weather and its downstream applications. Furthermore, differences in the exact evaluation setup — eg, which resolution and ground truth data is used — can make it difficult to compare models. Having a way to compare novel and established methods in a fair and reproducible manner is crucial to measure progress in the field. &lt;/p>; &lt;p>; To this end, we are announcing &lt;a href=&quot;https://arxiv.org/abs/2308.15560&quot;>;WeatherBench 2&lt;/a>; (WB2), a benchmark for the next generation of data-driven, global weather models. WB2 is an update to the &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002203&quot;>;original benchmark&lt;/a>; published in 2020, which was based on initial, lower-resolution ML models. The goal of WB2 is to accelerate the progress of data-driven weather models by providing a trusted, reproducible framework for evaluating and comparing different methodologies. The &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;official website&lt;/a>; contains scores from several state-of-the-art models (at the time of writing, these are &lt;a href=&quot;https://arxiv.org/abs/2202.07575&quot;>;Keisler (2022)&lt;/a>;, an early graph neural network, Google DeepMind&#39;s &lt;a href=&quot;https://arxiv.org/abs/2212.12794&quot;>;GraphCast&lt;/a>; and Huawei&#39;s &lt;a href=&quot;https://www.nature.com/articles/s41586-023-06185-3&quot;>;Pangu-Weather&lt;/a>;, a transformer-based ML model). In addition, forecasts from ECMWF&#39;s high-resolution and ensemble forecasting systems are included, which represent some of the best traditional weather forecasting models. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s1960/image3.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;372&quot; data-original-width=&quot;1960&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjRJ6Zic7q_7ljEHNtNdFWR7gkeUo05bYgufz-oARYQhC5HfPRafLrcSBiTx0pkKAjF0nTCNd7tsfFqi87CHWoL3hPB82GvFv2luh4j_BkavTXp9I0P61xnFiySI155saPzteM1vmkDKMODX7dCITr0QygUtbG9ZClwNJCRdlhh_AKlSoBk7s9uAOAKUTlB/s16000/image3.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Making evaluation easier&lt;/h2>; &lt;p>; The key component of WB2 is an &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;open-source evaluation framework&lt;/a>; that allows users to evaluate their forecasts in the same manner as other baselines. Weather forecast data at high-resolutions can be quite large, making even evaluation a computational challenge. For this reason, we built our evaluation code on &lt;a href=&quot;https://beam.apache.org/&quot;>;Apache Beam&lt;/a>;, which allows users to split computations into smaller chunks and evaluate them in a distributed fashion, for example using &lt;a href=&quot;https://cloud.google.com/dataflow&quot;>;DataFlow&lt;/a>; on Google Cloud. The code comes with a &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/evaluation.html&quot;>;quick-start guide&lt;/a>; to help people get up to speed. &lt;/p>; &lt;p>; Additionally, we &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/data-guide.html&quot;>;provide&lt;/a>; most of the ground-truth and baseline data on Google Cloud Storage in cloud-optimized &lt;a href=&quot;https://zarr.dev/&quot;>;Zarr&lt;/a>; format at different resolutions, for example, a comprehensive copy of the &lt;a href=&quot;https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803&quot;>;ERA5&lt;/a>; dataset used to train most ML models. This is part of a larger Google effort to provide &lt;a href=&quot;https://github.com/google-research/arco-era5&quot;>;analysis-ready, cloud-optimized weather and climate datasets&lt;/a>; to the research community and &lt;a href=&quot;https://github.com/google-research/arco-era5#roadmap&quot;>;beyond&lt;/a>;. Since downloading these data from the respective archives and converting them can be time-consuming and compute-intensive, we hope that this should considerably lower the entry barrier for the community. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Assessing forecast skill&lt;/h2>; &lt;p>; Together with our collaborators from &lt;a href=&quot;https://www.ecmwf.int/&quot;>;ECMWF&lt;/a>;, we defined a set of headline scores that best capture the quality of global weather forecasts. As the figure below shows, several of the ML-based forecasts have lower errors than the &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts&quot;>;state-of-the-art physical models&lt;/a>; on deterministic metrics. This holds for a range of variables and regions, and underlines the competitiveness and promise of ML-based approaches. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s1999/image1.png&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;960&quot; data-original-width=&quot;1999&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpPreKUXXp_lIzhut_DpaGJ14zOmmcY-BwafhfG4G3dbzlUN_-BlfWi5HxKB2upOLaUR38i-Qc1AkIKz0QX1BIhCI9XtxfFzMCqVOuSFzlg-7pAfYMQYN7YmIt84DRr_M9fHXT6hxAXGstGSbQ2duNZzJtZe5yIb1lrbBfFUkNkTgXhYXc9qjkqadlQwKh/s16000/image1.png&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;This scorecard shows the skill of different models compared to ECMWF&#39;s &lt;a href=&quot;https://www.ecmwf.int/en/forecasts/documentation-and-support/changes-ecmwf-model&quot;>;Integrated Forecasting System&lt;/a>; (IFS), one of the best physics-based weather forecasts, for several variables. IFS forecasts are evaluated against IFS analysis. All other models are evaluated against ERA5. The order of ML models reflects publication date.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Toward reliable probabilistic forecasts&lt;/h2>; &lt;p>; However, a single forecast often isn&#39;t enough. Weather is inherently chaotic because of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Butterfly_effect&quot;>;butterfly effect&lt;/a>;. For this reason, operational weather centers now run ~50 slightly perturbed realizations of their model, called an ensemble, to estimate the forecast probability distribution across various scenarios. This is important, for example, if one wants to know the likelihood of extreme weather. &lt;/p>; &lt;p>; Creating reliable probabilistic forecasts will be one of the next key challenges for global ML models. Regional ML models, such as Google&#39;s &lt;a href=&quot;https://arxiv.org/abs/2306.06079&quot;>;MetNet&lt;/a>; already estimate probabilities. To anticipate this next generation of global models, WB2 already provides probabilistic metrics and baselines, among them &lt;a href=&quot;https://www.ecmwf.int/en/about/media-centre/focus/2017/fact-sheet-ensemble-weather-forecasting&quot;>;ECMWF&#39;s IFS ensemble&lt;/a>;, to accelerate research in this direction. &lt;/p>; &lt;p>; As mentioned above, weather forecasting has many aspects, and while the headline metrics try to capture the most important aspects of forecast skill, they are by no means sufficient. One example is forecast realism. Currently, many ML forecast models tend to “hedge their bets” in the face of the intrinsic uncertainty of the atmosphere. In other words, they tend to predict smoothed out fields that give lower average error but do not represent a realistic, physically consistent state of the atmosphere. An example of this can be seen in the animation below. The two data-driven models, Pangu-Weather and GraphCast (bottom), predict the large-scale evolution of the atmosphere remarkably well. However, they also have less small-scale structure compared to the ground truth or the physical forecasting model IFS HRES (top). In WB2 we include a range of these case studies and also a spectral metric that quantifies such blurring. &lt;/p>; &lt;table align=&quot;center&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;tr-caption-container&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;tbody>;&lt;tr>;&lt;td style=&quot;text-align: center;&quot;>;&lt;a href=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s1200/image2.gif&quot; style=&quot;margin-left: auto; margin-right: auto;&quot;>;&lt;img border=&quot;0&quot; data-original-height=&quot;1020&quot; data-original-width=&quot;1200&quot; src=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4wXcTQkAGkbuhb__Df-rb5aC9iq11GMRSxV4ESzRlk22iiL2Y-08zH2oQDspXn4KxdQlzkO_SD0tX6-ZkxF_5hdQslK1PKIgB0HwKeCdYUZLtr_H2603WSXi0oDD_Brn7KHaAeO6Hq8g7-MscBpAGn7lV7WLUNX6RPxl1qA7RgO3ZUlOjLP0dX7ifrxsm/s16000/image2.gif&quot; />;&lt;/a>;&lt;/td>;&lt;/tr>;&lt;tr>;&lt;td class=&quot;tr-caption&quot; style=&quot;text-align: center;&quot;>;Forecasts of a front passing through the continental United States initialized on January 3, 2020. Maps show temperature at a pressure level of 850 &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;>;hPa&lt;/a>; (roughly equivalent to an altitude of 1.5km) and &lt;a href=&quot;https://sites.research.google/weatherbench/faq/&quot;>;geopotential&lt;/a>; at a pressure level of 500 hPa (roughly 5.5 km) in contours. ERA5 is the corresponding ground-truth analysis, IFS HRES is ECMWF&#39;s physics-based forecasting model.&lt;/td>;&lt;/tr>;&lt;/tbody>;&lt;/table>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Conclusion&lt;/h2>; &lt;p>; WeatherBench 2 will continue to evolve alongside ML model development. The &lt;a href=&quot;https://sites.research.google/weatherbench&quot;>;official website&lt;/a>; will be updated with the latest state-of-the-art models. (To submit a model, please follow &lt;a href=&quot;https://weatherbench2.readthedocs.io/en/latest/submit.html&quot;>;these instructions&lt;/a>;). We also invite the community to provide feedback and suggestions for improvements through issues and pull requests on the &lt;a href=&quot;https://github.com/google-research/weatherbench2&quot;>;WB2 GitHub page&lt;/a>;. &lt;/p>; &lt;p>; Designing evaluation well and targeting the right metrics is crucial in order to make sure ML weather models benefit society as quickly as possible. WeatherBench 2 as it is now is just the starting point. We plan to extend it in the future to address key issues for the future of ML-based weather forecasting. Specifically, we would like to add station observations and better precipitation datasets. Furthermore, we will explore the inclusion of nowcasting and subseasonal-to-seasonal predictions to the benchmark. &lt;/p>; &lt;p>; We hope that WeatherBench 2 can aid researchers and end-users as weather forecasting continues to evolve. &lt;/p>; &lt;div style=&quot;line-height: 40%;&quot;>; &lt;br />; &lt;/div>; &lt;h2>;Acknowledgements&lt;/h2>; &lt;p>; &lt;em>;WeatherBench 2 is the result of collaboration across many different teams at Google and external collaborators at ECMWF. From ECMWF, we would like to thank Matthew Chantry, Zied Ben Bouallegue and Peter Dueben. From Google, we would like to thank the core contributors to the project: Stephan Rasp, Stephan Hoyer, Peter Battaglia, Alex Merose, Ian Langmore, Tyler Russell, Alvaro Sanchez, Antonio Lobato, Laurence Chiu, Rob Carver, Vivian Yang, Shreya Agrawal, Thomas Turnbull, Jason Hickey, Carla Bromberg, Jared Sisk, Luke Barrington, Aaron Bell, and Fei Sha. We also would like to thank Kunal Shah, Rahul Mahrsee, Aniket Rawat, and Satish Kumar. Thanks to John Anderson for sponsoring WeatherBench 2. Furthermore, we would like to thank Kaifeng Bi from the Pangu-Weather team and Ryan Keisler for their help in adding their models to WeatherBench 2.&lt;/em>; &lt;/p>; &lt;/content>;&lt;link href=&quot;http://blog.research.google/feeds/6223088894812086013/comments/default&quot; rel=&quot;replies&quot; title=&quot;Post Comments&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html#comment-form&quot; rel=&quot;replies&quot; title=&quot;0 Comments&quot; type=&quot;text/html&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;edit&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://www.blogger.com/feeds/8474926331452026626/posts/default/6223088894812086013&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/>;&lt;link href=&quot;http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html&quot; rel=&quot;alternate&quot; title=&quot;WeatherBench 2: A benchmark for the next generation of data-driven weather models&quot; type=&quot;text/html&quot;/>;&lt;author>;&lt;name>;Google AI&lt;/name>;&lt;uri>;http://www.blogger.com/profile/12098626514775266161&lt;/uri>;&lt;email>;noreply@blogger.com&lt;/email>;&lt;gd:image height=&quot;16&quot; rel=&quot;http://schemas.google.com/g/2005#thumbnail&quot; src=&quot;https://img1.blogblog.com/img/b16-rounded.gif&quot; width=&quot;16&quot;>;&lt;/gd:image>;&lt;/author>;&lt;media:thumbnail height=&quot;72&quot; url=&quot;https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5eOQYPB02B9EYPx0YyLxhs7YAim5PDpywWihOvWr4zD18_NmXuUqzpZfZFdjdsi2hvZyKcB0ODholetAjBMQ43Q36V0UT-C4JYNHTCXN18ZxZGsR1MHeGsRCsp1CeZHU4D_vXhsojnMNxg_BWuhvONehmRZtqCoz5VuQsGavwfYUgPyC05Hg54wlRzivo/s72-c/weatherbenchgif.gif&quot; width=&quot;72&quot; xmlns:media=&quot;http://search.yahoo.com/mrss/&quot;>;&lt;/media:thumbnail>;&lt;thr:total>;0&lt;/thr:total>;&lt;/entry>;&lt;/feed>;